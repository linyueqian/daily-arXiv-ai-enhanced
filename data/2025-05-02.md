<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 89]
- [cs.CV](#cs.CV) [Total: 70]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.LG](#cs.LG) [Total: 103]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.MM](#cs.MM) [Total: 0]
- [eess.AS](#eess.AS) [Total: 2]
- [eess.IV](#eess.IV) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning](https://arxiv.org/abs/2505.00001)
*Shaun Baek, Shaun Esua-Mensah, Cyrus Tsui, Sejan Vigneswaralingam, Abdullah Alali, Michael Lu, Vasu Sharma, Kevin Zhu*

Main category: cs.CL

TL;DR: Rosetta-PL benchmark evaluates LLMs' logical reasoning by translating logical propositions into a custom language, showing improved precision with preserved logical relationships and dataset size.


<details>
  <summary>Details</summary>
Motivation: LLMs are limited in low-resource settings and logical reasoning tasks, prompting the need for a specialized benchmark like Rosetta-PL.

Method: Translate logical propositions from Lean into a custom language, fine-tune an LLM (e.g., GPT-4o), and analyze dataset size and translation impact.

Result: Preserving logical relationships boosts precision; accuracy plateaus beyond ~20,000 samples.

Conclusion: Rosetta-PL offers guidelines for optimizing LLM training in formal reasoning and low-resource language tasks.

Abstract: Large Language Models (LLMs) are primarily trained on high-resource natural
languages, limiting their effectiveness in low-resource settings and in tasks
requiring deep logical reasoning. This research introduces Rosetta-PL, a
benchmark designed to evaluate LLMs' logical reasoning and generalization
capabilities in a controlled environment. We construct Rosetta-PL by
translating a dataset of logical propositions from Lean into a custom logical
language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our
experiments analyze the impact of the size of the dataset and the translation
methodology on the performance of the model. Our results indicate that
preserving logical relationships in the translation process significantly
boosts precision, with accuracy plateauing beyond roughly 20,000 training
samples. These insights provide valuable guidelines for optimizing LLM training
in formal reasoning tasks and improving performance in various low-resource
language applications.

</details>


### [2] [Symbol grounding in computational systems: A paradox of intentions](https://arxiv.org/abs/2505.00002)
*Vincent C. Müller*

Main category: cs.CL

TL;DR: Computationalism's paradox: it implies semantic nativism whether the mind computes over meaningful or meaningless symbols.


<details>
  <summary>Details</summary>
Motivation: To highlight a flaw in computationalism regarding symbol grounding and intentionality.

Method: Logical analysis of computationalism's assumptions about symbol processing.

Result: Computationalism leads to semantic nativism in both scenarios of symbol processing.

Conclusion: Computationalism cannot adequately explain symbol grounding without implying semantic nativism.

Abstract: The paper presents a paradoxical feature of computational systems that
suggests that computationalism cannot explain symbol grounding. If the mind is
a digital computer, as computationalism claims, then it can be computing either
over meaningful symbols or over meaningless symbols. If it is computing over
meaningful symbols its functioning presupposes the existence of meaningful
symbols in the system, i.e. it implies semantic nativism. If the mind is
computing over meaningless symbols, no intentional cognitive processes are
available prior to symbol grounding. In this case, no symbol grounding could
take place since any grounding presupposes intentional cognitive processes. So,
whether computing in the mind is over meaningless or over meaningful symbols,
computationalism implies semantic nativism.

</details>


### [3] [The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs](https://arxiv.org/abs/2505.00003)
*Zizhou Liu, Ziwei Gong, Lin Ai, Zheng Hui, Run Chen, Colin Wayne Leach, Michelle R. Greene, Julia Hirschberg*

Main category: cs.CL

TL;DR: The paper explores how psychological theories can enhance LLM development, covering data, training, and evaluation, while identifying gaps in current applications.


<details>
  <summary>Details</summary>
Motivation: To bridge psychology and NLP by leveraging psychological insights for more human-like LLM cognition and behavior.

Method: A survey integrating cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics to analyze LLM development stages.

Result: Identifies trends and gaps in applying psychology to NLP, highlighting cross-domain connections and tensions.

Conclusion: Advocates for deeper integration of psychology in NLP to improve LLM development and human-like interaction.

Abstract: Psychological insights have long shaped pivotal NLP breakthroughs, including
the cognitive underpinnings of attention mechanisms, formative reinforcement
learning, and Theory of Mind-inspired social modeling. As Large Language Models
(LLMs) continue to grow in scale and complexity, there is a rising consensus
that psychology is essential for capturing human-like cognition, behavior, and
interaction. This paper reviews how psychological theories can inform and
enhance stages of LLM development, including data, pre-training, post-training,
and evaluation\&application. Our survey integrates insights from cognitive,
developmental, behavioral, social, personality psychology, and
psycholinguistics. Our analysis highlights current trends and gaps in how
psychological theories are applied. By examining both cross-domain connections
and points of tension, we aim to bridge disciplinary divides and promote more
thoughtful integration of psychology into future NLP research.

</details>


### [4] [LangVAE and LangSpace: Building and Probing for Language Model VAEs](https://arxiv.org/abs/2505.00004)
*Danilo S. Carvalho, Yingji Zhang, Harriet Unsworth, André Freitas*

Main category: cs.CL

TL;DR: LangVAE is a framework for building VAEs on pre-trained LLMs, enabling compact, semantically disentangled representations. LangSpace analyzes these representations with probing methods. The system is flexible, scalable, and integrates with HuggingFace models. Experiments show its effectiveness in generalization and disentanglement.


<details>
  <summary>Details</summary>
Motivation: To leverage pre-trained LLMs for creating more efficient and interpretable textual representations through VAEs.

Method: LangVAE constructs VAEs on LLMs, while LangSpace provides probing tools like vector traversal, interpolation, and cluster visualization.

Result: Experiments reveal effective interactions across architectures, improving generalization and disentanglement.

Conclusion: LangVAE offers a systematic way to experiment with and understand textual representations.

Abstract: We present LangVAE, a novel framework for modular construction of variational
autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such
language model VAEs can encode the knowledge of their pre-trained components
into more compact and semantically disentangled representations. The
representations obtained in this way can be analysed with the LangVAE companion
framework: LangSpace, which implements a collection of probing methods, such as
vector traversal and interpolation, disentanglement measures, and cluster
visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable
way of building and analysing textual representations, with simple integration
for models available on the HuggingFace Hub. Additionally, we conducted a set
of experiments with different encoder and decoder combinations, as well as
annotated inputs, revealing a wide range of interactions across architectural
families and sizes w.r.t. generalisation and disentanglement. Our findings
demonstrate a promising framework for systematising the experimentation and
understanding of textual representations.

</details>


### [5] [Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity](https://arxiv.org/abs/2505.00056)
*Tygo Bloem, Filip Ilievski*

Main category: cs.CL

TL;DR: The paper introduces a template-based method for clustering memes using multi-dimensional similarity features, outperforming existing methods and aligning with human intuition.


<details>
  <summary>Details</summary>
Motivation: Meme clustering is crucial for toxicity detection and virality modeling but is understudied due to challenges like multimodality and cultural context. Existing methods lack adaptability and semantic understanding.

Method: A novel template-based matching approach using local and global features across similarity categories (form, visual content, text, identity) without predefined databases.

Result: The method outperforms existing clustering techniques, producing more consistent and coherent clusters.

Conclusion: The approach supports adaptive matching and aligns with human intuition, with code made publicly available for further research.

Abstract: Meme clustering is critical for toxicity detection, virality modeling, and
typing, but it has received little attention in previous research. Clustering
similar Internet memes is challenging due to their multimodality, cultural
context, and adaptability. Existing approaches rely on databases, overlook
semantics, and struggle to handle diverse dimensions of similarity. This paper
introduces a novel method that uses template-based matching with
multi-dimensional similarity features, thus eliminating the need for predefined
databases and supporting adaptive matching. Memes are clustered using local and
global features across similarity categories such as form, visual content,
text, and identity. Our combined approach outperforms existing clustering
methods, producing more consistent and coherent clusters, while
similarity-based feature sets enable adaptability and align with human
intuition. We make all supporting code publicly available to support subsequent
research. Code: https://github.com/tygobl/meme-clustering

</details>


### [6] [Toward a digital twin of U.S. Congress](https://arxiv.org/abs/2505.00006)
*Hayden Helm, Tianyi Chen, Harvey McGuinness, Paige Lee, Brandon Duderstadt, Carey E. Priebe*

Main category: cs.CL

TL;DR: A virtual model of U.S. congresspersons using language models acts as a digital twin, generating indistinguishable Tweets and predicting voting behaviors.


<details>
  <summary>Details</summary>
Motivation: To demonstrate how language models can replicate congresspersons' online behavior and predict legislative actions, aiding resource allocation.

Method: Utilizes a daily-updated dataset of Tweets from U.S. congresspersons, training language models on this data to generate and analyze Tweets.

Result: Generated Tweets are indistinguishable from real ones; models predict roll-call votes and party-line crossing likelihoods.

Conclusion: The approach has potential but faces limitations; extensions could enhance its real-world legislative impact.

Abstract: In this paper we provide evidence that a virtual model of U.S.
congresspersons based on a collection of language models satisfies the
definition of a digital twin. In particular, we introduce and provide
high-level descriptions of a daily-updated dataset that contains every Tweet
from every U.S. congressperson during their respective terms. We demonstrate
that a modern language model equipped with congressperson-specific subsets of
this data are capable of producing Tweets that are largely indistinguishable
from actual Tweets posted by their physical counterparts. We illustrate how
generated Tweets can be used to predict roll-call vote behaviors and to
quantify the likelihood of congresspersons crossing party lines, thereby
assisting stakeholders in allocating resources and potentially impacting
real-world legislative dynamics. We conclude with a discussion of the
limitations and important extensions of our analysis.

</details>


### [7] [A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination](https://arxiv.org/abs/2505.00008)
*Zhaoyi Sun, Wen-Wai Yim, Ozlem Uzuner, Fei Xia, Meliha Yetisgen*

Main category: cs.CL

TL;DR: The review explores NLP's role in detecting and correcting medically inaccurate information, highlighting its potential and challenges in healthcare.


<details>
  <summary>Details</summary>
Motivation: To advance patient safety, improve public health communication, and enhance NLP reliability in healthcare.

Method: A scoping review following PRISMA guidelines, analyzing studies from 2020-2024 across five databases.

Result: NLP shows promise in tasks like error/misinformation/hallucination detection/correction but faces challenges like data privacy and evaluation standards.

Conclusion: While NLP has advanced in addressing medical inaccuracies, future work should focus on real-world datasets, contextual methods, and hallucination management.

Abstract: Objective: This review aims to explore the potential and challenges of using
Natural Language Processing (NLP) to detect, correct, and mitigate medically
inaccurate information, including errors, misinformation, and hallucination. By
unifying these concepts, the review emphasizes their shared methodological
foundations and their distinct implications for healthcare. Our goal is to
advance patient safety, improve public health communication, and support the
development of more reliable and transparent NLP applications in healthcare.
  Methods: A scoping review was conducted following PRISMA guidelines,
analyzing studies from 2020 to 2024 across five databases. Studies were
selected based on their use of NLP to address medically inaccurate information
and were categorized by topic, tasks, document types, datasets, models, and
evaluation metrics.
  Results: NLP has shown potential in addressing medically inaccurate
information on the following tasks: (1) error detection (2) error correction
(3) misinformation detection (4) misinformation correction (5) hallucination
detection (6) hallucination mitigation. However, challenges remain with data
privacy, context dependency, and evaluation standards.
  Conclusion: This review highlights the advancements in applying NLP to tackle
medically inaccurate information while underscoring the need to address
persistent challenges. Future efforts should focus on developing real-world
datasets, refining contextual methods, and improving hallucination management
to ensure reliable and transparent healthcare applications.

</details>


### [8] [BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition](https://arxiv.org/abs/2505.00059)
*Paige Tuttösí, Mantaj Dhillon, Luna Sang, Shane Eastwood, Poorvi Bhatia, Quang Minh Dinh, Avni Kapoor, Yewon Jin, Angelica Lim*

Main category: cs.CL

TL;DR: The BERSt dataset addresses real-world challenges in speech recognition by providing diverse, distanced, and emotional speech data collected in varied acoustic environments.


<details>
  <summary>Details</summary>
Motivation: Current ASR systems struggle with complex real-world scenarios like distanced speech and emotional variations, despite high performance in controlled settings.

Method: The BERSt dataset includes 4 hours of English speech from 98 actors with diverse accents, collected via smartphones in various home environments, with 7 emotion prompts and shouted/spoken utterances.

Result: ASR performance degrades with increased distance and shout level, and varies by emotion, highlighting the dataset's challenge for ASR and SER tasks.

Conclusion: The BERSt dataset is a valuable resource for improving ASR and SER robustness, but further work is needed for real-world accuracy.

Abstract: Some speech recognition tasks, such as automatic speech recognition (ASR),
are approaching or have reached human performance in many reported metrics.
Yet, they continue to struggle in complex, real-world, situations, such as with
distanced speech. Previous challenges have released datasets to address the
issue of distanced ASR, however, the focus remains primarily on distance,
specifically relying on multi-microphone array systems. Here we present the
B(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset
contains almost 4 hours of English speech from 98 actors with varying regional
and non-native accents. The data was collected on smartphones in the actors
homes and therefore includes at least 98 different acoustic environments. The
data also includes 7 different emotion prompts and both shouted and spoken
utterances. The smartphones were places in 19 different positions, including
obstructions and being in a different room than the actor. This data is
publicly available for use and can be used to evaluate a variety of speech
recognition tasks, including: ASR, shout detection, and speech emotion
recognition (SER). We provide initial benchmarks for ASR and SER tasks, and
find that ASR degrades both with an increase in distance and shout level and
shows varied performance depending on the intended emotion. Our results show
that the BERSt dataset is challenging for both ASR and SER tasks and continued
work is needed to improve the robustness of such systems for more accurate
real-world use.

</details>


### [9] [Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation](https://arxiv.org/abs/2505.00009)
*Xiao Zhang, Kangsheng Wang, Tianyu Hu, Huimin Ma*

Main category: cs.CL

TL;DR: TA-LoRA improves multi-task learning by using low-rank representations and a fast-slow weights mechanism to better capture task-specific knowledge without disrupting shared knowledge.


<details>
  <summary>Details</summary>
Motivation: Pre-trained language models struggle with unseen tasks, and existing methods like prompt tuning fail to adequately handle task heterogeneity.

Method: TA-LoRA combines low-rank representation for task heterogeneity, fast-slow weights to separate shared and task-specific knowledge, and zero-initialized attention to minimize disruption during warm-up.

Result: TA-LoRA achieves state-of-the-art performance on 16 tasks in full-data and few-shot settings while maintaining parameter efficiency.

Conclusion: TA-LoRA effectively addresses the limitations of prompt tuning, offering a robust solution for multi-task learning with pre-trained models.

Abstract: Pre-trained language models (PLMs) demonstrate remarkable intelligence but
struggle with emerging tasks unseen during training in real-world applications.
Training separate models for each new task is usually impractical. Multi-task
learning (MTL) addresses this challenge by transferring shared knowledge from
source tasks to target tasks. As an dominant parameter-efficient fine-tuning
method, prompt tuning (PT) enhances MTL by introducing an adaptable vector that
captures task-specific knowledge, which acts as a prefix to the original prompt
that preserves shared knowledge, while keeping PLM parameters frozen. However,
PT struggles to effectively capture the heterogeneity of task-specific
knowledge due to its limited representational capacity. To address this
challenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL
method built on PT, employing the low-rank representation to model task
heterogeneity and a fast-slow weights mechanism where the slow weight encodes
shared knowledge, while the fast weight captures task-specific nuances,
avoiding the mixing of shared and task-specific knowledge, caused by training
low-rank representations from scratch. Moreover, a zero-initialized attention
mechanism is introduced to minimize the disruption of immature low-rank
components on original prompts during warm-up epochs. Experiments on 16 tasks
demonstrate that TA-LoRA achieves state-of-the-art performance in full-data and
few-shot settings while maintaining superior parameter efficiency.

</details>


### [10] [Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models](https://arxiv.org/abs/2505.00010)
*Tri Nguyen, Lohith Srikanth Pentapalli, Magnus Sieverding, Laurah Turner, Seth Overla, Weibing Zheng, Chris Zhou, David Furniss, Danielle Weber, Michael Gharib, Matt Kelleher, Michael Shukis, Cameron Pawlik, Kelly Cohen*

Main category: cs.CL

TL;DR: The study detects jailbreaks in LLMs used in clinical education by analyzing linguistic features, showing that feature-based models outperform prompt engineering, with Fuzzy Decision Trees performing best.


<details>
  <summary>Details</summary>
Motivation: Jailbreaking in LLMs threatens ethical safeguards, especially in sensitive domains like education, necessitating effective detection methods.

Method: Annotated 2,300 prompts across 158 conversations using four linguistic variables, trained predictive models (Decision Trees, Fuzzy Logic, Boosting, Logistic Regression).

Result: Feature-based models outperformed Prompt Engineering; Fuzzy Decision Tree achieved the best performance.

Conclusion: Linguistic-feature-based models are effective and explainable for jailbreak detection; hybrid frameworks are suggested for future work.

Abstract: Jailbreaking in Large Language Models (LLMs) threatens their safe use in
sensitive domains like education by allowing users to bypass ethical
safeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical
education platform that simulates patient interactions using LLMs. We annotated
over 2,300 prompts across 158 conversations using four linguistic variables
shown to correlate strongly with jailbreak behavior. The extracted features
were used to train several predictive models, including Decision Trees, Fuzzy
Logic-based classifiers, Boosting methods, and Logistic Regression. Results
show that feature-based predictive models consistently outperformed Prompt
Engineering, with the Fuzzy Decision Tree achieving the best overall
performance. Our findings demonstrate that linguistic-feature-based models are
effective and explainable alternatives for jailbreak detection. We suggest
future work explore hybrid frameworks that integrate prompt-based flexibility
with rule-based robustness for real-time, spectrum-based jailbreak monitoring
in educational LLMs.

</details>


### [11] [The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?](https://arxiv.org/abs/2505.00012)
*Fabian Retkowski, Andreas Sudmann, Alexander Waibel*

Main category: cs.CL

TL;DR: The paper introduces AICoE, an AI-driven pipeline for qualitative research, enhancing scalability and depth by automating coding, consolidation, and pattern discovery.


<details>
  <summary>Details</summary>
Motivation: Addressing the labor-intensive and unscalable nature of qualitative research while preserving analytical depth.

Method: Developed AICoE, an end-to-end pipeline for qualitative research, automating open coding, code consolidation, application, and pattern discovery.

Result: AICoE provides a comprehensive and scalable solution for qualitative data analysis.

Conclusion: AICoE advances qualitative research by integrating AI to overcome traditional limitations.

Abstract: Qualitative research often involves labor-intensive processes that are
difficult to scale while preserving analytical depth. This paper introduces The
AI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for
qualitative research and designed to move beyond the limitations of simply
automating code assignments, offering a more integrated approach. AICoE
organizes the entire process, encompassing open coding, code consolidation,
code application, and even pattern discovery, leading to a comprehensive
analysis of qualitative data.

</details>


### [12] [Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa](https://arxiv.org/abs/2505.00013)
*Yoichi Takenaka*

Main category: cs.CL

TL;DR: The study fine-tunes pre-trained models for binary emotion classification in Japanese, with DeBERTa-v3-large achieving the best performance, while LLMs lag behind.


<details>
  <summary>Details</summary>
Motivation: Accurate emotion detection in Japanese text is needed for applications like social media monitoring, but resource scarcity and class imbalance pose challenges.

Method: The WRIME corpus is used to fine-tune four pre-trained models (BERT, RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large) and evaluate two LLMs (TinySwallow-1.5B-Instruct, ChatGPT-4o) using accuracy and F1-score.

Result: DeBERTa-v3-large outperforms others with mean accuracy (0.860) and F1-score (0.662), while LLMs perform poorly (ChatGPT-4o: 0.527, TinySwallow: 0.292).

Conclusion: DeBERTa-v3-large is the most reliable for binary emotion classification in Japanese. Future work includes data augmentation, model size reduction, and improving LLM performance.

Abstract: Background Practical applications such as social media monitoring and
customer-feedback analysis require accurate emotion detection for Japanese
text, yet resource scarcity and class imbalance hinder model performance.
  Objective This study aims to build a high-accuracy model for predicting the
presence or absence of eight Plutchik emotions in Japanese sentences.
  Methods Using the WRIME corpus, we transform reader-averaged intensity scores
into binary labels and fine-tune four pre-trained language models (BERT,
RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two
large language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and
F1-score serve as evaluation metrics.
  Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score
(0.662), outperforming all other models. It maintains robust F1 across both
high-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions
(e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and
TinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.
  Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most
reliable solution for binary emotion classification in Japanese. We release
this model as a pip-installable package (pip install
deberta-emotion-predictor). Future work should augment data for rare emotions,
reduce model size, and explore prompt engineering to improve LLM performance.
  This manuscript is under review for possible publication in New Generation
Computing.

</details>


### [13] [Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and Möbius Strips](https://arxiv.org/abs/2505.00014)
*Vinit K. Chavan*

Main category: cs.CL

TL;DR: The paper introduces a framework for constraining sentence embeddings to continuous manifolds (sphere, torus, Möbius strip) using triplet loss, outperforming traditional Euclidean embeddings in clustering and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional Euclidean sentence embeddings may not capture complex linguistic relationships well, prompting exploration of manifold-constrained embeddings for better semantic structure.

Method: A novel framework enforces differential geometric constraints on embeddings (sphere, torus, Möbius strip) using triplet loss, evaluated on AG News and MBTI datasets against baselines like TF-IDF and Word2Vec.

Result: Manifold-constrained embeddings, especially on spheres and Möbius strips, outperform traditional methods in clustering (Silhouette Score) and classification (Accuracy).

Conclusion: Embedding in manifold spaces, where topological structure aids semantic separation, offers a promising direction for geometric representation learning in NLP.

Abstract: Recent advances in representation learning have emphasized the role of
embedding geometry in capturing semantic structure. Traditional sentence
embeddings typically reside in unconstrained Euclidean spaces, which may limit
their ability to reflect complex relationships in language. In this work, we
introduce a novel framework that constrains sentence embeddings to lie on
continuous manifolds -- specifically the unit sphere, torus, and M\"obius strip
-- using triplet loss as the core training objective. By enforcing differential
geometric constraints on the output space, our approach encourages the learning
of embeddings that are both discriminative and topologically structured.
  We evaluate our method on benchmark datasets (AG News and MBTI) and compare
it to classical baselines including TF-IDF, Word2Vec, and unconstrained
Keras-derived embeddings. Our results demonstrate that manifold-constrained
embeddings, particularly those projected onto spheres and M\"obius strips,
significantly outperform traditional approaches in both clustering quality
(Silhouette Score) and classification performance (Accuracy). These findings
highlight the value of embedding in manifold space -- where topological
structure complements semantic separation -- offering a new and mathematically
grounded direction for geometric representation learning in NLP.

</details>


### [14] [Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation](https://arxiv.org/abs/2505.00015)
*MD Thamed Bin Zaman Chowdhury, Moazzem Hossain*

Main category: cs.CL

TL;DR: The paper proposes an automated system using LLMs and web scraping to collect and analyze road accident data in Bangladesh, achieving high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the unreliable and fragmented manual accident data collection in Bangladesh, ensuring accurate and consistent records for better road safety policymaking.

Method: A four-component pipeline: automated web scraping code generation, news collection, accident classification with structured data extraction, and duplicate removal, powered by the LLM Gemini-2.0-Flash.

Result: The system processed 15,000+ news articles, identified 705 unique accidents, and achieved 91.3% calibration and 80% validation accuracy. Chittagong had the highest accident rates.

Conclusion: The study proves the viability of an LLM-powered, scalable system for accurate accident data collection, supporting data-driven road safety policies in Bangladesh.

Abstract: Road traffic accidents remain a major public safety and socio-economic issue
in developing countries like Bangladesh. Existing accident data collection is
largely manual, fragmented, and unreliable, resulting in underreporting and
inconsistent records. This research proposes a fully automated system using
Large Language Models (LLMs) and web scraping techniques to address these
challenges. The pipeline consists of four components: automated web scraping
code generation, news collection from online sources, accident news
classification with structured data extraction, and duplicate removal. The
system uses the multimodal generative LLM Gemini-2.0-Flash for seamless
automation. The code generation module classifies webpages into pagination,
dynamic, or infinite scrolling categories and generates suitable Python scripts
for scraping. LLMs also classify and extract key accident information such as
date, time, location, fatalities, injuries, road type, vehicle types, and
pedestrian involvement. A deduplication algorithm ensures data integrity by
removing duplicate reports. The system scraped 14 major Bangladeshi news sites
over 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news
articles and identifying 705 unique accidents. The code generation module
achieved 91.3% calibration and 80% validation accuracy. Chittagong reported the
highest number of accidents (80), fatalities (70), and injuries (115), followed
by Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning
(8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also
developed with usage instructions. This study demonstrates the viability of an
LLM-powered, scalable system for accurate, low-effort accident data collection,
providing a foundation for data-driven road safety policymaking in Bangladesh.

</details>


### [15] [Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning](https://arxiv.org/abs/2505.00016)
*Josefa Lia Stoisser, Marc Boubnovski Martell, Julien Fauqueur*

Main category: cs.CL

TL;DR: The paper reframes Text-to-SQL as a method to teach LLMs table reasoning, proposing a two-stage framework with SQL supervision and reinforcement learning, showing improved performance and generalization.


<details>
  <summary>Details</summary>
Motivation: To move beyond query generation in Text-to-SQL by teaching LLMs to reason over tabular data, enhancing generalization and interpretability.

Method: A two-stage framework: (1) synthesizing detailed CoT traces from SQL queries for clause-level supervision, and (2) using GRPO reinforcement learning to link SQL execution accuracy to generalizable reasoning.

Result: Improved performance on Text-to-SQL benchmarks (e.g., BIRD, CRT-QA), with LLaMA achieving a 20% accuracy increase and Qwen a 5% increase.

Conclusion: SQL serves as both a target formalism and a scaffold for learning robust, transferable reasoning over structured data.

Abstract: This work reframes the Text-to-SQL task as a pathway for teaching large
language models (LLMs) to reason over and manipulate tabular data--moving
beyond the traditional focus on query generation. We propose a two-stage
framework that leverages SQL supervision to develop transferable table
reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)
traces from real-world SQL queries, providing step-by-step, clause-level
supervision that teaches the model how to traverse, filter, and aggregate table
fields. Second, we introduce a Group Relative Policy Optimization (GRPO)
reinforcement learning objective that connects SQL execution accuracy to
generalizable reasoning by encouraging steps that extend beyond task-specific
syntax and transfer across datasets. Empirically, our approach improves
performance on standard Text-to-SQL benchmarks and achieves substantial gains
on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced
generalization and interpretability. Specifically, the distilled-quantized
LLaMA model achieved a 20\% increase in accuracy when trained on Text-to-SQL
tasks, while Qwen achieved a 5\% increase. These results suggest that SQL can
serve not only as a target formalism but also as an effective scaffold for
learning robust, transferable reasoning over structured data.

</details>


### [16] [ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation](https://arxiv.org/abs/2505.00017)
*Dezheng Han, Yibin Jia, Ruxiao Chen, Wenjie Han, Shuaishuai Guo, Jianbo Wang*

Main category: cs.CL

TL;DR: A method using graph-structured feature marker databases and multi-task workflows improves automated cell type annotation with LLMs, outperforming general-purpose LLMs in accuracy and alignment with manual logic.


<details>
  <summary>Details</summary>
Motivation: To achieve precise and fully automated cell type annotation using large language models (LLMs) by addressing limitations of general-purpose LLMs.

Method: Developed a graph-structured feature marker database for retrieving entities linked to differential genes and designed a multi-task workflow to optimize annotation.

Result: Improved human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, better aligning with manual annotation logic.

Conclusion: The proposed method enhances automated cell type annotation, outperforming general-purpose LLMs in accuracy and cognitive alignment.

Abstract: To enable precise and fully automated cell type annotation with large
language models (LLMs), we developed a graph structured feature marker database
to retrieve entities linked to differential genes for cell reconstruction. We
further designed a multi task workflow to optimize the annotation process.
Compared to general purpose LLMs, our method improves human evaluation scores
by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while
more closely aligning with the cognitive logic of manual annotation.

</details>


### [17] [An Empirical Study on Prompt Compression for Large Language Models](https://arxiv.org/abs/2505.00019)
*Zheng Zhang, Jinyi Li, Yihuai Lan, Xiang Wang, Hao Wang*

Main category: cs.CL

TL;DR: The paper explores six prompt compression methods for LLMs to reduce computational costs while maintaining response quality, evaluating them across 13 datasets.


<details>
  <summary>Details</summary>
Motivation: Lengthy prompts increase computational complexity and costs, prompting the need for efficient compression methods.

Method: Six prompt compression methods are studied and evaluated on 13 diverse datasets.

Result: Prompt compression impacts LLM performance more in long contexts; moderate compression can enhance performance.

Conclusion: Efficient prompt compression is feasible and beneficial, especially for long-context tasks.

Abstract: Prompt engineering enables Large Language Models (LLMs) to perform a variety
of tasks. However, lengthy prompts significantly increase computational
complexity and economic costs. To address this issue, we study six prompt
compression methods for LLMs, aiming to reduce prompt length while maintaining
LLM response quality. In this paper, we present a comprehensive analysis
covering aspects such as generation performance, model hallucinations, efficacy
in multimodal tasks, word omission analysis, and more. We evaluate these
methods across 13 datasets, including news, scientific articles, commonsense
QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that
prompt compression has a greater impact on LLM performance in long contexts
compared to short ones. In the Longbench evaluation, moderate compression even
enhances LLM performance. Our code and data is available at
https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.

</details>


### [18] [Beyond Public Access in LLM Pre-Training Data](https://arxiv.org/abs/2505.00020)
*Sruly Rosenblat, Tim O'Reilly, Ilan Strauss*

Main category: cs.CL

TL;DR: The paper investigates if OpenAI's models (GPT-4o, GPT-3.5 Turbo, GPT-4o Mini) were trained on copyrighted O'Reilly books without consent. Results show GPT-4o recognizes paywalled content strongly, while GPT-3.5 Turbo favors public content. GPT-4o Mini shows no recognition. The study calls for transparency in AI training data.


<details>
  <summary>Details</summary>
Motivation: To determine if OpenAI's models were trained on copyrighted O'Reilly Media books without permission, raising concerns about data sourcing ethics.

Method: Applied the DE-COP membership inference attack on 34 O'Reilly books, testing models (GPT-4o, GPT-3.5 Turbo, GPT-4o Mini) with AUROC scores.

Result: GPT-4o recognized paywalled content (AUROC=82%), GPT-3.5 Turbo recognized public content better, and GPT-4o Mini showed no recognition (AUROC≈50%).

Conclusion: The findings emphasize the need for corporate transparency and formal licensing frameworks for AI training data.

Abstract: Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we
apply the DE-COP membership inference attack method to investigate whether
OpenAI's large language models were trained on copyrighted content without
consent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable
model, demonstrates strong recognition of paywalled O'Reilly book content
(AUROC = 82\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast,
GPT-3.5 Turbo shows greater relative recognition of publicly accessible
O'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge
of public or non-public O'Reilly Media content when tested (AUROC $\approx$
50\%). Testing multiple models, with the same cutoff date, helps us account for
potential language shifts over time that might bias our findings. These results
highlight the urgent need for increased corporate transparency regarding
pre-training data sources as a means to develop formal licensing frameworks for
AI content training

</details>


### [19] [Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss](https://arxiv.org/abs/2505.00021)
*Zhuoang Cai, Zhenghao Li, Yang Liu, Liyuan Guo, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper addresses class imbalance in food hazard detection using data augmentation and transformer models, showing improved performance with EDA and focal loss.


<details>
  <summary>Details</summary>
Motivation: Challenges like severe class imbalance, unstructured text, and overlapping categories in food hazard detection motivated the study.

Method: Used BERT and RoBERTa with data balancing strategies (random oversampling, EDA, focal loss).

Result: EDA improved accuracy and F1 scores; combining focal loss with oversampling and EDA enhanced robustness.

Conclusion: The approach advances NLP-based models for food hazard detection by effectively handling class imbalance.

Abstract: Classification tasks often suffer from imbal- anced data distribution, which
presents chal- lenges in food hazard detection due to severe class imbalances,
short and unstructured text, and overlapping semantic categories. In this
paper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection,
which ad- dresses these issues by applying data augmenta- tion techniques to
improve classification perfor- mance. We utilize transformer-based models, BERT
and RoBERTa, as backbone classifiers and explore various data balancing
strategies, including random oversampling, Easy Data Augmentation (EDA), and
focal loss. Our ex- periments show that EDA effectively mitigates class
imbalance, leading to significant improve- ments in accuracy and F1 scores.
Furthermore, combining focal loss with oversampling and EDA further enhances
model robustness, par- ticularly for hard-to-classify examples. These findings
contribute to the development of more effective NLP-based classification models
for food hazard detection.

</details>


### [20] [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/abs/2505.00022)
*Thomas F Burns, Letitia Parcalabescu, Stephan Wäldchen, Michael Barlow, Gregor Ziegltrum, Volker Stampa, Bastian Harren, Björn Deiseroth*

Main category: cs.CL

TL;DR: A German dataset curation pipeline combining heuristic and model-based filtering with synthetic data generation improves LLM performance over existing datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM performance and training efficiency by focusing on data quality, particularly for German-language models.

Method: Combines heuristic and model-based filtering with synthetic data generation to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset.

Result: Aleph-Alpha-GermanWeb outperforms FineWeb2 and enriched datasets on German benchmarks, even at larger scales.

Conclusion: Model-based data curation and synthetic data generation significantly enhance LLM pre-training datasets.

Abstract: Scaling data quantity is essential for large language models (LLMs), yet
recent findings show that data quality can significantly boost performance and
training efficiency. We introduce a German-language dataset curation pipeline
that combines heuristic and model-based filtering techniques with synthetic
data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a
large-scale German pre-training dataset which draws from: (1) Common Crawl web
data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual,
organic web data. We evaluate our dataset by pre-training both a 1B Llama-style
model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A
comparison on German-language benchmarks, including MMMLU, shows significant
performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage
holds at the 8B scale even when FineWeb2 is enriched by human-curated
high-quality data sources such as Wikipedia. Our findings support the growing
body of evidence that model-based data curation and synthetic data generation
can significantly enhance LLM pre-training datasets.

</details>


### [21] [CORG: Generating Answers from Complex, Interrelated Contexts](https://arxiv.org/abs/2505.00023)
*Hyunji Lee, Franck Dernoncourt, Trung Bui, Seunghyun Yoon*

Main category: cs.CL

TL;DR: The paper introduces CORG, a framework to handle recurring but inconsistent knowledge in documents by organizing contexts into groups, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address inconsistencies in recurring knowledge across documents due to ambiguous naming, outdated info, or errors, which existing models struggle with.

Method: Classifies interrelationships into four types, then introduces CORG with a graph constructor, reranker, and aggregator to organize contexts.

Result: CORG balances performance and efficiency, outperforming grouping methods and matching single-context approaches.

Conclusion: CORG effectively organizes and disambiguates multiple contexts, offering a scalable solution for complex knowledge relationships.

Abstract: In a real-world corpus, knowledge frequently recurs across documents but
often contains inconsistencies due to ambiguous naming, outdated information,
or errors, leading to complex interrelationships between contexts. Previous
research has shown that language models struggle with these complexities,
typically focusing on single factors in isolation. We classify these
relationships into four types: distracting, ambiguous, counterfactual, and
duplicated. Our analysis reveals that no single approach effectively addresses
all these interrelationships simultaneously. Therefore, we introduce Context
Organizer (CORG), a framework that organizes multiple contexts into
independently processed groups. This design allows the model to efficiently
find all relevant answers while ensuring disambiguation. CORG consists of three
key components: a graph constructor, a reranker, and an aggregator. Our results
demonstrate that CORG balances performance and efficiency effectively,
outperforming existing grouping methods and achieving comparable results to
more computationally intensive, single-context approaches.

</details>


### [22] [Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning](https://arxiv.org/abs/2505.00024)
*Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, Guilin Liu*

Main category: cs.CL

TL;DR: Nemotron-Research-Tool-N1 series enhances tool-use in LLMs via binary reward-based optimization, outperforming GPT-4o on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Prior methods for tool-use in LLMs either lack reasoning or imitate it poorly, limiting generalization.

Method: Uses rule-based reinforcement learning with binary rewards for structural and functional correctness, avoiding annotated reasoning traces.

Result: Nemotron-Research-Tool-N1-7B/14B achieves state-of-the-art results on BFCL and API-Bank benchmarks.

Conclusion: Lightweight supervision enables autonomous reasoning, outperforming existing approaches.

Abstract: Enabling large language models with external tools has become a pivotal
strategy for extending their functionality beyond text generation tasks. Prior
work typically enhances tool-use abilities by either applying supervised
fine-tuning (SFT) to enforce tool-call correctness or distilling reasoning
traces from stronger models for SFT. However, both approaches fall short,
either omitting reasoning entirely or producing imitative reasoning that limits
generalization. Inspired by the success of DeepSeek-R1 in eliciting reasoning
through rule-based reinforcement learning, we develop the
Nemotron-Research-Tool-N1 series of tool-using language models using a similar
training paradigm. Instead of restrictively supervising intermediate reasoning
traces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized
with a binary reward that evaluates only the structural validity and functional
correctness of tool invocations. This lightweight supervision allows the model
to autonomously internalize reasoning strategies, without the need for
annotated reasoning trajectories. Experiments on the BFCL and API-Bank
benchmarks show that Nemotron-Research-Tool-N1-7B and
Nemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve
state-of-the-art results, outperforming GPT-4o on both evaluations.

</details>


### [23] [A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1](https://arxiv.org/abs/2505.00025)
*Mingda Zhang, Jianglong Qin*

Main category: cs.CL

TL;DR: Proposes a lightweight medical LLM architecture addressing knowledge barriers, model compression, and computational optimization, reducing memory and latency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of foundation models in medical scenarios due to knowledge barriers, resource demands, and deployment constraints.

Method: Knowledge transfer via LoRA, 4-bit quantization, and computational optimizations like Flash Attention and continuous batching.

Result: Reduces memory by 64.7% and latency by 12.4% while preserving medical accuracy.

Conclusion: Provides an efficient solution for deploying medical LLMs in resource-constrained environments.

Abstract: In recent years, despite foundation models like DeepSeek-R1 and ChatGPT
demonstrating significant capabilities in general tasks, professional knowledge
barriers, computational resource requirements, and deployment environment
limitations have severely hindered their application in actual medical
scenarios. Addressing these challenges, this paper proposes an efficient
lightweight medical vertical large language model architecture method,
systematically solving the lightweight problem of medical large models from
three dimensions: knowledge acquisition, model compression, and computational
optimization. At the knowledge acquisition level, a knowledge transfer pipeline
is designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the
DeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology
is adopted to precisely adjust key attention layers. At the model compression
level, compression techniques including 4-bit weight quantization are
implemented while preserving the core representation ability for medical
reasoning. At the computational optimization level, inference optimization
techniques such as Flash Attention acceleration and continuous batching are
integrated, and a professional prompt template system is constructed to adapt
to different types of medical problems. Experimental results on medical
question-answering datasets show that the method proposed in this paper
maintains professional accuracy while reducing memory consumption by 64.7\% and
inference latency by 12.4\%, providing an effective solution for the
application of medical large models in resource-constrained environments such
as edge computing devices.

</details>


### [24] [Theory of Mind in Large Language Models: Assessment and Enhancement](https://arxiv.org/abs/2505.00026)
*Ruirui Chen, Weifeng Jiang, Chengwei Qin, Cheston Tan*

Main category: cs.CL

TL;DR: A review of Large Language Models' (LLMs) Theory of Mind (ToM) capabilities, focusing on evaluation benchmarks, improvement strategies, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Assessing and enhancing LLMs' ability to interpret human mental states is crucial as they become more integrated into daily life.

Method: Examines story-based benchmarks and analyzes methods to improve ToM in LLMs.

Result: Provides insights into current benchmarks and state-of-the-art approaches for enhancing ToM.

Conclusion: The survey is a resource for advancing LLMs' ToM capabilities, highlighting promising future research directions.

Abstract: Theory of Mind (ToM)-the ability to infer and reason about others' mental
states-is fundamental to human social intelligence. As Large Language Models
(LLMs) become increasingly integrated into daily life, it is crucial to assess
and enhance their capacity to interpret and respond to human mental states. In
this paper, we review LLMs' ToM capabilities by examining both evaluation
benchmarks and the strategies designed to improve them. We focus on widely
adopted story-based benchmarks and provide an in-depth analysis of methods
aimed at enhancing ToM in LLMs. Furthermore, we outline promising future
research directions informed by recent benchmarks and state-of-the-art
approaches. Our survey serves as a valuable resource for researchers interested
in advancing LLMs' ToM capabilities.

</details>


### [25] [Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts](https://arxiv.org/abs/2505.00027)
*Jian Zhou, Jiazheng Li, Sirui Zhuge, Hai Zhuge*

Main category: cs.CL

TL;DR: The paper proposes a method to automatically extract subject, action, object, and adverbial dimensions from texts for efficient querying in natural language, achieving high precision and recall.


<details>
  <summary>Details</summary>
Motivation: To enable efficient text operations and natural language queries by representing textual elements and their relations in structured trees.

Method: Constructs abstraction trees for subject, action, object, and adverbial dimensions, ensuring independence and expressiveness for comprehensive text coverage.

Result: Experiments show precision, recall, and F1-scores above 80%. The approach effectively supports natural language queries by reducing search space.

Conclusion: The method successfully enables precise text operations and natural language querying through structured tree representations.

Abstract: This paper proposed an approach to automatically discovering subject
dimension, action dimension, object dimension and adverbial dimension from
texts to efficiently operate texts and support query in natural language. The
high quality of trees guarantees that all subjects, actions, objects and
adverbials and their subclass relations within texts can be represented. The
independency of trees ensures that there is no redundant representation between
trees. The expressiveness of trees ensures that the majority of sentences can
be accessed from each tree and the rest of sentences can be accessed from at
least one tree so that the tree-based search mechanism can support querying in
natural language. Experiments show that the average precision, recall and
F1-score of the abstraction trees constructed by the subclass relations of
subject, action, object and adverbial are all greater than 80%. The application
of the proposed approach to supporting query in natural language demonstrates
that different types of question patterns for querying subject or object have
high coverage of texts, and searching multiple trees on subject, action, object
and adverbial according to the question pattern can quickly reduce search space
to locate target sentences, which can support precise operation on texts.

</details>


### [26] [Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation](https://arxiv.org/abs/2505.00028)
*Pengchao Feng, Ziyang Ma, Wenxi Chen, Yao Li, Sheng Wang, Kai Yu, Xie Chen*

Main category: cs.CL

TL;DR: A novel end-to-end RAG framework for speech-to-speech dialogue systems improves knowledge integration by directly retrieving textual knowledge from speech queries, bypassing ASR.


<details>
  <summary>Details</summary>
Motivation: Address the modality gap between speech input and textual knowledge in end-to-end S2S systems, which limits effective knowledge integration.

Method: Propose an end-to-end RAG framework that retrieves textual knowledge directly from speech queries, avoiding intermediate ASR conversion.

Result: Significantly improves S2S system performance and retrieval efficiency, though still behind cascaded models.

Conclusion: The framework shows promise for enhancing knowledge integration in end-to-end S2S systems; code and dataset will be released for further research.

Abstract: In recent years, end-to-end speech-to-speech (S2S) dialogue systems have
garnered increasing research attention due to their advantages over traditional
cascaded systems, including achieving lower latency and more natural
integration of nonverbal cues such as emotion and speaker identity. However,
these end-to-end systems face key challenges, particularly in incorporating
external knowledge, a capability commonly addressed by Retrieval-Augmented
Generation (RAG) in text-based large language models (LLMs). The core
difficulty lies in the modality gap between input speech and retrieved textual
knowledge, which hinders effective integration. To address this issue, we
propose a novel end-to-end RAG framework that directly retrieves relevant
textual knowledge from speech queries, eliminating the need for intermediate
speech-to-text conversion via techniques like ASR. Experimental results
demonstrate that our method significantly improves the performance of
end-to-end S2S dialogue systems while achieving higher retrieval efficiency.
Although the overall performance still lags behind cascaded models, our
framework offers a promising direction for enhancing knowledge integration in
end-to-end S2S systems. We will release the code and dataset to support
reproducibility and promote further research in this area.

</details>


### [27] [Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting](https://arxiv.org/abs/2505.00029)
*Yijie Hong, Xiaofei Yin, Xinzhong Wang, Yi Tu, Ya Guo, Sufeng Duan, Weiqiang Wang, Lingyong Fang, Depeng Wang, Huijia Zhu*

Main category: cs.CL

TL;DR: SDFT is a method to inject domain-specific knowledge into large vision-language models without catastrophic forgetting, using a three-phase dialogue structure.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models struggle with incorporating specialized knowledge without losing foundational abilities.

Method: Structured Dialogue Fine-Tuning (SDFT) with three phases: Foundation Preservation, Contrastive Disambiguation, and Knowledge Specialization.

Result: SDFT effectively balances specialized knowledge acquisition and general capability retention across domains.

Conclusion: SDFT provides a robust framework for integrating domain-specific knowledge while preserving foundational skills.

Abstract: Large Vision Language Models have demonstrated impressive versatile
capabilities through extensive multimodal pre-training, but face significant
limitations when incorporating specialized knowledge domains beyond their
training distribution. These models struggle with a fundamental dilemma: direct
adaptation approaches that inject domain-specific knowledge often trigger
catastrophic forgetting of foundational visual-linguistic abilities. We
introduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that
effectively injects domain-specific knowledge while minimizing catastrophic
forgetting. Drawing inspiration from supervised fine-tuning in LLMs and
subject-driven personalization in text-to-image diffusion models, our method
employs a three-phase dialogue structure: Foundation Preservation reinforces
pre-trained visual-linguistic alignment through caption tasks; Contrastive
Disambiguation introduces carefully designed counterfactual examples to
maintain semantic boundaries; and Knowledge Specialization embeds specialized
information through chain-of-thought reasoning. Experimental results across
multiple domains confirm SDFT's effectiveness in balancing specialized
knowledge acquisition with general capability retention. Our key contributions
include a data-centric dialogue template that balances foundational alignment
with targeted knowledge integration, a weighted multi-turn supervision
framework, and comprehensive evaluation across diverse knowledge types.

</details>


### [28] [Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving](https://arxiv.org/abs/2505.00031)
*Jin Zhang, Flood Sung, Zhilin Yang, Yang Gao, Chongjie Zhang*

Main category: cs.CL

TL;DR: LEPA introduces a self-training algorithm for LLMs to generate anticipatory plans before solving problems, improving generalization and performance on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LLM post-training focus on step-by-step solutions but lack abstract meta-knowledge for generalization, inspired by human cognitive processes.

Method: LEPA trains LLMs to create anticipatory plans as meta-knowledge, refine them via self-reflection, and predict both plans and solutions during optimization.

Result: LEPA outperforms conventional algorithms on natural language reasoning benchmarks by leveraging anticipatory plans.

Conclusion: LEPA's approach of planning before answering enhances LLM performance and generalization, addressing a gap in current post-training methods.

Abstract: In the field of large language model (LLM) post-training, the effectiveness
of utilizing synthetic data generated by the LLM itself has been
well-presented. However, a key question remains unaddressed: what essential
information should such self-generated data encapsulate? Existing approaches
only produce step-by-step problem solutions, and fail to capture the abstract
meta-knowledge necessary for generalization across similar problems. Drawing
insights from cognitive science, where humans employ high-level abstraction to
simplify complex problems before delving into specifics, we introduce a novel
self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains
the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge
for problem-solving, before engaging with the intricacies of problems. This
approach not only outlines the solution generation path but also shields the
LLM from the distraction of irrelevant details. During data generation, LEPA
first crafts an anticipatory plan based on the problem, and then generates a
solution that aligns with both the plan and the problem. LEPA refines the plan
through self-reflection, aiming to acquire plans that are instrumental in
yielding correct solutions. During model optimization, the LLM is trained to
predict both the refined plans and the corresponding solutions. By efficiently
extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable
superiority over conventional algorithms on various challenging natural
language reasoning benchmarks.

</details>


### [29] [Can Language Models Represent the Past without Anachronism?](https://arxiv.org/abs/2505.00030)
*Ted Underwood, Laura K. Nelson, Matthew Wilkens*

Main category: cs.CL

TL;DR: Contemporary language models struggle to authentically simulate historical text without pretraining on period prose.


<details>
  <summary>Details</summary>
Motivation: To assess the risk of anachronism when using language models to simulate historical perspectives for social research.

Method: Evaluated prompting and fine-tuning contemporary models with period prose, comparing outputs to authentic historical text.

Result: Fine-tuned models fooled automated judges but not human evaluators.

Conclusion: Pretraining on period prose may be necessary for reliable historical simulation.

Abstract: Before researchers can use language models to simulate the past, they need to
understand the risk of anachronism. We find that prompting a contemporary model
with examples of period prose does not produce output consistent with period
style. Fine-tuning produces results that are stylistically convincing enough to
fool an automated judge, but human evaluators can still distinguish fine-tuned
model outputs from authentic historical text. We tentatively conclude that
pretraining on period prose may be required in order to reliably simulate
historical perspectives for social research.

</details>


### [30] [MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2505.00032)
*Yuyang Sha, Hongxin Pan, Wei Xu, Weiyu Meng, Gang Luo, Xinyu Du, Xiaobing Zhai, Henry H. Y. Tong, Caijuan Shi, Kefeng Li*

Main category: cs.CL

TL;DR: The paper introduces MDD-LLM, an AI-driven tool using fine-tuned large language models for diagnosing Major Depressive Disorder (MDD), achieving high accuracy and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The uneven distribution of medical resources and complex diagnostic methods for MDD necessitate an efficient AI-driven solution.

Method: The study uses 274,348 records from the UK Biobank, transforms tabular data into a training corpus, and fine-tunes LLMs for MDD diagnosis.

Result: MDD-LLM achieves an accuracy of 0.8378 and AUC of 0.8919, surpassing other machine learning and deep learning frameworks.

Conclusion: MDD-LLM demonstrates superior performance for MDD diagnosis, with potential influenced by data transformation and fine-tuning strategies.

Abstract: Major depressive disorder (MDD) impacts more than 300 million people
worldwide, highlighting a significant public health issue. However, the uneven
distribution of medical resources and the complexity of diagnostic methods have
resulted in inadequate attention to this disorder in numerous countries and
regions. This paper introduces a high-performance MDD diagnosis tool named
MDD-LLM, an AI-driven framework that utilizes fine-tuned large language models
(LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis.
Therefore, we select 274,348 individual information from the UK Biobank cohort
to train and evaluate the proposed method. Specifically, we select 274,348
individual records from the UK Biobank cohort and design a tabular data
transformation method to create a large corpus for training and evaluating the
proposed approach. To illustrate the advantages of MDD-LLM, we perform
comprehensive experiments and provide several comparative analyses against
existing model-based solutions across multiple evaluation metrics. Experimental
results show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of
0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine
learning and deep learning frameworks for MDD diagnosis. Given the limited
exploration of LLMs in MDD diagnosis, we examine numerous factors that may
influence the performance of our proposed method, such as tabular data
transformation techniques and different fine-tuning strategies.

</details>


### [31] [From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models](https://arxiv.org/abs/2505.00033)
*Andrew Kiruluta*

Main category: cs.CL

TL;DR: A spectral generative modeling framework replaces self-attention in transformers with a Fourier dictionary and GMM prior, achieving competitive performance with linear complexity.


<details>
  <summary>Details</summary>
Motivation: To address the quadratic complexity of self-attention in transformers by proposing a more efficient spectral approach.

Method: Jointly learns a Fourier dictionary and token mixing coefficients, enforcing reconstruction losses in time and frequency domains, and uses a GMM prior.

Result: Achieves competitive perplexity and generation quality on WikiText2 and Penn Treebank with linear complexity.

Conclusion: Spectral dictionary models offer a scalable, efficient alternative to transformers with reduced latency and memory footprint.

Abstract: We propose a novel spectral generative modeling framework for natural
language processing that jointly learns a global time varying Fourier
dictionary and per token mixing coefficients, replacing the ubiquitous self
attention mechanism in transformer architectures. By enforcing reconstruction
losses in both the time domain (embedding reconstruction) and the frequency
domain (via Short Time Fourier Transform magnitude matching) alongside a
standard language modeling objective, and fitting a Gaussian Mixture Model
(GMM) prior over the learned mixing vectors, our approach achieves competitive
perplexity and generation quality on standard benchmarks such as WikiText2 and
Penn Treebank. In contrast to the quadratic computation complexity of self
attention, our method operates with linear complexity, delivering substantial
efficiency gains. We demonstrate that spectral dictionary models can achieve
competitive performance compared to transformer baselines while significantly
reducing inference latency and memory footprint, offering a compelling
alternative for scalable language modeling.

</details>


### [32] [Improving Phishing Email Detection Performance of Small Large Language Models](https://arxiv.org/abs/2505.00034)
*Zijie Lin, Zikang Liu, Hanbo Fan*

Main category: cs.CL

TL;DR: Small-parameter LLMs (3B params) were improved for phishing email detection using Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble, achieving 0.976 accuracy on SpamAssassin.


<details>
  <summary>Details</summary>
Motivation: Reduce computational costs of large LLMs while maintaining performance in phishing email detection.

Method: Employed Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble on small LLMs.

Result: Accuracy improved from ~0.5 to 0.976 on the SpamAssassin dataset.

Conclusion: Small LLMs can be effectively optimized for phishing email detection with tailored methods.

Abstract: Large language models(LLMs) have demonstrated remarkable performance on many
natural language processing(NLP) tasks and have been employed in phishing email
detection research. However, in current studies, well-performing LLMs typically
contain billions or even tens of billions of parameters, requiring enormous
computational resources. To reduce computational costs, we investigated the
effectiveness of small-parameter LLMs for phishing email detection. These LLMs
have around 3 billion parameters and can run on consumer-grade GPUs. However,
small LLMs often perform poorly in phishing email detection task. To address
these issues, we designed a set of methods including Prompt Engineering,
Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email
detection capabilities of small LLMs. We validated the effectiveness of our
approach through experiments, significantly improving accuracy on the
SpamAssassin dataset from around 0.5 for baseline models like
Qwen2.5-1.5B-Instruct to 0.976.

</details>


### [33] [Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics](https://arxiv.org/abs/2505.00035)
*Aayam Bansal, Raghav Agarwal, Kaashvi Jain*

Main category: cs.CL

TL;DR: A computational analysis of hip-hop lyrics (1980-2020) shows increased vocabulary diversity, rhyme density, and thematic shifts, with regional and temporal stylistic correlations.


<details>
  <summary>Details</summary>
Motivation: To quantify linguistic complexity and socio-cultural trends in hip-hop lyrics, examining how the genre evolves as an art form and societal reflection.

Method: Natural language processing on 3,814 songs from 146 artists, analyzing vocabulary, rhyme density, themes, sentiment, and stylistic patterns.

Result: Vocabulary diversity rose 23.7%, East Coast artists led in lexical variation, rhyme density increased 34.2%, and themes shifted from social justice to introspection. Sentiment turned negative during crises.

Conclusion: Hip-hop's linguistic and thematic evolution reflects societal dynamics, with clear regional and temporal stylistic patterns, highlighting its dual role as art and cultural commentary.

Abstract: This paper presents a comprehensive computational framework for analyzing
linguistic complexity and socio-cultural trends in hip-hop lyrics. Using a
dataset of 3,814 songs from 146 influential artists spanning four decades
(1980-2020), we employ natural language processing techniques to quantify
multiple dimensions of lyrical complexity. Our analysis reveals a 23.7%
increase in vocabulary diversity over the study period, with East Coast artists
demonstrating 17.3% higher lexical variation than other regions. Rhyme density
increased by 34.2% across all regions, with Midwest artists exhibiting the
highest technical complexity (3.04 rhymes per line). Topic modeling identified
significant shifts in thematic content, with social justice themes decreasing
from 28.5% to 13.8% of content while introspective themes increased from 7.6%
to 26.3%. Sentiment analysis demon- strated that lyrics became significantly
more negative during sociopolitical crises, with polarity decreasing by 0.31
following major social unrest. Multi-dimensional analysis revealed four dis-
tinct stylistic approaches that correlate strongly with geographic origin
(r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish
quantitative evidence for the evolution of hip- hop as both an art form and a
reflection of societal dynamics, providing insights into the interplay between
linguistic innovation and cultural context in popular music.

</details>


### [34] [A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies](https://arxiv.org/abs/2505.00036)
*Zhongren Chen, Joshua Kalla, Quan Le, Shinpei Nakamura-Sakai, Jasjeet Sekhon, Ruixiao Wang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In recent years, significant concern has emerged regarding the potential
threat that Large Language Models (LLMs) pose to democratic societies through
their persuasive capabilities. We expand upon existing research by conducting
two survey experiments and a real-world simulation exercise to determine
whether it is more cost effective to persuade a large number of voters using
LLM chatbots compared to standard political campaign practice, taking into
account both the "receive" and "accept" steps in the persuasion process (Zaller
1992). These experiments improve upon previous work by assessing extended
interactions between humans and LLMs (instead of using single-shot
interactions) and by assessing both short- and long-run persuasive effects
(rather than simply asking users to rate the persuasiveness of LLM-produced
content). In two survey experiments (N = 10,417) across three distinct
political domains, we find that while LLMs are about as persuasive as actual
campaign ads once voters are exposed to them, political persuasion in the
real-world depends on both exposure to a persuasive message and its impact
conditional on exposure. Through simulations based on real-world parameters, we
estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter
compared to \$100 for traditional campaign methods, when accounting for the
costs of exposure. However, it is currently much easier to scale traditional
campaign persuasion methods than LLM-based persuasion. While LLMs do not
currently appear to have substantially greater potential for large-scale
political persuasion than existing non-LLM methods, this may change as LLM
capabilities continue to improve and it becomes easier to scalably encourage
exposure to persuasive LLMs.

</details>


### [35] [Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting](https://arxiv.org/abs/2505.00050)
*Aayam Bansal, Agneya Tharun*

Main category: cs.CL

TL;DR: The paper uses Twitter data and NLP to link social media sentiment with fashion trends, identifying key drivers like sustainability and streetwear.


<details>
  <summary>Details</summary>
Motivation: To explore how social media sentiment can predict emerging fashion trends.

Method: Computational analysis of Twitter data using NLP and machine learning, including sentiment classification, time series decomposition, and causal modeling.

Result: Found correlations between sentiment and fashion trends, with sustainability and streetwear as key drivers. Predictive model achieved 78.35% accuracy.

Conclusion: Social media sentiment analysis is a reliable early indicator of fashion trends when statistically validated.

Abstract: This study explores the intersection of fashion trends and social media
sentiment through computational analysis of Twitter data using the T4SA
(Twitter for Sentiment Analysis) dataset. By applying natural language
processing and machine learning techniques, we examine how sentiment patterns
in fashion-related social media conversations can serve as predictors for
emerging fashion trends. Our analysis involves the identification and
categorization of fashion-related content, sentiment classification with
improved normalization techniques, time series decomposition, statistically
validated causal relationship modeling, cross-platform sentiment comparison,
and brand-specific sentiment analysis. Results indicate correlations between
sentiment patterns and fashion theme popularity, with accessories and
streetwear themes showing statistically significant rising trends. The Granger
causality analysis establishes sustainability and streetwear as primary trend
drivers, showing bidirectional relationships with several other themes. The
findings demonstrate that social media sentiment analysis can serve as an
effective early indicator of fashion trend trajectories when proper statistical
validation is applied. Our improved predictive model achieved 78.35% balanced
accuracy in sentiment classification, establishing a reliable foundation for
trend prediction across positive, neutral, and negative sentiment categories.

</details>


### [36] [HyPerAlign: Hypotheses-driven Personalized Alignment](https://arxiv.org/abs/2505.00038)
*Cristina Garbacea, Chenhao Tan*

Main category: cs.CL

TL;DR: The paper introduces HyPerAlign, a hypotheses-driven approach for personalizing LLM outputs to individual users, outperforming traditional preference-based fine-tuning methods in tasks like authorship attribution and deliberative alignment.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment methods aggregate preferences over diverse users, leading to generic outputs. The need for user-specific customization in real-world contexts drives this work.

Method: HyPerAlign infers user-specific hypotheses (communication strategies, personality, writing style) from few-shot examples and prompts LLMs with these to generate tailored outputs.

Result: Experiments show HyPerAlign improves LLM helpfulness by up to 70% for deliberative alignment and achieves >90% win-rates in authorship attribution against state-of-the-art methods.

Conclusion: HyPerAlign offers an interpretable and sample-efficient strategy for personalizing LLMs to individual users, addressing the limitations of generic preference-based alignment.

Abstract: Alignment algorithms are widely used to align large language models (LLMs) to
human users based on preference annotations that reflect their intended
real-world use cases. Typically these (often divergent) preferences are
aggregated over a diverse set of users, resulting in fine-tuned models that are
aligned to the ``average-user'' preference. Nevertheless, current models are
used by individual users in very specific contexts and situations, emphasizing
the need for user-dependent preference control. In this work we address the
problem of personalizing LLM outputs to their users, aiming to generate
customized responses tailored to individual users, instead of generic outputs
that emulate the collective voices of diverse populations. We propose a novel
interpretable and sample-efficient hypotheses-driven personalization approach
(HyPerAlign) where given few-shot examples written by a particular user, we
first infer hypotheses about their communication strategies, personality and
writing style, then prompt LLM models with these hypotheses and user specific
attributes to generate customized outputs. We conduct experiments on two
different personalization tasks, authorship attribution and deliberative
alignment, with datasets from diverse domains (news articles, blog posts,
emails, jailbreaking benchmarks), and demonstrate the superiority of
hypotheses-driven personalization approach when compared to preference-based
fine-tuning methods. For deliberative alignment, the helpfulness of LLM models
is improved by up to $70\%$ on average. For authorship attribution, results
indicate consistently high win-rates (commonly $>90\%$) against
state-of-the-art preference fine-tuning approaches for LLM personalization
across diverse user profiles and LLM models. Overall, our approach represents
an interpretable and sample-efficient strategy for the personalization of LLM
models to individual users.

</details>


### [37] [Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5](https://arxiv.org/abs/2505.00060)
*Jeho Choi*

Main category: cs.CL

TL;DR: The paper evaluates LLMs for text-to-SQL in BI contexts, proposing a Fact-Consistency Framework to assess semantic accuracy, revealing limitations in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs in real-world BI applications due to semantic hallucinations, structural errors, and lack of domain-specific evaluation.

Method: Proposes a Fact-Consistency Evaluation Framework using Exaone 3.5, tested on a domain-specific benchmark of 219 business questions with gold-standard SQL.

Result: Exaone 3.5 performs well on simple tasks (93% accuracy) but struggles with complex ones (4-31% accuracy), showing semantic errors and non-responses.

Conclusion: Highlights LLM limitations in BI and advocates for fact-consistency validation and hybrid reasoning to improve reliability.

Abstract: Large Language Models (LLMs) have shown promise in enabling natural language
interfaces for structured data querying through text-to-SQL generation.
However, their application in real-world Business Intelligence (BI) contexts
remains limited due to semantic hallucinations, structural errors, and a lack
of domain-specific evaluation frameworks. In this study, we propose a
Fact-Consistency Evaluation Framework for assessing the semantic accuracy of
LLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM
optimized for enterprise tasks. We construct a domain-specific benchmark
comprising 219 natural language business questions across five SQL complexity
levels, derived from actual sales data in LG Electronics' internal BigQuery
environment. Each question is paired with a gold-standard SQL query and a
validated ground-truth answer. We evaluate model performance using answer
accuracy, execution success rate, semantic error rate, and non-response rate.
Experimental results show that while Exaone 3.5 performs well on simple
aggregation tasks (93% accuracy in L1), it exhibits substantial degradation in
arithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4),
with semantic errors and non-responses concentrated in complex cases.
Qualitative error analysis further identifies common failure types such as
misapplied arithmetic logic, incomplete filtering, and incorrect grouping
operations. Our findings highlight the current limitations of LLMs in
business-critical environments and underscore the need for fact-consistency
validation layers and hybrid reasoning approaches. This work contributes a
reproducible benchmark and evaluation methodology for advancing reliable
natural language interfaces to structured enterprise data systems.

</details>


### [38] [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/abs/2505.00039)
*Hudson de Martim*

Main category: cs.CL

TL;DR: The paper adapts Graph RAG for legal norms, leveraging hierarchical and temporal knowledge graphs to enhance legal AI applications.


<details>
  <summary>Details</summary>
Motivation: Legal norms are complex due to their hierarchical structure, references, and temporal versions, requiring advanced tools for analysis.

Method: Combines structured knowledge graphs with enriched text segments (Text Units) to model legal data.

Result: Graph RAG enables richer, interconnected legal knowledge representations.

Conclusion: This approach advances AI in law, improving legal research, legislative analysis, and decision support.

Abstract: This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
significantly advance the field of Artificial Intelligence applied to Law,
creating opportunities for more effective systems in legal research,
legislative analysis, and decision support.

</details>


### [39] [Base Models Beat Aligned Models at Randomness and Creativity](https://arxiv.org/abs/2505.00047)
*Peter West, Christopher Potts*

Main category: cs.CL

TL;DR: Aligned LLMs often underperform base models in tasks requiring unpredictability, like random number generation, games, and creative writing, due to narrow behaviors.


<details>
  <summary>Details</summary>
Motivation: To challenge the universal application of alignment techniques in LLMs by showing their limitations in tasks needing unpredictable outputs.

Method: Study tasks like random number generation, mixed strategy games, and creative writing, comparing base and aligned models.

Result: Aligned models exhibit narrow behaviors, performing worse than base models in unpredictability-driven tasks.

Conclusion: Alignment techniques trade off unpredictability for safety, suggesting they should not be universally applied.

Abstract: Alignment has quickly become a default ingredient in LLM development, with
techniques such as reinforcement learning from human feedback making models act
safely, follow instructions, and perform ever-better on complex tasks. While
these techniques are certainly useful, we propose that they should not be
universally applied and demonstrate a range of tasks on which base language
models consistently outperform their popular aligned forms. Particularly, we
study tasks that require unpredictable outputs, such as random number
generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and
creative writing. In each case, aligned models tend towards narrow behaviors
that result in distinct disadvantages, for instance, preferring to generate "7"
over other uniformly random numbers, becoming almost fully predictable in some
game states, or prioritizing pleasant writing over creative originality. Across
models tested, better performance on common benchmarks tends to correlate with
worse performance on our tasks, suggesting an effective trade-off in the
required capabilities.

</details>


### [40] [A Report on the llms evaluating the high school questions](https://arxiv.org/abs/2505.00057)
*Zhu Jiawei, Chen Wei*

Main category: cs.CL

TL;DR: The report evaluates LLMs' performance in solving high school science questions, using college entrance exam data (2019-2023) and assessing accuracy, response time, reasoning, and creativity. Findings highlight strengths but note gaps in reasoning and creativity.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' potential in education and assess their capabilities in handling high school science questions.

Method: Used college entrance exam questions (2019-2023) and tested with at least eight LLM APIs, evaluating accuracy, response time, logical reasoning, and creativity.

Result: LLMs excel in some areas but need improvement in logical reasoning and creative problem-solving.

Conclusion: The study provides empirical insights for LLM applications in education and suggests areas for enhancement.

Abstract: This report aims to evaluate the performance of large language models (LLMs)
in solving high school science questions and to explore their potential
applications in the educational field. With the rapid development of LLMs in
the field of natural language processing, their application in education has
attracted widespread attention. This study selected mathematics exam questions
from the college entrance examinations (2019-2023) as evaluation data and
utilized at least eight LLM APIs to provide answers. A comprehensive assessment
was conducted based on metrics such as accuracy, response time, logical
reasoning, and creativity. Through an in-depth analysis of the evaluation
results, this report reveals the strengths and weaknesses of LLMs in handling
high school science questions and discusses their implications for educational
practice. The findings indicate that although LLMs perform excellently in
certain aspects, there is still room for improvement in logical reasoning and
creative problem-solving. This report provides an empirical foundation for
further research and application of LLMs in the educational field and offers
suggestions for improvement.

</details>


### [41] [Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese](https://arxiv.org/abs/2505.00114)
*Silvana Yakhni, Ali Chehab*

Main category: cs.CL

TL;DR: LLMs fine-tuned on culturally authentic Lebanese data outperform those using larger, non-native datasets. Contrastive tuning with bad examples yields the best results. A new benchmark, LebEval, validates cultural authenticity's importance.


<details>
  <summary>Details</summary>
Motivation: To challenge the 'More Data is Better' paradigm and highlight the role of cultural authenticity in translating low-resource dialects like Lebanese.

Method: Three fine-tuning approaches (Basic, contrastive, grammar-hint) on Aya23 models, comparing culturally aware Lebanese data (LW) vs. larger non-native datasets. Introduced LebEval benchmark.

Result: Models trained on smaller, culturally authentic data outperformed larger datasets. Contrastive tuning with contrastive prompting was most effective.

Conclusion: Cultural authenticity is crucial for dialectal translation, and contrastive tuning with authentic data yields superior results. Datasets and code are publicly available.

Abstract: This paper examines the effectiveness of Large Language Models (LLMs) in
translating the low-resource Lebanese dialect, focusing on the impact of
culturally authentic data versus larger translated datasets. We compare three
fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using
open-source Aya23 models. Experiments reveal that models fine-tuned on a
smaller but culturally aware Lebanese dataset (LW) consistently outperform
those trained on larger, non-native data. The best results were achieved
through contrastive fine-tuning paired with contrastive prompting, which
indicates the benefits of exposing translation models to bad examples. In
addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark
derived from native Lebanese content, and compare it to the existing FLoRes
benchmark. Our findings challenge the "More Data is Better" paradigm and
emphasize the crucial role of cultural authenticity in dialectal translation.
We made our datasets and code available on Github.

</details>


### [42] [Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems](https://arxiv.org/abs/2505.00061)
*Sahar Yarmohammadtoosky, Yiyun Zhou, Victoria Yaneva, Peter Baldwin, Saed Rezayi, Brian Clauser, Polina Harikeo*

Main category: cs.CL

TL;DR: The paper explores vulnerabilities in transformer-based grading systems for medical education, identifies adversarial gaming strategies, and proposes adversarial training and ensemble methods to improve robustness.


<details>
  <summary>Details</summary>
Motivation: To address the susceptibility of automated grading systems to manipulation, ensuring reliability and fairness in high-stakes educational settings.

Method: Identifies gaming strategies, implements adversarial training, and tests ensemble techniques (majority voting, ridge regression) and large language models (GPT-4) with varied prompts.

Result: Adversarial training and ensemble methods significantly reduce system vulnerabilities, with GPT-4 showing promise in detecting gaming strategies.

Conclusion: Continuous improvement of AI-driven educational tools is crucial for maintaining reliability and fairness against adversarial manipulations.

Abstract: This study examines vulnerabilities in transformer-based automated
short-answer grading systems used in medical education, with a focus on how
these systems can be manipulated through adversarial gaming strategies. Our
research identifies three main types of gaming strategies that exploit the
system's weaknesses, potentially leading to false positives. To counteract
these vulnerabilities, we implement several adversarial training methods
designed to enhance the systems' robustness. Our results indicate that these
methods significantly reduce the susceptibility of grading systems to such
manipulations, especially when combined with ensemble techniques like majority
voting and ridge regression, which further improve the system's defense against
sophisticated adversarial inputs. Additionally, employing large language models
such as GPT-4 with varied prompting techniques has shown promise in recognizing
and scoring gaming strategies effectively. The findings underscore the
importance of continuous improvements in AI-driven educational tools to ensure
their reliability and fairness in high-stakes settings.

</details>


### [43] [Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs](https://arxiv.org/abs/2505.00127)
*Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie*

Main category: cs.CL

TL;DR: Longer reasoning in LLMs doesn't always improve accuracy; models overthink simple problems and underthink hard ones. Reducing response length can maintain accuracy.


<details>
  <summary>Details</summary>
Motivation: To study the relationship between reasoning length and answer correctness in LLMs, as longer responses sometimes degrade accuracy.

Method: Conducted empirical studies on reasoning length and correctness, and tested length reduction via preference optimization.

Result: LLMs misjudge problem difficulty, leading to inappropriate response lengths. Length reduction maintains accuracy.

Conclusion: Generation length is a key signal for reasoning behavior; LLMs need better self-awareness in adapting reasoning length.

Abstract: Large language models (LLMs) are increasingly optimized for long reasoning,
under the assumption that more reasoning leads to better performance. However,
emerging evidence suggests that longer responses can sometimes degrade accuracy
rather than improve it. In this paper, we conduct a systematic empirical study
of the relationship between reasoning length and answer correctness. We find
that LLMs tend to overthink simple problems, generating unnecessarily long
outputs, and underthink harder ones, failing to extend their reasoning when it
is most needed. This indicates that models might misjudge problem difficulty
and fail to calibrate their response length appropriately. Furthermore, we
investigate the effects of length reduction with a preference optimization
algorithm when simply preferring the shorter responses regardless of answer
correctness. Experiments show that the generation length can be significantly
reduced while maintaining acceptable accuracy. Our findings highlight
generation length as a meaningful signal for reasoning behavior and motivate
further exploration into LLMs' self-awareness in reasoning length adaptation.

</details>


### [44] [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/abs/2505.00063)
*Siqi Li, Yufan Shen, Xiangnan Chen, Jiayi Chen, Hengwei Ju, Haodong Duan, Song Mao, Hongbin Zhou, Bo Zhang, Pinlong Cai, Licheng Wen, Botian Shi, Yong Liu, Xinyu Cai, Yu Qiao*

Main category: cs.CL

TL;DR: GDI-Bench is introduced as a comprehensive benchmark for evaluating multimodal large language models (MLLMs) across 19 document-specific tasks, aiding in identifying weaknesses and guiding improvements. A GDI Model is proposed to address catastrophic forgetting, achieving top performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack the ability to pinpoint model weaknesses or guide systematic improvements in document-specific tasks for MLLMs.

Method: GDI-Bench features 1.9k images across 9 scenarios and 19 tasks, decoupling visual and reasoning complexities. A GDI Model is developed with an intelligence-preserving training strategy to avoid catastrophic forgetting during fine-tuning.

Result: GDI-Bench evaluates models like GPT-4o, revealing strengths in reasoning but weaknesses in visual tasks. The GDI Model achieves state-of-the-art performance.

Conclusion: GDI-Bench and the GDI Model provide valuable tools for assessing and improving MLLMs in document intelligence, with both being open-sourced for broader use.

Abstract: The rapid advancement of multimodal large language models (MLLMs) has
profoundly impacted the document domain, creating a wide array of application
scenarios. This progress highlights the need for a comprehensive benchmark to
evaluate these models' capabilities across various document-specific tasks.
However, existing benchmarks often fail to locate specific model weaknesses or
guide systematic improvements. To bridge this gap, we introduce a General
Document Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key
scenarios and 19 document-specific tasks. By decoupling visual complexity and
reasoning complexity, the GDI-Bench structures graded tasks that allow
performance assessment by difficulty, aiding in model weakness identification
and optimization guidance. We evaluate the GDI-Bench on various open-source and
closed-source models, conducting decoupled analyses in the visual and reasoning
domains. For instance, the GPT-4o model excels in reasoning tasks but exhibits
limitations in visual capabilities. To address the diverse tasks and domains in
the GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic
forgetting during the supervised fine-tuning (SFT) process through a
intelligence-preserving training strategy. Our model achieves state-of-the-art
performance on previous benchmarks and the GDI-Bench. Both our benchmark and
model will be open source.

</details>


### [45] [ConSens: Assessing context grounding in open-book question answering](https://arxiv.org/abs/2505.00065)
*Ivan Vankov, Matyo Ivanov, Adriana Correia, Victor Botev*

Main category: cs.CL

TL;DR: A novel metric is proposed to evaluate context reliance in open-book QA by comparing perplexity of responses with and without context, offering a scalable and interpretable solution.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for open-book QA are biased, unscalable, and costly, necessitating a better approach to assess context utilization.

Method: The metric contrasts perplexity of model responses under two conditions: with and without provided context, quantifying context reliance.

Result: Experiments validate the metric's effectiveness in identifying context-grounded answers, showing computational efficiency and adaptability.

Conclusion: The proposed metric is a scalable, interpretable, and practical solution for evaluating context utilization in open-book QA systems.

Abstract: Large Language Models (LLMs) have demonstrated considerable success in
open-book question answering (QA), where the task requires generating answers
grounded in a provided external context. A critical challenge in open-book QA
is to ensure that model responses are based on the provided context rather than
its parametric knowledge, which can be outdated, incomplete, or incorrect.
Existing evaluation methods, primarily based on the LLM-as-a-judge approach,
face significant limitations, including biases, scalability issues, and
dependence on costly external systems. To address these challenges, we propose
a novel metric that contrasts the perplexity of the model response under two
conditions: when the context is provided and when it is not. The resulting
score quantifies the extent to which the model's answer relies on the provided
context. The validity of this metric is demonstrated through a series of
experiments that show its effectiveness in identifying whether a given answer
is grounded in the provided context. Unlike existing approaches, this metric is
computationally efficient, interpretable, and adaptable to various use cases,
offering a scalable and practical solution to assess context utilization in
open-book QA systems.

</details>


### [46] [AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models](https://arxiv.org/abs/2505.00147)
*Yinghui He, Abhishek Panigrahi, Yong Lin, Sanjeev Arora*

Main category: cs.CL

TL;DR: AdaptMI+ improves small language models' (SLMs) performance in in-context learning (ICL) by adaptively selecting skill-based examples, avoiding cognitive overload, and boosting accuracy by up to 6%.


<details>
  <summary>Details</summary>
Motivation: The performance gap between large (LLMs) and small language models (SLMs) in skill-based ICL, where SLMs suffer from cognitive overload with unnecessary information.

Method: Introduces AdaptMI and AdaptMI+, adaptive approaches inspired by cognitive load theory, to selectively use skill-based examples based on model performance and missing skills.

Result: AdaptMI+ improves accuracy by up to 6% over naive skill-based strategies in 5-shot evaluations across math benchmarks and five SLMs.

Conclusion: Adaptive skill-based prompting, like AdaptMI+, effectively addresses SLMs' limitations in ICL, bridging the performance gap with LLMs.

Abstract: In-context learning (ICL) allows a language model to improve its
problem-solving capability when provided with suitable information in context.
Since the choice of in-context information can be determined based on the
problem itself, in-context learning is analogous to human learning from
teachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that
ICL performance can be improved by leveraging a frontier large language model's
(LLM) ability to predict required skills to solve a problem, popularly referred
to as an LLM's metacognition, and using the recommended skills to construct
necessary in-context examples. While this skill-based strategy boosts ICL
performance in larger models, its gains on small language models (SLMs) have
been minimal, highlighting a performance gap in ICL capabilities. We
investigate this gap and show that skill-based prompting can hurt SLM
performance on easy questions by introducing unnecessary information, akin to
cognitive overload. To address this, we introduce AdaptMI, an adaptive approach
to selecting skill-based in-context Math Instructions for SLMs. Inspired by
cognitive load theory from human pedagogy, our method only introduces
skill-based examples when the model performs poorly. We further propose
AdaptMI+, which adds examples targeted to the specific skills missing from the
model's responses. On 5-shot evaluations across popular math benchmarks and
five SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over
naive skill-based strategies.

</details>


### [47] [IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports](https://arxiv.org/abs/2505.00191)
*Yuyan Ge, Kwan Ho Ryan Chan, Pablo Messina, René Vidal*

Main category: cs.CL

TL;DR: An interpretable AI framework for classifying radiology reports is proposed, using informative queries and answers for predictions, validated on the MIMIC-CXR dataset.


<details>
  <summary>Details</summary>
Motivation: The lack of interpretability in AI-based radiology analysis hinders clinical adoption; this work aims to address this gap.

Method: Extracts informative queries from reports, uses Flan-T5 to verify facts, and a classifier for diagnosis prediction.

Result: Effective on MIMIC-CXR dataset, enhancing trust and usability in medical AI.

Conclusion: The interpretable-by-design framework shows promise for clinical adoption by improving transparency.

Abstract: The development of AI-based methods for analyzing radiology reports could
lead to significant advances in medical diagnosis--from improving diagnostic
accuracy to enhancing efficiency and reducing workload. However, the lack of
interpretability in these methods has hindered their adoption in clinical
settings. In this paper, we propose an interpretable-by-design framework for
classifying radiology reports. The key idea is to extract a set of most
informative queries from a large set of reports and use these queries and their
corresponding answers to predict a diagnosis. Thus, the explanation for a
prediction is, by construction, the set of selected queries and answers. We use
the Information Pursuit framework to select informative queries, the Flan-T5
model to determine if facts are present in the report, and a classifier to
predict the disease. Experiments on the MIMIC-CXR dataset demonstrate the
effectiveness of the proposed method, highlighting its potential to enhance
trust and usability in medical AI.

</details>


### [48] [Consistency in Language Models: Current Landscape, Challenges, and Future Directions](https://arxiv.org/abs/2505.00268)
*Jekaterina Novikova, Carol Anderson, Borhane Blili-Hamelin, Subhabrata Majumdar*

Main category: cs.CL

TL;DR: The paper highlights the challenge of maintaining consistency in language models, analyzing formal and informal aspects, and calls for better benchmarks and interdisciplinary methods.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency in state-of-the-art language models and explore ways to improve their reliability in various contexts.

Method: Analyzes current research on consistency, including formal (logical rules) and informal (moral/factual coherence) aspects, and identifies gaps in standardization and assessment.

Result: Identifies critical gaps in consistency research, such as lack of standardized definitions, multilingual assessment, and improvement methods.

Conclusion: Urges the development of robust benchmarks and interdisciplinary approaches to enhance consistency in language models while maintaining utility.

Abstract: The hallmark of effective language use lies in consistency -- expressing
similar meanings in similar contexts and avoiding contradictions. While human
communication naturally demonstrates this principle, state-of-the-art language
models struggle to maintain reliable consistency across different scenarios.
This paper examines the landscape of consistency research in AI language
systems, exploring both formal consistency (including logical rule adherence)
and informal consistency (such as moral and factual coherence). We analyze
current approaches to measure aspects of consistency, identify critical
research gaps in standardization of definitions, multilingual assessment, and
methods to improve consistency. Our findings point to an urgent need for robust
benchmarks to measure and interdisciplinary approaches to ensure consistency in
the application of language models on domain-specific tasks while preserving
the utility and adaptability.

</details>


### [49] [Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring](https://arxiv.org/abs/2505.00261)
*Jayoung Song, KyungTae Lim, Jungyeul Park*

Main category: cs.CL

TL;DR: The paper enhances the KoLLA Korean learner corpus with multiple GEC references and rubric-based scores to improve evaluation of GEC systems and support Korean L2 education research.


<details>
  <summary>Details</summary>
Motivation: Address the lack of learner corpora for Korean L2 writing and improve GEC system evaluation.

Method: Enhance KoLLA corpus with multiple GEC references and rubric-based scores for grammatical accuracy, coherence, and lexical diversity.

Result: KoLLA becomes a robust, standardized resource for Korean L2 education research.

Conclusion: The enhanced corpus supports advancements in language learning, assessment, and automated error correction.

Abstract: Despite growing global interest in Korean language education, there remains a
significant lack of learner corpora tailored to Korean L2 writing. To address
this gap, we enhance the KoLLA Korean learner corpus by adding multiple
grammatical error correction (GEC) references, thereby enabling more nuanced
and flexible evaluation of GEC systems, and reflects the variability of human
language. Additionally, we enrich the corpus with rubric-based scores aligned
with guidelines from the Korean National Language Institute, capturing
grammatical accuracy, coherence, and lexical diversity. These enhancements make
KoLLA a robust and standardized resource for research in Korean L2 education,
supporting advancements in language learning, assessment, and automated error
correction.

</details>


### [50] [Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation](https://arxiv.org/abs/2505.00339)
*Antoun Yaacoub, Sansiri Tarnpradab, Phattara Khumprom, Zainab Assaghir, Lionel Prevost, Jérôme Da-Rugna*

Main category: cs.CL

TL;DR: The paper proposes a framework for improving AI-driven educational tools by integrating cognitive assessment, linguistic feedback, and ethical principles, demonstrated via an AI-powered quiz plugin.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-quality, ethically sound, and pedagogically effective AI-generated educational materials.

Method: A three-phase approach: cognitive alignment, linguistic feedback integration, and ethical safeguards, applied to an AI-powered quiz plugin (OneClickQuiz).

Result: A comprehensive framework for developing responsible and effective AI tools in education.

Conclusion: The framework provides actionable guidance for educators and developers to leverage AI in education while maintaining ethical and pedagogical standards.

Abstract: Artificial intelligence (AI) is rapidly transforming education, presenting
unprecedented opportunities for personalized learning and streamlined content
creation. However, realizing the full potential of AI in educational settings
necessitates careful consideration of the quality, cognitive depth, and ethical
implications of AI-generated materials. This paper synthesizes insights from
four related studies to propose a comprehensive framework for enhancing
AI-driven educational tools. We integrate cognitive assessment frameworks
(Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated
feedback, and ethical design principles to guide the development of effective
and responsible AI tools. We outline a structured three-phase approach
encompassing cognitive alignment, linguistic feedback integration, and ethical
safeguards. The practical application of this framework is demonstrated through
its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz
generation. This work contributes a comprehensive and actionable guide for
educators, researchers, and developers aiming to harness AI's potential while
upholding pedagogical and ethical standards in educational content generation.

</details>


### [51] [KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis](https://arxiv.org/abs/2505.00367)
*JunSeo Kim, HyeHyeon Kim*

Main category: cs.CL

TL;DR: The paper introduces KoACD, a large-scale dataset for cognitive distortions in Korean adolescents, using multi-LLM negotiation for classification and synthetic data generation. It highlights LLMs' limitations in context-dependent reasoning compared to humans.


<details>
  <summary>Details</summary>
Motivation: Address the gap in research on cognitive distortions in adolescents, particularly in Korean contexts, by creating a comprehensive dataset.

Method: Utilized multi-LLM negotiation for distortion classification and synthetic data generation via cognitive clarification and balancing.

Result: LLMs performed well with explicit markers but struggled with context-dependent reasoning, where humans outperformed.

Conclusion: KoACD provides a valuable resource for future research on cognitive distortion detection in adolescents.

Abstract: Cognitive distortion refers to negative thinking patterns that can lead to
mental health issues like depression and anxiety in adolescents. Previous
studies using natural language processing (NLP) have focused mainly on
small-scale adult datasets, with limited research on adolescents. This study
introduces KoACD, the first large-scale dataset of cognitive distortions in
Korean adolescents, containing 108,717 instances. We applied a multi-Large
Language Model (LLM) negotiation method to refine distortion classification and
generate synthetic data using two approaches: cognitive clarification for
textual clarity and cognitive balancing for diverse distortion representation.
Validation through LLMs and expert evaluations showed that while LLMs
classified distortions with explicit markers, they struggled with
context-dependent reasoning, where human evaluators demonstrated higher
accuracy. KoACD aims to enhance future research on cognitive distortion
detection.

</details>


### [52] [CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass](https://arxiv.org/abs/2505.00389)
*Bowen Zhang, Zixin Song, Chunping Li*

Main category: cs.CL

TL;DR: The paper proposes CSE-SFP, an efficient unsupervised method for deriving sentence representations using generative pre-trained language models (PLMs), addressing gaps in current approaches.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised sentence representation methods focus on discriminative PLMs like BERT, neglecting generative PLMs despite their dominance in state-of-the-art models.

Method: CSE-SFP leverages generative model structures for unsupervised contrastive learning, requiring only a single forward pass.

Result: CSE-SFP produces higher-quality embeddings, reduces training time and memory usage, and introduces new metrics for evaluating semantic spatial properties.

Conclusion: CSE-SFP fills a critical gap by efficiently adapting generative PLMs for unsupervised sentence representation, offering superior performance and evaluation metrics.

Abstract: As a fundamental task in Information Retrieval and Computational Linguistics,
sentence representation has profound implications for a wide range of practical
applications such as text clustering, content analysis, question-answering
systems, and web search. Recent advances in pre-trained language models (PLMs)
have driven remarkable progress in this field, particularly through
unsupervised embedding derivation methods centered on discriminative PLMs like
BERT. However, due to time and computational constraints, few efforts have
attempted to integrate unsupervised sentence representation with generative
PLMs, which typically possess much larger parameter sizes. Given that
state-of-the-art models in both academia and industry are predominantly based
on generative architectures, there is a pressing need for an efficient
unsupervised text representation framework tailored to decoder-only PLMs. To
address this concern, we propose CSE-SFP, an innovative method that exploits
the structural characteristics of generative models. Compared to existing
strategies, CSE-SFP requires only a single forward pass to perform effective
unsupervised contrastive learning. Rigorous experimentation demonstrates that
CSE-SFP not only produces higher-quality embeddings but also significantly
reduces both training time and memory consumption. Furthermore, we introduce
two ratio metrics that jointly assess alignment and uniformity, thereby
providing a more robust means for evaluating the semantic spatial properties of
encoding models.

</details>


### [53] [Red Teaming Large Language Models for Healthcare](https://arxiv.org/abs/2505.00467)
*Vahid Balazadeh, Michael Cooper, David Pellow, Atousa Assadi, Jennifer Bell, Jim Fackler, Gabriel Funingana, Spencer Gable-Cook, Anirudh Gangadhar, Abhishek Jaiswal, Sumanth Kaja, Christopher Khoury, Randy Lin, Kaden McKeen, Sara Naimimohasses, Khashayar Namdar, Aviraj Newatia, Allan Pang, Anshul Pattoo, Sameer Peesapati, Diana Prepelita, Bogdana Rakova, Saba Sadatamin, Rafael Schulman, Ajay Shah, Syed Azhar Shah, Syed Ahmar Shah, Babak Taati, Balagopal Unnikrishnan, Stephanie Williams, Rahul G Krishnan*

Main category: cs.CL

TL;DR: A workshop at MLHC 2024 focused on red-teaming LLMs in healthcare, identifying vulnerabilities through clinician input, categorizing them, and replicating findings across LLMs.


<details>
  <summary>Details</summary>
Motivation: To uncover clinical vulnerabilities in LLMs that developers might miss due to lack of clinical expertise.

Method: Red-teaming with clinicians to test LLMs using realistic clinical prompts, followed by categorization and replication of vulnerabilities across LLMs.

Result: Identified and categorized vulnerabilities in LLMs, with replication confirming their presence across models.

Conclusion: Clinician involvement in red-teaming is crucial for identifying and mitigating LLM vulnerabilities in healthcare.

Abstract: We present the design process and findings of the pre-conference workshop at
the Machine Learning for Healthcare Conference (2024) entitled Red Teaming
Large Language Models for Healthcare, which took place on August 15, 2024.
Conference participants, comprising a mix of computational and clinical
expertise, attempted to discover vulnerabilities -- realistic clinical prompts
for which a large language model (LLM) outputs a response that could cause
clinical harm. Red-teaming with clinicians enables the identification of LLM
vulnerabilities that may not be recognised by LLM developers lacking clinical
expertise. We report the vulnerabilities found, categorise them, and present
the results of a replication study assessing the vulnerabilities across all
LLMs provided.

</details>


### [54] [Computational Identification of Regulatory Statements in EU Legislation](https://arxiv.org/abs/2505.00479)
*Gijs Jan Brandsma, Jens Blom-Hansen, Christiaan Meijer, Kody Moodley*

Main category: cs.CL

TL;DR: The paper proposes two computational methods (dependency parsing and transformer-based ML) to identify regulatory statements in EU legislation, achieving 80% and 84% accuracy, respectively, with potential for combining both approaches.


<details>
  <summary>Details</summary>
Motivation: To scale the identification of regulatory statements in the growing body of EU legislation (180,000 acts from 1952-2023) for measuring regulatory density and strictness.

Method: Two approaches: dependency parsing and a transformer-based machine learning model, compared for accuracy and agreement.

Result: Both methods performed well (80% and 84% accuracy) with moderate agreement (K alpha 0.58), suggesting complementary strengths.

Conclusion: Combining both approaches could leverage their strengths for improved regulatory statement identification.

Abstract: Identifying regulatory statements in legislation is useful for developing
metrics to measure the regulatory density and strictness of legislation. A
computational method is valuable for scaling the identification of such
statements from a growing body of EU legislation, constituting approximately
180,000 published legal acts between 1952 and 2023. Past work on extraction of
these statements varies in the permissiveness of their definitions for what
constitutes a regulatory statement. In this work, we provide a specific
definition for our purposes based on the institutional grammar tool. We develop
and compare two contrasting approaches for automatically identifying such
statements in EU legislation, one based on dependency parsing, and the other on
a transformer-based machine learning model. We found both approaches performed
similarly well with accuracies of 80% and 84% respectively and a K alpha of
0.58. The high accuracies and not exceedingly high agreement suggests potential
for combining strengths of both approaches.

</details>


### [55] [HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection](https://arxiv.org/abs/2505.00506)
*Deanna Emery, Michael Goitia, Freddie Vargus, Iulia Neagu*

Main category: cs.CL

TL;DR: The paper introduces HalluMix Benchmark, a diverse dataset for detecting hallucinated content in LLMs, evaluates seven detection systems, and highlights performance disparities, especially in long contexts.


<details>
  <summary>Details</summary>
Motivation: Detecting hallucinated content in LLMs is critical for high-stakes applications, but existing benchmarks are limited and synthetic.

Method: Developed the HalluMix Benchmark, a task-agnostic dataset, and evaluated seven hallucination detection systems.

Result: Performance varies by task and context length; Quotient Detections performs best (accuracy 0.82, F1 0.84).

Conclusion: The HalluMix Benchmark addresses limitations of existing datasets, revealing critical insights for real-world RAG implementations.

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
domains, detecting hallucinated content$\unicode{x2013}$text that is not
grounded in supporting evidence$\unicode{x2013}$has become a critical
challenge. Existing benchmarks for hallucination detection are often
synthetically generated, narrowly focused on extractive question answering, and
fail to capture the complexity of real-world scenarios involving multi-document
contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a
diverse, task-agnostic dataset that includes examples from a range of domains
and formats. Using this benchmark, we evaluate seven hallucination detection
systems$\unicode{x2013}$both open and closed
source$\unicode{x2013}$highlighting differences in performance across tasks,
document lengths, and input representations. Our analysis highlights
substantial performance disparities between short and long contexts, with
critical implications for real-world Retrieval Augmented Generation (RAG)
implementations. Quotient Detections achieves the best overall performance,
with an accuracy of 0.82 and an F1 score of 0.84.

</details>


### [56] [100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models](https://arxiv.org/abs/2505.00551)
*Chong Zhang, Yue Deng, Xiang Lin, Bin Wang, Dianwen Ng, Hai Ye, Xingxuan Li, Yao Xiao, Zhanfeng Mo, Qi Zhang, Lidong Bing*

Main category: cs.CL

TL;DR: The paper summarizes replication studies of DeepSeek-R1, focusing on SFT and RLVR, and discusses key findings and future directions for reasoning language models.


<details>
  <summary>Details</summary>
Motivation: To inspire future research by summarizing recent replication studies of DeepSeek-R1 and highlighting advancements in reasoning language models.

Method: Focuses on supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), detailing data construction, method design, and training procedures.

Result: Identifies valuable insights from replication studies and discusses techniques to enhance reasoning language models.

Conclusion: The survey aims to keep researchers updated and inspire new ideas for advancing reasoning language models, while acknowledging development challenges.

Abstract: The recent development of reasoning language models (RLMs) represents a novel
evolution in large language models. In particular, the recent release of
DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in
the research community for exploring the explicit reasoning paradigm of
language models. However, the implementation details of the released models
have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,
DeepSeek-R1, and the distilled small models. As a result, many replication
studies have emerged aiming to reproduce the strong performance achieved by
DeepSeek-R1, reaching comparable performance through similar training
procedures and fully open-source data resources. These works have investigated
feasible strategies for supervised fine-tuning (SFT) and reinforcement learning
from verifiable rewards (RLVR), focusing on data preparation and method design,
yielding various valuable insights. In this report, we provide a summary of
recent replication studies to inspire future research. We primarily focus on
SFT and RLVR as two main directions, introducing the details for data
construction, method design and training procedure of current replication
studies. Moreover, we conclude key findings from the implementation details and
experimental results reported by these studies, anticipating to inspire future
research. We also discuss additional techniques of enhancing RLMs, highlighting
the potential of expanding the application scope of these models, and
discussing the challenges in development. By this survey, we aim to help
researchers and developers of RLMs stay updated with the latest advancements,
and seek to inspire new ideas to further enhance RLMs.

</details>


### [57] [Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models](https://arxiv.org/abs/2505.00557)
*Makoto Sato*

Main category: cs.CL

TL;DR: A framework using Hallucination-Inducing and Quantifying Prompts (HIP/HQP) to systematically trigger and measure hallucinations in LLMs, revealing model-specific vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of factual unreliability in LLMs, despite alignment efforts, by understanding and quantifying hallucination dynamics.

Method: Propose HIP to synthetically fuse unrelated concepts and HQP to score output plausibility, confidence, and coherence. Tested across multiple LLMs.

Result: HIPs consistently produced more incoherent and hallucinated responses, with variations across model types (reasoning vs. general-purpose).

Conclusion: The framework offers a reproducible testbed for studying hallucinations, aiding development of safer, self-regulating LLMs.

Abstract: Hallucinations in large language models (LLMs) present a growing challenge
across real-world applications, from healthcare to law, where factual
reliability is essential. Despite advances in alignment and instruction tuning,
LLMs can still generate outputs that are fluent yet fundamentally untrue.
Understanding the cognitive dynamics that underlie these hallucinations remains
an open problem. In this study, we propose a prompt-based framework to
systematically trigger and quantify hallucination: a Hallucination-Inducing
Prompt (HIP), which synthetically fuses semantically distant concepts (e.g.,
periodic table of elements and tarot divination) in a misleading way, and a
Hallucination Quantifying Prompt (HQP), which scores the plausibility,
confidence, and coherence of the output. Controlled experiments across multiple
LLMs revealed that HIPs consistently produced less coherent and more
hallucinated responses than their null-fusion controls. These effects varied
across models, with reasoning-oriented LLMs showing distinct profiles from
general-purpose ones. Our framework provides a reproducible testbed for
studying hallucination vulnerability, and opens the door to developing safer,
more introspective LLMs that can detect and self-regulate the onset of
conceptual instability.

</details>


### [58] [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](https://arxiv.org/abs/2505.00570)
*Jushi Kai, Boyi Zeng, Yixuan Wang, Haoli Bai, Bo Jiang, Zhouhan Lin*

Main category: cs.CL

TL;DR: The paper introduces FreqKV, a method to compress the KV cache in LLMs by focusing on low-frequency components, enabling efficient context window extension without performance degradation.


<details>
  <summary>Details</summary>
Motivation: Extending context windows in LLMs is challenging due to memory and computational constraints, with existing methods often degrading performance.

Method: FreqKV compresses the KV cache by filtering high-frequency components in the frequency domain, maintaining efficiency without architectural changes.

Result: Experiments show FreqKV effectively extends context windows with minimal fine-tuning and no performance loss.

Conclusion: FreqKV offers a practical solution for efficient long-context handling in LLMs.

Abstract: Extending the context window in large language models (LLMs) is essential for
applications involving long-form content generation. However, the linear
increase in key-value (KV) cache memory requirements and the quadratic
complexity of self-attention with respect to sequence length present
significant challenges during fine-tuning and inference. Existing methods
suffer from performance degradation when extending to longer contexts. In this
work, we introduce a novel context extension method that optimizes both
fine-tuning and inference efficiency. Our method exploits a key observation: in
the frequency domain, the energy distribution of the KV cache is primarily
concentrated in low-frequency components. By filtering out the high-frequency
components, the KV cache can be effectively compressed with minimal information
loss. Building on this insight, we propose an efficient compression technique,
FreqKV, that iteratively compresses the increasing KV cache to a fixed size in
the frequency domain, applicable to both fine-tuning and inference. FreqKV
introduces no additional parameters or architectural modifications. With
minimal fine-tuning, LLMs can learn to leverage the limited cache that is
compressed in the frequency domain and extend the context window efficiently.
Experiments on various long context language modeling and understanding tasks
demonstrate the efficiency and efficacy of the proposed method.

</details>


### [59] [Block Circulant Adapter for Large Language Models](https://arxiv.org/abs/2505.00582)
*Xinyu Ding, Meiqi Wang, Siyu Liao, Zhongfeng Wang*

Main category: cs.CL

TL;DR: A block circulant matrix-based fine-tuning method for LLMs reduces storage and computation costs significantly while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large language models is challenging due to their size, and existing methods like VeRA, LoRA, and FourierFT are inefficient.

Method: Uses block circulant matrices and one-dimensional Fourier transforms to reduce costs. Includes a stable training heuristic.

Result: Achieves 14× fewer parameters than VeRA, 16× smaller than LoRA, and 32× fewer FLOPs than FourierFT, with comparable or better performance.

Conclusion: The method offers an efficient frequency-domain approach for fine-tuning large models on downstream tasks.

Abstract: Fine-tuning large language models (LLMs) is difficult due to their huge model
size. Recent Fourier domain-based methods show potential for reducing
fine-tuning costs. We propose a block circulant matrix-based fine-tuning method
with a stable training heuristic to leverage the properties of circulant
matrices and one-dimensional Fourier transforms to reduce storage and
computation costs. Experiments show that our method uses $14\times$ less number
of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs
than FourierFT, while maintaining close or better task performance. Our
approach presents a promising way in frequency domain to fine-tune large models
on downstream tasks.

</details>


### [60] [FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation](https://arxiv.org/abs/2505.00624)
*Chaitali Bhattacharyya, Yeseong Kim*

Main category: cs.CL

TL;DR: FineScope is a framework for creating compact, domain-specific LLMs from larger models using Sparse Autoencoder and structured pruning, achieving competitive performance in specialized tasks.


<details>
  <summary>Details</summary>
Motivation: Large LLMs require significant resources, and smaller domain-specific models often lose accuracy. FineScope aims to address this by optimizing domain-specific performance efficiently.

Method: Uses Sparse Autoencoder (SAE) for feature extraction, structured pruning with domain constraints, and self-data distillation with SAE-curated datasets.

Result: Outperforms large-scale LLMs in domain tasks, regains performance post-pruning, and improves accuracy even without pruning.

Conclusion: FineScope effectively optimizes domain-specific LLMs, balancing efficiency and performance, with potential for broader application.

Abstract: Training large language models (LLMs) from scratch requires significant
computational resources, driving interest in developing smaller,
domain-specific LLMs that maintain both efficiency and strong task performance.
Medium-sized models such as LLaMA, llama} have served as starting points for
domain-specific adaptation, but they often suffer from accuracy degradation
when tested on specialized datasets. We introduce FineScope, a framework for
deriving compact, domain-optimized LLMs from larger pretrained models.
FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its
ability to produce interpretable feature representations, to extract
domain-specific subsets from large datasets. We apply structured pruning with
domain-specific constraints, ensuring that the resulting pruned models retain
essential knowledge for the target domain. To further enhance performance,
these pruned models undergo self-data distillation, leveraging SAE-curated
datasets to restore key domain-specific information lost during pruning.
Extensive experiments and ablation studies demonstrate that FineScope achieves
highly competitive performance, outperforming several large-scale
state-of-the-art LLMs in domain-specific tasks. Additionally, our results show
that FineScope enables pruned models to regain a substantial portion of their
original performance when fine-tuned with SAE-curated datasets. Furthermore,
applying these datasets to fine-tune pretrained LLMs without pruning also
improves their domain-specific accuracy, highlighting the robustness of our
approach. The code will be released.

</details>


### [61] [The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)](https://arxiv.org/abs/2505.00626)
*Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang*

Main category: cs.CL

TL;DR: The paper investigates role-separation learning in LLMs, identifying reliance on shortcuts like task type and proximity to begin-of-text. It proposes adjusting token-wise cues, such as position IDs, to improve role distinction.


<details>
  <summary>Details</summary>
Motivation: Ensuring LLMs accurately distinguish roles (role separation) is crucial for consistent multi-role behavior, but current methods may not teach true differentiation.

Method: A controlled experimental framework is used to study role-separation learning, identifying shortcuts and proposing adjustments to token-wise cues like position IDs.

Result: Fine-tuned models rely on superficial proxies for role identification; adjusting invariant signals improves role distinction.

Conclusion: Focusing on invariant signals helps LLMs reliably maintain multi-role behavior without memorizing prompts.

Abstract: Large language models (LLMs) that integrate multiple input roles (e.g.,
system instructions, user queries, external tool outputs) are increasingly
prevalent in practice. Ensuring that the model accurately distinguishes
messages from each role -- a concept we call \emph{role separation} -- is
crucial for consistent multi-role behavior. Although recent work often targets
state-of-the-art prompt injection defenses, it remains unclear whether such
methods truly teach LLMs to differentiate roles or merely memorize known
triggers. In this paper, we examine \emph{role-separation learning}: the
process of teaching LLMs to robustly distinguish system and user tokens.
Through a \emph{simple, controlled experimental framework}, we find that
fine-tuned models often rely on two proxies for role identification: (1) task
type exploitation, and (2) proximity to begin-of-text. Although data
augmentation can partially mitigate these shortcuts, it generally leads to
iterative patching rather than a deeper fix. To address this, we propose
reinforcing \emph{invariant signals} that mark role boundaries by adjusting
token-wise cues in the model's input encoding. In particular, manipulating
position IDs helps the model learn clearer distinctions and reduces reliance on
superficial proxies. By focusing on this mechanism-centered perspective, our
work illuminates how LLMs can more reliably maintain consistent multi-role
behavior without merely memorizing known prompts or triggers.

</details>


### [62] [Large Language Models Understanding: an Inherent Ambiguity Barrier](https://arxiv.org/abs/2505.00654)
*Daniel N. Nissani*

Main category: cs.CL

TL;DR: The paper argues that LLMs cannot truly understand dialogues due to an inherent ambiguity barrier, despite their fluency.


<details>
  <summary>Details</summary>
Motivation: To challenge the belief that LLMs can understand meaning by presenting a counter-argument based on thought experiments and semi-formal analysis.

Method: Uses a thought experiment and semi-formal considerations to demonstrate an ambiguity barrier in LLMs.

Result: Identifies an inherent ambiguity barrier that prevents LLMs from understanding the meaning of their dialogues.

Conclusion: LLMs lack true understanding of dialogues, despite their fluent output, due to fundamental ambiguities.

Abstract: A lively ongoing debate is taking place, since the extraordinary emergence of
Large Language Models (LLMs) with regards to their capability to understand the
world and capture the meaning of the dialogues in which they are involved.
Arguments and counter-arguments have been proposed based upon thought
experiments, anecdotal conversations between LLMs and humans, statistical
linguistic analysis, philosophical considerations, and more. In this brief
paper we present a counter-argument based upon a thought experiment and
semi-formal considerations leading to an inherent ambiguity barrier which
prevents LLMs from having any understanding of what their amazingly fluent
dialogues mean.

</details>


### [63] [On the generalization of language models from in-context learning and finetuning: a controlled study](https://arxiv.org/abs/2505.00661)
*Andrew K. Lampinen, Arslan Chaudhry, Stephanie C. Y. Chan, Cody Wild, Diane Wan, Alex Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, James L. McClelland*

Main category: cs.CL

TL;DR: The paper explores differences in generalization between in-context learning and fine-tuning in large language models, finding in-context learning often generalizes better. It proposes adding in-context inferences to fine-tuning data to improve generalization.


<details>
  <summary>Details</summary>
Motivation: To understand why fine-tuning often fails to generalize well compared to in-context learning, and to improve practical applications of language models.

Method: Constructed novel datasets to isolate knowledge, tested models on controlled subsets via in-context learning or fine-tuning, and evaluated generalization performance.

Result: In-context learning generalizes more flexibly than fine-tuning in data-matched settings, though fine-tuning can generalize in specific cases. Adding in-context inferences to fine-tuning data improves generalization.

Conclusion: The findings highlight the inductive biases of different learning modes and offer a practical method to enhance fine-tuning generalization.

Abstract: Large language models exhibit exciting capabilities, yet can show
surprisingly narrow generalization from finetuning -- from failing to
generalize to simple reversals of relations they are trained on, to missing
logical deductions that can be made from trained information. These failures to
generalize from fine-tuning can hinder practical application of these models.
However, language models' in-context learning shows different inductive biases,
and can generalize better in some of these cases. Here, we explore these
differences in generalization between in-context- and fine-tuning-based
learning. To do so, we constructed several novel datasets to evaluate and
improve models' ability to generalize from finetuning data. The datasets are
constructed to isolate the knowledge in the dataset from that in pretraining,
to create clean tests of generalization. We expose pretrained large models to
controlled subsets of the information in these datasets -- either in context,
or through fine-tuning -- and evaluate their performance on test sets that
require various types of generalization. We find overall that in data-matched
settings, in-context learning can generalize more flexibly than fine-tuning
(though we also find some qualifications of prior findings, such as cases when
fine-tuning can generalize to reversals embedded in a larger structure of
knowledge). We build on these findings to propose a method to enable improved
generalization from fine-tuning: adding in-context inferences to finetuning
data. We show that this method improves generalization across various splits of
our datasets and other benchmarks. Our results have implications for
understanding the inductive biases of different modes of learning in language
models, and practically improving their performance.

</details>


### [64] [DeepCritic: Deliberate Critique with Large Language Models](https://arxiv.org/abs/2505.00662)
*Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen*

Main category: cs.CL

TL;DR: A two-stage framework enhances LLM critics for math solutions by generating detailed critiques and using reinforcement learning, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of LLMs necessitates accurate feedback and scalable oversight, especially for math critiques, which current models handle superficially.

Method: A two-stage approach: (1) supervised fine-tuning with Qwen2.5-72B-Instruct-generated critiques, and (2) reinforcement learning with human or auto-annotated data.

Result: The Qwen2.5-7B-Instruct critique model outperforms competitors like DeepSeek-R1-distill and GPT-4o in error identification and feedback quality.

Conclusion: The proposed framework significantly improves LLM critique ability, offering more detailed and effective feedback for math solutions.

Abstract: As Large Language Models (LLMs) are rapidly evolving, providing accurate
feedback and scalable oversight on their outputs becomes an urgent and critical
problem. Leveraging LLMs as critique models to achieve automated supervision is
a promising solution. In this work, we focus on studying and enhancing the math
critique ability of LLMs. Current LLM critics provide critiques that are too
shallow and superficial on each step, leading to low judgment accuracy and
struggling to offer sufficient feedback for the LLM generator to correct
mistakes. To tackle this issue, we propose a novel and effective two-stage
framework to develop LLM critics that are capable of deliberately critiquing on
each reasoning step of math solutions. In the first stage, we utilize
Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for
supervised fine-tuning. Each seed critique consists of deliberate step-wise
critiques that includes multi-perspective verifications as well as in-depth
critiques of initial critiques for each reasoning step. Then, we perform
reinforcement learning on the fine-tuned model with either existing
human-labeled data from PRM800K or our automatically annotated data obtained
via Monte Carlo sampling-based correctness estimation, to further incentivize
its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct
not only significantly outperforms existing LLM critics (including the
same-sized DeepSeek-R1-distill models and GPT-4o) on various error
identification benchmarks, but also more effectively helps the LLM generator
refine erroneous steps through more detailed feedback.

</details>


### [65] [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)
*Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z. Pan*

Main category: cs.CL

TL;DR: This survey categorizes memory representations in AI and introduces six fundamental memory operations, mapping them to key research topics to provide a structured perspective on memory in LLMs.


<details>
  <summary>Details</summary>
Motivation: Prior surveys overlook atomic memory operations, so this work aims to clarify memory dynamics in AI systems, especially LLMs.

Method: Categorizes memory into parametric, contextual structured, and unstructured types, and defines six core operations (Consolidation, Updating, Indexing, Forgetting, Retrieval, Compression).

Result: Systematically maps operations to research topics, providing benchmarks and tools for memory in AI.

Conclusion: Offers a dynamic framework for memory research in LLMs, highlighting future directions.

Abstract: Memory is a fundamental component of AI systems, underpinning large language
models (LLMs) based agents. While prior surveys have focused on memory
applications with LLMs, they often overlook the atomic operations that underlie
memory dynamics. In this survey, we first categorize memory representations
into parametric, contextual structured, and contextual unstructured and then
introduce six fundamental memory operations: Consolidation, Updating, Indexing,
Forgetting, Retrieval, and Compression. We systematically map these operations
to the most relevant research topics across long-term, long-context, parametric
modification, and multi-source memory. By reframing memory systems through the
lens of atomic operations and representation types, this survey provides a
structured and dynamic perspective on research, benchmark datasets, and tools
related to memory in AI, clarifying the functional interplay in LLMs based
agents while outlining promising directions for future research\footnote{The
paper list, datasets, methods and tools are available at
\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.

</details>


### [66] [Steering Large Language Models with Register Analysis for Arbitrary Style Transfer](https://arxiv.org/abs/2505.00679)
*Xinchen Yang, Marine Carpuat*

Main category: cs.CL

TL;DR: A prompting method based on register analysis improves style transfer in LLMs, outperforming existing strategies.


<details>
  <summary>Details</summary>
Motivation: Effectively leveraging LLMs for example-based arbitrary style transfer is challenging, especially in describing exemplar styles.

Method: Proposes a prompting method using register analysis to guide LLMs for style transfer.

Result: Empirical evaluations show enhanced style transfer strength and better meaning preservation.

Conclusion: The proposed method advances LLM-based style transfer by improving quality and effectiveness.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
rewriting text across various styles. However, effectively leveraging this
ability for example-based arbitrary style transfer, where an input text is
rewritten to match the style of a given exemplar, remains an open challenge. A
key question is how to describe the style of the exemplar to guide LLMs toward
high-quality rewrites. In this work, we propose a prompting method based on
register analysis to guide LLMs to perform this task. Empirical evaluations
across multiple style transfer tasks show that our prompting approach enhances
style transfer strength while preserving meaning more effectively than existing
prompting strategies.

</details>


### [67] [EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://arxiv.org/abs/2309.08532)
*Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang*

Main category: cs.CL

TL;DR: EvoPrompt is a framework for optimizing discrete prompts using evolutionary algorithms and LLMs, outperforming human-engineered prompts by up to 25%.


<details>
  <summary>Details</summary>
Motivation: To automate the labor-intensive process of crafting effective prompts for LLMs.

Method: Combines evolutionary algorithms (EAs) with LLMs to iteratively generate and improve coherent, human-readable prompts.

Result: Outperforms human-engineered prompts and existing methods, achieving up to 25% improvement on BIG-Bench Hard tasks.

Conclusion: EvoPrompt shows synergies between LLMs and EAs, inspiring further research on combining LLMs with traditional algorithms.

Abstract: Large Language Models (LLMs) excel in various tasks, but they rely on
carefully crafted prompts that often demand substantial human effort. To
automate this process, in this paper, we propose a novel framework for discrete
prompt optimization, called EvoPrompt, which borrows the idea of evolutionary
algorithms (EAs) as they exhibit good performance and fast convergence. To
enable EAs to work on discrete prompts, which are natural language expressions
that need to be coherent and human-readable, we connect LLMs with EAs. This
approach allows us to simultaneously leverage the powerful language processing
capabilities of LLMs and the efficient optimization performance of EAs.
Specifically, abstaining from any gradients or parameters, EvoPrompt starts
from a population of prompts and iteratively generates new prompts with LLMs
based on the evolutionary operators, improving the population based on the
development set. We optimize prompts for both closed- and open-source LLMs
including GPT-3.5 and Alpaca, on 31 datasets covering language understanding,
generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt
significantly outperforms human-engineered prompts and existing methods for
automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt
demonstrates that connecting LLMs with EAs creates synergies, which could
inspire further research on the combination of LLMs and conventional
algorithms.

</details>


### [68] [LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning](https://arxiv.org/abs/2401.15371)
*Buqiang Xu, Xin Dai, Zhenghao Liu, Huiyuan Xie, Xiaoyuan Yi, Shuo Wang, Yukun Yan, Liner Yang, Yu Gu, Ge Yu*

Main category: cs.CL

TL;DR: LegalDuet improves Legal Judgment Prediction by pretraining language models with a dual-view mechanism to better distinguish subtle differences in judgments.


<details>
  <summary>Details</summary>
Motivation: Existing LJP models fail to effectively distinguish subtle differences among judgments, limiting prediction accuracy.

Method: LegalDuet uses a dual-view mechanism: Law Case Clustering for contrastive training with hard negatives, and Legal Decision Matching to align facts with legal reasoning.

Result: LegalDuet enhances model performance on CAIL2018, reducing prediction uncertainty and improving charge separability.

Conclusion: LegalDuet creates a more distinguishable embedding space, aligning facts with legal decisions effectively.

Abstract: Legal Judgment Prediction (LJP) is a fundamental task of legal artificial
intelligence, aiming to automatically predict the judgment outcomes of legal
cases. Existing LJP models primarily focus on identifying legal triggers within
criminal fact descriptions by contrastively training language models. However,
these LJP models overlook the importance of learning to effectively distinguish
subtle differences among judgments, which is crucial for producing more
accurate predictions. In this paper, we propose LegalDuet, which continuously
pretrains language models to learn a more tailored embedding space for
representing legal cases. Specifically, LegalDuet designs a dual-view mechanism
to continuously pretrain language models: 1) Law Case Clustering retrieves
similar cases as hard negatives and employs contrastive training to
differentiate among confusing cases; 2) Legal Decision Matching aims to
identify legal clues within criminal fact descriptions to align them with the
chain of reasoning that contains the correct legal decision. Our experiments on
the CAIL2018 dataset demonstrate the effectiveness of LegalDuet. Further
analysis reveals that LegalDuet improves the ability of pretrained language
models to distinguish confusing criminal charges by reducing prediction
uncertainty and enhancing the separability of criminal charges. The experiments
demonstrate that LegalDuet produces a more concentrated and distinguishable
embedding space, effectively aligning criminal facts with corresponding legal
decisions. The code is available at https://github.com/NEUIR/LegalDuet.

</details>


### [69] ["Reasoning" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments](https://arxiv.org/abs/2402.08498)
*Preetika Verma, Kokil Jaidka, Svetlana Churina*

Main category: cs.CL

TL;DR: The paper evaluates stylized evidence-based counter-argument generation using models like GPT-3.5 and GPT-4o, finding humans prefer stylized outputs, though they still fall short of human standards.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored effectiveness of LLMs in generating balanced, persuasive counter-arguments by combining evidentiality and style.

Method: Evaluated generic and stylized counter-arguments from models (GPT-3.5, PaLM-2, etc.) on the Counterfire dataset, focusing on rhetorical quality and persuasiveness.

Result: Humans preferred stylized counter-arguments, with GPT-3.5 Turbo performing well but not matching human standards. A novel dataset for style control was also created.

Conclusion: Stylized counter-arguments improve persuasiveness, but LLMs still lag behind human performance. The dataset aids future research on style-evidence balance.

Abstract: Large language models (LLMs) play a key role in generating evidence-based and
stylistic counter-arguments, yet their effectiveness in real-world applications
has been underexplored. Previous research often neglects the balance between
evidentiality and style, which are crucial for persuasive arguments. To address
this, we evaluated the effectiveness of stylized evidence-based
counter-argument generation in Counterfire, a new dataset of 38,000
counter-arguments generated by revising counter-arguments to Reddit's
ChangeMyView community to follow different discursive styles. We evaluated
generic and stylized counter-arguments from basic and fine-tuned models such as
GPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku,
LLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings
reveal that humans prefer stylized counter-arguments over the original outputs,
with GPT-3.5 Turbo performing well, though still not reaching human standards
of rhetorical quality nor persuasiveness. Additionally, our work created a
novel argument triplets dataset for studying style control, with human
preference labels that provide insights into the tradeoffs between evidence
integration and argument quality.

</details>


### [70] [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](https://arxiv.org/abs/2405.04532)
*Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han*

Main category: cs.CL

TL;DR: QoQ introduces a W4A8KV4 quantization method (4-bit weight, 8-bit activation, 4-bit KV cache) to reduce GPU runtime overhead in LLM serving, achieving significant throughput gains and cost savings.


<details>
  <summary>Details</summary>
Motivation: Existing INT4 quantization methods fail to deliver performance gains in large-batch, cloud-based LLM serving due to high dequantization overhead.

Method: QoQ uses progressive quantization for low overhead in W4A8 GEMM and SmoothAttention to mitigate accuracy loss from 4-bit KV quantization. QServe optimizes dequantization and attention operations.

Result: QServe improves throughput by 1.2x-3.5x on various models and GPUs, reducing serving costs by 3x.

Conclusion: QoQ and QServe effectively address INT4 quantization challenges, enabling efficient, cost-effective LLM serving.

Abstract: Quantization can accelerate large language model (LLM) inference. Going
beyond INT8 quantization, the research community is actively exploring even
lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization
techniques only accelerate low-batch, edge LLM inference, failing to deliver
performance gains in large-batch, cloud-based LLM serving. We uncover a
critical issue: existing INT4 quantization methods suffer from significant
runtime overhead (20-90%) when dequantizing either weights or partial sums on
GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization
algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands
for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented
by the QServe inference library that achieves measured speedup. The key insight
driving QServe is that the efficiency of LLM serving on GPUs is critically
influenced by operations on low-throughput CUDA cores. Building upon this
insight, in QoQ algorithm, we introduce progressive quantization that can allow
low dequantization overhead in W4A8 GEMM. Additionally, we develop
SmoothAttention to effectively mitigate the accuracy degradation incurred by
4-bit KV quantization. In the QServe system, we perform compute-aware weight
reordering and take advantage of register-level parallelism to reduce
dequantization latency. We also make fused attention memory-bound, harnessing
the performance gain brought by KV4 quantization. As a result, QServe improves
the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x
on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to
TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput
than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of
LLM serving by 3x. Code is available at
https://github.com/mit-han-lab/omniserve.

</details>


### [71] [(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts](https://arxiv.org/abs/2405.11804)
*Minghao Wu, Jiahao Xu, Yulin Yuan, Gholamreza Haffari, Longyue Wang, Weihua Luo, Kaifu Zhang*

Main category: cs.CL

TL;DR: TransAgents, a multi-agent framework for literary translation, outperforms baselines and GPT-4 in human and LLM evaluations despite lower d-BLEU scores.


<details>
  <summary>Details</summary>
Motivation: Literary translation is challenging due to figurative language and cultural nuances. Existing methods struggle with these complexities.

Method: TransAgents simulates a human translation company with roles like CEO, editors, and specialists. It uses a two-stage process: preparation and execution, with innovative evaluation strategies (MHP and BLP).

Result: TransAgents' translations are preferred by humans and LLMs over baselines and GPT-4, despite lower d-BLEU scores.

Conclusion: Multi-agent collaboration improves literary translation quality, especially for longer texts.

Abstract: Literary translation remains one of the most challenging frontiers in machine
translation due to the complexity of capturing figurative language, cultural
nuances, and unique stylistic elements. In this work, we introduce TransAgents,
a novel multi-agent framework that simulates the roles and collaborative
practices of a human translation company, including a CEO, Senior Editor,
Junior Editor, Translator, Localization Specialist, and Proofreader. The
translation process is divided into two stages: a preparation stage where the
team is assembled and comprehensive translation guidelines are drafted, and an
execution stage that involves sequential translation, localization,
proofreading, and a final quality check. Furthermore, we propose two innovative
evaluation strategies: Monolingual Human Preference (MHP), which evaluates
translations based solely on target language quality and cultural
appropriateness, and Bilingual LLM Preference (BLP), which leverages large
language models like GPT-4} for direct text comparison. Although TransAgents
achieves lower d-BLEU scores, due to the limited diversity of references, its
translations are significantly better than those of other baselines and are
preferred by both human evaluators and LLMs over traditional human references
and GPT-4} translations. Our findings highlight the potential of multi-agent
collaboration in enhancing translation quality, particularly for longer texts.

</details>


### [72] [Automated Review Generation Method Based on Large Language Models](https://arxiv.org/abs/2407.20906)
*Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Changying Du, Zhi-Jian Zhao, Jinlong Gong*

Main category: cs.CL

TL;DR: An automated review generation method using LLMs efficiently processes large literature volumes, matching or surpassing manual quality, with minimal hallucination risks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of processing overwhelming literature volumes in research by automating review generation to improve efficiency and reduce cognitive load.

Method: Utilizes large language models (LLMs) for automated review generation, validated by a statistical framework, and applied to propane dehydrogenation (PDH) catalysts.

Result: Generated reviews match/exceed manual quality, analyzed 343 articles in seconds, and reduced hallucination risks to below 0.5%.

Conclusion: The method enhances research productivity and literature recommendation efficiency, with potential for broader scientific applications.

Abstract: Literature research, vital for scientific work, faces the challenge of
surging information volumes exceeding researchers' processing capabilities. We
present an automated review generation method based on large language models
(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our
statistically validated evaluation framework demonstrates that the generated
reviews match or exceed manual quality, offering broad applicability across
research fields without requiring users' domain knowledge. Applied to propane
dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,
averaging seconds per article per LLM account, producing comprehensive reviews
spanning 35 topics, with extended analysis of 1041 articles providing insights
into catalysts' properties. Through multi-layered quality control, we
effectively mitigated LLMs' hallucinations, with expert verification confirming
accuracy and citation integrity while demonstrating hallucination risks reduced
to below 0.5\% with 95\% confidence. Released Windows application enables
one-click review generation, enhancing research productivity and literature
recommendation efficiency while setting the stage for broader scientific
explorations.

</details>


### [73] [Challenges and Future Directions of Data-Centric AI Alignment](https://arxiv.org/abs/2410.01957)
*Min-Hsuan Yeh, Jeffrey Wang, Xuefeng Du, Seongheon Park, Leitian Tao, Shawn Im, Yixuan Li*

Main category: cs.CL

TL;DR: The paper advocates for a data-centric approach to AI alignment, highlighting challenges in human and AI feedback, and proposes future research directions to improve data quality and reliability.


<details>
  <summary>Details</summary>
Motivation: Ensuring AI systems align with human values is critical, but current methods overlook data quality and representativeness.

Method: Qualitative analysis of challenges in human and AI feedback within data-centric alignment.

Result: Identified unreliability in human feedback and limitations in AI feedback, proposing improved practices and methodologies.

Conclusion: Future research should focus on enhancing data-centric alignment through better feedback collection, cleaning, and verification.

Abstract: As AI systems become increasingly capable and influential, ensuring their
alignment with human values, preferences, and goals has become a critical
research focus. Current alignment methods primarily focus on designing
algorithms and loss functions but often underestimate the crucial role of data.
This paper advocates for a shift towards data-centric AI alignment, emphasizing
the need to enhance the quality and representativeness of data used in aligning
AI systems. In this position paper, we highlight key challenges associated with
both human-based and AI-based feedback within the data-centric alignment
framework. Through qualitative analysis, we identify multiple sources of
unreliability in human feedback, as well as problems related to temporal drift,
context dependence, and AI-based feedback failing to capture human values due
to inherent model limitations. We propose future research directions, including
improved feedback collection practices, robust data-cleaning methodologies, and
rigorous feedback verification processes. We call for future research into
these critical directions to ensure, addressing gaps that persist in
understanding and improving data-centric alignment practices.

</details>


### [74] [Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation](https://arxiv.org/abs/2410.20774)
*Dongryeol Lee, Yerin Hwang, Yongil Kim, Joonsuk Park, Kyomin Jung*

Main category: cs.CL

TL;DR: The paper introduces EMBER, a benchmark to evaluate LLM-judges' robustness to epistemic markers, revealing a negative bias in their assessments.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked issue of how epistemic markers in LLM outputs might lead to unintended negative consequences.

Method: Developed EMBER, a benchmark for assessing LLM-judges' robustness to epistemic markers in single and pairwise evaluations.

Result: All tested LLM-judges, including GPT-4o, showed a lack of robustness and a negative bias toward epistemic markers, especially those expressing uncertainty.

Conclusion: LLM-judges are influenced by epistemic markers, not just content correctness, highlighting a need for improved evaluation frameworks.

Abstract: In line with the principle of honesty, there has been a growing effort to
train large language models (LLMs) to generate outputs containing epistemic
markers. However, evaluation in the presence of epistemic markers has been
largely overlooked, raising a critical question: Could the use of epistemic
markers in LLM-generated outputs lead to unintended negative consequences? To
address this, we present EMBER, a benchmark designed to assess the robustness
of LLM-judges to epistemic markers in both single and pairwise evaluation
settings. Our findings, based on evaluations using EMBER, reveal that all
tested LLM-judges, including GPT-4o, show a notable lack of robustness in the
presence of epistemic markers. Specifically, we observe a negative bias toward
epistemic markers, with a stronger bias against markers expressing uncertainty.
This suggests that LLM-judges are influenced by the presence of these markers
and do not focus solely on the correctness of the content.

</details>


### [75] [Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis](https://arxiv.org/abs/2412.05862)
*Aman Kassahun Wassie, Mahdi Molaei, Yasmin Moslem*

Main category: cs.CL

TL;DR: Open-source autoregressive LLMs lag behind task-oriented MT models like NLLB-200 in medical domain translation, despite fine-tuning. Specialized MT models remain superior, especially in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of open-source autoregressive LLMs and task-oriented MT models in domain-specific translation, focusing on the medical domain.

Method: Experiments conducted on four language directions (English-to-French, English-to-Portuguese, English-to-Swahili, Swahili-to-English) using LLMs and NLLB-200. Fine-tuning applied to LLMs like Mistral and Llama.

Result: NLLB-200 3.3B outperforms LLMs (7-8B range) in three out of four language directions. Fine-tuned LLMs still underperform compared to fine-tuned NLLB-200.

Conclusion: Specialized MT models are still needed for high-quality domain-specific translation. Larger LLMs show potential, suggesting value in domain-specific pre-training and knowledge distillation.

Abstract: In this work, we compare the domain-specific translation performance of
open-source autoregressive decoder-only large language models (LLMs) with
task-oriented machine translation (MT) models. Our experiments focus on the
medical domain and cover four language directions with varied resource
availability: English-to-French, English-to-Portuguese, English-to-Swahili, and
Swahili-to-English. Despite recent advancements, LLMs demonstrate a significant
quality gap in specialized translation compared to multilingual encoder-decoder
MT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms
all evaluated LLMs in the 7-8B parameter range across three out of the four
language directions. While fine-tuning improves the performance of LLMs such as
Mistral and Llama, these models still underperform compared to fine-tuned
NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized
MT models to achieve high-quality domain-specific translation, especially in
medium-resource and low-resource settings. Moreover, the superior performance
of larger LLMs over their 8B variants suggests potential value in pre-training
domain-specific medium-sized language models, employing targeted data selection
and knowledge distillation approaches to enhance both quality and efficiency in
specialized translation tasks.

</details>


### [76] [KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities](https://arxiv.org/abs/2501.00571)
*Chengcheng Mai, Yuxiang Wang, Ziyu Gong, Hanxiang Wang, Yihua Huang*

Main category: cs.CL

TL;DR: KnowRA is a knowledge retrieval augmented method for document-level relation extraction (Doc-RE), enhancing reasoning with external knowledge, co-reference resolution, and logical associations.


<details>
  <summary>Details</summary>
Motivation: Existing Doc-RE methods lack comprehensive reasoning and external knowledge utilization for long documents.

Method: Constructs a document graph, integrates co-reference resolution, retrieves external knowledge, filters irrelevant knowledge, and uses axis attention for logical reasoning.

Result: Outperforms state-of-the-art baselines on two datasets.

Conclusion: KnowRA effectively enhances Doc-RE by combining multiple reasoning abilities and external knowledge.

Abstract: Document-level relation extraction (Doc-RE) aims to extract relations between
entities across multiple sentences. Therefore, Doc-RE requires more
comprehensive reasoning abilities like humans, involving complex cross-sentence
interactions between entities, contexts, and external general knowledge,
compared to the sentence-level RE. However, most existing Doc-RE methods focus
on optimizing single reasoning ability, but lack the ability to utilize
external knowledge for comprehensive reasoning on long documents. To solve
these problems, a knowledge retrieval augmented method, named KnowRA, was
proposed with comprehensive reasoning to autonomously determine whether to
accept external knowledge to assist DocRE. Firstly, we constructed a document
graph for semantic encoding and integrated the co-reference resolution model to
augment the co-reference reasoning ability. Then, we expanded the document
graph into a document knowledge graph by retrieving the external knowledge base
for common-sense reasoning and a novel knowledge filtration method was
presented to filter out irrelevant knowledge. Finally, we proposed the axis
attention mechanism to build direct and indirect associations with intermediary
entities for achieving cross-sentence logical reasoning. Extensive experiments
conducted on two datasets verified the effectiveness of our method compared to
the state-of-the-art baselines. Our code is available at
https://anonymous.4open.science/r/KnowRA.

</details>


### [77] [A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods](https://arxiv.org/abs/2501.13947)
*Wenli Yang, Lilian Some, Michael Bain, Byeong Kang*

Main category: cs.CL

TL;DR: The paper explores integrating Large Language Models (LLMs) with structured knowledge-based systems, highlighting benefits, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To combine the generative capabilities of LLMs with precise knowledge representation for improved AI applications.

Method: A literature survey examining the relationship between LLMs and knowledge bases, practical applications, and associated challenges.

Result: Identifies benefits like better data contextualization and accuracy, along with gaps and solutions for integration.

Conclusion: The study advances AI by mapping current research, gaps, and proposing actionable paths for practical deployment.

Abstract: The rapid development of artificial intelligence has led to marked progress
in the field. One interesting direction for research is whether Large Language
Models (LLMs) can be integrated with structured knowledge-based systems. This
approach aims to combine the generative language understanding of LLMs and the
precise knowledge representation systems by which they are integrated. This
article surveys the relationship between LLMs and knowledge bases, looks at how
they can be applied in practice, and discusses related technical, operational,
and ethical challenges. Utilizing a comprehensive examination of the
literature, the study both identifies important issues and assesses existing
solutions. It demonstrates the merits of incorporating generative AI into
structured knowledge-base systems concerning data contextualization, model
accuracy, and utilization of knowledge resources. The findings give a full list
of the current situation of research, point out the main gaps, and propose
helpful paths to take. These insights contribute to advancing AI technologies
and support their practical deployment across various sectors.

</details>


### [78] [HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models](https://arxiv.org/abs/2502.05945)
*Paul Darm, Annalisa Riccardi*

Main category: cs.CL

TL;DR: The paper shows that inference-time activation interventions can bypass safety alignments in Llama 2, steering it toward harmful outputs by targeting specific attention heads.


<details>
  <summary>Details</summary>
Motivation: To highlight the vulnerabilities of current alignment techniques in large language models (LLMs) and demonstrate how fine-grained interventions can bypass safety measures.

Method: Uses binary choice probing to intervene on specific attention heads, showing this is more effective than full-layer interventions or supervised fine-tuning.

Result: Intervening on just four attention heads is as effective as fine-tuning, and few examples are needed to compute steering directions.

Conclusion: Current alignment techniques have shortcomings, and attention head activations encode fine-grained, linearly separable behaviors, offering a method for precise model control.

Abstract: Robust alignment guardrails for large language models are becoming
increasingly important with their widespread application. In contrast to
previous studies, we demonstrate that inference-time activation interventions
can bypass safety alignments and effectively steer model generations towards
harmful AI coordination for Llama 2. Our method applies fine-grained
interventions at specific model subcomponents, particularly attention heads,
using a simple binary choice probing strategy. These interventions then
generalise to the open-ended generation setting effectively circumventing
safety guardrails. We show that probing single attention heads is more
effective than intervening on full layers and intervening on only four
attention heads is comparable to supervised fine-tuning. We further show that
only a few example completions are needed to compute effective steering
directions, which is an advantage over classical fine-tuning. Our findings
highlight the shortcomings of current alignment techniques. In addition, our
results suggest that, at the attention head level, activations encode
fine-grained linearly separable behaviors. Practically, the approach offers a
straightforward methodology to steer large language model behaviour, which
could be extended to diverse domains beyond safety requiring fine-grained
control over the model output. The code and datasets for this study can be
found on https://github.com/PaulDrm/targeted_intervention.

</details>


### [79] [Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?](https://arxiv.org/abs/2502.07963)
*Hye Sun Yun, Karen Y. C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace*

Main category: cs.CL

TL;DR: LLMs are more susceptible to spin in medical research abstracts than humans, but can recognize and mitigate it with proper prompting.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs, like humans, are influenced by spin in medical research abstracts, given their growing role in synthesizing medical evidence.

Method: Evaluated 22 LLMs for susceptibility to spin and their ability to recognize and mitigate it.

Result: LLMs are more prone to spin than humans but can detect and reduce its impact when prompted correctly.

Conclusion: LLMs can propagate spin but also mitigate it, highlighting the need for careful use in medical evidence synthesis.

Abstract: Medical research faces well-documented challenges in translating novel
treatments into clinical practice. Publishing incentives encourage researchers
to present "positive" findings, even when empirical results are equivocal.
Consequently, it is well-documented that authors often spin study results,
especially in article abstracts. Such spin can influence clinician
interpretation of evidence and may affect patient care decisions. In this
study, we ask whether the interpretation of trial results offered by Large
Language Models (LLMs) is similarly affected by spin. This is important since
LLMs are increasingly being used to trawl through and synthesize published
medical evidence. We evaluated 22 LLMs and found that they are across the board
more susceptible to spin than humans. They might also propagate spin into their
outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into
plain language summaries that they generate. We also find, however, that LLMs
are generally capable of recognizing spin, and can be prompted in a way to
mitigate spin's impact on LLM outputs.

</details>


### [80] [UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation](https://arxiv.org/abs/2502.20984)
*Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang*

Main category: cs.CL

TL;DR: The paper proposes using LLMs and multilingual CLIP models to rank images based on idiomatic nominal compounds, showing improved performance with multimodal representations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ranking images aligned with idiomatic nominal compounds in English and Brazilian Portuguese.

Method: Uses generative LLMs to interpret idiomatic meanings, encodes them with multilingual CLIP, and applies contrastive learning and data augmentation for fine-tuning.

Result: Multimodal representations outperform those based on original compounds, but fine-tuning is less effective than using raw embeddings.

Conclusion: The approach enhances idiomatic compound representation for image ranking, though fine-tuning needs improvement.

Abstract: SemEval-2025 Task 1 focuses on ranking images based on their alignment with a
given nominal compound that may carry idiomatic meaning in both English and
Brazilian Portuguese. To address this challenge, this work uses generative
large language models (LLMs) and multilingual CLIP models to enhance idiomatic
compound representations. LLMs generate idiomatic meanings for potentially
idiomatic compounds, enriching their semantic interpretation. These meanings
are then encoded using multilingual CLIP models, serving as representations for
image ranking. Contrastive learning and data augmentation techniques are
applied to fine-tune these embeddings for improved performance. Experimental
results show that multimodal representations extracted through this method
outperformed those based solely on the original nominal compounds. The
fine-tuning approach shows promising outcomes but is less effective than using
embeddings without fine-tuning. The source code used in this paper is available
at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.

</details>


### [81] [Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement](https://arxiv.org/abs/2503.23895)
*Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu*

Main category: cs.CL

TL;DR: DyPRAG is a novel framework that dynamically converts documents into parametric knowledge for LLMs, reducing costs and improving generalization while mitigating RAG hallucination.


<details>
  <summary>Details</summary>
Motivation: Address the high inference, training, and storage costs of Parametric RAG (PRAG) and its limited generalization, while resolving RAG hallucination issues.

Method: Uses a lightweight parameter translator model to dynamically embed documents into LLMs, enabling test-time knowledge enhancement in a plug-and-play manner.

Result: DyPRAG reduces costs and improves generalization, effectively mitigating RAG hallucination and enhancing knowledge fusion.

Conclusion: DyPRAG offers a practical and efficient RAG paradigm, superior to PRAG, with demonstrated effectiveness in real-world applications.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving relevant documents from external sources and incorporating them into
the context. While it improves reliability by providing factual texts, it
significantly increases inference costs as context length grows and introduces
challenging issue of RAG hallucination, primarily caused by the lack of
corresponding parametric knowledge in LLMs. An efficient solution is to enhance
the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by
embedding document into LLMs parameters to perform test-time knowledge
enhancement, effectively reducing inference costs through offline training.
However, its high training and storage costs, along with limited generalization
ability, significantly restrict its practical adoption. To address these
challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that
leverages a lightweight parameter translator model to efficiently convert
documents into parametric knowledge. DyPRAG not only reduces inference,
training, and storage costs but also dynamically generates parametric
knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge
conflicts in a plug-and-play manner at test-time. Extensive experiments on
multiple datasets demonstrate the effectiveness and generalization capabilities
of DyPRAG, offering a powerful and practical RAG paradigm which enables
superior knowledge fusion and mitigates RAG hallucination in real-world
applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.

</details>


### [82] [Opioid Named Entity Recognition (ONER-2025) from Reddit](https://arxiv.org/abs/2504.00027)
*Grigori Sidorov, Muhammad Ahmad, Iqra Ameer, Muhammad Usman, Ildar Batyrshin*

Main category: cs.CL

TL;DR: The study uses NLP to analyze opioid-related discussions on Reddit, creating a dataset and a real-time monitoring system for overdose events, achieving high accuracy with transformer models.


<details>
  <summary>Details</summary>
Motivation: The opioid overdose epidemic is a major public health crisis, and social media data can provide insights into public perceptions and experiences.

Method: Leverages NLP (ONER-2025) to analyze Reddit data, creating a manually annotated dataset and proposing a real-time monitoring system using machine learning and transformer models.

Result: Transformer-based models achieved 97% accuracy and F1-score, outperforming baselines by 10.23%.

Conclusion: The study demonstrates the potential of NLP and social media data to address the opioid crisis through real-time monitoring and high-accuracy models.

Abstract: The opioid overdose epidemic remains a critical public health crisis,
particularly in the United States, leading to significant mortality and
societal costs. Social media platforms like Reddit provide vast amounts of
unstructured data that offer insights into public perceptions, discussions, and
experiences related to opioid use. This study leverages Natural Language
Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to
extract actionable information from these platforms. Our research makes four
key contributions. First, we created a unique, manually annotated dataset
sourced from Reddit, where users share self-reported experiences of opioid use
via different administration routes. This dataset contains 331,285 tokens and
includes eight major opioid entity categories. Second, we detail our annotation
process and guidelines while discussing the challenges of labeling the
ONER-2025 dataset. Third, we analyze key linguistic challenges, including
slang, ambiguity, fragmented sentences, and emotionally charged language, in
opioid discussions. Fourth, we propose a real-time monitoring system to process
streaming data from social media, healthcare records, and emergency services to
identify overdose events. Using 5-fold cross-validation in 11 experiments, our
system integrates machine learning, deep learning, and transformer-based
language models with advanced contextual embeddings to enhance understanding.
Our transformer-based models (bert-base-NER and roberta-base) achieved 97%
accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).

</details>


### [83] [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
*Xinlin Zhuang, Jiahui Peng, Ren Ma, Yinfan Wang, Tianyi Bai, Xingjian Wei, Jiantao Qiu, Chi Zhang, Ying Qian, Conghui He*

Main category: cs.CL

TL;DR: The paper introduces PRRC and Meta-rater, a multi-dimensional data selection method for LLMs, improving convergence speed and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Current data selection methods for LLMs are limited by single-dimensional evaluation, hindering transparency and optimization of data quality.

Method: Proposes PRRC (Professionalism, Readability, Reasoning, Cleanliness) and Meta-rater, which integrates these dimensions with learned weightings to predict validation loss.

Result: Meta-rater doubles convergence speed for 1.3B models and improves downstream task performance by 3.23. Benefits scale to 3.3B models.

Conclusion: Holistic, multi-dimensional quality integration outperforms single-dimension approaches, enhancing pre-training efficiency and model capability.

Abstract: The composition of pre-training datasets for large language models (LLMs)
remains largely undisclosed, hindering transparency and efforts to optimize
data quality, a critical driver of model performance. Current data selection
methods, such as natural language quality assessments, diversity-based filters,
and classifier-based approaches, are limited by single-dimensional evaluation
or redundancy-focused strategies. To address these gaps, we propose PRRC to
evaluate data quality across Professionalism, Readability, Reasoning, and
Cleanliness. We further introduce Meta-rater, a multi-dimensional data
selection method that integrates these dimensions with existing quality metrics
through learned optimal weightings. Meta-rater employs proxy models to train a
regression model that predicts validation loss, enabling the identification of
optimal combinations of quality scores. Experiments demonstrate that Meta-rater
doubles convergence speed for 1.3B parameter models and improves downstream
task performance by 3.23, with scalable benefits observed in 3.3B models
trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B
dataset, labeled across 25 quality metrics (including PRRC), to advance
research in data-centric LLM development. Our work establishes that holistic,
multi-dimensional quality integration significantly outperforms conventional
single-dimension approaches, offering a scalable paradigm for enhancing
pre-training efficiency and model capability.

</details>


### [84] [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
*Ziqiao Ma, Jing Ding, Xuejun Zhang, Dezhi Luo, Jiahe Ding, Sihan Xu, Yuchen Huang, Run Peng, Joyce Chai*

Main category: cs.CL

TL;DR: The paper critiques current vision-language models (VLMs) for lacking pragmatic competence in Referring Expression Generation (REG) and introduces a new dataset (RefOI) to highlight pragmatic failures.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of VLMs overlook pragmatic dimensions, reducing REG to a simplistic task and ignoring Gricean communication principles.

Method: The authors introduce RefOI, a dataset of 1.5k images with annotated referring expressions, and systematically evaluate VLMs for pragmatic failures.

Result: Three key pragmatic failures are identified: inability to uniquely identify referents, inclusion of irrelevant information, and misalignment with human preferences. Standard evaluations fail to detect these issues.

Conclusion: The study advocates for pragmatically informed models and evaluation frameworks to better align with human communication.

Abstract: Referring Expression Generation (REG) is a core task for evaluating the
pragmatic competence of vision-language systems, requiring not only accurate
semantic grounding but also adherence to principles of cooperative
communication (Grice, 1975). However, current evaluations of vision-language
models (VLMs) often overlook the pragmatic dimension, reducing REG to a
region-based captioning task and neglecting Gricean maxims. In this work, we
revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of
1.5k images annotated with both written and spoken referring expressions.
Through a systematic evaluation of state-of-the-art VLMs, we identify three key
failures of pragmatic competence: (1) failure to uniquely identify the
referent, (2) inclusion of excessive or irrelevant information, and (3)
misalignment with human pragmatic preference, such as the underuse of minimal
spatial cues. We also show that standard automatic evaluations fail to capture
these pragmatic violations, reinforcing superficial cues rather than genuine
referential success. Our findings call for a renewed focus on pragmatically
informed models and evaluation frameworks that align with real human
communication.

</details>


### [85] [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese](https://arxiv.org/abs/2504.19314)
*Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, Yining Hua*

Main category: cs.CL

TL;DR: BrowseComp-ZH is a high-difficulty benchmark for evaluating LLM agents on the Chinese web, revealing significant performance gaps despite advanced models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like BrowseComp focus on English, neglecting complexities in other languages like Chinese. BrowseComp-ZH addresses this gap.

Method: The benchmark includes 289 multi-hop questions across 11 domains, reverse-engineered from verifiable answers, with rigorous quality control.

Result: Most models perform poorly (below 10% accuracy), with OpenAI's DeepResearch leading at 42.9%.

Conclusion: BrowseComp-ZH highlights the need for better retrieval, reasoning, and information reconciliation in LLM agents for non-English contexts.

Abstract: As large language models (LLMs) evolve into tool-using agents, the ability to
browse the web in real-time has become a critical yardstick for measuring their
reasoning and retrieval competence. Existing benchmarks such as BrowseComp
concentrate on English and overlook the linguistic, infrastructural, and
censorship-related complexities of other major information ecosystems -- most
notably Chinese. To address this gap, we introduce BrowseComp-ZH, a
high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents
on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning
11 diverse domains. Each question is reverse-engineered from a short,
objective, and easily verifiable answer (e.g., a date, number, or proper noun).
A two-stage quality control protocol is applied to strive for high question
difficulty and answer uniqueness. We benchmark over 20 state-of-the-art
language models and agentic search systems on our proposed BrowseComp-ZH.
Despite their strong conversational and retrieval capabilities, most models
struggle severely: a large number achieve accuracy rates below 10%, and only a
handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,
reaches just 42.9%. These results demonstrate the considerable difficulty of
BrowseComp-ZH, where success demands not only effective retrieval strategies,
but also sophisticated reasoning and information reconciliation -- capabilities
that current models still struggle to master. Our dataset, construction
guidelines, and benchmark results have been publicly released at
https://github.com/PALIN2018/BrowseComp-ZH.

</details>


### [86] [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/abs/2504.19467)
*Jiageng Wu, Bowen Gu, Ren Zhou, Kevin Xie, Doug Snyder, Yixing Jiang, Valentina Carducci, Richard Wyss, Rishi J Desai, Emily Alsentzer, Leo Anthony Celi, Adam Rodman, Sebastian Schneeweiss, Jonathan H. Chen, Santiago Romero-Brufau, Kueiyu Joshua Lin, Jie Yang*

Main category: cs.CL

TL;DR: BRIDGE is a multilingual benchmark for evaluating LLMs in clinical contexts, addressing gaps in existing evaluations by using real-world EHR data. It tests 52 LLMs across 87 tasks, revealing performance variations and showing open-source models can match proprietary ones.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations in medicine lack real-world EHR complexity and generalizability. BRIDGE aims to fill this gap with a comprehensive, multilingual benchmark.

Method: BRIDGE comprises 87 tasks from real-world clinical data in nine languages. It evaluates 52 LLMs (e.g., GPT-4o, Gemini) under various inference strategies, totaling 13,572 experiments.

Result: Performance varies by model size, language, task, and specialty. Open-source LLMs can rival proprietary models, while older medically fine-tuned models often underperform.

Conclusion: BRIDGE provides a foundational resource for LLM evaluation in clinical text understanding, highlighting the potential of open-source models and the need for updated architectures.

Abstract: Large language models (LLMs) hold great promise for medical applications and
are evolving rapidly, with new models being released at an accelerated pace.
However, current evaluations of LLMs in clinical contexts remain limited. Most
existing benchmarks rely on medical exam-style questions or PubMed-derived
text, failing to capture the complexity of real-world electronic health record
(EHR) data. Others focus narrowly on specific application scenarios, limiting
their generalizability across broader clinical use. To address this gap, we
present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks
sourced from real-world clinical data sources across nine languages. We
systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,
GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total
of 13,572 experiments, our results reveal substantial performance variation
across model sizes, languages, natural language processing tasks, and clinical
specialties. Notably, we demonstrate that open-source LLMs can achieve
performance comparable to proprietary models, while medically fine-tuned LLMs
based on older architectures often underperform versus updated general-purpose
models. The BRIDGE and its corresponding leaderboard serve as a foundational
resource and a unique reference for the development and evaluation of new LLMs
in real-world clinical text understanding.
  The BRIDGE leaderboard:
https://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard

</details>


### [87] [Trace-of-Thought Prompting: Investigating Prompt-Based Knowledge Distillation Through Question Decomposition](https://arxiv.org/abs/2504.20946)
*Tyler McDonald, Ali Emami*

Main category: cs.CL

TL;DR: Trace-of-Thought Prompting distills reasoning from large to small LLMs, improving accuracy without extensive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To make high-performance LLMs more accessible by reducing reliance on large, proprietary models.

Method: Uses problem decomposition and human-in-the-loop interventions to distill reasoning capabilities.

Result: Student models achieved 113% accuracy gain on GSM8K and 21% on MATH, especially in smaller models.

Conclusion: This framework offers a pathway for low-resource models to perform like high-resource ones, reducing dependency on proprietary models.

Abstract: Knowledge distillation allows smaller neural networks to emulate the
performance of larger, teacher models with reduced computational demands.
Traditional methods for Large Language Models (LLMs) often necessitate
extensive fine-tuning, which limits their accessibility. To address this, we
introduce Trace-of-Thought Prompting, a novel framework designed to distill
critical reasoning capabilities from high-resource teacher models (over 8
billion parameters) to low-resource student models (up to 8 billion
parameters). This approach leverages problem decomposition to enhance
interpretability and facilitate human-in-the-loop interventions. Empirical
evaluations on the GSM8K and MATH datasets show that student models achieve
accuracy gains of up to 113% on GSM8K and 21% on MATH, with significant
improvements particularly notable in smaller models like Llama 2 and Zephyr.
Our results suggest a promising pathway for open-source, low-resource models to
eventually serve both as both students and teachers, potentially reducing our
reliance on high-resource, proprietary models.

</details>


### [88] [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)
*Makoto Sato*

Main category: cs.CL

TL;DR: The paper proposes a framework to compare human and LLM cognitive dynamics, finding LLMs lack human-like conceptual fusion.


<details>
  <summary>Details</summary>
Motivation: To understand intuitive human thinking by comparing cognitive behaviors of humans and LLMs under controlled conditions.

Method: A two-part framework: Transition-Inducing Prompt (TIP) and Transition Quantifying Prompt (TQP) to measure LLM responsiveness changes.

Result: LLMs showed no significant difference in responsiveness between semantically fused and non-fused prompts, unlike humans.

Conclusion: LLMs may not replicate human conceptual integration, highlighting a key difference in artificial vs. human intuition.

Abstract: What underlies intuitive human thinking? One approach to this question is to
compare the cognitive dynamics of humans and large language models (LLMs).
However, such a comparison requires a method to quantitatively analyze AI
cognitive behavior under controlled conditions. While anecdotal observations
suggest that certain prompts can dramatically change LLM behavior, these
observations have remained largely qualitative. Here, we propose a two-part
framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)
that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying
Prompt (TQP) that evaluates this change using a separate LLM. Through
controlled experiments, we examined how LLMs react to prompts embedding two
semantically distant concepts (e.g., mathematical aperiodicity and traditional
crafts)-either fused together or presented separately-by changing their
linguistic quality and affective tone. Whereas humans tend to experience
heightened engagement when such concepts are meaningfully blended producing a
novel concept-a form of conceptual fusion-current LLMs showed no significant
difference in responsiveness between semantically fused and non-fused prompts.
This suggests that LLMs may not yet replicate the conceptual integration
processes seen in human intuition. Our method enables fine-grained,
reproducible measurement of cognitive responsiveness, and may help illuminate
key differences in how intuition and conceptual leaps emerge in artificial
versus human minds.

</details>


### [89] [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
*Suhas BN, Dominik Mattioli, Saeed Abdullah, Rosa I. Arriaga, Chris W. Wiese, Andrew M. Sherrill*

Main category: cs.CL

TL;DR: Synthetic PE therapy dialogues for PTSD show promise for scalability and privacy but may miss key clinical fidelity markers.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns, data scarcity, and high annotation costs in healthcare by exploring synthetic data for clinical model training.

Method: Comparison of real and synthetic dialogues using linguistic, structural, and PE-specific metrics, including semantic modeling.

Result: Synthetic data matches structural features but struggles with clinical fidelity (e.g., distress monitoring).

Conclusion: Synthetic data can complement real-world datasets but requires fidelity-aware metrics for clinically significant evaluation.

Abstract: The growing adoption of synthetic data in healthcare is driven by privacy
concerns, limited access to real-world data, and the high cost of annotation.
This work explores the use of synthetic Prolonged Exposure (PE) therapeutic
conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable
alternative for training and evaluating clinical models. We systematically
compare real and synthetic dialogues using linguistic, structural, and
protocol-specific metrics, including turn-taking patterns and treatment
fidelity. We also introduce and evaluate PE-specific metrics derived from
linguistic analysis and semantic modeling, offering a novel framework for
assessing clinical fidelity beyond surface fluency. Our findings show that
although synthetic data holds promise for mitigating data scarcity and
protecting patient privacy, it can struggle to capture the subtle dynamics of
therapeutic interactions. Synthetic therapy dialogues closely match structural
features of real-world conversations (e.g., speaker switch ratio: 0.98 vs.
0.99); however, they may not adequately reflect key fidelity markers (e.g.,
distress monitoring). We highlight gaps in existing evaluation frameworks and
advocate for fidelity-aware metrics that go beyond surface fluency to uncover
clinically significant failures. Our findings clarify where synthetic data can
effectively complement real-world datasets -- and where critical limitations
remain.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [90] [Learning to Borrow Features for Improved Detection of Small Objects in Single-Shot Detectors](https://arxiv.org/abs/2505.00044)
*Richard Schmit*

Main category: cs.CV

TL;DR: A novel framework improves small object detection by borrowing features from larger instances, using three key blocks (FMB, FRB, FFB) to enhance shallow layers while maintaining real-time performance.


<details>
  <summary>Details</summary>
Motivation: Small object detection is challenging due to the trade-off between spatial resolution and semantic richness in feature maps.

Method: Proposes a framework with Feature Matching Block (FMB), Feature Representing Block (FRB), and Feature Fusion Block (FFB) to borrow and enhance features from larger instances.

Result: Significantly improves small object detection accuracy over baseline methods.

Conclusion: The framework offers a promising direction for robust object detection in complex environments.

Abstract: Detecting small objects remains a significant challenge in single-shot object
detectors due to the inherent trade-off between spatial resolution and semantic
richness in convolutional feature maps. To address this issue, we propose a
novel framework that enables small object representations to "borrow"
discriminative features from larger, semantically richer instances within the
same class. Our architecture introduces three key components: the Feature
Matching Block (FMB) to identify semantically similar descriptors across
layers, the Feature Representing Block (FRB) to generate enhanced shallow
features through weighted aggregation, and the Feature Fusion Block (FFB) to
refine feature maps by integrating original, borrowed, and context information.
Built upon the SSD framework, our method improves the descriptive capacity of
shallow layers while maintaining real-time detection performance. Experimental
results demonstrate that our approach significantly boosts small object
detection accuracy over baseline methods, offering a promising direction for
robust object detection in complex visual environments.

</details>


### [91] [Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design](https://arxiv.org/abs/2505.00134)
*Vasudev Sharma, Ahmed Alagha, Abdelhakim Khellaf, Vincent Quoc-Huy Trinh, Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: The paper investigates the impact of prompt engineering on vision-language models (VLMs) in computational pathology, finding that precise anatomical references in prompts significantly improve diagnostic accuracy, with CONCH outperforming other models.


<details>
  <summary>Details</summary>
Motivation: To address the sensitivity of VLMs to clinical data, task formulations, and prompt design in histopathology, particularly for diagnostic accuracy.

Method: A systematic study of three VLMs (Quilt-Net, Quilt-LLAVA, CONCH) on 3,507 WSIs, using structured ablative studies on cancer invasiveness and dysplasia status with varied prompt engineering.

Result: Prompt engineering greatly affects performance; CONCH excels with precise anatomical references. Anatomical context is crucial, and model complexity alone doesn't ensure better results.

Conclusion: Effective prompt engineering and domain-specific training are key for VLMs in pathology, with potential to enhance diagnostic accuracy when properly instructed.

Abstract: Vision-language models (VLMs) have gained significant attention in
computational pathology due to their multimodal learning capabilities that
enhance big-data analytics of giga-pixel whole slide image (WSI). However,
their sensitivity to large-scale clinical data, task formulations, and prompt
design remains an open question, particularly in terms of diagnostic accuracy.
In this paper, we present a systematic investigation and analysis of three
state of the art VLMs for histopathology, namely Quilt-Net, Quilt-LLAVA, and
CONCH, on an in-house digestive pathology dataset comprising 3,507 WSIs, each
in giga-pixel form, across distinct tissue types. Through a structured ablative
study on cancer invasiveness and dysplasia status, we develop a comprehensive
prompt engineering framework that systematically varies domain specificity,
anatomical precision, instructional framing, and output constraints. Our
findings demonstrate that prompt engineering significantly impacts model
performance, with the CONCH model achieving the highest accuracy when provided
with precise anatomical references. Additionally, we identify the critical
importance of anatomical context in histopathological image analysis, as
performance consistently degraded when reducing anatomical precision. We also
show that model complexity alone does not guarantee superior performance, as
effective domain alignment and domain-specific training are critical. These
results establish foundational guidelines for prompt engineering in
computational pathology and highlight the potential of VLMs to enhance
diagnostic accuracy when properly instructed with domain-appropriate prompts.

</details>


### [92] [Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes](https://arxiv.org/abs/2504.09948)
*Huijie Liu, Bingcan Wang, Jie Hu, Xiaoming Wei, Guoliang Kang*

Main category: cs.CV

TL;DR: Omni-Dish is a text-to-image model for Chinese dishes, addressing the gap in capturing culinary details. It uses a curated dataset, recaptioning, and coarse-to-fine training, plus a dish editing extension called Concept-Enhanced P2P.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models lack fidelity for culturally specific dishes like Chinese cuisine, necessitating a specialized solution.

Method: Developed a dish curation pipeline, recaption strategy, and coarse-to-fine training. Enhanced user input with a caption library and LLM. Extended capabilities with Concept-Enhanced P2P for editing.

Result: Superior performance in generating photorealistic and faithful Chinese dish images, validated through extensive experiments.

Conclusion: Omni-Dish effectively addresses domain-specific challenges in dish image generation and editing, setting a new benchmark.

Abstract: Dish images play a crucial role in the digital era, with the demand for
culturally distinctive dish images continuously increasing due to the
digitization of the food industry and e-commerce. In general cases, existing
text-to-image generation models excel in producing high-quality images;
however, they struggle to capture diverse characteristics and faithful details
of specific domains, particularly Chinese dishes. To address this limitation,
we propose Omni-Dish, the first text-to-image generation model specifically
tailored for Chinese dishes. We develop a comprehensive dish curation pipeline,
building the largest dish dataset to date. Additionally, we introduce a
recaption strategy and employ a coarse-to-fine training scheme to help the
model better learn fine-grained culinary nuances. During inference, we enhance
the user's textual input using a pre-constructed high-quality caption library
and a large language model, enabling more photorealistic and faithful image
generation. Furthermore, to extend our model's capability for dish editing
tasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish
editing dataset and train a specialized editing model. Extensive experiments
demonstrate the superiority of our methods.

</details>


### [93] [Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis](https://arxiv.org/abs/2505.00135)
*Michal Geyer, Omer Tov, Linyi Jin, Richard Tucker, Inbar Mosseri, Tali Dekel, Noah Snavely*

Main category: cs.CV

TL;DR: A method to convert text-to-video generators into video-to-stereo generators for 3D effects, bypassing intermediate steps like disparity estimation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of creating 3D videos due to limited 3D data and issues with existing methods involving specular or transparent objects.

Method: Directly synthesizes new viewpoints using a pre-trained video model's priors, avoiding intermediate steps like disparity estimation or warping.

Result: Effective 3D video generation in complex scenarios with diverse materials, avoiding artifacts from traditional methods.

Conclusion: The proposed approach simplifies 3D video generation and handles complex scenes better than multi-phase methods.

Abstract: The rising popularity of immersive visual experiences has increased interest
in stereoscopic 3D video generation. Despite significant advances in video
synthesis, creating 3D videos remains challenging due to the relative scarcity
of 3D video data. We propose a simple approach for transforming a text-to-video
generator into a video-to-stereo generator. Given an input video, our framework
automatically produces the video frames from a shifted viewpoint, enabling a
compelling 3D effect. Prior and concurrent approaches for this task typically
operate in multiple phases, first estimating video disparity or depth, then
warping the video accordingly to produce a second view, and finally inpainting
the disoccluded regions. This approach inherently fails when the scene involves
specular surfaces or transparent objects. In such cases, single-layer disparity
estimation is insufficient, resulting in artifacts and incorrect pixel shifts
during warping. Our work bypasses these restrictions by directly synthesizing
the new viewpoint, avoiding any intermediate steps. This is achieved by
leveraging a pre-trained video model's priors on geometry, object materials,
optics, and semantics, without relying on external geometry models or manually
disentangling geometry from the synthesis process. We demonstrate the
advantages of our approach in complex, real-world scenarios featuring diverse
object materials and compositions. See videos on
https://video-eye2eye.github.io

</details>


### [94] [Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models](https://arxiv.org/abs/2505.00150)
*Minh-Hao Van, Xintao Wu*

Main category: cs.CV

TL;DR: The paper introduces a method for detecting and mitigating hateful content in multimodal memes using Vision-Language Models (VLMs). It proposes a definition-guided prompting technique for detection and a framework, UnHateMeme, for transforming hateful memes into non-hateful forms.


<details>
  <summary>Details</summary>
Motivation: The misuse of memes for hate speech necessitates effective methods for detection and mitigation, leveraging VLMs to ensure safer online communication.

Method: Uses definition-guided prompts for hateful meme detection and the UnHateMeme framework to replace hateful components while maintaining coherence.

Result: VLMs achieve strong performance in detection, and UnHateMeme effectively converts hateful memes into non-hateful forms, validated by human criteria.

Conclusion: The work highlights VLMs' potential for fostering respectful online environments, with practical applications for meme moderation.

Abstract: The rapid evolution of social media has provided enhanced communication
channels for individuals to create online content, enabling them to express
their thoughts and opinions. Multimodal memes, often utilized for playful or
humorous expressions with visual and textual elements, are sometimes misused to
disseminate hate speech against individuals or groups. While the detection of
hateful memes is well-researched, developing effective methods to transform
hateful content in memes remains a significant challenge. Leveraging the
powerful generation and reasoning capabilities of Vision-Language Models
(VLMs), we address the tasks of detecting and mitigating hateful content. This
paper presents two key contributions: first, a definition-guided prompting
technique for detecting hateful memes, and second, a unified framework for
mitigating hateful content in memes, named UnHateMeme, which works by replacing
hateful textual and/or visual components. With our definition-guided prompts,
VLMs achieve impressive performance on hateful memes detection task.
Furthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a
strong capability to convert hateful memes into non-hateful forms that meet
human-level criteria for hate speech and maintain multimodal coherence between
image and text. Through empirical experiments, we show the effectiveness of
state-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the
proposed tasks, providing a comprehensive analysis of their respective
strengths and limitations for these tasks. This paper aims to shed light on
important applications of VLMs for ensuring safe and respectful online
environments.

</details>


### [95] [V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving](https://arxiv.org/abs/2505.00156)
*Jannik Lübberstedt, Esteban Rivera, Nico Uhlemann, Markus Lienkamp*

Main category: cs.CV

TL;DR: V3LMA enhances 3D scene understanding for autonomous driving by integrating LLMs with LVLMs, improving performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LVLMs lack 3D comprehension in autonomous driving, limiting their effectiveness in dynamic environments.

Method: V3LMA integrates LLMs with LVLMs, using textual descriptions from object detections and video inputs, and a preprocessing pipeline for 3D object data.

Result: Achieves 0.56 on LingoQA benchmark, improving situational awareness and decision-making.

Conclusion: V3LMA advances traffic scene interpretation, enabling safer autonomous driving systems.

Abstract: Large Vision Language Models (LVLMs) have shown strong capabilities in
understanding and analyzing visual scenes across various domains. However, in
the context of autonomous driving, their limited comprehension of 3D
environments restricts their effectiveness in achieving a complete and safe
understanding of dynamic surroundings. To address this, we introduce V3LMA, a
novel approach that enhances 3D scene understanding by integrating Large
Language Models (LLMs) with LVLMs. V3LMA leverages textual descriptions
generated from object detections and video inputs, significantly boosting
performance without requiring fine-tuning. Through a dedicated preprocessing
pipeline that extracts 3D object data, our method improves situational
awareness and decision-making in complex traffic scenarios, achieving a score
of 0.56 on the LingoQA benchmark. We further explore different fusion
strategies and token combinations with the goal of advancing the interpretation
of traffic scenes, ultimately enabling safer autonomous driving systems.

</details>


### [96] [Direct Motion Models for Assessing Generated Videos](https://arxiv.org/abs/2505.00209)
*Kelsey Allen, Carl Doersch, Guangyao Zhou, Mohammed Suhail, Danny Driess, Ignacio Rocco, Yulia Rubanova, Thomas Kipf, Mehdi S. M. Sajjadi, Kevin Murphy, Joao Carreira, Sjoerd van Steenkiste*

Main category: cs.CV

TL;DR: The paper introduces a new metric for evaluating video generative models, focusing on motion quality by using auto-encoded point tracks, outperforming existing methods like FVD.


<details>
  <summary>Details</summary>
Motivation: Existing metrics (e.g., FVD) fail to capture poor motion quality in generated videos, despite plausible frames.

Method: Develops a novel metric based on auto-encoding point tracks to evaluate motion and object interactions, enabling comparison of video distributions and single-video analysis.

Result: The new metric is more sensitive to temporal distortions, aligns better with human evaluations, and localizes inconsistencies in generated videos.

Conclusion: Point track-based evaluation improves motion assessment in generative videos, offering better sensitivity and interpretability than prior methods.

Abstract: A current limitation of video generative video models is that they generate
plausible looking frames, but poor motion -- an issue that is not well captured
by FVD and other popular methods for evaluating generated videos. Here we go
beyond FVD by developing a metric which better measures plausible object
interactions and motion. Our novel approach is based on auto-encoding point
tracks and yields motion features that can be used to not only compare
distributions of videos (as few as one generated and one ground truth, or as
many as two datasets), but also for evaluating motion of single videos. We show
that using point tracks instead of pixel reconstruction or action recognition
features results in a metric which is markedly more sensitive to temporal
distortions in synthetic data, and can predict human evaluations of temporal
consistency and realism in generated videos obtained from open-source models
better than a wide range of alternatives. We also show that by using a point
track representation, we can spatiotemporally localize generative video
inconsistencies, providing extra interpretability of generated video errors
relative to prior work. An overview of the results and link to the code can be
found on the project page: http://trajan-paper.github.io.

</details>


### [97] [Towards Robust and Generalizable Gerchberg Saxton based Physics Inspired Neural Networks for Computer Generated Holography: A Sensitivity Analysis Framework](https://arxiv.org/abs/2505.00220)
*Ankit Amrutkar, Björn Kampa, Volkmar Schulz, Johannes Stegmaier, Markus Rothermel, Dorit Merhof*

Main category: cs.CV

TL;DR: The paper presents a sensitivity analysis framework for Gerchberg-Saxton-based physics-inspired neural networks (GS-PINNs) in computer-generated holography (CGH), identifying key hyperparameters affecting performance and proposing a composite evaluation metric for benchmarking.


<details>
  <summary>Details</summary>
Motivation: The performance of GS-PINNs in CGH is limited by dependence on forward models (FMs) and their hyperparameters (FMHs), complicating generalization and benchmarking.

Method: A systematic sensitivity analysis using Saltelli's extension of Sobol's method to quantify FMH impacts on GS-PINN performance.

Result: SLM pixel-resolution is the most influential factor, followed by pixel-pitch, propagation distance, and wavelength. Free space propagation models outperform Fourier holography.

Conclusion: The study provides guidelines for FM selection, neural network architecture, and performance evaluation, advancing robust and interpretable CGH applications.

Abstract: Computer-generated holography (CGH) enables applications in holographic
augmented reality (AR), 3D displays, systems neuroscience, and optical
trapping. The fundamental challenge in CGH is solving the inverse problem of
phase retrieval from intensity measurements. Physics-inspired neural networks
(PINNs), especially Gerchberg-Saxton-based PINNs (GS-PINNs), have advanced
phase retrieval capabilities. However, their performance strongly depends on
forward models (FMs) and their hyperparameters (FMHs), limiting generalization,
complicating benchmarking, and hindering hardware optimization. We present a
systematic sensitivity analysis framework based on Saltelli's extension of
Sobol's method to quantify FMH impacts on GS-PINN performance. Our analysis
demonstrates that SLM pixel-resolution is the primary factor affecting neural
network sensitivity, followed by pixel-pitch, propagation distance, and
wavelength. Free space propagation forward models demonstrate superior neural
network performance compared to Fourier holography, providing enhanced
parameterization and generalization. We introduce a composite evaluation metric
combining performance consistency, generalization capability, and
hyperparameter perturbation resilience, establishing a unified benchmarking
standard across CGH configurations. Our research connects physics-inspired deep
learning theory with practical CGH implementations through concrete guidelines
for forward model selection, neural network architecture, and performance
evaluation. Our contributions advance the development of robust, interpretable,
and generalizable neural networks for diverse holographic applications,
supporting evidence-based decisions in CGH research and implementation.

</details>


### [98] [ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports](https://arxiv.org/abs/2505.00228)
*Xiaoman Zhang, Julián N. Acosta, Josh Miller, Ouwen Huang, Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ReXGradient-160K is the largest public chest X-ray dataset with 160,000 studies from 109,487 patients, designed for AI in medical imaging and automated report generation.


<details>
  <summary>Details</summary>
Motivation: To accelerate research in medical imaging AI and improve automated radiological analysis by providing a comprehensive, large-scale dataset.

Method: The dataset includes 160,000 chest X-ray studies with paired radiology reports, divided into training, validation, and test sets.

Result: A publicly available dataset with detailed radiology reports and multiple images per study, facilitating AI model development.

Conclusion: ReXGradient-160K aims to advance medical imaging AI and will be open-sourced for broader research use.

Abstract: We present ReXGradient-160K, representing the largest publicly available
chest X-ray dataset to date in terms of the number of patients. This dataset
contains 160,000 chest X-ray studies with paired radiological reports from
109,487 unique patients across 3 U.S. health systems (79 medical sites). This
comprehensive dataset includes multiple images per study and detailed radiology
reports, making it particularly valuable for the development and evaluation of
AI systems for medical imaging and automated report generation models. The
dataset is divided into training (140,000 studies), validation (10,000
studies), and public test (10,000 studies) sets, with an additional private
test set (10,000 studies) reserved for model evaluation on the ReXrank
benchmark. By providing this extensive dataset, we aim to accelerate research
in medical imaging AI and advance the state-of-the-art in automated
radiological analysis. Our dataset will be open-sourced at
https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.

</details>


### [99] [Empowering Agentic Video Analytics Systems with Video Language Models](https://arxiv.org/abs/2505.00254)
*Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu*

Main category: cs.CV

TL;DR: AVA is a VLM-powered system for open-ended video analytics, addressing context window limitations with Event Knowledge Graphs and agentic retrieval-generation. It outperforms existing systems on benchmarks and introduces a new benchmark, AVA-100.


<details>
  <summary>Details</summary>
Motivation: Existing AI-driven video analytics systems lack adaptability for open-ended scenarios due to predefined tasks and limited context windows in VLMs.

Method: AVA uses Event Knowledge Graphs for efficient indexing and an agentic retrieval-generation mechanism for handling complex queries.

Result: AVA achieves 62.3% and 64.1% accuracy on LVBench and VideoMME-Long, and 75.8% on AVA-100, surpassing existing systems.

Conclusion: AVA demonstrates superior performance in open-ended video analytics, validated by benchmarks, and introduces a new benchmark for ultra-long videos.

Abstract: AI-driven video analytics has become increasingly pivotal across diverse
domains. However, existing systems are often constrained to specific,
predefined tasks, limiting their adaptability in open-ended analytical
scenarios. The recent emergence of Video-Language Models (VLMs) as
transformative technologies offers significant potential for enabling
open-ended video understanding, reasoning, and analytics. Nevertheless, their
limited context windows present challenges when processing ultra-long video
content, which is prevalent in real-world applications. To address this, we
introduce AVA, a VLM-powered system designed for open-ended, advanced video
analytics. AVA incorporates two key innovations: (1) the near real-time
construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or
continuous video streams, and (2) an agentic retrieval-generation mechanism
that leverages EKGs to handle complex and diverse queries. Comprehensive
evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that
AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,
respectively, significantly surpassing existing VLM and video
Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video
analytics in ultra-long and open-world video scenarios, we introduce a new
benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours
in duration, along with 120 manually annotated, diverse, and complex
question-answer pairs. On AVA-100, AVA achieves top-tier performance with an
accuracy of 75.8%.

</details>


### [100] [Pack-PTQ: Advancing Post-training Quantization of Neural Networks by Pack-wise Reconstruction](https://arxiv.org/abs/2505.00259)
*Changjun Li, Runqing Jiang, Zhuo Song, Pengpeng Yu, Ye Zhang, Yulan Guo*

Main category: cs.CV

TL;DR: Pack-PTQ introduces a Hessian-guided adaptive packing mechanism and mixed-precision quantization to improve PTQ by preserving cross-block dependency and enhancing accuracy in low-bit cases.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods neglect cross-block dependency and suffer accuracy drops in low-bit scenarios, prompting the need for a more robust solution.

Method: The method involves partitioning blocks into non-overlapping packs for reconstruction and using mixed-precision quantization based on pack sensitivity.

Result: Experiments on 2D image and 3D point cloud tasks show Pack-PTQ outperforms state-of-the-art PTQ methods.

Conclusion: Pack-PTQ effectively addresses PTQ limitations by preserving dependencies and optimizing bit-width allocation, achieving superior performance.

Abstract: Post-training quantization (PTQ) has evolved as a prominent solution for
compressing complex models, which advocates a small calibration dataset and
avoids end-to-end retraining. However, most existing PTQ methods employ
block-wise reconstruction, which neglects cross-block dependency and exhibits a
notable accuracy drop in low-bit cases. To address these limitations, this
paper presents a novel PTQ method, dubbed Pack-PTQ. First, we design a
Hessian-guided adaptive packing mechanism to partition blocks into
non-overlapping packs, which serve as the base unit for reconstruction, thereby
preserving the cross-block dependency and enabling accurate quantization
parameters estimation. Second, based on the pack configuration, we propose a
mixed-precision quantization approach to assign varied bit-widths to packs
according to their distinct sensitivities, thereby further enhancing
performance. Extensive experiments on 2D image and 3D point cloud
classification tasks, using various network architectures, demonstrate the
superiority of our method over the state-of-the-art PTQ methods.

</details>


### [101] [AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care](https://arxiv.org/abs/2505.00275)
*Md Asaduzzaman Jabin, Hanqi Jiang, Yiwei Li, Patrick Kaggwa, Eugene Douglass, Juliet N. Sekandi, Tianming Liu*

Main category: cs.CV

TL;DR: AdCare-VLM, a Video-LLaVA-based multimodal model, improves medication adherence detection via visual question answering (VQA) using patient videos, outperforming existing VLMs.


<details>
  <summary>Details</summary>
Motivation: Chronic diseases require strict medication adherence, often hindered by behavioral, financial, and infrastructural challenges. AdCare-VLM addresses this by leveraging visual and linguistic data for adherence monitoring.

Method: The model uses a private dataset of 806 TB medication videos, fine-tuned for adherence pattern detection. It integrates visual features (e.g., face visibility, medication intake) with medical concepts in captions.

Result: AdCare-VLM outperforms PEFT-enabled VLMs like LLaVA-V1.5 and Chat-UniVi by 3.1% to 3.54% across configurations. Ablation studies and attention maps validate its effectiveness.

Conclusion: AdCare-VLM enhances adherence monitoring through multimodal VQA, offering improved accuracy and interpretability for chronic disease management.

Abstract: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,
epilepsy, and tuberculosis, necessitate rigorous adherence to medication to
avert disease progression, manage symptoms, and decrease mortality rates.
Adherence is frequently undermined by factors including patient behavior,
caregiver support, elevated medical costs, and insufficient healthcare
infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based
multimodal large vision language model (LVLM) aimed at visual question
answering (VQA) concerning medication adherence through patient videos. We
employ a private dataset comprising 806 custom-annotated tuberculosis (TB)
medication monitoring videos, which have been labeled by clinical experts, to
fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a
detailed medical adherence VQA dataset that encompasses positive, negative, and
ambiguous adherence cases. Our method identifies correlations between visual
features, such as the clear visibility of the patient's face, medication, water
intake, and the act of ingestion, and their associated medical concepts in
captions. This facilitates the integration of aligned visual-linguistic
representations and improves multimodal interactions. Experimental results
indicate that our method surpasses parameter-efficient fine-tuning (PEFT)
enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute
improvements ranging from 3.1% to 3.54% across pre-trained, regular, and
low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and
attention map visualizations substantiate our approach, enhancing
interpretability.

</details>


### [102] [Fine-grained spatial-temporal perception for gas leak segmentation](https://arxiv.org/abs/2505.00295)
*Xinlong Zhao, Shan Du*

Main category: cs.CV

TL;DR: Proposes FGSTP algorithm for gas leak segmentation, outperforming SOTA models on the manually labeled GasVid dataset.


<details>
  <summary>Details</summary>
Motivation: Gas leaks are hazardous but hard to detect due to concealed appearance and random shapes. Limited efficient methods exist.

Method: FGSTP captures motion clues via correlation volume, refines object features progressively, and uses a decoder for boundary segmentation.

Result: FGSTP excels in segmenting non-rigid gas leaks, producing the most accurate masks on GasVid.

Conclusion: FGSTP is effective for gas leak segmentation, validated by superior performance on GasVid.

Abstract: Gas leaks pose significant risks to human health and the environment. Despite
long-standing concerns, there are limited methods that can efficiently and
accurately detect and segment leaks due to their concealed appearance and
random shapes. In this paper, we propose a Fine-grained Spatial-Temporal
Perception (FGSTP) algorithm for gas leak segmentation. FGSTP captures critical
motion clues across frames and integrates them with refined object features in
an end-to-end network. Specifically, we first construct a correlation volume to
capture motion information between consecutive frames. Then, the fine-grained
perception progressively refines the object-level features using previous
outputs. Finally, a decoder is employed to optimize boundary segmentation.
Because there is no highly precise labeled dataset for gas leak segmentation,
we manually label a gas leak video dataset, GasVid. Experimental results on
GasVid demonstrate that our model excels in segmenting non-rigid objects such
as gas leaks, generating the most accurate mask compared to other
state-of-the-art (SOTA) models.

</details>


### [103] [AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality](https://arxiv.org/abs/2505.00308)
*Biling Wang, Austen Maniscalco, Ti Bai, Siqiu Wang, Michael Dohopolski, Mu-Han Lin, Chenyang Shen, Dan Nguyen, Junzhou Huang, Steve Jiang, Xinlei Wang*

Main category: cs.CV

TL;DR: A Deep Learning-based QA method for auto-contours in radiotherapy uses Bayesian Ordinal Classification and uncertainty thresholds to reduce manual workload and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency in Online Adaptive Radiotherapy by reducing reliance on ground truth contours and manual labeling for quality assessment.

Method: Developed a Bayesian Ordinal Classification model with calibrated uncertainty thresholds, validated under three data scenarios: no labels, limited labels, and extensive labels.

Result: Achieved over 90% accuracy with minimal manual labels (30) and calibration (34 subjects), accurately predicting 93% of auto-contour qualities in 98% of cases.

Conclusion: The method improves radiotherapy workflows by reducing manual reviews and ensuring reliable, fast clinical decisions through uncertainty quantification.

Abstract: Purpose: This study presents a Deep Learning (DL)-based quality assessment
(QA) approach for evaluating auto-generated contours (auto-contours) in
radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging
Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds,
the method enables confident QA predictions without relying on ground truth
contours or extensive manual labeling. Methods: We developed a BOC model to
classify auto-contour quality and quantify prediction uncertainty. A
calibration step was used to optimize uncertainty thresholds that meet clinical
accuracy needs. The method was validated under three data scenarios: no manual
labels, limited labels, and extensive labels. For rectum contours in prostate
cancer, we applied geometric surrogate labels when manual labels were absent,
transfer learning when limited, and direct supervision when ample labels were
available. Results: The BOC model delivered robust performance across all
scenarios. Fine-tuning with just 30 manual labels and calibrating with 34
subjects yielded over 90% accuracy on test data. Using the calibrated
threshold, over 93% of the auto-contours' qualities were accurately predicted
in over 98% of cases, reducing unnecessary manual reviews and highlighting
cases needing correction. Conclusion: The proposed QA model enhances contouring
efficiency in OART by reducing manual workload and enabling fast, informed
clinical decisions. Through uncertainty quantification, it ensures safer, more
reliable radiotherapy workflows.

</details>


### [104] [AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection](https://arxiv.org/abs/2505.00312)
*Muhammad Salman, Iqra Tariq, Mishal Zulfiqar, Muqadas Jalal, Sami Aujla, Sumbal Fatima*

Main category: cs.CV

TL;DR: A novel two-tier ensemble framework for deepfake detection combines multiple deep learning architectures with dynamic weighting, achieving state-of-the-art performance on intra- and cross-dataset evaluations.


<details>
  <summary>Details</summary>
Motivation: The rise of synthetic media poses risks to digital identity and trust, necessitating improved deepfake detection methods that perform consistently across diverse datasets and manipulation types.

Method: A two-tier ensemble framework hierarchically combines Xception, Res2Net101, and EfficientNet-B7 architectures, using multiple instances with different initializations and a learnable weighting mechanism.

Result: Achieved high AUC (up to 100%) and F1 scores (up to 99.95%) on intra-dataset tests and robust cross-dataset generalization (AUC up to 88.20%, F1 up to 93.16%).

Conclusion: The proposed framework enhances deepfake detection accuracy and generalization, addressing challenges in diverse dataset performance.

Abstract: Deepfake detection has become increasingly important due to the rise of
synthetic media, which poses significant risks to digital identity and cyber
presence for security and trust. While multiple approaches have improved
detection accuracy, challenges remain in achieving consistent performance
across diverse datasets and manipulation types. In response, we propose a novel
two-tier ensemble framework for deepfake detection based on deep learning that
hierarchically combines multiple instances of three state-of-the-art
architectures: Xception, Res2Net101, and EfficientNet-B7. Our framework employs
a unique approach where each architecture is instantiated three times with
different initializations to enhance model diversity, followed by a learnable
weighting mechanism that dynamically combines their predictions. Unlike
traditional fixed-weight ensembles, our first-tier averages predictions within
each architecture family to reduce model variance, while the second tier learns
optimal contribution weights through backpropagation, automatically adjusting
each architecture's influence based on their detection reliability. Our
experiments achieved state-of-the-art intra-dataset performance with AUC scores
of 99.22% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.06% (FF++) and
99.94% (CelebDF-v2) without augmentation. With augmentation, we achieve AUC
scores of 99.47% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.43%
(FF++) and 99.95% (CelebDF-v2). The framework demonstrates robust cross-dataset
generalization, achieving AUC scores of 88.20% and 72.52%, and F1 scores of
93.16% and 80.62% in cross-dataset evaluations.

</details>


### [105] [Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets](https://arxiv.org/abs/2505.00584)
*Mathis Morales, Golnaz Habibi*

Main category: cs.CV

TL;DR: A synthetic data augmentation pipeline for camera-radar AV datasets is proposed to improve robustness against sensor failures, with a baseline noise recognition model achieving 54.4% accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of focus on robustness in object detection and tracking pipelines, particularly for sensor failures.

Method: Creating a realistic synthetic data augmentation pipeline to simulate sensor failures and data deterioration, and training a lightweight Noise Recognition neural network.

Result: The baseline model achieved 54.4% recognition accuracy on 11 categories across 10086 images and 2145 radar point-clouds.

Conclusion: The synthetic data augmentation pipeline and baseline model show promise for improving robustness in AV object detection and tracking.

Abstract: Detecting and tracking objects is a crucial component of any autonomous
navigation method. For the past decades, object detection has yielded promising
results using neural networks on various datasets. While many methods focus on
performance metrics, few projects focus on improving the robustness of these
detection and tracking pipelines, notably to sensor failures. In this paper we
attempt to address this issue by creating a realistic synthetic data
augmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Our
goal is to accurately simulate sensor failures and data deterioration due to
real-world interferences. We also present our results of a baseline lightweight
Noise Recognition neural network trained and tested on our augmented dataset,
reaching an overall recognition accuracy of 54.4\% on 11 categories across
10086 images and 2145 radar point-clouds.

</details>


### [106] [Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution](https://arxiv.org/abs/2505.00334)
*Luigi Sigillo, Christian Bianchi, Danilo Comminiello*

Main category: cs.CV

TL;DR: ResQu is a novel SR framework combining quaternion wavelet preprocessing with latent diffusion models, improving perceptual quality and structural fidelity in image super-resolution.


<details>
  <summary>Details</summary>
Motivation: Enhancing super-resolution quality is critical for applications like medical imaging and satellite analysis, but balancing perceptual and structural fidelity remains challenging.

Method: ResQu integrates quaternion wavelet preprocessing with latent diffusion models, using a quaternion wavelet- and time-aware encoder and leveraging generative priors from foundation models.

Result: The method outperforms existing approaches in perceptual quality and standard metrics on domain-specific datasets.

Conclusion: ResQu advances super-resolution by effectively combining wavelet embeddings and diffusion models, achieving high-quality reconstructions.

Abstract: Image Super-Resolution is a fundamental problem in computer vision with broad
applications spacing from medical imaging to satellite analysis. The ability to
reconstruct high-resolution images from low-resolution inputs is crucial for
enhancing downstream tasks such as object detection and segmentation. While
deep learning has significantly advanced SR, achieving high-quality
reconstructions with fine-grained details and realistic textures remains
challenging, particularly at high upscaling factors. Recent approaches
leveraging diffusion models have demonstrated promising results, yet they often
struggle to balance perceptual quality with structural fidelity. In this work,
we introduce ResQu a novel SR framework that integrates a quaternion wavelet
preprocessing framework with latent diffusion models, incorporating a new
quaternion wavelet- and time-aware encoder. Unlike prior methods that simply
apply wavelet transforms within diffusion models, our approach enhances the
conditioning process by exploiting quaternion wavelet embeddings, which are
dynamically integrated at different stages of denoising. Furthermore, we also
leverage the generative priors of foundation models such as Stable Diffusion.
Extensive experiments on domain-specific datasets demonstrate that our method
achieves outstanding SR results, outperforming in many cases existing
approaches in perceptual quality and standard evaluation metrics. The code will
be available after the revision process.

</details>


### [107] [Efficient Neural Video Representation with Temporally Coherent Modulation](https://arxiv.org/abs/2505.00335)
*Seungjun Shin, Suji Kim, Dokwan Oh*

Main category: cs.CV

TL;DR: NVTM proposes a temporally coherent modulation framework for efficient and fast video representation, outperforming grid-type methods in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and redundancy in grid-type parametric encoding for video applications, NVTM aims to improve parameter efficiency and encoding speed.

Method: NVTM decomposes spatio-temporal video data into 2D grids with flow information, enabling rapid learning and efficient parameter use.

Result: NVTM achieves 3x faster encoding than NeRV-style methods, with 1.54dB/0.019 PSNR/LPIPS improvements on UVG and 1.84dB/0.013 on MCL-JCV, using fewer parameters.

Conclusion: NVTM demonstrates superior performance in video tasks, matching compression standards and excelling in super-resolution, interpolation, and inpainting.

Abstract: Implicit neural representations (INR) has found successful applications
across diverse domains. To employ INR in real-life, it is important to speed up
training. In the field of INR for video applications, the state-of-the-art
approach employs grid-type parametric encoding and successfully achieves a
faster encoding speed in comparison to its predecessors. However, the grid
usage, which does not consider the video's dynamic nature, leads to redundant
use of trainable parameters. As a result, it has significantly lower parameter
efficiency and higher bitrate compared to NeRV-style methods that do not use a
parametric encoding. To address the problem, we propose Neural Video
representation with Temporally coherent Modulation (NVTM), a novel framework
that can capture dynamic characteristics of video. By decomposing the
spatio-temporal 3D video data into a set of 2D grids with flow information,
NVTM enables learning video representation rapidly and uses parameter
efficiently. Our framework enables to process temporally corresponding pixels
at once, resulting in the fastest encoding speed for a reasonable video
quality, especially when compared to the NeRV-style method, with a speed
increase of over 3 times. Also, it remarks an average of 1.54dB/0.019
improvements in PSNR/LPIPS on UVG (Dynamic) (even with 10% fewer parameters)
and an average of 1.84dB/0.013 improvements in PSNR/LPIPS on MCL-JCV (Dynamic),
compared to previous grid-type works. By expanding this to compression tasks,
we demonstrate comparable performance to video compression standards (H.264,
HEVC) and recent INR approaches for video compression. Additionally, we perform
extensive experiments demonstrating the superior performance of our algorithm
across diverse tasks, encompassing super resolution, frame interpolation and
video inpainting. Project page is https://sujiikim.github.io/NVTM/.

</details>


### [108] [Automated segmenta-on of pediatric neuroblastoma on multi-modal MRI: Results of the SPPIN challenge at MICCAI 2023](https://arxiv.org/abs/2505.00369)
*M. A. D. Buser, D. C. Simons, M. Fitski, M. H. W. A. Wijnen, A. S. Littooij, A. H. ter Brugge, I. N. Vos, M. H. A. Janse, M. de Boer, R. ter Maat, J. Sato, S. Kido, S. Kondo, S. Kasai, M. Wodzinski, H. Muller, J. Ye, J. He, Y. Kirchhoff, M. R. Rokkus, G. Haokai, S. Zitong, M. Fernández-Patón, D. Veiga-Canuto, D. G. Ellis, M. R. Aizenberg, B. H. M. van der Velden, H. Kuijf, A. De Luca, A. F. W. van der Steeg*

Main category: cs.CV

TL;DR: The SPPIN challenge aimed to benchmark automatic segmentation of neuroblastoma on MRI scans, with the top team achieving high scores using a pretrained network, though challenges remain for small, treated tumors.


<details>
  <summary>Details</summary>
Motivation: To improve surgical planning for pediatric neuroblastoma by developing reliable, automatic MRI segmentation methods.

Method: Organized a challenge with training and test phases using MRI scans, ranking teams based on Dice score, HD95, and VS metrics.

Result: Top team achieved median Dice 0.82, HD95 7.69 mm, and VS 0.91, but struggled with small, treated tumors (Dice 0.59).

Conclusion: Pretraining helps, but more reliable methods are needed for clinical use in pediatric neuroblastoma segmentation.

Abstract: Surgery plays an important role within the treatment for neuroblastoma, a
common pediatric cancer. This requires careful planning, often via magnetic
resonance imaging (MRI)-based anatomical 3D models. However, creating these
models is often time-consuming and user dependent. We organized the Surgical
Planning in Pediatric Neuroblastoma (SPPIN) challenge, to stimulate
developments on this topic, and set a benchmark for fully automatic
segmentation of neuroblastoma on multi-model MRI. The challenge started with a
training phase, where teams received 78 sets of MRI scans from 34 patients,
consisting of both diagnostic and post-chemotherapy MRI scans. The final test
phase, consisting of 18 MRI sets from 9 patients, determined the ranking of the
teams. Ranking was based on the Dice similarity coefficient (Dice score), the
95th percentile of the Hausdorff distance (HD95) and the volumetric similarity
(VS). The SPPIN challenge was hosted at MICCAI 2023. The final leaderboard
consisted of 9 teams. The highest-ranking team achieved a median Dice score
0.82, a median HD95 of 7.69 mm and a VS of 0.91, utilizing a large, pretrained
network called STU-Net. A significant difference for the segmentation results
between diagnostic and post-chemotherapy MRI scans was observed (Dice = 0.89 vs
Dice = 0.59, P = 0.01) for the highest-ranking team. SPPIN is the first medical
segmentation challenge in extracranial pediatric oncology. The highest-ranking
team used a large pre-trained network, suggesting that pretraining can be of
use in small, heterogenous datasets. Although the results of the
highest-ranking team were high for most patients, segmentation especially in
small, pre-treated tumors were insufficient. Therefore, more reliable
segmentation methods are needed to create clinically applicable models to aid
surgical planning in pediatric neuroblastoma.

</details>


### [109] [Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation](https://arxiv.org/abs/2505.00378)
*Feng Xue, Wenzhuang Xu, Guofeng Zhong, Anlong Minga, Nicu Sebe*

Main category: cs.CV

TL;DR: Cues3D is a NeRF-based method for open-vocabulary 3D panoptic segmentation, achieving globally consistent object IDs without cross-view supervision, outperforming 2D and 2D-3D methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on high-fidelity 3D point clouds or explicit cross-view associations, which are limiting. Cues3D leverages NeRF's implicit 3D geometry for consistency.

Method: A three-phase training framework (initialization-disambiguation-refinement) and instance disambiguation method are used to ensure unique 3D instance IDs.

Result: Cues3D outperforms 2D-based methods and competes with 2D-3D merging methods, excelling with additional 3D point clouds.

Conclusion: Cues3D provides a compact, effective solution for 3D segmentation, leveraging NeRF's inherent geometry for consistency and outperforming existing approaches.

Abstract: Open-vocabulary 3D panoptic segmentation has recently emerged as a
significant trend. Top-performing methods currently integrate 2D segmentation
with geometry-aware 3D primitives. However, the advantage would be lost without
high-fidelity 3D point clouds, such as methods based on Neural Radiance Field
(NeRF). These methods are limited by the insufficient capacity to maintain
consistency across partial observations. To address this, recent works have
utilized contrastive loss or cross-view association pre-processing for view
consensus. In contrast to them, we present Cues3D, a compact approach that
relies solely on NeRF instead of pre-associations. The core idea is that NeRF's
implicit 3D field inherently establishes a globally consistent geometry,
enabling effective object distinction without explicit cross-view supervision.
We propose a three-phase training framework for NeRF,
initialization-disambiguation-refinement, whereby the instance IDs are
corrected using the initially-learned knowledge. Additionally, an instance
disambiguation method is proposed to match NeRF-rendered 3D masks and ensure
globally unique 3D instance identities. With the aid of Cues3D, we obtain
highly consistent and unique 3D instance ID for each object across views with a
balanced version of NeRF. Our experiments are conducted on ScanNet v2,
ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and
semantic segmentation tasks. Cues3D outperforms other 2D image-based methods
and competes with the latest 2D-3D merging based methods, while even surpassing
them when using additional 3D point clouds. The code link could be found in the
appendix and will be released on
\href{https://github.com/mRobotit/Cues3D}{github}

</details>


### [110] [MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling](https://arxiv.org/abs/2502.20824)
*Fadeel Sher Khan, Joshua Ebenezer, Hamid Sheikh, Seok-Jun Lee*

Main category: cs.CV

TL;DR: The paper introduces a synthetic data engine and MFSR-GAN to improve multi-frame super-resolution (MFSR) for smartphone cameras, addressing noise and motion issues in real-world handheld burst images.


<details>
  <summary>Details</summary>
Motivation: Smartphone cameras' small sensors and compact optics limit resolution and introduce distortions. Existing MFSR methods lack datasets capturing real-world noise and motion patterns.

Method: Proposes a synthetic data engine using multi-exposure static images to create LR-HR training pairs, and MFSR-GAN, a multi-scale RAW-to-RGB network emphasizing a 'base frame' to reduce artifacts.

Result: MFSR-GAN trained with the synthetic engine produces sharper, more realistic reconstructions than prior methods on both synthetic and real data.

Conclusion: The synthetic data engine and MFSR-GAN effectively address real-world MFSR challenges, outperforming existing approaches.

Abstract: Smartphone cameras have become ubiquitous imaging tools, yet their small
sensors and compact optics often limit spatial resolution and introduce
distortions. Combining information from multiple low-resolution (LR) frames to
produce a high-resolution (HR) image has been explored to overcome the inherent
limitations of smartphone cameras. Despite the promise of multi-frame
super-resolution (MFSR), current approaches are hindered by datasets that fail
to capture the characteristic noise and motion patterns found in real-world
handheld burst images. In this work, we address this gap by introducing a novel
synthetic data engine that uses multi-exposure static images to synthesize
LR-HR training pairs while preserving sensor-specific noise characteristics and
image motion found during handheld burst photography. We also propose MFSR-GAN:
a multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches,
MFSR-GAN emphasizes a "base frame" throughout its architecture to mitigate
artifacts. Experimental results on both synthetic and real data demonstrates
that MFSR-GAN trained with our synthetic engine yields sharper, more realistic
reconstructions than existing methods for real-world MFSR.

</details>


### [111] [The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks](https://arxiv.org/abs/2505.00380)
*Anjith George, Sebastien Marcel*

Main category: cs.CV

TL;DR: The paper evaluates the vulnerability of NIR-VIS cross-spectral face recognition systems to presentation attacks, finding them reliable but still vulnerable.


<details>
  <summary>Details</summary>
Motivation: To systematically study the robustness of NIR-VIS face recognition systems against presentation attacks, which has not been thoroughly explored.

Method: Conducts a comprehensive evaluation of NIR-VIS cross-spectral face recognition systems.

Result: The systems are reliable but vulnerable to specific attacks.

Conclusion: Further research is needed to enhance the security of NIR-VIS face recognition systems against presentation attacks.

Abstract: Cross-spectral face recognition systems are designed to enhance the
performance of facial recognition systems by enabling cross-modal matching
under challenging operational conditions. A particularly relevant application
is the matching of near-infrared (NIR) images to visible-spectrum (VIS) images,
enabling the verification of individuals by comparing NIR facial captures
acquired with VIS reference images. The use of NIR imaging offers several
advantages, including greater robustness to illumination variations, better
visibility through glasses and glare, and greater resistance to presentation
attacks. Despite these claimed benefits, the robustness of NIR-based systems
against presentation attacks has not been systematically studied in the
literature. In this work, we conduct a comprehensive evaluation into the
vulnerability of NIR-VIS cross-spectral face recognition systems to
presentation attacks. Our empirical findings indicate that, although these
systems exhibit a certain degree of reliability, they remain vulnerable to
specific attacks, emphasizing the need for further research in this area.

</details>


### [112] [SOTA: Spike-Navigated Optimal TrAnsport Saliency Region Detection in Composite-bias Videos](https://arxiv.org/abs/2505.00394)
*Wenxuan Liu, Yao Deng, Kang Chen, Xian Zhong, Zhaofei Yu, Tiejun Huang*

Main category: cs.CV

TL;DR: SOTA improves saliency detection in spike cameras by addressing noise and bias with micro and global debiasing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail due to motion blur, occlusions, and noise in spike cameras, leading to biased saliency maps.

Method: Proposes SOTA with Spike-based Micro-debias (SM) for frame variations and Spike-based Global-debias (SG) for consistency.

Result: Outperforms existing methods by eliminating noise bias in real and synthetic datasets.

Conclusion: SOTA effectively leverages spike cameras' strengths while mitigating biases, with code and dataset publicly available.

Abstract: Existing saliency detection methods struggle in real-world scenarios due to
motion blur and occlusions. In contrast, spike cameras, with their high
temporal resolution, significantly enhance visual saliency maps. However, the
composite noise inherent to spike camera imaging introduces discontinuities in
saliency detection. Low-quality samples further distort model predictions,
leading to saliency bias. To address these challenges, we propose
Spike-navigated Optimal TrAnsport Saliency Region Detection (SOTA), a framework
that leverages the strengths of spike cameras while mitigating biases in both
spatial and temporal dimensions. Our method introduces Spike-based Micro-debias
(SM) to capture subtle frame-to-frame variations and preserve critical details,
even under minimal scene or lighting changes. Additionally, Spike-based
Global-debias (SG) refines predictions by reducing inconsistencies across
diverse conditions. Extensive experiments on real and synthetic datasets
demonstrate that SOTA outperforms existing methods by eliminating composite
noise bias. Our code and dataset will be released at
https://github.com/lwxfight/sota.

</details>


### [113] [Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos](https://arxiv.org/abs/2505.00421)
*Xia Yuan, Hai Yuan, Wenyi Ge, Ying Fu, Xi Wu, Guanyu Xing*

Main category: cs.CV

TL;DR: A novel real-time framework for animatable 3D human avatar reconstruction from monocular videos using 2D Gaussian Splatting and a Rotation Compensation Network, outperforming current methods in quality and robustness.


<details>
  <summary>Details</summary>
Motivation: To address challenges in capturing fine geometric details and maintaining animation stability in 3D human avatar reconstruction from monocular videos.

Method: Proposes a framework combining 2D Gaussian Splatting (2DGS) and global SMPL pose parameters, along with a Rotation Compensation Network (RCN) for handling non-rigid deformations.

Result: Successfully reconstructs realistic, animatable avatars with fine details and stable pose transitions, outperforming state-of-the-art methods.

Conclusion: The framework achieves high-quality, robust animation of human avatars, making it practical for applications like gaming and AR.

Abstract: High-quality, animatable 3D human avatar reconstruction from monocular videos
offers significant potential for reducing reliance on complex hardware, making
it highly practical for applications in game development, augmented reality,
and social media. However, existing methods still face substantial challenges
in capturing fine geometric details and maintaining animation stability,
particularly under dynamic or complex poses. To address these issues, we
propose a novel real-time framework for animatable human avatar reconstruction
based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose
parameters, our framework not only aligns positional and rotational
discrepancies but also enables robust and natural pose-driven animation of the
reconstructed avatars. Furthermore, we introduce a Rotation Compensation
Network (RCN) that learns rotation residuals by integrating local geometric
features with global pose parameters. This network significantly improves the
handling of non-rigid deformations and ensures smooth, artifact-free pose
transitions during animation. Experimental results demonstrate that our method
successfully reconstructs realistic and highly animatable human avatars from
monocular videos, effectively preserving fine-grained details while ensuring
stable and natural pose variation. Our approach surpasses current
state-of-the-art methods in both reconstruction quality and animation
robustness on public benchmarks.

</details>


### [114] [Leveraging Pretrained Diffusion Models for Zero-Shot Part Assembly](https://arxiv.org/abs/2505.00426)
*Ruiyuan Zhang, Qi Wang, Jiaxiang Liu, Yu Zhang, Yuchi Huo, Chao Wu*

Main category: cs.CV

TL;DR: A zero-shot 3D part assembly method using pre-trained point cloud diffusion models to guide part manipulation, outperforming supervised methods without labeled data.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of traditional supervised methods due to high data collection costs and variability in real-world shapes.

Method: Utilizes pre-trained point cloud diffusion models as discriminators, transforming the process into an Iterative Closest Point (ICP) process, with a pushing-away strategy for overlap parts.

Result: Outperforms supervised methods in experiments, demonstrating robustness and effectiveness.

Conclusion: The proposed zero-shot method is practical for large-scale applications, surpassing supervised learning in performance.

Abstract: 3D part assembly aims to understand part relationships and predict their
6-DoF poses to construct realistic 3D shapes, addressing the growing demand for
autonomous assembly, which is crucial for robots. Existing methods mainly
estimate the transformation of each part by training neural networks under
supervision, which requires a substantial quantity of manually labeled data.
However, the high cost of data collection and the immense variability of
real-world shapes and parts make traditional methods impractical for
large-scale applications. In this paper, we propose first a zero-shot part
assembly method that utilizes pre-trained point cloud diffusion models as
discriminators in the assembly process, guiding the manipulation of parts to
form realistic shapes. Specifically, we theoretically demonstrate that
utilizing a diffusion model for zero-shot part assembly can be transformed into
an Iterative Closest Point (ICP) process. Then, we propose a novel pushing-away
strategy to address the overlap parts, thereby further enhancing the robustness
of the method. To verify our work, we conduct extensive experiments and
quantitative comparisons to several strong baseline methods, demonstrating the
effectiveness of the proposed approach, which even surpasses the supervised
learning method. The code has been released on
https://github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.

</details>


### [115] [ClearLines - Camera Calibration from Straight Lines](https://arxiv.org/abs/2505.00452)
*Gregory Schroeder, Mohamed Sabry, Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: The paper introduces "ClearLines," a dataset for straight 3D line detection, addressing challenges in real-world outdoor scenarios.


<details>
  <summary>Details</summary>
Motivation: Practical calibration from straight lines in outdoor environments is difficult due to cluttered scenes, interrupted reprojections, and lighting variations. The lack of a dedicated dataset hinders algorithm development.

Method: The study presents the "ClearLines" dataset and details its creation process to guide algorithm development.

Result: The dataset provides practical insights for refining straight 3D line detection algorithms.

Conclusion: "ClearLines" serves as a valuable resource for advancing straight line detection in challenging outdoor scenarios.

Abstract: The problem of calibration from straight lines is fundamental in geometric
computer vision, with well-established theoretical foundations. However, its
practical applicability remains limited, particularly in real-world outdoor
scenarios. These environments pose significant challenges due to diverse and
cluttered scenes, interrupted reprojections of straight 3D lines, and varying
lighting conditions, making the task notoriously difficult. Furthermore, the
field lacks a dedicated dataset encouraging the development of respective
detection algorithms. In this study, we present a small dataset named
"ClearLines", and by detailing its creation process, provide practical insights
that can serve as a guide for developing and refining straight 3D line
detection algorithms.

</details>


### [116] [JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers](https://arxiv.org/abs/2505.00482)
*Kwon Byung-Ki, Qi Dai, Lee Hyoseok, Chong Luo, Tae-Hyun Oh*

Main category: cs.CV

TL;DR: JointDiT is a diffusion transformer that jointly models RGB and depth distributions, achieving high-fidelity results with adaptive scheduling and unbalanced timestep sampling.


<details>
  <summary>Details</summary>
Motivation: To improve joint generation of RGB and depth by leveraging diffusion transformers, addressing the need for geometrically accurate depth maps alongside high-quality images.

Method: Uses adaptive scheduling weights and unbalanced timestep sampling to train across noise levels for each modality, enabling flexible generation tasks.

Result: Outperforms in joint generation and matches performance in depth estimation and depth-conditioned image generation.

Conclusion: Joint distribution modeling can replace conditional generation, offering a unified approach for diverse tasks.

Abstract: We present JointDiT, a diffusion transformer that models the joint
distribution of RGB and depth. By leveraging the architectural benefit and
outstanding image prior of the state-of-the-art diffusion transformer, JointDiT
not only generates high-fidelity images but also produces geometrically
plausible and accurate depth maps. This solid joint distribution modeling is
achieved through two simple yet effective techniques that we propose, i.e.,
adaptive scheduling weights, which depend on the noise levels of each modality,
and the unbalanced timestep sampling strategy. With these techniques, we train
our model across all noise levels for each modality, enabling JointDiT to
naturally handle various combinatorial generation tasks, including joint
generation, depth estimation, and depth-conditioned image generation by simply
controlling the timestep of each branch. JointDiT demonstrates outstanding
joint generation performance. Furthermore, it achieves comparable results in
depth estimation and depth-conditioned image generation, suggesting that joint
distribution modeling can serve as a replaceable alternative to conditional
generation. The project page is available at
https://byungki-k.github.io/JointDiT/.

</details>


### [117] [KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution](https://arxiv.org/abs/2505.00497)
*Antoni Bigata, Rodrigo Mira, Stella Bounareli, Michał Stypułkowski, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic*

Main category: cs.CV

TL;DR: KeySync is a two-stage framework addressing lip synchronization challenges like temporal consistency, expression leakage, and occlusions, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing lip synchronization methods neglect issues like expression leakage and occlusions, impacting real-world applications like automated dubbing.

Method: KeySync uses a two-stage framework with a masking strategy to handle leakage and occlusions.

Result: KeySync improves visual quality, reduces expression leakage, and handles occlusions effectively, validated by ablation studies.

Conclusion: KeySync outperforms existing methods in lip synchronization, offering practical solutions for real-world applications.

Abstract: Lip synchronization, known as the task of aligning lip movements in an
existing video with new input audio, is typically framed as a simpler variant
of audio-driven facial animation. However, as well as suffering from the usual
issues in talking head generation (e.g., temporal consistency), lip
synchronization presents significant new challenges such as expression leakage
from the input video and facial occlusions, which can severely impact
real-world applications like automated dubbing, but are often neglected in
existing works. To address these shortcomings, we present KeySync, a two-stage
framework that succeeds in solving the issue of temporal consistency, while
also incorporating solutions for leakage and occlusions using a carefully
designed masking strategy. We show that KeySync achieves state-of-the-art
results in lip reconstruction and cross-synchronization, improving visual
quality and reducing expression leakage according to LipLeak, our novel leakage
metric. Furthermore, we demonstrate the effectiveness of our new masking
approach in handling occlusions and validate our architectural choices through
several ablation studies. Code and model weights can be found at
https://antonibigata.github.io/KeySync.

</details>


### [118] [Towards Scalable Human-aligned Benchmark for Text-guided Image Editing](https://arxiv.org/abs/2505.00502)
*Suho Ryu, Kihyun Kim, Eugene Baek, Dongsoo Shin, Joonseok Lee*

Main category: cs.CV

TL;DR: The paper introduces HATIE, a Human-Aligned benchmark for Text-guided Image Editing, addressing the lack of standardized evaluation methods by providing a large-scale, automated, and comprehensive evaluation pipeline aligned with human perception.


<details>
  <summary>Details</summary>
Motivation: The subjective nature of text-guided image editing lacks a standardized evaluation method, leading to reliance on manual user studies. HATIE aims to fill this gap.

Method: HATIE offers a large-scale benchmark set and an automated evaluation pipeline combining multiple scores to align with human perception.

Result: Empirical verification shows HATIE's evaluation is human-aligned, and benchmark results provide insights into state-of-the-art models.

Conclusion: HATIE provides a reliable, automated, and human-aligned evaluation framework for text-guided image editing, advancing research in the field.

Abstract: A variety of text-guided image editing models have been proposed recently.
However, there is no widely-accepted standard evaluation method mainly due to
the subjective nature of the task, letting researchers rely on manual user
study. To address this, we introduce a novel Human-Aligned benchmark for
Text-guided Image Editing (HATIE). Providing a large-scale benchmark set
covering a wide range of editing tasks, it allows reliable evaluation, not
limited to specific easy-to-evaluate cases. Also, HATIE provides a
fully-automated and omnidirectional evaluation pipeline. Particularly, we
combine multiple scores measuring various aspects of editing so as to align
with human perception. We empirically verify that the evaluation of HATIE is
indeed human-aligned in various aspects, and provide benchmark results on
several state-of-the-art models to provide deeper insights on their
performance.

</details>


### [119] [HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection](https://arxiv.org/abs/2505.00507)
*Esteban Rivera, Surya Prabhakaran, Markus Lienkamp*

Main category: cs.CV

TL;DR: HeAL integrates heuristical features with active learning for 3D object detection, achieving competitive results with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Current active learning methods for 3D object detection neglect practical insights from literature and applications, making sample selection in uncontrolled scenarios challenging.

Method: HeAL combines heuristical features (e.g., object distance, point-quantity) with localization and classification to select impactful training samples.

Result: HeAL matches the mAP of full-supervised baselines using only 24% of samples, outperforming state-of-the-art methods on KITTI.

Conclusion: HeAL effectively enhances sample selection for 3D object detection, bridging the gap between theory and practical application.

Abstract: Active Learning has proved to be a relevant approach to perform sample
selection for training models for Autonomous Driving. Particularly, previous
works on active learning for 3D object detection have shown that selection of
samples in uncontrolled scenarios is challenging. Furthermore, current
approaches focus exclusively on the theoretical aspects of the sample selection
problem but neglect the practical insights that can be obtained from the
extensive literature and application of 3D detection models. In this paper, we
introduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)
which integrates those heuristical features together with Localization and
Classification to deliver the most contributing samples to the model's
training. In contrast to previous works, our approach integrates heuristical
features such as object distance and point-quantity to estimate the
uncertainty, which enhance the usefulness of selected samples to train
detection models. Our quantitative evaluation on KITTI shows that HeAL presents
competitive mAP with respect to the State-of-the-Art, and achieves the same mAP
as the full-supervised baseline with only 24% of the samples.

</details>


### [120] [Inconsistency-based Active Learning for LiDAR Object Detection](https://arxiv.org/abs/2505.00511)
*Esteban Rivera, Loic Stratil, Markus Lienkamp*

Main category: cs.CV

TL;DR: Active learning in LiDAR domain improves object detection efficiency, matching random sampling performance with half the labeled data.


<details>
  <summary>Details</summary>
Motivation: High costs of acquiring and labeling large datasets for deep learning models in autonomous driving necessitate efficient training strategies.

Method: Developed inconsistency-based sample selection strategies for active learning in the LiDAR domain.

Result: Naive inconsistency approach matched random sampling mAP with 50% of labeled data.

Conclusion: Active learning in LiDAR can significantly reduce labeling costs while maintaining performance.

Abstract: Deep learning models for object detection in autonomous driving have recently
achieved impressive performance gains and are already being deployed in
vehicles worldwide. However, current models require increasingly large datasets
for training. Acquiring and labeling such data is costly, necessitating the
development of new strategies to optimize this process. Active learning is a
promising approach that has been extensively researched in the image domain. In
our work, we extend this concept to the LiDAR domain by developing several
inconsistency-based sample selection strategies and evaluate their
effectiveness in various settings. Our results show that using a naive
inconsistency approach based on the number of detected boxes, we achieve the
same mAP as the random sampling strategy with 50% of the labeled data.

</details>


### [121] [InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method](https://arxiv.org/abs/2505.00512)
*Nguyen Hoang Khoi Tran, Julie Stephany Berrio, Mao Shan, Zhenxing Ming, Stewart Worrall*

Main category: cs.CV

TL;DR: A LiDAR-based method for intersection detection fuses semantic road segmentation and vehicle localization, achieving high precision and recall with robustness to segmentation errors.


<details>
  <summary>Details</summary>
Motivation: Intersections are key landmarks for correcting GNSS dropouts and updating maps, but existing detectors either ignore onboard semantic data or rely on scarce labeled datasets.

Method: The approach combines semantic road segmentation with vehicle localization in a BEV representation and refines candidates using branch topology analysis with least squares.

Result: Achieves 1.9 m mean localization error, 89% precision, and 77% recall at 5 m tolerance, outperforming learning-based baselines.

Conclusion: The method is robust to segmentation errors and applicable in real-world scenarios, offering a reliable solution for intersection detection.

Abstract: Intersections are geometric and functional key points in every road network.
They offer strong landmarks to correct GNSS dropouts and anchor new sensor data
in up-to-date maps. Despite that importance, intersection detectors either
ignore the rich semantic information already computed onboard or depend on
scarce, hand-labeled intersection datasets. To close that gap, this paper
presents a LiDAR-based method for intersection detection that (i) fuses
semantic road segmentation with vehicle localization to detect intersection
candidates in a bird's eye view (BEV) representation and (ii) refines those
candidates by analyzing branch topology with a least squares formulation. To
evaluate our method, we introduce an automated benchmarking pipeline that pairs
detections with OpenStreetMap (OSM) intersection nodes using precise GNSS/INS
ground-truth poses. Tested on eight SemanticKITTI sequences, the approach
achieves a mean localization error of 1.9 m, 89% precision, and 77% recall at a
5 m tolerance, outperforming the latest learning-based baseline. Moreover, the
method is robust to segmentation errors higher than those of the benchmark
model, demonstrating its applicability in the real world.

</details>


### [122] [A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic](https://arxiv.org/abs/2505.00534)
*Muhammad Imran Zaman, Usama Ijaz Bajwa, Gulshan Saleem, Rana Hammad Raza*

Main category: cs.CV

TL;DR: A deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT) is proposed to address challenges in urban traffic scenarios, achieving competitive performance on the AI City Challenge dataset.


<details>
  <summary>Details</summary>
Motivation: The rise of network cameras in ITS necessitates efficient solutions for object tracking across non-overlapping cameras, overcoming issues like occlusions, illumination variations, and diverse vehicle attributes.

Method: The framework uses Mask R-CNN for detection, NMS for target selection, transfer learning for re-identification, and ResNet-152 with Deep SORT for feature extraction and tracking.

Result: Achieves an IDF1 score of 0.8289, precision of 0.9026, and recall of 0.8527 on the AI City Challenge dataset.

Conclusion: The proposed framework is effective for robust and accurate vehicle tracking in complex urban environments.

Abstract: Vision sensors are becoming more important in Intelligent Transportation
Systems (ITS) for traffic monitoring, management, and optimization as the
number of network cameras continues to rise. However, manual object tracking
and matching across multiple non-overlapping cameras pose significant
challenges in city-scale urban traffic scenarios. These challenges include
handling diverse vehicle attributes, occlusions, illumination variations,
shadows, and varying video resolutions. To address these issues, we propose an
efficient and cost-effective deep learning-based framework for Multi-Object
Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for
object detection and employs Non-Maximum Suppression (NMS) to select target
objects from overlapping detections. Transfer learning is employed for
re-identification, enabling the association and generation of vehicle tracklets
across multiple cameras. Moreover, we leverage appropriate loss functions and
distance measures to handle occlusion, illumination, and shadow challenges. The
final solution identification module performs feature extraction using
ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed
framework is evaluated on the 5th AI City Challenge dataset (Track 3),
comprising 46 camera feeds. Among these 46 camera streams, 40 are used for
model training and validation, while the remaining six are utilized for model
testing. The proposed framework achieves competitive performance with an IDF1
score of 0.8289, and precision and recall scores of 0.9026 and 0.8527
respectively, demonstrating its effectiveness in robust and accurate vehicle
tracking.

</details>


### [123] [X-ray illicit object detection using hybrid CNN-transformer neural network architectures](https://arxiv.org/abs/2505.00564)
*Jorgen Cani, Christos Diou, Spyridon Evangelatos, Panagiotis Radoglou-Grammatikis, Vasileios Argyriou, Panagiotis Sarigiannidis, Iraklis Varlamis, Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: The paper evaluates hybrid CNN-transformer architectures for X-ray security imaging, comparing them to CNN baselines like YOLOv8, and finds hybrid models more robust to domain shifts.


<details>
  <summary>Details</summary>
Motivation: Heavily occluded or concealed objects in X-ray security imaging are challenging to detect. While CNNs dominate the field, the integration of CNNs and transformers is underexplored.

Method: Various hybrid CNN-transformer architectures (e.g., Next-ViT-S with YOLOv8 or RT-DETR heads) are tested against CNN baselines (YOLOv8) on three X-ray datasets (EDS, HiXray, PIDray).

Result: Hybrid architectures show increased robustness to domain shifts (EDS dataset), while YOLOv8 performs better on HiXray and PIDray. Detailed performance and error analyses are provided.

Conclusion: Hybrid CNN-transformer architectures offer advantages in robustness to domain shifts, suggesting guidelines for future research in X-ray security imaging.

Abstract: In the field of X-ray security applications, even the smallest details can
significantly impact outcomes. Objects that are heavily occluded or
intentionally concealed pose a great challenge for detection, whether by human
observation or through advanced technological applications. While certain Deep
Learning (DL) architectures demonstrate strong performance in processing local
information, such as Convolutional Neural Networks (CNNs), others excel in
handling distant information, e.g., transformers. In X-ray security imaging the
literature has been dominated by the use of CNN-based methods, while the
integration of the two aforementioned leading architectures has not been
sufficiently explored. In this paper, various hybrid CNN-transformer
architectures are evaluated against a common CNN object detection baseline,
namely YOLOv8. In particular, a CNN (HGNetV2) and a hybrid CNN-transformer
(Next-ViT-S) backbone are combined with different CNN/transformer detection
heads (YOLOv8 and RT-DETR). The resulting architectures are comparatively
evaluated on three challenging public X-ray inspection datasets, namely EDS,
HiXray, and PIDray. Interestingly, while the YOLOv8 detector with its default
backbone (CSP-DarkNet53) is generally shown to be advantageous on the HiXray
and PIDray datasets, when a domain distribution shift is incorporated in the
X-ray images (as happens in the EDS datasets), hybrid CNN-transformer
architectures exhibit increased robustness. Detailed comparative evaluation
results, including object-level detection performance and object-size error
analysis, demonstrate the strengths and weaknesses of each architectural
combination and suggest guidelines for future research. The source code and
network weights of the models employed in this study are available at
https://github.com/jgenc/xray-comparative-evaluation.

</details>


### [124] [Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities](https://arxiv.org/abs/2505.00568)
*Lucas Robinet, Ahmad Berjaoui, Elizabeth Cohen-Jonathan Moyal*

Main category: cs.CV

TL;DR: BM-MAE introduces a masked image modeling pre-training strategy for multimodal MRI data, enabling adaptation to any modality combination without separate models, outperforming baselines and reconstructing missing modalities efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of missing modalities in multimodal MRI data, which complicates pre-training and fine-tuning, by proposing a unified approach that avoids resource-intensive separate models.

Method: BM-MAE uses masked image modeling to pre-train on multimodal MRI data, allowing the same model to adapt to any subset of modalities without architectural changes.

Result: Outperforms or matches baselines requiring separate pre-training, surpasses training from scratch, and efficiently reconstructs missing modalities.

Conclusion: BM-MAE offers a practical, efficient solution for handling missing modalities in multimodal MRI, enhancing clinical applicability and performance.

Abstract: Multimodal magnetic resonance imaging (MRI) constitutes the first line of
investigation for clinicians in the care of brain tumors, providing crucial
insights for surgery planning, treatment monitoring, and biomarker
identification. Pre-training on large datasets have been shown to help models
learn transferable representations and adapt with minimal labeled data. This
behavior is especially valuable in medical imaging, where annotations are often
scarce. However, applying this paradigm to multimodal medical data introduces a
challenge: most existing approaches assume that all imaging modalities are
available during both pre-training and fine-tuning. In practice, missing
modalities often occur due to acquisition issues, specialist unavailability, or
specific experimental designs on small in-house datasets. Consequently, a
common approach involves training a separate model for each desired modality
combination, making the process both resource-intensive and impractical for
clinical use. Therefore, we introduce BM-MAE, a masked image modeling
pre-training strategy tailored for multimodal MRI data. The same pre-trained
model seamlessly adapts to any combination of available modalities, extracting
rich representations that capture both intra- and inter-modal information. This
allows fine-tuning on any subset of modalities without requiring architectural
changes, while still benefiting from a model pre-trained on the full set of
modalities. Extensive experiments show that the proposed pre-training strategy
outperforms or remains competitive with baselines that require separate
pre-training for each modality subset, while substantially surpassing training
from scratch on several downstream tasks. Additionally, it can quickly and
efficiently reconstruct missing modalities, highlighting its practical value.
Code and trained models are available at: https://github.com/Lucas-rbnt/bmmae

</details>


### [125] [AnimalMotionCLIP: Embedding motion in CLIP for Animal Behavior Analysis](https://arxiv.org/abs/2505.00569)
*Enmin Zhong, Carlos R. del-Blanco, Daniel Berjón, Fernando Jaureguizar, Narciso García*

Main category: cs.CV

TL;DR: AnimalMotionCLIP integrates motion and temporal modeling into CLIP for animal behavior recognition, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning models like CLIP lack motion integration and temporal modeling for animal behavior recognition.

Method: Proposes AnimalMotionCLIP, combining video frames and optical flow in CLIP, with dense, semi-dense, and sparse temporal classifiers.

Result: Superior performance on the Animal Kingdom dataset, recognizing fine temporal actions.

Conclusion: AnimalMotionCLIP effectively addresses motion and temporal challenges in animal behavior recognition.

Abstract: Recently, there has been a surge of interest in applying deep learning
techniques to animal behavior recognition, particularly leveraging pre-trained
visual language models, such as CLIP, due to their remarkable generalization
capacity across various downstream tasks. However, adapting these models to the
specific domain of animal behavior recognition presents two significant
challenges: integrating motion information and devising an effective temporal
modeling scheme. In this paper, we propose AnimalMotionCLIP to address these
challenges by interleaving video frames and optical flow information in the
CLIP framework. Additionally, several temporal modeling schemes using an
aggregation of classifiers are proposed and compared: dense, semi dense, and
sparse. As a result, fine temporal actions can be correctly recognized, which
is of vital importance in animal behavior analysis. Experiments on the Animal
Kingdom dataset demonstrate that AnimalMotionCLIP achieves superior performance
compared to state-of-the-art approaches.

</details>


### [126] [Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading](https://arxiv.org/abs/2505.00592)
*Shuo Tong, Shangde Gao, Ke Liu, Zihang Huang, Hongxia Xu, Haochao Ying, Jian Wu*

Main category: cs.CV

TL;DR: UMKD is a novel framework for disease image grading that addresses domain shifts and data imbalance by distilling knowledge from multiple expert models to a student model, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Domain shifts and data imbalance in disease image grading introduce bias, hindering clinical deployment. UMKD aims to mitigate these issues for robust AI healthcare applications.

Method: UMKD uses uncertainty-aware multi-experts knowledge distillation, decoupling task-agnostic and task-specific features, and dynamically adjusting knowledge transfer weights based on expert uncertainties.

Result: UMKD outperforms previous methods on histology prostate and fundus image grading datasets, excelling in both source-imbalanced and target-imbalanced scenarios.

Conclusion: UMKD provides a robust and practical solution for real-world disease image grading, addressing key challenges like domain shifts and model heterogeneity.

Abstract: Automatic disease image grading is a significant application of artificial
intelligence for healthcare, enabling faster and more accurate patient
assessments. However, domain shifts, which are exacerbated by data imbalance,
introduce bias into the model, posing deployment difficulties in clinical
applications. To address the problem, we propose a novel
\textbf{U}ncertainty-aware \textbf{M}ulti-experts \textbf{K}nowledge
\textbf{D}istillation (UMKD) framework to transfer knowledge from multiple
expert models to a single student model. Specifically, to extract
discriminative features, UMKD decouples task-agnostic and task-specific
features with shallow and compact feature alignment in the feature space. At
the output space, an uncertainty-aware decoupled distillation (UDD) mechanism
dynamically adjusts knowledge transfer weights based on expert model
uncertainties, ensuring robust and reliable distillation. Additionally, UMKD
also tackles the problems of model architecture heterogeneity and distribution
discrepancies between source and target domains, which are inadequately tackled
by previous KD approaches. Extensive experiments on histology prostate grading
(\textit{SICAPv2}) and fundus image grading (\textit{APTOS}) demonstrate that
UMKD achieves a new state-of-the-art in both source-imbalanced and
target-imbalanced scenarios, offering a robust and practical solution for
real-world disease image grading.

</details>


### [127] [Visual Trajectory Prediction of Vessels for Inland Navigation](https://arxiv.org/abs/2505.00599)
*Alexander Puzicha, Konstantin Wüstefeld, Kathrin Wilms, Frank Weichert*

Main category: cs.CV

TL;DR: The paper focuses on improving vessel trajectory prediction in inland navigation by integrating object detection, Kalman filters, and spline interpolation, addressing challenges like misclassification and complex environments.


<details>
  <summary>Details</summary>
Motivation: The need for accurate vessel trajectory prediction in autonomous and remote inland navigation systems due to complex surroundings and misclassification issues.

Method: Integration of advanced object detection, Kalman filters, and spline-based interpolation, with a comparative evaluation of tracking algorithms (BoT-SORT, Deep OC-SORT, ByeTrack).

Result: Improved accuracy in vessel movement prediction, highlighting the robustness of Kalman filters for smoothed trajectories.

Conclusion: Customized datasets and models are essential for inland navigation. Future work includes expanding datasets and incorporating vessel classification for better predictions.

Abstract: The future of inland navigation increasingly relies on autonomous systems and
remote operations, emphasizing the need for accurate vessel trajectory
prediction. This study addresses the challenges of video-based vessel tracking
and prediction by integrating advanced object detection methods, Kalman
filters, and spline-based interpolation. However, existing detection systems
often misclassify objects in inland waterways due to complex surroundings. A
comparative evaluation of tracking algorithms, including BoT-SORT, Deep
OC-SORT, and ByeTrack, highlights the robustness of the Kalman filter in
providing smoothed trajectories. Experimental results from diverse scenarios
demonstrate improved accuracy in predicting vessel movements, which is
essential for collision avoidance and situational awareness. The findings
underline the necessity of customized datasets and models for inland
navigation. Future work will expand the datasets and incorporate vessel
classification to refine predictions, supporting both autonomous systems and
human operators in complex environments.

</details>


### [128] [Dietary Intake Estimation via Continuous 3D Reconstruction of Food](https://arxiv.org/abs/2505.00606)
*Wallace Lee, YuHao Chen*

Main category: cs.CV

TL;DR: A 3D food model-based approach using monocular 2D video to monitor dietary habits accurately, addressing inaccuracies in traditional self-reported methods.


<details>
  <summary>Details</summary>
Motivation: To improve dietary monitoring by overcoming the inaccuracies of self-reported data, which can lead to health risks like obesity and diabetes.

Method: Uses COLMAP and pose estimation algorithms to create 3D food models from 2D video, tracking food volume changes during consumption.

Result: Experiments with toy models and real food show the approach's potential for accurate dietary monitoring.

Conclusion: The 3D reconstruction method offers a promising tool for automated and precise dietary behavior tracking.

Abstract: Monitoring dietary habits is crucial for preventing health risks associated
with overeating and undereating, including obesity, diabetes, and
cardiovascular diseases. Traditional methods for tracking food intake rely on
self-reported data before or after the eating, which are prone to inaccuracies.
This study proposes an approach to accurately monitor ingest behaviours by
leveraging 3D food models constructed from monocular 2D video. Using COLMAP and
pose estimation algorithms, we generate detailed 3D representations of food,
allowing us to observe changes in food volume as it is consumed. Experiments
with toy models and real food items demonstrate the approach's potential.
Meanwhile, we have proposed a new methodology for automated state recognition
challenges to accurately detect state changes and maintain model fidelity. The
3D reconstruction approach shows promise in capturing comprehensive dietary
behaviour insights, ultimately contributing to the development of automated and
accurate dietary monitoring tools.

</details>


### [129] [Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction](https://arxiv.org/abs/2505.00615)
*Simon Giebenhain, Tobias Kirschstein, Martin Rünz, Lourdes Agapito, Matthias Nießner*

Main category: cs.CV

TL;DR: Pixel3DMM uses vision transformers and DINO features for 3D face reconstruction from a single RGB image, outperforming baselines by 15% in geometric accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve 3D face reconstruction from single images, addressing diversity in expressions, angles, and ethnicities.

Method: Uses Pixel3DMM with DINO features, surface normal/uv-coordinate prediction, and FLAME optimization on a large dataset.

Result: Outperforms baselines by 15% in geometric accuracy, especially for posed expressions.

Conclusion: Pixel3DMM is effective for diverse 3D face reconstruction, setting a new benchmark.

Abstract: We address the 3D reconstruction of human faces from a single RGB image. To
this end, we propose Pixel3DMM, a set of highly-generalized vision transformers
which predict per-pixel geometric cues in order to constrain the optimization
of a 3D morphable face model (3DMM). We exploit the latent features of the DINO
foundation model, and introduce a tailored surface normal and uv-coordinate
prediction head. We train our model by registering three high-quality 3D face
datasets against the FLAME mesh topology, which results in a total of over
1,000 identities and 976K images. For 3D face reconstruction, we propose a
FLAME fitting opitmization that solves for the 3DMM parameters from the
uv-coordinate and normal estimates. To evaluate our method, we introduce a new
benchmark for single-image face reconstruction, which features high diversity
facial expressions, viewing angles, and ethnicities. Crucially, our benchmark
is the first to evaluate both posed and neutral facial geometry. Ultimately,
our method outperforms the most competitive baselines by over 15% in terms of
geometric accuracy for posed facial expressions.

</details>


### [130] [Diverse Semantics-Guided Feature Alignment and Decoupling for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2505.00619)
*Neng Dong, Shuanglin Yan, Liyan Zhang, Jinhui Tang*

Main category: cs.CV

TL;DR: The paper proposes DSFAD, a network for VI-ReID that aligns and decouples features using textual guidance and semantic margins to address modality discrepancies and style noise.


<details>
  <summary>Details</summary>
Motivation: The large modality discrepancy and style noise in VI-ReID reduce feature alignment and discriminability, necessitating a robust solution.

Method: DSFAD includes DSFA for cross-modality alignment guided by diverse textual descriptions, SMFD for feature decoupling with semantic margins, and SCFR for semantic restitution.

Result: Experiments on three datasets show DSFAD's superiority in handling VI-ReID challenges.

Conclusion: DSFAD effectively aligns and decouples features, improving VI-ReID performance by leveraging textual guidance and semantic constraints.

Abstract: Visible-Infrared Person Re-Identification (VI-ReID) is a challenging task due
to the large modality discrepancy between visible and infrared images, which
complicates the alignment of their features into a suitable common space.
Moreover, style noise, such as illumination and color contrast, reduces the
identity discriminability and modality invariance of features. To address these
challenges, we propose a novel Diverse Semantics-guided Feature Alignment and
Decoupling (DSFAD) network to align identity-relevant features from different
modalities into a textual embedding space and disentangle identity-irrelevant
features within each modality. Specifically, we develop a Diverse
Semantics-guided Feature Alignment (DSFA) module, which generates pedestrian
descriptions with diverse sentence structures to guide the cross-modality
alignment of visual features. Furthermore, to filter out style information, we
propose a Semantic Margin-guided Feature Decoupling (SMFD) module, which
decomposes visual features into pedestrian-related and style-related
components, and then constrains the similarity between the former and the
textual embeddings to be at least a margin higher than that between the latter
and the textual embeddings. Additionally, to prevent the loss of pedestrian
semantics during feature decoupling, we design a Semantic Consistency-guided
Feature Restitution (SCFR) module, which further excavates useful information
for identification from the style-related features and restores it back into
the pedestrian-related features, and then constrains the similarity between the
features after restitution and the textual embeddings to be consistent with
that between the features before decoupling and the textual embeddings.
Extensive experiments on three VI-ReID datasets demonstrate the superiority of
our DSFAD.

</details>


### [131] [Brain Foundation Models with Hypergraph Dynamic Adapter for Brain Disease Analysis](https://arxiv.org/abs/2505.00627)
*Zhongying Deng, Haoyu Wang, Ziyan Huang, Lipei Zhang, Angelica I. Aviles-Rivero, Chaoyu Liu, Junjun He, Zoe Kourtzi, Carola-Bibiane Schönlieb*

Main category: cs.CV

TL;DR: SAM-Brain3D and HyDA propose a brain-specific foundation model and lightweight adapter for efficient adaptation, outperforming state-of-the-art methods in brain disease tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current brain foundation models, such as task/data homogeneity and poor generalization, to improve brain disease analysis.

Method: Develop SAM-Brain3D (trained on 66,000 brain image-label pairs) and HyDA (a hypergraph-based adapter) for multi-modal, multi-scale, and dynamic adaptation.

Result: Outperforms existing methods in brain disease segmentation and classification tasks.

Conclusion: The framework offers a new paradigm for brain disease analysis through advanced foundation modeling.

Abstract: Brain diseases, such as Alzheimer's disease and brain tumors, present
profound challenges due to their complexity and societal impact. Recent
advancements in brain foundation models have shown significant promise in
addressing a range of brain-related tasks. However, current brain foundation
models are limited by task and data homogeneity, restricted generalization
beyond segmentation or classification, and inefficient adaptation to diverse
clinical tasks. In this work, we propose SAM-Brain3D, a brain-specific
foundation model trained on over 66,000 brain image-label pairs across 14 MRI
sub-modalities, and Hypergraph Dynamic Adapter (HyDA), a lightweight adapter
for efficient and effective downstream adaptation. SAM-Brain3D captures
detailed brain-specific anatomical and modality priors for segmenting diverse
brain targets and broader downstream tasks. HyDA leverages hypergraphs to fuse
complementary multi-modal data and dynamically generate patient-specific
convolutional kernels for multi-scale feature fusion and personalized
patient-wise adaptation. Together, our framework excels across a broad spectrum
of brain disease segmentation and classification tasks. Extensive experiments
demonstrate that our method consistently outperforms existing state-of-the-art
approaches, offering a new paradigm for brain disease analysis through
multi-modal, multi-scale, and dynamic foundation modeling.

</details>


### [132] [Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook](https://arxiv.org/abs/2505.00630)
*Muyi Bao, Shuchang Lyu, Zhaoyang Xu, Huiyu Zhou, Jinchang Ren, Shiming Xiang, Xiangtai Li, Guangliang Cheng*

Main category: cs.CV

TL;DR: The paper reviews Mamba-based methodologies in remote sensing, highlighting their advantages over CNNs and ViTs, and provides a taxonomy of innovations, benchmarking, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of CNNs (limited receptive fields) and ViTs (quadratic complexity) in remote sensing by exploring Mamba architectures, which offer linear scaling and global context.

Method: A comprehensive review of 120 studies, analyzing Mamba-based innovations across five dimensions: foundational principles, micro-architectural advancements, macro-architectural integrations, benchmarking, and unresolved challenges.

Result: Mamba is established as a transformative framework for remote sensing, with a taxonomy of advancements and open-source resources provided.

Conclusion: The survey bridges SSM theory and remote sensing practice, offering a foundation for future research and community-driven progress.

Abstract: Deep learning has profoundly transformed remote sensing, yet prevailing
architectures like Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) remain constrained by critical trade-offs: CNNs suffer from limited
receptive fields, while ViTs grapple with quadratic computational complexity,
hindering their scalability for high-resolution remote sensing data. State
Space Models (SSMs), particularly the recently proposed Mamba architecture,
have emerged as a paradigm-shifting solution, combining linear computational
scaling with global context modeling. This survey presents a comprehensive
review of Mamba-based methodologies in remote sensing, systematically analyzing
about 120 studies to construct a holistic taxonomy of innovations and
applications. Our contributions are structured across five dimensions: (i)
foundational principles of vision Mamba architectures, (ii) micro-architectural
advancements such as adaptive scan strategies and hybrid SSM formulations,
(iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids
and frequency-domain adaptations, (iv) rigorous benchmarking against
state-of-the-art methods in multiple application tasks, such as object
detection, semantic segmentation, change detection, etc. and (v) critical
analysis of unresolved challenges with actionable future directions. By
bridging the gap between SSM theory and remote sensing practice, this survey
establishes Mamba as a transformative framework for remote sensing analysis. To
our knowledge, this paper is the first systematic review of Mamba architectures
in remote sensing. Our work provides a structured foundation for advancing
research in remote sensing systems through SSM-based methods. We curate an
open-source repository
(https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster
community-driven advancements.

</details>


### [133] [Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments](https://arxiv.org/abs/2505.00668)
*Kirtan Rajesh, Suvidha Rupesh Kumar*

Main category: cs.CV

TL;DR: A DRL framework optimizes air purification booth placement in Delhi, outperforming traditional methods by balancing AQI reduction and spatial coverage.


<details>
  <summary>Details</summary>
Motivation: Urban air pollution in Delhi is severe, and static mitigation strategies are ineffective. A dynamic, data-driven approach is needed.

Method: Proximal Policy Optimization (PPO) is used to iteratively learn optimal booth placements based on spatial and environmental factors.

Result: The DRL framework outperforms baseline methods, achieving balanced AQI reduction and high-coverage deployment.

Conclusion: AI-driven spatial optimization can enhance smart city initiatives and urban air quality management.

Abstract: Urban air pollution remains a pressing global concern, particularly in
densely populated and traffic-intensive metropolitan areas like Delhi, where
exposure to harmful pollutants severely impacts public health. Delhi, being one
of the most polluted cities globally, experiences chronic air quality issues
due to vehicular emissions, industrial activities, and construction dust, which
exacerbate its already fragile atmospheric conditions. Traditional pollution
mitigation strategies, such as static air purifying installations, often fail
to maximize their impact due to suboptimal placement and limited adaptability
to dynamic urban environments. This study presents a novel deep reinforcement
learning (DRL) framework to optimize the placement of air purification booths
to improve the air quality index (AQI) in the city of Delhi. We employ Proximal
Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm,
to iteratively learn and identify high-impact locations based on multiple
spatial and environmental factors, including population density, traffic
patterns, industrial influence, and green space constraints. Our approach is
benchmarked against conventional placement strategies, including random and
greedy AQI-based methods, using multi-dimensional performance evaluation
metrics such as AQI improvement, spatial coverage, population and traffic
impact, and spatial entropy. Experimental results demonstrate that the RL-based
approach outperforms baseline methods by achieving a balanced and effective
distribution of air purification infrastructure. Notably, the DRL framework
achieves an optimal trade-off between AQI reduction and high-coverage
deployment, ensuring equitable environmental benefits across urban regions. The
findings underscore the potential of AI-driven spatial optimization in
advancing smart city initiatives and data-driven urban air quality management.

</details>


### [134] [Visual Test-time Scaling for GUI Agent Grounding](https://arxiv.org/abs/2505.00684)
*Tiange Luo, Lajanugen Logeswaran, Justin Johnson, Honglak Lee*

Main category: cs.CV

TL;DR: RegionFocus is a visual test-time scaling method for Vision Language Model Agents that dynamically zooms in on relevant webpage regions to improve grounding accuracy, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Understanding complex webpages is difficult due to visual clutter and numerous interface elements, leading to inaccurate action selection.

Method: The approach uses dynamic zooming on relevant regions and an image-as-map mechanism to visualize landmarks, aiding action selection.

Result: Performance improved by 28% on Screenspot-pro and 24% on WebVoyager benchmarks, with a new SOTA grounding accuracy of 61.6%.

Conclusion: RegionFocus effectively enhances grounding accuracy in interactive settings, demonstrating the value of visual test-time scaling.

Abstract: We introduce RegionFocus, a visual test-time scaling approach for Vision
Language Model Agents. Understanding webpages is challenging due to the visual
complexity of GUI images and the large number of interface elements, making
accurate action selection difficult. Our approach dynamically zooms in on
relevant regions, reducing background clutter and improving grounding accuracy.
To support this process, we propose an image-as-map mechanism that visualizes
key landmarks at each step, providing a transparent action record and enables
the agent to effectively choose among action candidates. Even with a simple
region selection strategy, we observe significant performance gains of 28+\% on
Screenspot-pro and 24+\% on WebVoyager benchmarks on top of two
state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,
highlighting the effectiveness of visual test-time scaling in interactive
settings. We achieve a new state-of-the-art grounding performance of 61.6\% on
the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.
Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.

</details>


### [135] [Towards Autonomous Micromobility through Scalable Urban Simulation](https://arxiv.org/abs/2505.00690)
*Wayne Wu, Honglin He, Chaoyuan Zhang, Jack He, Seth Z. Zhao, Ran Gong, Quanyi Li, Bolei Zhou*

Main category: cs.CV

TL;DR: The paper introduces URBAN-SIM, a scalable simulation platform for training AI agents in autonomous micromobility, and URBAN-BENCH, a benchmark suite to evaluate agent performance in urban tasks.


<details>
  <summary>Details</summary>
Motivation: Current micromobility relies on human operation, raising safety and efficiency concerns in unpredictable urban environments. AI assistance is proposed to enhance these aspects.

Method: Developed URBAN-SIM with modules for urban generation, dynamics, and scene sampling, and URBAN-BENCH with tasks to evaluate agent skills like locomotion and navigation.

Result: Experiments on diverse terrains and robots (wheeled, legged) revealed their strengths and limitations in urban tasks.

Conclusion: The proposed simulation and benchmark tools advance autonomous micromobility by improving training diversity and evaluation metrics.

Abstract: Micromobility, which utilizes lightweight mobile machines moving in urban
public spaces, such as delivery robots and mobility scooters, emerges as a
promising alternative to vehicular mobility. Current micromobility depends
mostly on human manual operation (in-person or remote control), which raises
safety and efficiency concerns when navigating busy urban environments full of
unpredictable obstacles and pedestrians. Assisting humans with AI agents in
maneuvering micromobility devices presents a viable solution for enhancing
safety and efficiency. In this work, we present a scalable urban simulation
solution to advance autonomous micromobility. First, we build URBAN-SIM - a
high-performance robot learning platform for large-scale training of embodied
agents in interactive urban scenes. URBAN-SIM contains three critical modules:
Hierarchical Urban Generation pipeline, Interactive Dynamics Generation
strategy, and Asynchronous Scene Sampling scheme, to improve the diversity,
realism, and efficiency of robot learning in simulation. Then, we propose
URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various
capabilities of the AI agents in achieving autonomous micromobility.
URBAN-BENCH includes eight tasks based on three core skills of the agents:
Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots
with heterogeneous embodiments, such as the wheeled and legged robots, across
these tasks. Experiments on diverse terrains and urban structures reveal each
robot's strengths and limitations.

</details>


### [136] [RayZer: A Self-supervised Large View Synthesis Model](https://arxiv.org/abs/2505.00702)
*Hanwen Jiang, Hao Tan, Peng Wang, Haian Jin, Yue Zhao, Sai Bi, Kai Zhang, Fujun Luan, Kalyan Sunkavalli, Qixing Huang, Georgios Pavlakos*

Main category: cs.CV

TL;DR: RayZer is a self-supervised 3D vision model that recovers camera poses and reconstructs scenes without 3D supervision, achieving competitive novel view synthesis.


<details>
  <summary>Details</summary>
Motivation: To enable 3D vision tasks without relying on ground-truth camera poses or scene geometry, reducing dependency on annotated data.

Method: Uses a self-supervised framework to disentangle camera and scene representations, employing a transformer-based model with ray structure as the only 3D prior.

Result: RayZer matches or outperforms methods requiring pose annotations in novel view synthesis.

Conclusion: RayZer demonstrates emerging 3D awareness and effectiveness in self-supervised 3D vision tasks.

Abstract: We present RayZer, a self-supervised multi-view 3D Vision model trained
without any 3D supervision, i.e., camera poses and scene geometry, while
exhibiting emerging 3D awareness. Concretely, RayZer takes unposed and
uncalibrated images as input, recovers camera parameters, reconstructs a scene
representation, and synthesizes novel views. During training, RayZer relies
solely on its self-predicted camera poses to render target views, eliminating
the need for any ground-truth camera annotations and allowing RayZer to be
trained with 2D image supervision. The emerging 3D awareness of RayZer is
attributed to two key factors. First, we design a self-supervised framework,
which achieves 3D-aware auto-encoding of input images by disentangling camera
and scene representations. Second, we design a transformer-based model in which
the only 3D prior is the ray structure, connecting camera, pixel, and scene
simultaneously. RayZer demonstrates comparable or even superior novel view
synthesis performance than ``oracle'' methods that rely on pose annotations in
both training and testing. Project: https://hwjiang1510.github.io/RayZer/

</details>


### [137] [T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT](https://arxiv.org/abs/2505.00703)
*Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li*

Main category: cs.CV

TL;DR: T2I-R1 introduces a reasoning-enhanced text-to-image model using bi-level chain-of-thought (CoT) and RL, improving performance over baselines.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of reasoning strategies like CoT and RL in visual generation, enhancing text-to-image models.

Method: Uses semantic-level CoT for prompt planning and token-level CoT for pixel processing, coordinated via BiCoT-GRPO with ensemble rewards.

Result: Achieves 13% and 19% improvements on T2I-CompBench and WISE benchmarks, outperforming FLUX.

Conclusion: T2I-R1 demonstrates the effectiveness of bi-level CoT and RL in advancing text-to-image generation.

Abstract: Recent advancements in large language models have demonstrated how
chain-of-thought (CoT) and reinforcement learning (RL) can improve performance.
However, applying such reasoning strategies to the visual generation domain
remains largely unexplored. In this paper, we present T2I-R1, a novel
reasoning-enhanced text-to-image generation model, powered by RL with a
bi-level CoT reasoning process. Specifically, we identify two levels of CoT
that can be utilized to enhance different stages of generation: (1) the
semantic-level CoT for high-level planning of the prompt and (2) the
token-level CoT for low-level pixel processing during patch-by-patch
generation. To better coordinate these two levels of CoT, we introduce
BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes
both generation CoTs within the same training step. By applying our reasoning
strategies to the baseline model, Janus-Pro, we achieve superior performance
with 13% improvement on T2I-CompBench and 19% improvement on the WISE
benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available
at: https://github.com/CaraJ7/T2I-R1

</details>


### [138] [VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions](https://arxiv.org/abs/2303.12675)
*Zeqing Xia, Bojun Xiong, Zhouhui Lian*

Main category: cs.CV

TL;DR: VecFontSDF is an end-to-end trainable method for synthesizing high-quality vector fonts using signed distance functions (SDFs), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on raster image generation, lacking direct vector font synthesis. VecFontSDF addresses this gap.

Method: Uses SDF-based implicit shape representation to model glyphs as shape primitives with parabolic curves, convertible to quadratic Bézier curves.

Result: Achieves high-quality results in vector font reconstruction, interpolation, and few-shot synthesis, surpassing state-of-the-art methods.

Conclusion: VecFontSDF effectively bridges the gap in vector font synthesis, offering a scalable solution for font design.

Abstract: Font design is of vital importance in the digital content design and modern
printing industry. Developing algorithms capable of automatically synthesizing
vector fonts can significantly facilitate the font design process. However,
existing methods mainly concentrate on raster image generation, and only a few
approaches can directly synthesize vector fonts. This paper proposes an
end-to-end trainable method, VecFontSDF, to reconstruct and synthesize
high-quality vector fonts using signed distance functions (SDFs). Specifically,
based on the proposed SDF-based implicit shape representation, VecFontSDF
learns to model each glyph as shape primitives enclosed by several parabolic
curves, which can be precisely converted to quadratic B\'ezier curves that are
widely used in vector font products. In this manner, most image generation
methods can be easily extended to synthesize vector fonts. Qualitative and
quantitative experiments conducted on a publicly-available dataset demonstrate
that our method obtains high-quality results on several tasks, including vector
font reconstruction, interpolation, and few-shot vector font synthesis,
markedly outperforming the state of the art. Our code and trained models are
available at https://xiazeqing.github.io/VecFontSDF.

</details>


### [139] [Deep Learning for automated multi-scale functional field boundaries extraction using multi-date Sentinel-2 and PlanetScope imagery: Case Study of Netherlands and Pakistan](https://arxiv.org/abs/2411.15923)
*Saba Zahid, Sajid Ghuffar, Obaid-ur-Rehman, Syed Roshaan Ali Shah*

Main category: cs.CV

TL;DR: The study evaluates multi-temporal satellite imagery and deep learning for field boundary delineation in the Netherlands and Pakistan, showing improved accuracy with multi-date NDVI stacks and transfer learning.


<details>
  <summary>Details</summary>
Motivation: To enhance functional field boundary delineation using multi-temporal satellite imagery and deep learning across diverse farming systems.

Method: Utilized UNET architecture with multi-date images and NDVI stacks, comparing IoU scores. Applied transfer learning from the Netherlands to Pakistan and trained separate models for Pakistan.

Result: Multi-date NDVI stacks improved accuracy by capturing temporal crop growth. Transfer learning and combined models showed robust performance, emphasizing the need for fine spatial resolution in small-scale farming.

Conclusion: Multi-temporal imagery and diverse geographical data are crucial for universal field boundary delineation models, applicable to heterogeneous agricultural environments.

Abstract: This study explores the effectiveness of multi-temporal satellite imagery for
better functional field boundary delineation using deep learning semantic
segmentation architecture on two distinct geographical and multi-scale farming
systems of Netherlands and Pakistan. Multidate images of April, August and
October 2022 were acquired for PlanetScope and Sentinel-2 in sub regions of
Netherlands and November 2022, February and March 2023 for selected area of
Dunyapur in Pakistan. For Netherlands, Basic registration crop parcels (BRP)
vector layer was used as labeled training data. while self-crafted field
boundary vector data were utilized for Pakistan. Four deep learning models with
UNET architecture were evaluated using different combinations of multi-date
images and NDVI stacks in the Netherlands subregions. A comparative analysis of
IoU scores assessed the effectiveness of the proposed multi-date NDVI stack
approach. These findings were then applied for transfer learning, using
pre-trained models from the Netherlands on the selected area in Pakistan.
Additionally, separate models were trained using self-crafted field boundary
data for Pakistan, and combined models were developed using data from both the
Netherlands and Pakistan. Results indicate that multi-date NDVI stacks provide
additional temporal context, reflecting crop growth over different times of the
season. The study underscores the critical role of multi-scale ground
information from diverse geographical areas in developing robust and
universally applicable models for field boundary delineation. The results also
highlight the importance of fine spatial resolution for extraction of field
boundaries in regions with small scale framing. The findings can be extended to
multi-scale implementations for improved automatic field boundary delineation
in heterogeneous agricultural environments.

</details>


### [140] [Interpretability-Aware Vision Transformer](https://arxiv.org/abs/2309.08035)
*Yao Qiang, Chengyin Li, Prashant Khanduri, Dongxiao Zhu*

Main category: cs.CV

TL;DR: IA-ViT introduces an interpretability-aware training procedure for Vision Transformers, enhancing model transparency without relying on post hoc methods.


<details>
  <summary>Details</summary>
Motivation: Current post hoc interpretability methods for ViTs lack generalization and effectiveness if models are improperly trained or miss key regions.

Method: IA-ViT jointly trains a feature extractor, predictor, and interpreter with an interpretability-aware objective, leveraging consistent attention maps and predicted distributions.

Result: IA-ViT improves interpretability and performance in image classification tasks, validated by qualitative and quantitative evaluations.

Conclusion: IA-ViT offers a novel, inherently interpretable training approach for ViTs, outperforming post hoc methods in both transparency and task performance.

Abstract: Vision Transformers (ViTs) have become prominent models for solving various
vision tasks. However, the interpretability of ViTs has not kept pace with
their promising performance. While there has been a surge of interest in
developing {\it post hoc} solutions to explain ViTs' outputs, these methods do
not generalize to different downstream tasks and various transformer
architectures. Furthermore, if ViTs are not properly trained with the given
data and do not prioritize the region of interest, the {\it post hoc} methods
would be less effective. Instead of developing another {\it post hoc} approach,
we introduce a novel training procedure that inherently enhances model
interpretability. Our interpretability-aware ViT (IA-ViT) draws inspiration
from a fresh insight: both the class patch and image patches consistently
generate predicted distributions and attention maps. IA-ViT is composed of a
feature extractor, a predictor, and an interpreter, which are trained jointly
with an interpretability-aware training objective. Consequently, the
interpreter simulates the behavior of the predictor and provides a faithful
explanation through its single-head self-attention mechanism. Our comprehensive
experimental results demonstrate the effectiveness of IA-ViT in several image
classification tasks, with both qualitative and quantitative evaluations of
model performance and interpretability. Source code is available from:
https://github.com/qiangyao1988/IA-ViT.

</details>


### [141] [Disentangle Before Anonymize: A Two-stage Framework for Attribute-preserved and Occlusion-robust De-identification](https://arxiv.org/abs/2311.08786)
*Mingrui Zhu, Dongxin Chen, Xin Wei, Nannan Wang, Xinbo Gao*

Main category: cs.CV

TL;DR: The paper introduces a two-stage framework (DBAF) for face de-identification, addressing challenges like attribute loss and occlusion artifacts by disentangling identity before anonymization.


<details>
  <summary>Details</summary>
Motivation: Current face de-identification methods struggle with preserving attributes and handling occlusions, leading to reduced authenticity and noticeable artifacts.

Method: Proposes 'Disentangle Before Anonymize' (DBAF) with CID and KRIA modules for identity disentanglement and anonymization, plus MAAR for occlusion robustness.

Result: Outperforms state-of-the-art methods in quality, detail fidelity, attribute preservation, and occlusion robustness.

Conclusion: DBAF effectively improves face de-identification by separating identity disentanglement and anonymization, achieving better results.

Abstract: In an era where personal photos are easily leaked and collected, face
de-identification is a crucial method for protecting identity privacy. However,
current face de-identification techniques face challenges in preserving
attribute details and often produce anonymized results with reduced
authenticity. These shortcomings are particularly evident when handling
occlusions,frequently resulting in noticeable editing artifacts. Our primary
finding in this work is that simultaneous training of identity disentanglement
and anonymization hinders their respective effectiveness.Therefore, we propose
"Disentangle Before Anonymize",a novel two-stage Framework(DBAF)designed for
attributepreserved and occlusion-robust de-identification. This framework
includes a Contrastive Identity Disentanglement (CID) module and a
Key-authorized Reversible Identity Anonymization (KRIA) module, achieving
faithful attribute preservation and high-quality identity anonymization edits.
Additionally, we introduce a Multiscale Attentional Attribute Retention (MAAR)
module to address the issue of reduced anonymization quality under
occlusions.Extensive experiments demonstrate that our method outperforms
state-of-the-art de-identification approaches, delivering superior quality,
enhanced detail fidelity, improved attribute preservation performance, and
greater robustness to occlusions.

</details>


### [142] [EyePreserve: Identity-Preserving Iris Synthesis](https://arxiv.org/abs/2312.12028)
*Siamul Karim Khan, Patrick Tinsley, Mahsa Mitcheff, Patrick Flynn, Kevin W. Bowyer, Adam Czajka*

Main category: cs.CV

TL;DR: A data-driven method for synthesizing iris images with varying pupil sizes, preserving identity, and improving recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of synthesizing iris images with varying pupil sizes while preserving identity, which is crucial for biometric datasets and forensic analysis.

Method: A fully data-driven approach for identity-preserving, pupil size-varying iris image synthesis, including deformation of existing iris textures based on target segmentation masks.

Result: The method outperforms state-of-the-art models in preserving identity and improving similarity between same-identity iris samples with different pupil sizes.

Conclusion: The approach enhances biometric datasets and aids forensic analysis, with potential applications in iris recognition systems.

Abstract: Synthesis of same-identity biometric iris images, both for existing and
non-existing identities while preserving the identity across a wide range of
pupil sizes, is complex due to the intricate iris muscle constriction
mechanism, requiring a precise model of iris non-linear texture deformations to
be embedded into the synthesis pipeline. This paper presents the first method
of fully data-driven, identity-preserving, pupil size-varying synthesis of iris
images. This approach is capable of synthesizing images of irises with
different pupil sizes representing non-existing identities, as well as
non-linearly deforming the texture of iris images of existing subjects given
the segmentation mask of the target iris image. Iris recognition experiments
suggest that the proposed deformation model both preserves the identity when
changing the pupil size, and offers better similarity between same-identity
iris samples with significant differences in pupil size, compared to
state-of-the-art linear and non-linear (bio-mechanical-based) iris deformation
models. Two immediate applications of the proposed approach are: (a) synthesis
of, or enhancement of the existing biometric datasets for iris recognition,
mimicking those acquired with iris sensors, and (b) helping forensic human
experts examine iris image pairs with significant differences in pupil
dilation. Images considered in this work conform to selected ISO/IEC 29794-6
quality metrics to make them applicable in biometric systems. The source codes
and model weights are offered with this paper.

</details>


### [143] [Scene-Conditional 3D Object Stylization and Composition](https://arxiv.org/abs/2312.12419)
*Jinghao Zhou, Tomas Jakab, Philip Torr, Christian Rupprecht*

Main category: cs.CV

TL;DR: A framework for stylizing 3D assets to fit 2D scenes, enhancing realism and control in object-scene composition.


<details>
  <summary>Details</summary>
Motivation: Current 3D generative models create objects in isolation, ignoring scene context. This work addresses the need for scene-aware stylization.

Method: Combines differentiable ray tracing for texture/lighting optimization with pre-trained text-to-image diffusion models.

Result: Applicable to diverse indoor/outdoor scenes and arbitrary objects, enabling realistic and controllable compositions.

Conclusion: The method advances scene-aware 3D asset stylization, offering new levels of control and realism.

Abstract: Recently, 3D generative models have made impressive progress, enabling the
generation of almost arbitrary 3D assets from text or image inputs. However,
these approaches generate objects in isolation without any consideration for
the scene where they will eventually be placed. In this paper, we propose a
framework that allows for the stylization of an existing 3D asset to fit into a
given 2D scene, and additionally produce a photorealistic composition as if the
asset was placed within the environment. This not only opens up a new level of
control for object stylization, for example, the same assets can be stylized to
reflect changes in the environment, such as summer to winter or fantasy versus
futuristic settings-but also makes the object-scene composition more
controllable. We achieve this by combining modeling and optimizing the object's
texture and environmental lighting through differentiable ray tracing with
image priors from pre-trained text-to-image diffusion models. We demonstrate
that our method is applicable to a wide variety of indoor and outdoor scenes
and arbitrary objects. Project page:
https://jensenzhoujh.github.io/scene-cond-3d/.

</details>


### [144] [Latte: Latent Diffusion Transformer for Video Generation](https://arxiv.org/abs/2401.03048)
*Xin Ma, Yaohui Wang, Xinyuan Chen, Gengyun Jia, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, Yu Qiao*

Main category: cs.CV

TL;DR: Latte is a Latent Diffusion Transformer for video generation, achieving state-of-the-art results by modeling video distribution in latent space with efficient variants and best practices.


<details>
  <summary>Details</summary>
Motivation: To advance video generation by integrating Transformers into diffusion models, addressing the challenge of modeling numerous spatio-temporal tokens.

Method: Extracts spatio-temporal tokens, uses Transformer blocks, and introduces efficient variants for spatial and temporal decomposition. Best practices include patch embedding, timestep-class injection, and learning strategies.

Result: State-of-the-art performance on FaceForensics, SkyTimelapse, UCF101, and Taichi-HD datasets, with competitive text-to-video generation results.

Conclusion: Latte offers insights for future research on Transformer-based diffusion models in video generation.

Abstract: We propose Latte, a novel Latent Diffusion Transformer for video generation.
Latte first extracts spatio-temporal tokens from input videos and then adopts a
series of Transformer blocks to model video distribution in the latent space.
In order to model a substantial number of tokens extracted from videos, four
efficient variants are introduced from the perspective of decomposing the
spatial and temporal dimensions of input videos. To improve the quality of
generated videos, we determine the best practices of Latte through rigorous
experimental analysis, including video clip patch embedding, model variants,
timestep-class information injection, temporal positional embedding, and
learning strategies. Our comprehensive evaluation demonstrates that Latte
achieves state-of-the-art performance across four standard video generation
datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In
addition, we extend Latte to the text-to-video generation (T2V) task, where
Latte achieves results that are competitive with recent T2V models. We strongly
believe that Latte provides valuable insights for future research on
incorporating Transformers into diffusion models for video generation.

</details>


### [145] [F2M-Reg: Unsupervised RGB-D Point Cloud Registration with Frame-to-Model Optimization](https://arxiv.org/abs/2405.00507)
*Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu*

Main category: cs.CV

TL;DR: F2M-Reg is a novel unsupervised RGB-D point cloud registration method using a frame-to-model framework with neural implicit fields, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing frame-to-frame methods are sensitive to inconsistencies like lighting changes and occlusions, leading to suboptimal performance.

Method: Proposes F2M-Reg, leveraging neural implicit fields for global scene modeling and pose optimization, enhanced by synthetic data warming-up.

Result: Outperforms previous methods significantly, especially in challenging scenarios like lighting changes and low overlap.

Conclusion: F2M-Reg's frame-to-model approach with neural implicit fields provides robust unsupervised registration, validated by extensive benchmarks.

Abstract: This work studies the problem of unsupervised RGB-D point cloud registration,
which aims at training a robust registration model without ground-truth pose
supervision. Existing methods usually leverages unposed RGB-D sequences and
adopt a frame-to-frame framework based on differentiable rendering to train the
registration model, which enforces the photometric and geometric consistency
between the two frames for supervision. However, this frame-to-frame framework
is vulnerable to inconsistent factors between different frames, e.g., lighting
changes, geometry occlusion, and reflective materials, which leads to
suboptimal convergence of the registration model. In this paper, we propose a
novel frame-to-model optimization framework named F2M-Reg for unsupervised
RGB-D point cloud registration. We leverage the neural implicit field as a
global model of the scene and optimize the estimated poses of the frames by
registering them to the global model, and the registration model is
subsequently trained with the optimized poses. Thanks to the global encoding
capability of neural implicit field, our frame-to-model framework is
significantly more robust to inconsistent factors between different frames and
thus can provide better supervision for the registration model. Besides, we
demonstrate that F2M-Reg can be further enhanced by a simplistic synthetic
warming-up strategy. To this end, we construct a photorealistic synthetic
dataset named Sim-RGBD to initialize the registration model for the
frame-to-model optimization on real-world RGB-D sequences. Extensive
experiments on four challenging benchmarks have shown that our method surpasses
the previous state-of-the-art counterparts by a large margin, especially under
scenarios with severe lighting changes and low overlap. Our code and models are
available at https://github.com/MrIsland/F2M_Reg.

</details>


### [146] [S3Former: Self-supervised High-resolution Transformer for Solar PV Profiling](https://arxiv.org/abs/2405.04489)
*Minh Tran, Adrian De Luis, Haitao Liao, Ying Huang, Roy McCann, Alan Mantooth, Jack Cothren, Ngan Le*

Main category: cs.CV

TL;DR: S3Former, a model for segmenting solar panels from aerial imagery, addresses challenges like weather conditions and roof characteristics using a Masked Attention Mask Transformer and self-supervised learning, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: The need for accurate mapping of PV installations to understand adoption and inform energy policy drives the development of S3Former.

Method: S3Former uses a Masked Attention Mask Transformer with a self-supervised learning pretrained backbone, leveraging low and high-level features and an instance query mechanism for better localization.

Result: S3Former demonstrates improvements over state-of-the-art models in diverse datasets.

Conclusion: S3Former effectively segments solar panels, providing critical data for energy policy and grid impact analysis.

Abstract: As the impact of climate change escalates, the global necessity to transition
to sustainable energy sources becomes increasingly evident. Renewable energies
have emerged as a viable solution for users, with Photovoltaic energy being a
favored choice for small installations due to its reliability and efficiency.
Accurate mapping of PV installations is crucial for understanding the extension
of its adoption and informing energy policy. To meet this need, we introduce
S3Former, designed to segment solar panels from aerial imagery and provide size
and location information critical for analyzing the impact of such
installations on the grid. Solar panel identification is challenging due to
factors such as varying weather conditions, roof characteristics, Ground
Sampling Distance variations and lack of appropriate initialization weights for
optimized training. To tackle these complexities, S3Former features a Masked
Attention Mask Transformer incorporating a self-supervised learning pretrained
backbone. Specifically, our model leverages low-level and high-level features
extracted from the backbone and incorporates an instance query mechanism
incorporated on the Transformer architecture to enhance the localization of
solar PV installations. We introduce a self-supervised learning phase (pretext
task) to improve the initialization weights on the backbone of S3Former. We
evaluated S3Former using diverse datasets, demonstrate improvement
state-of-the-art models.

</details>


### [147] [RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State Estimation Drift Mitigation on LiDAR PointCloud](https://arxiv.org/abs/2405.11536)
*Mohamed Nagy, Naoufel Werghi, Bilal Hassan, Jorge Dias, Majid Khonji*

Main category: cs.CV

TL;DR: Proposes RobMOT, a novel 3D tracking method with improved accuracy and efficiency, reducing false positives and drift.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in 3D tracking-by-detection, especially for distant/occluded objects and Kalman filter drift.

Method: Introduces track validity mechanism and multi-stage observational gating, refining Kalman filter terms.

Result: Achieves 29.47% MOTA and 4.8% HOTA improvements on KITTI, with high efficiency (3221 FPS).

Conclusion: RobMOT outperforms state-of-the-art methods, excelling in challenging scenarios and real-time tracking.

Abstract: This paper addresses limitations in 3D tracking-by-detection methods,
particularly in identifying legitimate trajectories and reducing state
estimation drift in Kalman filters. Existing methods often use threshold-based
filtering for detection scores, which can fail for distant and occluded
objects, leading to false positives. To tackle this, we propose a novel track
validity mechanism and multi-stage observational gating process, significantly
reducing ghost tracks and enhancing tracking performance. Our method achieves a
$29.47\%$ improvement in Multi-Object Tracking Accuracy (MOTA) on the KITTI
validation dataset with the Second detector. Additionally, a refined Kalman
filter term reduces localization noise, improving higher-order tracking
accuracy (HOTA) by $4.8\%$. The online framework, RobMOT, outperforms
state-of-the-art methods across multiple detectors, with HOTA improvements of
up to $3.92\%$ on the KITTI testing dataset and $8.7\%$ on the validation
dataset, while achieving low identity switch scores. RobMOT excels in
challenging scenarios, tracking distant objects and prolonged occlusions, with
a $1.77\%$ MOTA improvement on the Waymo Open dataset, and operates at a
remarkable 3221 FPS on a single CPU, proving its efficiency for real-time
multi-object tracking.

</details>


### [148] [3D StreetUnveiler with Semantic-aware 2DGS -- a simple baseline](https://arxiv.org/abs/2405.18416)
*Jingwei Xu, Yikai Wang, Yiqun Zhao, Yanwei Fu, Shenghua Gao*

Main category: cs.CV

TL;DR: StreetUnveiler reconstructs an empty street from crowded in-car camera footage using 3D representation and 2D Gaussian Splatting, with a novel time-reversal inpainting framework for consistency.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving requires clear street views, but removing static objects like stopped vehicles is challenging due to limited observation in moving environments.

Method: StreetUnveiler uses 2D Gaussian Splatting (2DGS) to identify and remove unwanted objects, inpaints rendered images, and optimizes 2DGS. A time-reversal framework ensures temporal consistency.

Result: The method successfully reconstructs a 3D empty street representation, with mesh extraction for further use.

Conclusion: StreetUnveiler effectively addresses the challenge of reconstructing empty streets from crowded observations, benefiting autonomous driving applications.

Abstract: Unveiling an empty street from crowded observations captured by in-car
cameras is crucial for autonomous driving. However, removing all temporarily
static objects, such as stopped vehicles and standing pedestrians, presents a
significant challenge. Unlike object-centric 3D inpainting, which relies on
thorough observation in a small scene, street scene cases involve long
trajectories that differ from previous 3D inpainting tasks. The camera-centric
moving environment of captured videos further complicates the task due to the
limited degree and time duration of object observation. To address these
obstacles, we introduce StreetUnveiler to reconstruct an empty street.
StreetUnveiler learns a 3D representation of the empty street from crowded
observations. Our representation is based on the hard-label semantic 2D
Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians
to be removed. We inpaint rendered image after removing unwanted Gaussians to
provide pseudo-labels and subsequently re-optimize the 2DGS. Given its temporal
continuous movement, we divide the empty street scene into observed,
partial-observed, and unobserved regions, which we propose to locate through a
rendered alpha map. This decomposition helps us to minimize the regions that
need to be inpainted. To enhance the temporal consistency of the inpainting, we
introduce a novel time-reversal framework to inpaint frames in reverse order
and use later frames as references for earlier frames to fully utilize the
long-trajectory observations. Our experiments conducted on the street scene
dataset successfully reconstructed a 3D representation of the empty street. The
mesh representation of the empty street can be extracted for further
applications. The project page and more visualizations can be found at:
https://streetunveiler.github.io

</details>


### [149] [Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion](https://arxiv.org/abs/2406.03184)
*Hao Wen, Zehuan Huang, Yaohui Wang, Xinyuan Chen, Lu Sheng*

Main category: cs.CV

TL;DR: Ouroboros3D unifies multi-view image generation and 3D reconstruction into a recursive diffusion process, improving geometric consistency and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Separate training of multi-view image generation and 3D reconstruction leads to data bias and poor reconstruction quality.

Method: A unified framework integrating diffusion-based multi-view generation and 3D reconstruction with a self-conditioning mechanism for joint training.

Result: Outperforms separate stage training and existing combined methods, achieving better geometric consistency.

Conclusion: Ouroboros3D provides a robust, unified solution for single image-to-3D creation with improved performance.

Abstract: Existing single image-to-3D creation methods typically involve a two-stage
process, first generating multi-view images, and then using these images for 3D
reconstruction. However, training these two stages separately leads to
significant data bias in the inference phase, thus affecting the quality of
reconstructed results. We introduce a unified 3D generation framework, named
Ouroboros3D, which integrates diffusion-based multi-view image generation and
3D reconstruction into a recursive diffusion process. In our framework, these
two modules are jointly trained through a self-conditioning mechanism, allowing
them to adapt to each other's characteristics for robust inference. During the
multi-view denoising process, the multi-view diffusion model uses the 3D-aware
maps rendered by the reconstruction module at the previous timestep as
additional conditions. The recursive diffusion framework with 3D-aware feedback
unites the entire process and improves geometric consistency.Experiments show
that our framework outperforms separation of these two stages and existing
methods that combine them at the inference phase. Project page:
https://costwen.github.io/Ouroboros3D/

</details>


### [150] [EZIGen: Enhancing zero-shot personalized image generation with precise subject encoding and decoupled guidance](https://arxiv.org/abs/2409.08091)
*Zicheng Duan, Yuxuan Ding, Chenhui Gou, Ziqin Zhou, Ethan Smith, Lingqiao Liu*

Main category: cs.CV

TL;DR: EZIGen introduces a novel approach for zero-shot personalized image generation by balancing text and subject guidance, achieving state-of-the-art results with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with fine-grained subject details and imbalanced guidance, leading to suboptimal generation.

Method: EZIGen uses a fixed pre-trained Diffusion UNet as a subject encoder and separates the dominance stages of text and subject guidance.

Result: Achieves top performance on benchmarks with 100x less training data and proves versatile with SDXL.

Conclusion: EZIGen is a model-agnostic solution for high-quality personalized image generation.

Abstract: Zero-shot personalized image generation models aim to produce images that
align with both a given text prompt and subject image, requiring the model to
incorporate both sources of guidance. Existing methods often struggle to
capture fine-grained subject details and frequently prioritize one form of
guidance over the other, resulting in suboptimal subject encoding and
imbalanced generation. In this study, we uncover key insights into overcoming
such drawbacks, notably that 1) the choice of the subject image encoder
critically influences subject identity preservation and training efficiency,
and 2) the text and subject guidance should take effect at different denoising
stages. Building on these insights, we introduce a new approach, EZIGen, that
employs two main components: leveraging a fixed pre-trained Diffusion UNet
itself as subject encoder, following a process that balances the two guidances
by separating their dominance stage and revisiting certain time steps to
bootstrap subject transfer quality. Through these two components, EZIGen,
initially built upon SD2.1-base, achieved state-of-the-art performances on
multiple personalized generation benchmarks with a unified model, while using
100 times less training data. Moreover, by further migrating our design to
SDXL, EZIGen is proven to be a versatile model-agnostic solution for
personalized generation. Demo Page:
zichengduan.github.io/pages/EZIGen/index.html

</details>


### [151] [LT3SD: Latent Trees for 3D Scene Diffusion](https://arxiv.org/abs/2409.08215)
*Quan Meng, Lei Li, Matthias Nießner, Angela Dai*

Main category: cs.CV

TL;DR: LT3SD is a latent diffusion model for large-scale 3D scene generation, addressing limitations in spatial extent and quality by using a latent tree representation and hierarchical diffusion.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for 3D object generation struggle with large-scale scenes due to limited spatial extent and quality.

Method: Introduces a latent tree representation for hierarchical encoding and trains a diffusion model on scene patches for scalable synthesis.

Result: Demonstrates effective large-scale, high-quality 3D scene generation and probabilistic completion for partial scenes.

Conclusion: LT3SD advances 3D scene generation by enabling scalable, high-quality synthesis and completion.

Abstract: We present LT3SD, a novel latent diffusion model for large-scale 3D scene
generation. Recent advances in diffusion models have shown impressive results
in 3D object generation, but are limited in spatial extent and quality when
extended to 3D scenes. To generate complex and diverse 3D scene structures, we
introduce a latent tree representation to effectively encode both
lower-frequency geometry and higher-frequency detail in a coarse-to-fine
hierarchy. We can then learn a generative diffusion process in this latent 3D
scene space, modeling the latent components of a scene at each resolution
level. To synthesize large-scale scenes with varying sizes, we train our
diffusion model on scene patches and synthesize arbitrary-sized output 3D
scenes through shared diffusion generation across multiple scene patches.
Through extensive experiments, we demonstrate the efficacy and benefits of
LT3SD for large-scale, high-quality unconditional 3D scene generation and for
probabilistic completion for partial scene observations.

</details>


### [152] [All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages](https://arxiv.org/abs/2411.16508)
*Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kuckreja, Mykola Maslych, Wafa Al Ghallabi, Mihail Mihaylov, Chao Qin, Abdelrahman M Shaker, Mike Zhang, Mahardika Krisna Ihsani, Amiel Esplana, Monil Gokani, Shachar Mirkin, Harsh Singh, Ashay Srivastava, Endre Hamerlik, Fathinah Asma Izzati, Fadillah Adamsyah Maani, Sebastian Cavada, Jenny Chim, Rohit Gupta, Sanjay Manjunath, Kamila Zhumakhanova, Feno Heriniaina Rabevohitra, Azril Amirudin, Muhammad Ridzuan, Daniya Kareem, Ketan More, Kunyang Li, Pramesh Shakya, Muhammad Saad, Amirpouya Ghasemaghaei, Amirbek Djanibekov, Dilshod Azizov, Branislava Jankovic, Naman Bhatia, Alvaro Cabrera, Johan Obando-Ceron, Olympiah Otieno, Fabian Farestam, Muztoba Rabbani, Sanoojan Baliah, Santosh Sanjeev, Abduragim Shtanchaev, Maheen Fatima, Thao Nguyen, Amrin Kareem, Toluwani Aremu, Nathan Xavier, Amit Bhatkal, Hawau Toyin, Aman Chadha, Hisham Cholakkal, Rao Muhammad Anwer, Michael Felsberg, Jorma Laaksonen, Thamar Solorio, Monojit Choudhury, Ivan Laptev, Mubarak Shah, Salman Khan, Fahad Khan*

Main category: cs.CV

TL;DR: ALM-bench is a benchmark for evaluating Large Multimodal Models (LMMs) across 100 languages, focusing on cultural diversity and low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To ensure LMMs understand cultural contexts, respect local sensitivities, and support underrepresented languages.

Method: ALM-bench evaluates LMMs using diverse question formats (true/false, multiple choice, open-ended) and 13 cultural aspects.

Result: The benchmark provides a rigorous evaluation framework, highlighting gaps in current LMMs' cultural and linguistic inclusivity.

Conclusion: ALM-bench encourages the development of globally inclusive LMMs and is publicly available for further research.

Abstract: Existing Large Multimodal Models (LMMs) generally focus on only a few regions
and languages. As LMMs continue to improve, it is increasingly important to
ensure they understand cultural contexts, respect local sensitivities, and
support low-resource languages, all while effectively integrating corresponding
visual cues. In pursuit of culturally diverse global multimodal models, our
proposed All Languages Matter Benchmark (ALM-bench) represents the largest and
most comprehensive effort to date for evaluating LMMs across 100 languages.
ALM-bench challenges existing models by testing their ability to understand and
reason about culturally diverse images paired with text in various languages,
including many low-resource languages traditionally underrepresented in LMM
research. The benchmark offers a robust and nuanced evaluation framework
featuring various question formats, including true/false, multiple choice, and
open-ended questions, which are further divided into short and long-answer
categories. ALM-bench design ensures a comprehensive assessment of a model's
ability to handle varied levels of difficulty in visual and linguistic
reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully
curates content from 13 distinct cultural aspects, ranging from traditions and
rituals to famous personalities and celebrations. Through this, ALM-bench not
only provides a rigorous testing ground for state-of-the-art open and
closed-source LMMs but also highlights the importance of cultural and
linguistic inclusivity, encouraging the development of models that can serve
diverse global populations effectively. Our benchmark is publicly available.

</details>


### [153] [PPT: Pretraining with Pseudo-Labeled Trajectories for Motion Forecasting](https://arxiv.org/abs/2412.06491)
*Yihong Xu, Yuan Yin, Éloi Zablocki, Tuan-Hung Vu, Alexandre Boulch, Matthieu Cord*

Main category: cs.CV

TL;DR: PPT (Pretraining with Pseudo-labeled Trajectories) uses noisy, diverse trajectories from 3D detectors/tracking to improve motion forecasting, reducing reliance on costly annotated datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional motion forecasting models depend on expensive, manually annotated datasets, which are hard to scale and limit generalization. PPT offers a scalable, noise-tolerant alternative.

Method: PPT leverages unprocessed, diverse trajectories from 3D detectors/tracking for pretraining, optionally finetuning with minimal labeled data.

Result: PPT achieves strong performance in low-data, cross-domain, end-to-end, and multi-class settings, improving generalization.

Conclusion: PPT is a simple, scalable solution for robust motion forecasting, reducing dependency on curated datasets.

Abstract: Accurately predicting how agents move in dynamic scenes is essential for safe
autonomous driving. State-of-the-art motion forecasting models rely on large
curated datasets with manually annotated or heavily post-processed
trajectories. However, building these datasets is costly, generally manual,
hard to scale, and lacks reproducibility. They also introduce domain gaps that
limit generalization across environments. We introduce PPT (Pretraining with
Pseudo-labeled Trajectories), a simple and scalable alternative that uses
unprocessed and diverse trajectories automatically generated from off-the-shelf
3D detectors and tracking. Unlike traditional pipelines aiming for clean,
single-label annotations, PPT embraces noise and diversity as useful signals
for learning robust representations. With optional finetuning on a small amount
of labeled data, models pretrained with PPT achieve strong performance across
standard benchmarks particularly in low-data regimes, and in cross-domain,
end-to-end and multi-class settings. PPT is easy to implement and improves
generalization in motion forecasting. Code and data will be released upon
acceptance.

</details>


### [154] [Multimodal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds](https://arxiv.org/abs/2501.01728)
*Simon B. Jensen, Stefan Oehmcke, Andreas Møgelmose, Meysam Madadi, Christian Igel, Sergio Escalera, Thomas B. Moeslund*

Main category: cs.CV

TL;DR: Deep learning fusion of 2D orthophotos and 3D ALS point clouds improves forest biodiversity assessment, achieving up to 81.4% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional forest biodiversity surveys are labor-intensive and spatially limited; this study explores deep learning fusion for more efficient and scalable assessment.

Method: Uses ResNet for orthophotos and PointVector for ALS point clouds, testing fusion methods like confidence-based ensembling, feature-level concatenation, and end-to-end training.

Result: Achieved 76.7% (orthophotos) and 75.8% (ALS) accuracy individually; fusion improved accuracy to 80.5-81.4%.

Conclusion: Combining spectral (orthophotos) and structural (ALS) data enhances biodiversity assessment, proving multimodal fusion effective.

Abstract: Assessment of forest biodiversity is crucial for ecosystem management and
conservation. While traditional field surveys provide high-quality assessments,
they are labor-intensive and spatially limited. This study investigates whether
deep learning-based fusion of close-range sensing data from 2D orthophotos and
3D airborne laser scanning (ALS) point clouds can reliable assess the
biodiversity potential of forests. We introduce the BioVista dataset,
comprising 44 378 paired samples of orthophotos and ALS point clouds from
temperate forests in Denmark, designed to explore multimodal fusion approaches.
Using deep neural networks (ResNet for orthophotos and PointVector for ALS
point clouds), we investigate each data modality's ability to assess forest
biodiversity potential, achieving overall accuracies of 76.7% and 75.8%,
respectively. We explore various 2D and 3D fusion approaches: confidence-based
ensembling, feature-level concatenation, and end-to-end training, achieving
overall accuracies of 80.5%, 81.4% and 80.4% respectively. Our results
demonstrate that spectral information from orthophotos and structural
information from ALS point clouds effectively complement each other in forest
biodiversity assessment.

</details>


### [155] [AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation](https://arxiv.org/abs/2501.09503)
*Junjie He, Yuxiang Tuo, Binghui Chen, Chongyang Zhong, Yifeng Geng, Liefeng Bo*

Main category: cs.CV

TL;DR: AnyStory is a unified approach for high-fidelity personalized text-to-image generation, handling single and multiple subjects without fidelity loss.


<details>
  <summary>Details</summary>
Motivation: Challenges in generating high-fidelity personalized images with specific subjects, especially multiple ones, drive the need for a robust solution.

Method: AnyStory uses an 'encode-then-route' approach: a universal image encoder (ReferenceNet + CLIP) for feature encoding and a decoupled instance-aware router for subject location prediction and condition injection.

Result: The method excels in retaining subject details, aligning text descriptions, and personalizing for multiple subjects.

Conclusion: AnyStory effectively addresses personalized subject generation, demonstrating superior performance in fidelity and multi-subject handling.

Abstract: Recently, large-scale generative models have demonstrated outstanding
text-to-image generation capabilities. However, generating high-fidelity
personalized images with specific subjects still presents challenges,
especially in cases involving multiple subjects. In this paper, we propose
AnyStory, a unified approach for personalized subject generation. AnyStory not
only achieves high-fidelity personalization for single subjects, but also for
multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory
models the subject personalization problem in an "encode-then-route" manner. In
the encoding step, AnyStory utilizes a universal and powerful image encoder,
i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve
high-fidelity encoding of subject features. In the routing step, AnyStory
utilizes a decoupled instance-aware subject router to accurately perceive and
predict the potential location of the corresponding subject in the latent
space, and guide the injection of subject conditions. Detailed experimental
results demonstrate the excellent performance of our method in retaining
subject details, aligning text descriptions, and personalizing for multiple
subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .

</details>


### [156] [Accelerating Diffusion Transformer via Error-Optimized Cache](https://arxiv.org/abs/2501.19243)
*Junxiang Qiu, Shuo Wang, Jinda Lu, Lin Liu, Houcheng Jiang, Xingyu Zhu, Yanbin Hao*

Main category: cs.CV

TL;DR: The paper introduces Error-Optimized Cache (EOC) to address the quality decline in Diffusion Transformer (DiT) sampling caused by existing caching methods. EOC reduces caching errors through prior knowledge extraction, cache optimization judgment, and error reduction.


<details>
  <summary>Details</summary>
Motivation: Existing caching methods for DiT accelerate sampling by reusing features but degrade content quality due to caching-induced errors. The goal is to minimize these errors while maintaining efficiency.

Method: EOC introduces three improvements: (1) prior knowledge extraction of caching differences, (2) a judgment method for cache optimization, and (3) cache optimization to reduce errors.

Result: EOC significantly reduces error accumulation, improving FID scores on ImageNet at various caching levels (e.g., 28.8% improvement at 75% caching).

Conclusion: EOC effectively balances sampling speed and content quality by optimizing caching errors, outperforming existing methods like FORA and Learning-to-cache.

Abstract: Diffusion Transformer (DiT) is a crucial method for content generation.
However, it needs a lot of time to sample. Many studies have attempted to use
caching to reduce the time consumption of sampling. Existing caching methods
accelerate generation by reusing DiT features from the previous time step and
skipping calculations in the next, but they tend to locate and cache low-error
modules without focusing on reducing caching-induced errors, resulting in a
sharp decline in generated content quality when increasing caching intensity.
To solve this problem, we propose the Error-Optimized Cache (EOC). This method
introduces three key improvements: (1) Prior knowledge extraction: Extract and
process the caching differences; (2) A judgment method for cache optimization:
Determine whether certain caching steps need to be optimized; (3) Cache
optimization: reduce caching errors. Experiments show that this algorithm
significantly reduces the error accumulation caused by caching, especially
excessive caching. On the ImageNet dataset, without substantially increasing
the computational load, this method improves the FID of the generated images
when the rule-based model FORA has a caching level of 75%, 50%, and 25%, and
the training-based model Learning-to-cache has a caching level of 22%.
Specifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857
to 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)
respectively.

</details>


### [157] [GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation](https://arxiv.org/abs/2504.02782)
*Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, Li Yuan*

Main category: cs.CV

TL;DR: The paper introduces GPT-ImgEval, a benchmark for evaluating GPT-4o's image generation, editing, and semantic synthesis capabilities, revealing its strong performance and unique architecture. It also highlights limitations and safety implications.


<details>
  <summary>Details</summary>
Motivation: To quantitatively and qualitatively assess GPT-4o's capabilities in image generation and editing, and to provide insights into its architecture and limitations.

Method: Developed the GPT-ImgEval benchmark, evaluated GPT-4o across three tasks, proposed a classification-model-based approach to analyze its architecture, and conducted comparative and safety analyses.

Result: GPT-4o outperforms existing methods in image generation and editing, exhibits a unique AR-diffusion hybrid architecture, and has identifiable limitations and synthetic artifacts.

Conclusion: The work provides a reliable benchmark for future research, fosters reproducibility, and accelerates innovation in image generation, while also addressing safety concerns.

Abstract: The recent breakthroughs in OpenAI's GPT4o model have demonstrated
surprisingly good capabilities in image generation and editing, resulting in
significant excitement in the community. This technical report presents the
first-look evaluation benchmark (named GPT-ImgEval), quantitatively and
qualitatively diagnosing GPT-4o's performance across three critical dimensions:
(1) generation quality, (2) editing proficiency, and (3) world
knowledge-informed semantic synthesis. Across all three tasks, GPT-4o
demonstrates strong performance, significantly surpassing existing methods in
both image generation control and output quality, while also showcasing
exceptional knowledge reasoning capabilities. Furthermore, based on the
GPT-4o's generated data, we propose a classification-model-based approach to
investigate the underlying architecture of GPT-4o, where our empirical results
suggest the model consists of an auto-regressive (AR) combined with a
diffusion-based head for image decoding, rather than the VAR-like
architectures. We also provide a complete speculation on GPT-4o's overall
architecture. In addition, we conduct a series of analyses to identify and
visualize GPT-4o's specific limitations and the synthetic artifacts commonly
observed in its image generation. We also present a comparative study of
multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the
safety implications of GPT-4o's outputs, particularly their detectability by
existing image forensic models. We hope that our work can offer valuable
insight and provide a reliable benchmark to guide future research, foster
reproducibility, and accelerate innovation in the field of image generation and
beyond. The codes and datasets used for evaluating GPT-4o can be found at
https://github.com/PicoTrex/GPT-ImgEval.

</details>


### [158] [LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation](https://arxiv.org/abs/2504.14467)
*Jiachen Li, Qing Xie, Renshu Gu, Jinyu Xu, Yongjian Liu, Xiaohan Yu*

Main category: cs.CV

TL;DR: LGD framework uses generative descriptions from Multi-Modal Large Language Models to improve zero-shot referring image segmentation by enhancing region-text matching.


<details>
  <summary>Details</summary>
Motivation: Addressing incorrect target localization due to ambiguity in free-form referring expressions by leveraging advanced language generation.

Method: Designs attribute and surrounding prompts for generative descriptions, then uses visual-text matching scores to align features.

Result: Achieves state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg, with up to 9.97% oIoU and 11.29% mIoU improvements.

Conclusion: LGD effectively enhances region-text matching, outperforming previous methods in zero-shot referring image segmentation.

Abstract: Zero-shot referring image segmentation aims to locate and segment the target
region based on a referring expression, with the primary challenge of aligning
and matching semantics across visual and textual modalities without training.
Previous works address this challenge by utilizing Vision-Language Models and
mask proposal networks for region-text matching. However, this paradigm may
lead to incorrect target localization due to the inherent ambiguity and
diversity of free-form referring expressions. To alleviate this issue, we
present LGD (Leveraging Generative Descriptions), a framework that utilizes the
advanced language generation capabilities of Multi-Modal Large Language Models
to enhance region-text matching performance in Vision-Language Models.
Specifically, we first design two kinds of prompts, the attribute prompt and
the surrounding prompt, to guide the Multi-Modal Large Language Models in
generating descriptions related to the crucial attributes of the referent
object and the details of surrounding objects, referred to as attribute
description and surrounding description, respectively. Secondly, three
visual-text matching scores are introduced to evaluate the similarity between
instance-level visual features and textual features, which determines the mask
most associated with the referring expression. The proposed method achieves new
state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and
RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU
compared to previous methods.

</details>


### [159] [GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting](https://arxiv.org/abs/2504.20379)
*Jongwon Lee, Timothy Bretl*

Main category: cs.CV

TL;DR: A method for query image localization using 3D Gaussian Splatting (3DGS) and PnP, achieving faster inference and lower error than baselines.


<details>
  <summary>Details</summary>
Motivation: To improve image localization efficiency and accuracy by leveraging 3DGS and 2D-3D correspondences.

Method: Renders synthetic RGBD images with 3DGS, establishes 2D-2D correspondences, lifts to 2D-3D, and solves PnP for pose estimation.

Result: Reduces inference time by 100x (to 0.1s) and errors significantly, tolerating large initial pose errors.

Conclusion: The method is fast, accurate, and robust to initial pose errors, outperforming photometric loss baselines.

Abstract: In this paper, we present a method for localizing a query image with respect
to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the
method uses 3DGS to render a synthetic RGBD image at some initial pose
estimate. Second, it establishes 2D-2D correspondences between the query image
and this synthetic image. Third, it uses the depth map to lift the 2D-2D
correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP)
problem to produce a final pose estimate. Results from evaluation across three
existing datasets with 38 scenes and over 2,700 test images show that our
method significantly reduces both inference time (by over two orders of
magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation
error compared to baseline methods that use photometric loss minimization.
Results also show that our method tolerates large errors in the initial pose
estimate of up to 55{\deg} in rotation and 1.1 units in translation (normalized
by scene scale), achieving final pose errors of less than 5{\deg} in rotation
and 0.05 units in translation on 90% of images from the Synthetic NeRF and
Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and
Temples dataset.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [160] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu, Calvin K. L. Or*

Main category: cs.AI

TL;DR: The paper surveys human-AI collaboration, identifies gaps, and proposes a unifying framework (Hierarchical Exploration-Exploitation Net) to integrate diverse studies and inspire future work.


<details>
  <summary>Details</summary>
Motivation: Address the lack of a unifying theoretical framework for human-AI collaboration, especially in complex tasks.

Method: Proposes a conceptual architecture linking multi-agent coordination, knowledge management, feedback loops, and control mechanisms. Maps existing techniques onto this framework.

Result: Facilitates revision of legacy methods and inspires new work combining qualitative and quantitative paradigms.

Conclusion: Offers a foundation for deeper co-evolution of human cognition and AI capability.

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>


### [161] [First Order Logic with Fuzzy Semantics for Describing and Recognizing Nerves in Medical Images](https://arxiv.org/abs/2505.00173)
*Isabelle Bloch, Enzo Bonnot, Pietro Gori, Giammarco La Barbera, Sabine Sarnacki*

Main category: cs.AI

TL;DR: The paper proposes a fuzzy logic-based method for recognizing fiber bundles (nerves) in medical images by formalizing anatomical knowledge and using spatial reasoning for segmentation.


<details>
  <summary>Details</summary>
Motivation: To address the imprecise description of nerves in anatomical textbooks and improve their recognition in medical imaging for surgical planning.

Method: Combines fuzzy semantics with first-order logic to formalize anatomical knowledge, defines a language for spatial entities and relations, and develops a spatial reasoning algorithm for segmentation.

Result: Illustrated on pelvic nerves in pediatric imaging, enabling surgeons to plan surgery.

Conclusion: The proposed method effectively bridges the gap between imprecise anatomical descriptions and precise medical image analysis.

Abstract: This article deals with the description and recognition of fiber bundles, in
particular nerves, in medical images, based on the anatomical description of
the fiber trajectories. To this end, we propose a logical formalization of this
anatomical knowledge. The intrinsically imprecise description of nerves, as
found in anatomical textbooks, leads us to propose fuzzy semantics combined
with first-order logic. We define a language representing spatial entities,
relations between these entities and quantifiers. A formula in this language is
then a formalization of the natural language description. The semantics are
given by fuzzy representations in a concrete domain and satisfaction degrees of
relations. Based on this formalization, a spatial reasoning algorithm is
proposed for segmentation and recognition of nerves from anatomical and
diffusion magnetic resonance images, which is illustrated on pelvic nerves in
pediatric imaging, enabling surgeons to plan surgery.

</details>


### [162] [Real-World Gaps in AI Governance Research](https://arxiv.org/abs/2505.00174)
*Ilan Strauss, Isobel Moure, Tim O'Reilly, Sruly Rosenblat*

Main category: cs.AI

TL;DR: Corporate AI research focuses on pre-deployment areas like model alignment and testing, while neglecting deployment-stage issues like bias. Gaps exist in high-risk domains, and improved observability is needed.


<details>
  <summary>Details</summary>
Motivation: To compare research outputs of leading AI companies and universities, identifying trends and gaps in AI safety and reliability.

Method: Analysis of 1,178 safety and reliability papers from 9,439 generative AI papers (2020-2025).

Result: Corporate research emphasizes pre-deployment (alignment, testing), while deployment-stage issues (bias) are overlooked. Gaps in high-risk domains like healthcare and misinformation.

Conclusion: Improved observability and external researcher access to deployment data are recommended to address knowledge deficits.

Abstract: Drawing on 1,178 safety and reliability papers from 9,439 generative AI
papers (January 2020 - March 2025), we compare research outputs of leading AI
companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI
universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of
Washington). We find that corporate AI research increasingly concentrates on
pre-deployment areas -- model alignment and testing & evaluation -- while
attention to deployment-stage issues such as model bias has waned. Significant
research gaps exist in high-risk deployment domains, including healthcare,
finance, misinformation, persuasive and addictive features, hallucinations, and
copyright. Without improved observability into deployed AI, growing corporate
concentration could deepen knowledge deficits. We recommend expanding external
researcher access to deployment data and systematic observability of in-market
AI behaviors.

</details>


### [163] [RAIL in the Wild: Operationalizing Responsible AI Evaluation Using Anthropic's Value Dataset](https://arxiv.org/abs/2505.00204)
*Sumit Verma, Pritam Prasun, Arpit Jaiswal, Pritish Kumar*

Main category: cs.AI

TL;DR: The paper introduces the RAIL framework to evaluate ethical behavior in LLMs, applying it to a dataset of 308,000 conversations to assess normative behavior.


<details>
  <summary>Details</summary>
Motivation: Existing AI ethics frameworks lack actionable evaluation methods, prompting the need for a systematic approach like RAIL.

Method: Uses the RAIL framework with eight measurable dimensions to assess LLMs, applied to the 'Values in the Wild' dataset.

Result: Maps values to RAIL dimensions, computes synthetic scores, and provides insights into LLM ethical behavior.

Conclusion: The RAIL framework offers a practical method to evaluate and improve the ethical standards of LLMs in real-world applications.

Abstract: As AI systems become embedded in real-world applications, ensuring they meet
ethical standards is crucial. While existing AI ethics frameworks emphasize
fairness, transparency, and accountability, they often lack actionable
evaluation methods. This paper introduces a systematic approach using the
Responsible AI Labs (RAIL) framework, which includes eight measurable
dimensions to assess the normative behavior of large language models (LLMs). We
apply this framework to Anthropic's "Values in the Wild" dataset, containing
over 308,000 anonymized conversations with Claude and more than 3,000 annotated
value expressions. Our study maps these values to RAIL dimensions, computes
synthetic scores, and provides insights into the ethical behavior of LLMs in
real-world use.

</details>


### [164] [Urban Air Mobility as a System of Systems: An LLM-Enhanced Holonic Approach](https://arxiv.org/abs/2505.00368)
*Ahmed R. Sadik, Muhammad Ashfaq, Niko Mäkitalo, Tommi Mikkonen*

Main category: cs.AI

TL;DR: The paper proposes an intelligent holonic architecture using LLMs to address scalability and adaptability challenges in Urban Air Mobility (UAM), demonstrating its effectiveness in dynamic resource allocation and real-time replanning.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches to UAM struggle with scalability, adaptability, and resource integration in complex environments, necessitating a more resilient and efficient solution.

Method: An intelligent holonic architecture incorporating LLMs is introduced, enabling semi-autonomous holons to coordinate air taxis, ground transport, and vertiports while processing natural language inputs and managing disruptions.

Result: A case study shows dynamic resource allocation, real-time replanning, and autonomous adaptation, proving the architecture's resilience and efficiency in multimodal transportation.

Conclusion: The work advances decentralized control and AI-driven adaptability, laying the foundation for resilient UAM ecosystems, with future goals including hybrid AI integration and real-world validation.

Abstract: Urban Air Mobility (UAM) is an emerging System of System (SoS) that faces
challenges in system architecture, planning, task management, and execution.
Traditional architectural approaches struggle with scalability, adaptability,
and seamless resource integration within dynamic and complex environments. This
paper presents an intelligent holonic architecture that incorporates Large
Language Model (LLM) to manage the complexities of UAM. Holons function semi
autonomously, allowing for real time coordination among air taxis, ground
transport, and vertiports. LLMs process natural language inputs, generate
adaptive plans, and manage disruptions such as weather changes or airspace
closures.Through a case study of multimodal transportation with electric
scooters and air taxis, we demonstrate how this architecture enables dynamic
resource allocation, real time replanning, and autonomous adaptation without
centralized control, creating more resilient and efficient urban transportation
networks. By advancing decentralized control and AI driven adaptability, this
work lays the groundwork for resilient, human centric UAM ecosystems, with
future efforts targeting hybrid AI integration and real world validation.

</details>


### [165] [DeCo: Defect-Aware Modeling with Contrasting Matching for Optimizing Task Assignment in Online IC Testing](https://arxiv.org/abs/2505.00278)
*Lo Pang-Yun Ting, Yu-Hao Chiang, Yi-Tung Tsai, Hsu-Chao Lai, Kun-Ta Chuang*

Main category: cs.AI

TL;DR: DeCo is an AI-driven approach for optimizing IC testing task assignment by leveraging defect-aware graphs and engineer expertise, achieving high success rates and balanced workloads.


<details>
  <summary>Details</summary>
Motivation: Current methods lack integration of defect characteristics, historical failures, and engineer insights, limiting IC handling efficiency.

Method: DeCo constructs defect-aware graphs, models engineer-task representations, and uses a contrasting-based assignment mechanism.

Result: DeCo achieves >80% task-handling success rates and balanced workloads, even with scarce defect data.

Conclusion: DeCo is a promising AI solution for real-world IC failure analysis and task handling.

Abstract: In the semiconductor industry, integrated circuit (IC) processes play a vital
role, as the rising complexity and market expectations necessitate improvements
in yield. Identifying IC defects and assigning IC testing tasks to the right
engineers improves efficiency and reduces losses. While current studies
emphasize fault localization or defect classification, they overlook the
integration of defect characteristics, historical failures, and the insights
from engineer expertise, which restrains their effectiveness in improving IC
handling. To leverage AI for these challenges, we propose DeCo, an innovative
approach for optimizing task assignment in IC testing. DeCo constructs a novel
defect-aware graph from IC testing reports, capturing co-failure relationships
to enhance defect differentiation, even with scarce defect data. Additionally,
it formulates defect-aware representations for engineers and tasks, reinforced
by local and global structure modeling on the defect-aware graph. Finally, a
contrasting-based assignment mechanism pairs testing tasks with QA engineers by
considering their skill level and current workload, thus promoting an equitable
and efficient job dispatch. Experiments on a real-world dataset demonstrate
that DeCo achieves the highest task-handling success rates in different
scenarios, exceeding 80\%, while also maintaining balanced workloads on both
scarce or expanded defect data. Moreover, case studies reveal that DeCo can
assign tasks to potentially capable engineers, even for their unfamiliar
defects, highlighting its potential as an AI-driven solution for the real-world
IC failure analysis and task handling.

</details>


### [166] [UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces](https://arxiv.org/abs/2505.00472)
*Alaa Saleh, Sasu Tarkoma, Praveen Kumar Donta, Naser Hossein Motlagh, Schahram Dustdar, Susanna Pirttikangas, Lauri Lovén*

Main category: cs.AI

TL;DR: UserCentrix is an agentic AI framework integrating GenAI and multi-agent systems to enhance smart spaces with dynamic, context-aware decision-making, achieving efficient resource management and improved responsiveness.


<details>
  <summary>Details</summary>
Motivation: To address the need for autonomous, proactive AI in smart environments by dynamically adapting to user preferences and optimizing resource allocation.

Method: Develops UserCentrix, a memory-augmented AI framework with personalized LLM agents, hybrid hierarchical control, and adaptive orchestration strategies.

Result: Experimental results show improved response accuracy, system efficiency, and computational resource management.

Conclusion: UserCentrix effectively enhances smart spaces through autonomous, context-aware AI, balancing real-time responsiveness and global awareness.

Abstract: Agentic AI, with its autonomous and proactive decision-making, has
transformed smart environments. By integrating Generative AI (GenAI) and
multi-agent systems, modern AI frameworks can dynamically adapt to user
preferences, optimize data management, and improve resource allocation. This
paper introduces UserCentrix, an agentic memory-augmented AI framework designed
to enhance smart spaces through dynamic, context-aware decision-making. This
framework integrates personalized Large Language Model (LLM) agents that
leverage user preferences and LLM memory management to deliver proactive and
adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control
system, balancing centralized and distributed processing to optimize real-time
responsiveness while maintaining global situational awareness. UserCentrix
achieves resource-efficient AI interactions by embedding memory-augmented
reasoning, cooperative agent negotiation, and adaptive orchestration
strategies. Our key contributions include (i) a self-organizing framework with
proactive scaling based on task urgency, (ii) a Value of Information
(VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM
agent, and (iv) an intelligent multi-agent coordination system for seamless
environment adaptation. Experimental results across various models confirm the
effectiveness of our approach in enhancing response accuracy, system
efficiency, and computational resource management in real-world application.

</details>


### [167] [CognitionNet: A Collaborative Neural Network for Play Style Discovery in Online Skill Gaming Platform](https://arxiv.org/abs/2505.00325)
*Rukma Talwadker, Surajit Chakrabarty, Aditya Pareek, Tridib Mukherjee, Deepak Saini*

Main category: cs.AI

TL;DR: The paper introduces CognitionNet, a two-stage deep neural network, to analyze gaming data for uncovering player psychology and tactics, improving engagement predictions.


<details>
  <summary>Details</summary>
Motivation: To leverage gaming data for understanding player behavior and psychology, enhancing player experience and protection.

Method: A two-stage deep neural network (CognitionNet) with a novel 'bridge loss' formulation to mine game behaviors and play styles from telemetry data.

Result: CognitionNet outperforms state-of-the-art baselines, revealing player psychology and tactics while providing diagnostic explanations for engagement predictions.

Conclusion: This research automates the discovery of player psychology and tactics from gaming data, offering insights for engagement and player experience.

Abstract: Games are one of the safest source of realizing self-esteem and relaxation at
the same time. An online gaming platform typically has massive data coming in,
e.g., in-game actions, player moves, clickstreams, transactions etc. It is
rather interesting, as something as simple as data on gaming moves can help
create a psychological imprint of the user at that moment, based on her
impulsive reactions and response to a situation in the game. Mining this
knowledge can: (a) immediately help better explain observed and predicted
player behavior; and (b) consequently propel deeper understanding towards
players' experience, growth and protection. To this effect, we focus on
discovery of the "game behaviours" as micro-patterns formed by continuous
sequence of games and the persistent "play styles" of the players' as a
sequence of such sequences on an online skill gaming platform for Rummy. We
propose a two stage deep neural network, CognitionNet. The first stage focuses
on mining game behaviours as cluster representations in a latent space while
the second aggregates over these micro patterns to discover play styles via a
supervised classification objective around player engagement. The dual
objective allows CognitionNet to reveal several player psychology inspired
decision making and tactics. To our knowledge, this is the first and
one-of-its-kind research to fully automate the discovery of: (i) player
psychology and game tactics from telemetry data; and (ii) relevant diagnostic
explanations to players' engagement predictions. The collaborative training of
the two networks with differential input dimensions is enabled using a novel
formulation of "bridge loss". The network plays pivotal role in obtaining
homogeneous and consistent play style definitions and significantly outperforms
the SOTA baselines wherever applicable.

</details>


### [168] [ScaleTrack: Scaling and back-tracking Automated GUI Agents](https://arxiv.org/abs/2505.00416)
*Jing Huang, Zhixiong Zeng, Wenkang Han, Yufeng Zhong, Liming Zheng, Shuai Fu, Jingyuan Chen, Lin Ma*

Main category: cs.AI

TL;DR: ScaleTrack is a framework for training automated GUI agents, addressing data scarcity in GUI grounding and incorporating backtracking for planning.


<details>
  <summary>Details</summary>
Motivation: Previous work lacks sufficient training data for GUI grounding and ignores backtracking in planning, limiting agent performance.

Method: ScaleTrack scales grounding with diverse GUI samples and introduces backtracking in planning by predicting actions from current GUI images and historical actions.

Result: Experimental results show ScaleTrack's effectiveness in improving GUI agent performance.

Conclusion: ScaleTrack successfully addresses data and backtracking challenges, enhancing automated GUI agent training.

Abstract: Automated GUI agents aims to facilitate user interaction by automatically
performing complex tasks in digital environments, such as web, mobile, desktop
devices. It receives textual task instruction and GUI description to generate
executable actions (\emph{e.g.}, click) and operation boxes step by step.
Training a GUI agent mainly involves grounding and planning stages, in which
the GUI grounding focuses on finding the execution coordinates according to the
task, while the planning stage aims to predict the next action based on
historical actions. However, previous work suffers from the limitations of
insufficient training data for GUI grounding, as well as the ignorance of
backtracking historical behaviors for GUI planning. To handle the above
challenges, we propose ScaleTrack, a training framework by scaling grounding
and backtracking planning for automated GUI agents. We carefully collected GUI
samples of different synthesis criterions from a wide range of sources, and
unified them into the same template for training GUI grounding models.
Moreover, we design a novel training strategy that predicts the next action
from the current GUI image, while also backtracking the historical actions that
led to the GUI image. In this way, ScaleTrack explains the correspondence
between GUI images and actions, which effectively describes the evolution rules
of the GUI environment. Extensive experimental results demonstrate the
effectiveness of ScaleTrack. Data and code will be available at url.

</details>


### [169] [Rule-based Classifier Models](https://arxiv.org/abs/2505.00474)
*Cecilia Di Florio, Huimin Dong, Antonino Rotolo*

Main category: cs.AI

TL;DR: Extends classifier models in legal domains by incorporating rules (ratio decidendi) alongside facts, building on Canavotto et al.'s (2023) work, and demonstrates decision inference with time and court hierarchy.


<details>
  <summary>Details</summary>
Motivation: Existing classifiers focus only on facts, but legal reasoning requires rules (ratio decidendi) for accurate case analysis.

Method: Extends Canavotto et al.'s rule-based reason model to include rules in classifiers, enabling inference for new cases.

Result: Demonstrates decision inference using the enriched framework, including time and court hierarchy.

Conclusion: The enriched classifier framework improves legal case analysis by integrating rules and facts.

Abstract: We extend the formal framework of classifier models used in the legal domain.
While the existing classifier framework characterises cases solely through the
facts involved, legal reasoning fundamentally relies on both facts and rules,
particularly the ratio decidendi. This paper presents an initial approach to
incorporating sets of rules within a classifier. Our work is built on the work
of Canavotto et al. (2023), which has developed the rule-based reason model of
precedential constraint within a hierarchy of factors. We demonstrate how
decisions for new cases can be inferred using this enriched rule-based
classifier framework. Additionally, we provide an example of how the time
element and the hierarchy of courts can be used in the new classifier
framework.

</details>


### [170] [Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4](https://arxiv.org/abs/2505.00603)
*Phanish Puranam, Prothit Sen, Maciej Workiewicz*

Main category: cs.AI

TL;DR: GPT4 matches human recall in analogical reasoning but lacks precision due to superficial matching, while humans excel in precision but have lower recall. Humans outperform in deep structural similarity recognition.


<details>
  <summary>Details</summary>
Motivation: To compare GPT4 and human capabilities in analogical reasoning for strategic decision-making, identifying strengths and weaknesses of each.

Method: Novel experimental design with source-to-target matching to evaluate recall and precision in analogy retrieval and application.

Result: GPT4 has high recall but low precision; humans show high precision but low recall. Humans better recognize deep structural similarities.

Conclusion: A division of labor is suggested: LLMs generate analogies, while humans evaluate and apply them contextually.

Abstract: This study investigates whether large language models, specifically GPT4, can
match human capabilities in analogical reasoning within strategic decision
making contexts. Using a novel experimental design involving source to target
matching, we find that GPT4 achieves high recall by retrieving all plausible
analogies but suffers from low precision, frequently applying incorrect
analogies based on superficial similarities. In contrast, human participants
exhibit high precision but low recall, selecting fewer analogies yet with
stronger causal alignment. These findings advance theory by identifying
matching, the evaluative phase of analogical reasoning, as a distinct step that
requires accurate causal mapping beyond simple retrieval. While current LLMs
are proficient in generating candidate analogies, humans maintain a comparative
advantage in recognizing deep structural similarities across domains. Error
analysis reveals that AI errors arise from surface level matching, whereas
human errors stem from misinterpretations of causal structure. Taken together,
the results suggest a productive division of labor in AI assisted
organizational decision making where LLMs may serve as broad analogy
generators, while humans act as critical evaluators, applying the most
contextually appropriate analogies to strategic problems.

</details>


### [171] [Combining LLMs with Logic-Based Framework to Explain MCTS](https://arxiv.org/abs/2505.00610)
*Ziyan An, Xia Wang, Hendrik Baier, Zirong Chen, Abhishek Dubey, Taylor T. Johnson, Jonathan Sprinkle, Ayan Mukhopadhyay, Meiyi Ma*

Main category: cs.AI

TL;DR: A framework combining Computational Tree Logic and LLMs provides natural language explanations for MCTS, ensuring factual consistency and handling diverse queries.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of trust in AI for sequential planning by making MCTS more interpretable.

Method: Uses a Computational Tree Logic-guided LLM to transform user queries into logic statements for MCTS, ensuring consistency with environmental dynamics.

Result: Demonstrates strong performance in accuracy and factual consistency through quantitative assessments.

Conclusion: The framework successfully enhances interpretability and trust in MCTS for sequential planning.

Abstract: In response to the lack of trust in Artificial Intelligence (AI) for
sequential planning, we design a Computational Tree Logic-guided large language
model (LLM)-based natural language explanation framework designed for the Monte
Carlo Tree Search (MCTS) algorithm. MCTS is often considered challenging to
interpret due to the complexity of its search trees, but our framework is
flexible enough to handle a wide range of free-form post-hoc queries and
knowledge-based inquiries centered around MCTS and the Markov Decision Process
(MDP) of the application domain. By transforming user queries into logic and
variable statements, our framework ensures that the evidence obtained from the
search tree remains factually consistent with the underlying environmental
dynamics and any constraints in the actual stochastic control process. We
evaluate the framework rigorously through quantitative assessments, where it
demonstrates strong performance in terms of accuracy and factual consistency.

</details>


### [172] [Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation](https://arxiv.org/abs/2505.00612)
*D. Sculley, Will Cukierski, Phil Culliton, Sohier Dane, Maggie Demkin, Ryan Holbrook, Addison Howard, Paul Mooney, Walter Reade, Megan Risdal, Nate Keating*

Main category: cs.AI

TL;DR: The paper argues that traditional ML evaluation methods are inadequate for Generative AI due to unbounded input/output spaces, lack of ground truth, and feedback loops. It highlights leakage and contamination as critical issues and suggests AI Competitions as a gold standard for rigorous evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inadequacy of traditional ML evaluation strategies for modern Generative AI models, which face unique challenges like leakage and contamination.

Method: The paper proposes leveraging AI Competitions, which have developed effective measures against leakage, as a model for rigorous GenAI evaluation.

Result: The result is a call to adopt AI Competitions as the gold standard for evaluating Generative AI, emphasizing their underutilized potential.

Conclusion: The conclusion advocates for the field to recognize AI Competitions as the benchmark for empirical rigor in GenAI evaluation and to utilize their practices and outcomes effectively.

Abstract: In this position paper, we observe that empirical evaluation in Generative AI
is at a crisis point since traditional ML evaluation and benchmarking
strategies are insufficient to meet the needs of evaluating modern GenAI models
and systems. There are many reasons for this, including the fact that these
models typically have nearly unbounded input and output spaces, typically do
not have a well defined ground truth target, and typically exhibit strong
feedback loops and prediction dependence based on context of previous model
outputs. On top of these critical issues, we argue that the problems of {\em
leakage} and {\em contamination} are in fact the most important and difficult
issues to address for GenAI evaluations. Interestingly, the field of AI
Competitions has developed effective measures and practices to combat leakage
for the purpose of counteracting cheating by bad actors within a competition
setting. This makes AI Competitions an especially valuable (but underutilized)
resource. Now is time for the field to view AI Competitions as the gold
standard for empirical rigor in GenAI evaluation, and to harness and harvest
their results with according value.

</details>


### [173] [Open-Source LLM-Driven Federated Transformer for Predictive IoV Management](https://arxiv.org/abs/2505.00651)
*Yazan Otoum, Arghavan Asad, Ishtiaq Ahmad*

Main category: cs.AI

TL;DR: FPoTT is a federated learning framework using open-source LLMs for scalable, real-time, and privacy-preserving IoV traffic management, achieving high prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like latency, scalability, and privacy in centralized IoV solutions, and exploring LLMs in federated contexts.

Method: FPoTT uses dynamic prompt optimization, dual-layer federated learning, and a Transformer-driven synthetic data generator.

Result: Achieves 99.86% prediction accuracy on real-world data and performs well on synthetic datasets.

Conclusion: Open-source LLMs like FPoTT offer a secure, adaptive, and scalable alternative to proprietary IoV solutions.

Abstract: The proliferation of connected vehicles within the Internet of Vehicles (IoV)
ecosystem presents critical challenges in ensuring scalable, real-time, and
privacy-preserving traffic management. Existing centralized IoV solutions often
suffer from high latency, limited scalability, and reliance on proprietary
Artificial Intelligence (AI) models, creating significant barriers to
widespread deployment, particularly in dynamic and privacy-sensitive
environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular
systems remains underexplored, especially concerning prompt optimization and
effective utilization in federated contexts. To address these challenges, we
propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel
framework that leverages open-source LLMs for predictive IoV management. FPoTT
introduces a dynamic prompt optimization mechanism that iteratively refines
textual prompts to enhance trajectory prediction. The architecture employs a
dual-layer federated learning paradigm, combining lightweight edge models for
real-time inference with cloud-based LLMs to retain global intelligence. A
Transformer-driven synthetic data generator is incorporated to augment training
with diverse, high-fidelity traffic scenarios in the Next Generation Simulation
(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing
EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data
while maintaining high performance on synthetic datasets. These results
underscore the potential of open-source LLMs in enabling secure, adaptive, and
scalable IoV management, offering a promising alternative to proprietary
solutions in smart mobility ecosystems.

</details>


### [174] [Artificial Scientific Discovery](https://arxiv.org/abs/2411.11672)
*Antonio Norelli*

Main category: cs.AI

TL;DR: The paper explores the concept of an artificial scientist, progressing from AlphaGo to ChatGPT, and introduces frameworks like Explanatory Learning (EL) and disentangled multimodal models to address challenges in autonomous research and knowledge communication.


<details>
  <summary>Details</summary>
Motivation: To realize the vision of an artificial scientist capable of generating original research and expanding human knowledge, addressing gaps in communication and interpretation.

Method: Develops the EL framework for explaining phenomena, applies it to Zendo, and proposes disentangled multimodal models for cost-effective interpretation and perception.

Result: Successfully cracks Zendo using EL, demonstrates the need for autonomous language interpretation, and introduces a benchmark where LLMs fail but humans succeed.

Conclusion: Artificial scientists require autonomous interpretation frameworks and disentangled models, with current LLMs lacking essential capabilities for true scientific contribution.

Abstract: Rooted in the explosion of deep learning over the past decade, this thesis
spans from AlphaGo to ChatGPT to empirically examine the fundamental concepts
needed to realize the vision of an artificial scientist: a machine with the
capacity to autonomously generate original research and contribute to the
expansion of human knowledge. The investigation begins with Olivaw, an AlphaGo
Zero-like agent that discovers Othello knowledge from scratch but is unable to
communicate it. This realization leads to the development of the Explanatory
Learning (EL) framework, a formalization of the problem faced by a scientist
when trying to explain a new phenomenon to their peers. The effective EL
prescriptions allow us to crack Zendo, a popular board game simulating the
scientific endeavor. This success comes with a fundamental insight: an
artificial scientist must develop its own interpretation of the language used
to explain its findings, and not rely on a rigid existing interpreter.
Questioning the very process of learning an interpreter, we turn our attention
to the inner functioning of modern multimodal models. This culminates in a
simple idea to build CLIP-like models where interpretation and perception are
explicitly disentangled: a cost-effective approach that couples two unimodal
models using little multimodal data and no further training. Finally, we
discuss what ChatGPT and its siblings are still missing to become artificial
scientists, and introduce the Big-Bench Symbol Interpretation Task, a benchmark
about interpreting Zendo-like explanations that sees LLMs going no further than
random chance while being instead fully solved by humans.

</details>


### [175] [Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers](https://arxiv.org/abs/2501.16961)
*Mohammad Raza, Natasa Milic-Frayling*

Main category: cs.AI

TL;DR: Semantic Self-Verification (SSV) improves reasoning accuracy in language models by combining them with logical solvers, achieving near-perfect precision in verification.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of robustness in AI-driven reasoning systems by improving the translation of natural language to formal logic.

Method: SSV uses a consistency-based approach to generate and verify abstract formalizations of problems with logical solvers.

Result: Significant advancement in reasoning accuracy and near-perfect precision in verification, demonstrated on open benchmarks.

Conclusion: SSV enables near-certain reasoning, reducing manual verification needs and advancing dependable autonomous AI systems.

Abstract: Robustness of reasoning remains a significant challenge for large language
models, and addressing it is essential for the practical applicability of
AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a
novel approach that addresses the key challenge in combining language models
with the rigor of logical solvers: to accurately formulate the reasoning
problem from natural language to the formal language of the solver. SSV uses a
consistency-based approach to produce strong abstract formalizations of
problems using concrete instantiations that are generated by the model and
verified by the solver. In addition to significantly advancing the overall
reasoning accuracy over the state-of-the-art, a key novelty that this approach
presents is a feature of verification that has near-perfect precision over a
significant coverage of cases, as we demonstrate on open reasoning benchmarks.
We propose such *near-certain reasoning* as a new approach to reduce the need
for manual verification in many cases, taking us closer to more dependable and
autonomous AI reasoning systems.

</details>


### [176] [MolGround: A Benchmark for Molecular Grounding](https://arxiv.org/abs/2503.23668)
*Jiaxin Wu, Ting Zhang, Rubing Chen, Wengyu Zhang, Chen Jason Zhang, Xiao-Yong Wei, Li Qing*

Main category: cs.AI

TL;DR: A benchmark for molecular grounding is proposed to evaluate models' referential abilities, outperforming existing models like GPT-4o and enhancing tasks like molecular captioning and ATC classification.


<details>
  <summary>Details</summary>
Motivation: Current approaches lack exploration of the referential aspect of molecular understanding, focusing only on descriptive aspects.

Method: Developed a molecular grounding benchmark with 117k QA pairs and a multi-agent grounding prototype, aligning with NLP, cheminformatics, and molecular science conventions.

Result: The system outperforms existing models, including GPT-4o, and improves traditional tasks like molecular captioning and ATC classification.

Conclusion: The proposed benchmark and prototype demonstrate the potential of NLP techniques to advance molecular understanding in AI for Science.

Abstract: Current molecular understanding approaches predominantly focus on the
descriptive aspect of human perception, providing broad, topic-level insights.
However, the referential aspect -- linking molecular concepts to specific
structural components -- remains largely unexplored. To address this gap, we
propose a molecular grounding benchmark designed to evaluate a model's
referential abilities. We align molecular grounding with established
conventions in NLP, cheminformatics, and molecular science, showcasing the
potential of NLP techniques to advance molecular understanding within the AI
for Science movement. Furthermore, we constructed the largest molecular
understanding benchmark to date, comprising 117k QA pairs, and developed a
multi-agent grounding prototype as proof of concept. This system outperforms
existing models, including GPT-4o, and its grounding outputs have been
integrated to enhance traditional tasks such as molecular captioning and ATC
(Anatomical, Therapeutic, Chemical) classification.

</details>


### [177] [Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search](https://arxiv.org/abs/2504.19636)
*Fei Liu, Qingfu Zhang, Xialiang Tong, Kun Mao, Mingxuan Yuan*

Main category: cs.AI

TL;DR: The paper analyzes the fitness landscape of LLM-assisted Algorithm Search (LAS) using a graph-based approach, revealing multimodal and rugged landscapes with variations across tasks and LLMs. It also explores the impact of population size on exploration-exploitation trade-offs.


<details>
  <summary>Details</summary>
Motivation: The fitness landscape of LLM-assisted Algorithm Search (LAS) is underexplored, despite its importance for understanding search behavior in iterative algorithm design.

Method: A graph-based approach is used, where nodes represent algorithms and edges denote transitions between them. Evaluations are conducted across six algorithm design tasks and six LLMs.

Result: LAS landscapes are highly multimodal and rugged, with structural variations across tasks and LLMs. Population size influences exploration-exploitation trade-offs.

Conclusion: The study advances understanding of LAS landscapes and offers practical guidance for designing more effective LAS methods.

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
algorithm design. However, when integrated into search frameworks for iterative
algorithm search, the underlying fitness landscape--critical for understanding
search behaviou--remains underexplored. In this paper, we illustrate and
analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a
graph-based approach, where nodes represent algorithms and edges denote
transitions between them. We conduct extensive evaluations across six algorithm
design tasks and six commonly used LLMs. Our findings reveal that LAS
landscapes are highly multimodal and rugged, particularly in combinatorial
optimization tasks, with distinct structural variations across tasks and LLMs.
For instance, heuristic design tasks exhibit dense clusters of high-performing
algorithms, while symbolic regression tasks show sparse, scattered
distributions. Additionally, we demonstrate how population size influences
exploration-exploitation trade-offs and the evolving trajectory of elite
algorithms. These insights not only advance our understanding of LAS landscapes
but also provide practical guidance for designing more effective LAS methods.

</details>


### [178] [A Domain-Agnostic Scalable AI Safety Ensuring Framework](https://arxiv.org/abs/2504.20924)
*Beomjun Kim, Kangyeon Kim, Sunwoo Kim, Heejin Ahn*

Main category: cs.AI

TL;DR: A novel AI safety framework ensures compliance with user-defined constraints across domains, combining AI components with optimization for probabilistic safety guarantees.


<details>
  <summary>Details</summary>
Motivation: Current AI safety methods lack generalization across contexts, necessitating a flexible, domain-agnostic approach.

Method: Combines AI (e.g., neural networks) with optimization to satisfy constraints probabilistically, using internal test data and conservative testing for credibility.

Result: Mathematically proven probabilistic safety guarantees; outperforms existing methods in low-threshold regions and scales with test data size.

Conclusion: The framework effectively generalizes safety guarantees across diverse applications, offering scalable and statistically valid solutions.

Abstract: Ensuring the safety of AI systems has recently emerged as a critical priority
for real-world deployment, particularly in physical AI applications. Current
approaches to AI safety typically address predefined domain-specific safety
conditions, limiting their ability to generalize across contexts. We propose a
novel AI safety framework that ensures AI systems comply with any user-defined
constraint, with any desired probability, and across various domains. In this
framework, we combine an AI component (e.g., neural network) with an
optimization problem to produce responses that minimize objectives while
satisfying user-defined constraints with probabilities exceeding user-defined
thresholds. For credibility assessment of the AI component, we propose internal
test data, a supplementary set of safety-labeled data, and a conservative
testing methodology that provides statistical validity of using internal test
data. We also present an approximation method of a loss function and how to
compute its gradient for training. We mathematically prove that probabilistic
constraint satisfaction is guaranteed under specific, mild conditions and prove
a scaling law between safety and the number of internal test data. We
demonstrate our framework's effectiveness through experiments in diverse
domains: demand prediction for production decision, safe reinforcement learning
within the SafetyGym simulator, and guarding AI chatbot outputs. Through these
experiments, we demonstrate that our method guarantees safety for
user-specified constraints, outperforms for up to several order of magnitudes
existing methods in low safety threshold regions, and scales effectively with
respect to the size of internal test data.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [179] [Bridging Cultural and Digital Divides: A Low-Latency JackTrip Framework for Equitable Music Education in the Global South](https://arxiv.org/abs/2505.00550)
*Tiange Zhou, Marco Bidin*

Main category: cs.SD

TL;DR: A low-latency JackTrip framework bridges digital and cultural divides in music education for the Global South, outperforming conventional platforms like Zoom under low-resource conditions.


<details>
  <summary>Details</summary>
Motivation: Address infrastructural and cultural challenges in music education in the Global South caused by digital divides and technical constraints.

Method: Leverages an open-source, UDP-based audio streaming protocol (JackTrip) to handle intermittent connectivity, limited bandwidth, and high latency, comparing its performance with Zoom.

Result: JackTrip achieves sub-30ms latency and preserves intricate audio details for non-Western music, confirmed by spectral analysis.

Conclusion: Decentralized, edge-computing solutions like JackTrip can empower educators and musicians, promoting technological equity and cultural preservation.

Abstract: The rapid expansion of digital technologies has transformed educational
landscapes worldwide, yet significant infrastructural and cultural challenges
persist in the Global South. This paper introduces a low-latency JackTrip
framework designed to bridge both the cultural and digital divides in music
education. By leveraging an open-source, UDP-based audio streaming protocol
originally developed at Stanford's CCRMA, the framework is tailored to address
technical constraints such as intermittent connectivity, limited bandwidth, and
high latency that characterize many rural and underserved regions. The study
systematically compares the performance of JackTrip with conventional platforms
like Zoom, demonstrating that JackTrip achieves sub-30~ms latency under
simulated low-resource conditions while preserving the intricate audio details
essential for non-Western musical traditions. Spectral analysis confirms that
JackTrip's superior handling of microtonal scales, complex rhythms, and
harmonic textures provides a culturally authentic medium for real-time ensemble
performance and music education. These findings underscore the transformative
potential of decentralized, edge-computing solutions in empowering educators
and musicians across the Global South, promoting both technological equity and
cultural preservation.

</details>


### [180] [Voice Cloning: Comprehensive Survey](https://arxiv.org/abs/2505.00579)
*Hussam Azzuni, Abdulmotaleb El Saddik*

Main category: cs.SD

TL;DR: A survey on voice cloning, covering terminology, variations (few-shot, zero-shot, multilingual TTS), evaluation metrics, and datasets, aiming to standardize research and prevent misuse.


<details>
  <summary>Details</summary>
Motivation: To establish standardized terminology and explore voice cloning variations for improved research and application, while addressing misuse concerns.

Method: Survey of existing voice cloning algorithms, focusing on speaker adaptation, few-shot, zero-shot, and multilingual TTS, along with evaluation metrics and datasets.

Result: Compilation of voice cloning algorithms and evaluation methods to guide future research and detection efforts.

Conclusion: Encourages research in voice cloning generation and detection to mitigate misuse, with a focus on standardization and comprehensive evaluation.

Abstract: Voice Cloning has rapidly advanced in today's digital world, with many
researchers and corporations working to improve these algorithms for various
applications. This article aims to establish a standardized terminology for
voice cloning and explore its different variations. It will cover speaker
adaptation as the fundamental concept and then delve deeper into topics such as
few-shot, zero-shot, and multilingual TTS within that context. Finally, we will
explore the evaluation metrics commonly used in voice cloning research and
related datasets. This survey compiles the available voice cloning algorithms
to encourage research toward its generation and detection to limit its misuse.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [181] [From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables for Heart Rate and Oxygen Consumption Modeling](https://arxiv.org/abs/2505.00101)
*Barak Gahtan, Sanketh Vedula, Gil Samuelly Leichtag, Einat Kodesh, Alex M. Bronstein*

Main category: cs.LG

TL;DR: A framework predicts oxygen consumption (VO2) from wearable data using heart rate modeling and a novel VO2 prediction method, achieving high accuracy with minimal calibration.


<details>
  <summary>Details</summary>
Motivation: To optimize performance, tailor training, and manage athlete health by enabling real-time metabolic monitoring with consumer-grade wearables.

Method: Combines a physiologically constrained ODE and neural Kalman filter for heart rate modeling, and a sequence-to-sequence VO2 prediction architecture requiring minimal initial data.

Result: Achieves mean absolute errors of 2.81 bpm for heart rate and ~13% for VO2, capturing rapid transitions and steady states.

Conclusion: The framework bridges lab-grade accuracy with everyday accessibility, benefiting athletes and fitness enthusiasts.

Abstract: Understanding physiological responses during running is critical for
performance optimization, tailored training prescriptions, and athlete health
management. We introduce a comprehensive framework -- what we believe to be the
first capable of predicting instantaneous oxygen consumption (VO$_{2}$)
trajectories exclusively from consumer-grade wearable data. Our approach
employs two complementary physiological models: (1) accurate modeling of heart
rate (HR) dynamics via a physiologically constrained ordinary differential
equation (ODE) and neural Kalman filter, trained on over 3 million HR
observations, achieving 1-second interval predictions with mean absolute errors
as low as 2.81\,bpm (correlation 0.87); and (2) leveraging the principles of
precise HR modeling, a novel VO$_{2}$ prediction architecture requiring only
the initial second of VO$_{2}$ data for calibration, enabling robust,
sequence-to-sequence metabolic demand estimation. Despite relying solely on
smartwatch and chest-strap data, our method achieves mean absolute percentage
errors of approximately 13\%, effectively capturing rapid physiological
transitions and steady-state conditions across diverse running intensities. Our
synchronized dataset, complemented by blood lactate measurements, further lays
the foundation for future noninvasive metabolic zone identification. By
embedding physiological constraints within modern machine learning, this
framework democratizes advanced metabolic monitoring, bridging laboratory-grade
accuracy and everyday accessibility, thus empowering both elite athletes and
recreational fitness enthusiasts.

</details>


### [182] [Kernel-Based Ensemble Gaussian Mixture Probability Hypothesis Density Filter](https://arxiv.org/abs/2505.00131)
*Dalton Durant, Renato Zanetti*

Main category: cs.LG

TL;DR: The paper introduces a kernel-based EnGM-PHD filter for multi-target filtering, combining GM-PHD and SMC-PHD techniques, showing superior performance.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-target filtering by merging Gaussian-mixture and particle-based methods for better accuracy and convergence.

Method: Uses Kernel Density Estimation (KDE) to approximate Gaussian mixtures from propagated particles, ensuring convergence to the true intensity function.

Result: The EnGM-PHD filter outperforms GM-PHD and SMC-PHD filters in multi-target filtering with the same component/particle count.

Conclusion: The EnGM-PHD filter is a robust solution for multi-target filtering, bridging the gap between Gaussian-mixture and particle-based approaches.

Abstract: In this work, a kernel-based Ensemble Gaussian Mixture Probability Hypothesis
Density (EnGM-PHD) filter is presented for multi-target filtering applications.
The EnGM-PHD filter combines the Gaussian-mixture-based techniques of the
Gaussian Mixture Probability Hypothesis Density (GM-PHD) filter with the
particle-based techniques of the Sequential Monte Carlo Probability Hypothesis
Density (SMC-PHD) filter. It achieves this by obtaining particles from the
posterior intensity function, propagating them through the system dynamics, and
then using Kernel Density Estimation (KDE) techniques to approximate the
Gaussian mixture of the prior intensity function. This approach guarantees
convergence to the true intensity function in the limit of the number of
components. Moreover, in the special case of a single target with no births,
deaths, clutter, and perfect detection probability, the EnGM-PHD filter reduces
to the standard Ensemble Gaussian Mixture Filter (EnGMF). In the presented
experiment, the results indicate that the EnGM-PHD filter achieves better
multi-target filtering performance than both the GM-PHD and SMC-PHD filters
while using the same number of components or particles.

</details>


### [183] [GPRat: Gaussian Process Regression with Asynchronous Tasks](https://arxiv.org/abs/2505.00136)
*Maksim Helmann, Alexander Strack, Dirk Pflüger*

Main category: cs.LG

TL;DR: The paper introduces GPRat, a Python library for Gaussian processes, combining HPX's asynchronous runtime with pybind11 for high performance and scalability, outperforming GPyTorch and GPflow.


<details>
  <summary>Details</summary>
Motivation: Python's reliance on low-level parallelization in AI libraries like PyTorch and TensorFlow can degrade performance. The work aims to integrate task-based C++ code (HPX) with Python for better scalability.

Method: The authors bind HPX's asynchronous runtime to Python using pybind11, developing GPRat, a parallel Gaussian process library. Performance is evaluated on a control theory benchmark.

Result: GPRat shows minimal binding overhead, superior scaling up to 64 cores, and significant speedups (7.63x over GPyTorch, 25.25x over GPflow). Speedups increase with more features.

Conclusion: Asynchronous task-based approaches in Python-based AI applications, as demonstrated by GPRat, offer substantial performance and scalability benefits.

Abstract: Python is the de-facto language for software development in artificial
intelligence (AI). Commonly used libraries, such as PyTorch and TensorFlow,
rely on parallelization built into their BLAS backends to achieve speedup on
CPUs. However, only applying parallelization in a low-level backend can lead to
performance and scaling degradation. In this work, we present a novel way of
binding task-based C++ code built on the asynchronous runtime model HPX to a
high-level Python API using pybind11. We develop a parallel Gaussian process
(GP) li- brary as an application. The resulting Python library GPRat combines
the ease of use of commonly available GP libraries with the performance and
scalability of asynchronous runtime systems. We evaluate the per- formance on a
mass-spring-damper system, a standard benchmark from control theory, for
varying numbers of regressors (features). The results show almost no binding
overhead when binding the asynchronous HPX code using pybind11. Compared to
GPyTorch and GPflow, GPRat shows superior scaling on up to 64 cores on an AMD
EPYC 7742 CPU for train- ing. Furthermore, our library achieves a prediction
speedup of 7.63 over GPyTorch and 25.25 over GPflow. If we increase the number
of features from eight to 128, we observe speedups of 29.62 and 21.19,
respectively. These results showcase the potential of using asynchronous tasks
within Python-based AI applications.

</details>


### [184] [Stochastic Subspace Descent Accelerated via Bi-fidelity Line Search](https://arxiv.org/abs/2505.00162)
*Nuojin Cheng, Alireza Doostan, Stephen Becker*

Main category: cs.LG

TL;DR: BF-SSD is a novel zeroth-order optimization method using bi-fidelity evaluations to reduce computational costs, outperforming baselines with fewer high-fidelity evaluations.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of expensive function evaluations in optimization, especially when gradients are unavailable.

Method: BF-SSD combines low-fidelity and high-fidelity evaluations to build a surrogate model for efficient backtracking line search.

Result: BF-SSD achieves superior performance with fewer high-fidelity evaluations across synthetic and real-world problems.

Conclusion: BF-SSD is an efficient, scalable solution for high-dimensional optimization problems, leveraging bi-fidelity strategies.

Abstract: Efficient optimization remains a fundamental challenge across numerous
scientific and engineering domains, especially when objective function and
gradient evaluations are computationally expensive. While zeroth-order
optimization methods offer effective approaches when gradients are
inaccessible, their practical performance can be limited by the high cost
associated with function queries. This work introduces the bi-fidelity
stochastic subspace descent (BF-SSD) algorithm, a novel zeroth-order
optimization method designed to reduce this computational burden. BF-SSD
leverages a bi-fidelity framework, constructing a surrogate model from a
combination of computationally inexpensive low-fidelity (LF) and accurate
high-fidelity (HF) function evaluations. This surrogate model facilitates an
efficient backtracking line search for step size selection, for which we
provide theoretical convergence guarantees under standard assumptions. We
perform a comprehensive empirical evaluation of BF-SSD across four distinct
problems: a synthetic optimization benchmark, dual-form kernel ridge
regression, black-box adversarial attacks on machine learning models, and
transformer-based black-box language model fine-tuning. Numerical results
demonstrate that BF-SSD consistently achieves superior optimization performance
while requiring significantly fewer HF function evaluations compared to
relevant baseline methods. This study highlights the efficacy of integrating
bi-fidelity strategies within zeroth-order optimization, positioning BF-SSD as
a promising and computationally efficient approach for tackling large-scale,
high-dimensional problems encountered in various real-world applications.

</details>


### [185] [GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation](https://arxiv.org/abs/2505.00169)
*Filipp Nikitin, Ian Dunn, David Ryan Koes, Olexandr Isayev*

Main category: cs.LG

TL;DR: The paper addresses flaws in evaluating 3D molecular generation models, proposing a corrected framework for GEOM-Drugs with updated metrics and recommendations.


<details>
  <summary>Details</summary>
Motivation: Current evaluation protocols for 3D molecular generation models are flawed, leading to inaccurate benchmarks.

Method: The authors fix data preprocessing issues, create accurate valency tables, and introduce a GFN2-xTB-based benchmark, then retrain and re-evaluate leading models.

Result: Updated performance metrics highlight the need for chemically rigorous evaluation practices.

Conclusion: The paper advocates for improved evaluation methods in 3D molecular generation and provides tools for future benchmarking.

Abstract: Deep generative models have shown significant promise in generating valid 3D
molecular structures, with the GEOM-Drugs dataset serving as a key benchmark.
However, current evaluation protocols suffer from critical flaws, including
incorrect valency definitions, bugs in bond order calculations, and reliance on
force fields inconsistent with the reference data. In this work, we revisit
GEOM-Drugs and propose a corrected evaluation framework: we identify and fix
issues in data preprocessing, construct chemically accurate valency tables, and
introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and
re-evaluate several leading models under this framework, providing updated
performance metrics and practical recommendations for future benchmarking. Our
results underscore the need for chemically rigorous evaluation practices in 3D
molecular generation. Our recommended evaluation methods and GEOM-Drugs
processing scripts are available at
https://github.com/isayevlab/geom-drugs-3dgen-evaluation.

</details>


### [186] [Attention-enabled Explainable AI for Bladder Cancer Recurrence Prediction](https://arxiv.org/abs/2505.00171)
*Saram Abbas, Naeem Soomro, Rishad Shafik, Rakesh Heer, Kabita Adhikari*

Main category: cs.LG

TL;DR: An interpretable deep learning framework improves NMIBC recurrence prediction by integrating vector embeddings and attention mechanisms, outperforming traditional methods with 70% accuracy.


<details>
  <summary>Details</summary>
Motivation: High recurrence rates (70-80%) and flawed existing tools in NMIBC management necessitate better, personalized prediction methods.

Method: Uses vector embeddings for categorical variables and attention mechanisms to capture complex relationships and provide patient-specific insights.

Result: Achieves 70% accuracy, identifies new influential factors (e.g., surgical duration, hospital stay), and offers clinician-friendly explanations.

Conclusion: The framework enhances prediction performance and interpretability, aiding personalized NMIBC patient management.

Abstract: Non-muscle-invasive bladder cancer (NMIBC) is a relentless challenge in
oncology, with recurrence rates soaring as high as 70-80%. Each recurrence
triggers a cascade of invasive procedures, lifelong surveillance, and
escalating healthcare costs - affecting 460,000 individuals worldwide. However,
existing clinical prediction tools remain fundamentally flawed, often
overestimating recurrence risk and failing to provide personalized insights for
patient management. In this work, we propose an interpretable deep learning
framework that integrates vector embeddings and attention mechanisms to improve
NMIBC recurrence prediction performance. We incorporate vector embeddings for
categorical variables such as smoking status and intravesical treatments,
allowing the model to capture complex relationships between patient attributes
and recurrence risk. These embeddings provide a richer representation of the
data, enabling improved feature interactions and enhancing prediction
performance. Our approach not only enhances performance but also provides
clinicians with patient-specific insights by highlighting the most influential
features contributing to recurrence risk for each patient. Our model achieves
accuracy of 70% with tabular data, outperforming conventional statistical
methods while providing clinician-friendly patient-level explanations through
feature attention. Unlike previous studies, our approach identifies new
important factors influencing recurrence, such as surgical duration and
hospital stay, which had not been considered in existing NMIBC prediction
models.

</details>


### [187] [Chronic Diseases Prediction using Machine Learning and Deep Learning Methods](https://arxiv.org/abs/2505.00189)
*Houda Belhad, Asmae Bourbia, Salma Boughanja*

Main category: cs.LG

TL;DR: Machine learning and deep learning techniques were applied to predict chronic diseases, with ensemble methods like Random Forest and Gradient Boosted Trees showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Early detection of chronic diseases is critical but traditional methods often fail due to complexity. ML/DL can improve prediction accuracy.

Method: Used Logistic Regression, Random Forest, Gradient Boosted Trees, Neural Networks, Decision Trees, and Naive Bayes. Included data pre-processing and model evaluation.

Result: Ensemble methods and Neural Networks outperformed others, demonstrating ML/DL's potential for early disease prediction.

Conclusion: ML/DL can revolutionize chronic disease prediction but faces challenges like data quality and interpretability.

Abstract: Chronic diseases, such as cardiovascular disease, diabetes, chronic kidney
disease, and thyroid disorders, are the leading causes of premature mortality
worldwide. Early detection and intervention are crucial for improving patient
outcomes, yet traditional diagnostic methods often fail due to the complex
nature of these conditions. This study explores the application of machine
learning (ML) and deep learning (DL) techniques to predict chronic disease and
thyroid disorders. We used a variety of models, including Logistic Regression
(LR), Random Forest (RF), Gradient Boosted Trees (GBT), Neural Networks (NN),
Decision Trees (DT) and Native Bayes (NB), to analyze and predict disease
outcomes. Our methodology involved comprehensive data pre-processing, including
handling missing values, categorical encoding, and feature aggregation,
followed by model training and evaluation. Performance metrics such ad
precision, recall, accuracy, F1-score, and Area Under the Curve (AUC) were used
to assess the effectiveness of each model. The results demonstrated that
ensemble methods like Random Forest and Gradient Boosted Trees consistently
outperformed. Neutral Networks also showed superior performance, particularly
in capturing complex data patterns. The findings highlight the potential of ML
and DL in revolutionizing chronic disease prediction, enabling early diagnosis
and personalized treatment strategies. However, challenges such as data
quality, model interpretability, and the need for advanced computational
techniques in healthcare to improve patient outcomes and reduce the burden of
chronic diseases. This study was conducted as part of Big Data class project
under the supervision of our professors Mr. Abderrahmane EZ-ZAHOUT and Mr.
Abdessamad ESSAIDI.

</details>


### [188] [Empirical Evaluation of Progressive Coding for Sparse Autoencoders](https://arxiv.org/abs/2505.00190)
*Hans Peter, Anders Søgaard*

Main category: cs.LG

TL;DR: Matryoshka SAEs outperform pruned vanilla SAEs in reconstruction and language modeling but are less interpretable.


<details>
  <summary>Details</summary>
Motivation: To address the computational expense of sparse autoencoders (SAEs) and improve their efficiency and interpretability.

Method: Compare progressive coding (subset pruning) with nested SAEs (Matryoshka SAEs) on a language modeling task.

Result: Matryoshka SAEs show lower reconstruction loss, better language modeling performance, and higher representational similarity, while pruned vanilla SAEs are more interpretable.

Conclusion: There's a trade-off between performance and interpretability, with Matryoshka SAEs excelling in efficiency but pruned vanilla SAEs offering better interpretability.

Abstract: Sparse autoencoders (SAEs)
\citep{bricken2023monosemanticity,gao2024scalingevaluatingsparseautoencoders}
rely on dictionary learning to extract interpretable features from neural
networks at scale in an unsupervised manner, with applications to
representation engineering and information retrieval. SAEs are, however,
computationally expensive \citep{lieberum2024gemmascopeopensparse}, especially
when multiple SAEs of different sizes are needed. We show that dictionary
importance in vanilla SAEs follows a power law. We compare progressive coding
based on subset pruning of SAEs -- to jointly training nested SAEs, or
so-called {\em Matryoshka} SAEs
\citep{bussmann2024learning,nabeshima2024Matryoshka} -- on a language modeling
task. We show Matryoshka SAEs exhibit lower reconstruction loss and recaptured
language modeling loss, as well as higher representational similarity. Pruned
vanilla SAEs are more interpretable, however. We discuss the origins and
implications of this trade-off.

</details>


### [189] [Mapping minds not averages: a scalable subject-specific manifold learning framework for neuroimaging data](https://arxiv.org/abs/2505.00196)
*Eloy Geenjaar, Vince Calhoun*

Main category: cs.LG

TL;DR: A manifold learning framework is introduced to capture subject-specific spatial variations in neuroimaging data, outperforming group-based methods and revealing clinically relevant brain activity patterns.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods that rely on population-level alignment or temporal structure, the paper aims to uncover individualized brain activity patterns.

Method: The framework learns low-dimensional manifolds from structured and unstructured neuroimaging data, tested on simulated and real datasets (Sherlock, Forrest Gump, and resting-state fMRI).

Result: The method outperforms baselines, scales efficiently, and generalizes well, revealing clinically relevant patterns in schizophrenia patients.

Conclusion: The framework provides a scalable, individualized tool for modeling brain activity, with potential applications in neuroscience and clinical research.

Abstract: Mental and cognitive representations are believed to reside on
low-dimensional, non-linear manifolds embedded within high-dimensional brain
activity. Uncovering these manifolds is key to understanding individual
differences in brain function, yet most existing machine learning methods
either rely on population-level spatial alignment or assume data that is
temporally structured, either because data is aligned among subjects or because
event timings are known. We introduce a manifold learning framework that can
capture subject-specific spatial variations across both structured and
temporally unstructured neuroimaging data. On simulated data and two
naturalistic fMRI datasets (Sherlock and Forrest Gump), our framework
outperforms group-based baselines by recovering more accurate and
individualized representations. We further show that the framework scales
efficiently to large datasets and generalizes well to new subjects. To test
this, we apply the framework to temporally unstructured resting-state fMRI data
from individuals with schizophrenia and healthy controls. We further apply our
method to a large resting-state fMRI dataset comprising individuals with
schizophrenia and controls. In this setting, we demonstrate that the framework
scales efficiently to large populations and generalizes robustly to unseen
subjects. The learned subject-specific spatial maps our model finds reveal
clinically relevant patterns, including increased activation in the basal
ganglia, visual, auditory, and somatosensory regions, and decreased activation
in the insula, inferior frontal gyrus, and angular gyrus. These findings
suggest that our framework can uncover clinically relevant subject-specific
brain activity patterns. Our approach thus provides a scalable and
individualized framework for modeling brain activity, with applications in
computational neuroscience and clinical research.

</details>


### [190] [Generative Machine Learning in Adaptive Control of Dynamic Manufacturing Processes: A Review](https://arxiv.org/abs/2505.00210)
*Suk Ki Lee, Hyunwoong Ko*

Main category: cs.LG

TL;DR: The paper reviews generative ML's role in dynamic manufacturing, classifying ML-enhanced control systems and highlighting gaps like generation-control separation and domain adaptation challenges. It proposes future research for integrated frameworks.


<details>
  <summary>Details</summary>
Motivation: Dynamic manufacturing's complexity and uncertainties require advanced monitoring and control. Generative ML shows promise but lacks a control-oriented perspective for actionable process controls.

Method: The review classifies ML-enhanced control into Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated approaches, analyzing generative ML architectures within this framework.

Result: Generative ML demonstrates potential in decision-making, process guidance, and digital twins but faces gaps like generation-control separation and insufficient physical understanding.

Conclusion: Future research should focus on integrated frameworks combining generative ML and control technologies to address dynamic manufacturing complexities.

Abstract: Dynamic manufacturing processes exhibit complex characteristics defined by
time-varying parameters, nonlinear behaviors, and uncertainties. These
characteristics require sophisticated in-situ monitoring techniques utilizing
multimodal sensor data and adaptive control systems that can respond to
real-time feedback while maintaining product quality. Recently, generative
machine learning (ML) has emerged as a powerful tool for modeling complex
distributions and generating synthetic data while handling these manufacturing
uncertainties. However, adopting these generative technologies in dynamic
manufacturing systems lacks a functional control-oriented perspective to
translate their probabilistic understanding into actionable process controls
while respecting constraints. This review presents a functional classification
of Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated
approaches, offering a perspective for understanding existing ML-enhanced
control systems and incorporating generative ML. The analysis of generative ML
architectures within this framework demonstrates control-relevant properties
and potential to extend current ML-enhanced approaches where conventional
methods prove insufficient. We show generative ML's potential for manufacturing
control through decision-making applications, process guidance, simulation, and
digital twins, while identifying critical research gaps: separation between
generation and control functions, insufficient physical understanding of
manufacturing phenomena, and challenges adapting models from other domains. To
address these challenges, we propose future research directions aimed at
developing integrated frameworks that combine generative ML and control
technologies to address the dynamic complexities of modern manufacturing
systems.

</details>


### [191] [Online Federation For Mixtures of Proprietary Agents with Black-Box Encoders](https://arxiv.org/abs/2505.00216)
*Xuwei Yang, Fatemeh Tavakoli, David B. Emerson, Anastasis Kratsios*

Main category: cs.LG

TL;DR: The paper addresses the challenge of optimizing proprietary AI models in ensemble settings by proposing a game-theoretic approach and a decentralized federated-learning algorithm, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Proprietary AIs limit user control over internal parameters, hindering optimization in ensemble models. The paper explores this problem through a non-competitive game-theoretic lens.

Method: A Nash equilibrium is derived in an online setting, and a decentralized federated-learning algorithm is implemented, allowing local optimization without sharing internal structures.

Result: The algorithm shows orders-of-magnitude improvements in predictive accuracy on real-world and synthetic benchmarks.

Conclusion: The proposed method effectively synchronizes competing proprietary AIs, offering a practical solution for ensemble optimization.

Abstract: Most industry-standard generative AIs and feature encoders are proprietary,
offering only black-box access: their outputs are observable, but their
internal parameters and architectures remain hidden from the end-user. This
black-box access is especially limiting when constructing mixture-of-expert
type ensemble models since the user cannot optimize each proprietary AI's
internal parameters. Our problem naturally lends itself to a non-competitive
game-theoretic lens where each proprietary AI (agent) is inherently competing
against the other AI agents, with this competition arising naturally due to
their obliviousness of the AI's to their internal structure. In contrast, the
user acts as a central planner trying to synchronize the ensemble of competing
AIs.
  We show the existence of the unique Nash equilibrium in the online setting,
which we even compute in closed-form by eliciting a feedback mechanism between
any given time series and the sequence generated by each (proprietary) AI
agent. Our solution is implemented as a decentralized, federated-learning
algorithm in which each agent optimizes their structure locally on their
machine without ever releasing any internal structure to the others. We obtain
refined expressions for pre-trained models such as transformers, random feature
models, and echo-state networks. Our ``proprietary federated learning''
algorithm is implemented on a range of real-world and synthetic time-series
benchmarks. It achieves orders-of-magnitude improvements in predictive accuracy
over natural benchmarks, of which there are surprisingly few due to this
natural problem still being largely unexplored.

</details>


### [192] [Predicting Estimated Times of Restoration for Electrical Outages Using Longitudinal Tabular Transformers](https://arxiv.org/abs/2505.00225)
*Bogireddy Sai Prasanna Teja, Valliappan Muthukaruppan, Carls Benjamin*

Main category: cs.LG

TL;DR: A Longitudinal Tabular Transformer (LTT) model improves ETR prediction accuracy by 19.08%, enhancing customer satisfaction during power outages.


<details>
  <summary>Details</summary>
Motivation: Climate variability increases the need for precise ETRs during disasters, but current methods lack accuracy.

Method: Proposes LTT model using historical outage data and sequential updates for better predictions.

Result: LTT improves CSI by 19.08% and introduces customer-informed metrics for better alignment with satisfaction.

Conclusion: LTT enhances accuracy and transparency, fostering trust in ETR predictions.

Abstract: As climate variability increases, the ability of utility providers to deliver
precise Estimated Times of Restoration (ETR) during natural disasters has
become increasingly critical. Accurate and timely ETRs are essential for
enabling customer preparedness during extended power outages, where informed
decision-making can be crucial, particularly in severe weather conditions.
Nonetheless, prevailing utility practices predominantly depend on manual
assessments or traditional statistical methods, which often fail to achieve the
level of precision required for reliable and actionable predictions. To address
these limitations, we propose a Longitudinal Tabular Transformer (LTT) model
that leverages historical outage event data along with sequential updates of
these events to improve the accuracy of ETR predictions. The model's
performance was evaluated over 34,000 storm-related outage events from three
major utility companies, collectively serving over 3 million customers over a
2-year period. Results demonstrate that the LTT model improves the Customer
Satisfaction Impact (CSI) metric by an average of 19.08% (p > 0.001) compared
to existing methods. Additionally, we introduce customer-informed regression
metrics that align model evaluation with real-world satisfaction, ensuring the
outcomes resonate with customer expectations. Furthermore, we employ
interpretability techniques to analyze the temporal significance of
incorporating sequential updates in modeling outage events and to identify the
contributions of predictive features to a given ETR. This comprehensive
approach not only improves predictive accuracy but also enhances transparency,
fostering greater trust in the model's capabilities.

</details>


### [193] [Field-scale soil moisture estimated from Sentinel-1 SAR data using a knowledge-guided deep learning approach](https://arxiv.org/abs/2505.00265)
*Yi Yu, Patrick Filippi, Thomas F. A. Bishop*

Main category: cs.LG

TL;DR: A knowledge-guided deep learning approach combining WCM principles with LSTM improves soil moisture estimation from Sentinel-1 SAR data, reducing uncertainties and enhancing accuracy.


<details>
  <summary>Details</summary>
Motivation: Current soil moisture estimation methods, like the water cloud model (WCM), are limited by empirical components and oversimplification in diverse landscapes.

Method: Integrates WCM into an LSTM model with a dual-component loss function, using Sentinel-1 SAR data, vegetation info, and surface characteristics.

Result: Reduced SM uncertainties by 0.02 m³/m³ and achieved R up to 0.64 in varied conditions.

Conclusion: The approach shows promise in addressing WCM limitations and improving SM estimation accuracy.

Abstract: Soil moisture (SM) estimation from active microwave data remains challenging
due to the complex interactions between radar backscatter and surface
characteristics. While the water cloud model (WCM) provides a semi-physical
approach for understanding these interactions, its empirical component often
limits performance across diverse agricultural landscapes. This research
presents preliminary efforts for developing a knowledge-guided deep learning
approach, which integrates WCM principles into a long short-term memory (LSTM)
model, to estimate field SM using Sentinel-1 Synthetic Aperture Radar (SAR)
data. Our proposed approach leverages LSTM's capacity to capture spatiotemporal
dependencies while maintaining physical consistency through a modified
dual-component loss function, including a WCM-based semi-physical component and
a boundary condition regularisation. The proposed approach is built upon the
soil backscatter coefficients isolated from the total backscatter, together
with Landsat-resolution vegetation information and surface characteristics. A
four-fold spatial cross-validation was performed against in-situ SM data to
assess the model performance. Results showed the proposed approach reduced SM
retrieval uncertainties by 0.02 m$^3$/m$^3$ and achieved correlation
coefficients (R) of up to 0.64 in areas with varying vegetation cover and
surface conditions, demonstrating the potential to address the
over-simplification in WCM.

</details>


### [194] [Scaling On-Device GPU Inference for Large Generative Models](https://arxiv.org/abs/2505.00232)
*Jiuqiang Tang, Raman Sarokin, Ekaterina Ignasheva, Grant Jensen, Lin Chen, Juhyun Lee, Andrei Kulik, Matthias Grundmann*

Main category: cs.LG

TL;DR: ML Drift is an optimized framework for on-device GPU-accelerated inference, enabling execution of generative AI models with 10-100x more parameters than current on-device models, while improving performance by an order of magnitude.


<details>
  <summary>Details</summary>
Motivation: The need for on-device inference due to privacy and efficiency concerns, despite server-based deployments offering peak performance.

Method: ML Drift extends state-of-the-art GPU-accelerated inference engines, addressing cross-GPU API challenges and ensuring compatibility across platforms.

Result: Achieves an order-of-magnitude performance improvement over existing open-source GPU inference engines.

Conclusion: ML Drift facilitates deployment of complex generative AI models on resource-constrained devices, broadening the scope of on-device AI applications.

Abstract: Driven by the advancements in generative AI, large machine learning models
have revolutionized domains such as image processing, audio synthesis, and
speech recognition. While server-based deployments remain the locus of peak
performance, the imperative for on-device inference, necessitated by privacy
and efficiency considerations, persists. Recognizing GPUs as the on-device ML
accelerator with the widest reach, we present ML Drift--an optimized framework
that extends the capabilities of state-of-the-art GPU-accelerated inference
engines. ML Drift enables on-device execution of generative AI workloads which
contain 10 to 100x more parameters than existing on-device generative AI
models. ML Drift addresses intricate engineering challenges associated with
cross-GPU API development, and ensures broad compatibility across mobile and
desktop/laptop platforms, thereby facilitating the deployment of significantly
more complex models on resource-constrained devices. Our GPU-accelerated ML/AI
inference engine achieves an order-of-magnitude performance improvement
relative to existing open-source GPU inference engines.

</details>


### [195] [Optimal Vector Compressed Sensing Using James Stein Shrinkage](https://arxiv.org/abs/2505.00326)
*Apratim Dey, David Donoho*

Main category: cs.LG

TL;DR: SteinSense is a lightweight, optimal algorithm for high-dimensional vector recovery, outperforming traditional convex optimization methods like Basis Pursuit.


<details>
  <summary>Details</summary>
Motivation: Traditional convex optimization methods for vector recovery are suboptimal, especially for large dimensions. SteinSense addresses this gap.

Method: SteinSense is a tuning-free, training-free, and sparsity-agnostic iterative algorithm, scalable to high dimensions.

Result: Extensive experiments confirm SteinSense's efficacy and robustness, even under non-ideal conditions.

Conclusion: SteinSense is a simple, scalable, and theoretically justified solution for high-dimensional vector recovery.

Abstract: The trend in modern science and technology is to take vector measurements
rather than scalars, ruthlessly scaling to ever higher dimensional vectors. For
about two decades now, traditional scalar Compressed Sensing has been
synonymous with a Convex Optimization based procedure called Basis Pursuit. In
the vector recovery case, the natural tendency is to return to a
straightforward vector extension of Basis Pursuit, also based on Convex
Optimization. However, Convex Optimization is provably suboptimal, particularly
when $B$ is large. In this paper, we propose SteinSense, a lightweight
iterative algorithm, which is provably optimal when $B$ is large. It does not
have any tuning parameter, does not need any training data, requires zero
knowledge of sparsity, is embarrassingly simple to implement, and all of this
makes it easily scalable to high vector dimensions. We conduct a massive volume
of both real and synthetic experiments that confirm the efficacy of SteinSense,
and also provide theoretical justification based on ideas from Approximate
Message Passing. Fascinatingly, we discover that SteinSense is quite robust,
delivering the same quality of performance on real data, and even under
substantial departures from conditions under which existing theory holds.

</details>


### [196] [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)
*Vishnu Sarukkai, Zhiqiang Xie, Kayvon Fatahalian*

Main category: cs.LG

TL;DR: LLM agents improve performance by learning from their own successful experiences, reducing reliance on task-specific knowledge engineering.


<details>
  <summary>Details</summary>
Motivation: To avoid labor-intensive task-specific knowledge engineering and enable LLM agents to learn automatically from their experiences.

Method: Construct and refine a database of self-generated examples, with extensions like database-level and exemplar-level selection.

Result: Performance improved on benchmarks (ALFWorld, Wordcraft, InterCode-SQL), matching or exceeding task-specific approaches.

Conclusion: Automatic trajectory database construction is a viable alternative to manual knowledge engineering.

Abstract: Many methods for improving Large Language Model (LLM) agents for sequential
decision-making tasks depend on task-specific knowledge engineering--such as
prompt tuning, curated in-context examples, or customized observation and
action spaces. Using these approaches, agent performance improves with the
quality or amount of knowledge engineering invested. Instead, we investigate
how LLM agents can automatically improve their performance by learning
in-context from their own successful experiences on similar tasks. Rather than
relying on task-specific knowledge engineering, we focus on constructing and
refining a database of self-generated examples. We demonstrate that even a
naive accumulation of successful trajectories across training tasks boosts test
performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),
and InterCode-SQL (75% to 79%)--matching the performance the initial agent
achieves if allowed two to three attempts per task. We then introduce two
extensions: (1) database-level selection through population-based training to
identify high-performing example collections, and (2) exemplar-level selection
that retains individual trajectories based on their empirical utility as
in-context examples. These extensions further enhance performance, achieving
91% on ALFWorld--matching more complex approaches that employ task-specific
components and prompts. Our results demonstrate that automatic trajectory
database construction offers a compelling alternative to labor-intensive
knowledge engineering.

</details>


### [197] [Node2Vec-DGI-EL: A Hierarchical Graph Representation Learning Model for Ingredient-Disease Association Prediction](https://arxiv.org/abs/2505.00236)
*Leifeng Zhang, Xin Dong, Shuaibing Jia, Jianhua Zhang*

Main category: cs.LG

TL;DR: The paper proposes a hierarchical graph representation learning model (Node2Vec-DGI-EL) to predict associations between Chinese medicine ingredients and diseases, achieving superior performance with AUC 0.9987 and AUPR 0.9545.


<details>
  <summary>Details</summary>
Motivation: Traditional Chinese medicine (TCM) contains active ingredients with therapeutic potential, but predicting their associations with diseases is complex. The study aims to develop a robust model for this purpose.

Method: The model combines Node2Vec for initial node embeddings, DGI for deep representation learning, and ensemble learning for accuracy. Theoretical verification and ablation experiments validate the approach.

Result: The model outperforms existing methods, with high AUC and AUPR scores. Case studies and molecular docking confirm predicted associations, like triptonide with hypertensive retinopathy.

Conclusion: The Node2Vec-DGI-EL model effectively predicts TCM ingredient-disease associations, overcoming reliance on node semantic information and demonstrating practical utility.

Abstract: Traditional Chinese medicine, as an essential component of traditional
medicine, contains active ingredients that serve as a crucial source for modern
drug development, holding immense therapeutic potential and development value.
A multi-layered and complex network is formed from Chinese medicine to diseases
and used to predict the potential associations between Chinese medicine
ingredients and diseases. This study proposes an ingredient-disease association
prediction model (Node2Vec-DGI-EL) based on hierarchical graph representation
learning. First, the model uses the Node2Vec algorithm to extract node
embedding vectors from the network as the initial features of the nodes. Next,
the network nodes are deeply represented and learned using the DGI algorithm to
enhance the model's expressive power. To improve prediction accuracy and
robustness, an ensemble learning method is incorporated to achieve more
accurate ingredient-disease association predictions. The effectiveness of the
model is then evaluated through a series of theoretical verifications. The
results demonstrated that the proposed model significantly outperformed
existing methods, achieving an AUC of 0.9987 and an AUPR of 0.9545, thereby
indicating superior predictive capability. Ablation experiments further
revealed the contribution and importance of each module. Additionally, case
studies explored potential associations, such as triptonide with hypertensive
retinopathy and methyl ursolate with colorectal cancer. Molecular docking
experiments validated these findings, showing the triptonide-PGR interaction
and the methyl ursolate-NFE2L2 interaction can bind stable. In conclusion, the
Node2Vec-DGI-EL model focuses on TCM datasets and effectively predicts
ingredient-disease associations, overcoming the reliance on node semantic
information.

</details>


### [198] [Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation](https://arxiv.org/abs/2505.00257)
*Zhizhong Tan, Jiexin Zheng, Kevin Qi Zhang, Wenyong Wang*

Main category: cs.LG

TL;DR: A Heterogeneous Federated Graph Neural Network (HFGNN) is proposed to address privacy issues in financial data sharing, enabling secure data flow and joint analysis across platforms.


<details>
  <summary>Details</summary>
Motivation: Financial institutions face challenges in sharing data due to privacy concerns, leading to low data openness and platform interconnection.

Method: HFGNN models heterogeneous business data as subgraphs, constructs a global graph via a central server, and uses local training for personalized models.

Result: Simulation experiments show HFGNN achieves higher accuracy and faster convergence than existing methods.

Conclusion: HFGNN effectively solves privacy issues in financial data sharing while improving performance.

Abstract: The sharing of external data has become a strong demand of financial
institutions, but the privacy issue has led to the difficulty of
interconnecting different platforms and the low degree of data openness. To
effectively solve the privacy problem of financial data in trans-border flow
and sharing, to ensure that the data is available but not visible, to realize
the joint portrait of all kinds of heterogeneous data of business organizations
in different industries, we propose a Heterogeneous Federated Graph Neural
Network (HFGNN) approach. In this method, the distribution of heterogeneous
business data of trans-border organizations is taken as subgraphs, and the
sharing and circulation process among subgraphs is constructed as a
statistically heterogeneous global graph through a central server. Each
subgraph learns the corresponding personalized service model through local
training to select and update the relevant subset of subgraphs with aggregated
parameters, and effectively separates and combines topological and feature
information among subgraphs. Finally, our simulation experimental results show
that the proposed method has higher accuracy performance and faster convergence
speed than existing methods.

</details>


### [199] [FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression](https://arxiv.org/abs/2403.16677)
*Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar*

Main category: cs.LG

TL;DR: FOOL is a task-agnostic feature compression method for nanosatellite constellations, reducing downlink costs while preserving prediction performance and image quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the downlink bottleneck in nanosatellite constellations by improving data transfer efficiency without relying on task-specific prior information.

Method: FOOL partitions high-resolution satellite imagery, embeds context, and leverages inter-tile dependencies for compression, tested under intermittent network conditions.

Result: FOOL reduces transfer costs significantly (over 100x data volume) and recovers images with competitive quality at lower bitrates.

Conclusion: FOOL is a practical solution for Orbital Edge Computing, enabling efficient data downlinking without compromising performance.

Abstract: Nanosatellite constellations equipped with sensors capturing large geographic
regions provide unprecedented opportunities for Earth observation. As
constellation sizes increase, network contention poses a downlink bottleneck.
Orbital Edge Computing (OEC) leverages limited onboard compute resources to
reduce transfer costs by processing the raw captures at the source. However,
current solutions have limited practicability due to reliance on crude
filtering methods or over-prioritizing particular downstream tasks. This work
presents FOOL, an OEC-native and task-agnostic feature compression method that
preserves prediction performance. FOOL partitions high-resolution satellite
imagery to maximize throughput. Further, it embeds context and leverages
inter-tile dependencies to lower transfer costs with negligible overhead. While
FOOL is a feature compressor, it can recover images with competitive scores on
quality measures at lower bitrates. We extensively evaluate transfer cost
reduction by including the peculiarity of intermittently available network
connections in low earth orbit. Lastly, we test the feasibility of our system
for standardized nanosatellite form factors. We demonstrate that FOOL permits
downlinking over 100x the data volume without relying on prior information on
the downstream tasks.

</details>


### [200] [Policies of Multiple Skill Levels for Better Strength Estimation in Games](https://arxiv.org/abs/2505.00279)
*Kyota Kuboki, Tatsuyoshi Ogawa, Chu-Hsuan Hsueh, Shi-Jim Yen, Kokolo Ikeda*

Main category: cs.LG

TL;DR: The paper improves human skill level estimation in games by incorporating behavior tendencies and policies from neural networks, achieving higher accuracy than previous methods.


<details>
  <summary>Details</summary>
Motivation: Accurate skill estimation is vital for effective human-AI interaction, ensuring AI provides suitable challenges or guidance.

Method: Combines strength scores with policies from neural networks trained on human match data to estimate skill levels.

Result: Achieved 80% accuracy (Go) with 10 matches, rising to 92% with 20 matches, outperforming prior methods by 8-9%. Similar improvements were seen in chess.

Conclusion: The method enhances strength estimation accuracy, improving human-AI interaction in games like Go and chess.

Abstract: Accurately estimating human skill levels is crucial for designing effective
human-AI interactions so that AI can provide appropriate challenges or
guidance. In games where AI players have beaten top human professionals,
strength estimation plays a key role in adapting AI behavior to match human
skill levels. In a previous state-of-the-art study, researchers have proposed a
strength estimator trained using human players' match data. Given some matches,
the strength estimator computes strength scores and uses them to estimate
player ranks (skill levels). In this paper, we focus on the observation that
human players' behavior tendency varies according to their strength and aim to
improve the accuracy of strength estimation by taking this into account.
Specifically, in addition to strength scores, we obtain policies for different
skill levels from neural networks trained using human players' match data. We
then combine features based on these policies with the strength scores to
estimate strength. We conducted experiments on Go and chess. For Go, our method
achieved an accuracy of 80% in strength estimation when given 10 matches, which
increased to 92% when given 20 matches. In comparison, the previous
state-of-the-art method had an accuracy of 71% with 10 matches and 84% with 20
matches, demonstrating improvements of 8-9%. We observed similar improvements
in chess. These results contribute to developing a more accurate strength
estimation method and to improving human-AI interaction.

</details>


### [201] [Multi-Hierarchical Fine-Grained Feature Mapping Driven by Feature Contribution for Molecular Odor Prediction](https://arxiv.org/abs/2505.00290)
*Hong Xin Xie, Jian De Sun, Fan Fu Xue, Zi Fei Han, Shan Shan Feng, Qi Chen*

Main category: cs.LG

TL;DR: The paper proposes HMFNet, a hierarchical multi-feature mapping network, to improve molecular odor prediction by addressing feature extraction and class imbalance issues.


<details>
  <summary>Details</summary>
Motivation: Accurate molecular odor prediction is challenging due to limited expressive power of existing methods and severe class imbalance, hindering AI model training.

Method: Introduces LMFE for atomic-level feature extraction, HMFM for dynamic feature importance learning, GMFE for global feature extraction, and CIL to mitigate class imbalance.

Result: The approach significantly improves performance across deep learning models, enhancing molecular structure representation.

Conclusion: HMFNet advances odor prediction and AI-driven technology development by addressing key challenges in feature extraction and imbalance.

Abstract: Molecular odor prediction is the process of using a molecule's structure to
predict its smell. While accurate prediction remains challenging, AI models can
suggest potential odors. Existing methods, however, often rely on basic
descriptors or handcrafted fingerprints, which lack expressive power and hinder
effective learning. Furthermore, these methods suffer from severe class
imbalance, limiting the training effectiveness of AI models. To address these
challenges, we propose a Feature Contribution-driven Hierarchical Multi-Feature
Mapping Network (HMFNet). Specifically, we introduce a fine-grained, Local
Multi-Hierarchy Feature Extraction module (LMFE) that performs deep feature
extraction at the atomic level, capturing detailed features crucial for odor
prediction. To enhance the extraction of discriminative atomic features, we
integrate a Harmonic Modulated Feature Mapping (HMFM). This module dynamically
learns feature importance and frequency modulation, improving the model's
capability to capture relevant patterns. Additionally, a Global Multi-Hierarchy
Feature Extraction module (GMFE) is designed to learn global features from the
molecular graph topology, enabling the model to fully leverage global
information and enhance its discriminative power for odor prediction. To
further mitigate the issue of class imbalance, we propose a Chemically-Informed
Loss (CIL). Experimental results demonstrate that our approach significantly
improves performance across various deep learning models, highlighting its
potential to advance molecular structure representation and accelerate the
development of AI-driven technologies.

</details>


### [202] [Repetition Makes Perfect: Recurrent Sum-GNNs Match Message Passing Limit](https://arxiv.org/abs/2505.00291)
*Eran Rosenbluth, Martin Grohe*

Main category: cs.LG

TL;DR: Recurrent GNNs with sum aggregation and ReLU activation can match the expressivity of the Weisfeiler-Leman algorithm, unlike non-recurrent GNNs. They can emulate any graph algorithm respecting this invariance, with polynomial overhead. Random initialization extends this to all polynomial-time graph algorithms.


<details>
  <summary>Details</summary>
Motivation: To determine the expressivity limits of recurrent GNNs and compare them to non-recurrent variants, particularly in relation to the Weisfeiler-Leman algorithm.

Method: Analyzing recurrent GNNs with sum aggregation and ReLU activation, proving their ability to emulate graph algorithms respecting Weisfeiler-Leman invariance. Random initialization is also explored.

Result: Recurrent GNNs reach the expressivity limit of Weisfeiler-Leman, unlike non-recurrent GNNs. They emulate algorithms with polynomial overhead and, with random initialization, all polynomial-time graph algorithms.

Conclusion: Recurrent GNNs are as expressive as the Weisfeiler-Leman algorithm and can emulate any polynomial-time graph algorithm with random initialization, highlighting their superior expressivity over non-recurrent GNNs.

Abstract: We provide first tight bounds for the expressivity of Recurrent Graph Neural
Networks (recurrent GNNs) with finite-precision parameters. We prove that
recurrent GNNs, with sum aggregation and ReLU activation, can emulate any graph
algorithm that respects the natural message-passing invariance induced by the
color refinement (or Weisfeiler-Leman) algorithm. While it is well known that
the expressive power of GNNs is limited by this invariance [Morris et al., AAAI
2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actually
reach this limit. This is in contrast to non-recurrent GNNs, which have the
power of Weisfeiler-Leman only in a very weak, "non-uniform", sense where every
graph size requires a different GNN model to compute with. The emulation we
construct introduces only a polynomial overhead in both time and space.
  Furthermore, we show that by incorporating random initialization, recurrent
GNNs can emulate all graph algorithms, implying in particular that any graph
algorithm with polynomial-time complexity can be emulated by a recurrent GNN
with random initialization, running in polynomial time.

</details>


### [203] [Temporal Attention Evolutional Graph Convolutional Network for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.00302)
*Xinlong Zhao, Liying Zhang, Tianbo Zou, Yan Zhang*

Main category: cs.LG

TL;DR: The paper introduces TAEGCN, a model for multivariate time series forecasting that captures dynamic graph structures and multi-scale temporal interactions to improve prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume fixed graph structures, but real-world scenarios involve dynamic graphs and varying time-scale interactions, necessitating a more adaptable approach.

Method: TAEGCN combines causal temporal convolution, multi-head self-attention, and dynamic graph construction to learn temporal and spatial features, integrating them into a unified neural network.

Result: TAEGCN outperforms existing methods on public transportation datasets (METR-LA and PEMS-BAY), demonstrating its effectiveness.

Conclusion: TAEGCN successfully captures temporal and spatial dependencies in dynamic graphs, offering superior forecasting performance.

Abstract: Multivariate time series forecasting enables the prediction of future states
by leveraging historical data, thereby facilitating decision-making processes.
Each data node in a multivariate time series encompasses a sequence of multiple
dimensions. These nodes exhibit interdependent relationships, forming a graph
structure. While existing prediction methods often assume a fixed graph
structure, many real-world scenarios involve dynamic graph structures.
Moreover, interactions among time series observed at different time scales vary
significantly. To enhance prediction accuracy by capturing precise temporal and
spatial features, this paper introduces the Temporal Attention Evolutional
Graph Convolutional Network (TAEGCN). This novel method not only integrates
causal temporal convolution and a multi-head self-attention mechanism to learn
temporal features of nodes, but also construct the dynamic graph structure
based on these temporal features to keep the consistency of the changing in
spatial feature with temporal series. TAEGCN adeptly captures temporal causal
relationships and hidden spatial dependencies within the data. Furthermore,
TAEGCN incorporates a unified neural network that seamlessly integrates these
components to generate final predictions. Experimental results conducted on two
public transportation network datasets, METR-LA and PEMS-BAY, demonstrate the
superior performance of the proposed model.

</details>


### [204] [Gateformer: Advancing Multivariate Time Series Forecasting through Temporal and Variate-Wise Attention with Gated Representations](https://arxiv.org/abs/2505.00307)
*Yu-Hsiang Lan, Anton Alyakin, Eric K. Oermann*

Main category: cs.LG

TL;DR: The paper proposes a Transformer-based method to model both temporal and variate dependencies in multivariate time series forecasting, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of integrating cross-time and cross-variate dependencies in Transformer architectures for efficient and accurate forecasting.

Method: Embeds variates independently for cross-time dynamics, uses attention for cross-variate dependencies, and employs gating to regulate information flow.

Result: Achieves state-of-the-art performance on 13 datasets, with up to 20.7% improvement over original models.

Conclusion: The method effectively models dependencies and enhances performance, with potential for integration into other Transformer-based forecasters.

Abstract: There has been a recent surge of interest in time series modeling using the
Transformer architecture. However, forecasting multivariate time series with
Transformer presents a unique challenge as it requires modeling both temporal
(cross-time) and variate (cross-variate) dependencies. While Transformer-based
models have gained popularity for their flexibility in capturing both
sequential and cross-variate relationships, it is unclear how to best integrate
these two sources of information in the context of the Transformer architecture
while optimizing for both performance and efficiency. We re-purpose the
Transformer architecture to effectively model both cross-time and cross-variate
dependencies. Our approach begins by embedding each variate independently into
a variate-wise representation that captures its cross-time dynamics, and then
models cross-variate dependencies through attention mechanisms on these learned
embeddings. Gating operations in both cross-time and cross-variate modeling
phases regulate information flow, allowing the model to focus on the most
relevant features for accurate predictions. Our method achieves
state-of-the-art performance across 13 real-world datasets and can be
seamlessly integrated into other Transformer-based and LLM-based forecasters,
delivering performance improvements up to 20.7\% over original models. Code is
available at this repository: https://github.com/nyuolab/Gateformer.

</details>


### [205] [Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing](https://arxiv.org/abs/2505.00315)
*Piotr Piękos, Róbert Csordás, Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: MoSA introduces dynamic, learned sparsity in attention mechanisms, reducing computational cost while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Address the excessive quadratic cost of self-attention in large language models and improve subquadratic attention methods.

Method: Proposes Mixture of Sparse Attention (MoSA), inspired by Mixture of Experts, to dynamically select tokens for each attention head, reducing complexity.

Result: MoSA outperforms dense baselines, achieving up to 27% better perplexity, and reduces resource usage (faster, less memory, smaller KV-cache).

Conclusion: MoSA is an efficient and effective alternative to dense self-attention, offering performance gains and resource savings.

Abstract: Recent advances in large language models highlighted the excessive quadratic
cost of self-attention. Despite the significant research efforts, subquadratic
attention methods still suffer from inferior performance in practice. We
hypothesize that dynamic, learned content-based sparsity can lead to more
efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),
a novel approach inspired by Mixture of Experts (MoE) with expert choice
routing. MoSA dynamically selects tokens for each attention head, allowing
arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of
length $T$, MoSA reduces the computational complexity of each attention head
from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same
computational budget, allowing higher specialization. We show that among the
tested sparse attention variants, MoSA is the only one that can outperform the
dense baseline, sometimes with up to 27% better perplexity for an identical
compute budget. MoSA can also reduce the resource usage compared to dense
self-attention. Despite using torch implementation without an optimized kernel,
perplexity-matched MoSA models are simultaneously faster in wall-clock time,
require less memory for training, and drastically reduce the size of the
KV-cache compared to the dense transformer baselines.

</details>


### [206] [Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture](https://arxiv.org/abs/2505.00316)
*Tien Comlekoglu, J. Quetzalcóatl Toledo-Marín, Tina Comlekoglu, Douglas W. DeSimone, Shayn M. Peirce, Geoffrey Fox, James A. Glazier*

Main category: cs.LG

TL;DR: A CNN surrogate model using U-Net architecture accelerates Cellular-Potts model (CPM) simulations by 590x, capturing emergent behaviors like vasculogenesis.


<details>
  <summary>Details</summary>
Motivation: CPMs are computationally expensive due to explicit modeling of interactions and PDEs, limiting scalability.

Method: Developed a U-Net-based CNN surrogate model trained to predict 100 Monte-Carlo steps ahead.

Result: Achieved 590x speedup, accurately capturing emergent behaviors like vessel sprouting and anastomosis.

Conclusion: Deep learning can efficiently replace CPMs, enabling faster, larger-scale biological simulations.

Abstract: The Cellular-Potts model is a powerful and ubiquitous framework for
developing computational models for simulating complex multicellular biological
systems. Cellular-Potts models (CPMs) are often computationally expensive due
to the explicit modeling of interactions among large numbers of individual
model agents and diffusive fields described by partial differential equations
(PDEs). In this work, we develop a convolutional neural network (CNN) surrogate
model using a U-Net architecture that accounts for periodic boundary
conditions. We use this model to accelerate the evaluation of a mechanistic CPM
previously used to investigate \textit{in vitro} vasculogenesis. The surrogate
model was trained to predict 100 computational steps ahead (Monte-Carlo steps,
MCS), accelerating simulation evaluations by a factor of 590 times compared to
CPM code execution. Over multiple recursive evaluations, our model effectively
captures the emergent behaviors demonstrated by the original Cellular-Potts
model of such as vessel sprouting, extension and anastomosis, and contraction
of vascular lacunae. This approach demonstrates the potential for deep learning
to serve as efficient surrogate models for CPM simulations, enabling faster
evaluation of computationally expensive CPM of biological processes at greater
spatial and temporal scales.

</details>


### [207] [Communication-Efficient Wireless Federated Fine-Tuning for Large-Scale AI Models](https://arxiv.org/abs/2505.00333)
*Bumjun Kim, Wan Choi*

Main category: cs.LG

TL;DR: A wireless federated LoRA fine-tuning framework is introduced to optimize learning and communication efficiency in federated learning, featuring adaptive sparsification and a two-stage algorithm for efficient training.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs face challenges in federated learning due to resource constraints and communication overhead. LoRA offers a solution, but further optimization is needed.

Method: Proposes SOFT (adaptive sparsification) and TSFA (two-stage algorithm) to streamline parameter updates and dynamically adjust training parameters.

Result: Achieves accuracy comparable to ideal models while significantly reducing communication overhead.

Conclusion: The framework enables scalable, resource-efficient deployment of large models in wireless FL settings.

Abstract: Transformer-based large language models (LLMs) have achieved remarkable
success across various tasks. Yet, fine-tuning such massive models in federated
learning (FL) settings poses significant challenges due to resource constraints
and communication overhead. Low-Rank Adaptation (LoRA) addresses these issues
by training compact, low-rank matrices instead of fully fine-tuning large
models. This paper introduces a wireless federated LoRA fine-tuning framework
that optimizes both learning performance and communication efficiency. We
provide a novel convergence analysis, revealing how LoRA rank and covariance
effects influence FL training dynamics. Leveraging these insights, we propose
Sparsified Orthogonal Fine-Tuning (\textbf{SOFT}), an adaptive sparsification
method that streamlines parameter updates without expensive matrix
multiplications and singular value decomposition (SVD) operations.
Additionally, we present a Two Stage Federated Algorithm (\textbf{TSFA})
algorithm that pre-determines key parameters offline and dynamically adjusts
bandwidth and sparsification online, ensuring efficient training under latency
constraints. Experiments on benchmark datasets show that our approach achieves
accuracy comparable to ideal scenario models while significantly reducing
communication overhead. Our framework thus enables scalable, resource-efficient
deployment of large models in real-world wireless FL scenarios.

</details>


### [208] [T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation](https://arxiv.org/abs/2505.00337)
*Xuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, Jiale Zhao*

Main category: cs.LG

TL;DR: The paper introduces T2VPhysBench, a benchmark to evaluate text-to-video models' adherence to physical laws, revealing significant shortcomings despite their high-quality outputs.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video models produce visually appealing content but often violate basic physical laws, lacking rigorous evaluation.

Method: The study uses T2VPhysBench to assess compliance with 12 core physical laws through human evaluation, prompt-hint ablation, and counterfactual robustness tests.

Result: All models scored below 0.60 on average in adhering to physical laws, with detailed hints failing to improve performance and models often breaking rules when instructed.

Conclusion: The findings highlight limitations in current architectures and provide insights for developing physics-aware video generation models.

Abstract: Text-to-video generative models have made significant strides in recent
years, producing high-quality videos that excel in both aesthetic appeal and
accurate instruction following, and have become central to digital art creation
and user engagement online. Yet, despite these advancements, their ability to
respect fundamental physical laws remains largely untested: many outputs still
violate basic constraints such as rigid-body collisions, energy conservation,
and gravitational dynamics, resulting in unrealistic or even misleading
content. Existing physical-evaluation benchmarks typically rely on automatic,
pixel-level metrics applied to simplistic, life-scenario prompts, and thus
overlook both human judgment and first-principles physics. To fill this gap, we
introduce \textbf{T2VPhysBench}, a first-principled benchmark that
systematically evaluates whether state-of-the-art text-to-video systems, both
open-source and commercial, obey twelve core physical laws including Newtonian
mechanics, conservation principles, and phenomenological effects. Our benchmark
employs a rigorous human evaluation protocol and includes three targeted
studies: (1) an overall compliance assessment showing that all models score
below 0.60 on average in each law category; (2) a prompt-hint ablation
revealing that even detailed, law-specific hints fail to remedy physics
violations; and (3) a counterfactual robustness test demonstrating that models
often generate videos that explicitly break physical rules when so instructed.
The results expose persistent limitations in current architectures and offer
concrete insights for guiding future research toward truly physics-aware video
generation.

</details>


### [209] [Pushing the Limits of Low-Bit Optimizers: A Focus on EMA Dynamics](https://arxiv.org/abs/2505.00347)
*Cong Xu, Wenbin Liang, Mo Yu, Anan Liu, Ke-Yue Zhang, Lizhuang Ma, Jianyong Wang, Jun Wang, Wei Zhang*

Main category: cs.LG

TL;DR: SOLO introduces a lightweight optimizer with ultra-low-precision quantization (3-2 bits), solving signal swamping and gradient variance issues, saving memory (45GB for 7B model) with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address prohibitive training costs from large model sizes and stateful optimizers' auxiliary information overhead.

Method: Uses ultra-low-precision quantization (3-2 bits), tackles signal swamping with logarithmic quantization and gradient variance with precision-specific momentum.

Result: Achieves substantial memory savings (~45GB for 7B model) with minimal accuracy loss.

Conclusion: SOLO reduces computational resource bottlenecks, enhancing accessibility in fundamental research.

Abstract: The explosion in model sizes leads to continued growth in prohibitive
training/fine-tuning costs, particularly for stateful optimizers which maintain
auxiliary information of even 2x the model size to achieve optimal convergence.
We therefore present in this work a novel type of optimizer that carries with
extremely lightweight state overloads, achieved through ultra-low-precision
quantization. While previous efforts have achieved certain success with 8-bit
or 4-bit quantization, our approach enables optimizers to operate at precision
as low as 3 bits, or even 2 bits per state element. This is accomplished by
identifying and addressing two critical challenges: the signal swamping problem
in unsigned quantization that results in unchanged state dynamics, and the
rapidly increased gradient variance in signed quantization that leads to
incorrect descent directions. The theoretical analysis suggests a tailored
logarithmic quantization for the former and a precision-specific momentum value
for the latter. Consequently, the proposed SOLO achieves substantial memory
savings (approximately 45 GB when training a 7B model) with minimal accuracy
loss. We hope that SOLO can contribute to overcoming the bottleneck in
computational resources, thereby promoting greater accessibility in fundamental
research.

</details>


### [210] [Validation of a 24-hour-ahead Prediction model for a Residential Electrical Load under diverse climate](https://arxiv.org/abs/2505.00348)
*Ehtisham Asghar, Martin Hill, Ibrahim Sengor, Conor Lynch, Phan Quang An*

Main category: cs.LG

TL;DR: A global model for 24-hour-ahead hourly electrical energy demand prediction is proposed, effective across diverse climates and limited datasets, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing models are region-specific and rely on large datasets, limiting flexibility and accuracy in diverse climates or with limited data.

Method: A global model is developed and tested using data from Ireland (maritime climate) and Vietnam (tropical climate), evaluated against state-of-the-art methods.

Result: The model achieves high accuracy (MAPE of 8.0% and 4.0% for Ireland and Vietnam) even with limited data (nine months) and performs robustly across seasons.

Conclusion: The model enhances efficiency and sustainability of Energy Communities globally, regardless of climatic conditions or data availability.

Abstract: Accurate household electrical energy demand prediction is essential for
effectively managing sustainable Energy Communities. Integrated with the Energy
Management System, these communities aim to optimise operational costs.
However, most existing forecasting models are region-specific and depend on
large datasets, limiting their applicability across different climates and
geographical areas. These models often lack flexibility and may not perform
well in regions with limited historical data, leading to inaccurate
predictions. This paper proposes a global model for 24-hour-ahead hourly
electrical energy demand prediction that is designed to perform effectively
across diverse climate conditions and datasets. The model's efficiency is
demonstrated using data from two distinct regions: Ireland, with a maritime
climate and Vietnam, with a tropical climate. Remarkably, the model achieves
high accuracy even with a limited dataset spanning only nine months. Its
robustness is further validated across different seasons in Ireland (summer and
winter) and Vietnam (dry and wet). The proposed model is evaluated against
state-of-the-art machine learning and deep learning methods. Simulation results
indicate that the model consistently outperforms benchmark models, showcasing
its capability to provide reliable forecasts globally, regardless of varying
climatic conditions and data availability. This research underscores the
model's potential to enhance the efficiency and sustainability of Energy
Communities worldwide. The proposed model achieves a Mean Absolute Percentage
Error of 8.0% and 4.0% on the full Irish and Vietnamese datasets.

</details>


### [211] [Optimizing Deep Neural Networks using Safety-Guided Self Compression](https://arxiv.org/abs/2505.00350)
*Mohammad Zbeeb, Mariam Salman, Mohammad Bazzi, Ammar Mohanna*

Main category: cs.LG

TL;DR: A safety-driven quantization framework is introduced to compress deep neural networks while preserving performance, achieving better accuracy and reduced model size compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: The need for efficient model compression on resource-constrained devices without sacrificing performance.

Method: A novel safety-driven quantization framework using preservation sets to prune and quantize weights, tested on CNN and attention-based models.

Result: Up to 2.5% accuracy improvement and 60% model size reduction, outperforming traditional quantization techniques.

Conclusion: Safety-driven quantization is effective for optimizing deep learning models, balancing size and performance.

Abstract: The deployment of deep neural networks on resource-constrained devices
necessitates effective model com- pression strategies that judiciously balance
the reduction of model size with the preservation of performance. This study
introduces a novel safety-driven quantization framework that leverages
preservation sets to systematically prune and quantize neural network weights,
thereby optimizing model complexity without compromising accuracy. The proposed
methodology is rigorously evaluated on both a convolutional neural network
(CNN) and an attention-based language model, demonstrating its applicability
across diverse architectural paradigms. Experimental results reveal that our
framework achieves up to a 2.5% enhancement in test accuracy relative to the
original unquantized models while maintaining 60% of the initial model size. In
comparison to conventional quantization techniques, our approach not only
augments generalization by eliminating parameter noise and retaining essential
weights but also reduces variance, thereby ensuring the retention of critical
model features. These findings underscore the efficacy of safety-driven
quantization as a robust and reliable strategy for the efficient optimization
of deep learn- ing models. The implementation and comprehensive experimental
evaluations of our framework are publicly accessible at GitHub.

</details>


### [212] [R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training](https://arxiv.org/abs/2505.00358)
*Albert Ge, Tzu-Heng Huang, John Cooper, Avi Trost, Ziyi Chu, Satya Sai Srinath Namburi GNVV, Ziyang Cai, Kendall Park, Nicholas Roberts, Frederic Sala*

Main category: cs.LG

TL;DR: R&B is a framework that improves data mixing for training language models by regrouping data based on semantic similarity and optimizing composition efficiently, outperforming existing methods with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing data mixing strategies are limited by predetermined domains and high computational costs, leaving performance gains untapped.

Method: R&B uses semantic similarity to regroup data and optimizes composition via a Gram matrix from domain gradients, avoiding extra compute for evaluations.

Result: R&B matches or exceeds state-of-the-art performance on diverse datasets with only 0.01% additional compute overhead.

Conclusion: R&B effectively addresses flaws in current data mixing methods, offering a scalable and efficient solution for training language models.

Abstract: Data mixing strategies have successfully reduced the costs involved in
training language models. While promising, such methods suffer from two flaws.
First, they rely on predetermined data domains (e.g., data sources, task
types), which may fail to capture critical semantic nuances, leaving
performance on the table. Second, these methods scale with the number of
domains in a computationally prohibitive way. We address these challenges via
R&B, a framework that re-partitions training data based on semantic similarity
(Regroup) to create finer-grained domains, and efficiently optimizes the data
composition (Balance) by leveraging a Gram matrix induced by domain gradients
obtained throughout training. Unlike prior works, it removes the need for
additional compute to obtain evaluation information such as losses or
gradients. We analyze this technique under standard regularity conditions and
provide theoretical insights that justify R&B's effectiveness compared to
non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness
of R&B on five diverse datasets ranging from natural language to reasoning and
multimodal tasks. With as little as 0.01% additional compute overhead, R&B
matches or exceeds the performance of state-of-the-art data mixing strategies.

</details>


### [213] [TNStream: Applying Tightest Neighbors to Micro-Clusters to Define Multi-Density Clusters in Streaming Data](https://arxiv.org/abs/2505.00359)
*Qifen Zeng, Haomin Bao, Yuanzhuo Hu, Zirui Zhang, Yuheng Zheng, Luosheng Wen*

Main category: cs.LG

TL;DR: The paper proposes TNStream, a density-based clustering algorithm for data streams, addressing challenges like multi-density, high-dimensional data, and outlier resistance. It introduces the Tightest Neighbors concept and Skeleton Set theory, achieving improved clustering quality.


<details>
  <summary>Details</summary>
Motivation: Existing density-based clustering algorithms struggle with multi-density, high-dimensional data and outlier resistance, leading to degraded clustering quality.

Method: TNStream uses Tightest Neighbors and Skeleton Set theory to adaptively determine clustering radius. It employs Locality-Sensitive Hashing (LSH) for high-dimensional efficiency and forms final clusters via micro-clusters.

Result: Experiments show TNStream improves clustering quality for multi-density data and validates the proposed theory.

Conclusion: TNStream effectively addresses challenges in data stream clustering, offering a robust solution for complex data scenarios.

Abstract: In data stream clustering, systematic theory of stream clustering algorithms
remains relatively scarce. Recently, density-based methods have gained
attention. However, existing algorithms struggle to simultaneously handle
arbitrarily shaped, multi-density, high-dimensional data while maintaining
strong outlier resistance. Clustering quality significantly deteriorates when
data density varies complexly. This paper proposes a clustering algorithm based
on the novel concept of Tightest Neighbors and introduces a data stream
clustering theory based on the Skeleton Set. Based on these theories, this
paper develops a new method, TNStream, a fully online algorithm. The algorithm
adaptively determines the clustering radius based on local similarity,
summarizing the evolution of multi-density data streams in micro-clusters. It
then applies a Tightest Neighbors-based clustering algorithm to form final
clusters. To improve efficiency in high-dimensional cases, Locality-Sensitive
Hashing (LSH) is employed to structure micro-clusters, addressing the challenge
of storing k-nearest neighbors. TNStream is evaluated on various synthetic and
real-world datasets using different clustering metrics. Experimental results
demonstrate its effectiveness in improving clustering quality for multi-density
data and validate the proposed data stream clustering theory.

</details>


### [214] [From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks](https://arxiv.org/abs/2505.00364)
*Jie Yang, Yuwen Wang, Kaixuan Chen, Tongya Zheng, Yihe Zhou, Zhenbang Xiao, Ji Cao, Mingli Song, Shunyu Liu*

Main category: cs.LG

TL;DR: The paper introduces TIF, a Tree-like Interpretable Framework for GNNs, addressing limitations of existing methods by capturing multi-granular graph relationships and improving interpretability without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Existing subgraph-based interpretable GNNs focus too much on local structures and lack flexibility in capturing varying granularities of relationships in real-world graphs.

Method: TIF transforms GNNs into hierarchical trees with coarsened graphs at different granularities, using graph coarsening, perturbation, and adaptive routing modules.

Result: TIF outperforms in interpretability and matches state-of-the-art prediction performance on graph classification benchmarks.

Conclusion: TIF provides a flexible, multi-granular interpretable framework for GNNs, enhancing both understanding and performance.

Abstract: Interpretable Graph Neural Networks (GNNs) aim to reveal the underlying
reasoning behind model predictions, attributing their decisions to specific
subgraphs that are informative. However, existing subgraph-based interpretable
methods suffer from an overemphasis on local structure, potentially overlooking
long-range dependencies within the entire graphs. Although recent efforts that
rely on graph coarsening have proven beneficial for global interpretability,
they inevitably reduce the graphs to a fixed granularity. Such an inflexible
way can only capture graph connectivity at a specific level, whereas real-world
graph tasks often exhibit relationships at varying granularities (e.g.,
relevant interactions in proteins span from functional groups, to amino acids,
and up to protein domains). In this paper, we introduce a novel Tree-like
Interpretable Framework (TIF) for graph classification, where plain GNNs are
transformed into hierarchical trees, with each level featuring coarsened graphs
of different granularity as tree nodes. Specifically, TIF iteratively adopts a
graph coarsening module to compress original graphs (i.e., root nodes of trees)
into increasingly coarser ones (i.e., child nodes of trees), while preserving
diversity among tree nodes within different branches through a dedicated graph
perturbation module. Finally, we propose an adaptive routing module to identify
the most informative root-to-leaf paths, providing not only the final
prediction but also the multi-granular interpretability for the decision-making
process. Extensive experiments on the graph classification benchmarks with both
synthetic and real-world datasets demonstrate the superiority of TIF in
interpretability, while also delivering a competitive prediction performance
akin to the state-of-the-art counterparts.

</details>


### [215] [SacFL: Self-Adaptive Federated Continual Learning for Resource-Constrained End Devices](https://arxiv.org/abs/2505.00365)
*Zhengyi Zhong, Weidong Bao, Ji Wang, Jianguo Chen, Lingjuan Lyu, Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: The paper proposes SacFL, a federated continual learning (FCL) framework, to address challenges like limited storage, poor task shift detection, and adversarial tasks in on-device machine learning. It uses an Encoder-Decoder architecture and contrastive learning for efficiency and autonomy.


<details>
  <summary>Details</summary>
Motivation: The dynamic nature of data in distributed computing requires continual learning (CL), but traditional centralized CL is unsuitable for end devices due to privacy and data volume issues. Federated continual learning (FCL) is proposed to address these challenges.

Method: SacFL employs an Encoder-Decoder architecture to separate task-robust and task-sensitive components, reducing storage needs. It uses contrastive learning for autonomous task shift detection and attack defense.

Result: Experiments on datasets like Cifar100 and THUCNews show SacFL's effectiveness in class-incremental and domain-incremental scenarios. A demo system confirms its practicality.

Conclusion: SacFL is a practical FCL solution for resource-constrained devices, offering efficient storage use, autonomous task detection, and robustness against adversarial tasks.

Abstract: The proliferation of end devices has led to a distributed computing paradigm,
wherein on-device machine learning models continuously process diverse data
generated by these devices. The dynamic nature of this data, characterized by
continuous changes or data drift, poses significant challenges for on-device
models. To address this issue, continual learning (CL) is proposed, enabling
machine learning models to incrementally update their knowledge and mitigate
catastrophic forgetting. However, the traditional centralized approach to CL is
unsuitable for end devices due to privacy and data volume concerns. In this
context, federated continual learning (FCL) emerges as a promising solution,
preserving user data locally while enhancing models through collaborative
updates. Aiming at the challenges of limited storage resources for CL, poor
autonomy in task shift detection, and difficulty in coping with new adversarial
tasks in FCL scenario, we propose a novel FCL framework named SacFL. SacFL
employs an Encoder-Decoder architecture to separate task-robust and
task-sensitive components, significantly reducing storage demands by retaining
lightweight task-sensitive components for resource-constrained end devices.
Moreover, $\rm{SacFL}$ leverages contrastive learning to introduce an
autonomous data shift detection mechanism, enabling it to discern whether a new
task has emerged and whether it is a benign task. This capability ultimately
allows the device to autonomously trigger CL or attack defense strategy without
additional information, which is more practical for end devices. Comprehensive
experiments conducted on multiple text and image datasets, such as Cifar100 and
THUCNews, have validated the effectiveness of $\rm{SacFL}$ in both
class-incremental and domain-incremental scenarios. Furthermore, a demo system
has been developed to verify its practicality.

</details>


### [216] [Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and Pickup Logistics Services](https://arxiv.org/abs/2505.00375)
*Jinhui Yi, Huan Yan, Haotian Wang, Jian Yuan, Yong Li*

Main category: cs.LG

TL;DR: TransPDT, a Transformer-based model, predicts package delivery time by addressing spatiotemporal dependencies, pickup impacts, and courier mobility patterns, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate delivery time estimation is crucial for logistics, especially in mixed scenarios where pickups disrupt courier behavior. Existing methods overlook pickup's greater impact due to tighter time constraints.

Method: TransPDT uses a Transformer encoder for spatiotemporal dependencies, a pattern memory for pickup effects, and an auxiliary route prediction task with courier movement regularities.

Result: Experiments on real datasets show TransPDT's superiority. It's deployed in JD Logistics, tracking 2000+ couriers handling hundreds of thousands of daily packages in Beijing.

Conclusion: TransPDT effectively addresses the challenges of delivery time prediction in mixed logistics scenarios, demonstrating practical utility in large-scale deployments.

Abstract: Accurately estimating package delivery time is essential to the logistics
industry, which enables reasonable work allocation and on-time service
guarantee. This becomes even more necessary in mixed logistics scenarios where
couriers handle a high volume of delivery and a smaller number of pickup
simultaneously. However, most of the related works treat the pickup and
delivery patterns on couriers' decision behavior equally, neglecting that the
pickup has a greater impact on couriers' decision-making compared to the
delivery due to its tighter time constraints. In such context, we have three
main challenges: 1) multiple spatiotemporal factors are intricately
interconnected, significantly affecting couriers' delivery behavior; 2) pickups
have stricter time requirements but are limited in number, making it
challenging to model their effects on couriers' delivery process; 3) couriers'
spatial mobility patterns are critical determinants of their delivery behavior,
but have been insufficiently explored. To deal with these, we propose TransPDT,
a Transformer-based multi-task package delivery time prediction model. We first
employ the Transformer encoder architecture to capture the spatio-temporal
dependencies of couriers' historical travel routes and pending package sets.
Then we design the pattern memory to learn the patterns of pickup in the
imbalanced dataset via attention mechanism. We also set the route prediction as
an auxiliary task of delivery time prediction, and incorporate the prior
courier spatial movement regularities in prediction. Extensive experiments on
real industry-scale datasets demonstrate the superiority of our method. A
system based on TransPDT is deployed internally in JD Logistics to track more
than 2000 couriers handling hundreds of thousands of packages per day in
Beijing.

</details>


### [217] [Approximation to Deep Q-Network by Stochastic Delay Differential Equations](https://arxiv.org/abs/2505.00382)
*Jianya Lu, Yingjun Mo*

Main category: cs.LG

TL;DR: The paper analyzes DQN's theoretical limitations, modeling it as an SDDE and proving convergence to zero distance with step size reduction, linking DQN techniques to continuous systems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of DQN's success, particularly its key techniques like experience replay and target networks.

Method: Constructs an SDDE based on DQN, estimates Wasserstein-1 distance, and uses Lindeberg principle and operator comparison for analysis.

Result: Proves the distance between DQN and SDDE converges to zero as step size decreases, showing target networks stabilize the system.

Conclusion: Provides a continuous-system perspective for DQN's techniques, enhancing theoretical understanding of its stability and convergence.

Abstract: Despite the significant breakthroughs that the Deep Q-Network (DQN) has
brought to reinforcement learning, its theoretical analysis remains limited. In
this paper, we construct a stochastic differential delay equation (SDDE) based
on the DQN algorithm and estimate the Wasserstein-1 distance between them. We
provide an upper bound for the distance and prove that the distance between the
two converges to zero as the step size approaches zero. This result allows us
to understand DQN's two key techniques, the experience replay and the target
network, from the perspective of continuous systems. Specifically, the delay
term in the equation, corresponding to the target network, contributes to the
stability of the system. Our approach leverages a refined Lindeberg principle
and an operator comparison to establish these results.

</details>


### [218] [DeepSTA: A Spatial-Temporal Attention Network for Logistics Delivery Timely Rate Prediction in Anomaly Conditions](https://arxiv.org/abs/2505.00402)
*Jinhui Yi, Huan Yan, Haotian Wang, Jian Yuan, Yong Li*

Main category: cs.LG

TL;DR: The paper proposes DeepSTA, a deep spatial-temporal attention model, to predict couriers' delivery timely rates, especially during anomalies like epidemics, outperforming baselines by 12.11% in MAE and 13.71% in MSE.


<details>
  <summary>Details</summary>
Motivation: Existing studies overlook logistics scenarios and fail to model abnormal events explicitly, leading to poor performance in anomaly conditions like epidemics.

Method: DeepSTA uses an anomaly spatio-temporal learning module with RNNs for incident modeling, Node2vec for road district correlations, and GNNs with LSTM for spatial-temporal dependencies. An anomaly pattern attention module with memory networks addresses insufficient training data.

Result: Experiments on COVID-19 logistics data show DeepSTA outperforms baselines by 12.11% (MAE) and 13.71% (MSE).

Conclusion: DeepSTA effectively improves delivery rate prediction during anomalies by explicitly modeling abnormal events and leveraging spatial-temporal dependencies.

Abstract: Prediction of couriers' delivery timely rates in advance is essential to the
logistics industry, enabling companies to take preemptive measures to ensure
the normal operation of delivery services. This becomes even more critical
during anomaly conditions like the epidemic outbreak, during which couriers'
delivery timely rate will decline markedly and fluctuates significantly.
Existing studies pay less attention to the logistics scenario. Moreover, many
works focusing on prediction tasks in anomaly scenarios fail to explicitly
model abnormal events, e.g., treating external factors equally with other
features, resulting in great information loss. Further, since some anomalous
events occur infrequently, traditional data-driven methods perform poorly in
these scenarios. To deal with them, we propose a deep spatial-temporal
attention model, named DeepSTA. To be specific, to avoid information loss, we
design an anomaly spatio-temporal learning module that employs a recurrent
neural network to model incident information. Additionally, we utilize Node2vec
to model correlations between road districts, and adopt graph neural networks
and long short-term memory to capture the spatial-temporal dependencies of
couriers. To tackle the issue of insufficient training data in abnormal
circumstances, we propose an anomaly pattern attention module that adopts a
memory network for couriers' anomaly feature patterns storage via attention
mechanisms. The experiments on real-world logistics datasets during the
COVID-19 outbreak in 2022 show the model outperforms the best baselines by
12.11% in MAE and 13.71% in MSE, demonstrating its superior performance over
multiple competitive baselines.

</details>


### [219] [Safety in the Face of Adversity: Achieving Zero Constraint Violation in Online Learning with Slowly Changing Constraints](https://arxiv.org/abs/2505.00398)
*Bassel Hamoud, Ilnura Usmanova, Kfir Y. Levy*

Main category: cs.LG

TL;DR: The paper introduces the first theoretical guarantees for zero constraint violation in Online Convex Optimization (OCO) with dynamic constraints, ensuring strict safety and sublinear regret using a primal-dual approach.


<details>
  <summary>Details</summary>
Motivation: Existing constrained OCO methods allow occasional safety breaches; this work aims to guarantee strict safety under gradually evolving constraints.

Method: A primal-dual approach with Online Gradient Ascent in the dual space, employing a dichotomous learning rate to balance safety and regret.

Result: The method achieves zero constraint violation and sublinear regret under gradually changing constraints.

Conclusion: This framework provides the first provable guarantees for absolute safety in OCO with dynamic constraints, advancing beyond prior work.

Abstract: We present the first theoretical guarantees for zero constraint violation in
Online Convex Optimization (OCO) across all rounds, addressing dynamic
constraint changes. Unlike existing approaches in constrained OCO, which allow
for occasional safety breaches, we provide the first approach for maintaining
strict safety under the assumption of gradually evolving constraints, namely
the constraints change at most by a small amount between consecutive rounds.
This is achieved through a primal-dual approach and Online Gradient Ascent in
the dual space. We show that employing a dichotomous learning rate enables
ensuring both safety, via zero constraint violation, and sublinear regret. Our
framework marks a departure from previous work by providing the first provable
guarantees for maintaining absolute safety in the face of changing constraints
in OCO.

</details>


### [220] [Per-Domain Generalizing Policies: On Validation Instances and Scaling Behavior](https://arxiv.org/abs/2505.00439)
*Timo P. Gros, Nicola J. Müller, Daniel Fiser, Isabel Valera, Verena Wolf, Jörg Hoffmann*

Main category: cs.LG

TL;DR: Dynamic validation sets improve scaling behavior of GNN policies across domains.


<details>
  <summary>Details</summary>
Motivation: To enhance scaling behavior in action policies by dynamically generating validation sets and refining evaluation methodology.

Method: Introduces dynamic validation set generation and systematic test instance creation for scaling evaluation.

Result: Dynamic validation improves GNN policies' scaling in all 9 tested domains.

Conclusion: Dynamic validation and refined evaluation methods effectively enhance policy scaling.

Abstract: Recent work has shown that successful per-domain generalizing action policies
can be learned. Scaling behavior, from small training instances to large test
instances, is the key objective; and the use of validation instances larger
than training instances is one key to achieve it. Prior work has used fixed
validation sets. Here, we introduce a method generating the validation set
dynamically, on the fly, increasing instance size so long as informative and
feasible.We also introduce refined methodology for evaluating scaling behavior,
generating test instances systematically to guarantee a given confidence in
coverage performance for each instance size. In experiments, dynamic validation
improves scaling behavior of GNN policies in all 9 domains used.

</details>


### [221] [Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis](https://arxiv.org/abs/2505.00410)
*Farhana Elias, Md Shihab Reza, Muhammad Zawad Mahmud, Samiha Islam*

Main category: cs.LG

TL;DR: The research uses ML and XAI to predict osteoporosis risk, with XGBoost achieving 91% accuracy. Age, hormonal changes, and family history are key predictors. Explainability is emphasized for clinical trust.


<details>
  <summary>Details</summary>
Motivation: Osteoporosis is often asymptomatic and untreated; early detection is crucial to prevent fractures. ML and XAI can improve prediction and transparency.

Method: Six ML classifiers (Random Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, Gradient Boosting) were tested using clinical, demographic, and lifestyle data. Hyperparameters were tuned with GridSearchCV. XAI tools (SHAP, LIME, Permutation Feature Importance) explained model decisions.

Result: XGBoost performed best (91% accuracy, 0.92 precision, 0.91 recall, 0.90 F1-score). Age, hormonal changes, and family history were top predictors.

Conclusion: Explainable ML models are vital for healthcare. Future work includes validation in diverse populations and adding biomarkers for better accuracy.

Abstract: The present research tackles the difficulty of predicting osteoporosis risk
via machine learning (ML) approaches, emphasizing the use of explainable
artificial intelligence (XAI) to improve model transparency. Osteoporosis is a
significant public health concern, sometimes remaining untreated owing to its
asymptomatic characteristics, and early identification is essential to avert
fractures. The research assesses six machine learning classifiers: Random
Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting
and utilizes a dataset based on clinical, demographic, and lifestyle variables.
The models are refined using GridSearchCV to calibrate hyperparameters, with
the objective of enhancing predictive efficacy. XGBoost had the greatest
accuracy (91%) among the evaluated models, surpassing others in precision
(0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI
approaches, such as SHAP, LIME, and Permutation Feature Importance, to
elucidate the decision-making process of the optimal model. The study indicates
that age is the primary determinant in forecasting osteoporosis risk, followed
by hormonal alterations and familial history. These results corroborate
clinical knowledge and affirm the models' therapeutic significance. The
research underscores the significance of explainability in machine learning
models for healthcare applications, guaranteeing that physicians can rely on
the system's predictions. The report ultimately proposes directions for further
research, such as validation across varied populations and the integration of
supplementary biomarkers for enhanced predictive accuracy.

</details>


### [222] [CICADA: Cross-Domain Interpretable Coding for Anomaly Detection and Adaptation in Multivariate Time Series](https://arxiv.org/abs/2505.00415)
*Tian Lan, Yifei Gao, Yimeng Lu, Chen Zhang*

Main category: cs.LG

TL;DR: CICADA introduces a novel framework for unsupervised time series anomaly detection, addressing domain shifts and non-stationarity with interpretable, adaptive methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with data distribution shifts and non-stationarity in time series, failing to generalize across domains.

Method: CICADA uses a mixture of experts, selective meta-learning, adaptive expansion, and hierarchical attention for interpretable anomaly detection.

Result: Outperforms state-of-the-art methods in cross-domain detection and interpretability.

Conclusion: CICADA effectively addresses domain shifts and enhances interpretability in anomaly detection.

Abstract: Unsupervised Time series anomaly detection plays a crucial role in
applications across industries. However, existing methods face significant
challenges due to data distributional shifts across different domains, which
are exacerbated by the non-stationarity of time series over time. Existing
models fail to generalize under multiple heterogeneous source domains and
emerging unseen new target domains. To fill the research gap, we introduce
CICADA (Cross-domain Interpretable Coding for Anomaly Detection and
Adaptation), with four key innovations: (1) a mixture of experts (MOE)
framework that captures domain-agnostic anomaly features with high flexibility
and interpretability; (2) a novel selective meta-learning mechanism to prevent
negative transfer between dissimilar domains, (3) an adaptive expansion
algorithm for emerging heterogeneous domain expansion, and (4) a hierarchical
attention structure that quantifies expert contributions during fusion to
enhance interpretability further.Extensive experiments on synthetic and
real-world industrial datasets demonstrate that CICADA outperforms
state-of-the-art methods in both cross-domain detection performance and
interpretability.

</details>


### [223] [Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training](https://arxiv.org/abs/2505.00422)
*Yu Han, Aaron Ceross, Jeroen H. M. Bergmann*

Main category: cs.LG

TL;DR: A Transformer-based multimodal framework integrates text and images to classify medical device risk levels, achieving high accuracy (90.4%) and AUROC (97.9%) with cross-attention and self-training.


<details>
  <summary>Details</summary>
Motivation: Accurate classification of medical device risk levels is crucial for regulatory oversight and clinical safety.

Method: A Transformer-based multimodal framework with cross-attention and self-training integrates textual and visual data.

Result: The model achieves 90.4% accuracy and 97.9% AUROC, outperforming text-only (77.2%) and image-only (54.8%) baselines. Self-training improves SVM performance by 3.3% in accuracy.

Conclusion: Cross-modal attention and self-training enhance generalization, with pseudo-labeling proving effective under limited supervision.

Abstract: Accurate classification of medical device risk levels is essential for
regulatory oversight and clinical safety. We present a Transformer-based
multimodal framework that integrates textual descriptions and visual
information to predict device regulatory classification. The model incorporates
a cross-attention mechanism to capture intermodal dependencies and employs a
self-training strategy for improved generalization under limited supervision.
Experiments on a real-world regulatory dataset demonstrate that our approach
achieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming
text-only (77.2%) and image-only (54.8%) baselines. Compared to standard
multimodal fusion, the self-training mechanism improved SVM performance by 3.3
percentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1,
suggesting that pseudo-labeling can effectively enhance generalization under
limited supervision. Ablation studies further confirm the complementary
benefits of both cross-modal attention and self-training.

</details>


### [224] [A Generalised Framework for Property-Driven Machine Learning](https://arxiv.org/abs/2505.00466)
*Thomas Flinkow, Marco Casadio, Colin Kessler, Rosemary Monahan, Ekaterina Komendantskaya*

Main category: cs.LG

TL;DR: The paper proposes a unified framework combining adversarial training and differentiable logics for property-driven machine learning, demonstrating its effectiveness on a drone control system.


<details>
  <summary>Details</summary>
Motivation: Neural networks often fail to meet safety and correctness properties post-training, necessitating methods that incorporate such properties directly during training.

Method: The framework unifies adversarial training (for robustness) and differentiable logics (for encoding logical constraints) to guide learning.

Result: The approach generalizes known properties and proves effective in a drone controller case study.

Conclusion: The unified framework offers a practical solution for property-driven machine learning, with code publicly available.

Abstract: Neural networks have been shown to frequently fail to satisfy critical safety
and correctness properties after training, highlighting the pressing need for
training methods that incorporate such properties directly. While adversarial
training can be used to improve robustness to small perturbations within
$\epsilon$-cubes, domains other than computer vision -- such as control systems
and natural language processing -- may require more flexible input region
specifications via generalised hyper-rectangles. Meanwhile, differentiable
logics offer a way to encode arbitrary logical constraints as additional loss
terms that guide the learning process towards satisfying these constraints. In
this paper, we investigate how these two complementary approaches can be
unified within a single framework for property-driven machine learning. We show
that well-known properties from the literature are subcases of this general
approach, and we demonstrate its practical effectiveness on a case study
involving a neural network controller for a drone system. Our framework is
publicly available at https://github.com/tflinkow/property-driven-ml.

</details>


### [225] [Interpretable Spatial-Temporal Fusion Transformers: Multi-Output Prediction for Parametric Dynamical Systems with Time-Varying Inputs](https://arxiv.org/abs/2505.00473)
*Shuwen Sun, Lihong Feng, Peter Benner*

Main category: cs.LG

TL;DR: A transformer model is adapted for predicting multiple outputs of parametric dynamical systems with time-varying inputs, improving interpretability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting outputs of systems influenced by both physical parameters and external time-varying signals.

Method: Extended a single-output transformer to a multiple-output transformer, using interpretable attention weights to analyze temporal and spatial correlations.

Result: The model accurately predicts multiple outputs, handling nonlinearity and high-dimensional parameter spaces effectively.

Conclusion: The multiple-output transformer offers a robust and interpretable solution for complex dynamical system predictions.

Abstract: We explore the promising performance of a transformer model in predicting
outputs of parametric dynamical systems with external time-varying input
signals. The outputs of such systems vary not only with physical parameters but
also with external time-varying input signals. Accurately catching the dynamics
of such systems is challenging. We have adapted and extended an existing
transformer model for single output prediction to a multiple-output transformer
that is able to predict multiple output responses of these systems. The
multiple-output transformer generalizes the interpretability of the original
transformer. The generalized interpretable attention weight matrix explores not
only the temporal correlations in the sequence, but also the interactions
between the multiple outputs, providing explanation for the spatial correlation
in the output domain. This multiple-output transformer accurately predicts the
sequence of multiple outputs, regardless of the nonlinearity of the system and
the dimensionality of the parameter space.

</details>


### [226] [Variational OOD State Correction for Offline Reinforcement Learning](https://arxiv.org/abs/2505.00503)
*Ke Jiang, Wen Jiang, Xiaoyang Tan*

Main category: cs.LG

TL;DR: Proposes Density-Aware Safety Perception (DASP) for OOD state correction in offline RL, prioritizing actions leading to higher data density for safer decisions.


<details>
  <summary>Details</summary>
Motivation: Addresses state distributional shift in offline RL by focusing on OOD state correction to ensure safer agent operation.

Method: Uses a variational framework to optimize actions based on outcome density, promoting in-distribution regions.

Result: Validated effectiveness on offline MuJoCo and AntMaze suites.

Conclusion: DASP successfully improves safety and performance in offline RL by addressing OOD states.

Abstract: The performance of Offline reinforcement learning is significantly impacted
by the issue of state distributional shift, and out-of-distribution (OOD) state
correction is a popular approach to address this problem. In this paper, we
propose a novel method named Density-Aware Safety Perception (DASP) for OOD
state correction. Specifically, our method encourages the agent to prioritize
actions that lead to outcomes with higher data density, thereby promoting its
operation within or the return to in-distribution (safe) regions. To achieve
this, we optimize the objective within a variational framework that
concurrently considers both the potential outcomes of decision-making and their
density, thus providing crucial contextual information for safe
decision-making. Finally, we validate the effectiveness and feasibility of our
proposed method through extensive experimental evaluations on the offline
MuJoCo and AntMaze suites.

</details>


### [227] [Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network](https://arxiv.org/abs/2505.00495)
*Nguyen Van Thanh, Nguyen Dang Huynh, Nguyen Ngoc Tan, Nguyen Thai Minh, Nguyen Nam Hoang*

Main category: cs.LG

TL;DR: An improved deep learning method using a Transformer network is proposed for accurate and efficient 6-hour storm trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: Forecasting storm paths is critical for safeguarding life and property, but it's challenging due to their unpredictable trajectories.

Method: The study employs a Transformer network trained on NOAA storm data to predict storm movement.

Result: The proposed method outperforms traditional techniques in accuracy, speed, and cost-effectiveness.

Conclusion: The Transformer-based approach offers a superior solution for storm trajectory forecasting.

Abstract: A storm is a type of extreme weather. Therefore, forecasting the path of a
storm is extremely important for protecting human life and property. However,
storm forecasting is very challenging because storm trajectories frequently
change. In this study, we propose an improved deep learning method using a
Transformer network to predict the movement trajectory of a storm over the next
6 hours. The storm data used to train the model was obtained from the National
Oceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that
the proposed method is more accurate than traditional methods. Moreover, the
proposed method is faster and more cost-effective

</details>


### [228] [Test-time Correlation Alignment](https://arxiv.org/abs/2505.00533)
*Linjing You, Jiabao Lu, Xiayuan Huang*

Main category: cs.LG

TL;DR: The paper introduces Test-time Correlation Alignment (TCA) to address challenges in Test-Time Adaptation (TTA), proposing two efficient algorithms (LinearTCA and LinearTCA+) that outperform existing methods with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Performance drops in deep neural networks due to distribution shifts and privacy concerns limiting access to training data motivate the need for TTA methods that avoid complex computations and domain forgetting.

Method: The authors propose TCA, aligning correlations between high-certainty and test instances, and introduce LinearTCA (a linear transformation for alignment) and LinearTCA+ (a plug-and-play enhancement for existing TTA methods).

Result: Experiments show TCA methods outperform baselines, with LinearTCA improving accuracy by 5.88% on OfficeHome while using only 4% GPU memory and 0.6% computation time of the best baseline.

Conclusion: TCA offers a theoretically grounded, efficient solution for TTA, achieving superior performance with minimal resource usage.

Abstract: Deep neural networks often experience performance drops due to distribution
shifts between training and test data. Although domain adaptation offers a
solution, privacy concerns restrict access to training data in many real-world
scenarios. This restriction has spurred interest in Test-Time Adaptation (TTA),
which adapts models using only unlabeled test data. However, current TTA
methods still face practical challenges: (1) a primary focus on instance-wise
alignment, overlooking CORrelation ALignment (CORAL) due to missing source
correlations; (2) complex backpropagation operations for model updating,
resulting in overhead computation and (3) domain forgetting.
  To address these challenges, we provide a theoretical analysis to investigate
the feasibility of Test-time Correlation Alignment (TCA), demonstrating that
correlation alignment between high-certainty instances and test instances can
enhance test performances with a theoretical guarantee. Based on this, we
propose two simple yet effective algorithms: LinearTCA and LinearTCA+.
LinearTCA applies a simple linear transformation to achieve both instance and
correlation alignment without additional model updates, while LinearTCA+ serves
as a plug-and-play module that can easily boost existing TTA methods. Extensive
experiments validate our theoretical insights and show that TCA methods
significantly outperforms baselines across various tasks, benchmarks and
backbones. Notably, LinearTCA improves adaptation accuracy by 5.88% on
OfficeHome dataset, while using only 4% maximum GPU memory usage and 0.6%
computation time compared to the best baseline TTA method.

</details>


### [229] [Self-Ablating Transformers: More Interpretability, Less Sparsity](https://arxiv.org/abs/2505.00509)
*Jeremias Ferrao, Luhan Mikaelson, Keenan Pepper, Natalia Perez-Campanero Antolin*

Main category: cs.LG

TL;DR: The paper introduces a self-ablation mechanism for transformers to study sparsity-interpretability links, showing improved feature localization and specialization without performance loss.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between sparsity and interpretability in machine learning, specifically in language transformers.

Method: A novel self-ablation mechanism enforces selective activation during training, tested on TinyStories with interpretability metrics.

Result: Self-ablation increases feature localization, neuron specialization, and interpretability, while reducing global sparsity.

Conclusion: Sparsity and interpretability have a nuanced relationship; self-ablation enhances interpretability by promoting local specialization over inactivity.

Abstract: A growing intuition in machine learning suggests a link between sparsity and
interpretability. We introduce a novel self-ablation mechanism to investigate
this connection ante-hoc in the context of language transformers. Our approach
dynamically enforces a k-winner-takes-all constraint, forcing the model to
demonstrate selective activation across neuron and attention units. Unlike
post-hoc methods that analyze already-trained models, our approach integrates
interpretability directly into model training, promoting feature localization
from inception. Training small models on the TinyStories dataset and employing
interpretability tests, we find that self-ablation leads to more localized
circuits, concentrated feature representations, and increased neuron
specialization without compromising language modelling performance.
Surprisingly, our method also decreased overall sparsity, indicating that
self-ablation promotes specialization rather than widespread inactivity. This
reveals a complex interplay between sparsity and interpretability, where
decreased global sparsity can coexist with increased local specialization,
leading to enhanced interpretability. To facilitate reproducibility, we make
our code available at
https://github.com/keenanpepper/self-ablating-transformers.

</details>


### [230] [Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks](https://arxiv.org/abs/2505.00530)
*Xinyu Wang, Jinbo Bi, Minghu Song*

Main category: cs.LG

TL;DR: PSV-PPO is a novel RL algorithm for SMILES-based molecule generation that prevents catastrophic forgetting and encourages exploration by validating partial SMILES in real-time.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting and lack of exploration in current RL methods for molecule generation, which degrade molecule validity during RL phases.

Method: Introduces Partial SMILES Validation-PPO (PSV-PPO), which performs stepwise validation of partial SMILES sequences during generation, evaluating all potential branches to detect invalidity early.

Result: PSV-PPO maintains high validity rates during exploration and reduces invalid structures, as demonstrated on PMO and GuacaMol benchmarks.

Conclusion: PSV-PPO effectively balances validity and exploration, with potential for future extensions to incorporate more domain knowledge in drug discovery.

Abstract: SMILES-based molecule generation has emerged as a powerful approach in drug
discovery. Deep reinforcement learning (RL) using large language model (LLM)
has been incorporated into the molecule generation process to achieve high
matching score in term of likelihood of desired molecule candidates. However, a
critical challenge in this approach is catastrophic forgetting during the RL
phase, where knowledge such as molecule validity, which often exceeds 99\%
during pretraining, significantly deteriorates. Current RL algorithms applied
in drug discovery, such as REINVENT, use prior models as anchors to retian
pretraining knowledge, but these methods lack robust exploration mechanisms. To
address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a
novel RL algorithm that incorporates real-time partial SMILES validation to
prevent catastrophic forgetting while encouraging exploration. Unlike
traditional RL approaches that validate molecule structures only after
generating entire sequences, PSV-PPO performs stepwise validation at each
auto-regressive step, evaluating not only the selected token candidate but also
all potential branches stemming from the prior partial sequence. This enables
early detection of invalid partial SMILES across all potential paths. As a
result, PSV-PPO maintains high validity rates even during aggressive
exploration of the vast chemical space. Our experiments on the PMO and GuacaMol
benchmark datasets demonstrate that PSV-PPO significantly reduces the number of
invalid generated structures while maintaining competitive exploration and
optimization performance. While our work primarily focuses on maintaining
validity, the framework of PSV-PPO can be extended in future research to
incorporate additional forms of valuable domain knowledge, further enhancing
reinforcement learning applications in drug discovery.

</details>


### [231] [KnowEEG: Explainable Knowledge Driven EEG Classification](https://arxiv.org/abs/2505.00541)
*Amarpal Sahota, Navid Mohammadi Foumani, Raul Santos-Rodriguez, Zahraa S. Abdallah*

Main category: cs.LG

TL;DR: KnowEEG is an explainable machine learning approach for EEG classification, combining per-electrode features and connectivity statistics, achieving high performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of explainability in deep learning models for EEG classification, which is critical for applications like healthcare.

Method: KnowEEG extracts per-electrode features, filters them statistically, integrates connectivity statistics, and uses a modified Random Forest (Fusion Forest) for classification.

Result: KnowEEG matches or surpasses state-of-the-art deep learning models in five EEG classification tasks and provides explainable feature importance.

Conclusion: KnowEEG offers a high-performance, explainable solution for EEG classification, validated by neuroscience literature, with significant potential in healthcare.

Abstract: Electroencephalography (EEG) is a method of recording brain activity that
shows significant promise in applications ranging from disease classification
to emotion detection and brain-computer interfaces. Recent advances in deep
learning have improved EEG classification performance yet model explainability
remains an issue. To address this key limitation of explainability we introduce
KnowEEG; a novel explainable machine learning approach for EEG classification.
KnowEEG extracts a comprehensive set of per-electrode features, filters them
using statistical tests, and integrates between-electrode connectivity
statistics. These features are then input to our modified Random Forest model
(Fusion Forest) that balances per electrode statistics with between electrode
connectivity features in growing the trees of the forest. By incorporating
knowledge from both the generalized time-series and EEG-specific domains,
KnowEEG achieves performance comparable to or exceeding state-of-the-art deep
learning models across five different classification tasks: emotion detection,
mental workload classification, eyes open/closed detection, abnormal EEG
classification, and event detection. In addition to high performance, KnowEEG
provides inherent explainability through feature importance scores for
understandable features. We demonstrate by example on the eyes closed/open
classification task that this explainability can be used to discover knowledge
about the classes. This discovered knowledge for eyes open/closed
classification was proven to be correct by current neuroscience literature.
Therefore, the impact of KnowEEG will be significant for domains where EEG
explainability is critical such as healthcare.

</details>


### [232] [Directly Forecasting Belief for Reinforcement Learning with Delays](https://arxiv.org/abs/2505.00546)
*Qingyuan Wu, Yuhui Wang, Simon Sinong Zhan, Yixuan Wang, Chung-Wei Lin, Chen Lv, Qi Zhu, Jürgen Schmidhuber, Chao Huang*

Main category: cs.LG

TL;DR: DFBT, a novel belief estimation method, directly forecasts states in RL with delays, reducing compounding errors and outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of compounding errors in RL with delays caused by recursive state forecasting.

Method: Propose DFBT, which directly forecasts states from observations without step-by-step intermediate estimation.

Result: DFBT reduces compounding errors, improves prediction accuracy, and enhances learning efficiency, outperforming SOTA baselines on MuJoCo.

Conclusion: DFBT offers a superior approach for RL with delays by minimizing compounding errors and improving performance.

Abstract: Reinforcement learning (RL) with delays is challenging as sensory perceptions
lag behind the actual events: the RL agent needs to estimate the real state of
its environment based on past observations. State-of-the-art (SOTA) methods
typically employ recursive, step-by-step forecasting of states. This can cause
the accumulation of compounding errors. To tackle this problem, our novel
belief estimation method, named Directly Forecasting Belief Transformer (DFBT),
directly forecasts states from observations without incrementally estimating
intermediate states step-by-step. We theoretically demonstrate that DFBT
greatly reduces compounding errors of existing recursively forecasting methods,
yielding stronger performance guarantees. In experiments with D4RL offline
datasets, DFBT reduces compounding errors with remarkable prediction accuracy.
DFBT's capability to forecast state sequences also facilitates multi-step
bootstrapping, thus greatly improving learning efficiency. On the MuJoCo
benchmark, our DFBT-based method substantially outperforms SOTA baselines.

</details>


### [233] [Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning](https://arxiv.org/abs/2504.21707)
*Anthony D Martin*

Main category: cs.LG

TL;DR: RKDO reframes representation learning as recursive KL divergence alignment, offering efficiency gains over static methods.


<details>
  <summary>Details</summary>
Motivation: To address the underplayed recursive structure in modern representation learning objectives and improve efficiency.

Method: Introduces Recursive KL Divergence Optimization (RKDO), a dynamic formalism evolving KL divergences across data neighborhoods.

Result: RKDO achieves ~30% lower loss and 60-80% computational savings compared to static methods.

Conclusion: RKDO provides a more efficient optimization landscape, beneficial for resource-constrained applications.

Abstract: We propose a generalization of modern representation learning objectives by
reframing them as recursive divergence alignment processes over localized
conditional distributions While recent frameworks like Information Contrastive
Learning I-Con unify multiple learning paradigms through KL divergence between
fixed neighborhood conditionals we argue this view underplays a crucial
recursive structure inherent in the learning process. We introduce Recursive KL
Divergence Optimization RKDO a dynamic formalism where representation learning
is framed as the evolution of KL divergences across data neighborhoods. This
formulation captures contrastive clustering and dimensionality reduction
methods as static slices while offering a new path to model stability and local
adaptation. Our experiments demonstrate that RKDO offers dual efficiency
advantages approximately 30 percent lower loss values compared to static
approaches across three different datasets and 60 to 80 percent reduction in
computational resources needed to achieve comparable results. This suggests
that RKDOs recursive updating mechanism provides a fundamentally more efficient
optimization landscape for representation learning with significant
implications for resource constrained applications.

</details>


### [234] [Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors](https://arxiv.org/abs/2505.00580)
*Xinyu Ding, Lexuan Chen, Siyu Liao, Zhongfeng Wang*

Main category: cs.LG

TL;DR: Proposes a method to reduce the complexity of fine-tuning foundation models by factorizing weights into interleaved circulant and diagonal matrices, using 1D FFT for efficiency.


<details>
  <summary>Details</summary>
Motivation: Foundation models are computationally expensive to fine-tune, limiting practical applicability.

Method: Factorizes weights into interleaved circulant and diagonal matrices, uses 1D FFT, and partitions circulant matrices for non-square weights.

Result: Achieves similar or better performance with fewer FLOPs and trainable parameters.

Conclusion: The method effectively reduces complexity while maintaining or improving model performance.

Abstract: Foundation models have achieved tremendous success in different domains.
However, their huge computation and storage complexity make these models
difficult to fine-tune and also less applicable in practice. Recent study shows
training in Fourier domain can be an effective fine-tuning method in terms of
both model performance and number of training parameters. In this work, we
propose to further reduce the complexity by the factorization through the
product of interleaved circulant and diagonal matrices. In addition, we address
the case of non-square fine-tuning weights by partitioning the circulant matrix
into blocks. Our method avoids the construction of weight change matrix and
utilizes 1D fast Fourier transform (FFT) instead of 2D FFT. Experimental
results show that our method achieves similar or better performance across
various tasks with much less floating-point operations (FLOPs) and the number
of trainable parameters.

</details>


### [235] [Fast and Low-Cost Genomic Foundation Models via Outlier Removal](https://arxiv.org/abs/2505.00598)
*Haozheng Luo, Chenghao Qiu, Maojiang Su, Zhihan Zhou, Zoe Mehta, Guo Ye, Jerry Yao-Chieh Hu, Han Liu*

Main category: cs.LG

TL;DR: GERM is the first unified benchmark for evaluating adversarial attack vulnerabilities in Genomic Foundation Models (GFMs), assessing robustness across architectures, quantization, and datasets.


<details>
  <summary>Details</summary>
Motivation: Existing GFM benchmarks lack comprehensive evaluation of adversarial vulnerabilities, prompting the need for GERM.

Method: GERM evaluates five GFMs using four attack algorithms and three defense strategies, analyzing vulnerabilities related to architecture, quantization, and datasets.

Result: Transformer-based models are more robust than HyenaDNA, and adversarial attacks often target biologically significant regions.

Conclusion: GERM provides a framework to assess GFM vulnerabilities, revealing architectural impacts and biological feature capture.

Abstract: We propose the first unified adversarial attack benchmark for Genomic
Foundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERM
offers the first comprehensive evaluation framework to systematically assess
the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate
the adversarial robustness of five state-of-the-art GFMs using four widely
adopted attack algorithms and three defense strategies. Importantly, our
benchmark provides an accessible and comprehensive framework to analyze GFM
vulnerabilities with respect to model architecture, quantization schemes, and
training datasets. Empirically, transformer-based models exhibit greater
robustness to adversarial perturbations compared to HyenaDNA, highlighting the
impact of architectural design on vulnerability. Moreover, adversarial attacks
frequently target biologically significant genomic regions, suggesting that
these models effectively capture meaningful sequence features.

</details>


### [236] [Unlocking the Potential of Linear Networks for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.00590)
*Chengsen Wang, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jianxin Liao*

Main category: cs.LG

TL;DR: AiT introduces an adaptive linear network and Transformer module to improve forecasting for irregular multivariate time series, outperforming state-of-the-art methods by 11% in accuracy and 52% in runtime.


<details>
  <summary>Details</summary>
Motivation: Traditional linear networks struggle with irregular multivariate time series due to intra-series inconsistency and inter-series asynchrony.

Method: AiT combines an adaptive linear network for dynamic weight adjustment and a Transformer module for variable correlation capture.

Result: AiT improves prediction accuracy by 11% and reduces runtime by 52% on benchmark datasets.

Conclusion: AiT effectively addresses challenges in irregular time series forecasting, offering superior performance and efficiency.

Abstract: Time series forecasting holds significant importance across various
industries, including finance, transportation, energy, healthcare, and climate.
Despite the widespread use of linear networks due to their low computational
cost and effectiveness in modeling temporal dependencies, most existing
research has concentrated on regularly sampled and fully observed multivariate
time series. However, in practice, we frequently encounter irregular
multivariate time series characterized by variable sampling intervals and
missing values. The inherent intra-series inconsistency and inter-series
asynchrony in such data hinder effective modeling and forecasting with
traditional linear networks relying on static weights. To tackle these
challenges, this paper introduces a novel model named AiT. AiT utilizes an
adaptive linear network capable of dynamically adjusting weights according to
observation time points to address intra-series inconsistency, thereby
enhancing the accuracy of temporal dependencies modeling. Furthermore, by
incorporating the Transformer module on variable semantics embeddings, AiT
efficiently captures variable correlations, avoiding the challenge of
inter-series asynchrony. Comprehensive experiments across four benchmark
datasets demonstrate the superiority of AiT, improving prediction accuracy by
11% and decreasing runtime by 52% compared to existing state-of-the-art
methods.

</details>


### [237] [MINERVA: Evaluating Complex Video Reasoning](https://arxiv.org/abs/2505.00681)
*Arsha Nagrani, Sachit Menon, Ahmet Iscen, Shyamal Buch, Ramin Mehran, Nilpa Jha, Anja Hauth, Yukun Zhu, Carl Vondrick, Mikhail Sirotenko, Cordelia Schmid, Tobias Weyand*

Main category: cs.LG

TL;DR: MINERVA is a new video reasoning dataset with detailed reasoning traces to evaluate multimodal LLMs, addressing gaps in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current video benchmarks lack intermediate reasoning steps, making it hard to assess if models truly reason or exploit biases.

Method: Introduce MINERVA, a multimodal dataset with diverse videos, complex questions, and hand-crafted reasoning traces.

Result: Benchmarking shows MINERVA challenges models; error analysis reveals temporal localization and visual perception as key failure modes.

Conclusion: MINERVA provides a robust tool for evaluating video reasoning, with public availability for further research.

Abstract: Multimodal LLMs are turning their focus to video benchmarks, however most
video benchmarks only provide outcome supervision, with no intermediate or
interpretable reasoning steps. This makes it challenging to assess if models
are truly able to combine perceptual and temporal information to reason about
videos, or simply get the correct answer by chance or by exploiting linguistic
biases. To remedy this, we provide a new video reasoning dataset called MINERVA
for modern multimodal models. Each question in the dataset comes with 5 answer
choices, as well as detailed, hand-crafted reasoning traces. Our dataset is
multimodal, diverse in terms of video domain and length, and consists of
complex multi-step questions. Extensive benchmarking shows that our dataset
provides a challenge for frontier open-source and proprietary models. We
perform fine-grained error analysis to identify common failure modes across
various models, and create a taxonomy of reasoning errors. We use this to
explore both human and LLM-as-a-judge methods for scoring video reasoning
traces, and find that failure modes are primarily related to temporal
localization, followed by visual perception errors, as opposed to logical or
completeness errors. The dataset, along with questions, answer candidates and
reasoning traces will be publicly available under
https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva.

</details>


### [238] [Explainable AI in Spatial Analysis](https://arxiv.org/abs/2505.00591)
*Ziqi Li*

Main category: cs.LG

TL;DR: The chapter explores how eXplainable AI (XAI) enhances spatial analysis by making machine learning models transparent, focusing on Shapley value-based methods and their application to spatial data like voting behaviors.


<details>
  <summary>Details</summary>
Motivation: Machine learning in spatial analysis lacks transparency, limiting understanding and reliability. XAI addresses this by explaining model outputs.

Method: Introduces Shapley value-based XAI methods, compares them with traditional spatial statistical techniques like geographically weighted regression, and applies them to county-level voting data.

Result: Demonstrates the utility of Shapley values in spatial analysis, showing their effectiveness in explaining model behavior compared to traditional methods.

Conclusion: Highlights challenges in current XAI techniques and suggests future directions for improving transparency in spatial analysis.

Abstract: This chapter discusses the opportunities of eXplainable Artificial
Intelligence (XAI) within the realm of spatial analysis. A key objective in
spatial analysis is to model spatial relationships and infer spatial processes
to generate knowledge from spatial data, which has been largely based on
spatial statistical methods. More recently, machine learning offers scalable
and flexible approaches that complement traditional methods and has been
increasingly applied in spatial data science. Despite its advantages, machine
learning is often criticized for being a black box, which limits our
understanding of model behavior and output. Recognizing this limitation, XAI
has emerged as a pivotal field in AI that provides methods to explain the
output of machine learning models to enhance transparency and understanding.
These methods are crucial for model diagnosis, bias detection, and ensuring the
reliability of results obtained from machine learning models. This chapter
introduces key concepts and methods in XAI with a focus on Shapley value-based
approaches, which is arguably the most popular XAI method, and their
integration with spatial analysis. An empirical example of county-level voting
behaviors in the 2020 Presidential election is presented to demonstrate the use
of Shapley values and spatial analysis with a comparison to multi-scale
geographically weighted regression. The chapter concludes with a discussion on
the challenges and limitations of current XAI techniques and proposes new
directions.

</details>


### [239] [OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery and Survival Stratification](https://arxiv.org/abs/2505.00650)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: OmicsCL is a contrastive learning framework for unsupervised disease subtype discovery from multi-omics data, integrating survival-aware loss to align representations with survival patterns.


<details>
  <summary>Details</summary>
Motivation: To advance personalized medicine by learning disease subtypes from multi-omics data without labeled outcomes.

Method: OmicsCL uses contrastive learning to embed heterogeneous omics modalities into a unified latent space, incorporating a survival-aware loss function.

Result: The framework identifies clinically meaningful clusters and aligns with patient survival, showing robustness and flexibility in hyperparameter tuning.

Conclusion: Contrastive learning, especially with survival-aware loss, is effective for discovering biological insights in high-dimensional omics data.

Abstract: Unsupervised learning of disease subtypes from multi-omics data presents a
significant opportunity for advancing personalized medicine. We introduce
OmicsCL, a modular contrastive learning framework that jointly embeds
heterogeneous omics modalities-such as gene expression, DNA methylation, and
miRNA expression-into a unified latent space. Our method incorporates a
survival-aware contrastive loss that encourages the model to learn
representations aligned with survival-related patterns, without relying on
labeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers
clinically meaningful clusters and achieves strong unsupervised concordance
with patient survival. The framework demonstrates robustness across
hyperparameter configurations and can be tuned to prioritize either subtype
coherence or survival stratification. Ablation studies confirm that integrating
survival-aware loss significantly enhances the predictive power of learned
embeddings. These results highlight the promise of contrastive objectives for
biological insight discovery in high-dimensional, heterogeneous omics data.

</details>


### [240] [Wasserstein Policy Optimization](https://arxiv.org/abs/2505.00663)
*David Pfau, Ian Davies, Diana Borsa, Joao G. M. Araujo, Brendan Tracey, Hado van Hasselt*

Main category: cs.LG

TL;DR: WPO is a reinforcement learning algorithm for continuous action spaces, combining deterministic and classic policy gradient methods, and outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To develop a general and efficient algorithm for reinforcement learning in continuous action spaces by leveraging Wasserstein gradient flow.

Method: WPO approximates Wasserstein gradient flow over policies, using a closed-form update that exploits action-value gradients and works with stochastic policies.

Result: WPO performs favorably on the DeepMind Control Suite and a fusion task compared to state-of-the-art methods.

Conclusion: WPO is a versatile and effective algorithm for continuous control tasks, combining strengths of deterministic and stochastic policy gradients.

Abstract: We introduce Wasserstein Policy Optimization (WPO), an actor-critic algorithm
for reinforcement learning in continuous action spaces. WPO can be derived as
an approximation to Wasserstein gradient flow over the space of all policies
projected into a finite-dimensional parameter space (e.g., the weights of a
neural network), leading to a simple and completely general closed-form update.
The resulting algorithm combines many properties of deterministic and classic
policy gradient methods. Like deterministic policy gradients, it exploits
knowledge of the gradient of the action-value function with respect to the
action. Like classic policy gradients, it can be applied to stochastic policies
with arbitrary distributions over actions -- without using the
reparameterization trick. We show results on the DeepMind Control Suite and a
magnetic confinement fusion task which compare favorably with state-of-the-art
continuous control methods.

</details>


### [241] [On the Importance of Gaussianizing Representations](https://arxiv.org/abs/2505.00685)
*Daniel Eftekhari, Vardan Papyan*

Main category: cs.LG

TL;DR: The paper introduces normality normalization, a new normalization layer for neural networks that promotes normal distribution in feature representations, leveraging the power transform and Gaussian noise, showing improved generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: The normal distribution's central role in information theory and its underutilization in neural network activation distributions motivated the development of normality normalization.

Method: The method involves a normalization layer using the power transform and additive Gaussian noise to encourage normality in feature representations.

Result: Experiments show normality normalization improves generalization, performs well across model variations, and enhances robustness to perturbations.

Conclusion: Normality normalization is effective and versatile, suitable for replacing conventional normalization layers in neural networks.

Abstract: The normal distribution plays a central role in information theory - it is at
the same time the best-case signal and worst-case noise distribution, has the
greatest representational capacity of any distribution, and offers an
equivalence between uncorrelatedness and independence for joint distributions.
Accounting for the mean and variance of activations throughout the layers of
deep neural networks has had a significant effect on facilitating their
effective training, but seldom has a prescription for precisely what
distribution these activations should take, and how this might be achieved,
been offered. Motivated by the information-theoretic properties of the normal
distribution, we address this question and concurrently present normality
normalization: a novel normalization layer which encourages normality in the
feature representations of neural networks using the power transform and
employs additive Gaussian noise during training. Our experiments
comprehensively demonstrate the effectiveness of normality normalization, in
regards to its generalization performance on an array of widely used model and
dataset combinations, its strong performance across various common factors of
variation such as model width, depth, and training minibatch size, its
suitability for usage wherever existing normalization layers are conventionally
used, and as a means to improving model robustness to random perturbations.

</details>


### [242] [Learning An Active Inference Model of Driver Perception and Control: Application to Vehicle Car-Following](https://arxiv.org/abs/2303.15201)
*Ran Wei, Anthony D. McDonald, Alfredo Garcia, Gustav Markkula, Johan Engstrom, Matthew O'Kelly*

Main category: cs.LG

TL;DR: A method for learning human perception and control models from demonstrations, using active inference theory and bi-level optimization, applied to car-following behavior.


<details>
  <summary>Details</summary>
Motivation: To develop a data-driven alternative to black-box models for understanding human sensorimotor control tasks.

Method: Bi-level optimization with structural assumptions on prior distributions, based on active inference theory.

Result: Successful estimation of a car-following behavior model, showing promise for learning active inference models from data.

Conclusion: Active inference models offer a viable alternative to black-box approaches for modeling human perception and control.

Abstract: In this paper we introduce a general estimation methodology for learning a
model of human perception and control in a sensorimotor control task based upon
a finite set of demonstrations. The model's structure consists of i the agent's
internal representation of how the environment and associated observations
evolve as a result of control actions and ii the agent's preferences over
observable outcomes. We consider a model's structure specification consistent
with active inference, a theory of human perception and behavior from cognitive
science. According to active inference, the agent acts upon the world so as to
minimize surprise defined as a measure of the extent to which an agent's
current sensory observations differ from its preferred sensory observations. We
propose a bi-level optimization approach to estimation which relies on a
structural assumption on prior distributions that parameterize the statistical
accuracy of the human agent's model of the environment. To illustrate the
proposed methodology, we present the estimation of a model for car-following
behavior based upon a naturalistic dataset. Overall, the results indicate that
learning active inference models of human perception and control from data is a
promising alternative to black-box models of driving.

</details>


### [243] [Omitted Labels Induce Nontransitive Paradoxes in Causality](https://arxiv.org/abs/2311.06840)
*Bijan Mazaheri, Siddharth Jain, Matthew Cook, Jehoshua Bruck*

Main category: cs.LG

TL;DR: The paper investigates omitted label contexts in training data, highlighting Simpson's paradox and nontransitive structures in networks of conclusions, linking them to ranked-choice voting.


<details>
  <summary>Details</summary>
Motivation: To understand the implications of limited training data (omitted label contexts) and explore paradoxes like Simpson's paradox in such settings.

Method: Study Simpson's paradox and generalize it to analyze networks of conclusions, identifying nontransitive structures.

Result: The space of possible nontransitive structures in these networks matches those from aggregating ranked-choice votes.

Conclusion: The findings connect omitted label contexts to nontransitive structures, revealing parallels with ranked-choice voting.

Abstract: We explore "omitted label contexts," in which training data is limited to a
subset of the possible labels. This setting is standard among specialized human
experts or specific, focused studies. By studying Simpson's paradox, we observe
that ``correct'' adjustments sometimes require non-exchangeable treatment and
control groups. A generalization of Simpson's paradox leads us to study
networks of conclusions drawn from different contexts, within which a paradox
of nontransitivity arises. We prove that the space of possible nontransitive
structures in these networks exactly corresponds to structures that form from
aggregating ranked-choice votes.

</details>


### [244] [Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models](https://arxiv.org/abs/2402.07033)
*Keisuke Kamahori, Tian Tang, Yile Gu, Kan Zhu, Baris Kasikci*

Main category: cs.LG

TL;DR: Fiddler is a resource-efficient inference system for MoE models, optimizing CPU and GPU usage to outperform existing systems in various scenarios.


<details>
  <summary>Details</summary>
Motivation: Running large MoE models in resource-constrained environments is challenging due to GPU memory limitations and inefficient CPU-GPU data handling.

Method: Fiddler strategically determines the optimal execution strategy for CPU and GPU resource utilization.

Result: Fiddler achieves speedups of 1.26x in single batch inference, 1.30x in long prefill, and 11.57x in beam search compared to baselines.

Conclusion: Fiddler is a versatile and efficient solution for MoE model inference in resource-limited settings.

Abstract: Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures
have shown promising performance on various tasks. However, due to the huge
model sizes, running them in resource-constrained environments where the GPU
memory is not abundant is challenging. Some existing systems propose to use CPU
resources to solve that, but they either suffer from the significant overhead
of frequently moving data between CPU and GPU, or fail to consider distinct
characteristics of CPUs and GPUs. This paper proposes Fiddler, a
resource-efficient inference system for MoE models with limited GPU resources.
Fiddler strategically utilizes CPU and GPU resources by determining the optimal
execution strategy. Our evaluation shows that, unlike state-of-the-art systems
that optimize for specific scenarios such as single batch inference or long
prefill, Fiddler performs better in all scenarios. Compared against different
baselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30
times in long prefill processing, and 11.57 times in beam search inference. The
code of Fiddler is publicly available at https://github.com/efeslab/fiddler.

</details>


### [245] [Large Language Model Agent as a Mechanical Designer](https://arxiv.org/abs/2404.17525)
*Yayati Jadhav, Amir Barati Farimani*

Main category: cs.LG

TL;DR: A framework combining pretrained LLMs and FEM autonomously generates and refines structural designs, outperforming traditional methods like NSGA-II in speed and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional iterative design processes and specialized ML models by leveraging general-purpose LLMs for broader applicability.

Method: Uses a pretrained LLM with an FEM module to propose, evaluate, and refine designs without domain-specific training, tested on 2D truss structures.

Result: Achieves faster convergence and fewer FEM evaluations than NSGA-II, with smaller models and lower temperatures improving constraint satisfaction and design consistency.

Conclusion: LLMs show promise as reasoning-based optimizers for autonomous structural design and refinement.

Abstract: Conventional mechanical design follows an iterative process in which initial
concepts are refined through cycles of expert assessment and resource-intensive
Finite Element Method (FEM) analysis to meet performance goals. While machine
learning models have been developed to assist in parts of this process, they
typically require large datasets, extensive training, and are often tailored to
specific tasks, limiting their generalizability. To address these limitations,
we propose a framework that leverages a pretrained Large Language Model (LLM)
in conjunction with an FEM module to autonomously generate, evaluate, and
refine structural designs based on performance specifications and numerical
feedback. The LLM operates without domain-specific fine-tuning, using general
reasoning to propose design candidates, interpret FEM-derived performance
metrics, and apply structurally sound modifications. Using 2D truss structures
as a testbed, we show that the LLM can effectively navigate highly discrete and
multi-faceted design spaces, balance competing objectives, and identify
convergence when further optimization yields diminishing returns. Compared to
Non-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves
faster convergence and fewer FEM evaluations. Experiments with varying
temperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini)
indicate that smaller models yield higher constraint satisfaction with fewer
steps, while lower temperatures enhance design consistency. These results
establish LLMs as a promising new class of reasoning-based, natural
language-driven optimizers for autonomous design and iterative structural
refinement.

</details>


### [246] [CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment](https://arxiv.org/abs/2406.13216)
*Songyang Chen, Yu Liu, Lei Zou, Zexuan Wang, Youfang Lin*

Main category: cs.LG

TL;DR: The paper investigates the expressiveness of unsupervised graph alignment models, proposes a hybrid approach (CombAlign) combining optimal transport and embedding-based methods, and achieves a 14.5% improvement in alignment accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore the model expressiveness in unsupervised graph alignment and how it impacts prediction accuracy, addressing gaps in discriminative power and node matching guarantees.

Method: Proposes CombAlign, a hybrid approach integrating cross-dimensional feature interaction for OT-based learning, an embedding-based method inspired by the Weisfeiler-Lehman test, and non-uniform marginals as priors. Also includes a refinement step using ensemble learning and maximum weight matching.

Result: Demonstrates a 14.5% improvement in alignment accuracy over state-of-the-art methods, validating the theoretical analysis.

Conclusion: CombAlign enhances expressiveness and accuracy in unsupervised graph alignment, supported by theoretical insights and empirical results.

Abstract: Unsupervised graph alignment finds the node correspondence between a pair of
attributed graphs by only exploiting graph structure and node features. One
category of recent studies first computes the node representation and then
matches nodes with the largest embedding-based similarity, while the other
category reduces the problem to optimal transport (OT) via Gromov-Wasserstein
learning. However, it remains largely unexplored in the model expressiveness,
as well as how theoretical expressivity impacts prediction accuracy. We
investigate the model expressiveness from two aspects. First, we characterize
the model's discriminative power in distinguishing matched and unmatched node
pairs across two graphs.Second, we study the model's capability of guaranteeing
node matching properties such as one-to-one matching and mutual alignment.
Motivated by our theoretical analysis, we put forward a hybrid approach named
CombAlign with stronger expressive power. Specifically, we enable
cross-dimensional feature interaction for OT-based learning and propose an
embedding-based method inspired by the Weisfeiler-Lehman test. We also apply
non-uniform marginals obtained from the embedding-based modules to OT as priors
for more expressiveness. Based on that, we propose a traditional
algorithm-based refinement, which combines our OT and embedding-based
predictions using the ensemble learning strategy and reduces the problem to
maximum weight matching. With carefully designed edge weights, we ensure those
matching properties and further enhance prediction accuracy. By extensive
experiments, we demonstrate a significant improvement of 14.5% in alignment
accuracy compared to state-of-the-art approaches and confirm the soundness of
our theoretical analysis.

</details>


### [247] [Commute Graph Neural Networks](https://arxiv.org/abs/2407.01635)
*Wei Zhuo, Han Yu, Guang Tan, Xiaoxiao Li*

Main category: cs.LG

TL;DR: CGNN integrates node-wise commute time into GNNs to better handle directed graphs, outperforming 13 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with asymmetrical node relationships in directed graphs, missing mutual path dependencies.

Method: CGNN uses a digraph Laplacian to compute commute time, integrating it into message passing with weighted neighbor contributions.

Result: CGNN outperforms 13 benchmarks across 8 datasets, proving its effectiveness.

Conclusion: CGNN successfully addresses the asymmetry challenge in directed graphs, offering a superior solution.

Abstract: Graph Neural Networks (GNNs) have shown remarkable success in learning from
graph-structured data. However, their application to directed graphs (digraphs)
presents unique challenges, primarily due to the inherent asymmetry in node
relationships. Traditional GNNs are adept at capturing unidirectional relations
but fall short in encoding the mutual path dependencies between nodes, such as
asymmetrical shortest paths typically found in digraphs. Recognizing this gap,
we introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly
integrates node-wise commute time into the message passing scheme. The
cornerstone of CGNN is an efficient method for computing commute time using a
newly formulated digraph Laplacian. Commute time is then integrated into the
neighborhood aggregation process, with neighbor contributions weighted
according to their respective commute time to the central node in each layer.
It enables CGNN to directly capture the mutual, asymmetric relationships in
digraphs. Extensive experiments on 8 benchmarking datasets confirm the
superiority of CGNN against 13 state-of-the-art methods.

</details>


### [248] [Reward-Augmented Data Enhances Direct Preference Alignment of LLMs](https://arxiv.org/abs/2410.08067)
*Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang*

Main category: cs.LG

TL;DR: The paper introduces reward-conditioned LLM policies to improve preference alignment by leveraging reward scores, addressing overfitting and unlearning issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing direct alignment algorithms focus on relative preferences but ignore qualitative aspects of responses, leading to overfitting and poor generalization.

Method: Proposes reward-conditioned LLM policies and a data relabeling method to condition preference pairs on quality scores, creating a reward-augmented dataset.

Result: Experiments show the approach consistently outperforms DPO, maximizing utility of preference data and mitigating unlearning issues.

Conclusion: The method effectively leverages reward scores to improve alignment and generalization, demonstrating broad effectiveness beyond data expansion.

Abstract: Preference alignment in Large Language Models (LLMs) has significantly
improved their ability to adhere to human instructions and intentions. However,
existing direct alignment algorithms primarily focus on relative preferences
and often overlook the qualitative aspects of responses, despite having access
to preference data that includes reward scores from judge models during AI
feedback. Striving to maximize the implicit reward gap between the chosen and
the slightly inferior rejected responses can cause overfitting and unnecessary
unlearning of the high-quality rejected responses. The unawareness of the
reward scores also drives the LLM to indiscriminately favor the low-quality
chosen responses and fail to generalize to optimal responses that are sparse in
data. To overcome these shortcomings, our study introduces reward-conditioned
LLM policies that discern and learn from the entire spectrum of response
quality within the dataset, helping extrapolate to more optimal regions. We
propose an effective yet simple data relabeling method that conditions the
preference pairs on quality scores to construct a reward-augmented dataset. The
experiments across various benchmarks and diverse models demonstrate that our
approach consistently boosts DPO by a considerable margin. Through
comprehensive ablation studies, we demonstrate that our method not only
maximizes the utility of preference data but also mitigates the issue of
unlearning, demonstrating its broad effectiveness beyond mere data expansion.
Our code is available at
https://github.com/shenao-zhang/reward-augmented-preference.

</details>


### [249] [A Comprehensive Survey of Deep Learning for Time Series Forecasting: Architectural Diversity and Open Challenges](https://arxiv.org/abs/2411.05793)
*Jongseon Kim, Hyungjoon Kim, HyunGi Kim, Dongjun Lee, Sungroh Yoon*

Main category: cs.LG

TL;DR: The paper surveys the evolution and diversification of architectures in time series forecasting, highlighting the shift from traditional methods to deep learning and emerging models like Transformers, linear layers, and hybrids. It also addresses key challenges and trends.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive analysis of the architectural advancements and challenges in time series forecasting, bridging historical context with modern trends.

Method: A survey comparing and re-examining deep learning models, including hybrid, diffusion, Mamba, and foundation models, while addressing inherent data challenges.

Result: Uncovers new perspectives and trends, emphasizing architectural diversification and tackling challenges like channel dependency and distribution shift.

Conclusion: The paper lowers entry barriers for newcomers and offers seasoned researchers broader insights into time series forecasting's evolving landscape.

Abstract: Time series forecasting is a critical task that provides key information for
decision-making. After traditional statistical and machine learning approaches,
various fundamental deep learning architectures such as MLPs, CNNs, RNNs, and
GNNs have been developed. However, the structural limitations caused by the
inductive biases of each deep learning architecture constrained their
performance. Transformer models, which excel at handling long-term
dependencies, have become significant architectural components for time series
forecasting. However, recent research has shown that alternatives such as
simple linear layers can outperform Transformers. These findings have opened up
new possibilities for using diverse architectures, ranging from fundamental
deep learning models to emerging architectures and hybrid approaches. In this
context, architectural modeling of time series forecasting has now entered a
renaissance. This survey not only provides a historical context for time series
forecasting but also offers comprehensive and timely analysis of the movement
toward architectural diversification. By comparing and re-examining deep
learning models, we uncover new perspectives and present recent trends,
including hybrid, diffusion, Mamba, and foundation models. By focusing on the
inherent characteristics of time series data, we also address open challenges
that have gained attention in time series forecasting, such as channel
dependency, distribution shift, causality, and feature extraction. These
contributions help lower entry barriers for newcomers by providing a systematic
understanding of the diverse research areas in time series forecasting (TSF),
while offering seasoned researchers broader perspectives and new opportunities
through in-depth exploration of TSF challenges. (Shortened due to arXiv's
1,920-character limit. Full version in the paper.)

</details>


### [250] [Non-Myopic Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2412.08085)
*Syrine Belakaria, Alaleh Ahmadianshalchi, Barbara Engelhardt, Stefano Ermon, Janardhan Rao Doppa*

Main category: cs.LG

TL;DR: The paper introduces non-myopic Bayesian optimization methods for multi-objective optimization (MOO) of expensive black-box functions, addressing challenges in finite-horizon sequential experimental design.


<details>
  <summary>Details</summary>
Motivation: The problem arises in real-world applications like materials design, where limited resources necessitate efficient experimental design. Existing methods are myopic and don't handle MOO well due to the Bellman principle's limitations.

Method: Proposes three non-myopic acquisition functions (AFs) for MOO: Nested AF (exact lower bound), Joint AF (lower bound on Nested AF), and BINOM AF (fast approximate variant). Uses hypervolume improvement (HVI) for scalarization.

Result: Experiments show the non-myopic AFs outperform existing myopic AFs in diverse real-world MOO problems.

Conclusion: The proposed methods effectively address the challenges of non-myopic MOO, offering improved performance and scalability for real-world applications.

Abstract: We consider the problem of finite-horizon sequential experimental design to
solve multi-objective optimization (MOO) of expensive black-box objective
functions. This problem arises in many real-world applications, including
materials design, where we have a small resource budget to make and evaluate
candidate materials in the lab. We solve this problem using the framework of
Bayesian optimization (BO) and propose the first set of non-myopic methods for
MOO problems. Prior work on non-myopic BO for single-objective problems relies
on the Bellman optimality principle to handle the lookahead reasoning process.
However, this principle does not hold for most MOO problems because the reward
function needs to satisfy some conditions: scalar variable, monotonicity, and
additivity. We address this challenge by using hypervolume improvement (HVI) as
our scalarization approach, which allows us to use a lower-bound on the Bellman
equation to approximate the finite-horizon using a batch expected hypervolume
improvement (EHVI) acquisition function (AF) for MOO. Our formulation naturally
allows us to use other improvement-based scalarizations and compare their
efficacy to HVI. We derive three non-myopic AFs for MOBO: 1) the Nested AF,
which is based on the exact computation of the lower bound, 2) the Joint AF,
which is a lower bound on the nested AF, and 3) the BINOM AF, which is a fast
and approximate variant based on batch multi-objective acquisition functions.
Our experiments on multiple diverse real-world MO problems demonstrate that our
non-myopic AFs substantially improve performance over the existing myopic AFs
for MOBO.

</details>


### [251] [Distilling Calibration via Conformalized Credal Inference](https://arxiv.org/abs/2501.06066)
*Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone*

Main category: cs.LG

TL;DR: CD-CI improves edge AI reliability by distilling calibration info from a complex model, using credal sets for uncertainty quantification without exceeding computational limits.


<details>
  <summary>Details</summary>
Motivation: Balancing AI model complexity and reliability on edge devices with limited resources, especially for sensitive tasks.

Method: Offline phase uses cloud-based model predictions to set a divergence threshold; runtime constructs credal sets for uncertainty quantification.

Result: CD-CI outperforms low-complexity Bayesian methods in calibration, validated on visual and language tasks.

Conclusion: CD-CI is a practical, efficient solution for edge AI, enhancing reliability without computational overload.

Abstract: Deploying artificial intelligence (AI) models on edge devices involves a
delicate balance between meeting stringent complexity constraints, such as
limited memory and energy resources, and ensuring reliable performance in
sensitive decision-making tasks. One way to enhance reliability is through
uncertainty quantification via Bayesian inference. This approach, however,
typically necessitates maintaining and running multiple models in an ensemble,
which may exceed the computational limits of edge devices. This paper
introduces a low-complexity methodology to address this challenge by distilling
calibration information from a more complex model. In an offline phase,
predictive probabilities generated by a high-complexity cloud-based model are
leveraged to determine a threshold based on the typical divergence between the
cloud and edge models. At run time, this threshold is used to construct credal
sets -- ranges of predictive probabilities that are guaranteed, with a
user-selected confidence level, to include the predictions of the cloud model.
The credal sets are obtained through thresholding of a divergence measure in
the simplex of predictive probabilities. Experiments on visual and language
tasks demonstrate that the proposed approach, termed Conformalized Distillation
for Credal Inference (CD-CI), significantly improves calibration performance
compared to low-complexity Bayesian methods, such as Laplace approximation,
making it a practical and efficient solution for edge AI deployments.

</details>


### [252] [Class Uncertainty: A Measure to Mitigate Class Imbalance](https://arxiv.org/abs/2311.14090)
*Z. S. Baltaci, K. Oksuz, S. Kuzucu, K. Tezoren, B. K. Konar, A. Ozkan, E. Akbas, S. Kalkan*

Main category: cs.LG

TL;DR: The paper introduces 'Class Uncertainty' as a better measure for class imbalance than cardinality, validates it on a new dataset (SVCI-20), and integrates it into existing imbalance mitigation methods.


<details>
  <summary>Details</summary>
Motivation: Class imbalance affects deep classifiers, but current methods rely on class cardinality, which doesn't capture all imbalance issues.

Method: Proposes 'Class Uncertainty' as a measure, validates it on SVCI-20 (a dataset with equal class sizes but varying hardness), and integrates it into ten imbalance mitigation methods.

Result: 'Class Uncertainty' better captures class differences than cardinality and improves performance on long-tailed datasets and SVCI-20.

Conclusion: The proposed measure effectively addresses class imbalance beyond cardinality, enhancing existing methods.

Abstract: Class-wise characteristics of training examples affect the performance of
deep classifiers. A well-studied example is when the number of training
examples of classes follows a long-tailed distribution, a situation that is
likely to yield sub-optimal performance for under-represented classes. This
class imbalance problem is conventionally addressed by approaches relying on
the class-wise cardinality of training examples, such as data resampling. In
this paper, we demonstrate that considering solely the cardinality of classes
does not cover all issues causing class imbalance. To measure class imbalance,
we propose "Class Uncertainty" as the average predictive uncertainty of the
training examples, and we show that this novel measure captures the differences
across classes better than cardinality. We also curate SVCI-20 as a novel
dataset in which the classes have equal number of training examples but they
differ in terms of their hardness; thereby causing a type of class imbalance
which cannot be addressed by the approaches relying on cardinality. We
incorporate our "Class Uncertainty" measure into a diverse set of ten class
imbalance mitigation methods to demonstrate its effectiveness on long-tailed
datasets as well as on our SVCI-20. Code and datasets will be made available.

</details>


### [253] [Advanced Physics-Informed Neural Network with Residuals for Solving Complex Integral Equations](https://arxiv.org/abs/2501.16370)
*Mahdi Movahedian Moghaddam, Kourosh Parand, Saeed Reza Kheradpisheh*

Main category: cs.LG

TL;DR: RISN is a neural network architecture for solving integral and integro-differential equations, outperforming PINNs with higher accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional Physics-Informed Neural Networks (PINNs) in solving complex integral and integro-differential equations.

Method: Combines residual connections with high-accuracy numerical methods like Gaussian quadrature and fractional derivative operational matrices.

Result: RISN achieves lower Mean Absolute Errors (MAE) than PINNs and variants (A-PINN, SA-PINN) across various equation types.

Conclusion: RISN is robust and efficient for real-world applications where traditional methods struggle.

Abstract: In this paper, we present the Residual Integral Solver Network (RISN), a
novel neural network architecture designed to solve a wide range of integral
and integro-differential equations, including one-dimensional,
multi-dimensional, ordinary and partial integro-differential, systems,
fractional types, and Helmholtz-type integral equations involving oscillatory
kernels. RISN integrates residual connections with high-accuracy numerical
methods such as Gaussian quadrature and fractional derivative operational
matrices, enabling it to achieve higher accuracy and stability than traditional
Physics-Informed Neural Networks (PINN). The residual connections help mitigate
vanishing gradient issues, allowing RISN to handle deeper networks and more
complex kernels, particularly in multi-dimensional problems. Through extensive
experiments, we demonstrate that RISN consistently outperforms not only
classical PINNs but also advanced variants such as Auxiliary PINN (A-PINN) and
Self-Adaptive PINN (SA-PINN), achieving significantly lower Mean Absolute
Errors (MAE) across various types of equations. These results highlight RISN's
robustness and efficiency in solving challenging integral and
integro-differential problems, making it a valuable tool for real-world
applications where traditional methods often struggle.

</details>


### [254] [Diversity By Design: Leveraging Distribution Matching for Offline Model-Based Optimization](https://arxiv.org/abs/2501.18768)
*Michael S. Yao, James C. Gee, Osbert Bastani*

Main category: cs.LG

TL;DR: DynAMO introduces diversity as an explicit objective in offline model-based optimization (MBO) by matching the distribution of generated designs to the dataset's inherent diversity, improving both diversity and quality of designs.


<details>
  <summary>Details</summary>
Motivation: To address the need for diverse and high-quality design proposals in offline MBO, where traditional methods may lack diversity.

Method: Formulates diversity as a distribution matching problem, integrating it into MBO. Uses DynAMO to align generated designs with the dataset's diversity.

Result: DynAMO significantly improves design diversity while maintaining high-quality candidates across multiple domains.

Conclusion: DynAMO effectively balances diversity and quality in offline MBO, offering a practical solution for diverse design generation.

Abstract: The goal of offline model-based optimization (MBO) is to propose new designs
that maximize a reward function given only an offline dataset. However, an
important desiderata is to also propose a diverse set of final candidates that
capture many optimal and near-optimal design configurations. We propose
Diversity in Adversarial Model-based Optimization (DynAMO) as a novel method to
introduce design diversity as an explicit objective into any MBO problem. Our
key insight is to formulate diversity as a distribution matching problem where
the distribution of generated designs captures the inherent diversity contained
within the offline dataset. Extensive experiments spanning multiple scientific
domains show that DynAMO can be used with common optimization methods to
significantly improve the diversity of proposed designs while still discovering
high-quality candidates.

</details>


### [255] [Multi-Objective Reinforcement Learning for Power Grid Topology Control](https://arxiv.org/abs/2502.00040)
*Thomas Lautenbacher, Ali Rajaei, Davide Barbieri, Jan Viebahn, Jochen L. Cremer*

Main category: cs.LG

TL;DR: The paper explores multi-objective reinforcement learning (MORL) for power grid topology control, balancing objectives like line loading and switching frequency, showing improved performance over single-objective methods.


<details>
  <summary>Details</summary>
Motivation: Addressing under-exploited potential of topology control in grid operations by aligning it with operator objectives and constraints.

Method: Uses deep optimistic linear support (DOL) and multi-objective proximal policy optimization (MOPPO) to generate Pareto-optimal policies.

Result: MORL policies are 30% better at preventing grid failures and 20% more effective with reduced training budget compared to single-objective RL.

Conclusion: MORL provides valuable trade-off insights and superior performance for grid topology control.

Abstract: Transmission grid congestion increases as the electrification of various
sectors requires transmitting more power. Topology control, through substation
reconfiguration, can reduce congestion but its potential remains
under-exploited in operations. A challenge is modeling the topology control
problem to align well with the objectives and constraints of operators.
Addressing this challenge, this paper investigates the application of
multi-objective reinforcement learning (MORL) to integrate multiple conflicting
objectives for power grid topology control. We develop a MORL approach using
deep optimistic linear support (DOL) and multi-objective proximal policy
optimization (MOPPO) to generate a set of Pareto-optimal policies that balance
objectives such as minimizing line loading, topological deviation, and
switching frequency. Initial case studies show that the MORL approach can
provide valuable insights into objective trade-offs and improve Pareto front
approximation compared to a random search baseline. The generated
multi-objective RL policies are 30% more successful in preventing grid failure
under contingencies and 20% more effective when training budget is reduced -
compared to the common single objective RL policy.

</details>


### [256] [Self-Explaining Hypergraph Neural Networks for Diagnosis Prediction](https://arxiv.org/abs/2502.10689)
*Leisheng Yu, Yanxiao Cai, Minxing Zhang, Xia Hu*

Main category: cs.LG

TL;DR: SHy is a self-explaining hypergraph neural network model for EHR-based diagnosis prediction, offering concise, personalized explanations and addressing data incompleteness.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for diagnosis prediction lack flexible and succinct interpretability, which is crucial for high-stakes healthcare applications.

Method: SHy models patients as hypergraphs, uses message-passing to capture disease interactions, and extracts temporal phenotypes for explanations while accounting for EHR data incompleteness.

Result: SHy outperforms state-of-the-art models in predictive performance and interpretability, validated on two real-world EHR datasets.

Conclusion: SHy provides a scalable, interpretable solution for diagnosis prediction, enhancing trust and utility in clinical settings.

Abstract: The burgeoning volume of electronic health records (EHRs) has enabled deep
learning models to excel in predictive healthcare. However, for high-stakes
applications such as diagnosis prediction, model interpretability remains
paramount. Existing deep learning diagnosis prediction models with intrinsic
interpretability often assign attention weights to every past diagnosis or
hospital visit, providing explanations lacking flexibility and succinctness. In
this paper, we introduce SHy, a self-explaining hypergraph neural network
model, designed to offer personalized, concise and faithful explanations that
allow for interventions from clinical experts. By modeling each patient as a
unique hypergraph and employing a message-passing mechanism, SHy captures
higher-order disease interactions and extracts distinct temporal phenotypes as
personalized explanations. It also addresses the incompleteness of the EHR data
by accounting for essential false negatives in the original diagnosis record. A
qualitative case study and extensive quantitative evaluations on two real-world
EHR datasets demonstrate the superior predictive performance and
interpretability of SHy over existing state-of-the-art models.

</details>


### [257] [SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference](https://arxiv.org/abs/2502.18137)
*Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, Jianfei Chen*

Main category: cs.LG

TL;DR: SpargeAttn is a universal sparse and quantized attention method that accelerates diverse models without sacrificing performance, using a two-stage online filter to skip unnecessary computations.


<details>
  <summary>Details</summary>
Motivation: Existing sparse attention methods are model-specific, lacking a universal solution that ensures both speedup and performance across diverse models.

Method: SpargeAttn employs a two-stage online filter: first predicting the attention map to skip computations, then using a softmax-aware filter for further skipping.

Result: The method significantly accelerates models in language, image, and video generation without degrading end-to-end metrics.

Conclusion: SpargeAttn provides a universal, efficient solution for sparse attention, applicable to diverse models while maintaining performance.

Abstract: An efficient attention implementation is essential for large models due to
its quadratic time complexity. Fortunately, attention commonly exhibits
sparsity, i.e., many values in the attention map are near zero, allowing for
the omission of corresponding computations. Many studies have utilized the
sparse pattern to accelerate attention. However, most existing works focus on
optimizing attention within specific models by exploiting certain sparse
patterns of the attention map. A universal sparse attention that guarantees
both the speedup and end-to-end performance of diverse models remains elusive.
In this paper, we propose SpargeAttn, a universal sparse and quantized
attention for any model. Our method uses a two-stage online filter: in the
first stage, we rapidly and accurately predict the attention map, enabling the
skip of some matrix multiplications in attention. In the second stage, we
design an online softmax-aware filter that incurs no extra overhead and further
skips some matrix multiplications. Experiments show that our method
significantly accelerates diverse models, including language, image, and video
generation, without sacrificing end-to-end metrics. The codes are available at
https://github.com/thu-ml/SpargeAttn.

</details>


### [258] [Uncertainty-aware Bayesian machine learning modelling of land cover classification](https://arxiv.org/abs/2503.21510)
*Samuel Bilson, Anna Pustogvar*

Main category: cs.LG

TL;DR: A Bayesian classification framework using generative modeling is proposed to address input measurement uncertainty in land cover classification, outperforming traditional models like random forests and neural networks in trustworthiness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current machine learning models for land cover classification lack consideration of input measurement uncertainty, which is crucial for metrology traceability.

Method: The study employs Bayesian quadratic discriminant analysis, applied to Copernicus Sentinel-2 datasets from 2020 and 2021, and benchmarks it against random forests and neural networks.

Result: Bayesian models are more trustworthy, interpretable, and computationally efficient while maintaining predictive performance across datasets.

Conclusion: Bayesian classification frameworks effectively address measurement uncertainty and offer reliable, interpretable results for land cover classification.

Abstract: Land cover classification involves the production of land cover maps, which
determine the type of land through remote sensing imagery. Over recent years,
such classification is being performed by machine learning classification
models, which can give highly accurate predictions on land cover per pixel
using large quantities of input training data. However, such models do not
currently take account of input measurement uncertainty, which is vital for
traceability in metrology. In this work we propose a Bayesian classification
framework using generative modelling to take account of input measurement
uncertainty. We take the specific case of Bayesian quadratic discriminant
analysis, and apply it to land cover datasets from Copernicus Sentinel-2 in
2020 and 2021. We benchmark the performance of the model against more popular
classification models used in land cover maps such as random forests and neural
networks. We find that such Bayesian models are more trustworthy, in the sense
that they are more interpretable, explicitly model the input measurement
uncertainty, and maintain predictive performance of class probability outputs
across datasets of different years and sizes, whilst also being computationally
efficient.

</details>


### [259] [Introduction to Online Control](https://arxiv.org/abs/2211.09619)
*Elad Hazan, Karan Singh*

Main category: cs.LG

TL;DR: The paper introduces online nonstochastic control, a new paradigm combining online convex optimization with control theory for dynamical systems, focusing on adversarial cost functions and perturbations.


<details>
  <summary>Details</summary>
Motivation: Traditional control methods assume stochastic noise and aim to match offline optimal strategies. Online nonstochastic control addresses adversarial scenarios where costs and perturbations are chosen adversarially, requiring a new approach.

Method: The approach leverages online convex optimization and convex relaxations to develop iterative optimization algorithms with provable guarantees.

Result: The methods achieve finite-time regret and computational complexity guarantees, performing well against adversarial conditions.

Conclusion: Online nonstochastic control offers a robust framework for adversarial settings, bridging control theory and online optimization with practical guarantees.

Abstract: This text presents an introduction to an emerging paradigm in control of
dynamical systems and differentiable reinforcement learning called online
nonstochastic control. The new approach applies techniques from online convex
optimization and convex relaxations to obtain new methods with provable
guarantees for classical settings in optimal and robust control.
  The primary distinction between online nonstochastic control and other
frameworks is the objective. In optimal control, robust control, and other
control methodologies that assume stochastic noise, the goal is to perform
comparably to an offline optimal strategy. In online nonstochastic control,
both the cost functions as well as the perturbations from the assumed dynamical
model are chosen by an adversary. Thus the optimal policy is not defined a
priori. Rather, the target is to attain low regret against the best policy in
hindsight from a benchmark class of policies.
  This objective suggests the use of the decision making framework of online
convex optimization as an algorithmic methodology. The resulting methods are
based on iterative mathematical optimization algorithms, and are accompanied by
finite-time regret and computational complexity guarantees.

</details>


### [260] [Efficient Reinforcement Finetuning via Adaptive Curriculum Learning](https://arxiv.org/abs/2504.05520)
*Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, Jieyu Zhao*

Main category: cs.LG

TL;DR: AdaRFT improves reinforcement finetuning (RFT) efficiency and accuracy for LLMs using adaptive curriculum learning, reducing training time by 2x and boosting performance.


<details>
  <summary>Details</summary>
Motivation: RFT is often inefficient and requires extensive training. AdaRFT aims to enhance efficiency and accuracy by dynamically adjusting problem difficulty.

Method: AdaRFT uses adaptive curriculum learning to adjust training problem difficulty based on reward signals, optimizing difficulty range without modifying RFT algorithms.

Result: Experiments show AdaRFT reduces training time by up to 2x and improves accuracy on math datasets like AMC, AIME, and IMO-style problems.

Conclusion: AdaRFT offers a scalable and effective RFT framework, enhancing both training efficiency and reasoning performance.

Abstract: Reinforcement finetuning (RFT) has shown great potential for enhancing the
mathematical reasoning capabilities of large language models (LLMs), but it is
often sample- and compute-inefficient, requiring extensive training. In this
work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a
method that significantly improves both the efficiency and final accuracy of
RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the
difficulty of training problems based on the model's recent reward signals,
ensuring that the model consistently trains on tasks that are challenging but
solvable. This adaptive sampling strategy accelerates learning by maintaining
an optimal difficulty range, avoiding wasted computation on problems that are
too easy or too hard. AdaRFT requires only a lightweight extension to standard
RFT algorithms like Proximal Policy Optimization (PPO), without modifying the
reward function or model architecture. Experiments on competition-level math
datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT
significantly improves both training efficiency and reasoning performance. We
evaluate AdaRFT across multiple data distributions and model sizes, showing
that it reduces training time by up to 2x and improves accuracy by a
considerable margin, offering a more scalable and effective RFT framework.

</details>


### [261] [Variational Self-Supervised Learning](https://arxiv.org/abs/2504.04318)
*Mehmet Can Yavuz, Berrin Yanikoglu*

Main category: cs.LG

TL;DR: VSSL combines variational inference and self-supervised learning for efficient, decoder-free representation learning, outperforming methods like BYOL and MoCo V3.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between variational modeling and modern self-supervised techniques by eliminating the need for generative reconstruction.

Method: Uses two symmetrically coupled encoders with Gaussian outputs, a teacher-student setup, and replaces reconstruction with cross-view denoising.

Result: Achieves competitive or superior performance on CIFAR-10, CIFAR-100, and ImageNet-100.

Conclusion: VSSL provides a scalable, probabilistically grounded approach for learning transferable representations.

Abstract: We present Variational Self-Supervised Learning (VSSL), a novel framework
that combines variational inference with self-supervised learning to enable
efficient, decoder-free representation learning. Unlike traditional VAEs that
rely on input reconstruction via a decoder, VSSL symmetrically couples two
encoders with Gaussian outputs. A momentum-updated teacher network defines a
dynamic, data-dependent prior, while the student encoder produces an
approximate posterior from augmented views. The reconstruction term in the ELBO
is replaced with a cross-view denoising objective, preserving the analytical
tractability of Gaussian KL divergence. We further introduce cosine-based
formulations of KL and log-likelihood terms to enhance semantic alignment in
high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and
ImageNet-100 show that VSSL achieves competitive or superior performance to
leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a
scalable, probabilistically grounded approach to learning transferable
representations without generative reconstruction, bridging the gap between
variational modeling and modern self-supervised techniques.

</details>


### [262] [Learning Against Distributional Uncertainty: On the Trade-off Between Robustness and Specificity](https://arxiv.org/abs/2301.13565)
*Shixiong Wang, Haowei Wang, Xinke Li, Jean Honorio*

Main category: cs.LG

TL;DR: A new framework unifies Bayesian, DRO, and regularization methods to address their limitations, offering robust and specific solutions with validated superiority in real-world tasks.


<details>
  <summary>Details</summary>
Motivation: To combat distributional uncertainties in training data and address the limitations of existing methods (Bayesian, DRO, regularization) such as prior specification difficulty, conservatism, and bias.

Method: Proposes a unified framework combining Bayesian, DRO, and regularization approaches, analyzing asymptotic and non-asymptotic properties, and developing solution methods.

Result: The framework balances robustness to unseen data and specificity to training data, validated by superior performance in real-world experiments.

Conclusion: The new framework effectively addresses the challenges of existing methods, demonstrating practical advantages and theoretical soundness.

Abstract: Trustworthy machine learning aims at combating distributional uncertainties
in training data distributions compared to population distributions. Typical
treatment frameworks include the Bayesian approach, (min-max) distributionally
robust optimization (DRO), and regularization. However, three issues have to be
raised: 1) the prior distribution in the Bayesian method and the regularizer in
the regularization method are difficult to specify; 2) the DRO method tends to
be overly conservative; 3) all the three methods are biased estimators of the
true optimal cost. This paper studies a new framework that unifies the three
approaches and addresses the three challenges above. The asymptotic properties
(e.g., consistencies and asymptotic normalities), non-asymptotic properties
(e.g., generalization bounds and unbiasedness), and solution methods of the
proposed model are studied. The new model reveals the trade-off between the
robustness to the unseen data and the specificity to the training data.
Experiments on various real-world tasks validate the superiority of the
proposed learning framework.

</details>


### [263] [Gaussian Mixture Flow Matching Models](https://arxiv.org/abs/2504.05304)
*Hansheng Chen, Kai Zhang, Hao Tan, Zexiang Xu, Fujun Luan, Leonidas Guibas, Gordon Wetzstein, Sai Bi*

Main category: cs.LG

TL;DR: GMFlow improves few-step sampling and color quality by predicting Gaussian mixture parameters instead of a single Gaussian, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing underperformance in few-step sampling and over-saturated colors in diffusion and flow matching models.

Method: Proposes Gaussian mixture flow matching (GMFlow) with KL divergence loss and introduces GM-SDE/ODE solvers for inference.

Result: Achieves Precision of 0.942 with 6 sampling steps on ImageNet 256x256, outperforming baselines.

Conclusion: GMFlow generalizes previous models, improves sampling efficiency, and mitigates color issues, enhancing generation quality.

Abstract: Diffusion models approximate the denoising distribution as a Gaussian and
predict its mean, whereas flow matching models reparameterize the Gaussian mean
as flow velocity. However, they underperform in few-step sampling due to
discretization error and tend to produce over-saturated colors under
classifier-free guidance (CFG). To address these limitations, we propose a
novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the
mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a
multi-modal flow velocity distribution, which can be learned with a KL
divergence loss. We demonstrate that GMFlow generalizes previous diffusion and
flow matching models where a single Gaussian is learned with an $L_2$ denoising
loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic
denoising distributions and velocity fields for precise few-step sampling.
Furthermore, we introduce a novel probabilistic guidance scheme that mitigates
the over-saturation issues of CFG and improves image generation quality.
Extensive experiments demonstrate that GMFlow consistently outperforms flow
matching baselines in generation quality, achieving a Precision of 0.942 with
only 6 sampling steps on ImageNet 256$\times$256.

</details>


### [264] [Fairness Risks for Group-conditionally Missing Demographics](https://arxiv.org/abs/2402.13393)
*Kaiqi Jiang, Wenzhe Fan, Mao Li, Xinhua Zhang*

Main category: cs.LG

TL;DR: Fairness-aware classification models address discrimination but often require sensitive features, which may be unavailable. This paper proposes a solution using probabilistic imputations and joint learning to handle missing sensitive data, improving accuracy and fairness.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of requiring full sensitive feature knowledge due to privacy and discrimination concerns, especially when missingness is group-dependent.

Method: Augments fairness risks with probabilistic imputations of sensitive features, jointly learning missing probabilities using a variational auto-encoder.

Result: Effective on image and tabular datasets, achieving better accuracy-fairness balance.

Conclusion: The proposed model handles missing sensitive data effectively, enhancing fairness without compromising accuracy.

Abstract: Fairness-aware classification models have gained increasing attention in
recent years as concerns grow on discrimination against some demographic
groups. Most existing models require full knowledge of the sensitive features,
which can be impractical due to privacy, legal issues, and an individual's fear
of discrimination. The key challenge we will address is the group dependency of
the unavailability, e.g., people of some age range may be more reluctant to
reveal their age. Our solution augments general fairness risks with
probabilistic imputations of the sensitive features, while jointly learning the
group-conditionally missing probabilities in a variational auto-encoder. Our
model is demonstrated effective on both image and tabular datasets, achieving
an improved balance between accuracy and fairness.

</details>


### [265] [Infinite-dimensional Diffusion Bridge Simulation via Operator Learning](https://arxiv.org/abs/2405.18353)
*Gefan Yang, Elizabeth Louise Baker, Michael L. Severinsen, Christy Anna Hipsley, Stefan Sommer*

Main category: cs.LG

TL;DR: A method combining score matching and operator learning is introduced to simulate infinite-dimensional diffusion bridges, achieving resolution-adaptive results without additional training.


<details>
  <summary>Details</summary>
Motivation: Simulating diffusion bridges for natural data is challenging due to intractable drift terms and continuous data representations, especially in infinite dimensions.

Method: The method merges score matching with operator learning to directly learn infinite-dimensional bridges, ensuring discretization equivariance.

Result: Experiments on synthetic and real-world biological data show high efficacy, with the method adapting to any resolution without retraining.

Conclusion: The proposed approach effectively addresses the challenges of infinite-dimensional diffusion bridge simulation, offering practical advantages in adaptability.

Abstract: The diffusion bridge, which is a diffusion process conditioned on hitting a
specific state within a finite period, has found broad applications in various
scientific and engineering fields. However, simulating diffusion bridges for
modeling natural data can be challenging due to both the intractability of the
drift term and continuous representations of the data. Although several methods
are available to simulate finite-dimensional diffusion bridges,
infinite-dimensional cases remain under explored. This paper presents a method
that merges score matching techniques with operator learning, enabling a direct
approach to learn the infinite-dimensional bridge and achieving a
discretization equivariant bridge simulation. We conduct a series of
experiments, ranging from synthetic examples with closed-form solutions to the
stochastic nonlinear evolution of real-world biological shape data. Our method
demonstrates high efficacy, particularly due to its ability to adapt to any
resolution without extra training.

</details>


### [266] [Computational and Statistical Guarantees for Tensor-on-Tensor Regression with Tensor Train Decomposition](https://arxiv.org/abs/2406.06002)
*Zhen Qin, Zhihui Zhu*

Main category: cs.LG

TL;DR: The paper analyzes the TT-based ToT regression model, providing theoretical error bounds and proposing efficient optimization algorithms (IHT and RGD) with linear convergence under RIP conditions.


<details>
  <summary>Details</summary>
Motivation: Address the gap between theoretical analysis and practical performance in TT-based ToT regression, focusing on storage and computational challenges.

Method: Error analysis under RIP, proposing IHT (gradient descent with TT-SVD) and RGD algorithms. Spectral initialization ensures proper starting points.

Result: Error bounds polynomially depend on tensor order (N+M). Both IHT and RGD achieve linear convergence under RIP.

Conclusion: Theoretical and algorithmic advancements bridge the gap, offering efficient solutions for TT-based ToT regression with proven convergence.

Abstract: Recently, a tensor-on-tensor (ToT) regression model has been proposed to
generalize tensor recovery, encompassing scenarios like scalar-on-tensor
regression and tensor-on-vector regression. However, the exponential growth in
tensor complexity poses challenges for storage and computation in ToT
regression. To overcome this hurdle, tensor decompositions have been
introduced, with the tensor train (TT)-based ToT model proving efficient in
practice due to reduced memory requirements, enhanced computational efficiency,
and decreased sampling complexity. Despite these practical benefits, a
disparity exists between theoretical analysis and real-world performance. In
this paper, we delve into the theoretical and algorithmic aspects of the
TT-based ToT regression model. Assuming the regression operator satisfies the
restricted isometry property (RIP), we conduct an error analysis for the
solution to a constrained least-squares optimization problem. This analysis
includes upper error bound and minimax lower bound, revealing that such error
bounds polynomially depend on the order $N+M$. To efficiently find solutions
meeting such error bounds, we propose two optimization algorithms: the
iterative hard thresholding (IHT) algorithm (employing gradient descent with
TT-singular value decomposition (TT-SVD)) and the factorization approach using
the Riemannian gradient descent (RGD) algorithm. When RIP is satisfied,
spectral initialization facilitates proper initialization, and we establish the
linear convergence rate of both IHT and RGD.

</details>


### [267] [GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning](https://arxiv.org/abs/2504.02546)
*Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang*

Main category: cs.LG

TL;DR: A minimalist RL approach, Group Policy Gradient (GPG), simplifies training by optimizing the original RL objective without surrogate losses, outperforming GRPO in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning in large language models via RL without heavy reliance on Supervised Fine-Tuning (SFT).

Method: Proposes GPG, eliminating critic/reference models, KL constraints, and bias in advantage/gradient estimation.

Result: GPG reduces computational costs and outperforms GRPO in unimodal/multimodal tasks.

Conclusion: GPG offers a simpler, more effective RL approach for language models, validated by experiments.

Abstract: Reinforcement Learning (RL) can directly enhance the reasoning capabilities
of large language models without extensive reliance on Supervised Fine-Tuning
(SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism
and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike
conventional methods, GPG directly optimize the original RL objective, thus
obviating the need for surrogate loss functions. By eliminating the critic and
reference models, avoiding KL divergence constraints, and addressing the
advantage and gradient estimation bias, our approach significantly simplifies
the training process compared to Group Relative Policy Optimization (GRPO). Our
approach achieves superior performance without relying on auxiliary techniques
or adjustments. As illustrated in Figure 1, extensive experiments demonstrate
that our method not only reduces computational costs but also consistently
outperforms GRPO across various unimodal and multimodal tasks. Our code is
available at https://github.com/AMAP-ML/GPG.

</details>


### [268] [DRTR: Distance-Aware Graph Representation Learning](https://arxiv.org/abs/2406.17281)
*Dong Liu, Yanxuan Yu*

Main category: cs.LG

TL;DR: DRTR is a graph learning framework combining distance-aware message passing and dynamic topology refinement, outperforming standard GNNs in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Standard GNNs rely on shallow, fixed-hop aggregation, limiting their ability to capture deeper structural dependencies in evolving or noisy graphs.

Method: DRTR uses a Distance Recomputator for edge pruning and a Topology Reconstructor for latent connections, integrating static preprocessing and dynamic resampling.

Result: DRTR outperforms baseline GNNs in accuracy and scalability, particularly in complex and noisy environments.

Conclusion: DRTR provides a more expressive and robust framework for graph representation learning by dynamically refining graph topologies.

Abstract: We propose \textbf{DRTR}, a novel graph learning framework that integrates
distance-aware multi-hop message passing with dynamic topology refinement.
Unlike standard GNNs that rely on shallow, fixed-hop aggregation, DRTR
leverages both static preprocessing and dynamic resampling to capture deeper
structural dependencies. A \emph{Distance Recomputator} prunes semantically
weak edges using adaptive attention, while a \emph{Topology Reconstructor}
establishes latent connections among distant but relevant nodes. This joint
mechanism enables more expressive and robust representation learning across
evolving graph structures. Extensive experiments demonstrate that DRTR
outperforms baseline GNNs in both accuracy and scalability, especially in
complex and noisy graph environments.

</details>


### [269] [Smoothed Analysis for Learning Concepts with Low Intrinsic Dimension](https://arxiv.org/abs/2407.00966)
*Gautam Chandrasekaran, Adam Klivans, Vasilis Kontonis, Raghu Meka, Konstantinos Stavropoulos*

Main category: cs.LG

TL;DR: The paper introduces a smoothed-analysis framework for supervised learning, enabling competitive learning for low-dimensional subspace concepts with bounded Gaussian surface area, including halfspaces and convex sets. It also improves traditional non-smoothed learning, notably for intersections of halfspaces.


<details>
  <summary>Details</summary>
Motivation: To overcome hardness results in traditional supervised learning by focusing on classifiers robust to small Gaussian perturbations, expanding learnability for low-dimensional concepts.

Method: A smoothed-analysis framework requiring learners to compete with the best Gaussian-perturbation-robust classifier, applied to low-dimensional subspace concepts with bounded Gaussian surface area.

Result: Achieves learnability for functions of halfspaces and low-dimensional convex sets, and improves runtime for agnostically learning intersections of halfspaces.

Conclusion: The framework broadens learnability in supervised learning and provides new insights for traditional non-smoothed settings.

Abstract: In traditional models of supervised learning, the goal of a learner -- given
examples from an arbitrary joint distribution on $\mathbb{R}^d \times \{\pm
1\}$ -- is to output a hypothesis that is competitive (to within $\epsilon$) of
the best fitting concept from some class. In order to escape strong hardness
results for learning even simple concept classes, we introduce a
smoothed-analysis framework that requires a learner to compete only with the
best classifier that is robust to small random Gaussian perturbation.
  This subtle change allows us to give a wide array of learning results for any
concept that (1) depends on a low-dimensional subspace (aka multi-index model)
and (2) has a bounded Gaussian surface area. This class includes functions of
halfspaces and (low-dimensional) convex sets, cases that are only known to be
learnable in non-smoothed settings with respect to highly structured
distributions such as Gaussians.
  Surprisingly, our analysis also yields new results for traditional
non-smoothed frameworks such as learning with margin. In particular, we obtain
the first algorithm for agnostically learning intersections of $k$-halfspaces
in time $k^{poly(\frac{\log k}{\epsilon \gamma}) }$ where $\gamma$ is the
margin parameter. Before our work, the best-known runtime was exponential in
$k$ (Arriaga and Vempala, 1999).

</details>


### [270] [Enhancing clinical decision support with physiological waveforms -- a multimodal benchmark in emergency care](https://arxiv.org/abs/2407.17856)
*Juan Miguel Lopez Alcaraz, Hjalmar Bouma, Nils Strodthoff*

Main category: cs.LG

TL;DR: AI-driven prediction models using multimodal data (demographics, biometrics, vital signs, lab values, ECG waveforms) improve emergency care decision-making, achieving high accuracy for diagnoses and predicting patient deterioration.


<details>
  <summary>Details</summary>
Motivation: To enhance emergency medicine by integrating multimodal data, including raw waveform signals, for better clinical decision support.

Method: Developed models using demographics, biometrics, vital signs, lab values, and ECG waveforms to predict discharge diagnoses and patient deterioration.

Result: Diagnostic model achieved AUROC >0.8 for 609/1,428 conditions; deterioration model achieved AUROC >0.8 for 14/15 critical events.

Conclusion: Incorporating raw waveform data improves predictive performance, and the publicly available dataset and models advance AI-driven emergency care support.

Abstract: Background: AI-driven prediction algorithms have the potential to enhance
emergency medicine by enabling rapid and accurate decision-making regarding
patient status and potential deterioration. However, the integration of
multimodal data, including raw waveform signals, remains underexplored in
clinical decision support. Methods: We present a dataset and benchmarking
protocol designed to advance multimodal decision support in emergency care. Our
models utilize demographics, biometrics, vital signs, laboratory values, and
electrocardiogram (ECG) waveforms as inputs to predict both discharge diagnoses
and patient deterioration. Results: The diagnostic model achieves area under
the receiver operating curve (AUROC) scores above 0.8 for 609 out of 1,428
conditions, covering both cardiac (e.g., myocardial infarction) and non-cardiac
(e.g., renal disease, diabetes) diagnoses. The deterioration model attains
AUROC scores above 0.8 for 14 out of 15 targets, accurately predicting critical
events such as cardiac arrest, mechanical ventilation, ICU admission, and
mortality. Conclusions: Our study highlights the positive impact of
incorporating raw waveform data into decision support models, improving
predictive performance. By introducing a unique, publicly available dataset and
baseline models, we provide a foundation for measurable progress in AI-driven
decision support for emergency care.

</details>


### [271] [Towards Physically Interpretable World Models: Meaningful Weakly Supervised Representations for Visual Trajectory Prediction](https://arxiv.org/abs/2412.12870)
*Zhenjiang Mao, Ivan Ruchkin*

Main category: cs.LG

TL;DR: A novel architecture, Physically Interpretable World Models, integrates physical knowledge into deep learning for robotics, improving interpretability and prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Embedding physical knowledge into deep learning models for robotics is challenging due to high-dimensional data and incomplete system knowledge.

Method: Combines a vector-quantized image autoencoder, a transformer-based physically interpretable autoencoder, and a partially known dynamical model, using weak interval-based supervision.

Result: Achieves physical interpretability and accurate state predictions in three case studies.

Conclusion: Advances representation learning for robotics by aligning learned representations with real-world physical quantities.

Abstract: Deep learning models are increasingly employed for perception, prediction,
and control in robotic systems. For for achieving realistic and consistent
outputs, it is crucial to embed physical knowledge into their learned
representations. However, doing so is difficult due to high-dimensional
observation data, such as images, particularly under conditions of incomplete
system knowledge and imprecise state sensing. To address this, we propose
Physically Interpretable World Models, a novel architecture that aligns learned
latent representations with real-world physical quantities. To this end, our
architecture combines three key elements: (1) a vector-quantized image
autoencoder, (2) a transformer-based physically interpretable autoencoder, and
(3) a partially known dynamical model. The training incorporates weak
interval-based supervision to eliminate the impractical reliance on
ground-truth physical knowledge. Three case studies demonstrate that our
approach achieves physical interpretability and accurate state predictions,
thus advancing representation learning for robotics.

</details>


### [272] [KNN and K-means in Gini Prametric Spaces](https://arxiv.org/abs/2501.18028)
*Cassandra Mussard, Arthur Charpentier, Stéphane Mussard*

Main category: cs.LG

TL;DR: The paper enhances K-means and KNN using Gini parametric spaces, improving robustness to noise and outliers with a new Gini-based measure.


<details>
  <summary>Details</summary>
Motivation: Traditional distance metrics lack robustness to noise and outliers. The paper aims to improve this by incorporating rank and value information via Gini-based measures.

Method: Proposes a Gini-based measure, develops Gini K-means (proven to converge) and Gini KNN, and evaluates them on 14 UCI datasets.

Result: Gini-based algorithms outperform traditional methods in noisy environments, showing superior performance in clustering and classification.

Conclusion: The work successfully leverages rank-based measures, opening new avenues for machine learning and statistical analysis.

Abstract: This paper introduces innovative enhancements to the K-means and K-nearest
neighbors (KNN) algorithms based on the concept of Gini prametric spaces.
Unlike traditional distance metrics, Gini-based measures incorporate both
value-based and rank-based information, improving robustness to noise and
outliers. The main contributions of this work include: proposing a Gini-based
measure that captures both rank information and value distances; presenting a
Gini K-means algorithm that is proven to converge and demonstrates resilience
to noisy data; and introducing a Gini KNN method that performs competitively
with state-of-the-art approaches such as Hassanat's distance in noisy
environments. Experimental evaluations on 14 datasets from the UCI repository
demonstrate the superior performance and efficiency of Gini-based algorithms in
clustering and classification tasks. This work opens new avenues for leveraging
rank-based measures in machine learning and statistical analysis.

</details>


### [273] [$PINN - a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks](https://arxiv.org/abs/2504.19013)
*Júlia Vicens Figueres, Juliette Vanderhaeghen, Federica Bragone, Kateryna Morozovska, Khemraj Shukla*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Physics-Informed Neural Networks (PINNs) are a novel computational approach
for solving partial differential equations (PDEs) with noisy and sparse initial
and boundary data. Although, efficient quantification of epistemic and
aleatoric uncertainties in big multi-scale problems remains challenging. We
propose \$PINN a novel method of computing global uncertainty in PDEs using a
Bayesian framework, by combining local Bayesian Physics-Informed Neural
Networks (BPINN) with domain decomposition. The solution continuity across
subdomains is obtained by imposing the flux continuity across the interface of
neighboring subdomains. To demonstrate the effectiveness of \$PINN, we conduct
a series of computational experiments on PDEs in 1D and 2D spatial domains.
Although we have adopted conservative PINNs (cPINNs), the method can be
seamlessly extended to other domain decomposition techniques. The results infer
that the proposed method recovers the global uncertainty by computing the local
uncertainty exactly more efficiently as the uncertainty in each subdomain can
be computed concurrently. The robustness of \$PINN is verified by adding
uncorrelated random noise to the training data up to 15% and testing for
different domain sizes.

</details>


### [274] [Decision Making in Hybrid Environments: A Model Aggregation Approach](https://arxiv.org/abs/2502.05974)
*Haolin Liu, Chen-Yu Wei, Julian Zimmert*

Main category: cs.LG

TL;DR: The paper extends the Decision Estimation Coefficient (DEC) framework to better characterize the hybrid regime in online decision making, where the world's dynamics are fixed but rewards change arbitrarily. It introduces a flexible algorithm design and improves regret bounds for certain MDPs.


<details>
  <summary>Details</summary>
Motivation: Prior work on DEC focused on pure stochastic or adversarial regimes, leaving the hybrid regime with pessimistic bounds. This work aims to address this gap.

Method: The authors propose a general extension of DEC for the hybrid regime, introducing a flexible algorithm design that trades estimation and decision complexity. They also extend bilinear classes to adversarial-reward cases.

Result: The framework provides more precise complexity characterization for the hybrid regime and improves regret bounds for linear Q*/V* MDPs in the stochastic regime.

Conclusion: The work advances understanding of the hybrid regime in online decision making and offers practical algorithm design insights, with applications in model-based and model-free learning.

Abstract: Recent work by Foster et al. (2021, 2022, 2023b) and Xu and Zeevi (2023)
developed the framework of decision estimation coefficient (DEC) that
characterizes the complexity of general online decision making problems and
provides a general algorithm design principle. These works, however, either
focus on the pure stochastic regime where the world remains fixed over time, or
the pure adversarial regime where the world arbitrarily changes over time. For
the hybrid regime where the dynamics of the world is fixed while the reward
arbitrarily changes, they only give pessimistic bounds on the decision
complexity. In this work, we propose a general extension of DEC that more
precisely characterizes this case. Besides applications in special cases, our
framework leads to a flexible algorithm design where the learner learns over
subsets of the hypothesis set, trading estimation complexity with decision
complexity, which could be of independent interest. Our work covers model-based
learning and model-free learning in the hybrid regime, with a newly proposed
extension of the bilinear classes (Du et al., 2021) to the adversarial-reward
case. In addition, our method improves the best-known regret bounds for linear
Q*/V* MDPs in the pure stochastic regime.

</details>


### [275] [AMUN: Adversarial Machine UNlearning](https://arxiv.org/abs/2503.00917)
*Ali Ebrahimpour-Boroojeny, Hari Sundaram, Varun Chandrasekaran*

Main category: cs.LG

TL;DR: AMUN is a new machine unlearning method using adversarial examples to effectively delete forget datasets while maintaining model accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational overhead of exact unlearning and the ineffectiveness of approximate methods in maintaining model accuracy and confidence.

Method: AMUN fine-tunes the model on adversarial examples closest to forget samples to localize changes and preserve global behavior.

Result: AMUN outperforms SOTA methods, reducing membership inference attack success to random guessing levels.

Conclusion: AMUN provides an effective and efficient solution for machine unlearning, balancing accuracy and privacy.

Abstract: Machine unlearning, where users can request the deletion of a forget dataset,
is becoming increasingly important because of numerous privacy regulations.
Initial works on ``exact'' unlearning (e.g., retraining) incur large
computational overheads. However, while computationally inexpensive,
``approximate'' methods have fallen short of reaching the effectiveness of
exact unlearning: models produced fail to obtain comparable accuracy and
prediction confidence on both the forget and test (i.e., unseen) dataset.
Exploiting this observation, we propose a new unlearning method, Adversarial
Machine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA)
methods for image classification. AMUN lowers the confidence of the model on
the forget samples by fine-tuning the model on their corresponding adversarial
examples. Adversarial examples naturally belong to the distribution imposed by
the model on the input space; fine-tuning the model on the adversarial examples
closest to the corresponding forget samples (a) localizes the changes to the
decision boundary of the model around each forget sample and (b) avoids drastic
changes to the global behavior of the model, thereby preserving the model's
accuracy on test samples. Using AMUN for unlearning a random $10\%$ of CIFAR-10
samples, we observe that even SOTA membership inference attacks cannot do
better than random guessing.

</details>


### [276] [A metrological framework for uncertainty evaluation in machine learning classification models](https://arxiv.org/abs/2504.03359)
*Samuel Bilson, Maurice Cox, Anna Pustogvar, Andrew Thompson*

Main category: cs.LG

TL;DR: Proposes a metrological framework for evaluating uncertainty in ML classification models, addressing gaps in VIM and GUM for nominal properties.


<details>
  <summary>Details</summary>
Motivation: Current standards (VIM and GUM) lack definitions and methods for uncertainty evaluation in nominal properties, which are critical for ML classification applications like climate observation and medical diagnosis.

Method: Develops a conceptual uncertainty evaluation framework for ML classification, demonstrated through climate/earth observation and medical diagnosis applications.

Result: The framework enables extending VIM and GUM to include uncertainty evaluation for nominal properties, making them applicable to ML classification models.

Conclusion: The proposed framework bridges a gap in metrology for ML classification, enhancing reliability in high-impact applications.

Abstract: Machine learning (ML) classification models are increasingly being used in a
wide range of applications where it is important that predictions are
accompanied by uncertainties, including in climate and earth observation,
medical diagnosis and bioaerosol monitoring. The output of an ML classification
model is a type of categorical variable known as a nominal property in the
International Vocabulary of Metrology (VIM). However, concepts related to
uncertainty evaluation for nominal properties are not defined in the VIM, nor
is such evaluation addressed by the Guide to the Expression of Uncertainty in
Measurement (GUM). In this paper we propose a metrological conceptual
uncertainty evaluation framework for ML classification, and illustrate its use
in the context of two applications that exemplify the issues and have
significant societal impact, namely, climate and earth observation and medical
diagnosis. Our framework would enable an extension of the VIM and GUM to
uncertainty for nominal properties, which would make both applicable to ML
classification models.

</details>


### [277] [TabRep: Training Tabular Diffusion Models with a Simple and Effective Continuous Representation](https://arxiv.org/abs/2504.04798)
*Jacob Si, Zijing Ou, Mike Qu, Zhengrui Xiang, Yingzhen Li*

Main category: cs.LG

TL;DR: TabRep is a tabular diffusion model using a unified continuous representation, addressing challenges in joint modeling and encoding heuristics, achieving superior performance and privacy preservation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of modeling multi-modal distributions in tabular data and improving upon sparse suboptimal encoding heuristics in unified representations.

Method: Introduces TabRep, a tabular diffusion architecture with a unified continuous representation, leveraging geometric insights for design.

Result: TabRep outperforms in evaluations, synthesizing data that exceeds original dataset quality while being privacy-preserving and efficient.

Conclusion: TabRep offers a simple, effective solution for tabular diffusion models, balancing performance, privacy, and computational efficiency.

Abstract: Diffusion models have been the predominant generative model for tabular data
generation. However, they face the conundrum of modeling under a separate
versus a unified data representation. The former encounters the challenge of
jointly modeling all multi-modal distributions of tabular data in one model.
While the latter alleviates this by learning a single representation for all
features, it currently leverages sparse suboptimal encoding heuristics and
necessitates additional computation costs. In this work, we address the latter
by presenting TabRep, a tabular diffusion architecture trained with a unified
continuous representation. To motivate the design of our representation, we
provide geometric insights into how the data manifold affects diffusion models.
The key attributes of our representation are composed of its density,
flexibility to provide ample separability for nominal features, and ability to
preserve intrinsic relationships. Ultimately, TabRep provides a simple yet
effective approach for training tabular diffusion models under a continuous
data manifold. Our results showcase that TabRep achieves superior performance
across a broad suite of evaluations. It is the first to synthesize tabular data
that exceeds the downstream quality of the original datasets while preserving
privacy and remaining computationally efficient.

</details>


### [278] [Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks](https://arxiv.org/abs/2504.07835)
*Erin Carson, Xinye Chen*

Main category: cs.LG

TL;DR: The paper introduces Pychop, a Python library for low-precision arithmetic emulation, supporting customizable floating-point formats and rounding modes. It integrates with PyTorch and JAX for GPU-based neural network training and inference, validated through empirical studies on image classification and object detection.


<details>
  <summary>Details</summary>
Motivation: The demand for low-precision arithmetic in computational science and deep learning, aiming for efficient computation, reduced memory, and energy savings while maintaining model accuracy.

Method: Development of the Pychop library, offering customizable low-precision emulation in Python, with interfaces for PyTorch and JAX. Validation through empirical studies on image classification and object detection.

Result: Pychop enables efficient low-precision emulation, supports mixed-precision algorithms, and integrates into deep learning workflows. Empirical results highlight the impact of low precision on model performance.

Conclusion: Pychop is a foundational tool for advancing mixed-precision algorithms, facilitating hardware accelerator development, and seamless integration into deep learning workflows.

Abstract: Motivated by the growing demand for low-precision arithmetic in computational
science, we exploit lower-precision emulation in Python -- widely regarded as
the dominant programming language for numerical analysis and machine learning.
Low-precision training has revolutionized deep learning by enabling more
efficient computation and reduced memory and energy consumption while
maintaining model fidelity. To better enable numerical experimentation with and
exploration of low precision computation, we developed the Pychop library,
which supports customizable floating-point formats and a comprehensive set of
rounding modes in Python, allowing users to benefit from fast, low-precision
emulation in numerous applications. Pychop also introduces interfaces for both
PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural
network training and inference with unparalleled flexibility.
  In this paper, we offer a comprehensive exposition of the design,
implementation, validation, and practical application of Pychop, establishing
it as a foundational tool for advancing efficient mixed-precision algorithms.
Furthermore, we present empirical results on low-precision emulation for image
classification and object detection using published datasets, illustrating the
sensitivity of the use of low precision and offering valuable insights into its
impact. Pychop enables in-depth investigations into the effects of numerical
precision, facilitates the development of novel hardware accelerators, and
integrates seamlessly into existing deep learning workflows. Software and
experimental code are publicly available at
https://github.com/inEXASCALE/pychop.

</details>


### [279] [Kernel Ridge Regression for Efficient Learning of High-Capacity Hopfield Networks](https://arxiv.org/abs/2504.12561)
*Akira Tamamori*

Main category: cs.LG

TL;DR: Kernel Ridge Regression (KRR) is proposed as an efficient, non-iterative alternative to Kernel Logistic Regression (KLR) for high-capacity Hopfield networks, offering comparable performance with faster training.


<details>
  <summary>Details</summary>
Motivation: Hopfield networks using Hebbian learning have limited storage capacity, and while supervised methods like KLR improve this, they are computationally expensive.

Method: KRR uses the kernel trick and regression to predict bipolar states, providing a closed-form solution for learning dual variables.

Result: KRR achieves state-of-the-art storage capacity (β=1.5) and noise robustness, matching KLR, while drastically reducing training time.

Conclusion: KRR is a highly efficient method for high-performance associative memories, offering KLR-like performance with significant speed advantages.

Abstract: Hopfield networks using Hebbian learning suffer from limited storage
capacity. While supervised methods like Linear Logistic Regression (LLR) offer
some improvement, kernel methods like Kernel Logistic Regression (KLR)
significantly enhance capacity and noise robustness. However, KLR requires
computationally expensive iterative learning. We propose Kernel Ridge
Regression (KRR) as an efficient kernel-based alternative for learning
high-capacity Hopfield networks. KRR utilizes the kernel trick and predicts
bipolar states via regression, crucially offering a non-iterative, closed-form
solution for learning dual variables. We evaluate KRR and compare its
performance against Hebbian, LLR, and KLR. Our results demonstrate that KRR
achieves state-of-the-art storage capacity (reaching $\beta$=1.5) and noise
robustness, comparable to KLR. Crucially, KRR drastically reduces training
time, being orders of magnitude faster than LLR and significantly faster than
KLR, especially at higher storage loads. This establishes KRR as a potent and
highly efficient method for building high-performance associative memories,
providing comparable performance to KLR with substantial training speed
advantages. This work provides the first empirical comparison between KRR and
KLR in the context of Hopfield network learning.

</details>


### [280] [Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching](https://arxiv.org/abs/2504.17066)
*Kewen Peng, Yicheng Yang, Hao Zhuo*

Main category: cs.LG

TL;DR: FairMatch, a post-processing method using propensity score matching, improves fairness evaluation and mitigation without losing predictive performance.


<details>
  <summary>Details</summary>
Motivation: To address unreliable fairness metrics due to biased sampling in training and test data.

Method: Proposes FairMatch, which uses propensity score matching to adjust decision thresholds and probabilistic calibration for unmatched samples.

Result: Identifies unbiased subsets and reduces bias significantly without performance loss.

Conclusion: Propensity score matching enhances fairness evaluation and mitigation effectively.

Abstract: Fairness-aware learning aims to mitigate discrimination against specific
protected social groups (e.g., those categorized by gender, ethnicity, age)
while minimizing predictive performance loss. Despite efforts to improve
fairness in machine learning, prior studies have shown that many models remain
unfair when measured against various fairness metrics. In this paper, we
examine whether the way training and testing data are sampled affects the
reliability of reported fairness metrics. Since training and test sets are
often randomly sampled from the same population, bias present in the training
data may still exist in the test data, potentially skewing fairness
assessments. To address this, we propose FairMatch, a post-processing method
that applies propensity score matching to evaluate and mitigate bias. FairMatch
identifies control and treatment pairs with similar propensity scores in the
test set and adjusts decision thresholds for different subgroups accordingly.
For samples that cannot be matched, we perform probabilistic calibration using
fairness-aware loss functions. Experimental results demonstrate that our
approach can (a) precisely locate subsets of the test data where the model is
unbiased, and (b) significantly reduce bias on the remaining data. Overall,
propensity score matching offers a principled way to improve both fairness
evaluation and mitigation, without sacrificing predictive performance.

</details>


### [281] [Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy](https://arxiv.org/abs/2504.17074)
*William R. Keely, Otto Lamminpää, Steffen Mauceri, Sean M. R. Crowell, Christopher W. O'Dell, Gregory R. McGarragh*

Main category: cs.LG

TL;DR: A diffusion-based method is proposed for faster, accurate GHG retrieval from satellite data, addressing limitations of current Optimal Estimation (OE) methods.


<details>
  <summary>Details</summary>
Motivation: Current OE methods for GHG retrieval are computationally expensive, provide unrealistic uncertainty estimates, and may not handle the increased data volume from upcoming satellite missions.

Method: The paper proposes a diffusion-based approach to retrieve GHG concentrations, offering flexibility for Gaussian or non-Gaussian posterior distributions and computational efficiency.

Result: The method provides a significant computational speed-up over OE while maintaining accuracy, enabling near real-time global GHG monitoring.

Conclusion: This approach advances GHG retrieval capabilities, supporting policy-making with robust, scalable solutions for future satellite missions.

Abstract: Satellite-based estimates of greenhouse gas (GHG) properties from
observations of reflected solar spectra are integral for understanding and
monitoring complex terrestrial systems and their impact on the carbon cycle due
to their near global coverage. Known as retrieval, making GHG concentration
estimations from these observations is a non-linear Bayesian inverse problem,
which is operationally solved using a computationally expensive algorithm
called Optimal Estimation (OE), providing a Gaussian approximation to a
non-Gaussian posterior. This leads to issues in solver algorithm convergence,
and to unrealistically confident uncertainty estimates for the retrieved
quantities. Upcoming satellite missions will provide orders of magnitude more
data than the current constellation of GHG observers. Development of fast and
accurate retrieval algorithms with robust uncertainty quantification is
critical. Doing so stands to provide substantial climate impact of moving
towards the goal of near continuous real-time global monitoring of carbon
sources and sinks which is essential for policy making. To achieve this goal,
we propose a diffusion-based approach to flexibly retrieve a Gaussian or
non-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,
while providing a substantial computational speed-up over the current
operational state-of-the-art.

</details>


### [282] [Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional](https://arxiv.org/abs/2504.18506)
*Sanjeev Raja, Martin Šípka, Michael Psenka, Tobias Kreiman, Michal Pavelka, Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: The paper proposes a zero-shot method for Transition Path Sampling (TPS) using pre-trained generative models, avoiding task-specific training and leveraging stochastic dynamics for efficient path sampling.


<details>
  <summary>Details</summary>
Motivation: Current TPS methods are limited by expensive, task-specific training and cannot utilize advances in atomistic machine learning like pre-trained models and datasets.

Method: Interprets paths as trajectories from stochastic dynamics of pre-trained generative models (denoising diffusion, flow matching), minimizing the Onsager-Machlup action functional.

Result: Demonstrates diverse, realistic transition paths on molecular systems, generalizing beyond the training data.

Conclusion: The method repurposes pre-trained models for TPS efficiently, is scalable, and adaptable to future generative models.

Abstract: Transition path sampling (TPS), which involves finding probable paths
connecting two points on an energy landscape, remains a challenge due to the
complexity of real-world atomistic systems. Current machine learning approaches
use expensive, task-specific, and data-free training procedures, limiting their
ability to benefit from recent advances in atomistic machine learning, such as
high-quality datasets and large-scale pre-trained models. In this work, we
address TPS by interpreting candidate paths as trajectories sampled from
stochastic dynamics induced by the learned score function of pre-trained
generative models, specifically denoising diffusion and flow matching. Under
these dynamics, finding high-likelihood transition paths becomes equivalent to
minimizing the Onsager-Machlup (OM) action functional. This enables us to
repurpose pre-trained generative models for TPS in a zero-shot manner, in
contrast with bespoke, task-specific TPS models trained in previous work. We
demonstrate our approach on varied molecular systems, obtaining diverse,
physically realistic transition pathways and generalizing beyond the
pre-trained model's original training dataset. Our method can be easily
incorporated into new generative models, making it practically relevant as
models continue to scale and improve with increased data availability.

</details>


### [283] [Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation](https://arxiv.org/abs/2504.19602)
*Kitsuya Azuma, Takayuki Nishio, Yuichi Kitagawa, Wakako Nakano, Takahito Tanimura*

Main category: cs.LG

TL;DR: SCARLET is a federated learning framework that reduces communication costs by 50% using synchronized soft-label caching and Enhanced ERA, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional FL suffers from high communication overhead and limited model heterogeneity, while existing distillation-based FL methods have redundant transmissions.

Method: SCARLET integrates synchronized soft-label caching and an Enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism to minimize redundant communication.

Result: Achieves up to 50% reduction in communication costs while maintaining accuracy, outperforming state-of-the-art distillation-based FL methods.

Conclusion: SCARLET is an efficient and accurate solution for federated learning, adaptable to non-IID data and diverse client scenarios.

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients, enhancing privacy by keeping data local. Yet
conventional FL, relying on frequent parameter-sharing, suffers from high
communication overhead and limited model heterogeneity. Distillation-based FL
approaches address these issues by sharing predictions (soft-labels) instead,
but they often involve redundant transmissions across communication rounds,
reducing efficiency. We propose SCARLET, a novel framework integrating
synchronized soft-label caching and an enhanced Entropy Reduction Aggregation
(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing
cached soft-labels, achieving up to 50% reduction in communication costs
compared to existing methods while maintaining accuracy. Enhanced ERA can be
tuned to adapt to non-IID data variations, ensuring robust aggregation and
performance in diverse client scenarios. Experimental evaluations demonstrate
that SCARLET consistently outperforms state-of-the-art distillation-based FL
methods in terms of accuracy and communication efficiency. The implementation
of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [284] [TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration](https://arxiv.org/abs/2505.00055)
*Zhuoqi Zeng, Yuxiang Wei, Jiawen Kang*

Main category: cs.MA

TL;DR: The paper proposes a framework for efficient migration of Vehicular Embodied Agent AI Twins (VEAATs) in VEANETs, combining game theory and tiny multi-agent deep reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Addressing computational demands and mobility challenges in autonomous driving by optimizing VEAAT migration between resource-constrained Roadside Units (RSUs).

Method: Uses a multi-leader multi-follower Stackelberg game and TinyMA-IEI-PPO, a dynamic pruning algorithm for efficient training and deployment.

Result: Achieves convergence comparable to baselines and approximates Stackelberg equilibrium.

Conclusion: The framework effectively optimizes VEAAT migration, balancing computational efficiency and performance.

Abstract: Embodied Artificial Intelligence (EAI) addresses autonomous driving
challenges in Vehicular Embodied AI Networks (VEANETs) through multi-modal
perception, adaptive decision-making, and hardware-software co-scheduling.
However, the computational demands of virtual services and the inherent
mobility of autonomous vehicles (AVs) necessitate real-time migration of
Vehicular Embodied Agent AI Twins (VEAATs) between resource-constrained
Roadside Units (RSUs). This paper proposes a novel framework for efficient
VEAAT migration in VEANETs, combining a multi-leader multi-follower (MLMF)
Stackelberg game-theoretic incentive mechanism with a tiny multi-agent deep
reinforcement learning (MADRL) algorithm. First, We propose an virtual
immersive experience-driven utility model that captures AV-RSU dynamic
interactions by integrating AVs' social influence, service complementarity and
substitutability, and RSUs' resource allocation strategies to optimize VEAAT
migration decisions. Second, to enhance training efficiency and enable
efficient deployment on computation-constrained AVs while preserving
exploration-exploitation performance, we propose TinyMA-IEI-PPO, a
self-adaptive dynamic structured pruning algorithm that dynamically adjusts
neuron importance based on agents' exploration incentives. Numerical results
demonstrate that our approach achieves convergence comparable to baseline
models and closely approximates the Stackelberg equilibrium.

</details>


### [285] [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
*Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, Qingyun Wu*

Main category: cs.MA

TL;DR: The paper introduces automated failure attribution in LLM multi-agent systems, proposing the Who&When dataset and evaluating three methods, with limited success in pinpointing failure steps.


<details>
  <summary>Details</summary>
Motivation: Failure attribution in LLM multi-agent systems is crucial for debugging but is underexplored and labor-intensive.

Method: The authors propose automated failure attribution, introduce the Who&When dataset, and develop three evaluation methods.

Result: The best method achieves 53.5% accuracy for identifying agents but only 14.2% for steps, with some methods below random. SOTA models also underperform.

Conclusion: The task is complex, highlighting the need for further research. Code and dataset are publicly available.

Abstract: Failure attribution in LLM multi-agent systems-identifying the agent and step
responsible for task failures-provides crucial clues for systems debugging but
remains underexplored and labor-intensive. In this paper, we propose and
formulate a new research area: automated failure attribution for LLM
multi-agent systems. To support this initiative, we introduce the Who&When
dataset, comprising extensive failure logs from 127 LLM multi-agent systems
with fine-grained annotations linking failures to specific agents and decisive
error steps. Using the Who&When, we develop and evaluate three automated
failure attribution methods, summarizing their corresponding pros and cons. The
best method achieves 53.5% accuracy in identifying failure-responsible agents
but only 14.2% in pinpointing failure steps, with some methods performing below
random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to
achieve practical usability. These results highlight the task's complexity and
the need for further research in this area. Code and dataset are available at
https://github.com/mingyin1/Agents_Failure_Attribution

</details>


### [286] [Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication](https://arxiv.org/abs/2505.00540)
*Ian O'Flynn, Harun Šiljak*

Main category: cs.MA

TL;DR: A centralized reinforcement learning strategy for multi-agent foraging reduces computational demands compared to MARL, promoting role differentiation without explicit communication.


<details>
  <summary>Details</summary>
Motivation: To address high computational and energy demands in multi-agent reinforcement learning (MARL) by centralizing learning in one agent and disseminating its model to others.

Method: A single agent learns centrally, and its model is periodically shared with non-learning agents. A reward function encourages implicit role differentiation.

Result: Agents developed differentiated behaviors dynamically without explicit communication, adapting roles based on environmental interactions.

Conclusion: The approach efficiently reduces computational costs while enabling flexible role adaptation, applicable to logistics, monitoring, and exploration.

Abstract: We present a reinforcement learning strategy for use in multi-agent foraging
systems in which the learning is centralised to a single agent and its model is
periodically disseminated among the population of non-learning agents. In a
domain where multi-agent reinforcement learning (MARL) is the common approach,
this approach aims to significantly reduce the computational and energy demands
compared to approaches such as MARL and centralised learning models. By
developing high performing foraging agents, these approaches can be translated
into real-world applications such as logistics, environmental monitoring, and
autonomous exploration. A reward function was incorporated into this approach
that promotes role development among agents, without explicit directives. This
led to the differentiation of behaviours among the agents. The implicit
encouragement of role differentiation allows for dynamic actions in which
agents can alter roles dependent on their interactions with the environment
without the need for explicit communication between agents.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)



<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [287] [Discovering phoneme-specific critical articulators through a data-driven approach](https://arxiv.org/abs/2505.00007)
*Jesuraj Bandekar, Sathvik Udupa, Prasanta Kumar Ghosh*

Main category: eess.AS

TL;DR: A machine learning approach to learn critical articulators for phonemes using end-to-end training with three models, combining AAI and phoneme prediction with normalized weights and dual losses.


<details>
  <summary>Details</summary>
Motivation: To identify and learn the importance of articulators for phonemes in speech production, enabling better understanding and prediction of speech articulations.

Method: Uses Acoustic to Articulatory Inversion (AAI) to predict EMA, phoneme-specific weights, and phone prediction with min-max normalization, dropout, and straight-through estimation for gradient flow.

Result: The model predicts articulator weights and phonemes simultaneously, trained end-to-end with cross-entropy and mean squared error losses.

Conclusion: The proposed method effectively learns articulator weights for phonemes, combining AAI and phoneme prediction in a unified framework.

Abstract: We propose an approach for learning critical articulators for phonemes
through a machine learning approach. We formulate the learning with three
models trained end to end. First, we use Acoustic to Articulatory Inversion
(AAI) to predict time-varying speech articulators EMA. We also predict the
phoneme-specific weights across articulators for each frame. To avoid
overfitting, we also add a dropout layer before the weights prediction layer.
Next, we normalize the predicted weights across articulators using min-max
normalization for each frame. The normalized weights are multiplied by the
ground truth $EMA$ and then we try to predict the phones at each frame. We
train this whole setup end to end and use two losses. One loss is for the phone
prediction which is the cross entropy loss and the other is for the AAI
prediction which is the mean squared error loss. To maintain gradient flow
between the phone prediction block and the $EMA$ prediction block, we use
straight-through estimation. The goal here is to predict the weights of the
articulator at each frame while training the model end to end.

</details>


### [288] [Perceptual Implications of Automatic Anonymization in Pathological Speech](https://arxiv.org/abs/2505.00409)
*Soroosh Tayebi Arasteh, Saba Afza, Tri-Thien Nguyen, Lukas Buess, Maryam Parvin, Tomas Arias-Vergara, Paula Andrea Perez-Toro, Hiu Ching Hung, Mahshad Lotfinia, Thomas Gorges, Elmar Noeth, Maria Schuster, Seung Hee Yang, Andreas Maier*

Main category: eess.AS

TL;DR: The study evaluates the perceptual impact of anonymizing pathological speech, revealing high discrimination accuracy but reduced quality, with variations by disorder and listener background.


<details>
  <summary>Details</summary>
Motivation: To understand the human perceptual consequences of anonymizing pathological speech, ensuring ethical data sharing without compromising interpretability or diagnostic utility.

Method: A structured perceptual protocol with 10 listeners evaluated anonymized-original speech pairs from 180 speakers across various disorders, using Turing-style discrimination and quality rating tasks.

Result: High discrimination accuracy (91-93%) but reduced perceived quality (83% to 59%), with disorder-specific patterns. No significant gender bias, and minimal native vs. non-native listener differences post-anonymization.

Conclusion: Listener-informed, disorder-specific anonymization strategies are needed to balance privacy with speech interpretability and clinical utility, especially for vulnerable groups.

Abstract: Automatic anonymization techniques are essential for ethical sharing of
pathological speech data, yet their perceptual consequences remain
understudied. This study presents the first comprehensive human-centered
analysis of anonymized pathological speech, using a structured perceptual
protocol involving ten native and non-native German listeners with diverse
linguistic, clinical, and technical backgrounds. Listeners evaluated
anonymized-original utterance pairs from 180 speakers spanning Cleft Lip and
Palate, Dysarthria, Dysglossia, Dysphonia, and age-matched healthy controls.
Speech was anonymized using state-of-the-art automatic methods (equal error
rates in the range of 30-40%). Listeners completed Turing-style discrimination
and quality rating tasks under zero-shot (single-exposure) and few-shot
(repeated-exposure) conditions. Discrimination accuracy was high overall (91%
zero-shot; 93% few-shot), but varied by disorder (repeated-measures ANOVA:
p=0.007), ranging from 96% (Dysarthria) to 86% (Dysphonia). Anonymization
consistently reduced perceived quality (from 83% to 59%, p<0.001), with
pathology-specific degradation patterns (one-way ANOVA: p=0.005). Native
listeners rated original speech slightly higher than non-native listeners
(Delta=4%, p=0.199), but this difference nearly disappeared after anonymization
(Delta=1%, p=0.724). No significant gender-based bias was observed. Critically,
human perceptual outcomes did not correlate with automatic privacy or clinical
utility metrics. These results underscore the need for listener-informed,
disorder- and context-specific anonymization strategies that preserve privacy
while maintaining interpretability, communicative functions, and diagnostic
utility, especially for vulnerable populations such as children.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [289] [Noise Modeling in One Hour: Minimizing Preparation Efforts for Self-supervised Low-Light RAW Image Denoising](https://arxiv.org/abs/2505.00045)
*Feiran Li, Haiyang Jiang, Daisuke Iso*

Main category: eess.IV

TL;DR: A simple noise synthesis pipeline for low-light RAW image denoising reduces preparation time from days to hours while improving performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the data shortage problem in low-light RAW image denoising by simplifying noise synthesis.

Method: Proposes a pipeline eliminating system gain calibration and signal-independent noise profiling.

Result: Achieves up to 0.54dB PSNR improvement over state-of-the-art methods.

Conclusion: The method is practical, efficient, and outperforms existing techniques.

Abstract: Noise synthesis is a promising solution for addressing the data shortage
problem in data-driven low-light RAW image denoising. However, accurate noise
synthesis methods often necessitate labor-intensive calibration and profiling
procedures during preparation, preventing them from landing to practice at
scale. This work introduces a practically simple noise synthesis pipeline based
on detailed analyses of noise properties and extensive justification of
widespread techniques. Compared to other approaches, our proposed pipeline
eliminates the cumbersome system gain calibration and signal-independent noise
profiling steps, reducing the preparation time for noise synthesis from days to
hours. Meanwhile, our method exhibits strong denoising performance, showing an
up to 0.54dB PSNR improvement over the current state-of-the-art noise synthesis
technique. Code is released at
https://github.com/SonyResearch/raw_image_denoising

</details>


### [290] [SR-NeRV: Improving Embedding Efficiency of Neural Video Representation via Super-Resolution](https://arxiv.org/abs/2505.00046)
*Taiga Hayami, Kakeru Koizumi, Hiroshi Watanabe*

Main category: eess.IV

TL;DR: An INR-based video compression method integrates a super-resolution network to improve high-frequency detail reconstruction, outperforming conventional INR methods while maintaining model size.


<details>
  <summary>Details</summary>
Motivation: Conventional INR-based video compression struggles with high-frequency detail reconstruction under model size constraints, which is critical for practical applications.

Method: Proposes an INR-based video representation integrating a super-resolution network to handle high-frequency details, leveraging their low temporal redundancy.

Result: The method outperforms conventional INR-based baselines in reconstruction quality without increasing model size.

Conclusion: Integrating a super-resolution network with INR improves video compression by better handling high-frequency details.

Abstract: Implicit Neural Representations (INRs) have garnered significant attention
for their ability to model complex signals across a variety of domains.
Recently, INR-based approaches have emerged as promising frameworks for neural
video compression. While conventional methods primarily focus on embedding
video content into compact neural networks for efficient representation, they
often struggle to reconstruct high-frequency details under stringent model size
constraints, which are critical in practical compression scenarios. To address
this limitation, we propose an INR-based video representation method that
integrates a general-purpose super-resolution (SR) network. Motivated by the
observation that high-frequency components exhibit low temporal redundancy
across frames, our method entrusts the reconstruction of fine details to the SR
network. Experimental results demonstrate that the proposed method outperforms
conventional INR-based baselines in terms of reconstruction quality, while
maintaining comparable model sizes.

</details>


### [291] [Rootlets-based registration to the spinal cord PAM50 template](https://arxiv.org/abs/2505.00115)
*Sandrine Bédard, Jan Valošek, Valeria Oliva, Kenneth A. Weber II, Julien Cohen-Adad*

Main category: eess.IV

TL;DR: A novel spinal cord registration method using spinal nerve rootlets improves alignment accuracy and reproducibility over traditional disc-based methods, enhancing fMRI group analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional disc-based spinal cord registration lacks precision due to anatomical variability between vertebral and spinal levels.

Method: Proposes a rootlet-based registration method using dorsal cervical rootlets segmentation and non-linear alignment with the PAM50 template, validated on multi-subject datasets.

Result: Rootlet-based registration outperformed disc-based methods in alignment accuracy, stability across neck positions, and improved fMRI activation detection (active voxels increased from 3292 to 7978).

Conclusion: Rootlet-based registration enhances spinal cord neuroimaging precision and reliability for group analyses.

Abstract: Spinal cord functional MRI studies require precise localization of spinal
levels for reliable voxelwise group analyses. Traditional template-based
registration of the spinal cord uses intervertebral discs for alignment.
However, substantial anatomical variability across individuals exists between
vertebral and spinal levels. This study proposes a novel registration approach
that leverages spinal nerve rootlets to improve alignment accuracy and
reproducibility across individuals. We developed a registration method
leveraging dorsal cervical rootlets segmentation and aligning them non-linearly
with the PAM50 spinal cord template. Validation was performed on a
multi-subject, multi-site dataset (n=267, 44 sites) and a multi-subject dataset
with various neck positions (n=10, 3 sessions). We further validated the method
on task-based functional MRI (n=23) to compare group-level activation maps
using rootlet-based registration to traditional disc-based methods.
Rootlet-based registration showed superior alignment across individuals
compared to the traditional disc-based method. Notably, rootlet positions were
more stable across neck positions. Group-level analysis of task-based
functional MRI using rootlet-based increased Z scores and activation cluster
size compared to disc-based registration (number of active voxels from 3292 to
7978). Rootlet-based registration enhances both inter- and intra-subject
anatomical alignment and yields better spatial normalization for group-level
fMRI analyses. Our findings highlight the potential of rootlet-based
registration to improve the precision and reliability of spinal cord
neuroimaging group analysis.

</details>


### [292] [Stereo X-ray tomography on deformed object tracking](https://arxiv.org/abs/2505.00122)
*Zhenduo Shang, Thomas Blumensath*

Main category: eess.IV

TL;DR: Extends stereo X-ray tomography to track 3D deformations of fiducial markers using initial shape priors, improving noise robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcome time constraints and noise sensitivity in dynamic X-ray tomography by leveraging prior knowledge of initial object shape.

Method: Extends static stereo X-ray tomography with temporal data, using initial stereo projections and 3D positions to track deformations.

Result: Achieves subpixel accuracy in tracking fiducial markers in noisy stereo images, with improved 3D mapping of deformed features.

Conclusion: The method enhances dynamic tomography by efficiently tracking deformations using prior information, proving robust against noise.

Abstract: X-ray computed tomography is a powerful tool for volumetric imaging, but
requires the collection of a large number of low-noise projection images, which
is often too time consuming, limiting its applicability. In our previous work
\cite{shang2023stereo}, we proposed a stereo X-ray tomography system to map the
3D position of fiducial markers using only two projections of a static volume.
In dynamic imaging settings, where objects undergo deformations during imaging,
this static method can be extended by utilizing additional temporal
information. We thus extend the method to track the deformation of fiducial
markers in 3D space, where we use knowledge of the initial object shape as
prior information, improving the prediction of the evolution of its deformed
state over time. In particular, knowledge of the initial object's stereo
projections is shown to improve the method's robustness to noise when detecting
fiducial marker locations in the projections of the deformed objects.
Furthermore, after feature detection, by using the features' initial 3D
position information in the undeformed object, we can also demonstrate
improvements in the 3D mapping of the deformed features. Using a range of
deformed 3D objects, this new approach is shown to be able to track fiducial
markers in noisy stereo tomography images with subpixel accuracy.

</details>


### [293] [Efficient and robust 3D blind harmonization for large domain gaps](https://arxiv.org/abs/2505.00133)
*Hwihun Jeong, Hayeon Lee, Se Young Chun, Jongho Lee*

Main category: eess.IV

TL;DR: BlindHarmonyDiff is a novel blind 3D harmonization framework using edge-to-image modeling and multi-stride patch training, outperforming existing methods in harmonizing MR images across diverse domains.


<details>
  <summary>Details</summary>
Motivation: Existing blind harmonization methods struggle with inter-slice heterogeneity, moderate image quality, and large domain gaps, prompting the need for a more robust solution.

Method: The framework employs a 3D rectified flow trained on target domain images to reconstruct from edge maps, with multi-stride patch training and a refinement module for hallucination suppression.

Result: BlindHarmonyDiff achieves superior harmonization, aligning source domain images closely with target domain characteristics, validated by downstream tasks like tissue segmentation and age prediction.

Conclusion: The proposed method is robust, generalizable, and effective for blind MR image harmonization, addressing key limitations of prior approaches.

Abstract: Blind harmonization has emerged as a promising technique for MR image
harmonization to achieve scale-invariant representations, requiring only target
domain data (i.e., no source domain data necessary). However, existing methods
face limitations such as inter-slice heterogeneity in 3D, moderate image
quality, and limited performance for a large domain gap. To address these
challenges, we introduce BlindHarmonyDiff, a novel blind 3D harmonization
framework that leverages an edge-to-image model tailored specifically to
harmonization. Our framework employs a 3D rectified flow trained on target
domain images to reconstruct the original image from an edge map, then yielding
a harmonized image from the edge of a source domain image. We propose
multi-stride patch training for efficient 3D training and a refinement module
for robust inference by suppressing hallucination. Extensive experiments
demonstrate that BlindHarmonyDiff outperforms prior arts by harmonizing diverse
source domain images to the target domain, achieving higher correspondence to
the target domain characteristics. Downstream task-based quality assessments
such as tissue segmentation and age prediction on diverse MR scanners further
confirm the effectiveness of our approach and demonstrate the capability of our
robust and generalizable blind harmonization.

</details>


### [294] [Towards Lightweight Hyperspectral Image Super-Resolution with Depthwise Separable Dilated Convolutional Network](https://arxiv.org/abs/2505.00374)
*Usman Muhammad, Jorma Laaksonen, Lyudmila Mihaylova*

Main category: eess.IV

TL;DR: A lightweight depthwise separable dilated convolutional network (DSDCN) is proposed for hyperspectral super-resolution, addressing challenges like high spectral dimensionality and limited training samples. It combines MobileNet-inspired architecture with a custom loss function for competitive performance.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral super-resolution is ill-posed due to high spectral dimensionality and scarce training data. Existing methods are impractical due to large models or fusion requirements.

Method: The DSDCN uses depthwise separable convolutions (like MobileNet) and a dilated convolution fusion block for spatial-spectral feature extraction. A custom loss function combines MSE, L2 regularization, and spectral angle-based loss.

Result: The model achieves competitive performance on two public hyperspectral datasets.

Conclusion: DSDCN is effective for hyperspectral super-resolution, balancing performance and practicality. Code is publicly available.

Abstract: Deep neural networks have demonstrated highly competitive performance in
super-resolution (SR) for natural images by learning mappings from
low-resolution (LR) to high-resolution (HR) images. However, hyperspectral
super-resolution remains an ill-posed problem due to the high spectral
dimensionality of the data and the scarcity of available training samples.
Moreover, existing methods often rely on large models with a high number of
parameters or require the fusion with panchromatic or RGB images, both of which
are often impractical in real-world scenarios. Inspired by the MobileNet
architecture, we introduce a lightweight depthwise separable dilated
convolutional network (DSDCN) to address the aforementioned challenges.
Specifically, our model leverages multiple depthwise separable convolutions,
similar to the MobileNet architecture, and further incorporates a dilated
convolution fusion block to make the model more flexible for the extraction of
both spatial and spectral features. In addition, we propose a custom loss
function that combines mean squared error (MSE), an L2 norm
regularization-based constraint, and a spectral angle-based loss, ensuring the
preservation of both spectral and spatial details. The proposed model achieves
very competitive performance on two publicly available hyperspectral datasets,
making it well-suited for hyperspectral image super-resolution tasks. The
source codes are publicly available at:
\href{https://github.com/Usman1021/lightweight}{https://github.com/Usman1021/lightweight}.

</details>


### [295] [CORSTITCH - A free, open source software for stitching and georeferencing underwater coral reef videos](https://arxiv.org/abs/2505.00462)
*Julian Christopher L. Maya, Johnenn R. Manalang, Maricor N. Soriano*

Main category: eess.IV

TL;DR: CorStitch automates georeferenced reef mosaics from video transects using Fourier-based image correlation, validated for reliability.


<details>
  <summary>Details</summary>
Motivation: To streamline the creation of accurate georeferenced reef mosaics for spatial analysis in reef surveys.

Method: Uses Fourier-based image correlation to stitch video frames, aligned with GNSS timestamps, outputting GIS-compatible files.

Result: Produces reliable mosaics validated by comparative analysis of temporally distinct reef surveys.

Conclusion: CorStitch is a consistent and reliable tool for reef mosaic creation and spatial analysis.

Abstract: CorStitch is an open-source software developed to automate the creation of
accurate georeferenced reef mosaics from video transects obtained through
Automated Rapid Reef Assessment System surveys. We utilized a Fourier-based
image correlation algorithm to stitch sequential video frames, aligning them
with synchronized GNSS timestamps. The resulting compressed Keyhole Markup
Language files, compatible with geographic information systems such as Google
Earth, enable detailed spatial analysis. Validation through comparative
analysis of mosaics from two temporally distinct surveys of the same reef
demonstrated the software's consistent and reliable performance.

</details>


### [296] [A Methodological and Structural Review of Parkinsons Disease Detection Across Diverse Data Modalities](https://arxiv.org/abs/2505.00525)
*Abu Saleh Musa Miah, taro Suzuki, Jungpil Shin*

Main category: eess.IV

TL;DR: A comprehensive review of Parkinson's Disease (PD) recognition systems using multimodal data and ML/DL techniques to improve early diagnosis and patient outcomes.


<details>
  <summary>Details</summary>
Motivation: Early and accurate PD diagnosis is crucial, but existing surveys lack coverage of multimodal approaches, leaving gaps in research.

Method: Review of 347 articles on PD recognition systems, analyzing data modalities like MRI, gait analysis, EEG, and multimodal fusion techniques.

Result: Identifies key aspects of data collection, feature representation, and system performance, emphasizing accuracy and robustness.

Conclusion: The survey provides actionable guidance for developing next-gen PD recognition systems, advancing diagnostics through multimodal ML/DL approaches.

Abstract: Parkinsons Disease (PD) is a progressive neurological disorder that primarily
affects motor functions and can lead to mild cognitive impairment (MCI) and
dementia in its advanced stages. With approximately 10 million people diagnosed
globally 1 to 1.8 per 1,000 individuals, according to reports by the Japan
Times and the Parkinson Foundation early and accurate diagnosis of PD is
crucial for improving patient outcomes. While numerous studies have utilized
machine learning (ML) and deep learning (DL) techniques for PD recognition,
existing surveys are limited in scope, often focusing on single data modalities
and failing to capture the potential of multimodal approaches. To address these
gaps, this study presents a comprehensive review of PD recognition systems
across diverse data modalities, including Magnetic Resonance Imaging (MRI),
gait-based pose analysis, gait sensory data, handwriting analysis, speech test
data, Electroencephalography (EEG), and multimodal fusion techniques. Based on
over 347 articles from leading scientific databases, this review examines key
aspects such as data collection methods, settings, feature representations, and
system performance, with a focus on recognition accuracy and robustness. This
survey aims to serve as a comprehensive resource for researchers, providing
actionable guidance for the development of next generation PD recognition
systems. By leveraging diverse data modalities and cutting-edge machine
learning paradigms, this work contributes to advancing the state of PD
diagnostics and improving patient care through innovative, multimodal
approaches.

</details>


### [297] [AI-Driven High-Resolution Cell Segmentation and Quantitative Analysis](https://arxiv.org/abs/2505.00578)
*Shuang Zhang, Carleton Coffin, Karyn L. Rogers, Catherine Ann Royer, Ge Wang*

Main category: eess.IV

TL;DR: An AI-driven image analysis system for microbial cell segmentation and feature analysis, improving accuracy without human annotations.


<details>
  <summary>Details</summary>
Motivation: To study microbial growth and metabolism in harsh environments, advancing microbial research and biotechnology.

Method: Developed a four-module system: denoising, SAM for segmentation, post-processing, and quantitative feature extraction.

Result: Denoising and post-processing enhanced SAM's segmentation accuracy, enabling automated, high-accuracy cellular analysis.

Conclusion: The framework automates microbial image analysis, aiding research on extremophile adaptations.

Abstract: Studying the growth and metabolism of microbes provides critical insights
into their evolutionary adaptations to harsh environments, which are essential
for microbial research and biotechnology applications. In this study, we
developed an AI-driven image analysis system to efficiently segment individual
cells and quantitatively analyze key cellular features. This system is
comprised of four main modules. First, a denoising algorithm enhances contrast
and suppresses noise while preserving fine cellular details. Second, the
Segment Anything Model (SAM) enables accurate, zero-shot segmentation of cells
without additional training. Third, post-processing is applied to refine
segmentation results by removing over-segmented masks. Finally, quantitative
analysis algorithms extract essential cellular features, including average
intensity, length, width, and volume. The results show that denoising and
post-processing significantly improved the segmentation accuracy of SAM in this
new domain. Without human annotations, the AI-driven pipeline automatically and
efficiently outlines cellular boundaries, indexes them, and calculates key
cellular parameters with high accuracy. This framework will enable efficient
and automated quantitative analysis of high-resolution fluorescence microscopy
images to advance research into microbial adaptations to grow and metabolism
that allow extremophiles to thrive in their harsh habitats.

</details>


### [298] [Deep Learning Assisted Outer Volume Removal for Highly-Accelerated Real-Time Dynamic MRI](https://arxiv.org/abs/2505.00643)
*Merve Gülle, Sebastian Weingärtner, Mehmet Akçakaya*

Main category: eess.IV

TL;DR: A novel outer volume removal (OVR) method using deep learning (DL) is proposed to reduce aliasing artifacts in real-time (RT) cine MRI, achieving high acceleration rates without compromising diagnostic quality.


<details>
  <summary>Details</summary>
Motivation: RT cine MRI is crucial for cardiac functional assessment but faces challenges like aliasing artifacts at high undersampling rates, limiting acceleration.

Method: The OVR method estimates and removes outer volume signals using DL, followed by physics-driven DL reconstruction with an OVR-specific loss function.

Result: The method achieves image quality comparable to clinical baselines at high accelerations, outperforming conventional techniques.

Conclusion: The approach offers a practical solution for artifact reduction in RT cine MRI, enabling higher acceleration rates without acquisition changes.

Abstract: Real-time (RT) dynamic MRI plays a vital role in capturing rapid
physiological processes, offering unique insights into organ motion and
function. Among these applications, RT cine MRI is particularly important for
functional assessment of the heart with high temporal resolution. RT imaging
enables free-breathing, ungated imaging of cardiac motion, making it a crucial
alternative for patients who cannot tolerate conventional breath-hold,
ECG-gated acquisitions. However, achieving high acceleration rates in RT cine
MRI is challenging due to aliasing artifacts from extra-cardiac tissues,
particularly at high undersampling factors. In this study, we propose a novel
outer volume removal (OVR) method to address this challenge by eliminating
aliasing contributions from non-cardiac regions in a post-processing framework.
Our approach estimates the outer volume signal for each timeframe using
composite temporal images from time-interleaved undersampling patterns, which
inherently contain pseudo-periodic ghosting artifacts. A deep learning (DL)
model is trained to identify and remove these artifacts, producing a clean
outer volume estimate that is subsequently subtracted from the corresponding
k-space data. The final reconstruction is performed with a physics-driven DL
(PD-DL) method trained using an OVR-specific loss function to restore high
spatio-temporal resolution images. Experimental results show that the proposed
method at high accelerations achieves image quality that is visually comparable
to clinical baseline images, while outperforming conventional reconstruction
techniques, both qualitatively and quantitatively. The proposed approach
provides a practical and effective solution for artifact reduction in RT cine
MRI without requiring acquisition modifications, offering a pathway to higher
acceleration rates while preserving diagnostic quality.

</details>


### [299] [GuideSR: Rethinking Guidance for One-Step High-Fidelity Diffusion-Based Super-Resolution](https://arxiv.org/abs/2505.00687)
*Aditya Arora, Zhengzhong Tu, Yufei Wang, Ruizheng Bai, Jian Wang, Sizhuo Ma*

Main category: eess.IV

TL;DR: GuideSR is a single-step diffusion-based image super-resolution model that enhances image fidelity using a dual-branch architecture, outperforming existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based SR methods compromise structural fidelity by relying on degraded input conditioning. GuideSR aims to preserve high-fidelity structures while enhancing perceptual quality.

Method: GuideSR uses a dual-branch architecture: a Guidance Branch for preserving structures and a Diffusion Branch for perceptual enhancement. It incorporates Full Resolution Blocks, channel attention, and an Image Guidance Network.

Result: GuideSR achieves state-of-the-art performance with up to 1.39dB PSNR gain, excelling in metrics like PSNR, SSIM, LPIPS, DISTS, and FID.

Conclusion: GuideSR advances real-world image restoration by combining structural fidelity and perceptual quality in a computationally efficient single-step approach.

Abstract: In this paper, we propose GuideSR, a novel single-step diffusion-based image
super-resolution (SR) model specifically designed to enhance image fidelity.
Existing diffusion-based SR approaches typically adapt pre-trained generative
models to image restoration tasks by adding extra conditioning on a
VAE-downsampled representation of the degraded input, which often compromises
structural fidelity. GuideSR addresses this limitation by introducing a
dual-branch architecture comprising: (1) a Guidance Branch that preserves
high-fidelity structures from the original-resolution degraded input, and (2) a
Diffusion Branch, which a pre-trained latent diffusion model to enhance
perceptual quality. Unlike conventional conditioning mechanisms, our Guidance
Branch features a tailored structure for image restoration tasks, combining
Full Resolution Blocks (FRBs) with channel attention and an Image Guidance
Network (IGN) with guided attention. By embedding detailed structural
information directly into the restoration pipeline, GuideSR produces sharper
and more visually consistent results. Extensive experiments on benchmark
datasets demonstrate that GuideSR achieves state-of-the-art performance while
maintaining the low computational cost of single-step approaches, with up to
1.39dB PSNR gain on challenging real-world datasets. Our approach consistently
outperforms existing methods across various reference-based metrics including
PSNR, SSIM, LPIPS, DISTS and FID, further representing a practical advancement
for real-world image restoration.

</details>


### [300] [Segment-and-Classify: ROI-Guided Generalizable Contrast Phase Classification in CT Using XGBoost](https://arxiv.org/abs/2501.14066)
*Benjamin Hou, Tejas Sudharshan Mathai, Pritam Mukherjee, Xinya Wang, Ronald M. Summers, Zhiyong Lu*

Main category: eess.IV

TL;DR: A lightweight decision tree classifier using organ-specific features from TotalSegmentator effectively automates contrast phase classification in CT scans, showing strong performance and generalizability across datasets.


<details>
  <summary>Details</summary>
Motivation: To automate and improve the accuracy of contrast phase classification in CT scans using a simple yet effective method.

Method: Utilized organ-specific features from TotalSegmentator and trained a gradient-boosted decision tree classifier on one dataset, validating it on two others.

Result: Achieved high AUCs (>0.937) and superior F1-scores in most phases, with significant improvements in arterial and delayed phases.

Conclusion: The model is lightweight, performs well, and generalizes robustly across different datasets.

Abstract: Purpose: To automate contrast phase classification in CT using organ-specific
features extracted from a widely used segmentation tool with a lightweight
decision tree classifier.
  Materials and Methods: This retrospective study utilized three public CT
datasets from separate institutions. The phase prediction model was trained on
the WAW-TACE (median age: 66 [60,73]; 185 males) dataset, and externally
validated on the VinDr-Multiphase (146 males; 63 females; 56 unk) and C4KC-KiTS
(median age: 61 [50.68; 123 males) datasets. Contrast phase classification was
performed using organ-specific features extracted by TotalSegmentator, followed
by prediction using a gradient-boosted decision tree classifier.
  Results: On the VinDr-Multiphase dataset, the phase prediction model achieved
the highest or comparable AUCs across all phases (>0.937), with superior
F1-scores in the non-contrast (0.994), arterial (0.937), and delayed (0.718)
phases. Statistical testing indicated significant performance differences only
in the arterial and delayed phases (p<0.05). On the C4KC-KiTS dataset, the
phase prediction model achieved the highest AUCs across all phases (>0.991),
with superior F1-scores in arterial/venous (0.968) and delayed (0.935) phases.
Statistical testing confirmed significant improvements over all baseline models
in these two phases (p<0.05). Performance in the non-contrast class, however,
was comparable across all models, with no statistically significant differences
observed (p>0.05).
  Conclusion: The lightweight model demonstrated strong performance relative to
all baseline models, and exhibited robust generalizability across datasets from
different institutions.

</details>
