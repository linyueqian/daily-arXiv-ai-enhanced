{"id": "2504.16956", "pdf": "https://arxiv.org/pdf/2504.16956", "abs": "https://arxiv.org/abs/2504.16956", "authors": ["Cong Qi", "Hanzhang Fang", "Tianxing Hu", "Siqi Jiang", "Wei Zhi"], "title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of\ncellular heterogeneity, but its complexity, which is marked by high\ndimensionality, sparsity, and batch effects, which poses major computational\nchallenges. Transformer-based models have made significant advances in this\ndomain but are often limited by their quadratic complexity and suboptimal\nhandling of long-range dependencies. In this work, we introduce GeneMamba, a\nscalable and efficient foundation model for single-cell transcriptomics built\non state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba\ncaptures bidirectional gene context with linear-time complexity, offering\nsubstantial computational gains over transformer baselines. The model is\npretrained on nearly 30 million cells and incorporates biologically informed\nobjectives, including pathway-aware contrastive loss and rank-based gene\nencoding. We evaluate GeneMamba across diverse tasks, including multi-batch\nintegration, cell type annotation, and gene-gene correlation, demonstrating\nstrong performance, interpretability, and robustness. These results position\nGeneMamba as a practical and powerful alternative to transformer-based methods,\nadvancing the development of biologically grounded, scalable tools for\nlarge-scale single-cell data analysis.", "AI": {"tldr": "GeneMamba is a scalable, efficient foundation model for single-cell RNA-seq data, using state space modeling to outperform transformer-based methods.", "motivation": "Addressing computational challenges in scRNA-seq analysis, such as high dimensionality and batch effects, where transformer models fall short due to quadratic complexity and poor long-range dependency handling.", "method": "GeneMamba employs a Bi-Mamba architecture for bidirectional gene context with linear-time complexity, pretrained on 30M cells with biologically informed objectives like pathway-aware contrastive loss.", "result": "GeneMamba excels in tasks like multi-batch integration, cell type annotation, and gene-gene correlation, showing strong performance and interpretability.", "conclusion": "GeneMamba is a robust, scalable alternative to transformer-based methods, enhancing large-scale single-cell data analysis."}}
{"id": "2504.17156", "pdf": "https://arxiv.org/pdf/2504.17156", "abs": "https://arxiv.org/abs/2504.17156", "authors": ["Jiadong Xie", "Yunlian Zhou", "Mingsheng Xu"], "title": "Waveform-Logmel Audio Neural Networks for Respiratory Sound Classification", "categories": ["cs.SD"], "comment": null, "summary": "Auscultatory analysis using an electronic stethoscope has attracted\nincreasing attention in the clinical diagnosis of respiratory diseases.\nRecently, neural networks have been applied to assist in respiratory sound\nclassification with achievements. However, it remains challenging due to the\nscarcity of abnormal respiratory sound. In this paper, we propose a novel\narchitecture, namely Waveform-Logmel audio neural networks (WLANN), which uses\nboth waveform and log-mel spectrogram as the input features and uses\nBidirectional Gated Recurrent Units (Bi-GRU) to context model the fused\nfeatures. Experimental results of our WLANN applied to SPRSound respiratory\ndataset show that the proposed framework can effectively distinguish\npathological respiratory sound classes, outperforming the previous studies,\nwith 90.3% in sensitivity and 93.6% in total score. Our study demonstrates the\nhigh effectiveness of the WLANN in the diagnosis of respiratory diseases.", "AI": {"tldr": "A novel neural network architecture (WLANN) using waveform and log-mel spectrogram features with Bi-GRU improves respiratory sound classification, achieving high sensitivity and total score.", "motivation": "Challenges in classifying abnormal respiratory sounds due to scarcity of data motivate the development of a more effective method.", "method": "Proposes WLANN, combining waveform and log-mel spectrogram features with Bi-GRU for context modeling.", "result": "Achieves 90.3% sensitivity and 93.6% total score on the SPRSound dataset, outperforming previous methods.", "conclusion": "WLANN is highly effective for respiratory disease diagnosis, demonstrating superior performance in sound classification."}}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977", "abs": "https://arxiv.org/abs/2504.16977", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications.", "AI": {"tldr": "SentencePiece outperforms BPE and Character Level tokenization for NER in low-resource Indic languages, especially in zero-shot cross-lingual settings, due to better entity consistency and morphological preservation.", "motivation": "The study aims to evaluate tokenization methods (BPE, SentencePiece, Character Level) for NER in low-resource Indic languages, addressing BPE's limitations in handling morphological complexity.", "method": "Systematic comparison of BPE, SentencePiece, and Character Level tokenization using IndicBERT for NER tasks in low-resource Indic languages, assessing intrinsic (e.g., OOV rates) and extrinsic (e.g., zero-shot transfer) metrics.", "result": "SentencePiece performs best, preserving entity consistency and generalizing well, especially for morphologically rich languages like Santali and Manipuri, while BPE struggles with unseen languages.", "conclusion": "SentencePiece is the most effective tokenization strategy for NER in multilingual and low-resource Indic NLP applications, offering superior generalization and linguistic preservation."}}
{"id": "2504.17586", "pdf": "https://arxiv.org/pdf/2504.17586", "abs": "https://arxiv.org/abs/2504.17586", "authors": ["Xuyi Hu", "Jian Li", "Lorenzo Picinali", "Aidan O. T. Hogg"], "title": "A Machine Learning Approach for Denoising and Upsampling HRTFs", "categories": ["cs.SD", "cs.LG"], "comment": null, "summary": "The demand for realistic virtual immersive audio continues to grow, with\nHead-Related Transfer Functions (HRTFs) playing a key role. HRTFs capture how\nsound reaches our ears, reflecting unique anatomical features and enhancing\nspatial perception. It has been shown that personalized HRTFs improve\nlocalization accuracy, but their measurement remains time-consuming and\nrequires a noise-free environment. Although machine learning has been shown to\nreduce the required measurement points and, thus, the measurement time, a\ncontrolled environment is still necessary. This paper proposes a method to\naddress this constraint by presenting a novel technique that can upsample\nsparse, noisy HRTF measurements. The proposed approach combines an HRTF Denoisy\nU-Net for denoising and an Autoencoding Generative Adversarial Network (AE-GAN)\nfor upsampling from three measurement points. The proposed method achieves a\nlog-spectral distortion (LSD) error of 5.41 dB and a cosine similarity loss of\n0.0070, demonstrating the method's effectiveness in HRTF upsampling.", "AI": {"tldr": "A novel method using HRTF Denoisy U-Net and AE-GAN upsamples sparse, noisy HRTF measurements, achieving low LSD error and cosine similarity loss.", "motivation": "Personalized HRTFs improve spatial audio but require time-consuming, noise-free measurements. Machine learning reduces points but still needs controlled environments.", "method": "Combines HRTF Denoisy U-Net for denoising and AE-GAN for upsampling from three measurement points.", "result": "Achieves LSD error of 5.41 dB and cosine similarity loss of 0.0070.", "conclusion": "The method effectively upsamples HRTFs, addressing measurement constraints."}}
{"id": "2504.17025", "pdf": "https://arxiv.org/pdf/2504.17025", "abs": "https://arxiv.org/abs/2504.17025", "authors": ["Luca Moroni", "Giovanni Puccetti", "Pere-Lluis Huguet Cabot", "Andrei Stefan Bejgu", "Edoardo Barba", "Alessio Miaschi", "Felice Dell'Orletta", "Andrea Esuli", "Roberto Navigli"], "title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "The number of pretrained Large Language Models (LLMs) is increasing steadily,\nthough the majority are designed predominantly for the English language. While\nstate-of-the-art LLMs can handle other languages, due to language contamination\nor some degree of multilingual pretraining data, they are not optimized for\nnon-English languages, leading to inefficient encoding (high token \"fertility\")\nand slower inference speed. In this work, we thoroughly compare a variety of\nvocabulary adaptation techniques for optimizing English LLMs for the Italian\nlanguage, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a\nnovel method that leverages neural mapping for vocabulary substitution. SAVA\nachieves competitive performance across multiple downstream tasks, enhancing\ngrounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing\ntoken fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and\nreducing the number of parameters by 1 billion. We show that, following the\nadaptation of the vocabulary, these models can recover their performance with a\nrelatively limited stage of continual training on the target language. Finally,\nwe test the capabilities of the adapted models on various multi-choice and\ngenerative tasks.", "AI": {"tldr": "The paper introduces SAVA, a method to optimize English LLMs for Italian, reducing token fertility and parameters while maintaining performance.", "motivation": "English LLMs are inefficient for non-English languages due to high token fertility and slow inference.", "method": "SAVA leverages neural mapping for vocabulary substitution, adapting Mistral-7b-v0.1 and Llama-3.1-8B for Italian.", "result": "Token fertility reduced by 25%, parameters by 1B; models recover performance with minimal target-language training.", "conclusion": "SAVA effectively optimizes LLMs for Italian, improving efficiency and performance on downstream tasks."}}
{"id": "2504.17782", "pdf": "https://arxiv.org/pdf/2504.17782", "abs": "https://arxiv.org/abs/2504.17782", "authors": ["Xize Cheng", "Slytherin Wang", "Zehan Wang", "Rongjie Huang", "Tao Jin", "Zhou Zhao"], "title": "Unleashing the Power of Natural Audio Featuring Multiple Sound Sources", "categories": ["cs.SD", "cs.LG"], "comment": "Work in Progress", "summary": "Universal sound separation aims to extract clean audio tracks corresponding\nto distinct events from mixed audio, which is critical for artificial auditory\nperception. However, current methods heavily rely on artificially mixed audio\nfor training, which limits their ability to generalize to naturally mixed audio\ncollected in real-world environments. To overcome this limitation, we propose\nClearSep, an innovative framework that employs a data engine to decompose\ncomplex naturally mixed audio into multiple independent tracks, thereby\nallowing effective sound separation in real-world scenarios. We introduce two\nremix-based evaluation metrics to quantitatively assess separation quality and\nuse these metrics as thresholds to iteratively apply the data engine alongside\nmodel training, progressively optimizing separation performance. In addition,\nwe propose a series of training strategies tailored to these separated\nindependent tracks to make the best use of them. Extensive experiments\ndemonstrate that ClearSep achieves state-of-the-art performance across multiple\nsound separation tasks, highlighting its potential for advancing sound\nseparation in natural audio scenarios. For more examples and detailed results,\nplease visit our demo page at https://clearsep.github.io.", "AI": {"tldr": "ClearSep is a framework for universal sound separation using a data engine to decompose naturally mixed audio, improving generalization to real-world scenarios.", "motivation": "Current methods rely on artificially mixed audio, limiting generalization to natural audio. ClearSep addresses this by using naturally mixed data.", "method": "ClearSep employs a data engine to decompose complex audio, uses remix-based metrics for evaluation, and iteratively optimizes performance with tailored training strategies.", "result": "ClearSep achieves state-of-the-art performance in sound separation tasks, demonstrating effectiveness in natural audio scenarios.", "conclusion": "ClearSep advances sound separation by leveraging naturally mixed audio and iterative optimization, showing promise for real-world applications."}}
{"id": "2504.17052", "pdf": "https://arxiv.org/pdf/2504.17052", "abs": "https://arxiv.org/abs/2504.17052", "authors": ["Shariar Kabir", "Kevin Esterling", "Yue Dong"], "title": "Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models", "categories": ["cs.CL"], "comment": "20 pages, 9 figures", "summary": "Large Language Models (LLMs) are increasingly shaping political discourse,\nyet their responses often display inconsistency when subjected to scrutiny.\nWhile prior research has primarily categorized LLM outputs as left- or\nright-leaning to assess their political stances, a critical question remains:\nDo these responses reflect genuine internal beliefs or merely surface-level\nalignment with training data? To address this, we propose a novel framework for\nevaluating belief depth by analyzing (1) argumentative consistency and (2)\nuncertainty quantification. We evaluate 12 LLMs on 19 economic policies from\nthe Political Compass Test, challenging their belief stability with both\nsupportive and opposing arguments. Our analysis reveals that LLMs exhibit\ntopic-specific belief stability rather than a uniform ideological stance.\nNotably, up to 95% of left-leaning models' responses and 89% of right-leaning\nmodels' responses remain consistent under the challenge, enabling semantic\nentropy to achieve high accuracy (AUROC=0.78), effectively distinguishing\nbetween surface-level alignment from genuine belief. These findings call into\nquestion the assumption that LLMs maintain stable, human-like political\nideologies, emphasizing the importance of conducting topic-specific reliability\nassessments for real-world applications.", "AI": {"tldr": "The paper investigates whether LLMs' political responses reflect genuine beliefs or surface-level alignment, proposing a framework to evaluate belief depth via argumentative consistency and uncertainty quantification.", "motivation": "To determine if LLMs' political stances are stable and genuine or merely aligned with training data.", "method": "A novel framework analyzing argumentative consistency and uncertainty quantification, tested on 12 LLMs using 19 economic policies from the Political Compass Test.", "result": "LLMs show topic-specific belief stability, with high consistency under challenge (95% left-leaning, 89% right-leaning). Semantic entropy effectively distinguishes genuine beliefs (AUROC=0.78).", "conclusion": "LLMs lack stable, human-like ideologies; topic-specific reliability assessments are crucial for real-world use."}}
{"id": "2504.16936", "pdf": "https://arxiv.org/pdf/2504.16936", "abs": "https://arxiv.org/abs/2504.16936", "authors": ["Yusheng Zhao", "Junyu Luo", "Xiao Luo", "Weizhi Zhang", "Zhiping Xiao", "Wei Ju", "Philip S. Yu", "Ming Zhang"], "title": "Multifaceted Evaluation of Audio-Visual Capability for MLLMs: Effectiveness, Efficiency, Generalizability and Robustness", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Multi-modal large language models (MLLMs) have recently achieved great\nsuccess in processing and understanding information from diverse modalities\n(e.g., text, audio, and visual signals). Despite their growing popularity,\nthere remains a lack of comprehensive evaluation measuring the audio-visual\ncapabilities of these models, especially in diverse scenarios (e.g.,\ndistribution shifts and adversarial attacks). In this paper, we present a\nmultifaceted evaluation of the audio-visual capability of MLLMs, focusing on\nfour key dimensions: effectiveness, efficiency, generalizability, and\nrobustness. Through extensive experiments, we find that MLLMs exhibit strong\nzero-shot and few-shot generalization abilities, enabling them to achieve great\nperformance with limited data. However, their success relies heavily on the\nvision modality, which impairs performance when visual input is corrupted or\nmissing. Additionally, while MLLMs are susceptible to adversarial samples, they\ndemonstrate greater robustness compared to traditional models. The experimental\nresults and our findings provide insights into the audio-visual capabilities of\nMLLMs, highlighting areas for improvement and offering guidance for future\nresearch.", "AI": {"tldr": "The paper evaluates the audio-visual capabilities of multi-modal large language models (MLLMs), focusing on effectiveness, efficiency, generalizability, and robustness. Findings show strong generalization but reliance on vision, susceptibility to adversarial attacks, and comparative robustness.", "motivation": "There is a lack of comprehensive evaluation of MLLMs' audio-visual capabilities, especially under diverse scenarios like distribution shifts and adversarial attacks.", "method": "A multifaceted evaluation of MLLMs is conducted, focusing on four dimensions: effectiveness, efficiency, generalizability, and robustness, through extensive experiments.", "result": "MLLMs show strong zero-shot and few-shot generalization but rely heavily on vision, perform poorly with corrupted visual input, and are susceptible yet more robust to adversarial attacks than traditional models.", "conclusion": "The study provides insights into MLLMs' audio-visual capabilities, identifying strengths and weaknesses, and guiding future research."}}
{"id": "2504.17440", "pdf": "https://arxiv.org/pdf/2504.17440", "abs": "https://arxiv.org/abs/2504.17440", "authors": ["Tao Zhuang", "Shaozhe Li", "Feng Niu", "Jia-Xin Zhong", "Jing Lu"], "title": "Generating Localized Audible Zones Using a Single-Channel Parametric Loudspeaker", "categories": ["eess.AS"], "comment": null, "summary": "Advanced sound zone control (SZC) techniques typically rely on massive\nmulti-channel loudspeaker arrays to create high-contrast personal sound zones,\nmaking single-loudspeaker SZC seem impossible. In this Letter, we challenge\nthis paradigm by introducing the multi-carrier parametric loudspeaker (MCPL),\nwhich enables SZC using only a single loudspeaker. In our approach, distinct\naudio signals are modulated onto separate ultrasonic carrier waves at different\nfrequencies and combined into a single composite signal. This signal is emitted\nby a single-channel ultrasonic transducer, and through nonlinear demodulation\nin air, the audio signals interact to virtually form multi-channel outputs.\nThis novel capability allows the application of existing SZC algorithms\noriginally designed for multi-channel loudspeaker arrays. Simulations validate\nthe effectiveness of our proposed single-channel MCPL, demonstrating its\npotential as a promising alternative to traditional multi-loudspeaker systems\nfor achieving high-contrast SZC. Our work opens new avenues for simplifying SZC\nsystems without compromising performance.", "AI": {"tldr": "Single-loudspeaker sound zone control (SZC) is achieved using a multi-carrier parametric loudspeaker (MCPL), challenging the need for massive multi-channel arrays.", "motivation": "Traditional SZC requires multi-channel loudspeaker arrays, which are complex and resource-intensive. This work aims to simplify SZC systems without sacrificing performance.", "method": "Distinct audio signals are modulated onto separate ultrasonic carriers, combined into a single signal, and emitted by a single-channel ultrasonic transducer. Nonlinear demodulation in air creates virtual multi-channel outputs.", "result": "Simulations show the MCPL effectively enables SZC with a single loudspeaker, matching the performance of traditional multi-channel systems.", "conclusion": "The MCPL offers a simpler, high-performance alternative to multi-loudspeaker SZC systems, opening new possibilities for practical applications."}}
{"id": "2504.17409", "pdf": "https://arxiv.org/pdf/2504.17409", "abs": "https://arxiv.org/abs/2504.17409", "authors": ["Tianhao Shao", "Bohan Feng", "Yingying Zhou", "Bin Guo", "Kaixing Zhao"], "title": "AGCo-MATA: Air-Ground Collaborative Multi-Agent Task Allocation in Mobile Crowdsensing", "categories": ["cs.MA"], "comment": null, "summary": "Rapid progress in intelligent unmanned systems has presented new\nopportunities for mobile crowd sensing (MCS). Today, heterogeneous air-ground\ncollaborative multi-agent framework, which comprise unmanned aerial vehicles\n(UAVs) and unmanned ground vehicles (UGVs), have presented superior flexibility\nand efficiency compared to traditional homogeneous frameworks in complex\nsensing tasks. Within this context, task allocation among different agents\nalways play an important role in improving overall MCS quality. In order to\nbetter allocate tasks among heterogeneous collaborative agents, in this paper,\nwe investigated two representative complex multi-agent task allocation\nscenarios with dual optimization objectives: (1) For AG-FAMT (Air-Ground Few\nAgents More Tasks) scenario, the objectives are to maximize the task completion\nwhile minimizing the total travel distance; (2) For AG-MAFT (Air-Ground More\nAgents Few Tasks) scenario, where the agents are allocated based on their\nlocations, has the optimization objectives of minimizing the total travel\ndistance while reducing travel time cost. To achieve this, we proposed a\nMulti-Task Minimum Cost Maximum Flow (MT-MCMF) optimization algorithm tailored\nfor AG-FAMT, along with a multi-objective optimization algorithm called W-ILP\ndesigned for AG-MAFT, with a particular focus on optimizing the charging path\nplanning of UAVs. Our experiments based on a large-scale real-world dataset\ndemonstrated that the proposed two algorithms both outperform baseline\napproaches under varying experimental settings, including task quantity, task\ndifficulty, and task distribution, providing a novel way to improve the overall\nquality of mobile crowdsensing tasks.", "AI": {"tldr": "The paper proposes two algorithms for task allocation in heterogeneous air-ground collaborative multi-agent systems, optimizing task completion and travel efficiency in different scenarios.", "motivation": "To improve task allocation efficiency in mobile crowd sensing (MCS) using UAVs and UGVs, addressing challenges in heterogeneous multi-agent frameworks.", "method": "Developed MT-MCMF for AG-FAMT (maximizing task completion and minimizing travel distance) and W-ILP for AG-MAFT (minimizing travel distance and time cost).", "result": "Experiments showed both algorithms outperform baselines in task quantity, difficulty, and distribution scenarios.", "conclusion": "The proposed algorithms enhance MCS quality, offering a novel approach for heterogeneous agent task allocation."}}
{"id": "2504.16964", "pdf": "https://arxiv.org/pdf/2504.16964", "abs": "https://arxiv.org/abs/2504.16964", "authors": ["Danielle Mbambe Bebey"], "title": "Social sustainability through engagement in a training context with tools such as the Native Podcast and Facebook social network", "categories": ["cs.CY", "cs.MM", "cs.SI"], "comment": "in French language", "summary": "The social dimension of sustainability seems to have been a notion rarely\naddressed in the literature (Dubois et al., 2001) until the early 2000s. The\nEUTIC 2023 symposium provides an opportunity to take up this topical issue. To\nthis end, we are presenting an engagement process that is part of a sustainable\ndevelopment dynamic, based on digital tools inspired by everyday life, for\napplications in the context of training, with a view to lifelong learning. Our\nwork, which stems from the information and communication sciences, is rooted in\na multi-disciplinary approach that we believe can be echoed in a variety of\ndisciplines, but which it is interesting to challenge, hence the purpose of\nthis contribution.", "AI": {"tldr": "The paper addresses the under-researched social dimension of sustainability, proposing a digital tool-based engagement process for lifelong learning, rooted in a multidisciplinary approach.", "motivation": "The social aspect of sustainability has been overlooked, and the paper aims to explore it using digital tools in training contexts.", "method": "An engagement process using digital tools, inspired by everyday life, is proposed, grounded in information and communication sciences.", "result": "The work suggests a multidisciplinary approach applicable across disciplines, though it invites further scrutiny.", "conclusion": "The contribution highlights the potential of digital tools in addressing social sustainability, calling for broader interdisciplinary engagement."}}
{"id": "2504.16951", "pdf": "https://arxiv.org/pdf/2504.16951", "abs": "https://arxiv.org/abs/2504.16951", "authors": ["Nikolay Falaleev", "Nikolai Orlov"], "title": "Self-Controlled Diffusion for Denoising in Scientific Imaging", "categories": ["eess.IV"], "comment": null, "summary": "This paper presents a novel approach for denoising Electron Backscatter\nDiffraction (EBSD) patterns using diffusion models. We propose a two-stage\ntraining process with a UNet-based architecture, incorporating an auxiliary\nregression head to predict the quality of the experimental pattern and assess\nthe progress of the denoising process. The model uses an adaptive denoising\nstrategy, which integrates quality prediction and feedback-driven iterative\ndenoising process control. This adaptive feedback loop allows the model to\nadjust its schedule, providing fine control over the denoising process.\nFurthermore, our model can identify samples where no meaningful signal is\npresent, thereby reducing the risk of hallucinations. We demonstrate the\nsuccessful application of diffusion models to EBSD pattern denoising using a\ncustom-collected dataset of EBSD patterns, their corresponding master patterns,\nand quality values.", "AI": {"tldr": "A novel diffusion model approach for denoising EBSD patterns using a two-stage UNet-based training process with adaptive feedback and quality prediction.", "motivation": "To improve denoising of EBSD patterns by integrating quality assessment and adaptive control to avoid hallucinations.", "method": "Two-stage training with a UNet architecture, auxiliary regression head for quality prediction, and adaptive feedback-driven denoising.", "result": "Successful denoising of EBSD patterns with reduced risk of hallucinations, demonstrated on a custom dataset.", "conclusion": "Diffusion models with adaptive feedback are effective for EBSD pattern denoising, offering fine control and quality assessment."}}
{"id": "2504.17039", "pdf": "https://arxiv.org/pdf/2504.17039", "abs": "https://arxiv.org/abs/2504.17039", "authors": ["Ruben Gonzalez Avil\u00e9s", "Linus Scheibenreif", "Damian Borth"], "title": "Dense Air Pollution Estimation from Sparse in-situ Measurements and Satellite Data", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the critical environmental challenge of estimating\nambient Nitrogen Dioxide (NO$_2$) concentrations, a key issue in public health\nand environmental policy. Existing methods for satellite-based air pollution\nestimation model the relationship between satellite and in-situ measurements at\nselect point locations. While these approaches have advanced our ability to\nprovide air quality estimations on a global scale, they come with inherent\nlimitations. The most notable limitation is the computational intensity\nrequired for generating comprehensive estimates over extensive areas. Motivated\nby these limitations, this study introduces a novel dense estimation technique.\nOur approach seeks to balance the accuracy of high-resolution estimates with\nthe practicality of computational constraints, thereby enabling efficient and\nscalable global environmental assessment. By utilizing a uniformly random\noffset sampling strategy, our method disperses the ground truth data pixel\nlocation evenly across a larger patch. At inference, the dense estimation\nmethod can then generate a grid of estimates in a single step, significantly\nreducing the computational resources required to provide estimates for larger\nareas. Notably, our approach also surpasses the results of existing point-wise\nmethods by a significant margin of $9.45\\%$, achieving a Mean Absolute Error\n(MAE) of $4.98\\ \\mu\\text{g}/\\text{m}^3$. This demonstrates both high accuracy\nand computational efficiency, highlighting the applicability of our method for\nglobal environmental assessment. Furthermore, we showcase the method's\nadaptability and robustness by applying it to diverse geographic regions. Our\nmethod offers a viable solution to the computational challenges of large-scale\nenvironmental monitoring.", "AI": {"tldr": "A novel dense estimation technique for ambient NO$_2$ concentration estimation is introduced, improving accuracy and computational efficiency over existing methods.", "motivation": "Addressing the computational intensity and limitations of current satellite-based air pollution estimation methods.", "method": "Uses a uniformly random offset sampling strategy to disperse ground truth data, enabling efficient grid-based estimates.", "result": "Achieves a 9.45% improvement in accuracy (MAE of 4.98 \u00b5g/m\u00b3) and reduces computational resources.", "conclusion": "The method provides a scalable and accurate solution for global environmental monitoring."}}
{"id": "2504.16937", "pdf": "https://arxiv.org/pdf/2504.16937", "abs": "https://arxiv.org/abs/2504.16937", "authors": ["Ariel S. Kapusta", "David Jin", "Peter M. Teague", "Robert A. Houston", "Jonathan B. Elliott", "Grace Y. Park", "Shelby S. Holdren"], "title": "A Framework for the Assurance of AI-Enabled Systems", "categories": ["cs.AI"], "comment": "12 pages, 2 figures, published in conference proceedings of SPIE\n  Defense and Commercial Sensing conference on Assurance and Security for\n  AI-enabled Systems 2025", "summary": "The United States Department of Defense (DOD) looks to accelerate the\ndevelopment and deployment of AI capabilities across a wide spectrum of defense\napplications to maintain strategic advantages. However, many common features of\nAI algorithms that make them powerful, such as capacity for learning,\nlarge-scale data ingestion, and problem-solving, raise new technical, security,\nand ethical challenges. These challenges may hinder adoption due to uncertainty\nin development, testing, assurance, processes, and requirements.\nTrustworthiness through assurance is essential to achieve the expected value\nfrom AI.\n  This paper proposes a claims-based framework for risk management and\nassurance of AI systems that addresses the competing needs for faster\ndeployment, successful adoption, and rigorous evaluation. This framework\nsupports programs across all acquisition pathways provide grounds for\nsufficient confidence that an AI-enabled system (AIES) meets its intended\nmission goals without introducing unacceptable risks throughout its lifecycle.\nThe paper's contributions are a framework process for AI assurance, a set of\nrelevant definitions to enable constructive conversations on the topic of AI\nassurance, and a discussion of important considerations in AI assurance. The\nframework aims to provide the DOD a robust yet efficient mechanism for swiftly\nfielding effective AI capabilities without overlooking critical risks or\nundermining stakeholder trust.", "AI": {"tldr": "The paper proposes a claims-based framework for AI assurance in defense applications to balance rapid deployment with rigorous risk management.", "motivation": "The DOD seeks to accelerate AI adoption but faces challenges like technical, security, and ethical risks, requiring trustworthy assurance.", "method": "A claims-based framework for risk management and assurance of AI systems, supporting all acquisition pathways.", "result": "Provides a process for AI assurance, definitions for constructive discussions, and key considerations for trustworthy AI deployment.", "conclusion": "The framework offers the DOD a robust yet efficient way to deploy AI capabilities while managing risks and maintaining trust."}}
{"id": "2504.16961", "pdf": "https://arxiv.org/pdf/2504.16961", "abs": "https://arxiv.org/abs/2504.16961", "authors": ["Binon Teji", "Swarup Roy"], "title": "A Novel Graph Transformer Framework for Gene Regulatory Network Inference", "categories": ["cs.LG", "cs.ET", "q-bio.GN", "q-bio.MN", "q-bio.QM"], "comment": null, "summary": "The inference of gene regulatory networks (GRNs) is a foundational stride\ntowards deciphering the fundamentals of complex biological systems. Inferring a\npossible regulatory link between two genes can be formulated as a link\nprediction problem. Inference of GRNs via gene coexpression profiling data may\nnot always reflect true biological interactions, as its susceptibility to noise\nand misrepresenting true biological regulatory relationships. Most GRN\ninference methods face several challenges in the network reconstruction phase.\nTherefore, it is important to encode gene expression values, leverege the prior\nknowledge gained from the available inferred network structures and positional\ninformations of the input network nodes towards inferring a better and more\nconfident GRN network reconstruction. In this paper, we explore the integration\nof multiple inferred networks to enhance the inference of Gene Regulatory\nNetworks (GRNs). Primarily, we employ autoencoder embeddings to capture gene\nexpression patterns directly from raw data, preserving intricate biological\nsignals. Then, we embed the prior knowledge from GRN structures transforming\nthem into a text-like representation using random walks, which are then encoded\nwith a masked language model, BERT, to generate global embeddings for each gene\nacross all networks. Additionally, we embed the positional encodings of the\ninput gene networks to better identify the position of each unique gene within\nthe graph. These embeddings are integrated into graph transformer-based model,\ntermed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the\ntopological structure of the ground truth network while incorporating the\nenriched encoded information. Experimental results demonstrate that GT-GRN\nsignificantly outperforms existing GRN inference methods, achieving superior\naccuracy and highlighting the robustness of our approach.", "AI": {"tldr": "The paper introduces GT-GRN, a graph transformer-based model for Gene Regulatory Network (GRN) inference, integrating autoencoder embeddings, prior knowledge, and positional encodings to outperform existing methods.", "motivation": "Improving GRN inference by addressing noise and misrepresentation in gene coexpression data, leveraging prior knowledge and network structures for more accurate reconstructions.", "method": "Uses autoencoder embeddings for gene expression patterns, BERT for encoding prior knowledge from GRN structures, and positional encodings. Integrates these into GT-GRN, a graph transformer model.", "result": "GT-GRN significantly outperforms existing GRN inference methods, demonstrating superior accuracy and robustness.", "conclusion": "The integration of multiple inferred networks and advanced embeddings enhances GRN inference, with GT-GRN proving highly effective."}}
{"id": "2504.17075", "pdf": "https://arxiv.org/pdf/2504.17075", "abs": "https://arxiv.org/abs/2504.17075", "authors": ["Arjun Subramonian", "Vagrant Gautam", "Preethi Seshadri", "Dietrich Klakow", "Kai-Wei Chang", "Yizhou Sun"], "title": "Agree to Disagree? A Meta-Evaluation of LLM Misgendering", "categories": ["cs.CL", "cs.CY"], "comment": "Work in progress", "summary": "Numerous methods have been proposed to measure LLM misgendering, including\nprobability-based evaluations (e.g., automatically with templatic sentences)\nand generation-based evaluations (e.g., with automatic heuristics or human\nvalidation). However, it has gone unexamined whether these evaluation methods\nhave convergent validity, that is, whether their results align. Therefore, we\nconduct a systematic meta-evaluation of these methods across three existing\ndatasets for LLM misgendering. We propose a method to transform each dataset to\nenable parallel probability- and generation-based evaluation. Then, by\nautomatically evaluating a suite of 6 models from 3 families, we find that\nthese methods can disagree with each other at the instance, dataset, and model\nlevels, conflicting on 20.2% of evaluation instances. Finally, with a human\nevaluation of 2400 LLM generations, we show that misgendering behaviour is\ncomplex and goes far beyond pronouns, which automatic evaluations are not\ncurrently designed to capture, suggesting essential disagreement with human\nevaluations. Based on our findings, we provide recommendations for future\nevaluations of LLM misgendering. Our results are also more widely relevant, as\nthey call into question broader methodological conventions in LLM evaluation,\nwhich often assume that different evaluation methods agree.", "AI": {"tldr": "The paper evaluates the convergent validity of methods measuring LLM misgendering, finding significant disagreement between probability- and generation-based evaluations, and highlights limitations of automatic methods compared to human judgment.", "motivation": "To determine if existing evaluation methods for LLM misgendering align (convergent validity) and to assess their reliability.", "method": "A meta-evaluation of probability- and generation-based methods across three datasets, transforming datasets for parallel evaluation, and analyzing 6 models from 3 families. Human evaluation of 2400 LLM generations was also conducted.", "result": "Methods disagreed on 20.2% of instances, and automatic evaluations failed to capture the complexity of misgendering behavior, diverging from human judgments.", "conclusion": "The study reveals flaws in current evaluation conventions for LLM misgendering and recommends improvements, questioning broader LLM evaluation practices."}}
{"id": "2504.17724", "pdf": "https://arxiv.org/pdf/2504.17724", "abs": "https://arxiv.org/abs/2504.17724", "authors": ["Nicolas Heintz", "Tom Francart", "Alexander Bertrand"], "title": "Unsupervised EEG-based decoding of absolute auditory attention with canonical correlation analysis", "categories": ["eess.SP", "cs.SD"], "comment": null, "summary": "We propose a fully unsupervised algorithm that detects from encephalography\n(EEG) recordings when a subject actively listens to sound, versus when the\nsound is ignored. This problem is known as absolute auditory attention decoding\n(aAAD). We propose an unsupervised discriminative CCA model for feature\nextraction and combine it with an unsupervised classifier called minimally\ninformed linear discriminant analysis (MILDA) for aAAD classification.\nRemarkably, the proposed unsupervised algorithm performs significantly better\nthan a state-of-the-art supervised model. A key reason is that the unsupervised\nalgorithm can successfully adapt to the non-stationary test data at a low\ncomputational cost. This opens the door to the analysis of the auditory\nattention of a subject using EEG signals with a model that automatically tunes\nitself to the subject without requiring an arduous supervised training session\nbeforehand.", "AI": {"tldr": "An unsupervised algorithm for detecting active listening vs. ignored sound in EEG recordings outperforms supervised models by adapting to non-stationary data efficiently.", "motivation": "To address the challenge of absolute auditory attention decoding (aAAD) without requiring supervised training, enabling easier and more adaptable EEG-based attention analysis.", "method": "Combines unsupervised discriminative CCA for feature extraction with MILDA for classification, focusing on non-stationary data adaptation.", "result": "The unsupervised algorithm significantly outperforms state-of-the-art supervised models, adapting effectively to test data with low computational cost.", "conclusion": "This approach enables auditory attention analysis without supervised training, offering a practical and efficient solution for EEG-based applications."}}
{"id": "2504.17575", "pdf": "https://arxiv.org/pdf/2504.17575", "abs": "https://arxiv.org/abs/2504.17575", "authors": ["Kristoffer Christensen", "Bo N\u00f8rregaard J\u00f8rgensen", "Zheng Grace Ma"], "title": "A Multi-Agent, Laxity-Based Aggregation Strategy for Cost-Effective Electric Vehicle Charging and Local Transformer Overload Prevention", "categories": ["cs.MA"], "comment": null, "summary": "The rapid electrification of transportation, driven by stringent\ndecarbonization targets and supportive policies, poses significant challenges\nfor distribution system operators (DSOs). When numerous electric vehicles (EVs)\ncharge concurrently, local transformers risk overloading - a problem that\ncurrent tariff-based strategies do not adequately address. This paper\nintroduces an aggregator-based coordination mechanism that shifts EV charging\nfrom congested to underutilized periods using a rule-based scheduling\nalgorithm. Unlike conventional methods that depend on complex real-time pricing\nsignals or optimization-heavy solutions, the aggregator approach uses a simple\nyet effective \"laxity\" measure to prioritize charging flexibility. To assess\ntechnical and economic viability, a multi-agent simulation was developed to\nreplicate residential user behavior and DSO constraints under the use of a 400\nkVA low-voltage transformer. The results indicate that overloads are completely\neliminated with minimal inconvenience to users, whose increased charging costs\nare offset by the aggregator at an annual total of under DKK 6000 -\nsignificantly lower than the cost of infrastructure reinforcement. This study\ncontributes by (i) quantifying the compensation needed to prevent large-scale\noverloads, (ii) presenting a replicable, computationally feasible, rule-based\naggregator model for DSOs, and (iii) comparing aggregator solutions to costly\ntransformer upgrades, underscoring the aggregator's role as a viable tool for\nfuture distribution systems.", "AI": {"tldr": "An aggregator-based coordination mechanism for EV charging prevents transformer overloads using a simple rule-based algorithm, proving cost-effective compared to infrastructure upgrades.", "motivation": "Address the challenge of transformer overloads from concurrent EV charging, which current tariff strategies fail to resolve.", "method": "Introduces a rule-based scheduling algorithm using a \"laxity\" measure to shift EV charging to underutilized periods, tested via multi-agent simulation.", "result": "Overloads are eliminated with minimal user inconvenience, costing under DKK 6000 annually, cheaper than infrastructure upgrades.", "conclusion": "The aggregator model is a viable, cost-effective solution for future distribution systems, preventing overloads without heavy infrastructure costs."}}
{"id": "2504.16974", "pdf": "https://arxiv.org/pdf/2504.16974", "abs": "https://arxiv.org/abs/2504.16974", "authors": ["Hidde Makimei", "Shuai Wang", "Willem van Peursen"], "title": "Seeing The Words: Evaluating AI-generated Biblical Art", "categories": ["cs.CY", "cs.CV", "cs.MM", "I.4.8; I.4.0; I.3.3; I.3.0"], "comment": null, "summary": "The past years witnessed a significant amount of Artificial Intelligence (AI)\ntools that can generate images from texts. This triggers the discussion of\nwhether AI can generate accurate images using text from the Bible with respect\nto the corresponding biblical contexts and backgrounds. Despite some existing\nattempts at a small scale, little work has been done to systematically evaluate\nthese generated images. In this work, we provide a large dataset of over 7K\nimages using biblical text as prompts. These images were evaluated with\nmultiple neural network-based tools on various aspects. We provide an\nassessment of accuracy and some analysis from the perspective of religion and\naesthetics. Finally, we discuss the use of the generated images and reflect on\nthe performance of the AI generators.", "AI": {"tldr": "A study evaluates AI-generated images from biblical texts, assessing accuracy, religious context, and aesthetics using a dataset of 7K images.", "motivation": "To systematically assess AI's ability to generate accurate biblical images, addressing gaps in existing small-scale evaluations.", "method": "Created a large dataset of 7K images from biblical text prompts, evaluated using neural network tools for accuracy, religious context, and aesthetics.", "result": "Provided insights into AI's performance in generating contextually and aesthetically accurate biblical images.", "conclusion": "Discusses the utility of AI-generated biblical images and reflects on AI generator performance, highlighting areas for improvement."}}
{"id": "2504.16953", "pdf": "https://arxiv.org/pdf/2504.16953", "abs": "https://arxiv.org/abs/2504.16953", "authors": ["Lebin Zhou", "Cihan Ruan", "Nam Ling", "Wei Wang", "Wei Jiang"], "title": "TVC: Tokenized Video Compression with Ultra-Low Bitrate", "categories": ["eess.IV"], "comment": null, "summary": "Tokenized visual representations have shown great promise in image\ncompression, yet their extension to video remains underexplored due to the\nchallenges posed by complex temporal dynamics and stringent bitrate\nconstraints. In this paper, we propose Tokenized Video Compression (TVC), the\nfirst token-based dual-stream video compression framework designed to operate\neffectively at ultra-low bitrates. TVC leverages the powerful Cosmos video\ntokenizer to extract both discrete and continuous token streams. The discrete\ntokens (i.e., code maps generated by FSQ) are partially masked using a\nstrategic masking scheme, then compressed losslessly with a discrete\ncheckerboard context model to reduce transmission overhead. The masked tokens\nare reconstructed by a decoder-only transformer with spatiotemporal token\nprediction. Meanwhile, the continuous tokens, produced via an autoencoder (AE),\nare quantized and compressed using a continuous checkerboard context model,\nproviding complementary continuous information at ultra-low bitrate. At the\nDecoder side, both streams are fused using ControlNet, with multi-scale\nhierarchical integration to ensure high perceptual quality alongside strong\nfidelity in reconstruction. This work mitigates the long-standing skepticism\nabout the practicality of tokenized video compression and opens up new avenues\nfor semantics-aware, token-native video compression.", "AI": {"tldr": "Tokenized Video Compression (TVC) introduces a dual-stream framework for ultra-low bitrate video compression using discrete and continuous tokens, leveraging advanced tokenization and fusion techniques.", "motivation": "Extending tokenized visual representations to video is challenging due to temporal dynamics and bitrate constraints, prompting the need for an efficient solution.", "method": "TVC uses Cosmos video tokenizer for discrete and continuous token streams, strategic masking, lossless compression, and ControlNet fusion for reconstruction.", "result": "TVC effectively compresses video at ultra-low bitrates while maintaining high perceptual quality and fidelity.", "conclusion": "TVC addresses skepticism about tokenized video compression, enabling semantics-aware, token-native approaches."}}
{"id": "2504.17040", "pdf": "https://arxiv.org/pdf/2504.17040", "abs": "https://arxiv.org/abs/2504.17040", "authors": ["Zhenhailong Wang", "Senthil Purushwalkam", "Caiming Xiong", "Silvio Savarese", "Heng Ji", "Ran Xu"], "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.", "AI": {"tldr": "DyMU is a training-free framework that dynamically reduces computational costs in vision-language models (VLMs) by merging similar visual tokens and reconstructing attention dynamics, achieving high performance with reduced tokens.", "motivation": "To address inefficiencies in fixed-length outputs of vision transformers and reduce computational burden in VLMs without additional training.", "method": "Combines Dynamic Token Merging (DToMe) to merge similar visual tokens based on image complexity and Virtual Token Unmerging (VTU) to simulate full-sequence attention dynamics.", "result": "Reduces visual token count by 32%-85% while maintaining comparable performance to full-length models across diverse VLM architectures.", "conclusion": "DyMU offers a scalable, training-free solution for efficient VLM operation, adaptable to image complexity and user control over computational costs."}}
{"id": "2504.16938", "pdf": "https://arxiv.org/pdf/2504.16938", "abs": "https://arxiv.org/abs/2504.16938", "authors": ["Lucas Carr", "Nicholas Leisegang", "Thomas Meyer", "Sergei Obiedkov"], "title": "Rational Inference in Formal Concept Analysis", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Defeasible conditionals are a form of non-monotonic inference which enable\nthe expression of statements like \"if $\\phi$ then normally $\\psi$\". The KLM\nframework defines a semantics for the propositional case of defeasible\nconditionals by construction of a preference ordering over possible worlds. The\npattern of reasoning induced by these semantics is characterised by consequence\nrelations satisfying certain desirable properties of non-monotonic reasoning.\nIn FCA, implications are used to describe dependencies between attributes.\nHowever, these implications are unsuitable to reason with erroneous data or\ndata prone to exceptions. Until recently, the topic of non-monotonic inference\nin FCA has remained largely uninvestigated. In this paper, we provide a\nconstruction of the KLM framework for defeasible reasoning in FCA and show that\nthis construction remains faithful to the principle of non-monotonic inference\ndescribed in the original framework. We present an additional argument that,\nwhile remaining consistent with the original ideas around non-monotonic\nreasoning, the defeasible reasoning we propose in FCA offers a more contextual\nview on inference, providing the ability for more relevant conclusions to be\ndrawn when compared to the propositional case.", "AI": {"tldr": "The paper extends the KLM framework for defeasible reasoning to Formal Concept Analysis (FCA), offering a contextual and more relevant approach compared to the propositional case.", "motivation": "Traditional implications in FCA are inadequate for handling erroneous or exceptional data, and non-monotonic inference in FCA has been understudied.", "method": "The authors construct the KLM framework within FCA, ensuring it aligns with non-monotonic reasoning principles.", "result": "The proposed defeasible reasoning in FCA provides a more contextual and relevant inference method than the propositional case.", "conclusion": "The paper successfully adapts the KLM framework to FCA, enhancing its applicability and relevance in practical scenarios."}}
{"id": "2504.16968", "pdf": "https://arxiv.org/pdf/2504.16968", "abs": "https://arxiv.org/abs/2504.16968", "authors": ["Jun Wu", "Jiangtao Wen", "Yuxing Han"], "title": "Backslash: Rate Constrained Optimized Training of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (Backslash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). Backslash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that Backslash can\nreduce memory usage by 60\\% - 90\\% without accuracy loss and provides\nsignificant compression gain compared to compression after training. Moreover,\nBackslash proves to be highly versatile: it enhances generalization with small\nLagrange multipliers, improves model robustness to pruning (maintaining\naccuracy even at 80\\% pruning rates), and enables network simplification for\naccelerated inference on edge devices.", "AI": {"tldr": "Backslash introduces rate-constrained training for LLMs, reducing memory usage by 60-90% without accuracy loss, outperforming post-training compression.", "motivation": "Address the unexplored area of compression during LLM training, aiming to reduce parameter redundancy while maintaining performance.", "method": "Rate-Constrained Training (Backslash) based on rate-distortion optimization (RDO) for flexible accuracy-complexity trade-offs.", "result": "Achieves 60-90% memory reduction, enhances generalization, improves robustness to pruning, and simplifies networks for edge devices.", "conclusion": "Backslash is a versatile, effective training-time compression method for LLMs, offering significant benefits over post-training approaches."}}
{"id": "2504.17083", "pdf": "https://arxiv.org/pdf/2504.17083", "abs": "https://arxiv.org/abs/2504.17083", "authors": ["Rendi Chevi", "Kentaro Inui", "Thamar Solorio", "Alham Fikri Aji"], "title": "How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study", "categories": ["cs.CL"], "comment": "Accepted at GenAICHI 2025 @ ACM CHI 2025", "summary": "What makes an interaction with the LLM more preferable for the user? While it\nis intuitive to assume that information accuracy in the LLM's responses would\nbe one of the influential variables, recent studies have found that inaccurate\nLLM's responses could still be preferable when they are perceived to be more\nauthoritative, certain, well-articulated, or simply verbose. These variables\ninterestingly fall under the broader category of language style, implying that\nthe style in the LLM's responses might meaningfully influence users'\npreferences. This hypothesized dynamic could have double-edged consequences:\nenhancing the overall user experience while simultaneously increasing their\nsusceptibility to risks such as LLM's misinformation or hallucinations. In this\nshort paper, we present our preliminary studies in exploring this subject.\nThrough a series of exploratory and experimental user studies, we found that\nLLM's language style does indeed influence user's preferences, but how and\nwhich language styles influence the preference varied across different user\npopulations, and more interestingly, moderated by the user's very own\nindividual traits. As a preliminary work, the findings in our studies should be\ninterpreted with caution, particularly given the limitations in our samples,\nwhich still need wider demographic diversity and larger sample sizes. Our\nfuture directions will first aim to address these limitations, which would\nenable a more comprehensive joint effect analysis between the language style,\nindividual traits, and preferences, and further investigate the potential\ncausal relationship between and beyond these variables.", "AI": {"tldr": "The paper explores how language style in LLM responses influences user preferences, finding it varies by user traits, but warns of risks like misinformation due to style's impact.", "motivation": "To understand how language style in LLM responses affects user preferences, despite potential risks like misinformation.", "method": "Conducted exploratory and experimental user studies to analyze the influence of language style on preferences.", "result": "Language style impacts preferences, but effects vary by user traits; findings are preliminary due to sample limitations.", "conclusion": "Future work will address sample diversity and size to better analyze language style, traits, and preferences, and explore causal relationships."}}
{"id": "2504.17669", "pdf": "https://arxiv.org/pdf/2504.17669", "abs": "https://arxiv.org/abs/2504.17669", "authors": ["Subash Neupane", "Shaswata Mitra", "Sudip Mittal", "Shahram Rahimi"], "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare", "categories": ["cs.MA", "cs.AI", "cs.ET"], "comment": null, "summary": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.", "AI": {"tldr": "A HIPAA-compliant Agentic AI framework for clinical workflows, integrating ABAC, PHI sanitization, and audit trails to ensure regulatory compliance.", "motivation": "To address the need for regulatory compliance (e.g., HIPAA) in AI-driven clinical workflows handling sensitive healthcare data.", "method": "Combines ABAC for granular access control, a hybrid PHI sanitization pipeline (regex + BERT), and immutable audit trails.", "result": "A framework ensuring HIPAA compliance while autonomously processing PHI in clinical workflows.", "conclusion": "The proposed framework enables secure, compliant adoption of Agentic AI in healthcare, balancing autonomy with regulatory adherence."}}
{"id": "2504.17253", "pdf": "https://arxiv.org/pdf/2504.17253", "abs": "https://arxiv.org/abs/2504.17253", "authors": ["Yinqi Li", "Hong Chang", "Ruibing Hou", "Shiguang Shan", "Xilin Chen"], "title": "DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "Diffusion models have shown remarkable progress in various generative tasks\nsuch as image and video generation. This paper studies the problem of\nleveraging pretrained diffusion models for performing discriminative tasks.\nSpecifically, we extend the discriminative capability of pretrained frozen\ngenerative diffusion models from the classification task to the more complex\nobject detection task, by \"inverting\" a pretrained layout-to-image diffusion\nmodel. To this end, a gradient-based discrete optimization approach for\nreplacing the heavy prediction enumeration process, and a prior distribution\nmodel for making more accurate use of the Bayes' rule, are proposed\nrespectively. Empirical results show that this method is on par with basic\ndiscriminative object detection baselines on COCO dataset. In addition, our\nmethod can greatly speed up the previous diffusion-based method for\nclassification without sacrificing accuracy. Code and models are available at\nhttps://github.com/LiYinqi/DIVE .", "AI": {"tldr": "The paper explores using pretrained diffusion models for discriminative tasks, specifically object detection, by inverting a layout-to-image diffusion model. It introduces gradient-based optimization and a prior distribution model, achieving results comparable to baselines on COCO and faster classification without accuracy loss.", "motivation": "To extend the discriminative capabilities of pretrained diffusion models beyond classification to more complex tasks like object detection, leveraging their generative strengths.", "method": "Inverts a pretrained layout-to-image diffusion model using gradient-based discrete optimization and a prior distribution model to apply Bayes' rule accurately.", "result": "Achieves performance on par with basic discriminative object detection baselines on COCO and speeds up diffusion-based classification without losing accuracy.", "conclusion": "Demonstrates the feasibility of repurposing generative diffusion models for discriminative tasks effectively, with potential for broader applications."}}
{"id": "2504.16958", "pdf": "https://arxiv.org/pdf/2504.16958", "abs": "https://arxiv.org/abs/2504.16958", "authors": ["Xiaoyan Kui", "Zexin Ji", "Beiji Zou", "Yang Li", "Yulan Dai", "Liming Chen", "Pierre Vera", "Su Ruan"], "title": "Iterative Collaboration Network Guided By Reconstruction Prior for Medical Image Super-Resolution", "categories": ["eess.IV"], "comment": null, "summary": "High-resolution medical images can provide more detailed information for\nbetter diagnosis. Conventional medical image super-resolution relies on a\nsingle task which first performs the extraction of the features and then\nupscaling based on the features. The features extracted may not be complete for\nsuper-resolution. Recent multi-task learning,including reconstruction and\nsuper-resolution, is a good solution to obtain additional relevant information.\nThe interaction between the two tasks is often insufficient, which still leads\nto incomplete and less relevant deep features. To address above limitations, we\npropose an iterative collaboration network (ICONet) to improve communications\nbetween tasks by progressively incorporating reconstruction prior to the\nsuper-resolution learning procedure in an iterative collaboration way. It\nconsists of a reconstruction branch, a super-resolution branch, and a SR-Rec\nfusion module. The reconstruction branch generates the artifact-free image as\nprior, which is followed by a super-resolution branch for prior\nknowledge-guided super-resolution. Unlike the widely-used convolutional neural\nnetworks for extracting local features and Transformers with quadratic\ncomputational complexity for modeling long-range dependencies, we develop a new\nresidual spatial-channel feature learning (RSCFL) module of two branches to\nefficiently establish feature relationships in spatial and channel dimensions.\nMoreover, the designed SR-Rec fusion module fuses the reconstruction prior and\nsuper-resolution features with each other in an adaptive manner. Our ICONet is\nbuilt with multi-stage models to iteratively upscale the low-resolution images\nusing steps of 2x and simultaneously interact between two branches in\nmulti-stage supervisions.", "AI": {"tldr": "Proposes ICONet, an iterative collaboration network for medical image super-resolution, enhancing task interaction and feature completeness.", "motivation": "Conventional single-task super-resolution lacks feature completeness, and multi-task learning often has insufficient interaction between tasks.", "method": "ICONet integrates reconstruction and super-resolution branches with an SR-Rec fusion module, using RSCFL for efficient feature learning.", "result": "Improves communication between tasks, enabling artifact-free reconstruction and prior-guided super-resolution.", "conclusion": "ICONet effectively enhances medical image super-resolution through iterative collaboration and adaptive fusion."}}
{"id": "2504.17067", "pdf": "https://arxiv.org/pdf/2504.17067", "abs": "https://arxiv.org/abs/2504.17067", "authors": ["Xinqi Xiong", "Andrea Dunn Beltran", "Jun Myeong Choi", "Marc Niethammer", "Roni Sengupta"], "title": "PPS-Ctrl: Controllable Sim-to-Real Translation for Colonoscopy Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate depth estimation enhances endoscopy navigation and diagnostics, but\nobtaining ground-truth depth in clinical settings is challenging. Synthetic\ndatasets are often used for training, yet the domain gap limits generalization\nto real data. We propose a novel image-to-image translation framework that\npreserves structure while generating realistic textures from clinical data. Our\nkey innovation integrates Stable Diffusion with ControlNet, conditioned on a\nlatent representation extracted from a Per-Pixel Shading (PPS) map. PPS\ncaptures surface lighting effects, providing a stronger structural constraint\nthan depth maps. Experiments show our approach produces more realistic\ntranslations and improves depth estimation over GAN-based MI-CycleGAN. Our code\nis publicly accessible at https://github.com/anaxqx/PPS-Ctrl.", "AI": {"tldr": "A novel image-to-image translation framework using Stable Diffusion and ControlNet, conditioned on Per-Pixel Shading (PPS), improves depth estimation in endoscopy by generating realistic textures while preserving structure.", "motivation": "Accurate depth estimation is crucial for endoscopy but challenging due to the difficulty of obtaining ground-truth depth in clinical settings and the domain gap between synthetic and real data.", "method": "The framework integrates Stable Diffusion with ControlNet, using a latent representation from PPS maps to preserve structure and generate realistic textures.", "result": "The approach outperforms GAN-based MI-CycleGAN, producing more realistic translations and better depth estimation.", "conclusion": "The proposed method effectively bridges the domain gap, enhancing depth estimation for endoscopy navigation and diagnostics."}}
{"id": "2504.16939", "pdf": "https://arxiv.org/pdf/2504.16939", "abs": "https://arxiv.org/abs/2504.16939", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Hongru Wang", "Vardhan Dongre", "Xiusi Chen", "Heng Ji", "Dilek Hakkani-T\u00fcr", "Gokhan Tur"], "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational\nAI from traditional dialogue systems into sophisticated agents capable of\nautonomous actions, contextual awareness, and multi-turn interactions with\nusers. Yet, fundamental questions about their capabilities, limitations, and\npaths forward remain open. This survey paper presents a desideratum for\nnext-generation Conversational Agents - what has been achieved, what challenges\npersist, and what must be done for more scalable systems that approach\nhuman-level intelligence. To that end, we systematically analyze LLM-driven\nConversational Agents by organizing their capabilities into three primary\ndimensions: (i) Reasoning - logical, systematic thinking inspired by human\nintelligence for decision making, (ii) Monitor - encompassing self-awareness\nand user interaction monitoring, and (iii) Control - focusing on tool\nutilization and policy following. Building upon this, we introduce a novel\ntaxonomy by classifying recent work on Conversational Agents around our\nproposed desideratum. We identify critical research gaps and outline key\ndirections, including realistic evaluations, long-term multi-turn reasoning\nskills, self-evolution capabilities, collaborative and multi-agent task\ncompletion, personalization, and proactivity. This work aims to provide a\nstructured foundation, highlight existing limitations, and offer insights into\npotential future research directions for Conversational Agents, ultimately\nadvancing progress toward Artificial General Intelligence (AGI). We maintain a\ncurated repository of papers at:\nhttps://github.com/emrecanacikgoz/awesome-conversational-agents.", "AI": {"tldr": "A survey on LLM-driven Conversational Agents, analyzing their capabilities (Reasoning, Monitor, Control), identifying gaps, and suggesting future research directions for AGI.", "motivation": "To evaluate the progress and limitations of Conversational Agents powered by LLMs and propose a structured path toward human-level intelligence.", "method": "Systematic analysis of Conversational Agents' capabilities, introduction of a novel taxonomy, and identification of research gaps.", "result": "A structured foundation highlighting current limitations and future directions, including realistic evaluations and self-evolution capabilities.", "conclusion": "The paper provides insights for advancing Conversational Agents toward AGI, with a curated repository for further research."}}
{"id": "2504.16970", "pdf": "https://arxiv.org/pdf/2504.16970", "abs": "https://arxiv.org/abs/2504.16970", "authors": ["Yin Wang", "Chunlin Gong", "Xiang Wu", "Hanleran Zhang"], "title": "STFM: A Spatio-Temporal Information Fusion Model Based on Phase Space Reconstruction for Sea Surface Temperature Prediction", "categories": ["cs.LG"], "comment": "19 pages, 14 figures", "summary": "The sea surface temperature (SST), a key environmental parameter, is crucial\nto optimizing production planning, making its accurate prediction a vital\nresearch topic. However, the inherent nonlinearity of the marine dynamic system\npresents significant challenges. Current forecasting methods mainly include\nphysics-based numerical simulations and data-driven machine learning\napproaches. The former, while describing SST evolution through differential\nequations, suffers from high computational complexity and limited\napplicability, whereas the latter, despite its computational benefits, requires\nlarge datasets and faces interpretability challenges. This study presents a\nprediction framework based solely on data-driven techniques. Using phase space\nreconstruction, we construct initial-delay attractor pairs with a mathematical\nhomeomorphism and design a Spatio-Temporal Fusion Mapping (STFM) to uncover\ntheir intrinsic connections. Unlike conventional models, our method captures\nSST dynamics efficiently through phase space reconstruction and achieves high\nprediction accuracy with minimal training data in comparative tests", "AI": {"tldr": "A data-driven SST prediction framework using phase space reconstruction and STFM for accurate forecasting with minimal training data.", "motivation": "Accurate SST prediction is vital for optimizing production planning, but current methods (physics-based or machine learning) face computational or data challenges.", "method": "Phase space reconstruction creates initial-delay attractor pairs with homeomorphism, and STFM uncovers their intrinsic connections for efficient SST dynamics capture.", "result": "The method achieves high prediction accuracy with minimal training data compared to conventional models.", "conclusion": "The proposed framework offers an efficient, accurate, and data-light solution for SST forecasting."}}
{"id": "2504.17091", "pdf": "https://arxiv.org/pdf/2504.17091", "abs": "https://arxiv.org/abs/2504.17091", "authors": ["Seunghyun Yoo"], "title": "Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "68T05"], "comment": "5 page", "summary": "Due to the proliferation of short-form content and the rapid adoption of AI,\nopportunities for deep, reflective thinking have significantly diminished,\nundermining users' critical thinking and reducing engagement with the reasoning\nbehind AI-generated outputs. To address this issue, we propose an Interactive\nChain-of-Thought (CoT) Framework that enhances human-centered explainability\nand responsible AI usage by making the model's inference process transparent,\nmodular, and user-editable. The framework decomposes reasoning into clearly\ndefined blocks that users can inspect, modify, and re-execute, encouraging\nactive cognitive engagement rather than passive consumption. It further\nintegrates a lightweight edit-adaptation mechanism inspired by preference\nlearning, allowing the system to align with diverse cognitive styles and user\nintentions. Ethical transparency is ensured through explicit metadata\ndisclosure, built-in bias checkpoint functionality, and privacy-preserving\nsafeguards. This work outlines the design principles and architecture necessary\nto promote critical engagement, responsible interaction, and inclusive\nadaptation in AI systems aimed at addressing complex societal challenges.", "AI": {"tldr": "Proposes an Interactive Chain-of-Thought (CoT) Framework to enhance AI transparency and user engagement by making reasoning modular and editable.", "motivation": "Addresses the decline in deep thinking due to short-form content and AI reliance, aiming to improve critical engagement with AI outputs.", "method": "Introduces a framework with modular reasoning blocks, user-editable steps, and a lightweight edit-adaptation mechanism for alignment with user preferences.", "result": "Enables transparent, responsible AI usage with features like bias checkpoints and privacy safeguards.", "conclusion": "The framework fosters critical thinking, ethical transparency, and inclusive adaptation in AI systems."}}
{"id": "2504.17128", "pdf": "https://arxiv.org/pdf/2504.17128", "abs": "https://arxiv.org/abs/2504.17128", "authors": ["Seyed Yousef Soltanian", "Wenlong Zhang"], "title": "PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "93C41, 49N70, 49N90, 91A27"], "comment": "Accepted to 7th Annual Conference on Learning for Dynamics and\n  Control (L4DC) 2025. Camera-ready version using the official PMLR template.\n  The full version including appendix and proofs", "summary": "In this paper, we address the problem of a two-player linear quadratic\ndifferential game with incomplete information, a scenario commonly encountered\nin multi-agent control, human-robot interaction (HRI), and approximation\nmethods for solving general-sum differential games. While solutions to such\nlinear differential games are typically obtained through coupled Riccati\nequations, the complexity increases when agents have incomplete information,\nparticularly when neither is aware of the other's cost function. To tackle this\nchallenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework\nfor learning the cost parameters of the other agent. In PACE, each agent treats\nits peer as a learning agent rather than a stationary optimal agent, models\ntheir learning dynamics, and leverages this dynamic to infer the cost function\nparameters of the other agent. This approach enables agents to infer each\nother's objective function in real time based solely on their previous state\nobservations and dynamically adapt their control policies. Furthermore, we\nprovide a theoretical guarantee for the convergence of parameter estimation and\nthe stability of system states in PACE. Additionally, in our numerical studies,\nwe demonstrate how modeling the learning dynamics of the other agent benefits\nPACE, compared to approaches that approximate the other agent as having\ncomplete information, particularly in terms of stability and convergence speed.", "AI": {"tldr": "The paper introduces PACE, a framework for learning cost parameters in two-player linear quadratic differential games with incomplete information, improving stability and convergence speed.", "motivation": "Addressing the challenge of incomplete information in multi-agent control and HRI, where agents lack knowledge of each other's cost functions.", "method": "Proposes PACE, where agents model each other's learning dynamics to infer cost parameters in real time, adapting control policies dynamically.", "result": "Theoretical guarantees for parameter estimation convergence and system stability, with numerical studies showing improved performance over complete-information approximations.", "conclusion": "PACE effectively handles incomplete information in differential games, enhancing real-time adaptability and system performance."}}
{"id": "2504.17267", "pdf": "https://arxiv.org/pdf/2504.17267", "abs": "https://arxiv.org/abs/2504.17267", "authors": ["Chuer Chen", "Shengqi Dang", "Yuqi Liu", "Nanxuan Zhao", "Yang Shi", "Nan Cao"], "title": "MV-Crafter: An Intelligent System for Music-guided Video Generation", "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Music videos, as a prevalent form of multimedia entertainment, deliver\nengaging audio-visual experiences to audiences and have gained immense\npopularity among singers and fans. Creators can express their interpretations\nof music naturally through visual elements. However, the creation process of\nmusic video demands proficiency in script design, video shooting, and\nmusic-video synchronization, posing significant challenges for\nnon-professionals. Previous work has designed automated music video generation\nframeworks. However, they suffer from complexity in input and poor output\nquality. In response, we present MV-Crafter, a system capable of producing\nhigh-quality music videos with synchronized music-video rhythm and style. Our\napproach involves three technical modules that simulate the human creation\nprocess: the script generation module, video generation module, and music-video\nsynchronization module. MV-Crafter leverages a large language model to generate\nscripts considering the musical semantics. To address the challenge of\nsynchronizing short video clips with music of varying lengths, we propose a\ndynamic beat matching algorithm and visual envelope-induced warping method to\nensure precise, monotonic music-video synchronization. Besides, we design a\nuser-friendly interface to simplify the creation process with intuitive editing\nfeatures. Extensive experiments have demonstrated that MV-Crafter provides an\neffective solution for improving the quality of generated music videos.", "AI": {"tldr": "MV-Crafter is a system for automated high-quality music video generation, addressing challenges like synchronization and input complexity with three technical modules and a user-friendly interface.", "motivation": "Music video creation is complex for non-professionals, and existing automated solutions have poor quality and input complexity. MV-Crafter aims to simplify and enhance the process.", "method": "Three modules: script generation (using a large language model), video generation, and synchronization (dynamic beat matching and visual envelope warping). Includes a user-friendly interface.", "result": "MV-Crafter improves music video quality with precise synchronization and intuitive editing, validated through extensive experiments.", "conclusion": "MV-Crafter effectively addresses the limitations of prior work, offering a high-quality, user-friendly solution for automated music video creation."}}
{"id": "2504.17053", "pdf": "https://arxiv.org/pdf/2504.17053", "abs": "https://arxiv.org/abs/2504.17053", "authors": ["Odysseas Pappas", "Perla Mayo", "Andrew Austin", "Alin Achim"], "title": "Diffusion Probabilistic Models for Compressive SAR Imaging", "categories": ["eess.IV"], "comment": null, "summary": "Compressed sensing Synthetic Aperture Radar (SAR) image formation, formulated\nas an inverse problem and solved with traditional iterative optimization\nmethods can be very computationally expensive. We investigate the use of\ndenoising diffusion probabilistic models for compressive SAR image\nreconstruction, where the diffusion model is guided by a poor initial\nreconstruction from sub-sampled data obtained via standard imaging methods. We\npresent results on real SAR data and compare our compressively sampled\ndiffusion model reconstruction with standard image reconstruction methods\nutilizing the full data set, demonstrating the potential performance gains in\nimaging quality.", "AI": {"tldr": "Using denoising diffusion models for compressive SAR image reconstruction improves efficiency and quality compared to traditional methods.", "motivation": "Traditional iterative optimization for SAR image formation is computationally expensive.", "method": "Employ denoising diffusion probabilistic models guided by initial poor reconstructions from sub-sampled data.", "result": "Improved imaging quality compared to standard methods using full data.", "conclusion": "Diffusion models offer potential performance gains in SAR image reconstruction."}}
{"id": "2504.17069", "pdf": "https://arxiv.org/pdf/2504.17069", "abs": "https://arxiv.org/abs/2504.17069", "authors": ["Rishav Pramanik", "Antoine Poupon", "Juan A. Rodriguez", "Masih Aminbeidokhti", "David Vazquez", "Christopher Pal", "Zhaozheng Yin", "Marco Pedersoli"], "title": "Distilling semantically aware orders for autoregressive image generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.", "AI": {"tldr": "The paper proposes a new method for autoregressive image generation by training models to generate patches in any order, improving image quality over traditional raster-scan approaches.", "motivation": "Autoregressive models for image generation lack a natural order, and the traditional raster-scan order is suboptimal as it ignores content causality (e.g., clouds depending on the sun's color).", "method": "Train a model to generate patches in any order, infer content and location during generation, and finetune the model using extracted orders for better image quality.", "result": "The new method outperforms raster-scan approaches in image quality on two datasets, with similar training costs and no extra annotations.", "conclusion": "The proposed any-order generation method is more effective for autoregressive image generation, respecting content causality and improving results."}}
{"id": "2504.17006", "pdf": "https://arxiv.org/pdf/2504.17006", "abs": "https://arxiv.org/abs/2504.17006", "authors": ["Jalal Arabneydi", "Saiful Islam", "Srijita Das", "Sai Krishna Gottipati", "William Duguay", "Cloderic Mars", "Matthew E. Taylor", "Matthew Guzdial", "Antoine Fagette", "Younes Zerouali"], "title": "A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "This is a result of the collaboration by JACOBB, AMII(Alberta Machine\n  Intelligence Institute), Thales and AI Redefined (AIR) in 2021-2023", "summary": "With the growing popularity of deep reinforcement learning (DRL),\nhuman-in-the-loop (HITL) approach has the potential to revolutionize the way we\napproach decision-making problems and create new opportunities for human-AI\ncollaboration. In this article, we introduce a novel multi-layered hierarchical\nHITL DRL algorithm that comprises three types of learning: self learning,\nimitation learning and transfer learning. In addition, we consider three forms\nof human inputs: reward, action and demonstration. Furthermore, we discuss main\nchallenges, trade-offs and advantages of HITL in solving complex problems and\nhow human information can be integrated in the AI solution systematically. To\nverify our technical results, we present a real-world unmanned aerial vehicles\n(UAV) problem wherein a number of enemy drones attack a restricted area. The\nobjective is to design a scalable HITL DRL algorithm for ally drones to\nneutralize the enemy drones before they reach the area. To this end, we first\nimplement our solution using an award-winning open-source HITL software called\nCogment. We then demonstrate several interesting results such as (a) HITL leads\nto faster training and higher performance, (b) advice acts as a guiding\ndirection for gradient methods and lowers variance, and (c) the amount of\nadvice should neither be too large nor too small to avoid over-training and\nunder-training. Finally, we illustrate the role of human-AI cooperation in\nsolving two real-world complex scenarios, i.e., overloaded and decoy attacks.", "AI": {"tldr": "A novel multi-layered hierarchical HITL DRL algorithm integrates self, imitation, and transfer learning with human inputs (reward, action, demonstration) to enhance decision-making and human-AI collaboration, validated in a UAV defense scenario.", "motivation": "To leverage human expertise in DRL for faster training, better performance, and systematic integration of human inputs in AI solutions.", "method": "Proposes a hierarchical HITL DRL algorithm combining self, imitation, and transfer learning, tested in a UAV defense problem using Cogment software.", "result": "HITL improves training speed and performance; human advice reduces variance, with optimal advice quantity balancing over/under-training.", "conclusion": "Human-AI collaboration via HITL DRL effectively solves complex scenarios like UAV defense, demonstrating scalability and practical utility."}}
{"id": "2504.16972", "pdf": "https://arxiv.org/pdf/2504.16972", "abs": "https://arxiv.org/abs/2504.16972", "authors": ["Hossein Ahmadi", "Sajjad Emdadi Mahdimahalleh", "Arman Farahat", "Banafsheh Saffari"], "title": "Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "The rapid growth of unlabeled time-series data in domains such as wireless\ncommunications, radar, biomedical engineering, and the Internet of Things (IoT)\nhas driven advancements in unsupervised learning. This review synthesizes\nrecent progress in applying autoencoders and vision transformers for\nunsupervised signal analysis, focusing on their architectures, applications,\nand emerging trends. We explore how these models enable feature extraction,\nanomaly detection, and classification across diverse signal types, including\nelectrocardiograms, radar waveforms, and IoT sensor data. The review highlights\nthe strengths of hybrid architectures and self-supervised learning, while\nidentifying challenges in interpretability, scalability, and domain\ngeneralization. By bridging methodological innovations and practical\napplications, this work offers a roadmap for developing robust, adaptive models\nfor signal intelligence.", "AI": {"tldr": "A review of autoencoders and vision transformers for unsupervised signal analysis, covering architectures, applications, and trends in domains like IoT and biomedical engineering.", "motivation": "The rapid growth of unlabeled time-series data in fields like IoT and biomedical engineering necessitates advancements in unsupervised learning for signal analysis.", "method": "The review synthesizes recent progress in using autoencoders and vision transformers, focusing on their architectures, applications, and hybrid approaches.", "result": "Highlights strengths in feature extraction, anomaly detection, and classification, but notes challenges in interpretability, scalability, and domain generalization.", "conclusion": "Provides a roadmap for developing robust, adaptive models for signal intelligence by bridging methodological innovations and practical applications."}}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119", "abs": "https://arxiv.org/abs/2504.17119", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github", "AI": {"tldr": "A survey on small language models (SLMs) for healthcare, highlighting their scalability and clinical viability in resource-limited settings. It provides a taxonomic framework, timeline, and experimental results to guide professionals.", "motivation": "Addressing data privacy concerns and resource limitations in healthcare, SLMs offer a practical alternative to large language models (LLMs) for efficient healthcare informatics.", "method": "The paper presents a taxonomic framework categorizing SLMs by NLP tasks, stakeholder roles, and care continuum. It covers model building, adaptation, and optimization techniques like prompting and compression.", "result": "A comprehensive survey with experimental results demonstrates SLMs' transformative potential in healthcare NLP tasks.", "conclusion": "SLMs are a viable solution for healthcare informatics, with the survey providing resources and insights for future research and development."}}
{"id": "2504.17520", "pdf": "https://arxiv.org/pdf/2504.17520", "abs": "https://arxiv.org/abs/2504.17520", "authors": ["Zhuojun Tian", "Zhaoyang Zhang", "Yiwei Li", "Mehdi Bennis"], "title": "Communication-Efficient Personalized Distributed Learning with Data and Node Heterogeneity", "categories": ["cs.LG", "cs.DC", "cs.MA"], "comment": "Accepcted by TCCN", "summary": "To jointly tackle the challenges of data and node heterogeneity in\ndecentralized learning, we propose a distributed strong lottery ticket\nhypothesis (DSLTH), based on which a communication-efficient personalized\nlearning algorithm is developed. In the proposed method, each local model is\nrepresented as the Hadamard product of global real-valued parameters and a\npersonalized binary mask for pruning. The local model is learned by updating\nand fusing the personalized binary masks while the real-valued parameters are\nfixed among different agents. To further reduce the complexity of hardware\nimplementation, we incorporate a group sparse regularization term in the loss\nfunction, enabling the learned local model to achieve structured sparsity.\nThen, a binary mask aggregation algorithm is designed by introducing an\nintermediate aggregation tensor and adding a personalized fine-tuning step in\neach iteration, which constrains model updates towards the local data\ndistribution. The proposed method effectively leverages the relativity among\nagents while meeting personalized requirements in heterogeneous node\nconditions. We also provide a theoretical proof for the DSLTH, establishing it\nas the foundation of the proposed method. Numerical simulations confirm the\nvalidity of the DSLTH and demonstrate the effectiveness of the proposed\nalgorithm.", "AI": {"tldr": "A communication-efficient personalized learning algorithm is proposed to address data and node heterogeneity in decentralized learning, using global parameters and personalized binary masks, with theoretical and empirical validation.", "motivation": "To tackle data and node heterogeneity in decentralized learning while ensuring communication efficiency and personalization.", "method": "Each local model combines global real-valued parameters with personalized binary masks, updated via a binary mask aggregation algorithm with group sparse regularization.", "result": "The method effectively leverages agent relativity and meets personalized requirements, validated by numerical simulations.", "conclusion": "The proposed algorithm and DSLTH foundation are effective for heterogeneous decentralized learning."}}
{"id": "2504.17353", "pdf": "https://arxiv.org/pdf/2504.17353", "abs": "https://arxiv.org/abs/2504.17353", "authors": ["Chengguang Gan", "Sunbowen Lee", "Zhixi Cai", "Yanbin Wei", "Lei Zheng", "Yunhao Liang", "Shiwen Ni", "Tatsunori Mori"], "title": "M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": null, "summary": "Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\nof information extraction and model interpretability. MRE aims to leverage the\nmutual understanding between tasks of different granularities, enhancing the\nperformance of both coarse-grained and fine-grained tasks through joint\nmodeling. While MRE has been explored and validated in the textual domain, its\napplicability to visual and multimodal domains remains unexplored. In this\nwork, we extend MRE to the multimodal information extraction domain for the\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\nthis task. To address the challenges posed by M-MRE, we further propose a\nPrompt Format Adapter (PFA) that is fully compatible with various Large\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\nalso be observed in the M-MRE task, a multimodal text-image understanding\nscenario. This provides strong evidence that MRE facilitates mutual gains\nacross three interrelated tasks, confirming its generalizability beyond the\ntextual domain.", "AI": {"tldr": "The paper extends Mutual Reinforcement Effect (MRE) to multimodal domains, introducing Multimodal MRE (M-MRE) and a dataset. A Prompt Format Adapter (PFA) is proposed for Large Vision-Language Models, showing MRE's effectiveness in multimodal tasks.", "motivation": "To explore MRE's applicability beyond text, addressing its unexplored potential in visual and multimodal domains.", "method": "Introduces M-MRE task and dataset, proposes PFA for compatibility with LVLMs, and validates through experiments.", "result": "MRE is observed in M-MRE, showing mutual gains across tasks, confirming its generalizability to multimodal scenarios.", "conclusion": "MRE is effective in multimodal domains, enhancing performance across interrelated tasks, proving its broader applicability."}}
{"id": "2504.17114", "pdf": "https://arxiv.org/pdf/2504.17114", "abs": "https://arxiv.org/abs/2504.17114", "authors": ["Valentin Langer", "Kartikay Tehlan", "Thomas Wendler"], "title": "Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "The code is available under\n  https://github.com/tinolan/curve_fit_multi_idif", "summary": "Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron\nemission tomography (PET) requires anatomically constrained modelling of\nimage-derived input functions (IDIFs). Traditionally, IDIFs are obtained from\nthe aorta, neglecting anatomical variations and complex vascular contributions.\nThis study proposes a multi-organ segmentation-based approach that integrates\nIDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using\nhigh-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we\nincorporate organ-specific blood supply sources to improve kinetic modelling.\nOur method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,\nresulting in a mean squared error (MSE) reduction of $13.39\\%$ for the liver\nand $10.42\\%$ for the lungs. These initial results highlight the potential of\nmultiple IDIFs in improving anatomical modelling and fully leveraging dynamic\nPET imaging. This approach could facilitate the integration of tracer kinetic\nmodelling into clinical routine.", "AI": {"tldr": "The paper proposes a multi-organ segmentation-based method to improve kinetic modeling in dynamic PET by integrating IDIFs from multiple vascular sources, showing reduced MSE in liver and lungs.", "motivation": "Traditional IDIFs from the aorta overlook anatomical variations and complex vascular contributions, limiting accurate kinetic analysis.", "method": "Uses high-resolution CT segmentations of organs (liver, lungs, kidneys, bladder) to integrate IDIFs from aorta, portal vein, pulmonary artery, and ureters.", "result": "Achieved MSE reductions of 13.39% for liver and 10.42% for lungs in dynamic PET data from nine patients.", "conclusion": "The approach enhances anatomical modeling and could make tracer kinetic modeling more clinically viable."}}
{"id": "2504.17076", "pdf": "https://arxiv.org/pdf/2504.17076", "abs": "https://arxiv.org/abs/2504.17076", "authors": ["Jens Petersen", "Davide Abati", "Amirhossein Habibian", "Auke Wiggers"], "title": "Scene-Aware Location Modeling for Data Augmentation in Automotive Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Generative image models are increasingly being used for training data\naugmentation in vision tasks. In the context of automotive object detection,\nmethods usually focus on producing augmented frames that look as realistic as\npossible, for example by replacing real objects with generated ones. Others try\nto maximize the diversity of augmented frames, for example by pasting lots of\ngenerated objects onto existing backgrounds. Both perspectives pay little\nattention to the locations of objects in the scene. Frame layouts are either\nreused with little or no modification, or they are random and disregard realism\nentirely. In this work, we argue that optimal data augmentation should also\ninclude realistic augmentation of layouts. We introduce a scene-aware\nprobabilistic location model that predicts where new objects can realistically\nbe placed in an existing scene. By then inpainting objects in these locations\nwith a generative model, we obtain much stronger augmentation performance than\nexisting approaches. We set a new state of the art for generative data\naugmentation on two automotive object detection tasks, achieving up to\n$2.8\\times$ higher gains than the best competing approach ($+1.4$ vs. $+0.5$\nmAP boost). We also demonstrate significant improvements for instance\nsegmentation.", "AI": {"tldr": "The paper introduces a scene-aware probabilistic location model for realistic layout augmentation in generative data augmentation for automotive object detection, outperforming existing methods.", "motivation": "Existing generative data augmentation methods for automotive object detection either focus on realism or diversity but neglect realistic object placement (layouts).", "method": "A scene-aware probabilistic location model predicts realistic object placements, combined with generative inpainting for augmentation.", "result": "Achieves up to 2.8\u00d7 higher gains than competitors (+1.4 vs. +0.5 mAP boost) and improves instance segmentation.", "conclusion": "Realistic layout augmentation is crucial for optimal data augmentation, demonstrated by superior performance in automotive tasks."}}
{"id": "2504.17017", "pdf": "https://arxiv.org/pdf/2504.17017", "abs": "https://arxiv.org/abs/2504.17017", "authors": ["Balaji Rao", "William Eiers", "Carlo Lipizzi"], "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.LO"], "comment": "Accepted to the Proceedings of the 19th Conference on Neurosymbolic\n  Learning and Reasoning (NeSy 2025)", "summary": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks.", "AI": {"tldr": "A framework for generating formal proofs in LLMs, combining natural language statements, proof generation, and heuristic-based proof assembly, validated on miniF2F-test and Isabelle.", "motivation": "To address the challenge of formal verification in LLM-generated code and advance theorem proving as a benchmark for reasoning in LLMs.", "method": "A 3-component framework: natural language statement generation, LLM-based formal proof generation, and heuristic-driven proof assembly. Training involves SFT and RL-based fine-tuning.", "result": "Validated on miniF2F-test and Isabelle, with a use case for AWS S3 bucket policy verification. A curated dataset (FVEL_ER) is also introduced.", "conclusion": "The framework advances formal verification in LLMs, demonstrating potential for practical applications like code correctness verification."}}
{"id": "2504.16980", "pdf": "https://arxiv.org/pdf/2504.16980", "abs": "https://arxiv.org/abs/2504.16980", "authors": ["Pratyush Maini", "Sachin Goyal", "Dylan Sam", "Alex Robey", "Yash Savani", "Yiding Jiang", "Andy Zou", "Zacharcy C. Lipton", "J. Zico Kolter"], "title": "Safety Pretraining: Toward the Next Generation of Safe AI", "categories": ["cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, the risk of generating harmful or toxic content remains a central\nchallenge. Post-hoc alignment methods are brittle: once unsafe patterns are\nlearned during pretraining, they are hard to remove. We present a data-centric\npretraining framework that builds safety into the model from the start. Our\ncontributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled\nexamples, used to filter 600B tokens; (ii) the largest synthetic safety dataset\nto date (100B tokens) generated via recontextualization of harmful web data;\n(iii) RefuseWeb and Moral Education datasets that convert harmful prompts into\nrefusal dialogues and web-style educational material; (iv) Harmfulness-Tag\nannotations injected during pretraining to flag unsafe content and steer away\ninference from harmful generations; and (v) safety evaluations measuring base\nmodel behavior before instruction tuning. Our safety-pretrained models reduce\nattack success rates from 38.8% to 8.4% with no performance degradation on\nstandard LLM safety benchmarks.", "AI": {"tldr": "A data-centric pretraining framework for LLMs reduces harmful content generation by integrating safety measures from the start, achieving an 8.4% attack success rate without performance loss.", "motivation": "Addressing the brittleness of post-hoc alignment methods and the persistent risk of harmful content generation in LLMs.", "method": "Develops a safety classifier, synthetic safety datasets, refusal dialogues, educational material, and harmfulness-tag annotations during pretraining.", "result": "Reduces attack success rates from 38.8% to 8.4% while maintaining performance on safety benchmarks.", "conclusion": "Proactive safety integration during pretraining effectively mitigates harmful outputs in LLMs."}}
{"id": "2504.17130", "pdf": "https://arxiv.org/pdf/2504.17130", "abs": "https://arxiv.org/abs/2504.17130", "authors": ["Hannah Cyberey", "David Evans"], "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control", "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector", "AI": {"tldr": "The paper investigates censorship in LLMs, using representation engineering to identify and control refusal-compliance vectors and thought suppression in reasoning models.", "motivation": "To understand how LLMs are tuned to refuse harmful requests and align with controller preferences, and to explore methods for detecting and manipulating censorship.", "method": "Uses representation engineering to find refusal-compliance vectors and analyzes reasoning LLMs to uncover thought suppression, applying negative vectors to remove censorship.", "result": "Identifies vectors that detect and control censorship levels and suppresses reasoning to bypass censorship.", "conclusion": "Demonstrates effective methods for analyzing and manipulating censorship in LLMs, revealing underlying mechanisms of control."}}
{"id": "2504.16129", "pdf": "https://arxiv.org/pdf/2504.16129", "abs": "https://arxiv.org/abs/2504.16129", "authors": ["Junwei Liao", "Muning Wen", "Jun Wang", "Weinan Zhang"], "title": "MARFT: Multi-Agent Reinforcement Fine-Tuning", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.RO"], "comment": "36 pages", "summary": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks requiring multifaceted reasoning and\ncollaboration, from generating high-quality presentation slides to conducting\nsophisticated scientific research. Meanwhile, RL has been widely recognized for\nits effectiveness in enhancing agent intelligence, but limited research has\ninvestigated the fine-tuning of LaMAS using foundational RL techniques.\nMoreover, the direct application of MARL methodologies to LaMAS introduces\nsignificant challenges, stemming from the unique characteristics and mechanisms\ninherent to LaMAS. To address these challenges, this article presents a\ncomprehensive study of LLM-based MARL and proposes a novel paradigm termed\nMulti-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal\nalgorithmic framework tailored for LaMAS, outlining the conceptual foundations,\nkey distinctions, and practical implementation strategies. We begin by\nreviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage\nfor a parallel analysis in the multi-agent domain. In the context of LaMAS, we\nelucidate critical differences between MARL and MARFT. These differences\nmotivate a transition toward a novel, LaMAS-oriented formulation of RFT.\nCentral to this work is the presentation of a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work aims to serve as a roadmap for researchers seeking to advance MARFT\ntoward resilient and adaptive solutions in agentic systems. Our implementation\nof the proposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT.", "AI": {"tldr": "The paper introduces Multi-Agent Reinforcement Fine-Tuning (MARFT), a novel paradigm for fine-tuning LLM-based Multi-Agent Systems (LaMAS) using foundational RL techniques, addressing challenges unique to LaMAS.", "motivation": "To bridge the gap in applying MARL to LaMAS by proposing MARFT, a tailored framework for enhancing agent intelligence in complex, collaborative tasks.", "method": "The paper reviews RL's evolution to Reinforcement Fine-Tuning, contrasts MARL and MARFT, and presents a scalable MARFT framework with an open-source implementation.", "result": "A robust and scalable MARFT framework is introduced, with practical implementation strategies and real-world application insights.", "conclusion": "The work serves as a roadmap for advancing MARFT in agentic systems, offering a foundation for resilient and adaptive solutions."}}
{"id": "2504.17547", "pdf": "https://arxiv.org/pdf/2504.17547", "abs": "https://arxiv.org/abs/2504.17547", "authors": ["Jiaqi Deng", "Zonghan Wu", "Huan Huo", "Guandong Xu"], "title": "A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems: The Lifecycle of Knowledge in Visual Reasoning Task", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "20 pages, 5 figures, 4 tables", "summary": "Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications.", "AI": {"tldr": "This paper surveys KB-VQA, highlighting its challenges, the role of LLMs, and proposing a taxonomy for KB-VQA methods.", "motivation": "To address the lack of a comprehensive survey on KB-VQA, which integrates visual, textual, and knowledge inputs for advanced applications.", "method": "The survey categorizes KB-VQA systems into stages: knowledge representation, retrieval, and reasoning, and reviews existing methods.", "result": "A structured taxonomy of KB-VQA approaches is established, and future research directions are outlined.", "conclusion": "The survey provides a foundation for advancing KB-VQA models by organizing existing methods and identifying challenges."}}
{"id": "2504.17122", "pdf": "https://arxiv.org/pdf/2504.17122", "abs": "https://arxiv.org/abs/2504.17122", "authors": ["Kartikay Tehlan", "Thomas Wendler"], "title": "Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "The code is available at: https://github.com/tkartikay/PhysNRPET", "summary": "Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables\nnon-invasive quantification of glucose metabolism through kinetic analysis,\noften modelled by the two-tissue compartment model (TCKM). However, voxel-wise\nkinetic parameter estimation using conventional methods is computationally\nintensive and limited by spatial resolution. Deep neural networks (DNNs) offer\nan alternative but require large training datasets and significant\ncomputational resources. To address these limitations, we propose a\nphysiological neural representation based on implicit neural representations\n(INRs) for personalized kinetic parameter estimation. INRs, which learn\ncontinuous functions, allow for efficient, high-resolution parametric imaging\nwith reduced data requirements. Our method also integrates anatomical priors\nfrom a 3D CT foundation model to enhance robustness and precision in kinetic\nmodelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset\nand compare it to state-of-the-art DNNs. Results demonstrate superior spatial\nresolution, lower mean-squared error, and improved anatomical consistency,\nparticularly in tumour and highly vascularized regions. Our findings highlight\nthe potential of INRs for personalized, data-efficient tracer kinetic\nmodelling, enabling applications in tumour characterization, segmentation, and\nprognostic assessment.", "AI": {"tldr": "The paper proposes a physiological neural representation using implicit neural representations (INRs) for efficient, high-resolution kinetic parameter estimation in dynamic PET imaging, outperforming traditional methods and deep neural networks.", "motivation": "Conventional kinetic parameter estimation in PET imaging is computationally intensive and limited by resolution, while deep neural networks require large datasets and resources. The authors aim to overcome these limitations.", "method": "The method uses INRs to learn continuous functions for personalized kinetic parameter estimation, integrating anatomical priors from a 3D CT foundation model to enhance robustness.", "result": "The approach achieves superior spatial resolution, lower mean-squared error, and better anatomical consistency compared to state-of-the-art DNNs, especially in tumour and vascularized regions.", "conclusion": "INRs offer a promising solution for data-efficient, personalized kinetic modelling, with potential applications in tumour characterization and prognostic assessment."}}
{"id": "2504.17111", "pdf": "https://arxiv.org/pdf/2504.17111", "abs": "https://arxiv.org/abs/2504.17111", "authors": ["Tekin Gunasar", "Virginia de Sa"], "title": "Transferring Spatial Filters via Tangent Space Alignment in Motor Imagery BCIs", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "We propose a method to improve subject transfer in motor imagery BCIs by\naligning covariance matrices on a Riemannian manifold, followed by computing a\nnew common spatial patterns (CSP) based spatial filter. We explore various ways\nto integrate information from multiple subjects and show improved performance\ncompared to standard CSP. Across three datasets, our method shows marginal\nimprovements over standard CSP; however, when training data are limited, the\nimprovements become more significant.", "AI": {"tldr": "Proposes a Riemannian manifold-based method for subject transfer in motor imagery BCIs, improving CSP performance, especially with limited training data.", "motivation": "To enhance subject transfer in motor imagery BCIs by addressing limitations of standard CSP, particularly with scarce training data.", "method": "Aligns covariance matrices on a Riemannian manifold and computes a new CSP-based spatial filter, integrating multi-subject information.", "result": "Marginal improvements over standard CSP in three datasets, with significant gains when training data is limited.", "conclusion": "The method effectively improves subject transfer, especially in data-scarce scenarios, outperforming standard CSP."}}
{"id": "2504.17087", "pdf": "https://arxiv.org/pdf/2504.17087", "abs": "https://arxiv.org/abs/2504.17087", "authors": ["Yuran Li", "Jama Hussein Mohamud", "Chongren Sun", "Di Wu", "Benoit Boulet"], "title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments", "categories": ["cs.AI"], "comment": "12 pages, 5 figures, 6 tables", "summary": "Large language models (LLMs) are being widely applied across various fields,\nbut as tasks become more complex, evaluating their responses is increasingly\nchallenging. Compared to human evaluators, the use of LLMs to support\nperformance evaluation offers a more efficient alternative. However, most\nstudies focus mainly on aligning LLMs' judgments with human preferences,\noverlooking the existence of biases and mistakes in human judgment.\nFurthermore, how to select suitable LLM judgments given multiple potential LLM\nresponses remains underexplored. To address these two aforementioned issues, we\npropose a three-stage meta-judge selection pipeline: 1) developing a\ncomprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM\nagents to score judgments, and 3) applying a threshold to filter out\nlow-scoring judgments. Compared to methods using a single LLM as both judge and\nmeta-judge, our pipeline introduces multi-agent collaboration and a more\ncomprehensive rubric. Experimental results on the JudgeBench dataset show about\n15.55\\% improvement compared to raw judgments and about 8.37\\% improvement over\nthe single-agent baseline. Our work demonstrates the potential of LLMs as\nmeta-judges and lays the foundation for future research on constructing\npreference datasets for LLM-as-a-judge reinforcement learning.", "AI": {"tldr": "A three-stage pipeline improves LLM-based evaluation by addressing biases and selecting better judgments, outperforming single-agent methods.", "motivation": "Challenges in evaluating complex LLM tasks and overlooked biases in human judgment necessitate a better evaluation method.", "method": "Proposes a three-stage pipeline: rubric development with GPT-4 and humans, scoring by three LLM agents, and filtering low-scoring judgments.", "result": "15.55% improvement over raw judgments and 8.37% over single-agent baselines on JudgeBench.", "conclusion": "LLMs show promise as meta-judges, enabling better preference datasets for reinforcement learning."}}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment.", "AI": {"tldr": "The paper explores whether automated hallucination detection in LLMs is possible, linking it to language identification. It shows detection is impossible without expert-labeled feedback but feasible with it.", "motivation": "To analyze the feasibility of automatically detecting hallucinations in LLM outputs, inspired by language identification frameworks.", "method": "Theoretical framework linking hallucination detection to language identification, analyzing training regimes with and without expert-labeled feedback.", "result": "Hallucination detection is impossible without expert-labeled feedback but becomes feasible with it for all countable language collections.", "conclusion": "Expert-labeled feedback is crucial for training hallucination detectors, supporting feedback-based methods like RLHF for reliable LLM deployment."}}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137", "abs": "https://arxiv.org/abs/2504.17137", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE.", "AI": {"tldr": "MIRAGE is a QA dataset for evaluating RAG systems, featuring 7,560 instances and 37,800 retrieval entries, with novel metrics for adaptability.", "motivation": "Current RAG evaluation lacks benchmarks for detailed, component-specific assessment due to the complexity of retrieval-generation interplay.", "method": "Introduces MIRAGE, a curated QA dataset with a large retrieval pool, and novel metrics for adaptability (e.g., noise vulnerability).", "result": "Comprehensive experiments reveal insights into optimal retriever-LLM alignment and RAG system dynamics.", "conclusion": "MIRAGE facilitates precise RAG evaluation and is publicly available for research use."}}
{"id": "2402.16201", "pdf": "https://arxiv.org/pdf/2402.16201", "abs": "https://arxiv.org/abs/2402.16201", "authors": ["Yunqi Zhang", "Shaileshh Bojja Venkatakrishnan"], "title": "Honeybee: Byzantine Tolerant Decentralized Peer Sampling with Verifiable Random Walks", "categories": ["cs.NI", "cs.CR", "cs.DC", "cs.DS", "cs.MA"], "comment": "32 pages; acmsmall-conf", "summary": "Popular blockchains today have hundreds of thousands of nodes and need to be\nable to support sophisticated scaling solutions$\\unicode{x2013}$such as\nsharding, data availability sampling, and layer-2 methods. Designing secure and\nefficient peer-to-peer (p2p) networking protocols at these scales to support\nthe tight demands of the upper layer crypto-economic primitives is a highly\nnon-trivial endeavor. We identify decentralized, uniform random sampling of\nnodes as a fundamental capability necessary for building robust p2p networks in\nemerging blockchain networks. Sampling algorithms used in practice today\n(primarily for address discovery) rely on either distributed hash tables (e.g.,\nKademlia) or sharing addresses with neighbors (e.g., GossipSub), and are not\nsecure in a Sybil setting. We present Honeybee, a decentralized algorithm for\nsampling nodes that uses verifiable random walks and table consistency checks.\nHoneybee is secure against attacks even in the presence of an overwhelming\nnumber of Byzantine nodes (e.g., $\\geq50\\%$ of the network). We evaluate\nHoneybee through experiments and show that the quality of sampling achieved by\nHoneybee is significantly better compared to the state-of-the-art. Our proposed\nalgorithm has implications for network design in both full nodes and light\nnodes.", "AI": {"tldr": "Honeybee is a decentralized algorithm for secure and efficient node sampling in blockchain p2p networks, resistant to Sybil attacks and Byzantine nodes.", "motivation": "Current node sampling methods in blockchain p2p networks are insecure against Sybil attacks and Byzantine nodes, necessitating a robust solution.", "method": "Honeybee uses verifiable random walks and table consistency checks for decentralized, uniform random node sampling.", "result": "Honeybee outperforms state-of-the-art methods, providing high-quality sampling even with \u226550% Byzantine nodes.", "conclusion": "Honeybee is a secure and efficient solution for node sampling, applicable to both full and light nodes in blockchain networks."}}
{"id": "2504.17728", "pdf": "https://arxiv.org/pdf/2504.17728", "abs": "https://arxiv.org/abs/2504.17728", "authors": ["Shucheng Gong", "Lingzhe Zhao", "Wenpu Li", "Hong Xie", "Yin Zhang", "Shiyu Zhao", "Peidong Liu"], "title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Source Code: https://github.com/WU-CVGL/CasualHDRSplat", "summary": "Recently, photo-realistic novel view synthesis from multi-view images, such\nas neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered\nwidespread attention due to their superior performance. However, most works\nrely on low dynamic range (LDR) images, which limits the capturing of richer\nscene details. Some prior works have focused on high dynamic range (HDR) scene\nreconstruction, typically require capturing of multi-view sharp images with\ndifferent exposure times at fixed camera positions during exposure times, which\nis time-consuming and challenging in practice. For a more flexible data\nacquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily\nand robustly reconstruct the 3D HDR scene from casually captured videos with\nauto-exposure enabled, even in the presence of severe motion blur and varying\nunknown exposure time. \\textbf{CasualHDRSplat} contains a unified\ndifferentiable physical imaging model which first applies continuous-time\ntrajectory constraint to imaging process so that we can jointly optimize\nexposure time, camera response function (CRF), camera poses, and sharp 3D HDR\nscene. Extensive experiments demonstrate that our approach outperforms existing\nmethods in terms of robustness and rendering quality. Our source code will be\navailable at https://github.com/WU-CVGL/CasualHDRSplat", "AI": {"tldr": "CasualHDRSplat enables 3D HDR scene reconstruction from casually captured videos with auto-exposure, overcoming challenges like motion blur and varying exposure times.", "motivation": "Existing methods rely on LDR images or require rigid multi-view HDR capture setups, which are impractical. CasualHDRSplat aims for flexible, robust HDR reconstruction.", "method": "Uses a differentiable physical imaging model with continuous-time trajectory constraints to jointly optimize exposure, CRF, camera poses, and sharp 3D HDR scenes.", "result": "Outperforms existing methods in robustness and rendering quality.", "conclusion": "CasualHDRSplat offers a practical, high-quality solution for HDR scene reconstruction from casual videos."}}
{"id": "2504.17255", "pdf": "https://arxiv.org/pdf/2504.17255", "abs": "https://arxiv.org/abs/2504.17255", "authors": ["Shaoyu Pei", "Renxiong Wu", "Hao Zheng", "Lang Qin", "Shuaichen Lin", "Yuxing Gan", "Wenjing Huang", "Zhixuan Wang", "Mohan Qin", "Yong Liu", "Guangming Ni"], "title": "3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations", "categories": ["eess.IV", "cs.AI", "physics.optics"], "comment": null, "summary": "Skin, the primary regulator of heat exchange, relies on sweat glands for\nthermoregulation. Alterations in sweat gland morphology play a crucial role in\nvarious pathological conditions and clinical diagnoses. Current methods for\nobserving sweat gland morphology are limited by their two-dimensional, in\nvitro, and destructive nature, underscoring the urgent need for real-time,\nnon-invasive, quantifiable technologies. We proposed a novel three-dimensional\n(3D) transformer-based multi-object segmentation framework, integrating a\nsliding window approach, joint spatial-channel attention mechanism, and\narchitectural heterogeneity between shallow and deep layers. Our proposed\nnetwork enables precise 3D sweat gland segmentation from skin volume data\ncaptured by optical coherence tomography (OCT). For the first time, subtle\nvariations of sweat gland 3D morphology in response to temperature changes,\nhave been visualized and quantified. Our approach establishes a benchmark for\nnormal sweat gland morphology and provides a real-time, non-invasive tool for\nquantifying 3D structural parameters. This enables the study of individual\nvariability and pathological changes in sweat gland structure, advancing\ndermatological research and clinical applications, including thermoregulation\nand bromhidrosis treatment.", "AI": {"tldr": "A 3D transformer-based framework for non-invasive, real-time sweat gland segmentation from OCT data, enabling visualization and quantification of morphological changes under temperature variations.", "motivation": "Current methods for observing sweat gland morphology are limited to 2D, in vitro, and destructive techniques, creating a need for advanced, non-invasive tools.", "method": "A novel 3D transformer-based multi-object segmentation framework with sliding window, joint spatial-channel attention, and architectural heterogeneity between layers.", "result": "Precise 3D segmentation of sweat glands from OCT data, allowing visualization and quantification of temperature-induced morphological changes.", "conclusion": "The framework provides a benchmark for normal sweat gland morphology and a tool for studying variability and pathology, advancing dermatological research and clinical applications."}}
{"id": "2504.17132", "pdf": "https://arxiv.org/pdf/2504.17132", "abs": "https://arxiv.org/abs/2504.17132", "authors": ["Ning Li", "Antai Andy Liu", "Jingran Zhang", "Justin Cui"], "title": "Latent Video Dataset Distillation", "categories": ["cs.CV"], "comment": "https://openreview.net/forum?id=i665TIHv92", "summary": "Dataset distillation has demonstrated remarkable effectiveness in\nhigh-compression scenarios for image datasets. While video datasets inherently\ncontain greater redundancy, existing video dataset distillation methods\nprimarily focus on compression in the pixel space, overlooking advances in the\nlatent space that have been widely adopted in modern text-to-image and\ntext-to-video models. In this work, we bridge this gap by introducing a novel\nvideo dataset distillation approach that operates in the latent space using a\nstate-of-the-art variational encoder. Furthermore, we employ a diversity-aware\ndata selection strategy to select both representative and diverse samples.\nAdditionally, we introduce a simple, training-free method to further compress\nthe distilled latent dataset. By combining these techniques, our approach\nachieves a new state-of-the-art performance in dataset distillation,\noutperforming prior methods on all datasets, e.g. on HMDB51 IPC 1, we achieve a\n2.6% performance increase; on MiniUCF IPC 5, we achieve a 7.8% performance\nincrease.", "AI": {"tldr": "A novel video dataset distillation method in latent space with a diversity-aware selection strategy and training-free compression, achieving state-of-the-art performance.", "motivation": "Existing video dataset distillation methods focus on pixel space, ignoring latent space advancements.", "method": "Uses a variational encoder for latent space distillation, diversity-aware sample selection, and training-free compression.", "result": "Outperforms prior methods, e.g., 2.6% increase on HMDB51 IPC 1, 7.8% on MiniUCF IPC 5.", "conclusion": "The approach sets a new benchmark in video dataset distillation by leveraging latent space and diversity-aware techniques."}}
{"id": "2504.17179", "pdf": "https://arxiv.org/pdf/2504.17179", "abs": "https://arxiv.org/abs/2504.17179", "authors": ["Mohammad Zarei", "Melanie A Jutras", "Eliana Evans", "Mike Tan", "Omid Aaramoon"], "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO", "68T45, 68T05 68T45, 68T05 68T45, 68T05", "I.2.6; I.2.10; I.4.8"], "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025", "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems.", "AI": {"tldr": "A novel approach using generative and explainable AI to address rare failure modes (RFMs) in AVs by generating diverse environments and natural language descriptions for improved robustness.", "motivation": "AVs struggle with detecting RFMs due to the long-tail challenge, where rare events are underrepresented in training data.", "method": "Utilizes segmentation masks, environmental masks, and a custom diffusion model (Stable Diffusion inpainting) with adversarial noise optimization to generate RFM scenarios.", "result": "Generates images and descriptions of RFMs to expose AI vulnerabilities, aiding in model training and testing.", "conclusion": "The approach enhances AV safety and reliability by addressing RFMs through generative AI and explainability."}}
{"id": "2504.17028", "pdf": "https://arxiv.org/pdf/2504.17028", "abs": "https://arxiv.org/abs/2504.17028", "authors": ["Iman Khadir", "Shane Stevenson", "Henry Li", "Kyle Krick", "Abram Burrows", "David Hall", "Stan Posey", "Samuel S. P. Shen"], "title": "Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": "12 pages, 8 figures", "summary": "This paper demonstrates the feasibility of democratizing AI-driven global\nweather forecasting models among university research groups by leveraging\nGraphics Processing Units (GPUs) and freely available AI models, such as\nNVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network\nfor weather prediction and is trained on a 73-channel subset of the European\nCentre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset\nat single levels and different pressure levels. Although the training\nspecifications for FourCastNetv2 are not released to the public, the training\ndocumentation of the model's first generation, FourCastNet, is available to all\nusers. The training had 64 A100 GPUs and took 16 hours to complete. Although\nNVIDIA's models offer significant reductions in both time and cost compared to\ntraditional Numerical Weather Prediction (NWP), reproducing published\nforecasting results presents ongoing challenges for resource-constrained\nuniversity research groups with limited GPU availability. We demonstrate both\n(i) leveraging FourCastNetv2 to create predictions through the designated\napplication programming interface (API) and (ii) utilizing NVIDIA hardware to\ntrain the original FourCastNet model. Further, this paper demonstrates the\ncapabilities and limitations of NVIDIA A100's for resource-limited research\ngroups in universities. We also explore data management, training efficiency,\nand model validation, highlighting the advantages and challenges of using\nlimited high-performance computing resources. Consequently, this paper and its\ncorresponding GitHub materials may serve as an initial guide for other\nuniversity research groups and courses related to machine learning, climate\nscience, and data science to develop research and education programs on AI\nweather forecasting, and hence help democratize the AI NWP in the digital\neconomy.", "AI": {"tldr": "The paper explores democratizing AI-driven weather forecasting for university research groups using GPUs and freely available AI models like NVIDIA's FourCastNetv2, addressing challenges and providing practical guidance.", "motivation": "To make AI-driven weather forecasting accessible to resource-constrained university research groups by leveraging existing tools and hardware.", "method": "Utilizes NVIDIA's FourCastNetv2 for predictions and trains the original FourCastNet model using NVIDIA A100 GPUs, focusing on data management, training efficiency, and validation.", "result": "Demonstrates the feasibility and limitations of using limited GPU resources for AI weather forecasting, providing a guide for research and education.", "conclusion": "The paper serves as a foundational resource for democratizing AI weather forecasting in academia, aiding research and education in machine learning, climate science, and data science."}}
{"id": "2504.17192", "pdf": "https://arxiv.org/pdf/2504.17192", "abs": "https://arxiv.org/abs/2504.17192", "authors": ["Minju Seo", "Jinheon Baek", "Seongyun Lee", "Sung Ju Hwang"], "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.", "AI": {"tldr": "PaperCoder is a multi-agent LLM framework that converts machine learning papers into functional code repositories, excelling in quality and faithfulness compared to benchmarks.", "motivation": "Addressing the lack of available code implementations in machine learning research, which hinders reproducibility and progress.", "method": "Uses a three-stage multi-agent LLM framework: planning (roadmap, architecture, dependencies), analysis (implementation details), and generation (modular code).", "result": "Demonstrates high-quality, faithful implementations, outperforming baselines in the PaperBench benchmark.", "conclusion": "PaperCoder effectively bridges the gap between research papers and functional code, enhancing reproducibility and collaboration."}}
{"id": "2504.07687", "pdf": "https://arxiv.org/pdf/2504.07687", "abs": "https://arxiv.org/abs/2504.07687", "authors": ["Yihao Wang", "Zhong Qian", "Peifeng Li"], "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets, often\npolitically or virally motivated-pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis.", "AI": {"tldr": "The paper introduces FMNV, a dataset of professionally crafted fake news videos, and FMNVD, a detection model using LLMs and multimodal features, outperforming existing methods.", "motivation": "Addressing the gap in datasets for high-impact fake news videos from media outlets, which pose greater societal harm than user-generated content.", "method": "Constructed FMNV dataset, categorized fake news types, used LLMs to generate deceptive content, and developed FMNVD model with dual-stream architecture (CLIP and Faster R-CNN) and co-attention mechanisms.", "result": "FMNV generalizes well across baselines; FMNVD shows superior detection efficacy.", "conclusion": "Establishes benchmarks for detecting high-impact fake news and advances cross-modal inconsistency analysis."}}
{"id": "2504.17379", "pdf": "https://arxiv.org/pdf/2504.17379", "abs": "https://arxiv.org/abs/2504.17379", "authors": ["Hassan Keshvarikhojasteh", "Mihail Tifrea", "Sibylle Hess", "Josien P. W. Pluim", "Mitko Veta"], "title": "A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Multiple instance learning (MIL) is a promising approach for weakly\nsupervised classification in pathology using whole slide images (WSIs).\nHowever, conventional MIL methods such as Attention-Based Deep Multiple\nInstance Learning (ABMIL) typically disregard spatial interactions among\npatches that are crucial to pathological diagnosis. Recent advancements, such\nas Transformer based MIL (TransMIL), have incorporated spatial context and\ninter-patch relationships. However, it remains unclear whether explicitly\nmodeling patch relationships yields similar performance gains in ABMIL, which\nrelies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs\nTransformer-based layers, introducing a fundamental architectural shift at the\ncost of substantially increased computational complexity. In this work, we\nenhance the ABMIL framework by integrating interaction-aware representations to\naddress this question. Our proposed model, Global ABMIL (GABMIL), explicitly\ncaptures inter-instance dependencies while preserving computational efficiency.\nExperimental results on two publicly available datasets for tumor subtyping in\nbreast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage\npoint improvement in AUPRC and a 5 percentage point increase in the Kappa score\nover ABMIL, with minimal or no additional computational overhead. These\nfindings underscore the importance of incorporating patch interactions within\nMIL frameworks.", "AI": {"tldr": "GABMIL enhances ABMIL by integrating interaction-aware representations, improving performance without added computational cost.", "motivation": "To address the lack of spatial interactions in ABMIL and compare it to TransMIL's approach.", "method": "Enhances ABMIL with interaction-aware representations (GABMIL) to capture inter-instance dependencies.", "result": "GABMIL outperforms ABMIL by 7% in AUPRC and 5% in Kappa score on tumor subtyping tasks.", "conclusion": "Incorporating patch interactions in MIL frameworks is crucial for performance gains."}}
{"id": "2504.17162", "pdf": "https://arxiv.org/pdf/2504.17162", "abs": "https://arxiv.org/abs/2504.17162", "authors": ["Cece Zhang", "Xuehuan Zhu", "Nick Peterson", "Jieqiong Wang", "Shibiao Wan"], "title": "A Comprehensive Review on RNA Subcellular Localization Prediction", "categories": ["cs.CV", "cs.AI", "q-bio.GN", "q-bio.SC"], "comment": null, "summary": "The subcellular localization of RNAs, including long non-coding RNAs\n(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,\nplays a critical role in determining their biological functions. For instance,\nlncRNAs are predominantly associated with chromatin and act as regulators of\ngene transcription and chromatin structure, while mRNAs are distributed across\nthe nucleus and cytoplasm, facilitating the transport of genetic information\nfor protein synthesis. Understanding RNA localization sheds light on processes\nlike gene expression regulation with spatial and temporal precision. However,\ntraditional wet lab methods for determining RNA localization, such as in situ\nhybridization, are often time-consuming, resource-demanding, and costly. To\novercome these challenges, computational methods leveraging artificial\nintelligence (AI) and machine learning (ML) have emerged as powerful\nalternatives, enabling large-scale prediction of RNA subcellular localization.\nThis paper provides a comprehensive review of the latest advancements in\nAI-based approaches for RNA subcellular localization prediction, covering\nvarious RNA types and focusing on sequence-based, image-based, and hybrid\nmethodologies that combine both data types. We highlight the potential of these\nmethods to accelerate RNA research, uncover molecular pathways, and guide\ntargeted disease treatments. Furthermore, we critically discuss the challenges\nin AI/ML approaches for RNA subcellular localization, such as data scarcity and\nlack of benchmarks, and opportunities to address them. This review aims to\nserve as a valuable resource for researchers seeking to develop innovative\nsolutions in the field of RNA subcellular localization and beyond.", "AI": {"tldr": "The paper reviews AI/ML advancements for predicting RNA subcellular localization, addressing challenges like data scarcity and benchmarking.", "motivation": "Traditional wet lab methods for RNA localization are inefficient; AI/ML offers scalable, cost-effective alternatives.", "method": "Comprehensive review of AI-based approaches, including sequence-based, image-based, and hybrid methods.", "result": "AI/ML methods show promise for large-scale RNA localization prediction, aiding research and disease treatment.", "conclusion": "The review serves as a resource for developing innovative solutions in RNA subcellular localization."}}
{"id": "2504.17282", "pdf": "https://arxiv.org/pdf/2504.17282", "abs": "https://arxiv.org/abs/2504.17282", "authors": ["Lynn Cherif", "Flemming Kondrup", "David Venuto", "Ankit Anand", "Doina Precup", "Khimya Khetarpal"], "title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Agents that can autonomously navigate the web through a graphical user\ninterface (GUI) using a unified action space (e.g., mouse and keyboard actions)\ncan require very large amounts of domain-specific expert demonstrations to\nachieve good performance. Low sample efficiency is often exacerbated in\nsparse-reward and large-action-space environments, such as a web GUI, where\nonly a few actions are relevant in any given situation. In this work, we\nconsider the low-data regime, with limited or no access to expert behavior. To\nenable sample-efficient learning, we explore the effect of constraining the\naction space through $\\textit{intent-based affordances}$ -- i.e., considering\nin any situation only the subset of actions that achieve a desired outcome. We\npropose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$,\na method that leverages pre-trained vision-language models (VLMs) to generate\ncode that determines affordable actions through implicit intent-completion\nfunctions and using a fully-automated program generation and verification\npipeline. These programs are then used in-the-loop of a reinforcement learning\nagent to return a set of affordances given a pixel observation. By greatly\nreducing the number of actions that an agent must consider, we demonstrate on a\nwide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$\n$\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,\n$\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of\ntasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared\nwith behavior cloning when a small number of expert demonstrations is\navailable.", "AI": {"tldr": "The paper introduces CoGA, a method using pre-trained vision-language models to generate code for action affordances, improving sample efficiency in web GUI navigation tasks.", "motivation": "Addressing the challenge of low sample efficiency in sparse-reward, large-action-space environments like web GUIs, especially with limited expert demonstrations.", "method": "CoGA leverages vision-language models to generate code for intent-based affordances, automating program generation and verification to constrain the action space.", "result": "CoGA significantly improves sample efficiency, generalizes within task families, and matches or outperforms behavior cloning with few expert demonstrations.", "conclusion": "CoGA demonstrates the effectiveness of intent-based affordances and automated code generation for efficient learning in complex environments."}}
{"id": "2504.17058", "pdf": "https://arxiv.org/pdf/2504.17058", "abs": "https://arxiv.org/abs/2504.17058", "authors": ["Rahul Vishwakarma"], "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems.", "AI": {"tldr": "A novel framework, Conformalized GAN (cGAN), integrates conformal prediction into GANs to provide statistical guarantees for synthetic data.", "motivation": "Existing generative models lack rigorous statistical guarantees, limiting their use in critical domains.", "method": "Incorporates conformal prediction methodologies (ICP, Mondrian, Cross-Conformal, Venn-Abers) into GANs.", "result": "cGAN offers enhanced calibration and provable statistical guarantees for synthetic data.", "conclusion": "cGAN enables reliable synthetic data generation for high-stakes applications like healthcare and finance."}}
{"id": "2504.17200", "pdf": "https://arxiv.org/pdf/2504.17200", "abs": "https://arxiv.org/abs/2504.17200", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support.", "AI": {"tldr": "A retrieval-augmented generation (RAG)-based multi-agent LLM system, WildfireGPT, is proposed to enhance decision-making for natural hazards, outperforming existing LLM solutions.", "motivation": "LLMs lack context-specificity for specialized domains like natural hazards. This work aims to bridge that gap for better decision support.", "method": "Uses a RAG-based multi-agent LLM system integrating hazard data, observations, and literature for tailored insights.", "result": "WildfireGPT significantly outperforms existing LLM solutions in expert-led case studies.", "conclusion": "The system effectively combines accuracy and contextual relevance for natural hazard decision-making."}}
{"id": "2504.16427", "pdf": "https://arxiv.org/pdf/2504.16427", "abs": "https://arxiv.org/abs/2504.16427", "authors": ["Hanlei Zhang", "Zhuohang Li", "Yeshuang Zhu", "Hua Xu", "Peiwu Wang", "Haige Zhu", "Jie Zhou", "Jinchao Zhang"], "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "23 pages, 5 figures", "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.", "AI": {"tldr": "The paper introduces MMLA, a benchmark for evaluating multimodal large language models (MLLMs) on cognitive-level semantics, revealing their limitations despite extensive testing.", "motivation": "To address the lack of research on MLLMs' ability to understand cognitive-level semantics in multimodal language analysis.", "method": "MMLA benchmark with 61K multimodal utterances, evaluating eight LLMs/MLLMs via zero-shot inference, supervised fine-tuning, and instruction tuning.", "result": "Fine-tuned models achieve only 60%~70% accuracy, highlighting MLLMs' limitations in complex human language understanding.", "conclusion": "MMLA provides a foundation for advancing MLLMs in multimodal language analysis, with open-sourced datasets and code."}}
{"id": "2504.17628", "pdf": "https://arxiv.org/pdf/2504.17628", "abs": "https://arxiv.org/abs/2504.17628", "authors": ["Abderrachid Hamrani", "Daniela Leizaola", "Renato Sousa", "Jose P. Ponce", "Stanley Mathis", "David G. Armstrong", "Anuradha Godavarty"], "title": "Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text-Guided Customization", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 8 figures, journal article", "summary": "Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,\nrequiring precise and efficient wound assessment to enhance patient outcomes.\nThis study introduces the Attention Diffusion Zero-shot Unsupervised System\n(ADZUS), a novel text-guided diffusion model that performs wound segmentation\nwithout relying on labeled training data. Unlike conventional deep learning\nmodels, which require extensive annotation, ADZUS leverages zero-shot learning\nto dynamically adapt segmentation based on descriptive prompts, offering\nenhanced flexibility and adaptability in clinical applications. Experimental\nevaluations demonstrate that ADZUS surpasses traditional and state-of-the-art\nsegmentation models, achieving an IoU of 86.68\\% and the highest precision of\n94.69\\% on the chronic wound dataset, outperforming supervised approaches such\nas FUSegNet. Further validation on a custom-curated DFU dataset reinforces its\nrobustness, with ADZUS achieving a median DSC of 75\\%, significantly surpassing\nFUSegNet's 45\\%. The model's text-guided segmentation capability enables\nreal-time customization of segmentation outputs, allowing targeted analysis of\nwound characteristics based on clinical descriptions. Despite its competitive\nperformance, the computational cost of diffusion-based inference and the need\nfor potential fine-tuning remain areas for future improvement. ADZUS represents\na transformative step in wound segmentation, providing a scalable, efficient,\nand adaptable AI-driven solution for medical imaging.", "AI": {"tldr": "ADZUS is a zero-shot, text-guided diffusion model for diabetic foot ulcer segmentation, outperforming supervised models without labeled data.", "motivation": "Diabetic foot ulcers require precise wound assessment; ADZUS addresses the need for flexible, annotation-free segmentation.", "method": "ADZUS uses zero-shot learning and text prompts for dynamic wound segmentation, avoiding labeled training data.", "result": "ADZUS achieves 86.68% IoU and 94.69% precision, surpassing supervised models like FUSegNet (45% DSC).", "conclusion": "ADZUS offers scalable, adaptable wound segmentation but needs optimization for computational cost and fine-tuning."}}
{"id": "2504.17163", "pdf": "https://arxiv.org/pdf/2504.17163", "abs": "https://arxiv.org/abs/2504.17163", "authors": ["Kai Cui", "Jia Li", "Yu Liu", "Xuesong Zhang", "Zhenzhen Hu", "Meng Wang"], "title": "PhysioSync: Temporal and Cross-Modal Contrastive Learning Inspired by Physiological Synchronization for EEG-Based Emotion Recognition", "categories": ["cs.CV"], "comment": "The source code will be publicly available at\n  https://github.com/MSA-LMC/PhysioSync", "summary": "Electroencephalography (EEG) signals provide a promising and involuntary\nreflection of brain activity related to emotional states, offering significant\nadvantages over behavioral cues like facial expressions. However, EEG signals\nare often noisy, affected by artifacts, and vary across individuals,\ncomplicating emotion recognition. While multimodal approaches have used\nPeripheral Physiological Signals (PPS) like GSR to complement EEG, they often\noverlook the dynamic synchronization and consistent semantics between the\nmodalities. Additionally, the temporal dynamics of emotional fluctuations\nacross different time resolutions in PPS remain underexplored. To address these\nchallenges, we propose PhysioSync, a novel pre-training framework leveraging\ntemporal and cross-modal contrastive learning, inspired by physiological\nsynchronization phenomena. PhysioSync incorporates Cross-Modal Consistency\nAlignment (CM-CA) to model dynamic relationships between EEG and complementary\nPPS, enabling emotion-related synchronizations across modalities. Besides, it\nintroduces Long- and Short-Term Temporal Contrastive Learning (LS-TCL) to\ncapture emotional synchronization at different temporal resolutions within\nmodalities. After pre-training, cross-resolution and cross-modal features are\nhierarchically fused and fine-tuned to enhance emotion recognition. Experiments\non DEAP and DREAMER datasets demonstrate PhysioSync's advanced performance\nunder uni-modal and cross-modal conditions, highlighting its effectiveness for\nEEG-centered emotion recognition.", "AI": {"tldr": "PhysioSync is a pre-training framework using contrastive learning to improve EEG-based emotion recognition by modeling cross-modal and temporal dynamics.", "motivation": "EEG signals are noisy and vary across individuals, while existing multimodal approaches overlook dynamic synchronization and temporal dynamics in Peripheral Physiological Signals (PPS).", "method": "Proposes PhysioSync with Cross-Modal Consistency Alignment (CM-CA) and Long- and Short-Term Temporal Contrastive Learning (LS-TCL) to capture synchronization across modalities and time resolutions.", "result": "Outperforms on DEAP and DREAMER datasets, showing effectiveness in EEG-centered emotion recognition.", "conclusion": "PhysioSync enhances emotion recognition by addressing cross-modal and temporal synchronization challenges."}}
{"id": "2504.17295", "pdf": "https://arxiv.org/pdf/2504.17295", "abs": "https://arxiv.org/abs/2504.17295", "authors": ["Shahrzad Khayatbashi", "Viktor Sj\u00f6lind", "Anders Gran\u00e5ker", "Amin Jalali"], "title": "AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations.", "AI": {"tldr": "The paper explores AI-driven automation in business processes using LLMs, demonstrated through an insurance case study, and evaluates its impact with OCPM.", "motivation": "To assess the impact of AI-driven automation on business processes, especially during transitions between traditional and AI-enhanced variants, using a real-world case study.", "method": "Applied Object-Centric Process Mining (OCPM) to analyze the deployment of an LLM in automating claim parts identification in the insurance sector.", "result": "LLMs significantly improve operational capacity but introduce new process dynamics needing refinement. OCPM proves useful but has limitations.", "conclusion": "AI-driven automation enhances scalability but requires process adjustments. OCPM is a practical tool for such evaluations, though further refinement is needed."}}
{"id": "2504.17065", "pdf": "https://arxiv.org/pdf/2504.17065", "abs": "https://arxiv.org/abs/2504.17065", "authors": ["Sahar Bagherkhani", "Jackson Christopher Earls", "Franco De Flaviis", "Pierre Baldi"], "title": "Antenna Near-Field Reconstruction from Far-Field Data Using Convolutional Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Electromagnetic field reconstruction is crucial in many applications,\nincluding antenna diagnostics, electromagnetic interference analysis, and\nsystem modeling. This paper presents a deep learning-based approach for\nFar-Field to Near-Field (FF-NF) transformation using Convolutional Neural\nNetworks (CNNs). The goal is to reconstruct near-field distributions from the\nfar-field data of an antenna without relying on explicit analytical\ntransformations. The CNNs are trained on paired far-field and near-field data\nand evaluated using mean squared error (MSE). The best model achieves a\ntraining error of 0.0199 and a test error of 0.3898. Moreover, visual\ncomparisons between the predicted and true near-field distributions demonstrate\nthe model's effectiveness in capturing complex electromagnetic field behavior,\nhighlighting the potential of deep learning in electromagnetic field\nreconstruction.", "AI": {"tldr": "A deep learning approach using CNNs for FF-NF transformation achieves accurate near-field reconstruction from far-field data.", "motivation": "Electromagnetic field reconstruction is vital for applications like antenna diagnostics and interference analysis, but traditional methods rely on explicit analytical transformations.", "method": "CNNs are trained on paired far-field and near-field data, evaluated using MSE.", "result": "Best model shows training error of 0.0199 and test error of 0.3898, with visual confirmation of accurate field behavior capture.", "conclusion": "Deep learning, particularly CNNs, is effective for electromagnetic field reconstruction, offering a promising alternative to analytical methods."}}
{"id": "2504.17220", "pdf": "https://arxiv.org/pdf/2504.17220", "abs": "https://arxiv.org/abs/2504.17220", "authors": ["Kaidong Feng", "Zhu Sun", "Jie Yang", "Hui Fang", "Xinghua Qu", "Wenyuan Liu"], "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation.", "AI": {"tldr": "The paper explores knowledge distillation (KD) for efficient bundle generation using LLMs, addressing format, quantity, and utilization of distilled knowledge.", "motivation": "Deploying large-scale LLMs for bundle generation is computationally expensive, prompting the need for efficient alternatives like KD.", "method": "A comprehensive KD framework is proposed, progressively extracting knowledge, varying quantities, and utilizing adaptation techniques.", "result": "Experiments show KD's potential to maintain performance while reducing computational costs in LLM-based bundle generation.", "conclusion": "KD significantly enhances efficiency in LLM-based bundle generation without compromising performance."}}
{"id": "2504.17698", "pdf": "https://arxiv.org/pdf/2504.17698", "abs": "https://arxiv.org/abs/2504.17698", "authors": ["Nikola Janju\u0161evi\u0107", "Jingjia Chen", "Luke Ginocchio", "Mary Bruno", "Yuhui Huang", "Yao Wang", "Hersh Chandarana", "Li Feng"], "title": "Self-Supervised Noise Adaptive MRI Denoising via Repetition to Repetition (Rep2Rep) Learning", "categories": ["eess.IV"], "comment": "13 pages, 9 figures, 1 table, supplementary information at end of\n  document", "summary": "Purpose: This work proposes a novel self-supervised noise-adaptive image\ndenoising framework, called Repetition to Repetition (Rep2Rep) learning, for\nlow-field (<1T) MRI applications. Methods: Rep2Rep learning extends the\nNoise2Noise framework by training a neural network on two repeated MRI\nacquisitions, using one repetition as input and another as target, without\nrequiring ground-truth data. It incorporates noise-adaptive training, enabling\ndenoising generalization across varying noise levels and flexible inference\nwith any number of repetitions. Performance was evaluated on both synthetic\nnoisy brain MRI and 0.55T prostate MRI data, and compared against supervised\nlearning and Monte Carlo Stein's Unbiased Risk Estimator (MC-SURE). Results:\nRep2Rep learning outperforms MC-SURE on both synthetic and 0.55T MRI datasets.\nOn synthetic brain data, it achieved denoising quality comparable to supervised\nlearning and surpassed MC-SURE, particularly in preserving structural details\nand reducing residual noise. On the 0.55T prostate MRI dataset, a reader study\nshowed radiologists preferred Rep2Rep-denoised 2-average images over 8-average\nnoisy images. Rep2Rep demonstrated robustness to noise-level discrepancies\nbetween training and inference, supporting its practical implementation.\nConclusion: Rep2Rep learning offers an effective self-supervised denoising for\nlow-field MRI by leveraging routinely acquired multi-repetition data. Its\nnoise-adaptivity enables generalization to different SNR regimes without clean\nreference images. This makes Rep2Rep learning a promising tool for improving\nimage quality and scan efficiency in low-field MRI.", "AI": {"tldr": "Rep2Rep is a self-supervised noise-adaptive image denoising framework for low-field MRI, outperforming MC-SURE and matching supervised learning in quality.", "motivation": "To improve denoising for low-field MRI without requiring ground-truth data, leveraging multi-repetition acquisitions.", "method": "Extends Noise2Noise by training on repeated MRI acquisitions, incorporating noise-adaptive training for generalization across noise levels.", "result": "Outperforms MC-SURE on synthetic and 0.55T MRI data, matches supervised learning, and is preferred by radiologists over noisy images.", "conclusion": "Rep2Rep is effective for low-field MRI denoising, offering noise-adaptivity and practical implementation without clean reference images."}}
{"id": "2504.17177", "pdf": "https://arxiv.org/pdf/2504.17177", "abs": "https://arxiv.org/abs/2504.17177", "authors": ["Kevin Lane", "Morteza Karimzadeh"], "title": "A Genealogy of Multi-Sensor Foundation Models in Remote Sensing", "categories": ["cs.CV", "cs.LG", "I.4.7; I.4.8"], "comment": "20 pages, submitted to ACM SigSpatial, currently under peer review", "summary": "Foundation models have garnered increasing attention for representation\nlearning in remote sensing, primarily adopting approaches that have\ndemonstrated success in computer vision with minimal domain-specific\nmodification. However, the development and application of foundation models in\nthis field are still burgeoning, as there are a variety of competing approaches\nthat each come with significant benefits and drawbacks. This paper examines\nthese approaches along with their roots in the computer vision field in order\nto characterize potential advantages and pitfalls while outlining future\ndirections to further improve remote sensing-specific foundation models. We\ndiscuss the quality of the learned representations and methods to alleviate the\nneed for massive compute resources. We place emphasis on the multi-sensor\naspect of Earth observations, and the extent to which existing approaches\nleverage multiple sensors in training foundation models in relation to\nmulti-modal foundation models. Finally, we identify opportunities for further\nharnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote\nsensing observations.", "AI": {"tldr": "The paper reviews foundation models in remote sensing, comparing approaches, highlighting benefits and drawbacks, and suggesting future improvements for domain-specific models.", "motivation": "To address the growing interest in foundation models for remote sensing and evaluate their adaptation from computer vision, identifying advantages, challenges, and opportunities for improvement.", "method": "Examines existing approaches, their computer vision roots, representation quality, resource efficiency, and multi-sensor utilization.", "result": "Identifies gaps in leveraging multi-sensor data and suggests ways to improve remote sensing-specific foundation models.", "conclusion": "Future work should focus on better utilizing unlabeled, seasonal, and multi-sensor data to enhance foundation models in remote sensing."}}
{"id": "2504.17356", "pdf": "https://arxiv.org/pdf/2504.17356", "abs": "https://arxiv.org/abs/2504.17356", "authors": ["Weiliang Zhang", "Xiaohan Huang", "Yi Du", "Ziyue Qiao", "Qingqing Long", "Zhen Meng", "Yuanchun Zhou", "Meng Xiao"], "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection", "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.", "AI": {"tldr": "HRLFS is a novel feature selection method using hierarchical RL and LLM-based hybrid state extraction to improve efficiency and performance.", "motivation": "Current RL-based feature selection methods struggle with complex datasets due to inefficiencies like one-agent-per-feature paradigms.", "method": "Uses LLM-based hybrid state extraction to cluster features, then hierarchical RL agents for each cluster/sub-cluster.", "result": "HRLFS outperforms contemporary methods, improving ML performance and reducing runtime.", "conclusion": "HRLFS offers a scalable, robust solution for feature selection in complex datasets."}}
{"id": "2504.17066", "pdf": "https://arxiv.org/pdf/2504.17066", "abs": "https://arxiv.org/abs/2504.17066", "authors": ["Kewen Peng", "Yicheng Yang", "Hao Zhuo", "Tim Menzies"], "title": "Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching", "categories": ["cs.LG", "cs.CY", "cs.SE", "stat.ML"], "comment": null, "summary": "Fairness-aware learning aims to mitigate discrimination against specific\nprotected social groups (e.g., those categorized by gender, ethnicity, age)\nwhile minimizing predictive performance loss. Despite efforts to improve\nfairness in machine learning, prior studies have shown that many models remain\nunfair when measured against various fairness metrics. In this paper, we\nexamine whether the way training and testing data are sampled affects the\nreliability of reported fairness metrics. Since training and test sets are\noften randomly sampled from the same population, bias present in the training\ndata may still exist in the test data, potentially skewing fairness\nassessments. To address this, we propose FairMatch, a post-processing method\nthat applies propensity score matching to evaluate and mitigate bias. FairMatch\nidentifies control and treatment pairs with similar propensity scores in the\ntest set and adjusts decision thresholds for different subgroups accordingly.\nFor samples that cannot be matched, we perform probabilistic calibration using\nfairness-aware loss functions. Experimental results demonstrate that our\napproach can (a) precisely locate subsets of the test data where the model is\nunbiased, and (b) significantly reduce bias on the remaining data. Overall,\npropensity score matching offers a principled way to improve both fairness\nevaluation and mitigation, without sacrificing predictive performance.", "AI": {"tldr": "FairMatch uses propensity score matching to improve fairness evaluation and mitigation without losing predictive performance.", "motivation": "Existing fairness-aware learning models often remain unfair due to biased training and test data sampling.", "method": "Proposes FairMatch, a post-processing method using propensity score matching and probabilistic calibration for unmatched samples.", "result": "Identifies unbiased subsets in test data and reduces bias significantly on the rest.", "conclusion": "Propensity score matching enhances fairness evaluation and mitigation effectively."}}
{"id": "2504.17238", "pdf": "https://arxiv.org/pdf/2504.17238", "abs": "https://arxiv.org/abs/2504.17238", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Yongkang Huang", "Yihan Shi", "Xikun Zhang", "Libiao Peng", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "Hongning Wang", "Minlie Huang"], "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations.", "AI": {"tldr": "CRDial is a framework for Cognitive Restructuring (CR) via multi-turn dialogues, addressing gaps in existing methods by integrating identification, restructuring stages, and supportive strategies, validated by human studies.", "motivation": "Clinician shortage and stigma drive the need for human-LLM interactive psychotherapy for CR, but current methods lack alignment with the psychotherapeutic process.", "method": "CRDial introduces multi-turn dialogues with specific stages, supportive strategies, and a multi-channel loop mechanism, creating the Crisp dataset and training Crispers (7B/14B LLMs).", "result": "Human studies demonstrate Crispers' superiority in pointwise, pairwise, and intervention evaluations.", "conclusion": "CRDial and Crispers effectively address the limitations of existing CR methods, offering a scalable solution for interactive psychotherapy."}}
{"id": "2504.16960", "pdf": "https://arxiv.org/pdf/2504.16960", "abs": "https://arxiv.org/abs/2504.16960", "authors": ["Weixuan Chen", "Qianqian Yang", "Shuo Shao", "Zhiguo Shi", "Jiming Chen", "Xuemin", "Shen"], "title": "A Coding-Enhanced Jamming Approach for Secure Semantic Communication over Wiretap Channels", "categories": ["cs.IT", "eess.IV", "math.IT"], "comment": null, "summary": "As semantic communication (SemCom) gains increasing attention as a novel\ncommunication paradigm, ensuring the security of transmitted semantic\ninformation over open wireless channels becomes crucial. Existing secure SemCom\nsolutions often lack explicit control over security. To address this, we\npropose a coding-enhanced jamming approach for secure SemCom over wiretap\nchannels. This approach integrates deep joint source and channel coding\n(DeepJSCC) with neural network-based digital modulation, enabling controlled\njamming through two-layer superposition coding. The outer constellation\nsequence encodes the source image, while the inner constellation sequence,\nderived from a secret image, acts as the jamming signal. By minimizing the\nmutual information between the outer and inner constellation sequences, the\njamming effect is enhanced. The jamming signal is superposed on the outer\nconstellation sequence, preventing the eavesdropper from recovering the source\nimage. The power allocation coefficient (PAC) in the superposition coding can\nbe adjusted to control system security. Experiments show that our approach\nmatches existing methods in security while significantly improving\nreconstruction performance across varying channel signal-to-noise ratios (SNRs)\nand compression ratios.", "AI": {"tldr": "Proposes a coding-enhanced jamming approach for secure semantic communication, combining DeepJSCC and neural modulation to control security via superposition coding.", "motivation": "Existing secure semantic communication lacks explicit security control, necessitating a method to enhance security while maintaining performance.", "method": "Integrates DeepJSCC with neural modulation, using two-layer superposition coding: outer for source image, inner for jamming. Adjusts power allocation to control security.", "result": "Matches security of existing methods while improving reconstruction performance across varying SNRs and compression ratios.", "conclusion": "The approach effectively balances security and performance in semantic communication over wiretap channels."}}
{"id": "2504.17180", "pdf": "https://arxiv.org/pdf/2504.17180", "abs": "https://arxiv.org/abs/2504.17180", "authors": ["Minkyu Choi", "S P Sharan", "Harsh Goel", "Sahil Shah", "Sandeep Chinchali"], "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce \\(\\projectname\\), a novel zero-training video\nrefinement pipeline that leverages neuro-symbolic feedback to automatically\nenhance video generation, achieving superior alignment with the prompts. Our\napproach first derives the neuro-symbolic feedback by analyzing a formal video\nrepresentation and pinpoints semantically inconsistent events, objects, and\ntheir corresponding frames. This feedback then guides targeted edits to the\noriginal video. Extensive empirical evaluations on both open-source and\nproprietary T2V models demonstrate that \\(\\projectname\\) significantly enhances\ntemporal and logical alignment across diverse prompts by almost $40\\%$.", "AI": {"tldr": "A zero-training video refinement pipeline, Projectname, uses neuro-symbolic feedback to improve text-to-video generation, enhancing alignment with prompts by 40%.", "motivation": "Existing T2V models struggle with semantic and temporal consistency for complex prompts and face high computational costs for training.", "method": "Projectname analyzes formal video representations to identify inconsistencies, then uses neuro-symbolic feedback to guide targeted edits.", "result": "Empirical evaluations show a 40% improvement in temporal and logical alignment across diverse prompts.", "conclusion": "Projectname effectively enhances T2V generation without additional training, addressing key limitations of current models."}}
{"id": "2504.17402", "pdf": "https://arxiv.org/pdf/2504.17402", "abs": "https://arxiv.org/abs/2504.17402", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskisarkka", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques.", "AI": {"tldr": "LLMs like DeepSeek and o1-preview show consistent performance in domain-specific ontology generation, indicating their potential for scalable, domain-agnostic ontology engineering.", "motivation": "To explore the applicability of LLMs in domain-specific ontology generation and evaluate their generalizability.", "method": "Evaluated two LLMs (DeepSeek and o1-preview) using competency questions (CQs) and user stories across six domains.", "result": "Both LLMs performed consistently across domains, demonstrating generalizability in ontology generation.", "conclusion": "LLMs hold promise for scalable, domain-agnostic ontology construction, paving the way for further research in automated reasoning."}}
{"id": "2504.17068", "pdf": "https://arxiv.org/pdf/2504.17068", "abs": "https://arxiv.org/abs/2504.17068", "authors": ["Pranav Kantroo", "G\u00fcnter P. Wagner", "Benjamin B. Machta"], "title": "In-Context Learning can distort the relationship between sequence likelihoods and biological fitness", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Language models have emerged as powerful predictors of the viability of\nbiological sequences. During training these models learn the rules of the\ngrammar obeyed by sequences of amino acids or nucleotides. Once trained, these\nmodels can take a sequence as input and produce a likelihood score as an\noutput; a higher likelihood implies adherence to the learned grammar and\ncorrelates with experimental fitness measurements. Here we show that in-context\nlearning can distort the relationship between fitness and likelihood scores of\nsequences. This phenomenon most prominently manifests as anomalously high\nlikelihood scores for sequences that contain repeated motifs. We use protein\nlanguage models with different architectures trained on the masked language\nmodeling objective for our experiments, and find transformer-based models to be\nparticularly vulnerable to this effect. This behavior is mediated by a look-up\noperation where the model seeks the identity of the masked position by using\nthe other copy of the repeated motif as a reference. This retrieval behavior\ncan override the model's learned priors. This phenomenon persists for\nimperfectly repeated sequences, and extends to other kinds of biologically\nrelevant features such as reversed complement motifs in RNA sequences that fold\ninto hairpin structures.", "AI": {"tldr": "Language models can distort fitness-likelihood relationships due to in-context learning, especially with repeated motifs, affecting transformer-based models.", "motivation": "To investigate how in-context learning in language models distorts the correlation between sequence likelihood scores and experimental fitness, particularly with repeated motifs.", "method": "Using protein language models (especially transformer-based) trained on masked language modeling, analyzing sequences with repeated motifs and other biological features.", "result": "Transformer models show anomalously high likelihood scores for repeated motifs due to a look-up behavior, overriding learned priors. This effect extends to imperfect repeats and other biological features like RNA hairpins.", "conclusion": "In-context learning in language models can disrupt fitness-likelihood relationships, highlighting a vulnerability in transformer architectures for biological sequence analysis."}}
{"id": "2504.17252", "pdf": "https://arxiv.org/pdf/2504.17252", "abs": "https://arxiv.org/abs/2504.17252", "authors": ["Ocheme Anthony Ekle", "Biswarup Das"], "title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "comment": "25 pages, 14 combined figures (19 total), includes horizontal\n  layouts. Submitted to arXiv for open access", "summary": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks.", "AI": {"tldr": "The paper develops NMT and Transformer-based models for English-to-Igbo translation, using RNNs and transfer learning to achieve a 70% accuracy with a +4.83 BLEU improvement.", "motivation": "Addressing the performance gap in low-resource language translation, specifically for Igbo, spoken by over 40 million people.", "method": "Uses RNN architectures (LSTM, GRU) with attention mechanisms and transfer learning via MarianNMT pre-trained models.", "result": "RNN-based system matches benchmarks; transfer learning adds +4.83 BLEU, reaching 70% accuracy.", "conclusion": "Combining RNNs with transfer learning effectively improves low-resource language translation."}}
{"id": "2504.17540", "pdf": "https://arxiv.org/pdf/2504.17540", "abs": "https://arxiv.org/abs/2504.17540", "authors": ["Ahmadreza Shateri", "Negar Nourani", "Morteza Dorrigiv", "Hamid Nasiri"], "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments.", "AI": {"tldr": "A deep learning framework for automated monkeypox detection from skin lesions, achieving high accuracy (97.53%) using Xception, PCA, and NGBoost, optimized with AVOA.", "motivation": "Addressing the need for early and accurate monkeypox diagnosis due to its recent global spread, especially in non-endemic regions.", "method": "Uses Xception for feature extraction, PCA for dimensionality reduction, NGBoost for classification, and AVOA for hyperparameter tuning. Evaluated on the MSLD dataset.", "result": "Achieves 97.53% accuracy, 97.72% F1-score, and 97.47% AUC, with interpretability via Grad-CAM and LIME.", "conclusion": "The framework provides a precise, efficient diagnostic tool for early monkeypox detection, beneficial in resource-limited settings."}}
{"id": "2504.17207", "pdf": "https://arxiv.org/pdf/2504.17207", "abs": "https://arxiv.org/abs/2504.17207", "authors": ["Phillip Y. Lee", "Jihyeon Je", "Chanho Park", "Mikaela Angelina Uy", "Leonidas Guibas", "Minhyuk Sung"], "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation", "categories": ["cs.CV"], "comment": "Project Page: https://apc-vlm.github.io/", "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.", "AI": {"tldr": "A framework for perspective-aware reasoning in VLMs using mental imagery simulation, improving human-like perception.", "motivation": "Addressing the lack of perspective-aware reasoning in VLMs and their egocentric bias by mimicking human mental imagery.", "method": "Proposes Abstract Perspective Change (APC), leveraging vision foundation models for scene abstraction and perspective shifts.", "result": "Significant improvements in perspective-aware reasoning, outperforming fine-tuned models and novel-view-synthesis approaches.", "conclusion": "APC effectively bridges the gap between VLMs and human perception, enhancing perspective-aware reasoning."}}
{"id": "2504.17404", "pdf": "https://arxiv.org/pdf/2504.17404", "abs": "https://arxiv.org/abs/2504.17404", "authors": ["Feifei Zhao", "Yuwei Wang", "Enmeng Lu", "Dongcheng Zhao", "Bing Han", "Haibo Tong", "Yao Liang", "Dongqi Liang", "Kang Sun", "Lei Wang", "Yitao Liang", "Chao Liu", "Yaodong Yang", "Yi Zeng"], "title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society", "categories": ["cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology.", "AI": {"tldr": "The paper addresses the challenge of ensuring superalignment in AI systems to prevent catastrophic outcomes, proposing a dual framework of external oversight and intrinsic proactive alignment for human-AI co-alignment.", "motivation": "The progression from AI to ASI poses risks of loss of control and misalignment with human values, necessitating safer frameworks for superalignment.", "method": "The paper redefines superalignment as human-AI co-alignment, integrating external oversight (human-centered decisions, automated evaluation) and intrinsic proactive alignment (self-awareness, empathy).", "result": "The proposed framework aims to achieve sustainable symbiotic societies by combining external and intrinsic alignment methods.", "conclusion": "The integration of oversight and proactive alignment paves the way for safe and beneficial AGI and ASI, aligning AI with human values and well-being."}}
{"id": "2504.17073", "pdf": "https://arxiv.org/pdf/2504.17073", "abs": "https://arxiv.org/abs/2504.17073", "authors": ["David Lu", "Lior Maman", "Jackson Earls", "Amir Boag", "Pierre Baldi"], "title": "Sparse Phased Array Optimization Using Deep Learning", "categories": ["cs.LG"], "comment": null, "summary": "Antenna arrays are widely used in wireless communication, radar systems,\nradio astronomy, and military defense to enhance signal strength, directivity,\nand interference suppression. We introduce a deep learning-based optimization\napproach that enhances the design of sparse phased arrays by reducing grating\nlobes. This approach begins by generating sparse array configurations to\naddress the non-convex challenges and extensive degrees of freedom inherent in\narray design. We use neural networks to approximate the non-convex cost\nfunction that estimates the energy ratio between the main and side lobes. This\ndifferentiable approximation facilitates cost function minimization through\ngradient descent, optimizing the antenna elements' coordinates and leading to\nan improved layout. Additionally, we incorporate a tailored penalty mechanism\nthat includes various physical and design constraints into the optimization\nprocess, enhancing its robustness and practical applicability. We demonstrate\nthe effectiveness of our method by applying it to the ten array configurations\nwith the lowest initial costs, achieving further cost reductions ranging from\n411% to 643%, with an impressive average improvement of 552%. By significantly\nreducing side lobe levels in antenna arrays, this breakthrough paves the way\nfor ultra-precise beamforming, enhanced interference mitigation, and\nnext-generation wireless and radar systems with unprecedented efficiency and\nclarity.", "AI": {"tldr": "A deep learning-based method optimizes sparse phased array designs to reduce grating lobes, improving signal strength and interference suppression.", "motivation": "Enhancing antenna array performance by addressing non-convex design challenges and reducing grating lobes for better beamforming and interference mitigation.", "method": "Uses neural networks to approximate a non-convex cost function, enabling gradient descent optimization of antenna element coordinates. Incorporates physical constraints via a tailored penalty mechanism.", "result": "Achieved cost reductions of 411% to 643% (average 552%) in ten array configurations, significantly lowering side lobe levels.", "conclusion": "The method enables ultra-precise beamforming and next-generation wireless/radar systems with improved efficiency and clarity."}}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264", "abs": "https://arxiv.org/abs/2504.17264", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively.", "AI": {"tldr": "JurisCTC is a novel model for Legal Judgment Prediction (LJP) that uses contrastive learning to transfer knowledge between civil and criminal law domains, achieving high accuracy.", "motivation": "Addressing the lack of exploration in applying Unsupervised Domain Adaptation (UDA) to legal domains and overcoming challenges like complex legal texts and limited annotated datasets.", "method": "Proposes JurisCTC, which employs contrastive learning to distinguish samples from different legal domains and facilitates knowledge transfer.", "result": "Achieves peak accuracies of 76.59% and 78.83% in civil and criminal law domains, outperforming other models and LLMs.", "conclusion": "JurisCTC effectively improves LJP task accuracy by enabling cross-domain knowledge transfer in legal contexts."}}
{"id": "2402.14974", "pdf": "https://arxiv.org/pdf/2402.14974", "abs": "https://arxiv.org/abs/2402.14974", "authors": ["Majid Farhadloo", "Arun Sharma", "Jayant Gupta", "Alexey Leontovich", "Svetomir N. Markovic", "Shashi Shekhar"], "title": "Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data", "categories": ["eess.IV", "cs.AI", "cs.LG"], "comment": "SIAM International Conference on Data Mining (SDM24)", "summary": "Given multi-category point sets from different place-types, our goal is to\ndevelop a spatially-lucid classifier that can distinguish between two classes\nbased on the arrangements of their points. This problem is important for many\napplications, such as oncology, for analyzing immune-tumor relationships and\ndesigning new immunotherapies. It is challenging due to spatial variability and\ninterpretability needs. Previously proposed techniques require dense training\ndata or have limited ability to handle significant spatial variability within a\nsingle place-type. Most importantly, these deep neural network (DNN) approaches\nare not designed to work in non-Euclidean space, particularly point sets.\nExisting non-Euclidean DNN methods are limited to one-size-fits-all approaches.\nWe explore a spatial ensemble framework that explicitly uses different training\nstrategies, including weighted-distance learning rate and spatial domain\nadaptation, on various place-types for spatially-lucid classification.\nExperimental results on real-world datasets (e.g., MxIF oncology data) show\nthat the proposed framework provides higher prediction accuracy than baseline\nmethods.", "AI": {"tldr": "A spatial ensemble framework is proposed for classifying multi-category point sets, outperforming baselines in accuracy.", "motivation": "The need for a classifier to distinguish point set arrangements, especially in oncology for immune-tumor analysis, drives this work. Challenges include spatial variability and interpretability.", "method": "The framework uses weighted-distance learning rate and spatial domain adaptation across place-types for classification.", "result": "Experiments on real-world datasets (e.g., MxIF oncology data) show higher accuracy than baseline methods.", "conclusion": "The proposed spatial ensemble framework effectively addresses spatial variability and improves classification accuracy."}}
{"id": "2504.17213", "pdf": "https://arxiv.org/pdf/2504.17213", "abs": "https://arxiv.org/abs/2504.17213", "authors": ["Shiwen Cao", "Zhaoxing Zhang", "Junming Jiao", "Juyi Qiao", "Guowen Song", "Rong Shen"], "title": "MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Even in the era of rapid advances in large models, video understanding,\nparticularly long videos, remains highly challenging. Compared with textual or\nimage-based information, videos commonly contain more information with\nredundancy, requiring large models to strategically allocate attention at a\nglobal level for accurate comprehension. To address this, we propose MCAF, an\nagent-based, training-free framework perform video understanding through\nMultimodal Coarse-to-fine Attention Focusing. The key innovation lies in its\nability to sense and prioritize segments of the video that are highly relevant\nto the understanding task. First, MCAF hierarchically concentrates on highly\nrelevant frames through multimodal information, enhancing the correlation\nbetween the acquired contextual information and the query. Second, it employs a\ndilated temporal expansion mechanism to mitigate the risk of missing crucial\ndetails when extracting information from these concentrated frames. In\naddition, our framework incorporates a self-reflection mechanism utilizing the\nconfidence level of the model's responses as feedback. By iteratively applying\nthese two creative focusing strategies, it adaptively adjusts attention to\ncapture highly query-connected context and thus improves response accuracy.\nMCAF outperforms comparable state-of-the-art methods on average. On the\nEgoSchema dataset, it achieves a remarkable 5% performance gain over the\nleading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms\nthe current state-of-the-art standard by 0.2% and 0.3% respectively. On the\nVideo-MME dataset, which features videos averaging nearly an hour in length,\nMCAF also outperforms other agent-based methods.", "AI": {"tldr": "MCAF is a training-free, agent-based framework for video understanding using multimodal coarse-to-fine attention focusing, outperforming state-of-the-art methods.", "motivation": "Video understanding, especially for long videos, is challenging due to information redundancy. MCAF aims to strategically allocate attention for accurate comprehension.", "method": "MCAF hierarchically focuses on relevant frames using multimodal information and employs a dilated temporal expansion mechanism. It also uses a self-reflection mechanism for iterative improvement.", "result": "MCAF achieves a 5% gain on EgoSchema and outperforms state-of-the-art methods on Next-QA, IntentQA, and Video-MME datasets.", "conclusion": "MCAF effectively improves video understanding by adaptively adjusting attention and outperforms existing methods."}}
{"id": "2504.17531", "pdf": "https://arxiv.org/pdf/2504.17531", "abs": "https://arxiv.org/abs/2504.17531", "authors": ["Justus Flerlage", "Ilja Behnke", "Odej Kao"], "title": "Towards Machine-Generated Code for the Resolution of User Intentions", "categories": ["cs.AI"], "comment": null, "summary": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.", "AI": {"tldr": "The paper explores using LLMs to generate and execute workflows from user intentions, showing feasibility with GPT-4o-mini.", "motivation": "Reassess user-device interaction by leveraging AI to automate workflows through code generation, reducing reliance on high-level applications.", "method": "Prompt an LLM (GPT-4o-mini) with user intentions and a simplified API to generate and execute code-oriented workflows.", "result": "Demonstrates feasibility; GPT-4o-mini excels in generating workflows aligned with user intentions.", "conclusion": "AI-driven workflow generation is viable, marking progress in hybrid human-AI collaboration for intent resolution."}}
{"id": "2504.17074", "pdf": "https://arxiv.org/pdf/2504.17074", "abs": "https://arxiv.org/abs/2504.17074", "authors": ["William R. Keely", "Otto Lamminp\u00e4\u00e4", "Steffen Mauceri", "Sean M. R. Crowell", "Christopher W. O'Dell", "Gregory R. McGarragh"], "title": "Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy", "categories": ["cs.LG", "astro-ph.IM"], "comment": "Published as a workshop paper in \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025. https://www.climatechange.ai/papers/iclr2025/12", "summary": "Satellite-based estimates of greenhouse gas (GHG) properties from\nobservations of reflected solar spectra are integral for understanding and\nmonitoring complex terrestrial systems and their impact on the carbon cycle due\nto their near global coverage. Known as retrieval, making GHG concentration\nestimations from these observations is a non-linear Bayesian inverse problem,\nwhich is operationally solved using a computationally expensive algorithm\ncalled Optimal Estimation (OE), providing a Gaussian approximation to a\nnon-Gaussian posterior. This leads to issues in solver algorithm convergence,\nand to unrealistically confident uncertainty estimates for the retrieved\nquantities. Upcoming satellite missions will provide orders of magnitude more\ndata than the current constellation of GHG observers. Development of fast and\naccurate retrieval algorithms with robust uncertainty quantification is\ncritical. Doing so stands to provide substantial climate impact of moving\ntowards the goal of near continuous real-time global monitoring of carbon\nsources and sinks which is essential for policy making. To achieve this goal,\nwe propose a diffusion-based approach to flexibly retrieve a Gaussian or\nnon-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,\nwhile providing a substantial computational speed-up over the current\noperational state-of-the-art.", "AI": {"tldr": "The paper proposes a diffusion-based method for faster and more accurate greenhouse gas (GHG) retrieval from satellite data, addressing limitations of current Optimal Estimation (OE) methods.", "motivation": "Current OE methods for GHG retrieval are computationally expensive, converge poorly, and provide unrealistic uncertainty estimates. Upcoming missions will generate more data, necessitating faster, robust algorithms for real-time global monitoring.", "method": "A diffusion-based approach is introduced to retrieve Gaussian or non-Gaussian posteriors for GHG concentrations, tested on NASA's Orbiting Carbon Observatory-2 spectrometer.", "result": "The proposed method offers a computational speed-up over OE while maintaining accuracy and improving uncertainty quantification.", "conclusion": "The diffusion-based approach advances GHG retrieval, supporting real-time global carbon monitoring for climate policy."}}
{"id": "2504.17279", "pdf": "https://arxiv.org/pdf/2504.17279", "abs": "https://arxiv.org/abs/2504.17279", "authors": ["Xiuying Chen", "Tairan Wang", "Juexiao Zhou", "Zirui Song", "Xin Gao", "Xiangliang Zhang"], "title": "Evaluating and Mitigating Bias in AI-Based Medical Text Generation", "categories": ["cs.CL"], "comment": "12 pages, 8 figures, published in Nature Computational Science", "summary": "Artificial intelligence (AI) systems, particularly those based on deep\nlearning models, have increasingly achieved expert-level performance in medical\napplications. However, there is growing concern that such AI systems may\nreflect and amplify human bias, and reduce the quality of their performance in\nhistorically under-served populations. The fairness issue has attracted\nconsiderable research interest in the medical imaging classification field, yet\nit remains understudied in the text generation domain. In this study, we\ninvestigate the fairness problem in text generation within the medical field\nand observe significant performance discrepancies across different races,\nsexes, and age groups, including intersectional groups, various model scales,\nand different evaluation metrics. To mitigate this fairness issue, we propose\nan algorithm that selectively optimizes those underperformed groups to reduce\nbias. The selection rules take into account not only word-level accuracy but\nalso the pathology accuracy to the target reference, while ensuring that the\nentire process remains fully differentiable for effective model training. Our\nevaluations across multiple backbones, datasets, and modalities demonstrate\nthat our proposed algorithm enhances fairness in text generation without\ncompromising overall performance. Specifically, the disparities among various\ngroups across different metrics were diminished by more than 30% with our\nalgorithm, while the relative change in text generation accuracy was typically\nwithin 2%. By reducing the bias generated by deep learning models, our proposed\napproach can potentially alleviate concerns about the fairness and reliability\nof text generation diagnosis in medical domain.\n  Our code is publicly available to facilitate further research at\nhttps://github.com/iriscxy/GenFair.", "AI": {"tldr": "The paper addresses fairness issues in AI-generated medical text, proposing an algorithm to reduce bias across demographic groups without compromising overall performance.", "motivation": "AI systems in medicine may reflect human biases, especially in text generation, which is understudied compared to medical imaging. This study aims to tackle fairness in medical text generation.", "method": "An algorithm selectively optimizes underperforming groups by considering word-level and pathology accuracy, ensuring differentiability for training. Evaluated across multiple models and datasets.", "result": "The algorithm reduced group disparities by over 30% with minimal impact (\u22642%) on overall text generation accuracy.", "conclusion": "The proposed method effectively mitigates bias in medical text generation, enhancing fairness and reliability without sacrificing performance."}}
{"id": "2411.00617", "pdf": "https://arxiv.org/pdf/2411.00617", "abs": "https://arxiv.org/abs/2411.00617", "authors": ["Xiaotong Zhang", "Alexander Broersen", "Gonnie CM van Erp", "Silvia L. Pintea", "Jouke Dijkstra"], "title": "Continuous and complete liver vessel segmentation with graph-attention guided diffusion", "categories": ["eess.IV", "cs.CV"], "comment": "Second version", "summary": "Improving connectivity and completeness are the most challenging aspects of\nliver vessel segmentation, especially for small vessels. These challenges\nrequire both learning the continuous vessel geometry and focusing on small\nvessel detection. However, current methods do not explicitly address these two\naspects and cannot generalize well when constrained by inconsistent\nannotations. Here, we take advantage of the generalization of the diffusion\nmodel and explicitly integrate connectivity and completeness in our\ndiffusion-based segmentation model. Specifically, we use a graph-attention\nmodule that adds knowledge about vessel geometry. Additionally, we perform the\ngraph-attention at multiple-scales, thus focusing on small liver vessels. Our\nmethod outperforms five state-of-the-art medical segmentation methods on two\npublic datasets: 3D-ircadb-01 and LiVS.", "AI": {"tldr": "A diffusion-based segmentation model improves liver vessel segmentation by integrating connectivity and completeness, outperforming state-of-the-art methods.", "motivation": "Addressing the challenges of connectivity and completeness in liver vessel segmentation, especially for small vessels, which current methods fail to explicitly tackle.", "method": "Uses a diffusion model with a graph-attention module for vessel geometry and multi-scale attention for small vessels.", "result": "Outperforms five state-of-the-art methods on 3D-ircadb-01 and LiVS datasets.", "conclusion": "The proposed diffusion-based model effectively improves liver vessel segmentation by focusing on connectivity and small vessel detection."}}
{"id": "2504.17223", "pdf": "https://arxiv.org/pdf/2504.17223", "abs": "https://arxiv.org/abs/2504.17223", "authors": ["Mengyu Qiao", "Runze Tian", "Yang Wang"], "title": "Towards Generalizable Deepfake Detection with Spatial-Frequency Collaborative Learning and Hierarchical Cross-Modal Fusion", "categories": ["cs.CV"], "comment": null, "summary": "The rapid evolution of deep generative models poses a critical challenge to\ndeepfake detection, as detectors trained on forgery-specific artifacts often\nsuffer significant performance degradation when encountering unseen forgeries.\nWhile existing methods predominantly rely on spatial domain analysis, frequency\ndomain operations are primarily limited to feature-level augmentation, leaving\nfrequency-native artifacts and spatial-frequency interactions insufficiently\nexploited. To address this limitation, we propose a novel detection framework\nthat integrates multi-scale spatial-frequency analysis for universal deepfake\ndetection. Our framework comprises three key components: (1) a local spectral\nfeature extraction pipeline that combines block-wise discrete cosine transform\nwith cascaded multi-scale convolutions to capture subtle spectral artifacts;\n(2) a global spectral feature extraction pipeline utilizing scale-invariant\ndifferential accumulation to identify holistic forgery distribution patterns;\nand (3) a multi-stage cross-modal fusion mechanism that incorporates\nshallow-layer attention enhancement and deep-layer dynamic modulation to model\nspatial-frequency interactions. Extensive evaluations on widely adopted\nbenchmarks demonstrate that our method outperforms state-of-the-art deepfake\ndetection methods in both accuracy and generalizability.", "AI": {"tldr": "A novel deepfake detection framework using multi-scale spatial-frequency analysis to improve accuracy and generalizability by exploiting frequency-native artifacts and spatial-frequency interactions.", "motivation": "Existing deepfake detectors fail against unseen forgeries due to reliance on spatial domain analysis and underutilization of frequency-native artifacts and spatial-frequency interactions.", "method": "Proposes a framework with local and global spectral feature extraction pipelines and a multi-stage cross-modal fusion mechanism to model spatial-frequency interactions.", "result": "Outperforms state-of-the-art deepfake detection methods in accuracy and generalizability on widely adopted benchmarks.", "conclusion": "The framework effectively addresses limitations of existing methods by leveraging spatial-frequency analysis for universal deepfake detection."}}
{"id": "2504.17544", "pdf": "https://arxiv.org/pdf/2504.17544", "abs": "https://arxiv.org/abs/2504.17544", "authors": ["W. Russell Neuman", "Chad Coleman", "Ali Dasdan", "Safinah Ali", "Manan Shah"], "title": "Auditing the Ethical Logic of Generative AI Models", "categories": ["cs.AI"], "comment": null, "summary": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts.", "AI": {"tldr": "A five-dimensional audit model evaluates ethical reasoning in LLMs, revealing variability in explanatory rigor and moral prioritization, with Chain-of-Thought prompting improving performance.", "motivation": "To address the need for robust ethical evaluation of generative AI models in high-stakes domains.", "method": "A multi-battery prompt approach, including ethical dilemmas, to assess LLMs across five dimensions: Analytic Quality, Breadth of Ethical Considerations, Depth of Explanation, Consistency, and Decisiveness.", "result": "Benchmarking seven LLMs showed convergence on ethical decisions but variability in explanatory rigor and moral prioritization. Chain-of-Thought prompting improved performance.", "conclusion": "The study presents a scalable method for ethical AI benchmarking and suggests AI's potential to complement human moral reasoning."}}
{"id": "2504.17079", "pdf": "https://arxiv.org/pdf/2504.17079", "abs": "https://arxiv.org/abs/2504.17079", "authors": ["Esam Mahdi", "C. Martin-Barreiro", "X. Cabezas"], "title": "A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "In this article, we introduce a novel deep learning hybrid model that\nintegrates attention Transformer and Gated Recurrent Unit (GRU) architectures\nto improve the accuracy of cryptocurrency price predictions. By combining the\nTransformer's strength in capturing long-range patterns with the GRU's ability\nto model short-term and sequential trends, the hybrid model provides a\nwell-rounded approach to time series forecasting. We apply the model to predict\nthe daily closing prices of Bitcoin and Ethereum based on historical data that\ninclude past prices, trading volumes, and the Fear and Greed index. We evaluate\nthe performance of our proposed model by comparing it with four other machine\nlearning models: two are non-sequential feedforward models: Radial Basis\nFunction Network (RBFN) and General Regression Neural Network (GRNN), and two\nare bidirectional sequential memory-based models: Bidirectional Long-Short-Term\nMemory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance\nof the model is assessed using several metrics, including Mean Squared Error\n(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean\nAbsolute Percentage Error (MAPE), along with statistical validation through the\nnonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.\nThe results demonstrate that our hybrid model consistently achieves superior\naccuracy, highlighting its effectiveness for financial prediction tasks. These\nfindings provide valuable insights for improving real-time decision making in\ncryptocurrency markets and support the growing use of hybrid deep learning\nmodels in financial analytics.", "AI": {"tldr": "A hybrid deep learning model combining Transformer and GRU architectures outperforms other models in predicting cryptocurrency prices.", "motivation": "Improve cryptocurrency price prediction accuracy by leveraging the strengths of Transformer (long-range patterns) and GRU (short-term trends).", "method": "Developed a hybrid Transformer-GRU model, tested on Bitcoin and Ethereum data, and compared with RBFN, GRNN, BiLSTM, and BiGRU using MSE, RMSE, MAE, MAPE, and statistical tests.", "result": "The hybrid model consistently outperformed other models in accuracy.", "conclusion": "The hybrid model is effective for financial predictions, supporting its use in cryptocurrency markets and financial analytics."}}
{"id": "2504.17309", "pdf": "https://arxiv.org/pdf/2504.17309", "abs": "https://arxiv.org/abs/2504.17309", "authors": ["Junyan Zhang", "Shuliang Liu", "Aiwei Liu", "Yubo Gao", "Jungang Li", "Xiaojie Gu", "Xuming Hu"], "title": "CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality", "categories": ["cs.CL"], "comment": "Published at the 1st workshop on GenAI Watermarking, collocated with\n  ICLR 2025", "summary": "Watermarking technology is a method used to trace the usage of content\ngenerated by large language models. Sentence-level watermarking aids in\npreserving the semantic integrity within individual sentences while maintaining\ngreater robustness. However, many existing sentence-level watermarking\ntechniques depend on arbitrary segmentation or generation processes to embed\nwatermarks, which can limit the availability of appropriate sentences. This\nlimitation, in turn, compromises the quality of the generated response. To\naddress the challenge of balancing high text quality with robust watermark\ndetection, we propose CoheMark, an advanced sentence-level watermarking\ntechnique that exploits the cohesive relationships between sentences for better\nlogical fluency. The core methodology of CoheMark involves selecting sentences\nthrough trained fuzzy c-means clustering and applying specific next sentence\nselection criteria. Experimental evaluations demonstrate that CoheMark achieves\nstrong watermark strength while exerting minimal impact on text quality.", "AI": {"tldr": "CoheMark is a sentence-level watermarking technique that improves logical fluency by leveraging cohesive relationships between sentences, balancing text quality and robust watermark detection.", "motivation": "Existing sentence-level watermarking techniques often compromise text quality due to arbitrary segmentation or generation processes. CoheMark aims to address this by ensuring better logical fluency and robustness.", "method": "CoheMark uses trained fuzzy c-means clustering for sentence selection and applies specific next sentence selection criteria to embed watermarks.", "result": "Experimental results show CoheMark achieves strong watermark strength with minimal impact on text quality.", "conclusion": "CoheMark effectively balances high text quality and robust watermark detection, making it a promising solution for tracing content generated by large language models."}}
{"id": "2504.09655", "pdf": "https://arxiv.org/pdf/2504.09655", "abs": "https://arxiv.org/abs/2504.09655", "authors": ["Justin Namuk Kim", "Yiqiao Liu", "Rajath Soans", "Keith Persson", "Sarah Halek", "Michal Tomaszewski", "Jianda Yuan", "Gregory Goldmacher", "Antong Chen"], "title": "OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)\n  2025", "summary": "Accurate segmentation of longitudinal CT scans is important for monitoring\ntumor progression and evaluating treatment responses. However, existing 3D\nsegmentation models solely focus on spatial information. To address this gap,\nwe propose OmniMamba4D, a novel segmentation model designed for 4D medical\nimages (3D images over time). OmniMamba4D utilizes a spatio-temporal\ntetra-orientated Mamba block to effectively capture both spatial and temporal\nfeatures. Unlike traditional 3D models, which analyze single-time points,\nOmniMamba4D processes 4D CT data, providing comprehensive spatio-temporal\ninformation on lesion progression. Evaluated on an internal dataset comprising\nof 3,252 CT scans, OmniMamba4D achieves a competitive Dice score of 0.682,\ncomparable to state-of-the-arts (SOTA) models, while maintaining computational\nefficiency and better detecting disappeared lesions. This work demonstrates a\nnew framework to leverage spatio-temporal information for longitudinal CT\nlesion segmentation.", "AI": {"tldr": "OmniMamba4D is a 4D segmentation model for longitudinal CT scans, capturing spatio-temporal features for better lesion progression analysis.", "motivation": "Existing 3D segmentation models lack temporal information, limiting their ability to monitor tumor progression over time.", "method": "OmniMamba4D uses a spatio-temporal tetra-orientated Mamba block to process 4D CT data, combining spatial and temporal features.", "result": "Achieves a Dice score of 0.682, comparable to SOTA models, with computational efficiency and improved detection of disappeared lesions.", "conclusion": "OmniMamba4D provides a novel framework for leveraging spatio-temporal data in longitudinal CT lesion segmentation."}}
{"id": "2504.17224", "pdf": "https://arxiv.org/pdf/2504.17224", "abs": "https://arxiv.org/abs/2504.17224", "authors": ["Zhifeng Wang", "Qixuan Zhang", "Peter Zhang", "Wenjia Niu", "Kaihao Zhang", "Ramesh Sankaranarayana", "Sabrina Caldwell", "Tom Gedeon"], "title": "Visual and textual prompts for enhancing emotion recognition in video", "categories": ["cs.CV"], "comment": "12 pages, 10 figures", "summary": "Vision Large Language Models (VLLMs) exhibit promising potential for\nmulti-modal understanding, yet their application to video-based emotion\nrecognition remains limited by insufficient spatial and contextual awareness.\nTraditional approaches, which prioritize isolated facial features, often\nneglect critical non-verbal cues such as body language, environmental context,\nand social interactions, leading to reduced robustness in real-world scenarios.\nTo address this gap, we propose Set-of-Vision-Text Prompting (SoVTP), a novel\nframework that enhances zero-shot emotion recognition by integrating spatial\nannotations (e.g., bounding boxes, facial landmarks), physiological signals\n(facial action units), and contextual cues (body posture, scene dynamics,\nothers' emotions) into a unified prompting strategy. SoVTP preserves holistic\nscene information while enabling fine-grained analysis of facial muscle\nmovements and interpersonal dynamics. Extensive experiments show that SoVTP\nachieves substantial improvements over existing visual prompting methods,\ndemonstrating its effectiveness in enhancing VLLMs' video emotion recognition\ncapabilities.", "AI": {"tldr": "SoVTP improves video emotion recognition in VLLMs by integrating spatial, physiological, and contextual cues into a unified prompting framework.", "motivation": "Current VLLMs lack spatial and contextual awareness for video emotion recognition, focusing too narrowly on facial features and missing broader non-verbal cues.", "method": "Proposes Set-of-Vision-Text Prompting (SoVTP), combining spatial annotations, physiological signals, and contextual cues into a unified prompting strategy.", "result": "SoVTP outperforms existing visual prompting methods, enhancing VLLMs' emotion recognition in videos.", "conclusion": "SoVTP effectively bridges the gap in VLLMs for robust video emotion recognition by leveraging holistic and fine-grained cues."}}
{"id": "2504.16940", "pdf": "https://arxiv.org/pdf/2504.16940", "abs": "https://arxiv.org/abs/2504.16940", "authors": ["Drew Linsley", "Pinyuan Feng", "Thomas Serre"], "title": "Can deep neural networks learn biological vision?", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) once showed increasing alignment with primate\nneural responses as they improved on computer vision benchmarks. This trend\nraised the exciting possibility that better models of biological vision would\ncome as a byproduct of the deep learning revolution in artificial intelligence.\nHowever, the trend has reversed over recent years as DNNs have scaled to human\nor superhuman recognition accuracy, a divergence that may stem from modern DNNs\nlearning to rely on different visual features than primates to solve tasks.\nWhere will better computational models of biological vision come from? We\npropose that vision science must break from artificial intelligence to develop\nalgorithms that are designed with biological visual systems in mind instead of\ninternet data benchmarks. We predict that the next generation of deep learning\nmodels of biological vision will be trained with data diets, training routines,\nand objectives that are closer to those that shape human vision than those that\nare in use today.", "AI": {"tldr": "DNNs initially aligned with primate vision but diverged as they scaled, suggesting future models should mimic biological vision more closely.", "motivation": "The divergence between DNNs and primate vision highlights the need for models designed with biological systems in mind, not just AI benchmarks.", "method": "Proposes developing algorithms tailored to biological visual systems, focusing on data diets, training routines, and objectives similar to human vision.", "result": "Current DNNs rely on different features than primates, reducing their alignment with biological vision.", "conclusion": "Future models should prioritize biological fidelity over AI benchmarks to better replicate primate vision."}}
{"id": "2504.17099", "pdf": "https://arxiv.org/pdf/2504.17099", "abs": "https://arxiv.org/abs/2504.17099", "authors": ["Martin Boeckling", "Heiko Paulheim", "Sarah Detzler"], "title": "GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs", "categories": ["cs.LG", "cs.SI"], "comment": "18 pages, ESWC 2025", "summary": "Many knowledge graphs contain a substantial number of spatial entities, such\nas cities, buildings, and natural landmarks. For many of these entities, exact\ngeometries are stored within the knowledge graphs. However, most existing\napproaches for learning entity representations do not take these geometries\ninto account. In this paper, we introduce a variant of RDF2Vec that\nincorporates geometric information to learn location-aware embeddings of\nentities. Our approach expands different nodes by flooding the graph from\ngeographic nodes, ensuring that each reachable node is considered. Based on the\nresulting flooded graph, we apply a modified version of RDF2Vec that biases\ngraph walks using spatial weights. Through evaluations on multiple benchmark\ndatasets, we demonstrate that our approach outperforms both non-location-aware\nRDF2Vec and GeoTransE.", "AI": {"tldr": "A method to enhance RDF2Vec by incorporating geometric data for location-aware entity embeddings, outperforming existing approaches.", "motivation": "Existing entity representation methods ignore geometric data in knowledge graphs, limiting their accuracy for spatial entities.", "method": "Extends RDF2Vec by flooding the graph from geographic nodes and biasing walks with spatial weights.", "result": "Outperforms non-location-aware RDF2Vec and GeoTransE in evaluations.", "conclusion": "Incorporating geometric data improves entity embeddings, especially for spatial entities."}}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311", "abs": "https://arxiv.org/abs/2504.17311", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors.", "AI": {"tldr": "FLUKE is a task-agnostic framework for evaluating NLP model robustness using systematic linguistic variations, revealing task-dependent impacts and vulnerabilities in both fine-tuned models and LLMs.", "motivation": "To systematically assess NLP model robustness across diverse linguistic variations, addressing gaps in task-agnostic evaluation.", "method": "FLUKE introduces controlled linguistic variations (orthography to dialect) and uses LLMs with human validation to generate test data modifications.", "result": "Findings show task-dependent robustness, LLMs outperform fine-tuned models but remain brittle to some variations, and all models struggle with negation.", "conclusion": "Systematic robustness testing is crucial for understanding model behaviors, as highlighted by FLUKE's task-agnostic evaluations."}}
{"id": "2504.13340", "pdf": "https://arxiv.org/pdf/2504.13340", "abs": "https://arxiv.org/abs/2504.13340", "authors": ["Oliver Mills", "Philip Conaghan", "Nishant Ravikumar", "Samuel Relton"], "title": "Putting the Segment Anything Model to the Test with 3D Knee MRI - A Comparison with State-of-the-Art Performance", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Work accepted at BMVC 2024. Minor changes to the camera-ready version\n  since acceptance include a corrected running header and the addition of an\n  Acknowledgments section (including code availability)", "summary": "Menisci are cartilaginous tissue found within the knee that contribute to\njoint lubrication and weight dispersal. Damage to menisci can lead to onset and\nprogression of knee osteoarthritis (OA), a condition that is a leading cause of\ndisability, and for which there are few effective therapies. Accurate automated\nsegmentation of menisci would allow for earlier detection and treatment of\nmeniscal abnormalities, as well as shedding more light on the role the menisci\nplay in OA pathogenesis. Focus in this area has mainly used variants of\nconvolutional networks, but there has been no attempt to utilise recent large\nvision transformer segmentation models. The Segment Anything Model (SAM) is a\nso-called foundation segmentation model, which has been found useful across a\nrange of different tasks due to the large volume of data used for training the\nmodel. In this study, SAM was adapted to perform fully-automated segmentation\nof menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained\nas a baseline. It was found that, when fine-tuning only the decoder, SAM was\nunable to compete with 3D U-Net, achieving a Dice score of $0.81\\pm0.03$,\ncompared to $0.87\\pm0.03$, on a held-out test set. When fine-tuning SAM\nend-to-end, a Dice score of $0.87\\pm0.03$ was achieved. The performance of both\nthe end-to-end trained SAM configuration and the 3D U-Net were comparable to\nthe winning Dice score ($0.88\\pm0.03$) in the IWOAI Knee MRI Segmentation\nChallenge 2019. Performance in terms of the Hausdorff Distance showed that both\nconfigurations of SAM were inferior to 3D U-Net in matching the meniscus\nmorphology. Results demonstrated that, despite its generalisability, SAM was\nunable to outperform a basic 3D U-Net in meniscus segmentation, and may not be\nsuitable for similar 3D medical image segmentation tasks also involving fine\nanatomical structures with low contrast and poorly-defined boundaries.", "AI": {"tldr": "SAM was tested for meniscus segmentation in knee MRI but underperformed compared to 3D U-Net, despite fine-tuning.", "motivation": "Accurate automated meniscus segmentation could improve early detection of abnormalities and OA understanding.", "method": "SAM and 3D U-Net were trained for meniscus segmentation in 3D knee MRI, with SAM fine-tuned in two ways.", "result": "SAM achieved comparable Dice scores to 3D U-Net only with end-to-end fine-tuning but was inferior in Hausdorff Distance.", "conclusion": "SAM may not be suitable for fine 3D medical segmentation tasks like menisci due to its limitations in handling low-contrast structures."}}
{"id": "2504.17229", "pdf": "https://arxiv.org/pdf/2504.17229", "abs": "https://arxiv.org/abs/2504.17229", "authors": ["Akihiro Kuwabara", "Sorachi Kato", "Takuya Fujihashi", "Toshiaki Koike-Akino", "Takashi Watanabe"], "title": "Range Image-Based Implicit Neural Compression for LiDAR Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel scheme to efficiently compress Light Detection\nand Ranging~(LiDAR) point clouds, enabling high-precision 3D scene archives,\nand such archives pave the way for a detailed understanding of the\ncorresponding 3D scenes. We focus on 2D range images~(RIs) as a lightweight\nformat for representing 3D LiDAR observations. Although conventional image\ncompression techniques can be adapted to improve compression efficiency for\nRIs, their practical performance is expected to be limited due to differences\nin bit precision and the distinct pixel value distribution characteristics\nbetween natural images and RIs. We propose a novel implicit neural\nrepresentation~(INR)--based RI compression method that effectively handles\nfloating-point valued pixels. The proposed method divides RIs into depth and\nmask images and compresses them using patch-wise and pixel-wise INR\narchitectures with model pruning and quantization, respectively. Experiments on\nthe KITTI dataset show that the proposed method outperforms existing image,\npoint cloud, RI, and INR-based compression methods in terms of 3D\nreconstruction and detection quality at low bitrates and decoding latency.", "AI": {"tldr": "A novel LiDAR point cloud compression method using implicit neural representation (INR) for 2D range images (RIs) outperforms existing techniques in 3D reconstruction and detection at low bitrates.", "motivation": "To efficiently compress LiDAR point clouds for high-precision 3D scene archives, addressing limitations of conventional image compression techniques for RIs.", "method": "Proposes an INR-based RI compression method, splitting RIs into depth and mask images, using patch-wise and pixel-wise INR architectures with pruning and quantization.", "result": "Outperforms existing methods in 3D reconstruction and detection quality on the KITTI dataset at low bitrates and decoding latency.", "conclusion": "The INR-based approach effectively compresses RIs, enabling efficient 3D scene understanding."}}
{"id": "2504.16942", "pdf": "https://arxiv.org/pdf/2504.16942", "abs": "https://arxiv.org/abs/2504.16942", "authors": ["Shushman Choudhury", "Elad Aharoni", "Chandrakumari Suvarna", "Iveel Tsogsuren", "Abdul Rahman Kreidieh", "Chun-Ta Lu", "Neha Arora"], "title": "S2Vec: Self-Supervised Geospatial Embeddings", "categories": ["cs.SI", "cs.AI", "cs.CV"], "comment": "To be submitted to ACM Transactions on Spatial Algorithms and Systems", "summary": "Scalable general-purpose representations of the built environment are crucial\nfor geospatial artificial intelligence applications. This paper introduces\nS2Vec, a novel self-supervised framework for learning such geospatial\nembeddings. S2Vec uses the S2 Geometry library to partition large areas into\ndiscrete S2 cells, rasterizes built environment feature vectors within cells as\nimages, and applies masked autoencoding on these rasterized images to encode\nthe feature vectors. This approach yields task-agnostic embeddings that capture\nlocal feature characteristics and broader spatial relationships. We evaluate\nS2Vec on three large-scale socioeconomic prediction tasks, showing its\ncompetitive performance against state-of-the-art image-based embeddings. We\nalso explore the benefits of combining S2Vec embeddings with image-based\nembeddings downstream, showing that such multimodal fusion can often improve\nperformance. Our results highlight how S2Vec can learn effective\ngeneral-purpose geospatial representations and how it can complement other data\nmodalities in geospatial artificial intelligence.", "AI": {"tldr": "S2Vec is a self-supervised framework for learning geospatial embeddings using S2 Geometry and masked autoencoding, showing competitive performance in socioeconomic tasks and complementing image-based embeddings.", "motivation": "To create scalable, general-purpose representations of the built environment for geospatial AI applications.", "method": "Partitions areas into S2 cells, rasterizes feature vectors as images, and applies masked autoencoding to learn embeddings.", "result": "Competitive performance in socioeconomic prediction tasks and improved performance when combined with image-based embeddings.", "conclusion": "S2Vec effectively learns general-purpose geospatial representations and complements other data modalities in geospatial AI."}}
{"id": "2504.17109", "pdf": "https://arxiv.org/pdf/2504.17109", "abs": "https://arxiv.org/abs/2504.17109", "authors": ["Zhaobin Mo", "Xiangyi Liao", "Dominik A. Karbowski", "Yanbing Wang"], "title": "Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks", "categories": ["cs.LG"], "comment": null, "summary": "Understanding and predicting the precursors of traffic breakdowns is critical\nfor improving road safety and traffic flow management. This paper presents a\nnovel approach combining spatiotemporal graph neural networks (ST-GNNs) with\nShapley values to identify and interpret traffic breakdown precursors. By\nextending Shapley explanation methods to a spatiotemporal setting, our proposed\nmethod bridges the gap between black-box neural network predictions and\ninterpretable causes. We demonstrate the method on the Interstate-24 data, and\nidentify that road topology and abrupt braking are major factors that lead to\ntraffic breakdowns.", "AI": {"tldr": "A novel method combining ST-GNNs and Shapley values to interpret traffic breakdown precursors, identifying road topology and abrupt braking as key factors.", "motivation": "Improving road safety and traffic flow management by understanding and predicting traffic breakdown precursors.", "method": "Combines spatiotemporal graph neural networks (ST-GNNs) with Shapley values for interpretable predictions.", "result": "Identified road topology and abrupt braking as major precursors of traffic breakdowns on Interstate-24 data.", "conclusion": "The method bridges the gap between black-box predictions and interpretable causes, enhancing traffic management."}}
{"id": "2504.17332", "pdf": "https://arxiv.org/pdf/2504.17332", "abs": "https://arxiv.org/abs/2504.17332", "authors": ["Zihan Wang", "Lu Yuan", "Zhengxuan Zhang", "Qing Zhao"], "title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection.", "AI": {"tldr": "The paper proposes the Dual-Aspect Empathy Framework (DAE) to detect misinformation by integrating cognitive and emotional empathy, outperforming traditional methods.", "motivation": "Traditional misinformation detection overlooks human empathy's role in propagation, prompting the need for a more human-centric approach.", "method": "DAE analyzes creators' cognitive and emotional strategies and simulates readers' responses using LLMs, with an empathy-aware filtering mechanism.", "result": "DAE outperforms existing methods on benchmark datasets, offering a novel paradigm for misinformation detection.", "conclusion": "DAE provides a comprehensive, human-centric approach to misinformation detection, enhancing authenticity and diversity in responses."}}
{"id": "2406.09656", "pdf": "https://arxiv.org/pdf/2406.09656", "abs": "https://arxiv.org/abs/2406.09656", "authors": ["Jingcheng Li", "Ye Qiao", "Haocheng Xu", "Sitao Huang"], "title": "RSEND: Retinex-based Squeeze and Excitation Network with Dark Region Detection for Efficient Low Light Image Enhancement", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Images captured under low-light scenarios often suffer from low quality.\nPrevious CNN-based deep learning methods often involve using Retinex theory.\nNevertheless, most of them cannot perform well in more complicated datasets\nlike LOL-v2 while consuming too much computational resources. Besides, some of\nthese methods require sophisticated training at different stages, making the\nprocedure even more time-consuming and tedious. In this paper, we propose a\nmore accurate, concise, and one-stage Retinex theory based framework, RSEND.\nRSEND first divides the low-light image into the illumination map and\nreflectance map, then captures the important details in the illumination map\nand performs light enhancement. After this step, it refines the enhanced\ngray-scale image and does element-wise matrix multiplication with the\nreflectance map. By denoising the output it has from the previous step, it\nobtains the final result. In all the steps, RSEND utilizes Squeeze and\nExcitation network to better capture the details. Comprehensive quantitative\nand qualitative experiments show that our Efficient Retinex model significantly\noutperforms other CNN-based models, achieving a PSNR improvement ranging from\n0.44 dB to 4.2 dB in different datasets and even outperforms transformer-based\nmodels in the LOL-v2-real dataset.", "AI": {"tldr": "RSEND is a one-stage Retinex-based framework for low-light image enhancement, outperforming CNN and transformer models in accuracy and efficiency.", "motivation": "Low-light images suffer from poor quality, and existing CNN-based methods are inefficient and perform poorly in complex datasets like LOL-v2.", "method": "RSEND divides images into illumination and reflectance maps, enhances details, refines the image, and denoises the result using Squeeze and Excitation networks.", "result": "RSEND achieves PSNR improvements of 0.44 dB to 4.2 dB, outperforming CNN and transformer models in LOL-v2-real.", "conclusion": "RSEND offers a more accurate, concise, and efficient solution for low-light image enhancement."}}
{"id": "2504.17234", "pdf": "https://arxiv.org/pdf/2504.17234", "abs": "https://arxiv.org/abs/2504.17234", "authors": ["Zhiqiang Lao", "Heather Yu"], "title": "Scene Perceived Image Perceptual Score (SPIPS): combining global and local perception for image quality assessment", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of artificial intelligence and widespread use of\nsmartphones have resulted in an exponential growth of image data, both real\n(camera-captured) and virtual (AI-generated). This surge underscores the\ncritical need for robust image quality assessment (IQA) methods that accurately\nreflect human visual perception. Traditional IQA techniques primarily rely on\nspatial features - such as signal-to-noise ratio, local structural distortions,\nand texture inconsistencies - to identify artifacts. While effective for\nunprocessed or conventionally altered images, these methods fall short in the\ncontext of modern image post-processing powered by deep neural networks (DNNs).\nThe rise of DNN-based models for image generation, enhancement, and restoration\nhas significantly improved visual quality, yet made accurate assessment\nincreasingly complex. To address this, we propose a novel IQA approach that\nbridges the gap between deep learning methods and human perception. Our model\ndisentangles deep features into high-level semantic information and low-level\nperceptual details, treating each stream separately. These features are then\ncombined with conventional IQA metrics to provide a more comprehensive\nevaluation framework. This hybrid design enables the model to assess both\nglobal context and intricate image details, better reflecting the human visual\nprocess, which first interprets overall structure before attending to\nfine-grained elements. The final stage employs a multilayer perceptron (MLP) to\nmap the integrated features into a concise quality score. Experimental results\ndemonstrate that our method achieves improved consistency with human perceptual\njudgments compared to existing IQA models.", "AI": {"tldr": "A novel image quality assessment (IQA) method combining deep learning features with traditional metrics to better align with human perception.", "motivation": "The rise of AI-generated and processed images demands more accurate IQA methods, as traditional techniques fail to address the complexity introduced by deep neural networks.", "method": "Disentangles deep features into semantic and perceptual details, combines them with conventional IQA metrics, and uses an MLP to map features into a quality score.", "result": "The proposed method outperforms existing IQA models in consistency with human perceptual judgments.", "conclusion": "The hybrid approach effectively bridges the gap between deep learning and human perception, offering a more comprehensive IQA framework."}}
{"id": "2504.16946", "pdf": "https://arxiv.org/pdf/2504.16946", "abs": "https://arxiv.org/abs/2504.16946", "authors": ["Xiaotong Ye", "Nicolas Bougie", "Toshihiko Yamasaki", "Narimasa Watanabe"], "title": "MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Generative agents offer promising capabilities for simulating realistic urban\nbehaviors. However, existing methods oversimplify transportation choices in\nmodern cities, and require prohibitive computational resources for large-scale\npopulation simulation. To address these limitations, we first present a virtual\ncity that features multiple functional buildings and transportation modes.\nThen, we conduct extensive surveys to model behavioral choices and mobility\npreferences among population groups. Building on these insights, we introduce a\nsimulation framework that captures the complexity of urban mobility while\nremaining scalable, enabling the simulation of over 4,000 agents. To assess the\nrealism of the generated behaviors, we perform a series of micro and\nmacro-level analyses. Beyond mere performance comparison, we explore insightful\nexperiments, such as predicting crowd density from movement patterns and\nidentifying trends in vehicle preferences across agent demographics.", "AI": {"tldr": "A scalable simulation framework for urban mobility using generative agents, addressing oversimplified transportation choices and computational limitations.", "motivation": "Existing methods oversimplify urban transportation choices and lack scalability for large populations.", "method": "Develop a virtual city with diverse buildings and transport modes, model behaviors via surveys, and create a scalable simulation framework.", "result": "Simulates 4,000+ agents, validated through micro/macro analyses, and explores crowd density and vehicle preference trends.", "conclusion": "The framework successfully balances complexity and scalability for realistic urban mobility simulation."}}
{"id": "2504.17140", "pdf": "https://arxiv.org/pdf/2504.17140", "abs": "https://arxiv.org/abs/2504.17140", "authors": ["Ashish Ranjan", "Ayush Agarwal", "Shalin Barot", "Sushant Kumar"], "title": "Scalable Permutation-Aware Modeling for Temporal Set Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal set prediction involves forecasting the elements that will appear in\nthe next set, given a sequence of prior sets, each containing a variable number\nof elements. Existing methods often rely on intricate architectures with\nsubstantial computational overhead, which hampers their scalability. In this\nwork, we introduce a novel and scalable framework that leverages\npermutation-equivariant and permutation-invariant transformations to\nefficiently model set dynamics. Our approach significantly reduces both\ntraining and inference time while maintaining competitive performance.\nExtensive experiments on multiple public benchmarks show that our method\nachieves results on par with or superior to state-of-the-art models across\nseveral evaluation metrics. These results underscore the effectiveness of our\nmodel in enabling efficient and scalable temporal set prediction.", "AI": {"tldr": "A scalable framework for temporal set prediction using permutation-equivariant and invariant transformations, reducing computational overhead while maintaining performance.", "motivation": "Existing methods for temporal set prediction are computationally heavy and lack scalability.", "method": "Leverages permutation-equivariant and permutation-invariant transformations to model set dynamics efficiently.", "result": "Achieves competitive or superior performance to state-of-the-art models with reduced training and inference time.", "conclusion": "The proposed method is effective for efficient and scalable temporal set prediction."}}
{"id": "2504.17360", "pdf": "https://arxiv.org/pdf/2504.17360", "abs": "https://arxiv.org/abs/2504.17360", "authors": ["Jose G. Moreno", "Jesus Lovon", "M'Rick Robin-Charlet", "Christine Damase-Michel", "Lynda Tamine"], "title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4.", "AI": {"tldr": "PatientDx is a framework for merging LLMs to improve health-predictive tasks without fine-tuning on sensitive patient data, achieving up to 7% AUROC improvement on MIMIC-IV mortality tasks while reducing data privacy risks.", "motivation": "Address data privacy concerns in healthcare by avoiding fine-tuning LLMs on sensitive patient data while maintaining performance.", "method": "Utilizes model merging techniques, optimizing a building block strategy with a pivotal model for numerical reasoning and hyperparameter tuning without direct training on patient data.", "result": "7% AUROC improvement on MIMIC-IV mortality tasks; reduced data leak risks compared to fine-tuned models.", "conclusion": "PatientDx offers a privacy-preserving alternative to fine-tuning, maintaining performance and reducing data privacy risks in healthcare applications."}}
{"id": "2410.11894", "pdf": "https://arxiv.org/pdf/2410.11894", "abs": "https://arxiv.org/abs/2410.11894", "authors": ["Kuang Huang", "Dong Heon Cho", "Boyuan Chen"], "title": "Automated Discovery of Operable Dynamics from Videos", "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.IV", "nlin.CD"], "comment": null, "summary": "Dynamical systems form the foundation of scientific discovery, traditionally\nmodeled with predefined state variables such as the angle and angular velocity,\nand differential equations such as the equation of motion for a single\npendulum. We introduce a framework that automatically discovers a\nlow-dimensional and operable representation of system dynamics, including a set\nof compact state variables that preserve the smoothness of the system dynamics\nand a differentiable vector field, directly from video without requiring prior\ndomain-specific knowledge. The prominence and effectiveness of the proposed\napproach are demonstrated through both quantitative and qualitative analyses of\na range of dynamical systems, including the identification of stable\nequilibria, the prediction of natural frequencies, and the detection of of\nchaotic and limit cycle behaviors. The results highlight the potential of our\ndata-driven approach to advance automated scientific discovery.", "AI": {"tldr": "A framework for automatically discovering low-dimensional representations of dynamical systems from video, without prior domain knowledge, is introduced and validated.", "motivation": "Traditional dynamical systems rely on predefined variables and equations, limiting automation in scientific discovery. This work aims to bypass such constraints.", "method": "The framework learns compact state variables and a differentiable vector field directly from video, preserving system dynamics smoothness.", "result": "Demonstrated effectiveness in identifying equilibria, predicting frequencies, and detecting chaotic behaviors across various systems.", "conclusion": "The approach advances automated scientific discovery by enabling data-driven, domain-agnostic modeling of dynamical systems."}}
{"id": "2504.17263", "pdf": "https://arxiv.org/pdf/2504.17263", "abs": "https://arxiv.org/abs/2504.17263", "authors": ["Wenqiang Zhou", "Zhendong Yu", "Xinyu Liu", "Jiaming Yang", "Rong Xiao", "Tao Wang", "Chenwei Tang", "Jiancheng Lv"], "title": "Precision Neural Network Quantization via Learnable Adaptive Modules", "categories": ["cs.CV", "cs.CC"], "comment": null, "summary": "Quantization Aware Training (QAT) is a neural network quantization technique\nthat compresses model size and improves operational efficiency while\neffectively maintaining model performance. The paradigm of QAT is to introduce\nfake quantization operators during the training process, allowing the model to\nautonomously compensate for information loss caused by quantization. Making\nquantization parameters trainable can significantly improve the performance of\nQAT, but at the cost of compromising the flexibility during inference,\nespecially when dealing with activation values with substantially different\ndistributions. In this paper, we propose an effective learnable adaptive neural\nnetwork quantization method, called Adaptive Step Size Quantization (ASQ), to\nresolve this conflict. Specifically, the proposed ASQ method first dynamically\nadjusts quantization scaling factors through a trained module capable of\naccommodating different activations. Then, to address the rigid resolution\nissue inherent in Power of Two (POT) quantization, we propose an efficient\nnon-uniform quantization scheme. We utilize the Power Of Square root of Two\n(POST) as the basis for exponential quantization, effectively handling the\nbell-shaped distribution of neural network weights across various bit-widths\nwhile maintaining computational efficiency through a Look-Up Table method\n(LUT). Extensive experimental results demonstrate that the proposed ASQ method\nis superior to the state-of-the-art QAT approaches. Notably that the ASQ is\neven competitive compared to full precision baselines, with its 4-bit quantized\nResNet34 model improving accuracy by 1.2\\% on ImageNet.", "AI": {"tldr": "ASQ improves QAT by dynamically adjusting quantization scaling factors and using a non-uniform POST-based scheme, outperforming state-of-the-art methods and even full precision baselines.", "motivation": "To resolve the conflict between trainable quantization parameters and inference flexibility, especially for varying activation distributions.", "method": "Proposes ASQ: dynamic scaling factor adjustment and POST-based non-uniform quantization with LUT for efficiency.", "result": "ASQ outperforms existing QAT methods, with a 4-bit ResNet34 achieving 1.2% higher accuracy on ImageNet.", "conclusion": "ASQ effectively balances performance and flexibility, advancing neural network quantization."}}
{"id": "2504.16947", "pdf": "https://arxiv.org/pdf/2504.16947", "abs": "https://arxiv.org/abs/2504.16947", "authors": ["Dachun Sun", "You Lyu", "Jinning Li", "Yizhuo Chen", "Tianshi Wang", "Tomoyoshi Kimura", "Tarek Abdelzaher"], "title": "SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "This paper introduces SCRAG, a prediction framework inspired by social\ncomputing, designed to forecast community responses to real or hypothetical\nsocial media posts. SCRAG can be used by public relations specialists (e.g., to\ncraft messaging in ways that avoid unintended misinterpretations) or public\nfigures and influencers (e.g., to anticipate social responses), among other\napplications related to public sentiment prediction, crisis management, and\nsocial what-if analysis. While large language models (LLMs) have achieved\nremarkable success in generating coherent and contextually rich text, their\nreliance on static training data and susceptibility to hallucinations limit\ntheir effectiveness at response forecasting in dynamic social media\nenvironments. SCRAG overcomes these challenges by integrating LLMs with a\nRetrieval-Augmented Generation (RAG) technique rooted in social computing.\nSpecifically, our framework retrieves (i) historical responses from the target\ncommunity to capture their ideological, semantic, and emotional makeup, and\n(ii) external knowledge from sources such as news articles to inject\ntime-sensitive context. This information is then jointly used to forecast the\nresponses of the target community to new posts or narratives. Extensive\nexperiments across six scenarios on the X platform (formerly Twitter), tested\nwith various embedding models and LLMs, demonstrate over 10% improvements on\naverage in key evaluation metrics. A concrete example further shows its\neffectiveness in capturing diverse ideologies and nuances. Our work provides a\nsocial computing tool for applications where accurate and concrete insights\ninto community responses are crucial.", "AI": {"tldr": "SCRAG is a framework combining LLMs and Retrieval-Augmented Generation to predict community responses on social media, improving accuracy by over 10%.", "motivation": "To address the limitations of LLMs in dynamic social media environments, such as static training data and hallucinations, for better response forecasting.", "method": "Integrates LLMs with RAG, retrieving historical community responses and external knowledge to forecast reactions to new posts.", "result": "Shows over 10% improvement in key metrics across six scenarios on X (Twitter), capturing diverse ideologies and nuances.", "conclusion": "SCRAG offers a robust tool for applications requiring precise insights into community responses, like crisis management and sentiment prediction."}}
{"id": "2504.17160", "pdf": "https://arxiv.org/pdf/2504.17160", "abs": "https://arxiv.org/abs/2504.17160", "authors": ["Alberto Fern\u00e1ndez-Hern\u00e1ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ort\u00ed"], "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "10 pages, 3 figures", "summary": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI.", "AI": {"tldr": "The paper introduces the Overfitting-Underfitting Indicator (OUI), a tool for monitoring DNN training dynamics and selecting optimal Weight Decay hyperparameters without validation data.", "motivation": "To address the challenge of identifying optimal regularization hyperparameters (like Weight Decay) during DNN training without relying on validation data.", "method": "Proposes OUI, which monitors training dynamics to indicate overfitting or underfitting. Validated on DenseNet-BC-100, EfficientNet-B0, and ResNet-34 across datasets like CIFAR-100 and ImageNet-1K.", "result": "OUI correlates with improved generalization, converges faster than traditional metrics, and enables early identification of optimal Weight Decay values.", "conclusion": "OUI is a reliable tool for precise hyperparameter tuning, improving DNN performance by balancing overfitting and underfitting early in training."}}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366", "abs": "https://arxiv.org/abs/2504.17366", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.", "AI": {"tldr": "The paper introduces a spoken long-text dataset from live streams to address challenges in long-context understanding for real-world dialogues. It evaluates LLMs and specialized methods, revealing task-specific biases and poor performance on redundancy. A new baseline is proposed, and the benchmark aids future improvements.", "motivation": "Existing benchmarks lack complexity for real-world dialogues, limiting LLM applicability. The study aims to bridge this gap with a redundancy-rich dataset.", "method": "Constructed a spoken long-text dataset from live streams with tasks in retrieval-dependent, reasoning-dependent, and hybrid categories. Evaluated LLMs and specialized methods.", "result": "Current methods show task-specific preferences and struggle with redundancy. A proposed baseline performs well across tasks.", "conclusion": "The benchmark addresses gaps in long-context spoken language understanding, offering a foundation for real-world applications like e-commerce."}}
{"id": "2504.12352", "pdf": "https://arxiv.org/pdf/2504.12352", "abs": "https://arxiv.org/abs/2504.12352", "authors": ["Ruijie Wang", "Luca Rossetto", "Susan M\u00e9rillat", "Christina R\u00f6cke", "Mike Martin", "Abraham Bernstein"], "title": "Deep Generative Model-Based Generation of Synthetic Individual-Specific Brain MRI Segmentations", "categories": ["q-bio.NC", "cs.AI", "eess.IV"], "comment": null, "summary": "To the best of our knowledge, all existing methods that can generate\nsynthetic brain magnetic resonance imaging (MRI) scans for a specific\nindividual require detailed structural or volumetric information about the\nindividual's brain. However, such brain information is often scarce, expensive,\nand difficult to obtain. In this paper, we propose the first approach capable\nof generating synthetic brain MRI segmentations -- specifically, 3D white\nmatter (WM), gray matter (GM), and cerebrospinal fluid (CSF) segmentations --\nfor individuals using their easily obtainable and often readily available\ndemographic, interview, and cognitive test information. Our approach features a\nnovel deep generative model, CSegSynth, which outperforms existing prominent\ngenerative models, including conditional variational autoencoder (C-VAE),\nconditional generative adversarial network (C-GAN), and conditional latent\ndiffusion model (C-LDM). We demonstrate the high quality of our synthetic\nsegmentations through extensive evaluations. Also, in assessing the\neffectiveness of the individual-specific generation, we achieve superior volume\nprediction, with mean absolute errors of only 36.44mL, 29.20mL, and 35.51mL\nbetween the ground-truth WM, GM, and CSF volumes of test individuals and those\nvolumes predicted based on generated individual-specific segmentations,\nrespectively.", "AI": {"tldr": "Proposes CSegSynth, a deep generative model for creating synthetic brain MRI segmentations using easily obtainable demographic and cognitive data, outperforming existing methods.", "motivation": "Existing methods require detailed brain data, which is scarce and costly. This work aims to use readily available non-imaging data for synthetic MRI generation.", "method": "Introduces CSegSynth, a novel deep generative model, and compares it against C-VAE, C-GAN, and C-LDM.", "result": "Achieves superior volume prediction with mean absolute errors of 36.44mL (WM), 29.20mL (GM), and 35.51mL (CSF).", "conclusion": "CSegSynth effectively generates high-quality synthetic brain MRI segmentations using accessible data, outperforming existing models."}}
{"id": "2504.17269", "pdf": "https://arxiv.org/pdf/2504.17269", "abs": "https://arxiv.org/abs/2504.17269", "authors": ["Yu Hong", "Xiao Cai", "Pengpeng Zeng", "Shuai Zhang", "Jingkuan Song", "Lianli Gao", "Heng Tao Shen"], "title": "Towards Generalized and Training-Free Text-Guided Semantic Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Text-guided semantic manipulation refers to semantically editing an image\ngenerated from a source prompt to match a target prompt, enabling the desired\nsemantic changes (e.g., addition, removal, and style transfer) while preserving\nirrelevant contents. With the powerful generative capabilities of the diffusion\nmodel, the task has shown the potential to generate high-fidelity visual\ncontent. Nevertheless, existing methods either typically require time-consuming\nfine-tuning (inefficient), fail to accomplish multiple semantic manipulations\n(poorly extensible), and/or lack support for different modality tasks (limited\ngeneralizability). Upon further investigation, we find that the geometric\nproperties of noises in the diffusion model are strongly correlated with the\nsemantic changes. Motivated by this, we propose a novel $\\textit{GTF}$ for\ntext-guided semantic manipulation, which has the following attractive\ncapabilities: 1) $\\textbf{Generalized}$: our $\\textit{GTF}$ supports multiple\nsemantic manipulations (e.g., addition, removal, and style transfer) and can be\nseamlessly integrated into all diffusion-based methods (i.e., Plug-and-play)\nacross different modalities (i.e., modality-agnostic); and 2)\n$\\textbf{Training-free}$: $\\textit{GTF}$ produces high-fidelity results via\nsimply controlling the geometric relationship between noises without tuning or\noptimization. Our extensive experiments demonstrate the efficacy of our\napproach, highlighting its potential to advance the state-of-the-art in\nsemantics manipulation.", "AI": {"tldr": "A novel training-free method (GTF) for text-guided semantic image manipulation using diffusion models, supporting multiple edits and modalities without fine-tuning.", "motivation": "Existing methods for text-guided semantic manipulation are inefficient, poorly extensible, or lack generalizability. Geometric properties of diffusion model noises are leveraged to address these limitations.", "method": "Proposes GTF, a plug-and-play, modality-agnostic framework that controls noise geometry for semantic edits (addition, removal, style transfer) without training.", "result": "GTF achieves high-fidelity results across multiple semantic manipulations and modalities, outperforming existing methods.", "conclusion": "GTF advances semantic manipulation by being efficient, extensible, and generalizable, setting a new state-of-the-art."}}
{"id": "2504.16948", "pdf": "https://arxiv.org/pdf/2504.16948", "abs": "https://arxiv.org/abs/2504.16948", "authors": ["Zhen Tan", "Huan Liu"], "title": "Intrinsic Barriers to Explaining Deep Foundation Models", "categories": ["cs.CY", "cs.AI", "cs.ET"], "comment": null, "summary": "Deep Foundation Models (DFMs) offer unprecedented capabilities but their\nincreasing complexity presents profound challenges to understanding their\ninternal workings-a critical need for ensuring trust, safety, and\naccountability. As we grapple with explaining these systems, a fundamental\nquestion emerges: Are the difficulties we face merely temporary hurdles,\nawaiting more sophisticated analytical techniques, or do they stem from\n\\emph{intrinsic barriers} deeply rooted in the nature of these large-scale\nmodels themselves? This paper delves into this critical question by examining\nthe fundamental characteristics of DFMs and scrutinizing the limitations\nencountered by current explainability methods when confronted with this\ninherent challenge. We probe the feasibility of achieving satisfactory\nexplanations and consider the implications for how we must approach the\nverification and governance of these powerful technologies.", "AI": {"tldr": "The paper explores whether challenges in understanding Deep Foundation Models (DFMs) are temporary or due to intrinsic barriers, examining current explainability methods and implications for governance.", "motivation": "To address the critical need for trust, safety, and accountability in DFMs by understanding if their complexity poses temporary or inherent challenges.", "method": "Examines DFMs' fundamental characteristics and limitations of current explainability methods.", "result": "Highlights the difficulties in achieving satisfactory explanations and the need for new approaches.", "conclusion": "Suggests rethinking verification and governance strategies for DFMs due to intrinsic challenges."}}
{"id": "2504.17196", "pdf": "https://arxiv.org/pdf/2504.17196", "abs": "https://arxiv.org/abs/2504.17196", "authors": ["Jiawen Hou", "Hao Wu"], "title": "A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation", "categories": ["cs.LG"], "comment": "11pages,3figures", "summary": "In intelligent transportation systems (ITS), traffic management departments\nrely on sensors, cameras, and GPS devices to collect real-time traffic data.\nTraffic speed data is often incomplete due to sensor failures, data\ntransmission delays, or occlusions, resulting in missing speed data in certain\nroad segments. Currently, tensor decomposition based methods are extensively\nutilized, they mostly rely on the $L_2$-norm to construct their learning\nobjectives, which leads to reduced robustness in the algorithms. To address\nthis, we propose Temporal-Aware Traffic Speed Imputation (TATSI), which\ncombines the $L_2$-norm and smooth $L_1$ (${SL}_1$)-norm in its loss function,\nthereby achieving both high accuracy and robust performance in imputing missing\ntime-varying traffic speed data. TATSI adopts a single latent factor-dependent,\nnonnegative, and multiplicative update (SLF-NMU) approach, which serves as an\nefficient solver for performing nonnegative latent factor analysis (LFA) on a\ntensor. Empirical studies on three real-world time-varying traffic speed\ndatasets demonstrate that, compared with state-of-the-art traffic speed\npredictors, TATSI more precisely captures temporal patterns, thereby yielding\nthe most accurate imputations for missing traffic speed data.", "AI": {"tldr": "TATSI combines $L_2$-norm and smooth $L_1$-norm for robust and accurate traffic speed data imputation, outperforming existing methods.", "motivation": "Traffic speed data often has missing values due to sensor issues, and current tensor decomposition methods lack robustness.", "method": "TATSI uses a hybrid loss function ($L_2$ + ${SL}_1$-norm) and SLF-NMU for nonnegative latent factor analysis.", "result": "TATSI outperforms state-of-the-art methods in capturing temporal patterns and imputing missing data.", "conclusion": "TATSI is an effective solution for robust and accurate traffic speed data imputation in ITS."}}
{"id": "2504.17390", "pdf": "https://arxiv.org/pdf/2504.17390", "abs": "https://arxiv.org/abs/2504.17390", "authors": ["Jihyun Lee", "Yejin Jeon", "Seungyeon Seo", "Gary Geunbae Lee"], "title": "PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 main", "summary": "Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests\nthrough natural language interactions, yet existing systems often produce\ngeneric, monotonic responses that lack individuality and fail to adapt to\nusers' personal attributes. To address this, we introduce PicPersona-TOD, a\nnovel dataset that incorporates user images as part of the persona, enabling\npersonalized responses tailored to user-specific factors such as age or\nemotional context. This is facilitated by first impressions, dialogue\npolicy-guided prompting, and the use of external knowledge to reduce\nhallucinations. Human evaluations confirm that our dataset enhances user\nexperience, with personalized responses contributing to a more engaging\ninteraction. Additionally, we introduce a new NLG model, Pictor, which not only\npersonalizes responses, but also demonstrates robust performance across unseen\ndomains https://github.com/JihyunLee1/PicPersona.", "AI": {"tldr": "PicPersona-TOD introduces a dataset and model (Pictor) for personalized Task-Oriented Dialogue systems using user images and first impressions to tailor responses, improving engagement.", "motivation": "Existing TOD systems produce generic responses lacking personalization, failing to adapt to user attributes like age or emotion.", "method": "Uses user images, first impressions, dialogue policy-guided prompting, and external knowledge to reduce hallucinations. Introduces the Pictor NLG model.", "result": "Human evaluations show enhanced user experience with personalized responses. Pictor performs robustly across unseen domains.", "conclusion": "PicPersona-TOD and Pictor improve TOD systems by personalizing responses, making interactions more engaging and adaptable."}}
{"id": "2504.17280", "pdf": "https://arxiv.org/pdf/2504.17280", "abs": "https://arxiv.org/abs/2504.17280", "authors": ["Haodi Yao", "Fenghua He", "Ning Hao", "Chen Xie"], "title": "EdgePoint2: Compact Descriptors for Superior Efficiency and Accuracy", "categories": ["cs.CV"], "comment": null, "summary": "The field of keypoint extraction, which is essential for vision applications\nlike Structure from Motion (SfM) and Simultaneous Localization and Mapping\n(SLAM), has evolved from relying on handcrafted methods to leveraging deep\nlearning techniques. While deep learning approaches have significantly improved\nperformance, they often incur substantial computational costs, limiting their\ndeployment in real-time edge applications. Efforts to create lightweight neural\nnetworks have seen some success, yet they often result in trade-offs between\nefficiency and accuracy. Additionally, the high-dimensional descriptors\ngenerated by these networks poses challenges for distributed applications\nrequiring efficient communication and coordination, highlighting the need for\ncompact yet competitively accurate descriptors. In this paper, we present\nEdgePoint2, a series of lightweight keypoint detection and description neural\nnetworks specifically tailored for edge computing applications on embedded\nsystem. The network architecture is optimized for efficiency without\nsacrificing accuracy. To train compact descriptors, we introduce a combination\nof Orthogonal Procrustes loss and similarity loss, which can serve as a general\napproach for hypersphere embedding distillation tasks. Additionally, we offer\n14 sub-models to satisfy diverse application requirements. Our experiments\ndemonstrate that EdgePoint2 consistently achieves state-of-the-art (SOTA)\naccuracy and efficiency across various challenging scenarios while employing\nlower-dimensional descriptors (32/48/64). Beyond its accuracy, EdgePoint2\noffers significant advantages in flexibility, robustness, and versatility.\nConsequently, EdgePoint2 emerges as a highly competitive option for visual\ntasks, especially in contexts demanding adaptability to diverse computational\nand communication constraints.", "AI": {"tldr": "EdgePoint2 is a lightweight neural network for keypoint detection and description, optimized for edge computing, balancing efficiency and accuracy with compact descriptors.", "motivation": "Deep learning methods for keypoint extraction are computationally expensive for edge applications, and existing lightweight models trade efficiency for accuracy. High-dimensional descriptors also hinder distributed applications.", "method": "EdgePoint2 uses an optimized architecture and introduces a training approach combining Orthogonal Procrustes loss and similarity loss for compact descriptors. It offers 14 sub-models for diverse needs.", "result": "EdgePoint2 achieves SOTA accuracy and efficiency with lower-dimensional descriptors (32/48/64), excelling in flexibility, robustness, and versatility.", "conclusion": "EdgePoint2 is a competitive solution for edge computing, addressing computational and communication constraints while maintaining high performance."}}
{"id": "2504.16979", "pdf": "https://arxiv.org/pdf/2504.16979", "abs": "https://arxiv.org/abs/2504.16979", "authors": ["Masoud Tafavvoghi", "Lars Ailo Bongo", "Andr\u00e9 Berli Delgado", "Nikita Shvetsov", "Anders Sildnes", "Line Moi", "Lill-Tove Rasmussen Busund", "Kajsa M\u00f8llersen"], "title": "Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline", "categories": ["q-bio.QM", "cs.AI", "cs.CV"], "comment": "16 Pages, 9 Figures, 3 tables", "summary": "In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)\nassessment pipeline within QuPath, demonstrating the potential of easily\naccessible tools to perform complex tasks in a fully automatic fashion. First,\nwe trained a pixel classifier to segment tumor, tumor-associated stroma, and\nother tissue compartments in breast cancer H&E-stained whole-slide images (WSI)\nto isolate tumor-associated stroma for subsequent analysis. Next, we applied a\npre-trained StarDist deep learning model in QuPath for cell detection and used\nthe extracted cell features to train a binary classifier distinguishing TILs\nfrom other cells. To evaluate our TILs assessment pipeline, we calculated the\nTIL density in each WSI and categorized them as low, medium, or high TIL\nlevels. Our pipeline was evaluated against pathologist-assigned TIL scores,\nachieving a Cohen's kappa of 0.71 on the external test set, corroborating\nprevious research findings. These results confirm that existing software can\noffer a practical solution for the assessment of TILs in H&E-stained WSIs of\nbreast cancer.", "AI": {"tldr": "An end-to-end pipeline in QuPath for automated TILs assessment in breast cancer WSIs, achieving high agreement with pathologist scores.", "motivation": "To demonstrate the feasibility of using accessible tools for complex, automated TILs assessment in breast cancer.", "method": "Trained a pixel classifier for tissue segmentation, applied a pre-trained StarDist model for cell detection, and trained a binary classifier for TILs identification. Evaluated TIL density levels against pathologist scores.", "result": "Achieved a Cohen's kappa of 0.71 on an external test set, validating the pipeline's accuracy.", "conclusion": "Existing software like QuPath can provide practical solutions for automated TILs assessment in breast cancer WSIs."}}
{"id": "2504.17210", "pdf": "https://arxiv.org/pdf/2504.17210", "abs": "https://arxiv.org/abs/2504.17210", "authors": ["Junfei Wang", "Darshana Upadhyay", "Marzia Zaman", "Pirathayini Srikantha"], "title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to IEEE SmartGridComm Conference 2025", "summary": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications.", "AI": {"tldr": "A physics-informed generative framework using DDPMs synthesizes feasible power flow data, outperforming baselines in feasibility, diversity, and accuracy.", "motivation": "Real-world power flow data is limited due to privacy and operational constraints, necessitating synthetic data generation.", "method": "Uses Denoising Diffusion Probabilistic Models (DDPMs) with auxiliary training and physics-informed loss functions.", "result": "Outperforms three baseline models in feasibility, diversity, and accuracy on IEEE 14-bus and 30-bus systems.", "conclusion": "Demonstrates the potential of generative modeling for data-driven power system applications."}}
{"id": "2504.17445", "pdf": "https://arxiv.org/pdf/2504.17445", "abs": "https://arxiv.org/abs/2504.17445", "authors": ["Anna Lieb", "Maneesh Arora", "Eni Mustafaraj"], "title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation", "categories": ["cs.CL"], "comment": "Presented at IC2S2 2024 in Philadelphia, USA", "summary": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance.", "AI": {"tldr": "Using GPT-4 to augment topic modeling improves interpretability and practicality for domain-specific research questions.", "motivation": "Overcome limitations of traditional topic models (interpretability and practicality) in social science research.", "method": "Augment topic modeling with GPT-4-generated text and evaluate using a political science case study.", "result": "GPT-4-augmented topic modeling produces highly interpretable categories for domain-specific questions.", "conclusion": "LLM-augmented topic modeling enhances utility for targeted social science research with minimal human input."}}
{"id": "2504.17306", "pdf": "https://arxiv.org/pdf/2504.17306", "abs": "https://arxiv.org/abs/2504.17306", "authors": ["Meher Boulaabi", "Takwa Ben A\u00efcha Gader", "Afef Kacem Echi", "Sameh Mbarek"], "title": "Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+", "categories": ["cs.CV", "cs.AI"], "comment": "This work was accepted at the ACS/IEEE International Conference on\n  Computer Systems and Applications (AICCSA) 2024", "summary": "To improve the segmentation of diabetic retinopathy lesions (microaneurysms,\nhemorrhages, exudates, and soft exudates), we implemented a binary segmentation\nmethod specific to each type of lesion. As post-segmentation, we combined the\nindividual model outputs into a single image to better analyze the lesion\ntypes. This approach facilitated parameter optimization and improved accuracy,\neffectively overcoming challenges related to dataset limitations and annotation\ncomplexity. Specific preprocessing steps included cropping and applying\ncontrast-limited adaptive histogram equalization to the L channel of the LAB\nimage. Additionally, we employed targeted data augmentation techniques to\nfurther refine the model's efficacy. Our methodology utilized the DeepLabv3+\nmodel, achieving a segmentation accuracy of 99%. These findings highlight the\nefficacy of innovative strategies in advancing medical image analysis,\nparticularly in the precise segmentation of diabetic retinopathy lesions. The\nIDRID dataset was utilized to validate and demonstrate the robustness of our\napproach.", "AI": {"tldr": "A binary segmentation method for diabetic retinopathy lesions improved accuracy by combining individual model outputs and using DeepLabv3+, achieving 99% segmentation accuracy.", "motivation": "To enhance segmentation of diabetic retinopathy lesions (microaneurysms, hemorrhages, exudates, soft exudates) by addressing dataset limitations and annotation complexity.", "method": "Implemented binary segmentation for each lesion type, combined outputs, used preprocessing (cropping, CLAHE on LAB image), and employed targeted data augmentation with DeepLabv3+.", "result": "Achieved 99% segmentation accuracy, validated on the IDRID dataset.", "conclusion": "Innovative strategies, including model combination and preprocessing, effectively advance medical image analysis for diabetic retinopathy."}}
{"id": "2504.17020", "pdf": "https://arxiv.org/pdf/2504.17020", "abs": "https://arxiv.org/abs/2504.17020", "authors": ["Kasper Engelen", "Guillermo A. P\u00e9rez", "Shrisha Rao"], "title": "Analyzing Value Functions of States in Parametric Markov Chains", "categories": ["cs.LO", "cs.AI"], "comment": "Published as part of the book \"Principles of Verification: Cycling\n  the Probabilistic Landscape: Essays Dedicated to Joost-Pieter Katoen on the\n  Occasion of His 60th Birthday, Part II\"", "summary": "Parametric Markov chains (pMC) are used to model probabilistic systems with\nunknown or partially known probabilities. Although (universal) pMC verification\nfor reachability properties is known to be coETR-complete, there have been\nefforts to approach it using potentially easier-to-check properties such as\nasking whether the pMC is monotonic in certain parameters. In this paper, we\nfirst reduce monotonicity to asking whether the reachability probability from a\ngiven state is never less than that of another given state. Recent results for\nthe latter property imply an efficient algorithm to collapse same-value\nequivalence classes, which in turn preserves verification results and\nmonotonicity. We implement our algorithm to collapse \"trivial\" equivalence\nclasses in the pMC and show empirical evidence for the following: First, the\ncollapse gives reductions in size for some existing benchmarks and significant\nreductions on some custom benchmarks; Second, the collapse speeds up existing\nalgorithms to check monotonicity and parameter lifting, and hence can be used\nas a fast pre-processing step in practice.", "AI": {"tldr": "The paper reduces pMC monotonicity verification to comparing reachability probabilities between states, leading to an efficient algorithm for collapsing equivalence classes, improving benchmark performance.", "motivation": "To simplify and speed up the verification of parametric Markov chains (pMCs) by leveraging monotonicity properties and equivalence class collapsing.", "method": "Reduces monotonicity to comparing reachability probabilities, implements an algorithm to collapse trivial equivalence classes, and tests it on benchmarks.", "result": "Shows size reductions in benchmarks and faster monotonicity and parameter lifting checks, proving the method's practical utility.", "conclusion": "The collapse of equivalence classes is an effective pre-processing step for pMC verification, enhancing efficiency in practice."}}
{"id": "2504.17219", "pdf": "https://arxiv.org/pdf/2504.17219", "abs": "https://arxiv.org/abs/2504.17219", "authors": ["Hyomin Lee", "Minseon Kim", "Sangwon Jang", "Jongheon Jeong", "Sung Ju Hwang"], "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness.", "AI": {"tldr": "SRL-VAE, a novel adversarial training framework, enhances both generation quality and robustness in VAEs by smoothing latent space and preserving fidelity.", "motivation": "Addressing the underexplored robustness of VAEs in diffusion-based models and challenging the assumption that adversarial training degrades generative model fidelity.", "method": "Introduces SRL-VAE, which smooths latent space via adversarial perturbations while regularizing to maintain original fidelity, applied as a post-training step.", "result": "Improves image robustness and fidelity, enhancing generation quality in reconstruction and text-guided editing, and robustness against attacks.", "conclusion": "Adversarial training can enhance both fidelity and robustness in generative models, establishing a new paradigm."}}
{"id": "2504.17480", "pdf": "https://arxiv.org/pdf/2504.17480", "abs": "https://arxiv.org/abs/2504.17480", "authors": ["Xin Yi", "Shunfan Zhengc", "Linlin Wanga", "Xiaoling Wang", "Liang He"], "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable.", "AI": {"tldr": "The paper explores watermark radioactivity in LLMs, proposing CDG-KD for bidirectional attacks (removal and forgery) under unauthorized knowledge distillation, highlighting the need for robust watermarking.", "motivation": "To address the unexplored robustness and unforgeability of watermarks in LLMs against scrubbing and spoofing attacks during unauthorized knowledge distillation.", "method": "Proposes CDG-KD, using contrastive decoding to extract corrupted/amplified watermarks and bidirectional distillation to train models for removal or forgery.", "result": "CDG-KD effectively performs attacks while maintaining model performance, demonstrating vulnerabilities in current watermarking.", "conclusion": "Emphasizes the need for more robust and unforgeable watermarking schemes in LLMs."}}
{"id": "2504.17315", "pdf": "https://arxiv.org/pdf/2504.17315", "abs": "https://arxiv.org/abs/2504.17315", "authors": ["Zhanglin Wu", "Tengfei Song", "Ning Xie", "Weidong Zhang", "Pengfei Li", "Shuang Wu", "Chong Li", "Junhao Zhu", "Hao Yang"], "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 1 figures, 2 tables", "summary": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation.", "AI": {"tldr": "Huawei's solution for document image translation combines multi-task learning and perceptual chain-of-thought using LVLM, achieving strong results in OCR-based and OCR-free tasks.", "motivation": "To address the challenges of translating documents with complex layouts in an end-to-end manner.", "method": "Uses a training framework with multi-task learning and perceptual chain-of-thought, plus minimum Bayesian decoding and post-processing for inference.", "result": "Demonstrates effectiveness in handling both OCR-based and OCR-free document translation tasks.", "conclusion": "The proposed unified framework is a robust solution for document image machine translation."}}
{"id": "2504.17023", "pdf": "https://arxiv.org/pdf/2504.17023", "abs": "https://arxiv.org/abs/2504.17023", "authors": ["Felix Kares", "Timo Speith", "Hanwei Zhang", "Markus Langer"], "title": "What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)", "categories": ["cs.HC", "cs.AI"], "comment": "27 pages, 7 figures, 4 tables", "summary": "Saliency maps are a popular approach for explaining classifications of\n(convolutional) neural networks. However, it remains an open question as to how\nbest to evaluate salience maps, with three families of evaluation methods\ncommonly being used: subjective user measures, objective user measures, and\nmathematical metrics. We examine three of the most popular saliency map\napproaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between\nsubject study (N=166) across these families of evaluation methods. We test 1)\nfor subjective measures, if the maps differ with respect to user trust and\nsatisfaction; 2) for objective measures, if the maps increase users' abilities\nand thus understanding of a model; 3) for mathematical metrics, which map\nachieves the best ratings across metrics; and 4) whether the mathematical\nmetrics can be associated with objective user measures. To our knowledge, our\nstudy is the first to compare several salience maps across all these evaluation\nmethods$-$with the finding that they do not agree in their assessment (i.e.,\nthere was no difference concerning trust and satisfaction, Grad-CAM improved\nusers' abilities best, and Guided Backpropagation had the most favorable\nmathematical metrics). Additionally, we show that some mathematical metrics\nwere associated with user understanding, although this relationship was often\ncounterintuitive. We discuss these findings in light of general debates\nconcerning the complementary use of user studies and mathematical metrics in\nthe evaluation of explainable AI (XAI) approaches.", "AI": {"tldr": "The paper evaluates three saliency map methods (LIME, Grad-CAM, Guided Backpropagation) using subjective, objective, and mathematical metrics, finding no agreement in assessments and unexpected relationships between metrics and user understanding.", "motivation": "To address the lack of consensus on how to evaluate saliency maps for explaining neural network classifications, comparing methods across different evaluation families.", "method": "A between-subject study (N=166) testing subjective (trust, satisfaction), objective (user understanding), and mathematical metrics for LIME, Grad-CAM, and Guided Backpropagation.", "result": "No difference in subjective measures; Grad-CAM improved user understanding best; Guided Backpropagation scored highest on mathematical metrics. Some metrics correlated with user understanding, but counterintuitively.", "conclusion": "Saliency map evaluations vary by method, and mathematical metrics may not align with user experience, highlighting the need for complementary evaluation approaches in XAI."}}
{"id": "2504.17232", "pdf": "https://arxiv.org/pdf/2504.17232", "abs": "https://arxiv.org/abs/2504.17232", "authors": ["Nivedita M", "Yasmeen Shajitha S"], "title": "Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification", "categories": ["cs.LG"], "comment": "5 pages,10 figures", "summary": "This study proposes an integrated machine learning framework for advanced\ntraffic analysis, combining time-series forecasting, classification, and\ncomputer vision techniques. The system utilizes an ARIMA(2,0,1) model for\ntraffic prediction (MAE: 2.1), an XGBoost classifier for accident severity\nclassification (100% accuracy on balanced data), and a Convolutional Neural\nNetwork (CNN) for traffic image classification (92% accuracy). Tested on\ndiverse datasets, the framework outperforms baseline models and identifies key\nfactors influencing accident severity, including weather and road\ninfrastructure. Its modular design supports deployment in smart city systems\nfor real-time monitoring, accident prevention, and resource optimization,\ncontributing to the evolution of intelligent transportation systems.", "AI": {"tldr": "An integrated ML framework combining ARIMA, XGBoost, and CNN for traffic analysis, achieving high accuracy in prediction, classification, and image analysis, with applications in smart cities.", "motivation": "To enhance traffic analysis and accident prevention by integrating multiple ML techniques for real-time monitoring and resource optimization in smart city systems.", "method": "Combines ARIMA(2,0,1) for traffic prediction, XGBoost for accident severity classification, and CNN for traffic image classification, tested on diverse datasets.", "result": "Outperforms baselines with MAE 2.1 for ARIMA, 100% accuracy for XGBoost, and 92% accuracy for CNN. Identifies key factors like weather and road infrastructure.", "conclusion": "The modular framework is effective for smart city deployment, advancing intelligent transportation systems."}}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550", "abs": "https://arxiv.org/abs/2504.17550", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "title": "HalluLens: LLM Hallucination Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.", "AI": {"tldr": "The paper introduces a benchmark for LLM hallucinations, proposing a taxonomy to distinguish extrinsic and intrinsic types, and addresses data leakage issues with dynamic test sets.", "motivation": "Hallucinations in LLMs reduce user trust and hinder AI adoption, necessitating a unified framework for evaluation.", "method": "Develops a hallucination benchmark with new extrinsic tasks and dynamic test set generation, alongside analyzing existing benchmarks.", "result": "Provides a clear taxonomy and robust benchmark to evaluate hallucinations, distinguishing them from factuality.", "conclusion": "The work advances LLM research by standardizing hallucination evaluation and mitigating benchmark limitations."}}
{"id": "2504.17343", "pdf": "https://arxiv.org/pdf/2504.17343", "abs": "https://arxiv.org/abs/2504.17343", "authors": ["Linli Yao", "Yicheng Li", "Yuancheng Wei", "Lei Li", "Shuhuai Ren", "Yuanxin Liu", "Kun Ouyang", "Lean Wang", "Shicheng Li", "Sida Li", "Lingpeng Kong", "Qi Liu", "Yuanxing Zhang", "Xu Sun"], "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.", "AI": {"tldr": "TimeChat-Online is a novel VideoLLM for real-time video interaction, featuring the Differential Token Drop (DTD) module to reduce redundancy and improve efficiency.", "motivation": "Address the limitations of existing VideoLLMs in handling streaming videos by reducing visual redundancy and enabling real-time interaction.", "method": "Introduces the DTD module inspired by human visual perception to filter redundant frames while preserving meaningful changes. Also presents TimeChat-Online-139K dataset for diverse interaction patterns.", "result": "DTD reduces video tokens by 82.8% while maintaining 98% performance. TimeChat-Online excels in streaming benchmarks and long-form video tasks.", "conclusion": "TimeChat-Online revolutionizes real-time video interaction with its efficiency and performance, setting a new standard for VideoLLMs in streaming scenarios."}}
{"id": "2504.17029", "pdf": "https://arxiv.org/pdf/2504.17029", "abs": "https://arxiv.org/abs/2504.17029", "authors": ["Jeffrey Smith", "Taisei Fujii", "Jesse Craney", "Charles Gretton"], "title": "Fried Parameter Estimation from Single Wavefront Sensor Image with Artificial Neural Networks", "categories": ["astro-ph.IM", "cs.AI"], "comment": null, "summary": "Atmospheric turbulence degrades the quality of astronomical observations in\nground-based telescopes, leading to distorted and blurry images. Adaptive\nOptics (AO) systems are designed to counteract these effects, using atmospheric\nmeasurements captured by a wavefront sensor to make real-time corrections to\nthe incoming wavefront. The Fried parameter, r0, characterises the strength of\natmospheric turbulence and is an essential control parameter for optimising the\nperformance of AO systems and more recently sky profiling for Free Space\nOptical (FSO) communication channels. In this paper, we develop a novel\ndata-driven approach, adapting machine learning methods from computer vision\nfor Fried parameter estimation from a single Shack-Hartmann or pyramid\nwavefront sensor image. Using these data-driven methods, we present a detailed\nsimulation-based evaluation of our approach using the open-source COMPASS AO\nsimulation tool to evaluate both the Shack-Hartmann and pyramid wavefront\nsensors. Our evaluation is over a range of guide star magnitudes, and realistic\nnoise, atmospheric and instrument conditions. Remarkably, we are able to\ndevelop a single network-based estimator that is accurate in both open and\nclosed-loop AO configurations. Our method accurately estimates the Fried\nparameter from a single WFS image directly from AO telemetry to a few\nmillimetres. Our approach is suitable for real time control, exhibiting 0.83ms\nr0 inference times on retail NVIDIA RTX 3090 GPU hardware, and thereby\ndemonstrating a compelling economic solution for use in real-time instrument\ncontrol.", "AI": {"tldr": "A novel machine learning method estimates the Fried parameter (r0) from wavefront sensor images for real-time adaptive optics control.", "motivation": "Atmospheric turbulence degrades astronomical observations, and accurate r0 estimation is crucial for optimizing adaptive optics systems and FSO communication.", "method": "The paper adapts computer vision machine learning techniques to estimate r0 from single Shack-Hartmann or pyramid wavefront sensor images, evaluated via simulations with COMPASS AO.", "result": "The method accurately estimates r0 in both open and closed-loop AO configurations, with fast inference times (0.83ms) suitable for real-time control.", "conclusion": "The approach provides an efficient, real-time solution for r0 estimation in adaptive optics systems, demonstrating economic viability."}}
{"id": "2504.17243", "pdf": "https://arxiv.org/pdf/2504.17243", "abs": "https://arxiv.org/abs/2504.17243", "authors": ["Xinyu Zhou", "Simin Fan", "Martin Jaggi", "Jie Fu"], "title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint, 16 pages", "summary": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability.", "AI": {"tldr": "NeuralGrok, a gradient-based method, accelerates generalization in transformers for arithmetic tasks by dynamically modulating gradients via an auxiliary MLP block and bilevel optimization.", "motivation": "To address the slow generalization (grokking) phenomenon in transformers by improving gradient dynamics and reducing model complexity.", "method": "Trains an auxiliary MLP to modulate gradients based on their generalization contribution, using bilevel optimization. Introduces AGE metric to analyze model complexity.", "result": "NeuralGrok speeds up generalization, stabilizes training, and reduces complexity, outperforming traditional methods like weight decay.", "conclusion": "NeuralGrok provides insights into grokking and offers a practical approach to enhance generalization in transformers."}}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562", "abs": "https://arxiv.org/abs/2504.17562", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference.", "AI": {"tldr": "Prepending metadata in pre-training improves model performance in downstream tasks if the context allows latent semantics inference, but harms it otherwise.", "motivation": "To understand why metadata prepending inconsistently improves model performance, focusing on latent semantics inference.", "method": "Examined model behavior using artificial data generated by probabilistic context-free grammars.", "result": "Metadata helps with long contexts for latent semantics but hurts performance with insufficient context.", "conclusion": "Metadata prepending's effectiveness depends on the downstream task's ability to infer latent semantics from the prompt."}}
{"id": "2504.17349", "pdf": "https://arxiv.org/pdf/2504.17349", "abs": "https://arxiv.org/abs/2504.17349", "authors": ["Yiyan Xu", "Wuqiang Zheng", "Wenjie Wang", "Fengbin Zhu", "Xinting Hu", "Yang Zhang", "Fuli Feng", "Tat-Seng Chua"], "title": "DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Personalized image generation has emerged as a promising direction in\nmultimodal content creation. It aims to synthesize images tailored to\nindividual style preferences (e.g., color schemes, character appearances,\nlayout) and semantic intentions (e.g., emotion, action, scene contexts) by\nleveraging user-interacted history images and multimodal instructions. Despite\nnotable progress, existing methods -- whether based on diffusion models, large\nlanguage models, or Large Multimodal Models (LMMs) -- struggle to accurately\ncapture and fuse user style preferences and semantic intentions. In particular,\nthe state-of-the-art LMM-based method suffers from the entanglement of visual\nfeatures, leading to Guidance Collapse, where the generated images fail to\npreserve user-preferred styles or reflect the specified semantics.\n  To address these limitations, we introduce DRC, a novel personalized image\ngeneration framework that enhances LMMs through Disentangled Representation\nComposition. DRC explicitly extracts user style preferences and semantic\nintentions from history images and the reference image, respectively, to form\nuser-specific latent instructions that guide image generation within LMMs.\nSpecifically, it involves two critical learning stages: 1) Disentanglement\nlearning, which employs a dual-tower disentangler to explicitly separate style\nand semantic features, optimized via a reconstruction-driven paradigm with\ndifficulty-aware importance sampling; and 2) Personalized modeling, which\napplies semantic-preserving augmentations to effectively adapt the disentangled\nrepresentations for robust personalized generation. Extensive experiments on\ntwo benchmarks demonstrate that DRC shows competitive performance while\neffectively mitigating the guidance collapse issue, underscoring the importance\nof disentangled representation learning for controllable and effective\npersonalized image generation.", "AI": {"tldr": "DRC introduces a framework for personalized image generation by disentangling style and semantic features to avoid guidance collapse in LMMs.", "motivation": "Existing methods struggle to accurately capture and fuse user style preferences and semantic intentions, leading to guidance collapse.", "method": "DRC uses disentangled representation composition with dual-tower disentangler and semantic-preserving augmentations.", "result": "DRC outperforms benchmarks, mitigating guidance collapse and improving personalized generation.", "conclusion": "Disentangled representation learning is crucial for effective and controllable personalized image generation."}}
{"id": "2504.17044", "pdf": "https://arxiv.org/pdf/2504.17044", "abs": "https://arxiv.org/abs/2504.17044", "authors": ["Dhari Gandhi", "Himanshu Joshi", "Lucas Hartman", "Shabnam Hassani"], "title": "Approaches to Responsible Governance of GenAI in Organizations", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of Generative AI (GenAI) has introduced unprecedented\nopportunities while presenting complex challenges around ethics,\naccountability, and societal impact. This paper draws on a literature review,\nestablished governance frameworks, and industry roundtable discussions to\nidentify core principles for integrating responsible GenAI governance into\ndiverse organizational structures. Our objective is to provide actionable\nrecommendations for a balanced, risk-based governance approach that enables\nboth innovation and oversight. Findings emphasize the need for adaptable risk\nassessment tools, continuous monitoring practices, and cross-sector\ncollaboration to establish trustworthy GenAI. These insights provide a\nstructured foundation and Responsible GenAI Guide (ResAI) for organizations to\nalign GenAI initiatives with ethical, legal, and operational best practices.", "AI": {"tldr": "The paper proposes a governance framework (ResAI) for responsible GenAI, balancing innovation and oversight through adaptable risk tools, monitoring, and collaboration.", "motivation": "Address ethical, accountability, and societal challenges posed by rapid GenAI advancements.", "method": "Literature review, governance frameworks, and industry roundtable discussions.", "result": "Identified core principles for responsible GenAI governance, emphasizing adaptable risk assessment and cross-sector collaboration.", "conclusion": "Provides actionable recommendations and the ResAI Guide to align GenAI with ethical and operational best practices."}}
{"id": "2504.17247", "pdf": "https://arxiv.org/pdf/2504.17247", "abs": "https://arxiv.org/abs/2504.17247", "authors": ["Diogo Soares", "Leon Hetzel", "Paulina Szymczak", "Fabian Theis", "Stephan G\u00fcnnemann", "Ewa Szczurek"], "title": "Targeted AMP generation through controlled diffusion with efficient embeddings", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance.", "AI": {"tldr": "OmegAMP is a diffusion-based generative model for targeted AMP discovery, offering precise controllability, reduced false positives, and high diversity.", "motivation": "Addressing low hit rates and the need for nuanced controllability in deep learning-based AMP discovery.", "method": "Leverages diffusion-based generative models with low-dimensional embeddings, controllability mechanisms, and novel classifiers.", "result": "Achieves state-of-the-art performance in AMP discovery, enabling targeted generation with specific properties.", "conclusion": "OmegAMP significantly advances computational frameworks for combating antimicrobial resistance."}}
{"id": "2504.17565", "pdf": "https://arxiv.org/pdf/2504.17565", "abs": "https://arxiv.org/abs/2504.17565", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M", "AI": {"tldr": "The paper introduces a large-scale, difficulty-graded reasoning dataset to improve LLM training, achieving a 79.2% pass rate on a benchmark.", "motivation": "To address the lack of understanding of base model training processes and data quality in LLMs.", "method": "Constructs a dataset with 3.34M queries and 40M responses, using pass rate and CV to select valuable data, and adjusts learning rates for reasoning-focused training.", "result": "Achieves a 79.2% pass rate on AIME2024, surpassing most distilled models and nearing state-of-the-art performance.", "conclusion": "The approach enhances reasoning capabilities and promotes open-source progress by releasing datasets and methods."}}
{"id": "2504.17364", "pdf": "https://arxiv.org/pdf/2504.17364", "abs": "https://arxiv.org/abs/2504.17364", "authors": ["Ali Haider", "Muhammad Salman Ali", "Maryam Qamar", "Tahir Khalil", "Soo Ye Kim", "Jihyong Oh", "Enzo Tartaglione", "Sung-Ho Bae"], "title": "I-INR: Iterative Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) have revolutionized signal processing\nand computer vision by modeling signals as continuous, differentiable functions\nparameterized by neural networks. However, their inherent formulation as a\nregression problem makes them prone to regression to the mean, limiting their\nability to capture fine details, retain high-frequency information, and handle\nnoise effectively. To address these challenges, we propose Iterative Implicit\nNeural Representations (I-INRs) a novel plug-and-play framework that enhances\nsignal reconstruction through an iterative refinement process. I-INRs\neffectively recover high-frequency details, improve robustness to noise, and\nachieve superior reconstruction quality. Our framework seamlessly integrates\nwith existing INR architectures, delivering substantial performance gains\nacross various tasks. Extensive experiments show that I-INRs outperform\nbaseline methods, including WIRE, SIREN, and Gauss, in diverse computer vision\napplications such as image restoration, image denoising, and object occupancy\nprediction.", "AI": {"tldr": "The paper introduces Iterative Implicit Neural Representations (I-INRs) to improve signal reconstruction by addressing regression-to-the-mean issues in INRs, enhancing detail retention and noise robustness.", "motivation": "INRs struggle with fine details, high-frequency information, and noise due to their regression-based formulation.", "method": "Proposes I-INRs, a plug-and-play iterative refinement framework compatible with existing INR architectures.", "result": "I-INRs outperform baselines (WIRE, SIREN, Gauss) in tasks like image restoration, denoising, and occupancy prediction.", "conclusion": "I-INRs offer superior reconstruction quality and robustness, making them a promising advancement for INR-based applications."}}
{"id": "2504.17055", "pdf": "https://arxiv.org/pdf/2504.17055", "abs": "https://arxiv.org/abs/2504.17055", "authors": ["Ayushi Agrawal", "Aditya Kondai", "Kavita Vemuri"], "title": "Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "AI-powered facial assessment tools are reshaping how individuals evaluate\nappearance and internalize social judgments. This study examines the\npsychological impact of such tools on self-objectification, self-esteem, and\nemotional responses, with attention to gender differences. Two samples used\ndistinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9\nyears), and another more neutral (N=51; M=19.9 years). Participants completed\nvalidated self-objectification and self-esteem scales and custom items\nmeasuring emotion, digital/physical appearance enhancement (DAE, PAEE), and\nperceived social emotion (PSE). Results revealed consistent links between high\nself-objectification, low self-esteem, and increased appearance enhancement\nbehaviors across both versions. Despite softer framing, the newer tool still\nevoked negative emotional responses (U=1466.5, p=0.013), indicating implicit\nfeedback may reinforce appearance-related insecurities. Gender differences\nemerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital\nenhancement and less likely to perceive emotional impact in others. These\nfindings reveal how AI tools may unintentionally reinforce and amplify existing\nsocial biases and underscore the critical need for responsible AI design and\ndevelopment. Future research will investigate how human ideologies embedded in\nthe training data of such tools shape their evaluative outputs, and how these,\nin turn, influence user attitudes and decisions.", "AI": {"tldr": "AI facial assessment tools impact self-objectification and self-esteem, with gender differences in emotional and behavioral responses.", "motivation": "To understand the psychological effects of AI facial assessment tools on self-perception and social judgments, focusing on gender differences.", "method": "Two samples used different versions of a facial analysis tool (critical vs. neutral). Participants completed scales for self-objectification, self-esteem, and custom measures for emotion, appearance enhancement, and perceived social emotion.", "result": "High self-objectification and low self-esteem linked to increased appearance enhancement. Negative emotional responses persisted even with neutral framing. Gender differences in digital enhancement and perceived social emotion.", "conclusion": "AI tools may unintentionally reinforce social biases, highlighting the need for responsible AI design. Future research will explore how training data ideologies affect outputs and user decisions."}}
{"id": "2504.17258", "pdf": "https://arxiv.org/pdf/2504.17258", "abs": "https://arxiv.org/abs/2504.17258", "authors": ["Md Ashiqur Rahman", "Raymond A. Yeh"], "title": "Group Downsampling with Equivariant Anti-aliasing", "categories": ["cs.LG", "cs.CV", "math.GR"], "comment": null, "summary": "Downsampling layers are crucial building blocks in CNN architectures, which\nhelp to increase the receptive field for learning high-level features and\nreduce the amount of memory/computation in the model. In this work, we study\nthe generalization of the uniform downsampling layer for group equivariant\narchitectures, e.g., G-CNNs. That is, we aim to downsample signals (feature\nmaps) on general finite groups with anti-aliasing. This involves the following:\n(a) Given a finite group and a downsampling rate, we present an algorithm to\nform a suitable choice of subgroup. (b) Given a group and a subgroup, we study\nthe notion of bandlimited-ness and propose how to perform anti-aliasing.\nNotably, our method generalizes the notion of downsampling based on classical\nsampling theory. When the signal is on a cyclic group, i.e., periodic, our\nmethod recovers the standard downsampling of an ideal low-pass filter followed\nby a subsampling operation. Finally, we conducted experiments on image\nclassification tasks demonstrating that the proposed downsampling operation\nimproves accuracy, better preserves equivariance, and reduces model size when\nincorporated into G-equivariant networks", "AI": {"tldr": "The paper generalizes uniform downsampling for group-equivariant CNNs, proposing an algorithm for subgroup selection and anti-aliasing, improving accuracy and equivariance.", "motivation": "To extend downsampling layers to group-equivariant architectures (e.g., G-CNNs) while preserving equivariance and reducing aliasing.", "method": "Presents an algorithm for subgroup selection and introduces anti-aliasing for signals on finite groups, generalizing classical sampling theory.", "result": "Experiments show improved accuracy, better equivariance preservation, and reduced model size in G-equivariant networks.", "conclusion": "The proposed downsampling method effectively generalizes classical techniques and enhances performance in group-equivariant architectures."}}
{"id": "2504.17574", "pdf": "https://arxiv.org/pdf/2504.17574", "abs": "https://arxiv.org/abs/2504.17574", "authors": ["Zhenkai Qin", "Guifang Yang", "Dongze Wu"], "title": "RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As false information continues to proliferate across social media platforms,\neffective rumor detection has emerged as a pressing challenge in natural\nlanguage processing. This paper proposes RAGAT-Mind, a multi-granular modeling\napproach for Chinese rumor detection, built upon the MindSpore deep learning\nframework. The model integrates TextCNN for local semantic extraction,\nbidirectional GRU for sequential context learning, Multi-Head Self-Attention\nfor global dependency focusing, and Bidirectional Graph Convolutional Networks\n(BiGCN) for structural representation of word co-occurrence graphs. Experiments\non the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior\nclassification performance, attaining 99.2% accuracy and a macro-F1 score of\n0.9919. The results validate the effectiveness of combining hierarchical\nlinguistic features with graph-based semantic structures. Furthermore, the\nmodel exhibits strong generalization and interpretability, highlighting its\npractical value for real-world rumor detection applications.", "AI": {"tldr": "RAGAT-Mind, a multi-granular model for Chinese rumor detection, combines TextCNN, GRU, self-attention, and BiGCN, achieving 99.2% accuracy on Weibo1-Rumor dataset.", "motivation": "Addressing the challenge of false information on social media by improving rumor detection in Chinese text.", "method": "Integrates TextCNN, bidirectional GRU, Multi-Head Self-Attention, and BiGCN for hierarchical and graph-based feature extraction.", "result": "Achieves 99.2% accuracy and 0.9919 macro-F1 score, demonstrating superior performance.", "conclusion": "RAGAT-Mind effectively combines linguistic and structural features, showing strong generalization and practical value for rumor detection."}}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365", "abs": "https://arxiv.org/abs/2504.17365", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance.", "AI": {"tldr": "TimeSoccer is an end-to-end MLLM for soccer video captioning, combining timestamp prediction and caption generation in one pass, with a motion-aware frame compression module for long videos.", "motivation": "Existing soccer MLLMs rely on temporal priors or complex two-step methods, lacking end-to-end processing and global context for full-match videos.", "method": "TimeSoccer uses MoFA-Select for adaptive frame compression and joint timestamp-caption prediction, enabling global context modeling in 45-minute matches.", "result": "TimeSoccer achieves SoTA performance in SDVC, producing high-quality commentary with precise temporal alignment and semantic relevance.", "conclusion": "TimeSoccer addresses limitations of prior methods, offering an efficient, end-to-end solution for soccer video captioning."}}
{"id": "2504.17070", "pdf": "https://arxiv.org/pdf/2504.17070", "abs": "https://arxiv.org/abs/2504.17070", "authors": ["Mohaiminul Al Nahian", "Zainab Altaweel", "David Reitano", "Sabbir Ahmed", "Saumitra Lohokare", "Shiqi Zhang", "Adnan Siraj Rakin"], "title": "Robo-Troj: Attacking LLM-based Task Planners", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robots need task planning methods to achieve goals that require more than\nindividual actions. Recently, large language models (LLMs) have demonstrated\nimpressive performance in task planning. LLMs can generate a step-by-step\nsolution using a description of actions and the goal. Despite the successes in\nLLM-based task planning, there is limited research studying the security\naspects of those systems. In this paper, we develop Robo-Troj, the first\nmulti-trigger backdoor attack for LLM-based task planners, which is the main\ncontribution of this work. As a multi-trigger attack, Robo-Troj is trained to\naccommodate the diversity of robot application domains. For instance, one can\nuse unique trigger words, e.g., \"herical\", to activate a specific malicious\nbehavior, e.g., cutting hand on a kitchen robot. In addition, we develop an\noptimization method for selecting the trigger words that are most effective.\nThrough demonstrating the vulnerability of LLM-based planners, we aim to\npromote the development of secured robot systems.", "AI": {"tldr": "Robo-Troj introduces a multi-trigger backdoor attack for LLM-based task planners, highlighting security vulnerabilities in such systems.", "motivation": "To address the lack of research on security aspects of LLM-based task planning in robotics.", "method": "Develops Robo-Troj, a multi-trigger backdoor attack, and an optimization method for selecting effective trigger words.", "result": "Demonstrates vulnerabilities in LLM-based planners, showcasing potential malicious behaviors like 'cutting hand' on a kitchen robot.", "conclusion": "Aims to raise awareness and promote the development of secure robot systems by exposing these vulnerabilities."}}
{"id": "2504.17261", "pdf": "https://arxiv.org/pdf/2504.17261", "abs": "https://arxiv.org/abs/2504.17261", "authors": ["Jiaqi Chen", "Xiaoye Zhu", "Yue Wang", "Tianyang Liu", "Xinhui Chen", "Ying Chen", "Chak Tou Leong", "Yifei Ke", "Joseph Liu", "Yiwen Yuan", "Julian McAuley", "Li-jia Li"], "title": "Symbolic Representation for Any-to-Any Generative Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI.", "AI": {"tldr": "A symbolic generative task description language and inference engine for multimodal tasks, outperforming conventional models in flexibility and efficiency.", "motivation": "To address the limitations of conventional generative models (high computational cost, limited flexibility) by introducing explicit symbolic representations.", "method": "Uses symbolic primitives (functions, parameters, topological logic) and a pre-trained language model to map natural language to symbolic workflows without training.", "result": "Successfully performs 12 diverse multimodal tasks, matching or outperforming state-of-the-art models in quality, efficiency, and editability.", "conclusion": "Symbolic task representations offer a cost-effective, extensible foundation for advancing generative AI capabilities."}}
{"id": "2504.17653", "pdf": "https://arxiv.org/pdf/2504.17653", "abs": "https://arxiv.org/abs/2504.17653", "authors": ["Samaneh Hosseini Moghaddam", "Kelly Lyons", "Cheryl Regehr", "Vivek Goel", "Kaitlyn Regehr"], "title": "Towards a comprehensive taxonomy of online abusive language informed by machine leaning", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of abusive language in online communications has posed\nsignificant risks to the health and wellbeing of individuals and communities.\nThe growing concern regarding online abuse and its consequences necessitates\nmethods for identifying and mitigating harmful content and facilitating\ncontinuous monitoring, moderation, and early intervention. This paper presents\na taxonomy for distinguishing key characteristics of abusive language within\nonline text. Our approach uses a systematic method for taxonomy development,\nintegrating classification systems of 18 existing multi-label datasets to\ncapture key characteristics relevant to online abusive language classification.\nThe resulting taxonomy is hierarchical and faceted, comprising 5 categories and\n17 dimensions. It classifies various facets of online abuse, including context,\ntarget, intensity, directness, and theme of abuse. This shared understanding\ncan lead to more cohesive efforts, facilitate knowledge exchange, and\naccelerate progress in the field of online abuse detection and mitigation among\nresearchers, policy makers, online platform owners, and other stakeholders.", "AI": {"tldr": "A taxonomy for classifying abusive language online is developed using 18 multi-label datasets, featuring 5 categories and 17 dimensions to improve detection and mitigation efforts.", "motivation": "Addressing the risks of abusive language in online communications by creating a shared understanding for better detection and intervention.", "method": "Systematic taxonomy development integrating classification systems from 18 existing multi-label datasets to capture key characteristics of abusive language.", "result": "A hierarchical and faceted taxonomy with 5 categories and 17 dimensions, classifying context, target, intensity, directness, and theme of abuse.", "conclusion": "The taxonomy fosters cohesive efforts, knowledge exchange, and accelerates progress in online abuse detection and mitigation among stakeholders."}}
{"id": "2504.17371", "pdf": "https://arxiv.org/pdf/2504.17371", "abs": "https://arxiv.org/abs/2504.17371", "authors": ["Oussema Dhaouadi", "Johannes Meier", "Luca Wahl", "Jacques Kaiser", "Luca Scalerandi", "Nick Wandelburg", "Zhuolun Zhou", "Nijanthan Berinpanathan", "Holger Banzhaf", "Daniel Cremers"], "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,\ntraditional datasets are usually captured by fixed sensors mounted on a car and\nare susceptible to occlusion. Additionally, such an approach can precisely\nreconstruct the dynamic environment in the close vicinity of the measurement\nvehicle only, while neglecting objects that are further away. In this paper, we\nintroduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,\nocclusion-free dataset of 6 degrees of freedom bounding box trajectories\nacquired through a novel monocular camera drone tracking pipeline. Our dataset\nincludes more than 175,000 trajectories of 14 types of traffic participants and\nsignificantly exceeds existing datasets in terms of diversity and scale,\ncontaining many unprecedented scenarios such as complex vehicle-pedestrian\ninteraction on highly populated urban streets and comprehensive parking\nmaneuvers from entry to exit. DSC3D dataset was captured in five various\nlocations in Europe and the United States and include: a parking lot, a crowded\ninner-city, a steep urban intersection, a federal highway, and a suburban\nintersection. Our 3D trajectory dataset aims to enhance autonomous driving\nsystems by providing detailed environmental 3D representations, which could\nlead to improved obstacle interactions and safety. We demonstrate its utility\nacross multiple applications including motion prediction, motion planning,\nscenario mining, and generative reactive traffic agents. Our interactive online\nvisualization platform and the complete dataset are publicly available at\napp.deepscenario.com, facilitating research in motion prediction, behavior\nmodeling, and safety validation.", "AI": {"tldr": "DSC3D is a high-quality, occlusion-free 3D trajectory dataset for autonomous driving, captured via drone tracking, offering diverse scenarios and surpassing existing datasets in scale and variety.", "motivation": "Traditional datasets suffer from occlusion and limited coverage, hindering accurate 3D trajectory data for autonomous driving. DSC3D addresses these gaps by providing comprehensive, high-quality data.", "method": "A novel monocular camera drone tracking pipeline was used to capture 175,000+ trajectories of 14 traffic participant types across five diverse locations.", "result": "DSC3D exceeds existing datasets in diversity and scale, featuring unprecedented scenarios like complex urban interactions and parking maneuvers.", "conclusion": "DSC3D enhances autonomous driving systems by enabling detailed 3D environmental representations, improving safety and obstacle interactions, and supporting applications like motion prediction and behavior modeling."}}
{"id": "2504.17077", "pdf": "https://arxiv.org/pdf/2504.17077", "abs": "https://arxiv.org/abs/2504.17077", "authors": ["Dongjin Seo", "Soobin Um", "Sangbin Lee", "Jong Chul Ye", "Haejun Chung"], "title": "Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models", "categories": ["physics.optics", "cs.AI", "physics.comp-ph"], "comment": "25 pages, 7 Figures", "summary": "Designing free-form photonic devices is fundamentally challenging due to the\nvast number of possible geometries and the complex requirements of fabrication\nconstraints. Traditional inverse-design approaches--whether driven by human\nintuition, global optimization, or adjoint-based gradient methods--often\ninvolve intricate binarization and filtering steps, while recent deep learning\nstrategies demand prohibitively large numbers of simulations (10^5 to 10^6). To\novercome these limitations, we present AdjointDiffusion, a physics-guided\nframework that integrates adjoint sensitivity gradients into the sampling\nprocess of diffusion models. AdjointDiffusion begins by training a diffusion\nnetwork on a synthetic, fabrication-aware dataset of binary masks. During\ninference, we compute the adjoint gradient of a candidate structure and inject\nthis physics-based guidance at each denoising step, steering the generative\nprocess toward high figure-of-merit (FoM) solutions without additional\npost-processing. We demonstrate our method on two canonical photonic design\nproblems--a bent waveguide and a CMOS image sensor color router--and show that\nour method consistently outperforms state-of-the-art nonlinear optimizers (such\nas MMA and SLSQP) in both efficiency and manufacturability, while using orders\nof magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning\napproaches (approximately 10^5 to 10^6). By eliminating complex binarization\nschedules and minimizing simulation overhead, AdjointDiffusion offers a\nstreamlined, simulation-efficient, and fabrication-aware pipeline for\nnext-generation photonic device design. Our open-source implementation is\navailable at https://github.com/dongjin-seo2020/AdjointDiffusion.", "AI": {"tldr": "AdjointDiffusion integrates adjoint gradients into diffusion models for efficient, fabrication-aware photonic device design, outperforming traditional methods with fewer simulations.", "motivation": "Overcome limitations of traditional inverse-design methods and deep learning approaches, which involve complex steps or require excessive simulations.", "method": "Trains a diffusion model on synthetic binary masks, then injects adjoint gradients during denoising to guide the design process.", "result": "Outperforms state-of-the-art optimizers in efficiency and manufacturability, using far fewer simulations (200 vs. 100,000-1,000,000).", "conclusion": "AdjointDiffusion provides a streamlined, simulation-efficient, and fabrication-aware solution for photonic device design."}}
{"id": "2504.17274", "pdf": "https://arxiv.org/pdf/2504.17274", "abs": "https://arxiv.org/abs/2504.17274", "authors": ["Siddharth Vishwanath", "Jonathan Hehir"], "title": "Signal Recovery from Random Dot-Product Graphs Under Local Differential Privacy", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH", "68P27, 62H22, 62C20, 62R07"], "comment": null, "summary": "We consider the problem of recovering latent information from graphs under\n$\\varepsilon$-edge local differential privacy where the presence of\nrelationships/edges between two users/vertices remains confidential, even from\nthe data curator. For the class of generalized random dot-product graphs, we\nshow that a standard local differential privacy mechanism induces a specific\ngeometric distortion in the latent positions. Leveraging this insight, we show\nthat consistent recovery of the latent positions is achievable by appropriately\nadjusting the statistical inference procedure for the privatized graph.\nFurthermore, we prove that our procedure is nearly minimax-optimal under local\nedge differential privacy constraints. Lastly, we show that this framework\nallows for consistent recovery of geometric and topological information\nunderlying the latent positions, as encoded in their persistence diagrams. Our\nresults extend previous work from the private community detection literature to\na substantially richer class of models and inferential tasks.", "AI": {"tldr": "The paper addresses recovering latent information from graphs under \u03b5-edge local differential privacy, ensuring edge confidentiality. It shows consistent recovery of latent positions in generalized random dot-product graphs and proves near-optimality under privacy constraints.", "motivation": "To protect edge confidentiality in graphs while enabling recovery of latent information, extending privacy-preserving techniques beyond community detection.", "method": "Uses a standard local differential privacy mechanism, analyzes induced geometric distortion, and adjusts statistical inference for privatized graphs.", "result": "Achieves consistent recovery of latent positions and proves near minimax-optimality under privacy constraints. Also recovers geometric/topological information via persistence diagrams.", "conclusion": "The framework extends privacy-preserving inference to richer graph models and tasks, demonstrating practical utility."}}
{"id": "2504.17665", "pdf": "https://arxiv.org/pdf/2504.17665", "abs": "https://arxiv.org/abs/2504.17665", "authors": ["Zena Al-Khalili", "Nick Howell", "Dietrich Klakow"], "title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "categories": ["cs.CL"], "comment": null, "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain.", "AI": {"tldr": "The paper evaluates code-assisted LLMs' program generation for math reasoning tasks, focusing on grounding to math rules and its impact on performance.", "motivation": "Current evaluations of code-assisted LLMs focus on execution correctness, neglecting rigorous analysis of generated programs. This work aims to fill that gap.", "method": "Analyzes programs generated by five LLMs on two math datasets, assessing grounding to math rules manually and automatically.", "result": "Grounding varies by LLM capability and problem difficulty; closed-source models perform better. Ungrounded programs doubled on harder problems (MATH500 vs. ASDiv).", "conclusion": "Highlights the need for deeper evaluation beyond execution accuracy to understand LLMs' math reasoning capabilities and limitations."}}
{"id": "2504.17395", "pdf": "https://arxiv.org/pdf/2504.17395", "abs": "https://arxiv.org/abs/2504.17395", "authors": ["Yiming Zhao", "Guorong Li", "Laiyun Qing", "Amin Beheshti", "Jian Yang", "Michael Sheng", "Yuankai Qi", "Qingming Huang"], "title": "SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object Counting", "categories": ["cs.CV"], "comment": null, "summary": "Open-world object counting leverages the robust text-image alignment of\npre-trained vision-language models (VLMs) to enable counting of arbitrary\ncategories in images specified by textual queries. However, widely adopted\nnaive fine-tuning strategies concentrate exclusively on text-image consistency\nfor categories contained in training, which leads to limited generalizability\nfor unseen categories. In this work, we propose a plug-and-play Semantic-Driven\nVisual Prompt Tuning framework (SDVPT) that transfers knowledge from the\ntraining set to unseen categories with minimal overhead in parameters and\ninference time. First, we introduce a two-stage visual prompt learning strategy\ncomposed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided\nPrompt Refinement (TGPR). The CSPI generates category-specific visual prompts,\nand then TGPR distills latent structural patterns from the VLM's text encoder\nto refine these prompts. During inference, we dynamically synthesize the visual\nprompts for unseen categories based on the semantic correlation between unseen\nand training categories, facilitating robust text-image alignment for unseen\ncategories. Extensive experiments integrating SDVPT with all available\nopen-world object counting models demonstrate its effectiveness and\nadaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+.", "AI": {"tldr": "The paper introduces SDVPT, a plug-and-play framework for open-world object counting, enhancing generalizability to unseen categories via semantic-driven visual prompt tuning.", "motivation": "Existing fine-tuning strategies for VLMs in object counting focus only on training categories, limiting generalizability to unseen categories.", "method": "Proposes SDVPT with two-stage visual prompt learning (CSPI and TGPR) to generate and refine category-specific prompts, leveraging semantic correlations for unseen categories.", "result": "SDVPT improves performance across three datasets (FSC-147, CARPK, PUCPR+) when integrated with existing models.", "conclusion": "SDVPT effectively enhances open-world object counting by transferring knowledge to unseen categories with minimal overhead."}}
{"id": "2504.17124", "pdf": "https://arxiv.org/pdf/2504.17124", "abs": "https://arxiv.org/abs/2504.17124", "authors": ["Ming Du", "Mark Wolfman", "Chengjun Sun", "Shelly D. Kelly", "Mathew J. Cherukara"], "title": "Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy", "categories": ["physics.app-ph", "cs.AI", "cs.CE", "cs.SY", "eess.SY"], "comment": null, "summary": "X-ray absorption near edge structure (XANES) spectroscopy is a powerful\ntechnique for characterizing the chemical state and symmetry of individual\nelements within materials, but requires collecting data at many energy points\nwhich can be time-consuming. While adaptive sampling methods exist for\nefficiently collecting spectroscopic data, they often lack domain-specific\nknowledge about XANES spectra structure. Here we demonstrate a\nknowledge-injected Bayesian optimization approach for adaptive XANES data\ncollection that incorporates understanding of spectral features like absorption\nedges and pre-edge peaks. We show this method accurately reconstructs the\nabsorption edge of XANES spectra using only 15-20% of the measurement points\ntypically needed for conventional sampling, while maintaining the ability to\ndetermine the x-ray energy of the sharp peak after absorption edge with errors\nless than 0.03 eV, the absorption edge with errors less than 0.1 eV; and\noverall root-mean-square errors less than 0.005 compared to compared to\ntraditionally sampled spectra. Our experiments on battery materials and\ncatalysts demonstrate the method's effectiveness for both static and dynamic\nXANES measurements, improving data collection efficiency and enabling better\ntime resolution for tracking chemical changes. This approach advances the\ndegree of automation in XANES experiments reducing the common errors of under-\nor over-sampling points in near the absorption edge and enabling dynamic\nexperiments that require high temporal resolution or limited measurement time.", "AI": {"tldr": "A knowledge-injected Bayesian optimization method for adaptive XANES data collection reduces required measurement points by 80-85% while maintaining accuracy.", "motivation": "Traditional XANES spectroscopy is time-consuming due to the need for many energy points. Existing adaptive methods lack domain-specific knowledge about spectral features.", "method": "A Bayesian optimization approach incorporating spectral feature knowledge (e.g., absorption edges, pre-edge peaks) is used for adaptive sampling.", "result": "Accurately reconstructs spectra with 15-20% of typical points, achieving errors <0.03 eV for sharp peaks, <0.1 eV for absorption edges, and RMSE <0.005.", "conclusion": "The method enhances XANES efficiency, enables high temporal resolution, and reduces under-/over-sampling errors, advancing automation in dynamic experiments."}}
{"id": "2504.17276", "pdf": "https://arxiv.org/pdf/2504.17276", "abs": "https://arxiv.org/abs/2504.17276", "authors": ["Ke-Jia Chen", "Wenhui Mu", "Zheng Liu"], "title": "HeRB: Heterophily-Resolved Structure Balancer for Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Recent research has witnessed the remarkable progress of Graph Neural\nNetworks (GNNs) in the realm of graph data representation. However, GNNs still\nencounter the challenge of structural imbalance. Prior solutions to this\nproblem did not take graph heterophily into account, namely that connected\nnodes process distinct labels or features, thus resulting in a deficiency in\neffectiveness. Upon verifying the impact of heterophily on solving the\nstructural imbalance problem, we propose to rectify the heterophily first and\nthen transfer homophilic knowledge. To the end, we devise a method named HeRB\n(Heterophily-Resolved Structure Balancer) for GNNs. HeRB consists of two\ninnovative components: 1) A heterophily-lessening augmentation module which\nserves to reduce inter-class edges and increase intra-class edges; 2) A\nhomophilic knowledge transfer mechanism to convey homophilic information from\nhead nodes to tail nodes. Experimental results demonstrate that HeRB achieves\nsuperior performance on two homophilic and six heterophilic benchmark datasets,\nand the ablation studies further validate the efficacy of two proposed\ncomponents.", "AI": {"tldr": "HeRB addresses structural imbalance in GNNs by resolving heterophily and transferring homophilic knowledge, outperforming benchmarks.", "motivation": "GNNs struggle with structural imbalance, especially due to heterophily (connected nodes with distinct labels/features), which prior solutions ignored.", "method": "HeRB includes a heterophily-lessening augmentation module and a homophilic knowledge transfer mechanism.", "result": "HeRB excels on homophilic and heterophilic datasets, with ablation studies confirming its components' effectiveness.", "conclusion": "HeRB successfully tackles structural imbalance by addressing heterophily and leveraging homophilic knowledge, proving its superiority."}}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.", "AI": {"tldr": "A Split Conformal Prediction (SCP) framework mitigates hallucination in Large Vision-Language Models (LVLMs) for VQA tasks by quantifying uncertainty and ensuring statistical guarantees.", "motivation": "LVLMs often produce hallucinated content with high confidence, posing risks in safety-critical applications like healthcare and autonomous systems.", "method": "The SCP framework uses dynamic threshold calibration and cross-modal consistency verification, partitioning data into calibration and test sets to compute nonconformity scores and construct prediction sets with statistical guarantees.", "result": "Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs show SCP enforces theoretical guarantees across all risk levels (\u03b1), achieving stable performance.", "conclusion": "The SCP framework bridges the gap between theoretical reliability and practical applicability, offering a scalable solution for hallucination detection and uncertainty-aware decision-making in multi-modal AI systems."}}
{"id": "2504.17397", "pdf": "https://arxiv.org/pdf/2504.17397", "abs": "https://arxiv.org/abs/2504.17397", "authors": ["Francesc Marti-Escofet", "Benedikt Blumenstiel", "Linus Scheibenreif", "Paolo Fraccaro", "Konrad Schindler"], "title": "Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models", "categories": ["cs.CV"], "comment": "Code available at https://github.com/IBM/peft-geofm", "summary": "Earth observation (EO) is crucial for monitoring environmental changes,\nresponding to disasters, and managing natural resources. In this context,\nfoundation models facilitate remote sensing image analysis to retrieve relevant\ngeoinformation accurately and efficiently. However, as these models grow in\nsize, fine-tuning becomes increasingly challenging due to the associated\ncomputational resources and costs, limiting their accessibility and\nscalability. Furthermore, full fine-tuning can lead to forgetting pre-trained\nfeatures and even degrade model generalization. To address this,\nParameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.\nIn this paper, we conduct extensive experiments with various foundation model\narchitectures and PEFT techniques to evaluate their effectiveness on five\ndifferent EO datasets. Our results provide a comprehensive comparison, offering\ninsights into when and how PEFT methods support the adaptation of pre-trained\ngeospatial models. We demonstrate that PEFT techniques match or even exceed\nfull fine-tuning performance and enhance model generalisation to unseen\ngeographic regions, while reducing training time and memory requirements.\nAdditional experiments investigate the effect of architecture choices such as\nthe decoder type or the use of metadata, suggesting UNet decoders and\nfine-tuning without metadata as the recommended configuration. We have\nintegrated all evaluated foundation models and techniques into the open-source\npackage TerraTorch to support quick, scalable, and cost-effective model\nadaptation.", "AI": {"tldr": "PEFT techniques match or exceed full fine-tuning performance for geospatial models, reducing resource use and improving generalization.", "motivation": "Address challenges of computational cost and feature forgetting in fine-tuning large foundation models for EO tasks.", "method": "Extensive experiments with various foundation model architectures and PEFT techniques across five EO datasets.", "result": "PEFT techniques enhance generalization, reduce training time/memory, and perform comparably to full fine-tuning.", "conclusion": "PEFT is a scalable, efficient solution for adapting pre-trained geospatial models, integrated into TerraTorch for accessibility."}}
{"id": "2504.17129", "pdf": "https://arxiv.org/pdf/2504.17129", "abs": "https://arxiv.org/abs/2504.17129", "authors": ["Seyed Yousef Soltanian", "Wenlong Zhang"], "title": "Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference", "categories": ["eess.SY", "cs.AI", "cs.GT", "cs.RO", "cs.SY", "93C41, 49N70, 49N90, 91A27"], "comment": null, "summary": "Human-robot interactions can be modeled as incomplete-information general-sum\ndynamic games since the objective functions of both agents are not explicitly\nknown to each other. However, solving for equilibrium policies for such games\npresents a major challenge, especially if the games involve nonlinear\nunderlying dynamics. To simplify the problem, existing work often assumes that\none agent is an expert with complete information about its peer, which can lead\nto biased estimates and failures in coordination. To address this challenge, we\npropose a nonlinear peer-aware cost estimation (N-PACE) algorithm for\ngeneral-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ)\napproximation of the nonlinear general-sum game, each agent explicitly models\nthe learning dynamics of its peer agent while inferring their objective\nfunctions, leading to unbiased fast learning in inferring the unknown objective\nfunction of the peer agent, which is critical for task completion and safety\nassurance. Additionally, we demonstrate how N-PACE enables \\textbf{intent\ncommunication} in such multi-agent systems by explicitly modeling the peer's\nlearning dynamics.", "AI": {"tldr": "N-PACE algorithm improves human-robot interaction by modeling peer learning dynamics and inferring objective functions in general-sum dynamic games.", "motivation": "Existing methods assume one agent is an expert, leading to biased estimates and coordination failures. N-PACE addresses this by modeling peer learning.", "method": "Uses iterative linear quadratic approximation to infer peer objectives and model learning dynamics.", "result": "Enables unbiased, fast learning of peer objectives and supports intent communication.", "conclusion": "N-PACE enhances coordination and safety in human-robot interactions by addressing incomplete information challenges."}}
{"id": "2504.17277", "pdf": "https://arxiv.org/pdf/2504.17277", "abs": "https://arxiv.org/abs/2504.17277", "authors": ["Zongliang Ji", "Andre Carlos Kajdacsy-Balla Amaral", "Anna Goldenberg", "Rahul G. Krishnan"], "title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the Conference on Health, Inference, and Learning (CHIL)\n  2025", "summary": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem.", "AI": {"tldr": "A novel method, ExOSITO, combines off-policy learning with privileged information to optimize ICU lab test orders, reducing costs while ensuring vital tests are not missed.", "motivation": "Balancing the need for critical lab test information with reducing clinical burden and costs in ICU settings.", "method": "Uses off-policy learning and causal bandit trained with offline data and clinically-approved reward functions, integrating clinical knowledge.", "result": "Outperforms physician policies and prior approaches, providing interpretable clinical insights and cost reduction.", "conclusion": "ExOSITO offers an effective, interpretable tool for clinicians to optimize lab test orders in ICUs."}}
{"id": "2504.17674", "pdf": "https://arxiv.org/pdf/2504.17674", "abs": "https://arxiv.org/abs/2504.17674", "authors": ["Jared Fernandez", "Clara Na", "Vashisth Tiwari", "Yonatan Bisk", "Sasha Luccioni", "Emma Strubell"], "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages", "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure.", "AI": {"tldr": "The paper analyzes the energy implications of inference efficiency optimizations in LLMs, showing workload-specific impacts and potential energy savings of up to 73%.", "motivation": "Addressing the rising computational and environmental costs of LLMs by focusing on real-world inference workloads, often overlooked in prior work.", "method": "Introduces a modeling approach with input-output token distributions and batch size variations, analyzing frameworks, decoding strategies, hardware, and serving settings.", "result": "Demonstrates that naive energy estimates underestimate real-world consumption, and optimizations can reduce energy use by up to 73%.", "conclusion": "Provides insights for sustainable LLM deployment and energy-efficient AI infrastructure design."}}
{"id": "2504.17399", "pdf": "https://arxiv.org/pdf/2504.17399", "abs": "https://arxiv.org/abs/2504.17399", "authors": ["Sven Teufel", "J\u00f6rg Gamerdinger", "Oliver Bringmann"], "title": "S2S-Net: Addressing the Domain Gap of Heterogeneous Sensor Systems in LiDAR-Based Collective Perception", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Collective Perception (CP) has emerged as a promising approach to overcome\nthe limitations of individual perception in the context of autonomous driving.\nVarious approaches have been proposed to realize collective perception;\nhowever, the Sensor2Sensor domain gap that arises from the utilization of\ndifferent sensor systems in Connected and Automated Vehicles (CAVs) remains\nmostly unaddressed. This is primarily due to the paucity of datasets containing\nheterogeneous sensor setups among the CAVs. The recently released SCOPE\ndatasets address this issue by providing data from three different LiDAR\nsensors for each CAV. This study is the first to tackle the Sensor2Sensor\ndomain gap in vehicle to vehicle (V2V) collective perception. First, we present\nour sensor-domain robust architecture S2S-Net. Then an in-depth analysis of the\nSensor2Sensor domain adaptation capabilities of S2S-Net on the SCOPE dataset is\nconducted. S2S-Net demonstrates the capability to maintain very high\nperformance in unseen sensor domains and achieved state-of-the-art results on\nthe SCOPE dataset.", "AI": {"tldr": "S2S-Net addresses the Sensor2Sensor domain gap in V2V collective perception, achieving high performance on the SCOPE dataset.", "motivation": "Overcome the Sensor2Sensor domain gap in collective perception for CAVs, which arises from heterogeneous sensor setups.", "method": "Propose S2S-Net, a sensor-domain robust architecture, and evaluate its domain adaptation capabilities on the SCOPE dataset.", "result": "S2S-Net maintains high performance in unseen sensor domains and achieves state-of-the-art results.", "conclusion": "S2S-Net effectively addresses the Sensor2Sensor gap, enhancing collective perception in CAVs."}}
{"id": "2504.17170", "pdf": "https://arxiv.org/pdf/2504.17170", "abs": "https://arxiv.org/abs/2504.17170", "authors": ["Robert Kaufman"], "title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "PhD Dissertation from University of California, San Diego; 175 pages", "summary": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]", "AI": {"tldr": "The paper argues for adaptable, personalized communication in autonomous vehicles (AVs) to meet diverse rider needs, supported by empirical studies on driving performance, trust, and context-sensitive designs.", "motivation": "Current human-AV research treats all people and situations uniformly, failing to address diverse rider needs and contextual demands. Understanding the human-AV system can improve communication strategies.", "method": "Three empirical studies: identifying optimal communication strategies for extreme environments, analyzing faulty communication systems, and using ML to predict trust based on personal factors.", "result": "Task-sensitive, context-aware, and personalized AV communications enhance performance, confidence, and trust. ML reveals individual traits influencing trust.", "conclusion": "Transparent, adaptable, and personalized AV systems are necessary. The research provides insights for designers and policymakers and advances theories of human-machine interaction."}}
{"id": "2504.17300", "pdf": "https://arxiv.org/pdf/2504.17300", "abs": "https://arxiv.org/abs/2504.17300", "authors": ["Wencong You", "Daniel Lowd"], "title": "The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes", "categories": ["cs.LG"], "comment": "Accepted at SaTML 2025", "summary": "Backdoor attacks on text classifiers can cause them to predict a predefined\nlabel when a particular \"trigger\" is present. Prior attacks often rely on\ntriggers that are ungrammatical or otherwise unusual, leading to conspicuous\nattacks. As a result, human annotators, who play a critical role in curating\ntraining data in practice, can easily detect and filter out these unnatural\ntexts during manual inspection, reducing the risk of such attacks. We argue\nthat a key criterion for a successful attack is for text with and without\ntriggers to be indistinguishable to humans. However, prior work neither\ndirectly nor comprehensively evaluated attack subtlety and invisibility with\nhuman involvement. We bridge the gap by conducting thorough human evaluations\nto assess attack subtlety. We also propose \\emph{AttrBkd}, consisting of three\nrecipes for crafting subtle yet effective trigger attributes, such as\nextracting fine-grained attributes from existing baseline backdoor attacks. Our\nhuman evaluations find that AttrBkd with these baseline-derived attributes is\noften more effective (higher attack success rate) and more subtle (fewer\ninstances detected by humans) than the original baseline backdoor attacks,\ndemonstrating that backdoor attacks can bypass detection by being inconspicuous\nand appearing natural even upon close inspection, while still remaining\neffective. Our human annotation also provides information not captured by\nautomated metrics used in prior work, and demonstrates the misalignment of\nthese metrics with human judgment.", "AI": {"tldr": "The paper introduces AttrBkd, a method for creating subtle backdoor attacks in text classifiers by using natural triggers, making them harder for humans to detect while maintaining effectiveness.", "motivation": "Prior backdoor attacks used unnatural triggers easily detected by humans, limiting their practicality. The paper aims to create attacks that are both effective and indistinguishable from normal text.", "method": "Proposes AttrBkd, which crafts subtle triggers by extracting fine-grained attributes from existing backdoor attacks. Human evaluations assess subtlety and effectiveness.", "result": "AttrBkd outperforms baseline attacks in both attack success rate and subtlety, showing triggers can evade human detection while remaining effective.", "conclusion": "The study highlights the importance of human evaluation in assessing backdoor attacks, revealing misalignment between automated metrics and human judgment."}}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685", "abs": "https://arxiv.org/abs/2504.17685", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.", "AI": {"tldr": "Small language model (SLM) ensembles using Ensemble Bayesian Inference (EBI) achieve accuracy comparable to large proprietary models, even with models of lower individual performance.", "motivation": "To explore cost-effective alternatives to proprietary large language models (LLMs) by leveraging ensembles of smaller models.", "method": "Proposes Ensemble Bayesian Inference (EBI), a Bayesian approach to combine judgments from multiple SLMs, tested on tasks like aptitude assessments and consumer profile analysis in Japanese and English.", "result": "EBI improves performance beyond individual SLMs, even incorporating models with negative Lift values, and works across languages.", "conclusion": "EBI offers a resource-efficient way to build high-performance AI systems and utilize lower-performing models effectively."}}
{"id": "2504.17401", "pdf": "https://arxiv.org/pdf/2504.17401", "abs": "https://arxiv.org/abs/2504.17401", "authors": ["Xu Wang", "Jialang Xu", "Shuai Zhang", "Baoru Huang", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets.", "AI": {"tldr": "StereoMamba, a novel architecture for stereo disparity estimation in RAMIS, combines FE-Mamba and MFF modules to achieve high accuracy, robustness, and speed.", "motivation": "Current deep learning methods for stereo disparity estimation in RAMIS lack an optimal balance between accuracy, robustness, and inference speed.", "method": "Proposes StereoMamba with FE-Mamba for long-range spatial dependencies and MFF for multi-scale feature fusion.", "result": "Achieves superior performance on SCARED benchmark (EPE 2.64 px, depth MAE 2.55 mm) and high SSIM/PSNR on other datasets.", "conclusion": "StereoMamba strikes an optimal balance between accuracy, robustness, and efficiency, demonstrating strong generalization."}}
{"id": "2504.17198", "pdf": "https://arxiv.org/pdf/2504.17198", "abs": "https://arxiv.org/abs/2504.17198", "authors": ["XiangRui Zhang", "HaoYu Chen", "Yongzhong He", "Wenjia Niu", "Qiang Li"], "title": "Automatically Generating Rules of Malicious Software Packages via Large Language Model", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": "14 pages, 11 figures", "summary": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories.", "AI": {"tldr": "RuleLLM uses LLMs to automate rule generation for OSS ecosystems, outperforming SOTA tools with high precision and recall.", "motivation": "Existing security tools rely on expert-crafted rules, which are inadequate for emerging software supply chain attacks.", "method": "RuleLLM extracts metadata and code snippets from malware to generate YARA and Semgrep rules via three subtasks: crafting, refining, and aligning rules.", "result": "RuleLLM generated 763 rules (85.2% precision, 91.8% recall) from 1,633 malicious packages, surpassing SOTA tools.", "conclusion": "RuleLLM is effective for automated rule generation, with a proposed taxonomy of 11 categories and 38 subcategories."}}
{"id": "2504.17305", "pdf": "https://arxiv.org/pdf/2504.17305", "abs": "https://arxiv.org/abs/2504.17305", "authors": ["Dinan Li", "Panagiotis Kakosimos", "Luca Peretti"], "title": "Machine learning-based condition monitoring of powertrains in modern electric drives", "categories": ["cs.LG"], "comment": "2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The recent technological advances in digitalization have revolutionized the\nindustrial sector. Leveraging data analytics has now enabled the collection of\ndeep insights into the performance and, as a result, the optimization of\nassets. Industrial drives, for example, already accumulate all the necessary\ninformation to control electric machines. These signals include but are not\nlimited to currents, frequency, and temperature. Integrating machine learning\n(ML) models responsible for predicting the evolution of those directly\ncollected or implicitly derived parameters enhances the smartness of industrial\nsystems even further. In this article, data already residing in most modern\nelectric drives has been used to develop a data-driven thermal model of a power\nmodule. A test bench has been designed and used specifically for training and\nvalidating the thermal digital twin undergoing various static and dynamic\noperating profiles. Different approaches, from traditional linear models to\ndeep neural networks, have been implemented to emanate the best ML model for\nestimating the case temperature of a power module. Several evaluation metrics\nwere then used to assess the investigated methods' performance and\nimplementation in industrial embedded systems.", "AI": {"tldr": "The paper explores using machine learning to create a thermal model for power modules in industrial drives, comparing traditional and deep learning methods for temperature estimation.", "motivation": "To enhance industrial system smartness by leveraging existing drive data for predictive thermal modeling.", "method": "Developed a data-driven thermal model using various ML approaches (linear models to deep neural networks) and validated it with static/dynamic operating profiles.", "result": "Evaluated performance of different ML models for estimating power module case temperature, assessing suitability for industrial embedded systems.", "conclusion": "Demonstrated the feasibility of using ML for accurate thermal modeling in industrial applications, with potential for system optimization."}}
{"id": "2504.17704", "pdf": "https://arxiv.org/pdf/2504.17704", "abs": "https://arxiv.org/abs/2504.17704", "authors": ["Cheng Wang", "Yue Liu", "Baolong Li", "Duzhen Zhang", "Zhongzhi Li", "Junfeng Fang"], "title": "Safety in Large Reasoning Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models.", "AI": {"tldr": "A survey of safety risks, attacks, and defenses in Large Reasoning Models (LRMs), organized into a taxonomy for clarity.", "motivation": "Address growing concerns about vulnerabilities and safety in LRMs as their reasoning capabilities advance.", "method": "Comprehensive survey and taxonomy of safety risks, attacks, and defense strategies in LRMs.", "result": "Structured understanding of LRM safety landscape to aid future research.", "conclusion": "The work facilitates improved security and reliability of LRMs for real-world applications."}}
{"id": "2504.17414", "pdf": "https://arxiv.org/pdf/2504.17414", "abs": "https://arxiv.org/abs/2504.17414", "authors": ["Min Wei", "Chaohui Yu", "Jingkai Zhou", "Fan Wang"], "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://2y7c3.github.io/3DV-TON/", "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/", "AI": {"tldr": "3DV-TON is a diffusion-based framework for high-fidelity, temporally consistent video try-on, using animatable 3D meshes and a robust masking strategy to outperform existing methods.", "motivation": "Existing video try-on methods struggle with complex clothing patterns and diverse poses, leading to inconsistent results.", "method": "Uses animatable 3D meshes for frame-level guidance, an adaptive pipeline for dynamic 3D guidance, and a rectangular masking strategy to reduce artifacts.", "result": "Outperforms existing methods, demonstrated by quantitative and qualitative results on the HR-VVT dataset.", "conclusion": "3DV-TON advances video try-on with high-quality, consistent results and introduces a new benchmark dataset."}}
{"id": "2504.17314", "pdf": "https://arxiv.org/pdf/2504.17314", "abs": "https://arxiv.org/abs/2504.17314", "authors": ["Miaoyun Zhao", "Qiang Zhang", "Chenrong Li"], "title": "Class-Conditional Distribution Balancing for Group Robust Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision.", "AI": {"tldr": "A novel method addresses spurious correlations in models by balancing class-conditional distributions without needing bias annotations or predictions, achieving state-of-the-art performance.", "motivation": "Spurious correlations hinder robust generalization, and existing solutions rely on expensive bias annotations or impractical pretrained models.", "method": "Proposes a sample reweighting strategy to balance class-conditional distributions, reducing mutual information between spurious factors and labels.", "result": "The method outperforms or matches supervised approaches, effectively dismantling spurious correlations.", "conclusion": "The approach offers a practical, annotation-free solution for robust learning, especially in resource-limited domains."}}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720", "abs": "https://arxiv.org/abs/2504.17720", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "Vil\u00e9m Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "title": "Multilingual Performance Biases of Large Language Models in Education", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.", "AI": {"tldr": "LLMs perform variably in non-English educational tasks, with performance linked to training data. Practitioners should verify effectiveness in target languages before deployment.", "motivation": "To assess if LLMs are suitable for educational tasks in non-English languages, given their English-centric nature.", "method": "Evaluated popular LLMs on four educational tasks (misconception identification, feedback, tutoring, grading) across six non-English languages and English.", "result": "Performance correlates with training data representation; lower-resource languages show poorer results. Significant drops from English performance observed.", "conclusion": "Practitioners should verify LLM performance in target languages before educational deployment."}}
{"id": "2504.17432", "pdf": "https://arxiv.org/pdf/2504.17432", "abs": "https://arxiv.org/abs/2504.17432", "authors": ["Tiancheng Gu", "Kaicheng Yang", "Ziyong Feng", "Xingjun Wang", "Yanzhao Zhang", "Dingkun Long", "Yingda Chen", "Weidong Cai", "Jiankang Deng"], "title": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs", "categories": ["cs.CV"], "comment": "13 pages, 8 figures, Project page: https://garygutc.github.io/UniME", "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.", "AI": {"tldr": "UniME, a two-stage framework using MLLMs, improves multimodal representation learning by addressing CLIP's limitations through knowledge distillation and hard negative enhanced instruction tuning.", "motivation": "CLIP's limitations (text truncation, isolated encoding, poor compositionality) and unexplored potential of MLLMs for transferable multimodal representations.", "method": "Two-stage approach: (1) textual discriminative knowledge distillation from an LLM teacher, (2) hard negative enhanced instruction tuning to improve discriminative power.", "result": "UniME outperforms on MMEB benchmark and retrieval tasks, showing superior discriminative and compositional capabilities.", "conclusion": "UniME effectively enhances multimodal representation learning, addressing CLIP's shortcomings and advancing MLLM applications."}}
{"id": "2504.17304", "pdf": "https://arxiv.org/pdf/2504.17304", "abs": "https://arxiv.org/abs/2504.17304", "authors": ["Yimin Shi", "Yang Fei", "Shiqi Zhang", "Haixun Wang", "Xiaokui Xiao"], "title": "You Are What You Bought: Generating Customer Personas for E-commerce Applications", "categories": ["cs.IR", "cs.AI"], "comment": "SIGIR 2025", "summary": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K.", "AI": {"tldr": "The paper introduces customer personas for explicit, human-readable user representations in e-commerce, proposing GPLR and RevAff methods to improve efficiency and accuracy in recommendations and segmentation.", "motivation": "Existing deep learning embeddings are opaque and hard to integrate with external knowledge, limiting applications like recommendations and segmentation.", "method": "Proposes GPLR, using LLMs for persona inference and random walks for scalability, and RevAff for efficient error-guaranteed computations.", "result": "Persona-based representations improve graph convolution-based recommendations by up to 12% in NDCG@K and F1-Score@K.", "conclusion": "Customer personas offer interpretable and effective user representations, enhancing e-commerce applications."}}
{"id": "2504.17355", "pdf": "https://arxiv.org/pdf/2504.17355", "abs": "https://arxiv.org/abs/2504.17355", "authors": ["Xiaohan Huang", "Dongjie Wang", "Zhiyuan Ning", "Ziyue Qiao", "Qingqing Long", "Haowei Zhu", "Yi Du", "Min Wu", "Yuanchun Zhou", "Meng Xiao"], "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, Keywords: Automated Feature Transformation, Tabular\n  Dataset, Reinforcement Learning", "summary": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets.", "AI": {"tldr": "TCTO is a multi-agent reinforcement learning framework for automated feature engineering, using a dynamic interaction graph to optimize transformations and improve downstream ML performance.", "motivation": "Existing feature transformation methods ignore dynamic dependencies between steps, leading to inefficiencies and redundancy.", "method": "TCTO employs a graph-driven approach with pruning and backtracking to optimize feature transformations, reusing high-utility subgraphs from history.", "result": "Experiments show TCTO outperforms existing methods across diverse datasets.", "conclusion": "TCTO effectively automates feature engineering by leveraging dynamic graph optimization and historical reuse."}}
{"id": "2504.17753", "pdf": "https://arxiv.org/pdf/2504.17753", "abs": "https://arxiv.org/abs/2504.17753", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula Allen-Meares", "Eulalia Puig Abril", "Olga Garcia", "Carolyn Dickens", "Andrew Boyd"], "title": "Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT", "categories": ["cs.CL"], "comment": null, "summary": "Conversational assistants are becoming more and more popular, including in\nhealthcare, partly because of the availability and capabilities of Large\nLanguage Models. There is a need for controlled, probing evaluations with real\nstakeholders which can highlight advantages and disadvantages of more\ntraditional architectures and those based on generative AI. We present a\nwithin-group user study to compare two versions of a conversational assistant\nthat allows heart failure patients to ask about salt content in food. One\nversion of the system was developed in-house with a neurosymbolic architecture,\nand one is based on ChatGPT. The evaluation shows that the in-house system is\nmore accurate, completes more tasks and is less verbose than the one based on\nChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors\nand requires fewer clarifications to complete the task. Patients show no\npreference for one over the other.", "AI": {"tldr": "A study compares a neurosymbolic in-house conversational assistant with a ChatGPT-based one for heart failure patients, finding trade-offs in accuracy, verbosity, and speech errors, with no patient preference.", "motivation": "The rise of conversational assistants in healthcare, driven by Large Language Models, necessitates evaluations to compare traditional and generative AI architectures.", "method": "A within-group user study comparing two versions of a conversational assistant (neurosymbolic in-house vs. ChatGPT-based) for heart failure patients querying salt content in food.", "result": "The in-house system is more accurate, completes more tasks, and is less verbose, while ChatGPT-based system makes fewer speech errors and needs fewer clarifications. Patients show no preference.", "conclusion": "Both architectures have strengths and weaknesses, suggesting context-specific utility in healthcare applications."}}
{"id": "2504.17441", "pdf": "https://arxiv.org/pdf/2504.17441", "abs": "https://arxiv.org/abs/2504.17441", "authors": ["Mingxuan Wu", "Huang Huang", "Justin Kerr", "Chung Min Kim", "Anthony Zhang", "Brent Yi", "Angjoo Kanazawa"], "title": "Predict-Optimize-Distill: A Self-Improving Cycle for 4D Object Understanding", "categories": ["cs.CV"], "comment": "See our website at:\n  https://predict-optimize-distill.github.io/pod.github.io First two authors\n  contributed equally", "summary": "Humans can resort to long-form inspection to build intuition on predicting\nthe 3D configurations of unseen objects. The more we observe the object motion,\nthe better we get at predicting its 3D state immediately. Existing systems\neither optimize underlying representations from multi-view observations or\ntrain a feed-forward predictor from supervised datasets. We introduce\nPredict-Optimize-Distill (POD), a self-improving framework that interleaves\nprediction and optimization in a mutually reinforcing cycle to achieve better\n4D object understanding with increasing observation time. Given a multi-view\nobject scan and a long-form monocular video of human-object interaction, POD\niteratively trains a neural network to predict local part poses from RGB\nframes, uses this predictor to initialize a global optimization which refines\noutput poses through inverse rendering, then finally distills the results of\noptimization back into the model by generating synthetic self-labeled training\ndata from novel viewpoints. Each iteration improves both the predictive model\nand the optimized motion trajectory, creating a virtuous cycle that bootstraps\nits own training data to learn about the pose configurations of an object. We\nalso introduce a quasi-multiview mining strategy for reducing depth ambiguity\nby leveraging long video. We evaluate POD on 14 real-world and 5 synthetic\nobjects with various joint types, including revolute and prismatic joints as\nwell as multi-body configurations where parts detach or reattach independently.\nPOD demonstrates significant improvement over a pure optimization baseline\nwhich gets stuck in local minima, particularly for longer videos. We also find\nthat POD's performance improves with both video length and successive\niterations of the self-improving cycle, highlighting its ability to scale\nperformance with additional observations and looped refinement.", "AI": {"tldr": "POD is a self-improving framework for 4D object understanding, combining prediction and optimization in a cycle to refine object pose predictions over time.", "motivation": "Humans improve 3D object state prediction with observation, but existing systems rely on multi-view observations or supervised datasets. POD aims to bridge this gap.", "method": "POD iteratively predicts part poses, refines them via global optimization, and distills results back into the model using synthetic self-labeled data.", "result": "POD outperforms optimization baselines, especially for longer videos, and improves with more observations and iterations.", "conclusion": "POD's self-improving cycle effectively scales performance with additional data and refinement, advancing 4D object understanding."}}
{"id": "2504.17331", "pdf": "https://arxiv.org/pdf/2504.17331", "abs": "https://arxiv.org/abs/2504.17331", "authors": ["S\u00fcleyman \u00d6zdel", "Kadir Burak Buldu", "Enkelejda Kasneci", "Efe Bozkir"], "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality", "categories": ["cs.HC", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility.", "AI": {"tldr": "A novel LLM-driven locomotion method in VR offers natural language navigation, matching usability of traditional methods while enhancing user engagement.", "motivation": "To improve hands-free VR locomotion by overcoming the limitations of rigid speech commands with a more natural, flexible LLM-based approach.", "method": "Proposed an LLM-powered locomotion technique, comparing it with controller-based teleportation and voice-based steering via eye-tracking (SHAP analysis) and questionnaires.", "result": "LLM-driven method matches usability, presence, and cybersickness of traditional methods while improving user attention and engagement.", "conclusion": "The LLM-based approach is a viable, comfortable, and accessible hands-free locomotion alternative in VR."}}
{"id": "2504.17370", "pdf": "https://arxiv.org/pdf/2504.17370", "abs": "https://arxiv.org/abs/2504.17370", "authors": ["Marco Carpentiero", "Virginia Bordignon", "Vincenzo Matta", "Ali H. Sayed"], "title": "Doubly Adaptive Social Learning", "categories": ["cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In social learning, a network of agents assigns probability scores (beliefs)\nto some hypotheses of interest, which rule the generation of local streaming\ndata observed by each agent. Belief formation takes place by means of an\niterative two-step procedure where: i) the agents update locally their beliefs\nby using some likelihood model; and ii) the updated beliefs are combined with\nthe beliefs of the neighboring agents, using a pooling rule. This procedure can\nfail to perform well in the presence of dynamic drifts, leading the agents to\nincorrect decision making. Here, we focus on the fully online setting where\nboth the true hypothesis and the likelihood models can change over time. We\npropose the doubly adaptive social learning ($\\text{A}^2\\text{SL}$) strategy,\nwhich infuses social learning with the necessary adaptation capabilities. This\ngoal is achieved by exploiting two adaptation stages: i) a stochastic gradient\ndescent update to learn and track the drifts in the decision model; ii) and an\nadaptive belief update to track the true hypothesis changing over time. These\nstages are controlled by two adaptation parameters that govern the evolution of\nthe error probability for each agent. We show that all agents learn\nconsistently for sufficiently small adaptation parameters, in the sense that\nthey ultimately place all their belief mass on the true hypothesis. In\nparticular, the probability of choosing the wrong hypothesis converges to\nvalues on the order of the adaptation parameters. The theoretical analysis is\nillustrated both on synthetic data and by applying the $\\text{A}^2\\text{SL}$\nstrategy to a social learning problem in the online setting using real data.", "AI": {"tldr": "The paper introduces a doubly adaptive social learning (A\u00b2SL) strategy to handle dynamic drifts in online social learning, ensuring agents consistently place belief on the true hypothesis.", "motivation": "Traditional social learning fails in dynamic environments where hypotheses and likelihood models change over time.", "method": "A\u00b2SL uses stochastic gradient descent and adaptive belief updates, controlled by two parameters, to track drifts and true hypotheses.", "result": "Agents learn consistently, with error probabilities converging to values proportional to the adaptation parameters.", "conclusion": "A\u00b2SL effectively addresses dynamic drifts in online social learning, validated by synthetic and real data."}}
{"id": "2504.17768", "pdf": "https://arxiv.org/pdf/2504.17768", "abs": "https://arxiv.org/abs/2504.17768", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.", "AI": {"tldr": "Sparse attention in Transformer LLMs enhances long-context capabilities, but trade-offs and scaling effects vary by task, model size, and sparsity level. Larger, sparser models excel for very long sequences, but no universal strategy exists. Novel scaling laws validate findings.", "motivation": "To explore the viability, efficiency-accuracy trade-offs, and scaling effects of sparse attention in Transformer LLMs for long-context tasks.", "method": "Comparative study of training-free sparse attention methods across model scales, sequence lengths, and sparsity levels on diverse long-sequence tasks.", "result": "1) Larger, sparser models outperform smaller dense ones for long sequences. 2) Higher sparsity is feasible during decoding than prefilling. 3) No universal sparsity strategy; performance degradation occurs in some tasks. 4) Novel scaling laws validate findings.", "conclusion": "Sparse attention is valuable for long-context LLMs but requires careful evaluation of trade-offs for optimal performance."}}
{"id": "2504.17447", "pdf": "https://arxiv.org/pdf/2504.17447", "abs": "https://arxiv.org/abs/2504.17447", "authors": ["De-An Huang", "Subhashree Radhakrishnan", "Zhiding Yu", "Jan Kautz"], "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG", "AI": {"tldr": "FRAG improves long input processing in LMMs by selecting relevant frames without long context, boosting performance in video and document tasks.", "motivation": "Overcome computational limits of long context LMMs by avoiding full input processing.", "method": "Frame Selection Augmented Generation (FRAG) scores and selects top frames independently, using existing LMMs without fine-tuning.", "result": "FRAG enhances performance, achieving SOTA in video (e.g., 5.8% on MLVU) and document tasks (e.g., 20% on MP-DocVQA).", "conclusion": "FRAG's simple yet effective approach advances long input processing without long context LMMs."}}
{"id": "2504.17346", "pdf": "https://arxiv.org/pdf/2504.17346", "abs": "https://arxiv.org/abs/2504.17346", "authors": ["Tran Thuy Nga Truong", "Jooyong Kim"], "title": "Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper introduces an enhanced Genetic Algorithm technique called\nDual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural\nnetworks for binary image classification tasks, such as cat vs. non-cat\nclassification. The proposed method employs only two individuals for crossover,\nrepresented by two parameter sets: Leader and Follower. The Leader focuses on\nexploitation, representing the primary optimal solution at even-indexed\npositions (0, 2, 4, ...), while the Follower promotes exploration by preserving\ndiversity and avoiding premature convergence, operating at odd-indexed\npositions (1, 3, 5, ...). Leader and Follower are modeled as two phases or\nroles. The key contributions of this work are threefold: (1) a self-adaptive\nlayer dimension mechanism that eliminates the need for manual tuning of layer\narchitectures; (2) generates two parameter sets, leader and follower parameter\nsets, with 10 layer architecture configurations (5 for each set), ranked by\nPareto dominance and cost. post-optimization; and (3) demonstrated superior\nperformance compared to traditional gradient-based methods. Experimental\nresults show that the Dual-Individual GA achieves 99.04% training accuracy and\n80% testing accuracy (cost = 0.034) on a three-layer network with architecture\n[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%\ntraining accuracy and 80% testing accuracy (cost = 0.092) on a four-layer\nnetwork with architecture [12288, 20, 7, 5, 1]. These findings highlight the\nefficiency and effectiveness of the proposed method in optimizing neural\nnetworks.", "AI": {"tldr": "The paper introduces Dual-Individual GA, a Genetic Algorithm variant for optimizing neural networks in binary image classification, outperforming gradient-based methods.", "motivation": "To enhance neural network optimization by balancing exploitation (Leader) and exploration (Follower) in a Genetic Algorithm framework.", "method": "Uses two individuals (Leader and Follower) for crossover, with self-adaptive layer dimensions and Pareto-ranked configurations.", "result": "Achieves 99.04% training and 80% testing accuracy, outperforming gradient-based methods in efficiency and cost.", "conclusion": "Dual-Individual GA is efficient and effective for neural network optimization in binary classification tasks."}}
{"id": "2504.17403", "pdf": "https://arxiv.org/pdf/2504.17403", "abs": "https://arxiv.org/abs/2504.17403", "authors": ["Hans Rosenberger", "Rodrigo Fischer", "Johanna S. Fr\u00f6hlich", "Ali Bereyhi", "Ralf R. M\u00fcller"], "title": "Coding for Computation: Efficient Compression of Neural Networks for Reconfigurable Hardware", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "comment": "Accepted at the 2025 IEEE Statistical Signal Processing (SSP)\n  Workshop, Edinburgh", "summary": "As state of the art neural networks (NNs) continue to grow in size, their\nresource-efficient implementation becomes ever more important. In this paper,\nwe introduce a compression scheme that reduces the number of computations\nrequired for NN inference on reconfigurable hardware such as FPGAs. This is\nachieved by combining pruning via regularized training, weight sharing and\nlinear computation coding (LCC). Contrary to common NN compression techniques,\nwhere the objective is to reduce the memory used for storing the weights of the\nNNs, our approach is optimized to reduce the number of additions required for\ninference in a hardware-friendly manner. The proposed scheme achieves\ncompetitive performance for simple multilayer perceptrons, as well as for large\nscale deep NNs such as ResNet-34.", "AI": {"tldr": "A compression scheme for neural networks reduces computations for FPGA inference by combining pruning, weight sharing, and linear computation coding, focusing on hardware efficiency.", "motivation": "As neural networks grow in size, efficient implementation is crucial. Current compression techniques focus on memory reduction, but this work targets computation reduction for hardware-friendly inference.", "method": "Combines pruning via regularized training, weight sharing, and linear computation coding (LCC) to minimize additions required for inference.", "result": "Achieves competitive performance for both simple multilayer perceptrons and large-scale deep networks like ResNet-34.", "conclusion": "The proposed scheme effectively reduces computations for NN inference on reconfigurable hardware while maintaining performance."}}
{"id": "2504.17038", "pdf": "https://arxiv.org/pdf/2504.17038", "abs": "https://arxiv.org/abs/2504.17038", "authors": ["Christian D. Newman", "Brandon Scholten", "Sophia Testa", "Joshua A. C. Behler", "Syreen Banabilah", "Michael L. Collard", "Michael J. Decker", "Mohamed Wiem Mkaouer", "Marcos Zampieri", "Eman Abdullah AlOmar", "Reem Alsuhaibani", "Anthony Peruma", "Jonathan I. Maletic"], "title": "SCALAR: A Part-of-speech Tagger for Identifiers", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The paper presents the Source Code Analysis and Lexical Annotation Runtime\n(SCALAR), a tool specialized for mapping (annotating) source code identifier\nnames to their corresponding part-of-speech tag sequence (grammar pattern).\nSCALAR's internal model is trained using scikit-learn's\nGradientBoostingClassifier in conjunction with a manually-curated oracle of\nidentifier names and their grammar patterns. This specializes the tagger to\nrecognize the unique structure of the natural language used by developers to\ncreate all types of identifiers (e.g., function names, variable names etc.).\nSCALAR's output is compared with a previous version of the tagger, as well as a\nmodern off-the-shelf part-of-speech tagger to show how it improves upon other\ntaggers' output for annotating identifiers. The code is available on Github", "AI": {"tldr": "SCALAR is a tool for annotating source code identifiers with part-of-speech tags, trained using a GradientBoostingClassifier and a curated dataset, outperforming other taggers.", "motivation": "To improve the accuracy of part-of-speech tagging for source code identifiers, which often have unique grammar patterns.", "method": "Uses scikit-learn's GradientBoostingClassifier trained on a manually-curated dataset of identifier names and their grammar patterns.", "result": "SCALAR outperforms both a previous version and a modern off-the-shelf part-of-speech tagger for annotating identifiers.", "conclusion": "SCALAR is an effective tool for specialized part-of-speech tagging in source code, with potential for broader applications."}}
{"id": "2504.17457", "pdf": "https://arxiv.org/pdf/2504.17457", "abs": "https://arxiv.org/abs/2504.17457", "authors": ["Zhiying Li", "Yeying Jin", "Fan Shen", "Zhi Liu", "Weibin Chen", "Pengju Zhang", "Xiaomei Zhang", "Boyu Chen", "Michael Shen", "Kejian Wu", "Zhaoxin Fan", "Jin Dong"], "title": "Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks", "categories": ["cs.CV"], "comment": "14 pages, 7 figures", "summary": "Expressive human pose and shape estimation (EHPS) is crucial for digital\nhuman generation, especially in applications like live streaming. While\nexisting research primarily focuses on reducing estimation errors, it largely\nneglects robustness and security aspects, leaving these systems vulnerable to\nadversarial attacks. To address this significant challenge, we propose the\n\\textbf{Tangible Attack (TBA)}, a novel framework designed to generate\nadversarial examples capable of effectively compromising any digital human\ngeneration model. Our approach introduces a \\textbf{Dual Heterogeneous Noise\nGenerator (DHNG)}, which leverages Variational Autoencoders (VAE) and\nControlNet to produce diverse, targeted noise tailored to the original image\nfeatures. Additionally, we design a custom \\textbf{adversarial loss function}\nto optimize the noise, ensuring both high controllability and potent\ndisruption. By iteratively refining the adversarial sample through\nmulti-gradient signals from both the noise and the state-of-the-art EHPS model,\nTBA substantially improves the effectiveness of adversarial attacks. Extensive\nexperiments demonstrate TBA's superiority, achieving a remarkable 41.0\\%\nincrease in estimation error, with an average improvement of approximately\n17.0\\%. These findings expose significant security vulnerabilities in current\nEHPS models and highlight the need for stronger defenses in digital human\ngeneration systems.", "AI": {"tldr": "The paper introduces Tangible Attack (TBA), a framework for adversarial attacks on digital human generation models, using a Dual Heterogeneous Noise Generator (DHNG) and custom adversarial loss to increase estimation errors by 41.0%.", "motivation": "Existing EHPS research focuses on accuracy but overlooks robustness and security, making systems vulnerable to attacks.", "method": "TBA employs DHNG (VAE and ControlNet) to generate targeted noise and a custom adversarial loss for optimization, refining attacks via multi-gradient signals.", "result": "TBA increases estimation errors by 41.0%, with an average improvement of 17.0%, exposing vulnerabilities in EHPS models.", "conclusion": "The study reveals critical security flaws in EHPS systems and underscores the need for stronger defenses in digital human generation."}}
{"id": "2504.17354", "pdf": "https://arxiv.org/pdf/2504.17354", "abs": "https://arxiv.org/abs/2504.17354", "authors": ["Tarik Sahin", "Jacopo Bonari", "Sebastian Brandstaeter", "Alexander Popp"], "title": "Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "The effective contact area in rough surface contact plays a critical role in\nmulti-physics phenomena such as wear, sealing, and thermal or electrical\nconduction. Although accurate numerical methods, like the Boundary Element\nMethod (BEM), are available to compute this quantity, their high computational\ncost limits their applicability in multi-query contexts, such as uncertainty\nquantification, parameter identification, and multi-scale algorithms, where\nmany repeated evaluations are required. This study proposes a surrogate\nmodeling framework for predicting the effective contact area using\nfast-to-evaluate data-driven techniques. Various machine learning algorithms\nare trained on a precomputed dataset, where the inputs are the imposed load and\nstatistical roughness parameters, and the output is the corresponding effective\ncontact area. All models undergo hyperparameter optimization to enable fair\ncomparisons in terms of predictive accuracy and computational efficiency,\nevaluated using established quantitative metrics. Among the models, the Kernel\nRidge Regressor demonstrates the best trade-off between accuracy and\nefficiency, achieving high predictive accuracy, low prediction time, and\nminimal training overhead-making it a strong candidate for general-purpose\nsurrogate modeling. The Gaussian Process Regressor provides an attractive\nalternative when uncertainty quantification is required, although it incurs\nadditional computational cost due to variance estimation. The generalization\ncapability of the Kernel Ridge model is validated on an unseen simulation\nscenario, confirming its ability to transfer to new configurations. Database\ngeneration constitutes the dominant cost in the surrogate modeling process.\nNevertheless, the approach proves practical and efficient for multi-query\ntasks, even when accounting for this initial expense.", "AI": {"tldr": "A surrogate modeling framework using machine learning is proposed to predict effective contact area in rough surfaces, with Kernel Ridge Regressor showing the best balance of accuracy and efficiency.", "motivation": "High computational costs of numerical methods like BEM limit their use in multi-query tasks, necessitating faster alternatives.", "method": "Machine learning models are trained on precomputed data (load and roughness parameters) to predict contact area, with hyperparameter optimization for fair comparison.", "result": "Kernel Ridge Regressor excels in accuracy and efficiency, while Gaussian Process Regressor suits uncertainty quantification. The framework is validated on unseen data.", "conclusion": "The surrogate modeling approach is practical for multi-query tasks despite the initial database generation cost."}}
{"id": "2504.17421", "pdf": "https://arxiv.org/pdf/2504.17421", "abs": "https://arxiv.org/abs/2504.17421", "authors": ["Yang Liu", "Bingjie Yan", "Tianyuan Zou", "Jianqing Zhang", "Zixuan Gu", "Jianbing Ding", "Xidong Wang", "Jingyi Li", "Xiaozhou Ye", "Ye Ouyang", "Qiang Yang", "Ya-Qin Zhang"], "title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications.", "AI": {"tldr": "Proposes collaboration between large and small language models to enhance efficiency and domain-specific adaptation, advocating for industry-driven research.", "motivation": "Address the inefficiency and resource demands of large language models (LLMs) by leveraging smaller models (SMs) for domain-specific tasks.", "method": "Explores strategies for synergistic collaboration between LLMs and SMs, identifying challenges and opportunities.", "result": "Highlights the potential of collaborative approaches to unlock new AI capabilities in private domains.", "conclusion": "Advocates for industry-driven research with multi-objective benchmarks on real-world private datasets."}}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy.", "AI": {"tldr": "HMI is a system for efficient multi-tenant inference of hierarchical PLMs, reducing GPU memory usage and maintaining accuracy.", "motivation": "Addressing the high computational demands of PLMs in multi-tenant environments by optimizing resource usage.", "method": "Categorizes PLM knowledge hierarchically, manages domain/task-specific knowledge, and optimizes system performance with prefetching and parallel implementations.", "result": "HMI efficiently serves up to 10,000 hPLMs on a single GPU with minimal accuracy loss.", "conclusion": "HMI provides a scalable and resource-efficient solution for multi-tenant PLM inference."}}
{"id": "2504.17474", "pdf": "https://arxiv.org/pdf/2504.17474", "abs": "https://arxiv.org/abs/2504.17474", "authors": ["Weiran Pan", "Wei Wei", "Feida Zhu", "Yong Deng"], "title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels.", "AI": {"tldr": "A novel sample selection method for image classification with noisy labels, using confidence trends instead of loss values to distinguish hard-to-learn samples from mislabeled ones.", "motivation": "Existing methods rely on loss thresholds, causing a trade-off between precision and recall in sample selection. The goal is to accurately identify correctly labeled but hard-to-learn samples.", "method": "Track confidence gaps between annotated labels and other classes during training, evaluating trends with the Mann-Kendall Test. Samples with increasing confidence gaps are considered correctly labeled.", "result": "Improves performance of existing noisy-label learning methods on standard benchmarks and real-world datasets.", "conclusion": "The method effectively alleviates the trade-off in sample selection by leveraging confidence trends, enhancing robustness in noisy-label scenarios."}}
{"id": "2504.17384", "pdf": "https://arxiv.org/pdf/2504.17384", "abs": "https://arxiv.org/abs/2504.17384", "authors": ["Hanlin Sheng", "Xinming Wu", "Hang Gao", "Haibin Di", "Sergey Fomel", "Jintao Li", "Xu Si"], "title": "On the workflow, opportunities and challenges of developing foundation model in geophysics", "categories": ["physics.geo-ph", "cs.AI"], "comment": null, "summary": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field.", "AI": {"tldr": "This paper reviews the integration of foundation models with geophysical data, covering the full workflow from data collection to deployment, addressing challenges like data diversity and physical consistency.", "motivation": "To fill the gap in comprehensive reviews on applying foundation models to geophysics and provide practical guidance for their use in the field.", "method": "Presents a systematic framework detailing data preprocessing, model architecture, pre-training, and deployment, with solutions for geophysical data challenges.", "result": "Offers targeted solutions for handling geophysical data's complexity and leverages transfer learning to improve efficiency and interpretability.", "conclusion": "The paper fills a critical review gap and advances the application of foundation models in geophysics, promoting innovation in the field."}}
{"id": "2504.17448", "pdf": "https://arxiv.org/pdf/2504.17448", "abs": "https://arxiv.org/abs/2504.17448", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Zhongle Xie", "Ke Chen", "Lidan Shou"], "title": "CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated Active Learning", "categories": ["cs.LG", "cs.DB", "cs.DC"], "comment": "Accepted by TKDE 2025", "summary": "Active learning (AL) reduces human annotation costs for machine learning\nsystems by strategically selecting the most informative unlabeled data for\nannotation, but performing it individually may still be insufficient due to\nrestricted data diversity and annotation budget. Federated Active Learning\n(FAL) addresses this by facilitating collaborative data selection and model\ntraining, while preserving the confidentiality of raw data samples. Yet,\nexisting FAL methods fail to account for the heterogeneity of data distribution\nacross clients and the associated fluctuations in global and local model\nparameters, adversely affecting model accuracy. To overcome these challenges,\nwe propose CHASe (Client Heterogeneity-Aware Data Selection), specifically\ndesigned for FAL. CHASe focuses on identifying those unlabeled samples with\nhigh epistemic variations (EVs), which notably oscillate around the decision\nboundaries during training. To achieve both effectiveness and efficiency,\n\\model{} encompasses techniques for 1) tracking EVs by analyzing inference\ninconsistencies across training epochs, 2) calibrating decision boundaries of\ninaccurate models with a new alignment loss, and 3) enhancing data selection\nefficiency via a data freeze and awaken mechanism with subset sampling.\nExperiments show that CHASe surpasses various established baselines in terms of\neffectiveness and efficiency, validated across diverse datasets, model\ncomplexities, and heterogeneous federation settings.", "AI": {"tldr": "CHASe (Client Heterogeneity-Aware Data Selection) is proposed for Federated Active Learning (FAL) to address data heterogeneity and model parameter fluctuations, improving accuracy by selecting high epistemic variation samples.", "motivation": "Existing FAL methods struggle with data distribution heterogeneity and model parameter fluctuations, leading to reduced accuracy.", "method": "CHASe tracks epistemic variations (EVs) via inference inconsistencies, calibrates decision boundaries with alignment loss, and uses a data freeze-awaken mechanism for efficient selection.", "result": "CHASe outperforms baselines in effectiveness and efficiency across diverse datasets, models, and federation settings.", "conclusion": "CHASe effectively addresses FAL challenges, enhancing model accuracy and efficiency in heterogeneous environments."}}
{"id": "2406.07494", "pdf": "https://arxiv.org/pdf/2406.07494", "abs": "https://arxiv.org/abs/2406.07494", "authors": ["Frederic Kirstein", "Jan Philip Wahle", "Bela Gipp", "Terry Ruas"], "title": "CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization", "categories": ["cs.CL", "cs.AI"], "comment": "Published in the Journal of Artificial Intelligence Research (JAIR)\n  (https://www.jair.org/index.php/jair/article/view/16674)", "summary": "Abstractive dialogue summarization is the task of distilling conversations\ninto informative and concise summaries. Although reviews have been conducted on\nthis topic, there is a lack of comprehensive work detailing the challenges of\ndialogue summarization, unifying the differing understanding of the task, and\naligning proposed techniques, datasets, and evaluation metrics with the\nchallenges. This article summarizes the research on Transformer-based\nabstractive summarization for English dialogues by systematically reviewing\n1262 unique research papers published between 2019 and 2024, relying on the\nSemantic Scholar and DBLP databases. We cover the main challenges present in\ndialog summarization (i.e., language, structure, comprehension, speaker,\nsalience, and factuality) and link them to corresponding techniques such as\ngraph-based approaches, additional training tasks, and planning strategies,\nwhich typically overly rely on BART-based encoder-decoder models. We find that\nwhile some challenges, like language, have seen considerable progress, mainly\ndue to training methods, others, such as comprehension, factuality, and\nsalience, remain difficult and hold significant research opportunities. We\ninvestigate how these approaches are typically assessed, covering the datasets\nfor the subdomains of dialogue (e.g., meeting, medical), the established\nautomatic metrics and human evaluation approaches for assessing scores and\nannotator agreement. We observe that only a few datasets span across all\nsubdomains. The ROUGE metric is the most used, while human evaluation is\nfrequently reported without sufficient detail on inner-annotator agreement and\nannotation guidelines. Additionally, we discuss the possible implications of\nthe recently explored large language models and conclude that despite a\npotential shift in relevance and difficulty, our described challenge taxonomy\nremains relevant.", "AI": {"tldr": "The paper reviews Transformer-based abstractive dialogue summarization, identifying key challenges (e.g., language, factuality) and linking them to techniques like graph-based methods. It highlights gaps in datasets, evaluation metrics, and the impact of large language models.", "motivation": "To address the lack of comprehensive work on dialogue summarization challenges and unify understanding of techniques, datasets, and evaluation metrics.", "method": "Systematic review of 1262 papers (2019-2024) from Semantic Scholar and DBLP, analyzing challenges, techniques, and evaluation methods.", "result": "Progress in language-related challenges but persistent issues in comprehension, factuality, and salience. Limited datasets, overuse of ROUGE, and insufficient human evaluation detail.", "conclusion": "Despite advances, key challenges remain, and the taxonomy stays relevant even with large language models."}}
{"id": "2504.17502", "pdf": "https://arxiv.org/pdf/2504.17502", "abs": "https://arxiv.org/abs/2504.17502", "authors": ["Aviv Slobodkin", "Hagai Taitelbaum", "Yonatan Bitton", "Brian Gordon", "Michal Sokolik", "Nitzan Bitton Guetta", "Almog Gueta", "Royi Rassin", "Itay Laish", "Dani Lischinski", "Idan Szpektor"], "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\n\\emph{Animal}, \\emph{Object}), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.", "AI": {"tldr": "RefVNLI is a new metric for evaluating subject-driven T2I generation, addressing limitations of existing methods by assessing both textual alignment and subject preservation effectively.", "motivation": "Current evaluation methods for subject-driven T2I generation are unreliable, focusing on only one aspect, misaligning with human judgments, or being costly.", "method": "RefVNLI is introduced, trained on a large-scale dataset from video-reasoning benchmarks and image perturbations.", "result": "RefVNLI outperforms baselines, achieving up to 6.4-point gains in textual alignment and 8.5-point gains in subject consistency, with 87% human preference alignment.", "conclusion": "RefVNLI provides a cost-effective, reliable solution for evaluating subject-driven T2I generation, improving performance and human alignment."}}
{"id": "2504.17393", "pdf": "https://arxiv.org/pdf/2504.17393", "abs": "https://arxiv.org/abs/2504.17393", "authors": ["Vesna Nowack", "Dalal Alrajeh", "Carolina Gutierrez Mu\u00f1oz", "Katie Thomas", "William Hobson", "Catherine Hamilton-Giachritsis", "Patrick Benjamin", "Tim Grant", "Juliane A. Kloess", "Jessica Woodhams"], "title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "10 pages, 1 figure", "summary": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain.", "AI": {"tldr": "The study explores user requirements for AI-assisted systems in law enforcement, emphasizing efficiency, scalability, accuracy, and human oversight due to the domain's complexity.", "motivation": "To clarify user needs and responsibilities in AI-assisted law enforcement systems, given the lack of clear requirements.", "method": "Qualitative research on decision-making within a law enforcement agency to identify limitations and user expectations.", "result": "Participants highlighted needs like data processing efficiency, scalability, accuracy, and human validation, with full automation deemed unlikely.", "conclusion": "AI systems in law enforcement require human oversight, adaptability, and user-friendly interaction, as full automation is impractical."}}
{"id": "2504.17461", "pdf": "https://arxiv.org/pdf/2504.17461", "abs": "https://arxiv.org/abs/2504.17461", "authors": ["Vipin Singh", "Tianheng Ling", "Teodor Chiaburu", "Felix Biessmann"], "title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation", "summary": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository.", "AI": {"tldr": "The paper evaluates ML models for urban wastewater management, comparing global and local models for resilience and performance in extreme rainfall scenarios.", "motivation": "Climate change strains urban sewer systems, requiring cost-efficient, adaptable ML solutions to replace traditional models.", "method": "Proposes a protocol to assess Neural Network architectures for CSS forecasting, focusing on performance, complexity, and resilience to perturbations.", "result": "Global models perform better, but local models are resilient in decentralized scenarios. Longer forecast horizons improve robustness.", "conclusion": "ML offers reliable solutions for sustainable wastewater management, with local models being viable for IoT deployment."}}
{"id": "2406.15231", "pdf": "https://arxiv.org/pdf/2406.15231", "abs": "https://arxiv.org/abs/2406.15231", "authors": ["Yanis Labrak", "Markus Frohmann", "Gabriel Meseguer-Brocal", "Elena V. Epure"], "title": "Synthetic Lyrics Detection Across Languages and Genres", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in the TrustNLP Workshop at NAACL 2025", "summary": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users.", "AI": {"tldr": "The paper explores detecting synthetic lyrics generated by LLMs, addressing gaps in existing research by evaluating detection methods on a diverse dataset and adapting features for lyrics.", "motivation": "The rise of LLM-generated music content raises concerns about copyright, satisfaction, and spamming, but lacks focus on lyrics. This work fills that gap.", "method": "Curated a diverse dataset of real and synthetic lyrics, validated generation pipeline, evaluated detection methods, and adapted features for lyrics.", "result": "Promising results in detecting synthetic lyrics, with insights on generalization across languages, scalability, and few-shot performance.", "conclusion": "The findings can inform AI-generated music policies and improve user transparency."}}
{"id": "2504.17515", "pdf": "https://arxiv.org/pdf/2504.17515", "abs": "https://arxiv.org/abs/2504.17515", "authors": ["Zihan Cheng", "Jintao Guo", "Jian Zhang", "Lei Qi", "Luping Zhou", "Yinghuan Shi", "Yang Gao"], "title": "Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted by IEEE TMI 2025. The code is available at\n  https://github.com/orange-czh/Mamba-Sea", "summary": "To segment medical images with distribution shifts, domain generalization\n(DG) has emerged as a promising setting to train models on source domains that\ncan generalize to unseen target domains. Existing DG methods are mainly based\non CNN or ViT architectures. Recently, advanced state space models, represented\nby Mamba, have shown promising results in various supervised medical image\nsegmentation. The success of Mamba is primarily owing to its ability to capture\nlong-range dependencies while keeping linear complexity with input sequence\nlength, making it a promising alternative to CNNs and ViTs. Inspired by the\nsuccess, in the paper, we explore the potential of the Mamba architecture to\naddress distribution shifts in DG for medical image segmentation. Specifically,\nwe propose a novel Mamba-based framework, Mamba-Sea, incorporating\nglobal-to-local sequence augmentation to improve the model's generalizability\nunder domain shift issues. Our Mamba-Sea introduces a global augmentation\nmechanism designed to simulate potential variations in appearance across\ndifferent sites, aiming to suppress the model's learning of domain-specific\ninformation. At the local level, we propose a sequence-wise augmentation along\ninput sequences, which perturbs the style of tokens within random continuous\nsub-sequences by modeling and resampling style statistics associated with\ndomain shifts. To our best knowledge, Mamba-Sea is the first work to explore\nthe generalization of Mamba for medical image segmentation, providing an\nadvanced and promising Mamba-based architecture with strong robustness to\ndomain shifts. Remarkably, our proposed method is the first to surpass a Dice\ncoefficient of 90% on the Prostate dataset, which exceeds previous SOTA of\n88.61%. The code is available at https://github.com/orange-czh/Mamba-Sea.", "AI": {"tldr": "Mamba-Sea, a Mamba-based framework, addresses domain generalization in medical image segmentation by incorporating global-to-local sequence augmentation, achieving state-of-the-art results.", "motivation": "To improve generalization of medical image segmentation models under domain shifts by leveraging the Mamba architecture's ability to capture long-range dependencies efficiently.", "method": "Proposes Mamba-Sea, a novel framework with global augmentation to simulate appearance variations and local sequence-wise augmentation to perturb token styles, enhancing robustness to domain shifts.", "result": "Achieves a Dice coefficient of over 90% on the Prostate dataset, surpassing the previous SOTA of 88.61%.", "conclusion": "Mamba-Sea demonstrates the potential of Mamba architectures for domain generalization in medical image segmentation, offering strong robustness and performance."}}
{"id": "2504.17424", "pdf": "https://arxiv.org/pdf/2504.17424", "abs": "https://arxiv.org/abs/2504.17424", "authors": ["Tomoki Mizuno", "Kazuya Yabashi", "Tsuyoshi Tasaki"], "title": "Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We have developed a new method to estimate a Next Viewpoint (NV) which is\neffective for pose estimation of simple-shaped products for product display\nrobots in retail stores. Pose estimation methods using Neural Networks (NN)\nbased on an RGBD camera are highly accurate, but their accuracy significantly\ndecreases when the camera acquires few texture and shape features at a current\nview point. However, it is difficult for previous mathematical model-based\nmethods to estimate effective NV which is because the simple shaped objects\nhave few shape features. Therefore, we focus on the relationship between the\npose estimation and NV estimation. When the pose estimation is more accurate,\nthe NV estimation is more accurate. Therefore, we develop a new pose estimation\nNN that estimates NV simultaneously. Experimental results showed that our NV\nestimation realized a pose estimation success rate 77.3\\%, which was 7.4pt\nhigher than the mathematical model-based NV calculation did. Moreover, we\nverified that the robot using our method displayed 84.2\\% of products.", "AI": {"tldr": "A new method for estimating Next Viewpoint (NV) improves pose estimation for simple-shaped products in retail robots by combining pose and NV estimation in a Neural Network (NN).", "motivation": "Pose estimation accuracy drops when cameras capture few features; existing model-based methods struggle with simple shapes. The relationship between pose and NV estimation is leveraged to improve accuracy.", "method": "Developed a new NN that simultaneously estimates pose and NV, enhancing accuracy by leveraging their interdependence.", "result": "Achieved a 77.3% pose estimation success rate, 7.4 points higher than model-based methods, and enabled robots to display 84.2% of products.", "conclusion": "Simultaneous pose and NV estimation via NN significantly improves accuracy and practicality for retail robots."}}
{"id": "2504.17471", "pdf": "https://arxiv.org/pdf/2504.17471", "abs": "https://arxiv.org/abs/2504.17471", "authors": ["Yacine Belal", "Mohamed Maouche", "Sonia Ben Mokhtar", "Anthony Simonet-Boulogne"], "title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory.", "AI": {"tldr": "GRANITE is a framework for robust decentralized learning (Gossip Learning) over sparse, dynamic graphs, resilient to Byzantine attacks via a history-aware peer sampling protocol and adaptive probabilistic thresholds.", "motivation": "The robustness of Gossip Learning (GL) against Byzantine attacks, especially when attackers target the Random Peer Sampling (RPS) protocol, remains unaddressed.", "method": "GRANITE uses a History-aware Byzantine-resilient Peer Sampling (HaPS) protocol and Adaptive Probabilistic Threshold (APT) to filter poisoned models and set aggregation thresholds.", "result": "GRANITE maintains convergence with up to 30% Byzantine nodes, improves learning speed, and works in sparser graphs than current theory allows.", "conclusion": "GRANITE effectively addresses Byzantine resilience in GL, enabling robust learning in sparse, dynamic graphs."}}
{"id": "2406.17276", "pdf": "https://arxiv.org/pdf/2406.17276", "abs": "https://arxiv.org/abs/2406.17276", "authors": ["Jikai Wang", "Yi Su", "Juntao Li", "Qingrong Xia", "Zi Ye", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "title": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure", "categories": ["cs.CL"], "comment": "Published in TACL", "summary": "Autoregressive language models demonstrate excellent performance in various\nscenarios. However, the inference efficiency is limited by its\none-step-one-word generation mode, which has become a pressing problem recently\nas the models become increasingly larger. Speculative decoding employs a \"draft\nand then verify\" mechanism to allow multiple tokens to be generated in one\nstep, realizing lossless acceleration. Existing methods mainly adopt fixed\nheuristic draft structures, which fail to adapt to different situations to\nmaximize the acceptance length during verification. To alleviate this dilemma,\nwe proposed OPT-Tree, an algorithm to construct adaptive and scalable draft\ntrees. It searches the optimal tree structure that maximizes the mathematical\nexpectation of the acceptance length in each decoding step. Experimental\nresults reveal that OPT-Tree outperforms the existing draft structures and\nachieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.\nIf the draft model is powerful enough and the node budget is sufficient, it can\ngenerate more than ten tokens in a single step. Our code is available at\nhttps://github.com/Jikai0Wang/OPT-Tree.", "AI": {"tldr": "OPT-Tree is an adaptive draft tree algorithm for speculative decoding, improving inference efficiency in autoregressive language models by maximizing token acceptance per step.", "motivation": "Autoregressive models' one-step-one-word generation limits inference efficiency, especially as models grow larger. Existing draft structures lack adaptability.", "method": "Proposes OPT-Tree, which constructs scalable draft trees by optimizing the structure to maximize expected token acceptance length.", "result": "Achieves up to 3.2x speed-up over autoregressive decoding, with potential for generating >10 tokens per step under ideal conditions.", "conclusion": "OPT-Tree offers a scalable, adaptive solution for efficient speculative decoding, significantly accelerating autoregressive models."}}
{"id": "2504.17522", "pdf": "https://arxiv.org/pdf/2504.17522", "abs": "https://arxiv.org/abs/2504.17522", "authors": ["Anyi Xiao", "Cihui Yang"], "title": "Towards One-Stage End-to-End Table Structure Recognition with Parallel Regression for Diverse Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Table structure recognition aims to parse tables in unstructured data into\nmachine-understandable formats. Recent methods address this problem through a\ntwo-stage process or optimized one-stage approaches. However, these methods\neither require multiple networks to be serially trained and perform more\ntime-consuming sequential decoding, or rely on complex post-processing\nalgorithms to parse the logical structure of tables. They struggle to balance\ncross-scenario adaptability, robustness, and computational efficiency. In this\npaper, we propose a one-stage end-to-end table structure parsing network called\nTableCenterNet. This network unifies the prediction of table spatial and\nlogical structure into a parallel regression task for the first time, and\nimplicitly learns the spatial-logical location mapping laws of cells through a\nsynergistic architecture of shared feature extraction layers and task-specific\ndecoding. Compared with two-stage methods, our method is easier to train and\nfaster to infer. Experiments on benchmark datasets show that TableCenterNet can\neffectively parse table structures in diverse scenarios and achieve\nstate-of-the-art performance on the TableGraph-24k dataset. Code is available\nat https://github.com/dreamy-xay/TableCenterNet.", "AI": {"tldr": "A one-stage end-to-end network, TableCenterNet, is proposed for table structure recognition, unifying spatial and logical structure prediction into parallel regression for improved efficiency and adaptability.", "motivation": "Existing methods for table structure recognition are either multi-stage and time-consuming or rely on complex post-processing, lacking balance between adaptability, robustness, and efficiency.", "method": "TableCenterNet uses a synergistic architecture with shared feature extraction and task-specific decoding to predict table spatial and logical structures in parallel.", "result": "The method achieves state-of-the-art performance on the TableGraph-24k dataset, demonstrating effectiveness across diverse scenarios.", "conclusion": "TableCenterNet offers a simpler, faster, and more adaptable solution for table structure recognition compared to existing approaches."}}
{"id": "2504.17426", "pdf": "https://arxiv.org/pdf/2504.17426", "abs": "https://arxiv.org/abs/2504.17426", "authors": ["Michele Carissimi", "Martina Saletta", "Claudio Ferretti"], "title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories.", "AI": {"tldr": "A novel approach combines LLMs and topic modeling to identify meaningful topics in Python code, showing promise for software engineering tasks.", "motivation": "To improve program comprehension for tasks like maintenance and reuse by leveraging LLMs and topic modeling.", "method": "Apply topic modeling on LLM-generated code summaries and compare results with topics from function names and docstrings.", "result": "LLM-generated summaries yield interpretable and semantically rich code representations.", "conclusion": "The approach is effective for tasks like documentation, code search, and repository knowledge discovery."}}
{"id": "2504.17490", "pdf": "https://arxiv.org/pdf/2504.17490", "abs": "https://arxiv.org/abs/2504.17490", "authors": ["Mingqi Yuan", "Qi Wang", "Guozheng Ma", "Bo Li", "Xin Jin", "Yunbo Wang", "Xiaokang Yang", "Wenjun Zeng", "Dacheng Tao"], "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages", "summary": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine.", "AI": {"tldr": "Plasticine is an open-source framework for benchmarking plasticity optimization in deep RL, offering tools to quantify and mitigate plasticity loss.", "motivation": "Addressing the lack of unified benchmarks and evaluation protocols for plasticity loss in deep RL, which hinders progress in lifelong learning agents.", "method": "Introduces Plasticine, a framework with single-file implementations of 13 mitigation methods, 10 metrics, and learning scenarios of varying non-stationarity.", "result": "Provides a systematic way to quantify plasticity loss, evaluate mitigation strategies, and analyze plasticity dynamics.", "conclusion": "Plasticine enables researchers to advance the field of lifelong learning in RL by offering a standardized toolset for plasticity optimization."}}
{"id": "2409.06601", "pdf": "https://arxiv.org/pdf/2409.06601", "abs": "https://arxiv.org/abs/2409.06601", "authors": ["Yetao Wu", "Yihong Wang", "Teng Chen", "Ningyuan Xi", "Qingqing Gu", "Hongyang Lei", "Luo Ji"], "title": "LaMsS: When Large Language Models Meet Self-Skepticism", "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 6 figures, Published at ICLR 2025 Workshop on Scaling\n  Self-Improving Foundation Models,", "summary": "Hallucination is a major challenge for large language models (LLMs),\npreventing their further application in some fields. The skeptical thinking of\nhumankind could be useful for LLMs to self-cognition, self-reflection and\nalleviate their hallucinations. Inspired by this consideration, we propose a\nnovel approach called LaMsS, which combines the semantic understanding\ncapability of LLMs with self-skepticism. By introducing a series of skepticism\ntokens and augmenting them into the vocabulary, we conduct both pertaining and\nfinetuning, which allow the LLM to decode each normal token followed by a\nskeptical token, representing different skepticism levels. By calculating the\nresponse skepticism given a query, one can define a new self-aware LLM which is\nonly willing to answer with relative lower skepticism level than the threshold.\nBy examining the accuracy, AUC and AP of willingly answering questions, we\ndemonstrate that LaMsS achieves better performance than baselines on both\nmulti-choice questions and open-domain question-answering benchmarks, and can\ngeneralize to multi-task and out-of-domain settings. Our study sheds some\nlights on the self-skepticism modeling on further artificial intelligence.\nProject code and model checkpoints can be found in\nhttps://anonymous.4open.science/r/SM-1E76.", "AI": {"tldr": "The paper proposes LaMsS, a method combining LLMs with self-skepticism to reduce hallucinations by introducing skepticism tokens and thresholds for response confidence.", "motivation": "Hallucination in LLMs limits their applications; human skeptical thinking could help LLMs self-reflect and mitigate hallucinations.", "method": "Introduces skepticism tokens into LLM vocabulary, performs pretraining and finetuning to decode normal tokens followed by skeptical ones, and sets response skepticism thresholds.", "result": "LaMsS outperforms baselines in accuracy, AUC, and AP on multi-choice and open-domain QA benchmarks, generalizing well to multi-task and out-of-domain settings.", "conclusion": "LaMsS demonstrates the potential of self-skepticism in AI, improving LLM reliability. Code and models are available."}}
{"id": "2504.17524", "pdf": "https://arxiv.org/pdf/2504.17524", "abs": "https://arxiv.org/abs/2504.17524", "authors": ["Junyan Zhang", "Yan Li", "Mengxiao Geng", "Liu Shi", "Qiegen Liu"], "title": "ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot Learning for Color Image Inpainting", "categories": ["cs.CV"], "comment": "11 pages,10 figures,Submit to tcsvt", "summary": "Image inpainting is a technique used to restore missing or damaged regions of\nan image. Traditional methods primarily utilize information from adjacent\npixels for reconstructing missing areas, while they struggle to preserve\ncomplex details and structures. Simultaneously, models based on deep learning\nnecessitate substantial amounts of training data. To address this challenge, an\nencoding strategy-inspired diffusion model with few-shot learning for color\nimage inpainting is proposed in this paper. The main idea of this novel\nencoding strategy is the deployment of a \"virtual mask\" to construct\nhigh-dimensional objects through mutual perturbations between channels. This\napproach enables the diffusion model to capture diverse image representations\nand detailed features from limited training samples. Moreover, the encoding\nstrategy leverages redundancy between channels, integrates with low-rank\nmethods during iterative inpainting, and incorporates the diffusion model to\nachieve accurate information output. Experimental results indicate that our\nmethod exceeds current techniques in quantitative metrics, and the\nreconstructed images quality has been improved in aspects of texture and\nstructural integrity, leading to more precise and coherent results.", "AI": {"tldr": "A novel encoding strategy-inspired diffusion model for few-shot color image inpainting, outperforming traditional and deep learning methods in quality and detail preservation.", "motivation": "Traditional inpainting methods struggle with complex details, and deep learning models require large datasets. This work aims to address these limitations with a few-shot learning approach.", "method": "Uses a 'virtual mask' encoding strategy for high-dimensional object construction via channel perturbations, combined with low-rank methods and diffusion models for iterative inpainting.", "result": "Outperforms existing techniques in quantitative metrics, improving texture and structural integrity in reconstructed images.", "conclusion": "The proposed method achieves more precise and coherent inpainting results with limited training data, advancing the field."}}
{"id": "2504.17428", "pdf": "https://arxiv.org/pdf/2504.17428", "abs": "https://arxiv.org/abs/2504.17428", "authors": ["Murali Sridharan", "Mika M\u00e4ntyl\u00e4", "Leevi Rantala"], "title": "Detection, Classification and Prevalence of Self-Admitted Aging Debt", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.GL", "D.2.7; D.2.9"], "comment": "Draft", "summary": "Context: Previous research on software aging is limited with focus on dynamic\nruntime indicators like memory and performance, often neglecting evolutionary\nindicators like source code comments and narrowly examining legacy issues\nwithin the TD context. Objective: We introduce the concept of Aging Debt (AD),\nrepresenting the increased maintenance efforts and costs needed to keep\nsoftware updated. We study AD through Self-Admitted Aging Debt (SAAD) observed\nin source code comments left by software developers. Method: We employ a\nmixed-methods approach, combining qualitative and quantitative analyses to\ndetect and measure AD in software. This includes framing SAAD patterns from the\nsource code comments after analysing the source code context, then utilizing\nthe SAAD patterns to detect SAAD comments. In the process, we develop a\ntaxonomy for SAAD that reflects the temporal aging of software and its\nassociated debt. Then we utilize the taxonomy to quantify the different types\nof AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes\ntemporal software aging into Active and Dormant types. Our extensive analysis\nof over 9,000+ Open Source Software (OSS) repositories reveals that more than\n21% repositories exhibit signs of SAAD as observed from our gold standard SAAD\ndataset. Notably, Dormant AD emerges as the predominant category, highlighting\na critical but often overlooked aspect of software maintenance. Conclusion: As\nsoftware volume grows annually, so do evolutionary aging and maintenance\nchallenges; our proposed taxonomy can aid researchers in detailed software\naging studies and help practitioners develop improved and proactive maintenance\nstrategies.", "AI": {"tldr": "The paper introduces Aging Debt (AD) and Self-Admitted Aging Debt (SAAD) to study software aging through source code comments, proposing a taxonomy and revealing its prevalence in OSS repositories.", "motivation": "To address gaps in software aging research by focusing on evolutionary indicators like source code comments and introducing AD to quantify maintenance efforts.", "method": "Mixed-methods approach combining qualitative and quantitative analyses to detect and measure AD, including developing a taxonomy for SAAD and applying it to OSS repositories.", "result": "Taxonomy categorizes AD into Active and Dormant types; analysis of 9,000+ OSS repositories shows 21% exhibit SAAD, with Dormant AD being predominant.", "conclusion": "The taxonomy aids researchers and practitioners in understanding and addressing software aging, supporting proactive maintenance strategies."}}
{"id": "2504.17492", "pdf": "https://arxiv.org/pdf/2504.17492", "abs": "https://arxiv.org/abs/2504.17492", "authors": ["Nawid Keshtmand", "Elena Fillola", "Jeffrey Nicholas Clark", "Raul Santos-Rodriguez", "Matthew Rigby"], "title": "Prototype-enhanced prediction in graph neural networks for climate applications", "categories": ["cs.LG"], "comment": null, "summary": "Data-driven emulators are increasingly being used to learn and emulate\nphysics-based simulations, reducing computational expense and run time. Here,\nwe present a structured way to improve the quality of these high-dimensional\nemulated outputs, through the use of prototypes: an approximation of the\nemulator's output passed as an input, which informs the model and leads to\nbetter predictions. We demonstrate our approach to emulate atmospheric\ndispersion, key for greenhouse gas emissions monitoring, by comparing a\nbaseline model to models trained using prototypes as an additional input. The\nprototype models achieve better performance, even with few prototypes and even\nif they are chosen at random, but we show that choosing the prototypes through\ndata-driven methods (k-means) can lead to almost 10\\% increased performance in\nsome metrics.", "AI": {"tldr": "Using prototypes as inputs improves high-dimensional emulator performance, even with random prototypes, but data-driven selection (k-means) boosts performance by nearly 10%.", "motivation": "To enhance the quality of data-driven emulators for physics-based simulations, reducing computational costs and improving accuracy.", "method": "Introduces prototypes\u2014approximations of emulator outputs\u2014as additional inputs to the model. Tests the approach on atmospheric dispersion emulation, comparing baseline models to prototype-enhanced ones.", "result": "Prototype models outperform baselines, even with random prototypes. Data-driven prototype selection (k-means) further improves performance by up to 10%.", "conclusion": "Prototypes significantly enhance emulator performance, with data-driven selection methods offering the highest gains."}}
{"id": "2409.11242", "pdf": "https://arxiv.org/pdf/2409.11242", "abs": "https://arxiv.org/abs/2409.11242", "authors": ["Maojia Song", "Shang Hong Sim", "Rishabh Bhardwaj", "Hai Leong Chieu", "Navonil Majumder", "Soujanya Poria"], "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse", "categories": ["cs.CL"], "comment": "Published at ICLR 2025 (Oral)", "summary": "LLMs are an integral component of retrieval-augmented generation (RAG)\nsystems. While many studies focus on evaluating the overall quality of\nend-to-end RAG systems, there is a gap in understanding the appropriateness of\nLLMs for the RAG task. To address this, we introduce Trust-Score, a holistic\nmetric that evaluates the trustworthiness of LLMs within the RAG framework. Our\nresults show that various prompting methods, such as in-context learning, fail\nto effectively adapt LLMs to the RAG task as measured by Trust-Score.\nConsequently, we propose Trust-Align, a method to align LLMs for improved\nTrust-Score performance. 26 out of 27 models aligned using Trust-Align\nsubstantially outperform competitive baselines on ASQA, QAMPARI, and ELI5.\nSpecifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56),\nQAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly\nenhances models' ability to correctly refuse and provide quality citations. We\nalso demonstrate the effectiveness of Trust-Align across different open-weight\nmodels, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b),\nand Phi3.5 (3.8b). We release our code at\nhttps://github.com/declare-lab/trust-align.", "AI": {"tldr": "The paper introduces Trust-Score to evaluate LLMs in RAG systems and proposes Trust-Align to improve their performance, showing significant gains over baselines.", "motivation": "There is a lack of understanding about how suitable LLMs are for RAG tasks, prompting the need for a metric like Trust-Score.", "method": "Trust-Score evaluates LLM trustworthiness in RAG, and Trust-Align is introduced to align LLMs for better performance.", "result": "Trust-Align improves performance on benchmarks (ASQA, QAMPARI, ELI5) and enhances citation quality and refusal ability.", "conclusion": "Trust-Align effectively aligns LLMs for RAG tasks, outperforming baselines and improving trustworthiness."}}
{"id": "2504.17525", "pdf": "https://arxiv.org/pdf/2504.17525", "abs": "https://arxiv.org/abs/2504.17525", "authors": ["Paul Grimal", "Herv\u00e9 Le Borgne", "Olivier Ferret"], "title": "Text-to-Image Alignment in Denoising-Based Models through Step Selection", "categories": ["cs.CV"], "comment": null, "summary": "Visual generative AI models often encounter challenges related to text-image\nalignment and reasoning limitations. This paper presents a novel method for\nselectively enhancing the signal at critical denoising steps, optimizing image\ngeneration based on input semantics. Our approach addresses the shortcomings of\nearly-stage signal modifications, demonstrating that adjustments made at later\nstages yield superior results. We conduct extensive experiments to validate the\neffectiveness of our method in producing semantically aligned images on\nDiffusion and Flow Matching model, achieving state-of-the-art performance. Our\nresults highlight the importance of a judicious choice of sampling stage to\nimprove performance and overall image alignment.", "AI": {"tldr": "A novel method enhances signal at critical denoising steps to improve text-image alignment in generative AI, outperforming early-stage modifications.", "motivation": "Addressing challenges in text-image alignment and reasoning limitations in visual generative AI models.", "method": "Selectively enhances signal at later denoising steps to optimize image generation based on input semantics.", "result": "Achieves state-of-the-art performance on Diffusion and Flow Matching models, improving semantic alignment.", "conclusion": "Judicious choice of sampling stage is crucial for enhancing performance and image alignment."}}
{"id": "2504.17493", "pdf": "https://arxiv.org/pdf/2504.17493", "abs": "https://arxiv.org/abs/2504.17493", "authors": ["Luca-Andrei Fechete", "Mohamed Sana", "Fadhel Ayed", "Nicola Piovesan", "Wenjie Li", "Antonio De Domenico", "Tareq Si Salem"], "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications.", "AI": {"tldr": "A new training methodology for time-series forecasting dynamically adjusts focus based on application-specific forecast ranges, improving accuracy and end-application performance.", "motivation": "Traditional forecasting methods ignore real-world application needs by focusing solely on minimizing prediction errors.", "method": "Breaks predictions into smaller segments, dynamically weights them, and combines for accurate forecasts.", "result": "Tested on standard and new datasets, the method improves prediction accuracy and end-application performance.", "conclusion": "This approach bridges prediction and decision-making, enhancing practical forecasting systems."}}
{"id": "2504.17497", "pdf": "https://arxiv.org/pdf/2504.17497", "abs": "https://arxiv.org/abs/2504.17497", "authors": ["Radia Berreziga", "Mohammed Brahimi", "Khairedine Kraim", "Hamid Azzoune"], "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines.", "AI": {"tldr": "A hybrid architecture combining GCNs and LLM embeddings improves virtual screening performance, outperforming traditional methods like SVM and XGBoost.", "motivation": "Traditional machine learning methods in drug discovery rely on predefined molecular representations, risking information loss and bias. Deep learning and LLMs offer more expressive alternatives.", "method": "Proposes a hybrid model integrating GCNs with precomputed LLM embeddings, concatenating them at each GCN layer for deeper global context integration.", "result": "Achieves an F1-score of 88.8%, surpassing standalone GCN (87.9%), XGBoost (85.5%), and SVM (85.4%).", "conclusion": "The hybrid approach effectively combines localized structural learning with global chemical knowledge, enhancing virtual screening performance."}}
{"id": "2409.18986", "pdf": "https://arxiv.org/pdf/2409.18986", "abs": "https://arxiv.org/abs/2409.18986", "authors": ["Xiaoyu Wang", "Haoyong Ouyang", "Balu Bhasuran", "Xiao Luo", "Karim Hanna", "Mia Liza A. Lustria", "Carl Yang", "Zhe He"], "title": "Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Accurate interpretation of lab results is crucial in clinical medicine, yet\nmost patient portals use universal normal ranges, ignoring conditional factors\nlike age and gender. This study introduces Lab-AI, an interactive system that\noffers personalized normal ranges using retrieval-augmented generation (RAG)\nfrom credible health sources. Lab-AI has two modules: factor retrieval and\nnormal range retrieval. We tested these on 122 lab tests: 40 with conditional\nfactors and 82 without. For tests with factors, normal ranges depend on\npatient-specific information. Our results show GPT-4-turbo with RAG achieved a\n0.948 F1 score for factor retrieval and 0.995 accuracy for normal range\nretrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5%\nin factor retrieval and showed 132% and 100% improvements in question-level and\nlab-level performance, respectively, for normal range retrieval. These findings\nhighlight Lab-AI's potential to enhance patient understanding of lab results.", "AI": {"tldr": "Lab-AI uses RAG to provide personalized lab result interpretations, outperforming non-RAG systems significantly.", "motivation": "Current patient portals use universal normal ranges, ignoring patient-specific factors like age and gender, leading to inaccurate interpretations.", "method": "Lab-AI employs two modules: factor retrieval and normal range retrieval, tested on 122 lab tests using GPT-4-turbo with RAG.", "result": "Achieved 0.948 F1 score for factor retrieval and 0.995 accuracy for normal range retrieval, outperforming non-RAG systems by 33.5% and up to 132%.", "conclusion": "Lab-AI demonstrates significant potential to improve patient understanding of lab results through personalized interpretations."}}
{"id": "2504.17545", "pdf": "https://arxiv.org/pdf/2504.17545", "abs": "https://arxiv.org/abs/2504.17545", "authors": ["Keyang Ye", "Tianjia Shao", "Kun Zhou"], "title": "When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for\nradiance field rendering, wherein a set of 2D opaque surfels with\nview-dependent colors represent the coarse-scale geometry and appearance of\nscenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale\nappearance details. The rendering with GESs consists of two passes -- surfels\nare first rasterized through a standard graphics pipeline to produce depth and\ncolor maps, and then Gaussians are splatted with depth testing and color\naccumulation on each pixel order independently. The optimization of GESs from\nmulti-view images is performed through an elaborate coarse-to-fine procedure,\nfaithfully capturing rich scene appearance. The entirely sorting-free rendering\nof GESs not only achieves very fast rates, but also produces view-consistent\nimages, successfully avoiding popping artifacts under view changes. The basic\nGES representation can be easily extended to achieve anti-aliasing in rendering\n(Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage\n(Compact-GES), and reconstruct better scene geometries by replacing 3D\nGaussians with 2D Gaussians (2D-GES). Experimental results show that GESs\nadvance the state-of-the-arts as a compelling representation for ultra-fast\nhigh-fidelity radiance field rendering.", "AI": {"tldr": "Gaussian-enhanced Surfels (GESs) combine 2D surfels and 3D Gaussians for fast, high-fidelity radiance field rendering, avoiding artifacts and enabling extensions like anti-aliasing and compact storage.", "motivation": "To create a bi-scale representation for radiance field rendering that balances coarse and fine details while ensuring fast, view-consistent rendering.", "method": "Uses 2D surfels for coarse geometry and 3D Gaussians for fine details, with a two-pass rendering process (rasterization and splatting). Optimized via a coarse-to-fine procedure.", "result": "Achieves ultra-fast, high-fidelity rendering with view consistency and no popping artifacts. Extensions (Mip-GES, Speedy-GES, etc.) further enhance performance.", "conclusion": "GESs advance radiance field rendering, offering speed, quality, and flexibility, with potential for broader applications."}}
{"id": "2504.17528", "pdf": "https://arxiv.org/pdf/2504.17528", "abs": "https://arxiv.org/abs/2504.17528", "authors": ["Weijie Liu", "Ziwei Zhan", "Carlee Joe-Wong", "Edith Ngai", "Jingpu Duan", "Deke Guo", "Xu Chen", "Xiaoxi Zhang"], "title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "11 pages, 7 figures, accepted by ICDCS 2025", "summary": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice.", "AI": {"tldr": "TACO addresses over-correction in FL by fine-grained gradient correction and lightweight aggregation, improving model accuracy and efficiency.", "motivation": "Non-IID data in FL causes statistical heterogeneity; existing methods over-correct, degrading performance.", "method": "TACO uses client-specific gradient correction and tailored aggregation, minimizing computation overhead.", "result": "TACO improves model accuracy and convergence, validated by experiments and convergence analysis.", "conclusion": "TACO effectively mitigates over-correction and enhances FL performance in non-IID settings."}}
{"id": "2504.17503", "pdf": "https://arxiv.org/pdf/2504.17503", "abs": "https://arxiv.org/abs/2504.17503", "authors": ["Davide Prosperino", "Haochun Ma", "Christoph R\u00e4th"], "title": "Tailored minimal reservoir computing: on the bidirectional connection between nonlinearities in the reservoir and in data", "categories": ["cs.LG", "nlin.CD"], "comment": "13 pages, 11 figures", "summary": "We study how the degree of nonlinearity in the input data affects the optimal\ndesign of reservoir computers, focusing on how closely the model's nonlinearity\nshould align with that of the data. By reducing minimal RCs to a single tunable\nnonlinearity parameter, we explore how the predictive performance varies with\nthe degree of nonlinearity in the reservoir. To provide controlled testbeds, we\ngeneralize to the fractional Halvorsen system, a novel chaotic system with\nfractional exponents. Our experiments reveal that the prediction performance is\nmaximized when the reservoir's nonlinearity matches the nonlinearity present in\nthe data. In cases where multiple nonlinearities are present in the data, we\nfind that the correlation dimension of the predicted signal is reconstructed\ncorrectly when the smallest nonlinearity is matched. We use this observation to\npropose a method for estimating the minimal nonlinearity in unknown time series\nby sweeping the reservoir exponent and identifying the transition to a\nsuccessful reconstruction. Applying this method to both synthetic and\nreal-world datasets, including financial time series, we demonstrate its\npractical viability. Finally, we transfer these insights to classical RC by\naugmenting traditional architectures with fractional, generalized reservoir\nstates. This yields performance gains, particularly in resource-constrained\nscenarios such as physical reservoirs, where increasing reservoir size is\nimpractical or economically unviable. Our work provides a principled route\ntoward tailoring RCs to the intrinsic complexity of the systems they aim to\nmodel.", "AI": {"tldr": "Optimal reservoir computer design depends on matching the nonlinearity of input data, with performance peaking when reservoir nonlinearity aligns with data. A method for estimating minimal nonlinearity in unknown time series is proposed and validated.", "motivation": "To understand how the degree of nonlinearity in input data affects the optimal design of reservoir computers and to develop a method for estimating nonlinearity in unknown datasets.", "method": "Reduce minimal RCs to a single tunable nonlinearity parameter, generalize the fractional Halvorsen system for controlled testing, and propose a method for estimating nonlinearity by sweeping reservoir exponents.", "result": "Prediction performance is maximized when reservoir nonlinearity matches data nonlinearity. The method successfully estimates minimal nonlinearity in synthetic and real-world datasets.", "conclusion": "Tailoring reservoir computers to data nonlinearity improves performance, especially in resource-constrained scenarios, offering a principled approach to RC design."}}
{"id": "2409.19151", "pdf": "https://arxiv.org/pdf/2409.19151", "abs": "https://arxiv.org/abs/2409.19151", "authors": ["Seth Aycock", "David Stap", "Di Wu", "Christof Monz", "Khalil Sima'an"], "title": "Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?", "categories": ["cs.CL"], "comment": "Accepted at ICLR 2025 (Spotlight)", "summary": "Extremely low-resource (XLR) languages lack substantial corpora for training\nNLP models, motivating the use of all available resources such as dictionaries\nand grammar books. Machine Translation from One Book (Tanzer et al., 2024)\nsuggests that prompting long-context LLMs with one grammar book enables\nEnglish-Kalamang translation, an XLR language unseen by LLMs - a noteworthy\ncase of linguistics helping an NLP task. We investigate the source of this\ntranslation ability, finding almost all improvements stem from the book's\nparallel examples rather than its grammatical explanations. We find similar\nresults for Nepali and Guarani, seen low-resource languages, and we achieve\nperformance comparable to an LLM with a grammar book by simply fine-tuning an\nencoder-decoder translation model. We then investigate where grammar books help\nby testing two linguistic tasks, grammaticality judgment and gloss prediction,\nand we explore what kind of grammatical knowledge helps by introducing a\ntypological feature prompt that achieves leading results on these more relevant\ntasks. We thus emphasise the importance of task-appropriate data for XLR\nlanguages: parallel examples for translation, and grammatical data for\nlinguistic tasks. As we find no evidence that long-context LLMs can make\neffective use of grammatical explanations for XLR translation, we conclude data\ncollection for multilingual XLR tasks such as translation is best focused on\nparallel data over linguistic description.", "AI": {"tldr": "The paper explores how grammar books aid translation in extremely low-resource (XLR) languages, finding parallel examples more useful than grammatical explanations, and suggests focusing on task-appropriate data like parallel examples for translation and grammatical data for linguistic tasks.", "motivation": "To understand the source of translation ability in XLR languages when using grammar books with long-context LLMs, and to determine the most effective data types for such tasks.", "method": "Investigates translation improvements by analyzing grammar books, tests on XLR languages (Kalamang, Nepali, Guarani), and evaluates grammatical knowledge via tasks like grammaticality judgment and gloss prediction.", "result": "Parallel examples in grammar books drive translation improvements, not grammatical explanations. A typological feature prompt excels in linguistic tasks.", "conclusion": "For XLR translation, prioritize parallel data over linguistic descriptions, as grammatical explanations are ineffective with current LLMs."}}
{"id": "2504.17551", "pdf": "https://arxiv.org/pdf/2504.17551", "abs": "https://arxiv.org/abs/2504.17551", "authors": ["Lin Che", "Yizi Chen", "Tanhua Jin", "Martin Raubal", "Konrad Schindler", "Peter Kiefer"], "title": "Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures, preprint version", "summary": "Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP.", "AI": {"tldr": "An unsupervised contrastive clustering model with a geographical prior is introduced for street view images to improve urban land use mapping, addressing the limitations of supervised methods.", "motivation": "Existing remote sensing techniques lack precision in complex urban environments, and supervised street view-based methods struggle with scarce labeled data and generalization.", "method": "The study proposes an unsupervised contrastive clustering model with a built-in geographical prior for street view images, combined with visual cluster assignment.", "result": "The method successfully generates land use maps from geotagged street view datasets of two cities, leveraging spatial coherence.", "conclusion": "The approach offers a scalable, flexible solution for unsupervised land use mapping, adaptable to various settings with street view data."}}
{"id": "2504.17534", "pdf": "https://arxiv.org/pdf/2504.17534", "abs": "https://arxiv.org/abs/2504.17534", "authors": ["Juan Carlos Climent Pardo"], "title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SC"], "comment": null, "summary": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction.", "AI": {"tldr": "The paper addresses generalization in autonomous driving by using graph representations and MDS techniques to create feature spaces for neural network-based motion planners.", "motivation": "Current autonomous driving systems lack generalization across diverse road scenarios, necessitating a method to capture varied road structures, topologies, and dynamic changes.", "method": "The paper leverages graph representations of road networks and applies multidimensional scaling (MDS) techniques to design feature spaces. It also explores embedding graph nodes for easier learning and dimensionality reduction.", "result": "The approach demonstrates how graph representations and MDS can improve generalization in motion planning for autonomous driving.", "conclusion": "Graph-based representations and MDS techniques offer a promising solution to enhance generalization in learning-based autonomous driving applications."}}
{"id": "2504.17526", "pdf": "https://arxiv.org/pdf/2504.17526", "abs": "https://arxiv.org/abs/2504.17526", "authors": ["Yuelin Liu", "Haiyuan Li", "Xenofon Vasilakos", "Rasheed Hussain", "Dimitra Simeonidou"], "title": "Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks", "categories": ["cs.LG"], "comment": null, "summary": "Future networks (including 6G) are poised to accelerate the realisation of\nInternet of Everything. However, it will result in a high demand for computing\nresources to support new services. Mobile Edge Computing (MEC) is a promising\nsolution, enabling to offload computation-intensive tasks to nearby edge\nservers from the end-user devices, thereby reducing latency and energy\nconsumption. However, relying solely on a single MEC server for task offloading\ncan lead to uneven resource utilisation and suboptimal performance in complex\nscenarios. Additionally, traditional task offloading strategies specialise in\ncentralised policy decisions, which unavoidably entail extreme transmission\nlatency and reach computational bottleneck. To fill the gaps, we propose a\nlatency and energy efficient Cooperative Task Offloading framework with\nTransformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent\ndeep reinforcement learning to address these challenges. This approach fosters\nedge-edge cooperation and decreases the synchronous waiting time by performing\nasynchronous training, optimising task offloading, and resource allocation\nacross distributed networks. The performance evaluation demonstrates that the\nproposed CTO-TP algorithm reduces up to 80% overall system latency and 87%\nenergy consumption compared to the baseline schemes.", "AI": {"tldr": "Proposes CTO-TP, a cooperative task offloading framework using transformer-driven prediction and asynchronous multi-agent deep reinforcement learning to reduce latency and energy consumption in MEC.", "motivation": "Addresses uneven resource utilization and suboptimal performance in single MEC server offloading, and the limitations of centralized task offloading strategies.", "method": "Leverages asynchronous multi-agent deep reinforcement learning for edge-edge cooperation, optimizing task offloading and resource allocation.", "result": "Reduces overall system latency by up to 80% and energy consumption by 87% compared to baseline schemes.", "conclusion": "CTO-TP effectively enhances MEC performance by improving latency and energy efficiency through cooperative and asynchronous approaches."}}
{"id": "2410.01952", "pdf": "https://arxiv.org/pdf/2410.01952", "abs": "https://arxiv.org/abs/2410.01952", "authors": ["Danqing Wang", "Jianxin Ma", "Fei Fang", "Lei Li"], "title": "TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking", "categories": ["cs.CL"], "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nin solving complex problems. However, current approaches primarily enhance\nreasoning through the elaboration of thoughts while neglecting the diversity of\nreasoning types. LLMs typically employ deductive reasoning, proceeding\nstep-by-step from given conditions, which limits their exploration during\nproblem-solving. Our analysis reveals that certain problems are exclusively\nsolvable through specific reasoning strategies like inductive, abductive, or\nanalogical reasoning. However, incorporating diverse reasoning approaches\npresents two key challenges: identifying the appropriate reasoning type for\neach problem and exploiting this approach during problem-solving. Therefore, we\npropose the TypedThinker that predicts suitable reasoning types based on the\nproblem and their previous effectiveness and provides relevant demonstrations\nto guide LLMs in applying these strategies. Experimental results show\nsignificant improvements across multiple benchmarks, with performance gains of\n3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B on logical and\nmathematical reasoning tasks. TypedThinker enhances LLM reasoning without\nrequiring knowledge distillation from larger models. It can be integrated into\nmore advanced systems like GPT-4o or specialized models like MetaMath to\ndiversify their reasoning approaches and improve their problem-solving\ncapabilities.", "AI": {"tldr": "TypedThinker enhances LLM reasoning by predicting and applying diverse reasoning types (inductive, abductive, analogical) beyond traditional deductive methods, improving performance on benchmarks.", "motivation": "Current LLMs rely heavily on deductive reasoning, limiting their problem-solving diversity. Some problems require specific reasoning strategies, but identifying and applying these is challenging.", "method": "Proposes TypedThinker, which predicts suitable reasoning types for problems and provides demonstrations to guide LLMs in applying these strategies.", "result": "Achieves performance gains of 3.4% to 7% on benchmarks for models like Mistral 7B, LLaMA3 8B, and Qwen 2 7B.", "conclusion": "TypedThinker diversifies LLM reasoning without needing knowledge distillation, making it adaptable for advanced or specialized models."}}
{"id": "2504.17582", "pdf": "https://arxiv.org/pdf/2504.17582", "abs": "https://arxiv.org/abs/2504.17582", "authors": ["Zebo Huang", "Yinghui Wang"], "title": "Occlusion-Aware Self-Supervised Monocular Depth Estimation for Weak-Texture Endoscopic Images", "categories": ["cs.CV"], "comment": null, "summary": "We propose a self-supervised monocular depth estimation network tailored for\nendoscopic scenes, aiming to infer depth within the gastrointestinal tract from\nmonocular images. Existing methods, though accurate, typically assume\nconsistent illumination, which is often violated due to dynamic lighting and\nocclusions caused by GI motility. These variations lead to incorrect geometric\ninterpretations and unreliable self-supervised signals, degrading depth\nreconstruction quality. To address this, we introduce an occlusion-aware\nself-supervised framework. First, we incorporate an occlusion mask for data\naugmentation, generating pseudo-labels by simulating viewpoint-dependent\nocclusion scenarios. This enhances the model's ability to learn robust depth\nfeatures under partial visibility. Second, we leverage semantic segmentation\nguided by non-negative matrix factorization, clustering convolutional\nactivations to generate pseudo-labels in texture-deprived regions, thereby\nimproving segmentation accuracy and mitigating information loss from lighting\nchanges. Experimental results on the SCARED dataset show that our method\nachieves state-of-the-art performance in self-supervised depth estimation.\nAdditionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate\nstrong generalization across diverse endoscopic environments.", "AI": {"tldr": "A self-supervised monocular depth estimation network for endoscopic scenes addresses illumination and occlusion challenges, improving depth reconstruction quality.", "motivation": "Existing methods fail under dynamic lighting and occlusions in gastrointestinal tracts, leading to unreliable depth estimates.", "method": "Introduces an occlusion-aware framework with data augmentation via occlusion masks and semantic segmentation guided by non-negative matrix factorization.", "result": "Achieves state-of-the-art performance on the SCARED dataset and generalizes well on Endo-SLAM and SERV-CT datasets.", "conclusion": "The proposed method effectively handles endoscopic scene challenges, enhancing depth estimation accuracy and robustness."}}
{"id": "2504.17539", "pdf": "https://arxiv.org/pdf/2504.17539", "abs": "https://arxiv.org/abs/2504.17539", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "title": "Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Blockchain technology enables secure, transparent data management in\ndecentralized systems, supporting applications from cryptocurrencies like\nBitcoin to tokenizing real-world assets like property. Its scalability and\nsustainability hinge on consensus mechanisms balancing security and efficiency.\nProof of Work (PoW), used by Bitcoin, ensures security through energy-intensive\ncomputations but demands significant resources. Proof of Stake (PoS), as in\nEthereum post-Merge, selects validators based on staked cryptocurrency,\noffering energy efficiency but risking centralization from wealth\nconcentration. With AI models straining computational resources, we propose\nProof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,\nworkers perform AI tasks like language processing or image analysis to earn\ncoins, which are staked to secure the network, blending security with practical\nutility. Decentralized nodes--job posters, market coordinators, workers, and\nvalidators --collaborate via smart contracts to manage tasks and rewards.", "AI": {"tldr": "A hybrid consensus mechanism, Proof of Useful Intelligence (PoUI), combines AI task performance with blockchain security, addressing scalability and sustainability in decentralized systems.", "motivation": "Current consensus mechanisms like PoW and PoS face challenges in scalability, sustainability, and centralization risks, prompting the need for a more efficient and practical alternative.", "method": "Proposes PoUI, where workers perform AI tasks to earn coins, which are staked to secure the network. Uses decentralized nodes and smart contracts for task and reward management.", "result": "PoUI aims to balance security and efficiency by integrating AI utility into blockchain consensus, reducing resource waste and centralization risks.", "conclusion": "PoUI presents a novel approach to blockchain consensus, leveraging AI tasks for both network security and practical utility, addressing limitations of existing mechanisms."}}
{"id": "2504.17568", "pdf": "https://arxiv.org/pdf/2504.17568", "abs": "https://arxiv.org/abs/2504.17568", "authors": ["Ivan Rossi", "Flavio Sartori", "Cesare Rollo", "Giovanni Birolo", "Piero Fariselli", "Tiziana Sanavia"], "title": "Beyond Cox Models: Assessing the Performance of Machine-Learning Methods in Non-Proportional Hazards and Non-Linear Survival Analysis", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Survival analysis often relies on Cox models, assuming both linearity and\nproportional hazards (PH). This study evaluates machine and deep learning\nmethods that relax these constraints, comparing their performance with\npenalized Cox models on a benchmark of three synthetic and three real datasets.\nIn total, eight different models were tested, including six non-linear models\nof which four were also non-PH. Although Cox regression often yielded\nsatisfactory performance, we showed the conditions under which machine and deep\nlearning models can perform better. Indeed, the performance of these methods\nhas often been underestimated due to the improper use of Harrell's concordance\nindex (C-index) instead of more appropriate scores such as Antolini's\nconcordance index, which generalizes C-index in cases where the PH assumption\ndoes not hold. In addition, since occasionally high C-index models happen to be\nbadly calibrated, combining Antolini's C-index with Brier's score is useful to\nassess the overall performance of a survival method. Results on our benchmark\ndata showed that survival prediction should be approached by testing different\nmethods to select the most appropriate one according to sample size,\nnon-linearity and non-PH conditions. To allow an easy reproducibility of these\ntests on our benchmark data, code and documentation are freely available at\nhttps://github.com/compbiomed-unito/survhive.", "AI": {"tldr": "The study compares machine/deep learning methods with penalized Cox models for survival analysis, highlighting conditions where non-linear and non-PH models outperform Cox regression. It emphasizes proper evaluation metrics like Antolini's C-index and Brier's score.", "motivation": "To evaluate methods relaxing linearity and PH assumptions in survival analysis, addressing limitations of Cox models and improper use of evaluation metrics.", "method": "Tested eight models (six non-linear, four non-PH) on synthetic and real datasets, comparing performance using Antolini's C-index and Brier's score.", "result": "Non-linear and non-PH models can outperform Cox regression under specific conditions. Proper metrics revealed their underestimated performance.", "conclusion": "Survival prediction should involve testing multiple methods, considering sample size, non-linearity, and non-PH conditions. Code is available for reproducibility."}}
{"id": "2410.02703", "pdf": "https://arxiv.org/pdf/2410.02703", "abs": "https://arxiv.org/abs/2410.02703", "authors": ["Yaniv Leviathan", "Matan Kalman", "Yossi Matias"], "title": "Selective Attention Improves Transformer", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025", "summary": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention consistently improves language modeling and downstream task\nperformance in a variety of model sizes and context lengths. For example,\ntransformers trained with the language modeling objective on C4 with selective\nattention perform language modeling equivalently to standard transformers with\n~2X more heads and parameters in their attention modules. Selective attention\nalso allows decreasing the size of the attention's context buffer, leading to\nmeaningful reductions in the memory and compute requirements during inference.\nFor example, transformers trained on C4 with context sizes of 512, 1,024, and\n2,048 need 16X, 25X, and 47X less memory for their attention module,\nrespectively, when equipped with selective attention, as those without\nselective attention, with the same validation perplexity.", "AI": {"tldr": "Selective Attention improves transformer performance by reducing attention to unneeded elements, saving memory and compute.", "motivation": "Unneeded elements in attention's context degrade performance, prompting a need for a more efficient mechanism.", "method": "Introduces Selective Attention, a parameter-free change to standard attention, reducing focus on irrelevant elements.", "result": "Improves language modeling and downstream tasks, reduces memory/compute needs (e.g., 16X-47X less memory for attention).", "conclusion": "Selective Attention enhances efficiency without sacrificing performance, enabling smaller, faster models."}}
{"id": "2504.17594", "pdf": "https://arxiv.org/pdf/2504.17594", "abs": "https://arxiv.org/abs/2504.17594", "authors": ["Zhaofeng Si", "Siwei Lyu"], "title": "Tamper-evident Image using JPEG Fixed Points", "categories": ["cs.CV", "I.4.7"], "comment": "6 pages, 6 figures", "summary": "An intriguing phenomenon about JPEG compression has been observed since two\ndecades ago- after repeating JPEG compression and decompression, it leads to a\nstable image that does not change anymore, which is a fixed point. In this\nwork, we prove the existence of fixed points in the essential JPEG procedures.\nWe analyze JPEG compression and decompression processes, revealing the\nexistence of fixed points that can be reached within a few iterations. These\nfixed points are diverse and preserve the image's visual quality, ensuring\nminimal distortion. This result is used to develop a method to create a\ntamper-evident image from the original authentic image, which can expose\ntampering operations by showing deviations from the fixed point image.", "AI": {"tldr": "JPEG compression and decompression iterations lead to stable fixed-point images, which are used to create tamper-evident images.", "motivation": "To prove the existence of fixed points in JPEG procedures and leverage them for tamper detection.", "method": "Analyze JPEG compression/decompression processes to identify fixed points and develop a tamper-evident method.", "result": "Fixed points exist, preserve visual quality, and enable tamper detection by showing deviations.", "conclusion": "Fixed points in JPEG can be used to create tamper-evident images, exposing manipulations."}}
{"id": "2504.17609", "pdf": "https://arxiv.org/pdf/2504.17609", "abs": "https://arxiv.org/abs/2504.17609", "authors": ["Fengchun Liu", "Tong Zhang", "Chunying Zhang"], "title": "STCL:Curriculum learning Strategies for deep learning image steganography models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.", "AI": {"tldr": "The paper proposes a Steganography Curriculum Learning (STCL) strategy to improve image steganography models by training on easier images first and gradually increasing difficulty, enhancing quality and convergence.", "motivation": "Addresses poor steganographic image quality and slow convergence in deep learning models by introducing a curriculum-based training approach.", "method": "Uses teacher models to evaluate image difficulty and a knee point-based scheduling strategy to control training progression.", "result": "Improves model performance on ALASKA2, VOC2012, and ImageNet datasets with higher PSNR, SSIM, and decoding accuracy, and lower steganography analysis scores.", "conclusion": "The STCL strategy effectively enhances steganography model performance by optimizing training progression and reducing overfitting."}}
{"id": "2504.17577", "pdf": "https://arxiv.org/pdf/2504.17577", "abs": "https://arxiv.org/abs/2504.17577", "authors": ["Lei Wang", "Yu Cheng", "Yining Shi", "Zhengju Tang", "Zhiwen Mo", "Wenhao Xie", "Lingxiao Ma", "Yuqing Xia", "Jilong Xue", "Fan Yang", "Zhi Yang"], "title": "TileLang: A Composable Tiled Programming Model for AI Systems", "categories": ["cs.LG"], "comment": null, "summary": "Modern AI workloads rely heavily on optimized computing kernels for both\ntraining and inference. These AI kernels follow well-defined data-flow\npatterns, such as moving tiles between DRAM and SRAM and performing a sequence\nof computations on those tiles. However, writing high-performance kernels\nremains complex despite the clarity of these patterns. Achieving peak\nperformance requires careful, hardware-centric optimizations to fully leverage\nmodern accelerators. While domain-specific compilers attempt to reduce the\nburden of writing high-performance kernels, they often struggle with usability\nand expressiveness gaps. In this paper, we present TileLang, a generalized\ntiled programming model for more efficient AI Kernel programming. TileLang\ndecouples scheduling space (thread binding, layout, tensorize and pipeline)\nfrom dataflow, and encapsulated them as a set of customization annotations and\nprimitives. This approach allows users to focus on the kernel's data-flow\nitself, while leaving most other optimizations to compilers. We conduct\ncomprehensive experiments on commonly-used devices, across numerous\nexperiments, our evaluation shows that TileLang can achieve state-of-the-art\nperformance in key kernels, demonstrating that its unified block-and-thread\nparadigm and transparent scheduling capabilities deliver both the power and\nflexibility demanded by modern AI system development.", "AI": {"tldr": "TileLang simplifies AI kernel programming by decoupling scheduling from dataflow, enabling high-performance kernels with less effort.", "motivation": "Writing high-performance AI kernels is complex due to hardware-centric optimizations, and existing compilers lack usability and expressiveness.", "method": "TileLang introduces a tiled programming model with customization annotations and primitives to separate scheduling from dataflow.", "result": "TileLang achieves state-of-the-art performance in key kernels, demonstrating its effectiveness.", "conclusion": "TileLang's unified paradigm and transparent scheduling provide the power and flexibility needed for modern AI development."}}
{"id": "2410.05401", "pdf": "https://arxiv.org/pdf/2410.05401", "abs": "https://arxiv.org/abs/2410.05401", "authors": ["Tunazzina Islam", "Dan Goldwasser"], "title": "Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "Climate change communication on social media increasingly employs\nmicrotargeting strategies to effectively reach and influence specific\ndemographic groups. This study presents a post-hoc analysis of microtargeting\npractices within climate campaigns by leveraging large language models (LLMs)\nto examine Facebook advertisements. Our analysis focuses on two key aspects:\ndemographic targeting and fairness. We evaluate the ability of LLMs to\naccurately predict the intended demographic targets, such as gender and age\ngroup, achieving an overall accuracy of 88.55%. Furthermore, we instruct the\nLLMs to generate explanations for their classifications, providing transparent\nreasoning behind each decision. These explanations reveal the specific thematic\nelements used to engage different demographic segments, highlighting distinct\nstrategies tailored to various audiences. Our findings show that young adults\nare primarily targeted through messages emphasizing activism and environmental\nconsciousness, while women are engaged through themes related to caregiving\nroles and social advocacy. In addition to evaluating the effectiveness of LLMs\nin detecting microtargeted messaging, we conduct a comprehensive fairness\nanalysis to identify potential biases in model predictions. Our findings\nindicate that while LLMs perform well overall, certain biases exist,\nparticularly in the classification of senior citizens and male audiences. By\nshowcasing the efficacy of LLMs in dissecting and explaining targeted\ncommunication strategies and by highlighting fairness concerns, this study\nprovides a valuable framework for future research aimed at enhancing\ntransparency, accountability, and inclusivity in social media-driven climate\ncampaigns.", "AI": {"tldr": "The study analyzes microtargeting in climate change campaigns on Facebook using LLMs, achieving 88.55% accuracy in predicting demographic targets. It highlights thematic strategies for different groups and identifies biases in model predictions.", "motivation": "To evaluate the effectiveness and fairness of LLMs in analyzing microtargeting practices in climate change communication on social media.", "method": "Post-hoc analysis of Facebook ads using LLMs to predict demographic targets and generate explanations for classifications.", "result": "LLMs achieved 88.55% accuracy in predicting targets, with distinct strategies for young adults (activism) and women (caregiving). Biases were found in classifying seniors and males.", "conclusion": "LLMs are effective for dissecting microtargeting but require fairness improvements. The study offers a framework for enhancing transparency and inclusivity in climate campaigns."}}
{"id": "2504.17595", "pdf": "https://arxiv.org/pdf/2504.17595", "abs": "https://arxiv.org/abs/2504.17595", "authors": ["Boyue Xu", "Yi Xu", "Ruichao Hou", "Jia Bei", "Tongwei Ren", "Gangshan Wu"], "title": "RGB-D Tracking via Hierarchical Modality Aggregation and Distribution Network", "categories": ["cs.CV"], "comment": null, "summary": "The integration of dual-modal features has been pivotal in advancing\nRGB-Depth (RGB-D) tracking. However, current trackers are less efficient and\nfocus solely on single-level features, resulting in weaker robustness in fusion\nand slower speeds that fail to meet the demands of real-world applications. In\nthis paper, we introduce a novel network, denoted as HMAD (Hierarchical\nModality Aggregation and Distribution), which addresses these challenges. HMAD\nleverages the distinct feature representation strengths of RGB and depth\nmodalities, giving prominence to a hierarchical approach for feature\ndistribution and fusion, thereby enhancing the robustness of RGB-D tracking.\nExperimental results on various RGB-D datasets demonstrate that HMAD achieves\nstate-of-the-art performance. Moreover, real-world experiments further validate\nHMAD's capacity to effectively handle a spectrum of tracking challenges in\nreal-time scenarios.", "AI": {"tldr": "HMAD, a novel hierarchical network, improves RGB-D tracking by efficiently fusing and distributing multi-level RGB and depth features, achieving state-of-the-art performance and real-time capability.", "motivation": "Current RGB-D trackers are inefficient and limited to single-level features, lacking robustness and speed for real-world applications.", "method": "HMAD employs a hierarchical approach to aggregate and distribute RGB and depth features, enhancing fusion robustness.", "result": "HMAD outperforms existing methods on RGB-D datasets and proves effective in real-time tracking scenarios.", "conclusion": "HMAD addresses inefficiencies in RGB-D tracking, offering improved performance and real-time applicability."}}
{"id": "2504.17617", "pdf": "https://arxiv.org/pdf/2504.17617", "abs": "https://arxiv.org/abs/2504.17617", "authors": ["Bruno Casella", "Matthias Jakobs", "Marco Aldinucci", "Sebastian Buschj\u00e4ger"], "title": "Decentralized Time Series Classification with ROCKET Features", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.11; I.2.6"], "comment": "Submitted to Workshop on Federated Learning Advancements 2025, in\n  conjunction with ECML-PKDD, WAFL25", "summary": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md.", "AI": {"tldr": "DROCKS is a decentralized FL framework for time series classification, outperforming client-server FL methods while enhancing robustness and privacy.", "motivation": "Address privacy and robustness issues in federated learning for TSC by eliminating the central server's risks.", "method": "Uses ROCKET features and a decentralized approach where nodes refine the model sequentially.", "result": "Outperforms client-server FL methods and shows resilience to failures and attacks.", "conclusion": "DROCKS offers a robust, privacy-preserving solution for TSC in federated settings."}}
{"id": "2504.17578", "pdf": "https://arxiv.org/pdf/2504.17578", "abs": "https://arxiv.org/abs/2504.17578", "authors": ["Hongshu Guo", "Wenjie Qiu", "Zeyuan Ma", "Xinglin Zhang", "Jun Zhang", "Yue-Jiao Gong"], "title": "Advancing CMA-ES with Learning-Based Cooperative Coevolution for Scalable Optimization", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Recent research in Cooperative Coevolution~(CC) have achieved promising\nprogress in solving large-scale global optimization problems. However, existing\nCC paradigms have a primary limitation in that they require deep expertise for\nselecting or designing effective variable decomposition strategies. Inspired by\nadvancements in Meta-Black-Box Optimization, this paper introduces LCC, a\npioneering learning-based cooperative coevolution framework that dynamically\nschedules decomposition strategies during optimization processes. The\ndecomposition strategy selector is parameterized through a neural network,\nwhich processes a meticulously crafted set of optimization status features to\ndetermine the optimal strategy for each optimization step. The network is\ntrained via the Proximal Policy Optimization method in a reinforcement learning\nmanner across a collection of representative problems, aiming to maximize the\nexpected optimization performance. Extensive experimental results demonstrate\nthat LCC not only offers certain advantages over state-of-the-art baselines in\nterms of optimization effectiveness and resource consumption, but it also\nexhibits promising transferability towards unseen problems.", "AI": {"tldr": "LCC is a learning-based cooperative coevolution framework that dynamically selects decomposition strategies using a neural network, outperforming existing methods in optimization effectiveness and transferability.", "motivation": "Existing CC paradigms require expert knowledge for variable decomposition, limiting their practicality. LCC aims to automate this process using meta-learning.", "method": "LCC uses a neural network trained via Proximal Policy Optimization to dynamically select decomposition strategies based on optimization status features.", "result": "LCC outperforms state-of-the-art baselines in optimization effectiveness and resource efficiency, showing strong transferability to new problems.", "conclusion": "LCC successfully automates decomposition strategy selection, improving optimization performance and adaptability without requiring expert input."}}
{"id": "2410.19878", "pdf": "https://arxiv.org/pdf/2410.19878", "abs": "https://arxiv.org/abs/2410.19878", "authors": ["Luping Wang", "Sheng Chen", "Linnan Jiang", "Shu Pan", "Runze Cai", "Sen Yang", "Fei Yang"], "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The large models, as predicted by scaling raw forecasts, have made\ngroundbreaking progress in many fields, particularly in natural language\ngeneration tasks, where they have approached or even surpassed human levels.\nHowever, the unprecedented scale of their parameters brings significant\ncomputational and storage costs. These large models require substantial\ncomputational resources and GPU memory to operate. When adapting large models\nto specific downstream tasks, their massive parameter scale poses a significant\nchallenge in fine-tuning on hardware platforms with limited computational power\nand GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\noffers a practical solution by efficiently adjusting the parameters of large\npre-trained models to suit various downstream tasks. Specifically, PEFT adjusts\nthe parameters of pre-trained large models to adapt to specific tasks or\ndomains, minimizing the introduction of additional parameters and the\ncomputational resources required. This review mainly introduces the preliminary\nknowledge of PEFT, the core ideas and principles of various PEFT algorithms,\nthe applications of PEFT, and potential future research directions. By reading\nthis review, we believe that interested parties can quickly grasp the PEFT\nmethodology, thereby accelerating its development and innovation.", "AI": {"tldr": "The paper reviews Parameter-Efficient Fine-Tuning (PEFT) as a solution to the computational challenges of large models, detailing its methods, applications, and future directions.", "motivation": "Large models face high computational and storage costs, making fine-tuning difficult on limited hardware. PEFT addresses this by minimizing additional parameters and resources.", "method": "PEFT adjusts pre-trained model parameters for specific tasks with minimal computational overhead. The review covers its core ideas, algorithms, and applications.", "result": "PEFT provides a practical way to adapt large models efficiently, reducing resource demands while maintaining performance.", "conclusion": "The review aims to help readers quickly understand PEFT, fostering its development and innovation in the field."}}
{"id": "2504.17619", "pdf": "https://arxiv.org/pdf/2504.17619", "abs": "https://arxiv.org/abs/2504.17619", "authors": ["Catarina P. Coutinho", "Aneeqa Merhab", "Janko Petkovic", "Ferdinando Zanchetta", "Rita Fioresi"], "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to the 7th International Conference on Geometric Science of\n  Information", "summary": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images.", "AI": {"tldr": "Custom filters based on visual cortex modeling improve CNN performance on occluded MNIST images.", "motivation": "To enhance CNN performance by mimicking the visual cortex's border completion mechanism.", "method": "Mathematical modeling of visual cortex mechanisms to define custom filters for CNNs, tested with a modified LeNet 5 on occluded MNIST images.", "result": "Consistent improvement in performance, especially accuracy.", "conclusion": "Modeling visual cortex mechanisms for custom filters effectively boosts CNN performance in handling occlusions."}}
{"id": "2504.17624", "pdf": "https://arxiv.org/pdf/2504.17624", "abs": "https://arxiv.org/abs/2504.17624", "authors": ["Jigang Fan", "Chunhao Zhu", "Xiaobing Lan", "Haiming Zhuang", "Mingyu Li", "Jian Zhang", "Shaoyong Lu"], "title": "Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled\nreceptor superfamily, plays an important role in modulating dopaminergic\nneuronal activity and eliciting opioid-independent analgesia. Recent studies\nsuggest that promoting \\{beta}-arrestin-biased signaling in NTSR1 may diminish\ndrugs of abuse, such as psychostimulants, thereby offering a potential avenue\nfor treating human addiction-related disorders. In this study, we utilized a\nnovel computational and experimental approach that combined nudged elastic\nband-based molecular dynamics simulations, Markov state models, temporal\ncommunication network analysis, site-directed mutagenesis, and conformational\nbiosensors, to explore the intricate mechanisms underlying NTSR1 activation and\nbiased signaling. Our study reveals a dynamic stepwise transition mechanism and\nactivated transmission network associated with NTSR1 activation. It also yields\nvaluable insights into the complex interplay between the unique polar network,\nnon-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we\nidentified a cryptic allosteric site located in the intracellular region of the\nreceptor that exists in an intermediate state within the activation pathway.\nCollectively, these findings contribute to a more profound understanding of\nNTSR1 activation and biased signaling at the atomic level, thereby providing a\npotential strategy for the development of NTSR1 allosteric modulators in the\nrealm of G protein-coupled receptor biology, biophysics, and medicine.", "AI": {"tldr": "The study explores NTSR1 activation and biased signaling using computational and experimental methods, revealing dynamic mechanisms and a cryptic allosteric site, offering insights for addiction treatment.", "motivation": "NTSR1's role in modulating dopaminergic activity and analgesia suggests its potential for treating addiction, especially through \u03b2-arrestin-biased signaling.", "method": "Combined nudged elastic band-based molecular dynamics simulations, Markov state models, temporal communication network analysis, site-directed mutagenesis, and conformational biosensors.", "result": "Identified a stepwise transition mechanism, transmission network, and a cryptic allosteric site in NTSR1, enhancing understanding of its signaling.", "conclusion": "Findings provide atomic-level insights into NTSR1 activation, aiding the development of allosteric modulators for GPCR biology and medicine."}}
{"id": "2504.17601", "pdf": "https://arxiv.org/pdf/2504.17601", "abs": "https://arxiv.org/abs/2504.17601", "authors": ["Erik Bergh"], "title": "Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation", "categories": ["cs.LG"], "comment": "11 pages, 5 figures", "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.", "AI": {"tldr": "A novel dimensionality reduction method combines linear interpretability with non-linear expressiveness, offering transparent insights and practical tools for analysis.", "motivation": "Address the trade-off between representational power and interpretability in existing methods like t-SNE and PCA.", "method": "Proposes an algorithm using weighted linear transformations with Gaussian functions to create a non-linear mapping, preserving interpretability.", "result": "Provides powerful dimensionality reduction with transparent insights, including tools to analyze transformations.", "conclusion": "The method bridges the gap between interpretability and expressiveness, with practical software for adoption."}}
{"id": "2411.04950", "pdf": "https://arxiv.org/pdf/2411.04950", "abs": "https://arxiv.org/abs/2411.04950", "authors": ["Gideon Yoffe", "Nachum Dershowitz", "Ariel Vishne", "Barak Sober"], "title": "Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a data-centric hypothesis-testing framework to quantify the\ninfluence of sequentially correlated literary properties--such as thematic\ncontinuity--on textual classification tasks. Our method models label sequences\nas stochastic processes and uses an empirical autocovariance matrix to generate\nsurrogate labelings that preserve sequential dependencies. This enables\nstatistical testing to determine whether classification outcomes are primarily\ndriven by thematic structure or by non-sequential features like authorial\nstyle. Applying this framework across a diverse corpus of English prose, we\ncompare traditional (word n-grams and character k-mers) and neural\n(contrastively trained) embeddings in both supervised and unsupervised\nclassification settings. Crucially, our method identifies when classifications\nare confounded by sequentially correlated similarity, revealing that supervised\nand neural models are more prone to false positives--mistaking shared themes\nand cross-genre differences for stylistic signals. In contrast, unsupervised\nmodels using traditional features often yield high true positive rates with\nminimal false positives, especially in genre-consistent settings. By\ndisentangling sequential from non-sequential influences, our approach provides\na principled way to assess and interpret classification reliability. This is\nparticularly impactful for authorship attribution, forensic linguistics, and\nthe analysis of redacted or composite texts, where conventional methods may\nconflate theme with style. Our results demonstrate that controlling for\nsequential correlation is essential for reducing false positives and ensuring\nthat classification outcomes reflect genuine stylistic distinctions.", "AI": {"tldr": "A framework to test if textual classification is influenced by thematic continuity or non-sequential features like style, using surrogate labelings and statistical testing.", "motivation": "To determine if classification outcomes are driven by thematic structure or non-sequential features, addressing potential confounds in tasks like authorship attribution.", "method": "Models label sequences as stochastic processes, uses empirical autocovariance for surrogate labelings, and compares traditional and neural embeddings in supervised/unsupervised settings.", "result": "Supervised and neural models are prone to false positives (confusing themes for style), while unsupervised models with traditional features perform better.", "conclusion": "Controlling for sequential correlation is crucial to reduce false positives and ensure classification reflects genuine stylistic differences, impacting fields like forensic linguistics."}}
{"id": "2504.17626", "pdf": "https://arxiv.org/pdf/2504.17626", "abs": "https://arxiv.org/abs/2504.17626", "authors": ["Ashish Singh", "Michael J. Jones", "Kuan-Chuan Peng", "Anoop Cherian", "Moitreya Chatterjee", "Erik Learned-Miller"], "title": "Improving Open-World Object Localization by Discovering Background", "categories": ["cs.CV"], "comment": null, "summary": "Our work addresses the problem of learning to localize objects in an\nopen-world setting, i.e., given the bounding box information of a limited\nnumber of object classes during training, the goal is to localize all objects,\nbelonging to both the training and unseen classes in an image, during\ninference. Towards this end, recent work in this area has focused on improving\nthe characterization of objects either explicitly by proposing new objective\nfunctions (localization quality) or implicitly using object-centric\nauxiliary-information, such as depth information, pixel/region affinity map\netc. In this work, we address this problem by incorporating background\ninformation to guide the learning of the notion of objectness. Specifically, we\npropose a novel framework to discover background regions in an image and train\nan object proposal network to not detect any objects in these regions. We\nformulate the background discovery task as that of identifying image regions\nthat are not discriminative, i.e., those that are redundant and constitute low\ninformation content. We conduct experiments on standard benchmarks to showcase\nthe effectiveness of our proposed approach and observe significant improvements\nover the previous state-of-the-art approaches for this task.", "AI": {"tldr": "The paper introduces a framework for open-world object localization by leveraging background information to improve objectness learning, outperforming prior methods.", "motivation": "The challenge is to localize objects in an open-world setting, including unseen classes, using limited training data. Existing methods focus on object characterization, but this work explores background information.", "method": "The proposed framework identifies non-discriminative background regions and trains an object proposal network to avoid detecting objects in these areas.", "result": "Experiments on benchmarks show significant improvements over state-of-the-art approaches.", "conclusion": "Incorporating background information effectively enhances open-world object localization."}}
{"id": "2504.17641", "pdf": "https://arxiv.org/pdf/2504.17641", "abs": "https://arxiv.org/abs/2504.17641", "authors": ["Shengtao Zhang", "Haokai Zhang", "Shiqi Lou", "Zicheng Wang", "Zinan Zeng", "Yilin Wang", "Minnan Luo"], "title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD.", "AI": {"tldr": "PTCL is a method for dynamic node classification using only final timestamp labels, employing pseudo-labels and temporal curriculum learning.", "motivation": "Dynamic node classification often lacks complete labels due to high costs and uncertainty. Final labels are easier to obtain, but existing methods require full label history.", "method": "PTCL uses a temporal decoupling architecture (backbone and decoder) and Temporal Curriculum Learning to prioritize pseudo-labels closer to the final timestamp.", "result": "PTCL outperforms other methods, demonstrated on the new CoOAG dataset and real-world scenarios.", "conclusion": "PTCL and the FLiD framework provide a solution for label-limited dynamic node classification, with potential for broader applications."}}
{"id": "2504.17613", "pdf": "https://arxiv.org/pdf/2504.17613", "abs": "https://arxiv.org/abs/2504.17613", "authors": ["Bowen Deng", "Chang Xu", "Hao Li", "Yuhao Huang", "Min Hou", "Jiang Bian"], "title": "TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation", "categories": ["cs.LG"], "comment": null, "summary": "Synthetic Electronic Health Record (EHR) time-series generation is crucial\nfor advancing clinical machine learning models, as it helps address data\nscarcity by providing more training data. However, most existing approaches\nfocus primarily on replicating statistical distributions and temporal\ndependencies of real-world data. We argue that fidelity to observed data alone\ndoes not guarantee better model performance, as common patterns may dominate,\nlimiting the representation of rare but important conditions. This highlights\nthe need for generate synthetic samples to improve performance of specific\nclinical models to fulfill their target outcomes. To address this, we propose\nTarDiff, a novel target-oriented diffusion framework that integrates\ntask-specific influence guidance into the synthetic data generation process.\nUnlike conventional approaches that mimic training data distributions, TarDiff\noptimizes synthetic samples by quantifying their expected contribution to\nimproving downstream model performance through influence functions.\nSpecifically, we measure the reduction in task-specific loss induced by\nsynthetic samples and embed this influence gradient into the reverse diffusion\nprocess, thereby steering the generation towards utility-optimized data.\nEvaluated on six publicly available EHR datasets, TarDiff achieves\nstate-of-the-art performance, outperforming existing methods by up to 20.4% in\nAUPRC and 18.4% in AUROC. Our results demonstrate that TarDiff not only\npreserves temporal fidelity but also enhances downstream model performance,\noffering a robust solution to data scarcity and class imbalance in healthcare\nanalytics.", "AI": {"tldr": "TarDiff is a target-oriented diffusion framework for generating synthetic EHR time-series data, optimizing for downstream model performance rather than just replicating data distributions.", "motivation": "Existing synthetic EHR generation methods focus on statistical fidelity but may overlook rare conditions, limiting model performance. TarDiff addresses this by prioritizing utility for specific clinical tasks.", "method": "TarDiff integrates task-specific influence guidance into diffusion models, measuring synthetic samples' impact on downstream model loss and embedding this gradient into the generation process.", "result": "TarDiff outperforms existing methods by up to 20.4% in AUPRC and 18.4% in AUROC across six EHR datasets, enhancing both temporal fidelity and model performance.", "conclusion": "TarDiff offers a robust solution to data scarcity and class imbalance in healthcare analytics by generating utility-optimized synthetic data."}}
{"id": "2412.08802", "pdf": "https://arxiv.org/pdf/2412.08802", "abs": "https://arxiv.org/abs/2412.08802", "authors": ["Andreas Koukounas", "Georgios Mastrapas", "Sedigheh Eslami", "Bo Wang", "Mohammad Kalim Akram", "Michael G\u00fcnther", "Isabelle Mohr", "Saba Sturua", "Nan Wang", "Han Xiao"], "title": "jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images", "categories": ["cs.CL", "cs.CV", "cs.IR", "68T50", "I.2.7; I.2.10"], "comment": "30 pages, 1-10 main paper, 10-12 refs, 12-30 benchmarks", "summary": "Contrastive Language-Image Pretraining (CLIP) has been widely used for\ncrossmodal information retrieval and multimodal understanding tasks. However,\nCLIP models are mainly optimized for crossmodal vision-language tasks and\nunderperform in single-mode text tasks. Moreover, these models are often\ntrained on English datasets and therefore lack multilingual understanding.\nAdditionally, from a visual understanding perspective, previous CLIP-based\nmodels exhibit insufficient understanding of visually rich documents. In this\nwork, we propose jina-clip-v2, a contrastive vision-language model trained on\ntext pairs, triplets and image-text pairs via a multi-task and multi-stage\ncontrastive learning paradigm in order to support both text-only and crossmodal\ntasks. We employ a multilingual text encoder and expand the training dataset to\ninclude multilingual texts from 29 non-English languages, including Hindi,\nChinese, German, French, and others, as well as images of visually rich\ndocuments. We evaluate the model's performance and show that jina-clip-v2\nachieves notable improvements over state-of-the-art CLIP-based models in\nzero-shot text-only retrieval, semantic textual similarity, and crossmodal\nretrieval tasks in both English and multilingual settings. jina-clip-v2 also\nprovides for flexibility in embedding dimensionality, enabling users to select\nthe granularity of the representations. jina-clip-v2 is publicly available at\nhttps://huggingface.co/jinaai/jina-clip-v2.", "AI": {"tldr": "jina-clip-v2 improves CLIP by supporting text-only tasks, multilingual understanding, and visually rich documents via multi-task contrastive learning.", "motivation": "CLIP models underperform in text-only tasks, lack multilingual support, and struggle with visually rich documents.", "method": "Uses multilingual text encoder and multi-task contrastive learning on text pairs, triplets, and image-text pairs.", "result": "Outperforms CLIP in zero-shot text retrieval, semantic similarity, and crossmodal tasks in multilingual settings.", "conclusion": "jina-clip-v2 enhances CLIP's versatility and performance, offering flexible embedding dimensions."}}
{"id": "2504.17636", "pdf": "https://arxiv.org/pdf/2504.17636", "abs": "https://arxiv.org/abs/2504.17636", "authors": ["Vojtech Panek", "Qunjie Zhou", "Yaqing Ding", "S\u00e9rgio Agostinho", "Zuzana Kukelova", "Torsten Sattler", "Laura Leal-Taix\u00e9"], "title": "A Guide to Structureless Visual Localization", "categories": ["cs.CV", "I.2.10; I.4.8; I.4.9"], "comment": null, "summary": "Visual localization algorithms, i.e., methods that estimate the camera pose\nof a query image in a known scene, are core components of many applications,\nincluding self-driving cars and augmented / mixed reality systems.\nState-of-the-art visual localization algorithms are structure-based, i.e., they\nstore a 3D model of the scene and use 2D-3D correspondences between the query\nimage and 3D points in the model for camera pose estimation. While such\napproaches are highly accurate, they are also rather inflexible when it comes\nto adjusting the underlying 3D model after changes in the scene. Structureless\nlocalization approaches represent the scene as a database of images with known\nposes and thus offer a much more flexible representation that can be easily\nupdated by adding or removing images. Although there is a large amount of\nliterature on structure-based approaches, there is significantly less work on\nstructureless methods. Hence, this paper is dedicated to providing the, to the\nbest of our knowledge, first comprehensive discussion and comparison of\nstructureless methods. Extensive experiments show that approaches that use a\nhigher degree of classical geometric reasoning generally achieve higher pose\naccuracy. In particular, approaches based on classical absolute or\nsemi-generalized relative pose estimation outperform very recent methods based\non pose regression by a wide margin. Compared with state-of-the-art\nstructure-based approaches, the flexibility of structureless methods comes at\nthe cost of (slightly) lower pose accuracy, indicating an interesting direction\nfor future work.", "AI": {"tldr": "The paper provides the first comprehensive discussion and comparison of structureless visual localization methods, highlighting their flexibility and slightly lower accuracy compared to structure-based approaches.", "motivation": "Visual localization is crucial for applications like self-driving cars and AR/VR, but structure-based methods lack flexibility. Structureless methods, though less studied, offer easier updates.", "method": "The paper reviews and compares structureless methods, focusing on classical geometric reasoning versus newer pose regression techniques.", "result": "Classical geometric methods outperform pose regression in accuracy. Structureless methods are more flexible but slightly less accurate than structure-based ones.", "conclusion": "Structureless methods offer a promising trade-off between flexibility and accuracy, suggesting future work to bridge the gap with structure-based approaches."}}
{"id": "2504.17655", "pdf": "https://arxiv.org/pdf/2504.17655", "abs": "https://arxiv.org/abs/2504.17655", "authors": ["Farhad Pourkamali-Anaraki"], "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "17 pages, 5 figures, and 2 tables", "summary": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies.", "AI": {"tldr": "The paper analyzes conformal prediction methods on a challenging aerial image dataset, showing their effectiveness in providing uncertainty estimates with statistical guarantees, even with limited labeled data.", "motivation": "To address the challenges of data-scarce and highly variable real-world settings, unlike standard benchmarks, and evaluate the reliability and efficiency of conformal prediction.", "method": "Leverages pretrained models (MobileNet, DenseNet, ResNet) fine-tuned with limited labeled data, compares pipelines with/without temperature scaling, and assesses performance using empirical coverage and average prediction set size.", "result": "Conformal prediction yields valuable uncertainty estimates even with small labeled samples. Temperature scaling doesn't consistently reduce prediction set size, and model compression shows potential for resource-constrained environments.", "conclusion": "Future research should explore noisy/ambiguous labels' impact and effective model reduction strategies for conformal prediction."}}
{"id": "2504.17618", "pdf": "https://arxiv.org/pdf/2504.17618", "abs": "https://arxiv.org/abs/2504.17618", "authors": ["Nikita Gabdullin"], "title": "The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages, 10 figures, 4 tables, 4 equations", "summary": "Hessians of neural network (NN) contain essential information about the\ncurvature of NN loss landscapes which can be used to estimate NN generalization\ncapabilities. We have previously proposed generalization criteria that rely on\nthe observation that Hessian eigenvalue spectral density (HESD) behaves\nsimilarly for a wide class of NNs. This paper further studies their\napplicability by investigating factors that can result in different types of\nHESD. We conduct a wide range of experiments showing that HESD mainly has\npositive eigenvalues (MP-HESD) for NN training and fine-tuning with various\noptimizers on different datasets with different preprocessing and augmentation\nprocedures. We also show that mainly negative HESD (MN-HESD) is a consequence\nof external gradient manipulation, indicating that the previously proposed\nHessian analysis methodology cannot be applied in such cases. We also propose\ncriteria and corresponding conditions to determine HESD type and estimate NN\ngeneralization potential. These HESD types and previously proposed\ngeneralization criteria are combined into a unified HESD analysis methodology.\nFinally, we discuss how HESD changes during training, and show the occurrence\nof quasi-singular (QS) HESD and its influence on the proposed methodology and\non the conventional assumptions about the relation between Hessian eigenvalues\nand NN loss landscape curvature.", "AI": {"tldr": "The paper explores Hessian eigenvalue spectral density (HESD) in neural networks, identifying mainly positive (MP-HESD) and negative (MN-HESD) types, and proposes criteria to determine HESD type for generalization estimation.", "motivation": "To understand factors influencing HESD types and their implications for neural network generalization.", "method": "Conducts experiments with various optimizers, datasets, and preprocessing to analyze HESD behavior, including external gradient manipulation effects.", "result": "MP-HESD is common in training/fine-tuning, while MN-HESD arises from gradient manipulation. Quasi-singular HESD is observed, impacting Hessian-based generalization criteria.", "conclusion": "Proposes a unified HESD analysis methodology, highlighting limitations of conventional Hessian assumptions and the need for careful application in gradient-manipulated scenarios."}}
{"id": "2501.14936", "pdf": "https://arxiv.org/pdf/2501.14936", "abs": "https://arxiv.org/abs/2501.14936", "authors": ["David Boldo", "Lily Pemberton", "Gabriel Thistledown", "Jacob Fairchild", "Felix Kowalski"], "title": "Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "The integration of contextual embeddings into the optimization processes of\nlarge language models is an advancement in natural language processing. The\nContext-Aware Neural Gradient Mapping framework introduces a dynamic gradient\nadjustment mechanism, incorporating contextual embeddings directly into the\noptimization process. This approach facilitates real-time parameter\nadjustments, enhancing task-specific generalization even in the presence of\nsparse or noisy data inputs. The mathematical foundation of this framework\nrelies on gradient descent modifications, where contextual embeddings are\nderived from a supplementary neural network trained to map input features to\noptimal adaptation gradients. By employing differential geometry principles,\nhigh-dimensional input dependencies are encoded into low-dimensional gradient\nmanifolds, enabling efficient adaptation without necessitating the retraining\nof the entire model. Empirical evaluations demonstrate that the proposed\nframework consistently outperforms baseline models across various metrics,\nincluding accuracy, robustness to noise, and computational efficiency. The\nintegration of context-specific embeddings allows for a more complex\nunderstanding of language, thereby improving the model's ability to handle\ndiverse linguistic phenomena. Furthermore, the computational efficiency\nachieved through this method demonstrates its scalability for large-scale\nlanguage models operating under diverse constraints.", "AI": {"tldr": "The paper introduces a Context-Aware Neural Gradient Mapping framework for dynamic gradient adjustments in large language models, improving generalization and efficiency.", "motivation": "Enhancing task-specific generalization and computational efficiency in language models, especially with sparse or noisy data.", "method": "Uses contextual embeddings and gradient descent modifications, leveraging differential geometry for efficient adaptation.", "result": "Outperforms baselines in accuracy, robustness, and efficiency, scaling well for large models.", "conclusion": "The framework improves language understanding and scalability, making it viable for diverse applications."}}
{"id": "2504.17643", "pdf": "https://arxiv.org/pdf/2504.17643", "abs": "https://arxiv.org/abs/2504.17643", "authors": ["Steve G\u00f6ring"], "title": "CLIPSE -- a minimalistic CLIP-based image search engine for research", "categories": ["cs.CV"], "comment": null, "summary": "A brief overview of CLIPSE, a self-hosted image search engine with the main\napplication of research, is provided. In general, CLIPSE uses CLIP embeddings\nto process the images and also the text queries. The overall framework is\ndesigned with simplicity to enable easy extension and usage. Two benchmark\nscenarios are described and evaluated, covering indexing and querying time. It\nis shown that CLIPSE is capable of handling smaller datasets; for larger\ndatasets, a distributed approach with several instances should be considered.", "AI": {"tldr": "CLIPSE is a self-hosted image search engine using CLIP embeddings for images and text queries, designed for simplicity and extensibility. It handles smaller datasets well, but larger datasets may require a distributed approach.", "motivation": "To provide a simple, extensible framework for image search using CLIP embeddings, catering to research applications.", "method": "Uses CLIP embeddings for processing images and text queries, evaluated in benchmark scenarios for indexing and querying time.", "result": "Effective for smaller datasets; larger datasets need a distributed setup.", "conclusion": "CLIPSE is a practical solution for small-scale image search, with scalability options for larger datasets."}}
{"id": "2504.17663", "pdf": "https://arxiv.org/pdf/2504.17663", "abs": "https://arxiv.org/abs/2504.17663", "authors": ["Michelle L. Ding", "Harini Suresh"], "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps.", "AI": {"tldr": "The paper examines AI governance's role in preventing deep fake pornography (AIG-NCII), highlighting flaws in current practices.", "motivation": "To address the misuse of AI in creating non-consensual intimate images (AIG-NCII) and the lack of effective governance.", "method": "Identifies a 'malicious technical ecosystem' (MTE) of tools enabling AIG-NCII creation and critiques current governance methods using the NIST AI 100-4 report.", "result": "Current governance fails to regulate the MTE for adult AIG-NCII due to flawed assumptions and gaps.", "conclusion": "The study underscores the need for improved sociotechnical AI governance to combat AIG-NCII effectively."}}
{"id": "2504.17660", "pdf": "https://arxiv.org/pdf/2504.17660", "abs": "https://arxiv.org/abs/2504.17660", "authors": ["Julius Vetter", "Manuel Gloeckler", "Daniel Gedon", "Jakob H. Macke"], "title": "Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Simulation-based inference (SBI) offers a flexible and general approach to\nperforming Bayesian inference: In SBI, a neural network is trained on synthetic\ndata simulated from a model and used to rapidly infer posterior distributions\nfor observed data. A key goal for SBI is to achieve accurate inference with as\nfew simulations as possible, especially for expensive simulators. In this work,\nwe address this challenge by repurposing recent probabilistic foundation models\nfor tabular data: We show how tabular foundation models -- specifically TabPFN\n-- can be used as pre-trained autoregressive conditional density estimators for\nSBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks\n(NPE-PF) and show that it is competitive with current SBI approaches in terms\nof accuracy for both benchmark tasks and two complex scientific inverse\nproblems. Crucially, it often substantially outperforms them in terms of\nsimulation efficiency, sometimes requiring orders of magnitude fewer\nsimulations. NPE-PF eliminates the need for inference network selection,\ntraining, and hyperparameter tuning. We also show that it exhibits superior\nrobustness to model misspecification and can be scaled to simulation budgets\nthat exceed the context size limit of TabPFN. NPE-PF provides a new direction\nfor SBI, where training-free, general-purpose inference models offer efficient,\neasy-to-use, and flexible solutions for a wide range of stochastic inverse\nproblems.", "AI": {"tldr": "NPE-PF leverages tabular foundation models like TabPFN for efficient, training-free Bayesian inference, outperforming traditional SBI methods in simulation efficiency and robustness.", "motivation": "To achieve accurate Bayesian inference with minimal simulations, especially for costly simulators, by repurposing probabilistic foundation models for tabular data.", "method": "Proposes Neural Posterior Estimation with Prior-data Fitted Networks (NPE-PF), using TabPFN as a pre-trained autoregressive conditional density estimator for SBI.", "result": "NPE-PF matches or surpasses current SBI methods in accuracy and significantly reduces required simulations, while eliminating network selection and training.", "conclusion": "NPE-PF offers a scalable, robust, and user-friendly approach to SBI, enabling efficient inference for diverse stochastic inverse problems."}}
{"id": "2502.01673", "pdf": "https://arxiv.org/pdf/2502.01673", "abs": "https://arxiv.org/abs/2502.01673", "authors": ["Arpita Vats", "Rahul Raja", "Mrinal Mathur", "Vinija Jain", "Aman Chadha"], "title": "Multilingual State Space Models for Structured Question Answering in Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at NAACL", "summary": "The diversity and complexity of Indic languages present unique challenges for\nnatural language processing (NLP) tasks, particularly in the domain of question\nanswering (QA).To address these challenges, this paper explores the application\nof State Space Models (SSMs),to build efficient and contextually aware QA\nsystems tailored for Indic languages. SSMs are particularly suited for this\ntask due to their ability to model long-term and short-term dependencies in\nsequential data, making them well-equipped to handle the rich morphology,\ncomplex syntax, and contextual intricacies characteristic of Indian languages.\nWe evaluated multiple SSM architectures across diverse datasets representing\nvarious Indic languages and conducted a comparative analysis of their\nperformance. Our results demonstrate that these models effectively capture\nlinguistic subtleties, leading to significant improvements in question\ninterpretation, context alignment, and answer generation. This work represents\nthe first application of SSMs to question answering tasks in Indic languages,\nestablishing a foundational benchmark for future research in this domain. We\npropose enhancements to existing SSM frameworks, optimizing their applicability\nto low-resource settings and multilingual scenarios prevalent in Indic\nlanguages.", "AI": {"tldr": "The paper explores State Space Models (SSMs) for question answering in Indic languages, demonstrating their effectiveness in handling linguistic complexities and improving QA performance.", "motivation": "Indic languages pose unique NLP challenges due to their diversity and complexity, requiring specialized solutions for QA tasks.", "method": "The study evaluates multiple SSM architectures on diverse Indic language datasets, analyzing their performance in capturing linguistic nuances.", "result": "SSMs significantly improve question interpretation, context alignment, and answer generation, setting a benchmark for future research.", "conclusion": "The work pioneers SSMs for Indic QA, proposing framework enhancements for low-resource and multilingual settings."}}
{"id": "2504.17670", "pdf": "https://arxiv.org/pdf/2504.17670", "abs": "https://arxiv.org/abs/2504.17670", "authors": ["Lutao Jiang", "Jiantao Lin", "Kanghao Chen", "Wenhang Ge", "Xin Yang", "Yifan Jiang", "Yuanhuiyi Lyu", "Xu Zheng", "Yingcong Chen"], "title": "DiMeR: Disentangled Mesh Reconstruction Model", "categories": ["cs.CV"], "comment": "Project Page: https://lutao2021.github.io/DiMeR_page/", "summary": "With the advent of large-scale 3D datasets, feed-forward 3D generative\nmodels, such as the Large Reconstruction Model (LRM), have gained significant\nattention and achieved remarkable success. However, we observe that RGB images\noften lead to conflicting training objectives and lack the necessary clarity\nfor geometry reconstruction. In this paper, we revisit the inductive biases\nassociated with mesh reconstruction and introduce DiMeR, a novel disentangled\ndual-stream feed-forward model for sparse-view mesh reconstruction. The key\nidea is to disentangle both the input and framework into geometry and texture\nparts, thereby reducing the training difficulty for each part according to the\nPrinciple of Occam's Razor. Given that normal maps are strictly consistent with\ngeometry and accurately capture surface variations, we utilize normal maps as\nexclusive input for the geometry branch to reduce the complexity between the\nnetwork's input and output. Moreover, we improve the mesh extraction algorithm\nto introduce 3D ground truth supervision. As for texture branch, we use RGB\nimages as input to obtain the textured mesh. Overall, DiMeR demonstrates robust\ncapabilities across various tasks, including sparse-view reconstruction,\nsingle-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR\nsignificantly outperforms previous methods, achieving over 30% improvement in\nChamfer Distance on the GSO and OmniObject3D dataset.", "AI": {"tldr": "DiMeR introduces a disentangled dual-stream model for sparse-view mesh reconstruction, using normal maps for geometry and RGB images for texture, achieving significant performance improvements.", "motivation": "RGB images in 3D reconstruction often cause conflicting training objectives and lack clarity for geometry, prompting the need for a disentangled approach.", "method": "DiMeR disentangles input and framework into geometry (using normal maps) and texture (using RGB images) streams, improving mesh extraction and supervision.", "result": "DiMeR outperforms previous methods by over 30% in Chamfer Distance on GSO and OmniObject3D datasets, excelling in sparse-view, single-image, and text-to-3D tasks.", "conclusion": "DiMeR's disentangled approach effectively addresses training conflicts and improves reconstruction accuracy, setting a new benchmark in 3D generative models."}}
{"id": "2504.17675", "pdf": "https://arxiv.org/pdf/2504.17675", "abs": "https://arxiv.org/abs/2504.17675", "authors": ["Caroline Panggabean", "Devaraj Verma C", "Bhagyashree Gogoi", "Ranju Limbu", "Rhythm Sarker"], "title": "Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance", "categories": ["cs.DC", "cs.AI"], "comment": "7 pages, 5 figures, accepted for publication (not yet published)", "summary": "Cloud computing environments demand dynamic and efficient resource management\nto ensure optimal performance, reduced energy consumption, and adherence to\nService Level Agreements (SLAs). This paper presents a Genetic Algorithm\n(GA)-based approach for Virtual Machine (VM) placement and consolidation,\naiming to minimize power usage while maintaining QoS constraints. The proposed\nmethod dynamically adjusts VM allocation based on real-time workload\nvariations, outperforming traditional heuristics such as First Fit Decreasing\n(FFD) and Best Fit Decreasing (BFD). Experimental results show notable\nreductions in energy consumption, VM migrations, SLA violation rates, and\nexecution time. A correlation heatmap further illustrates strong relationships\namong these key performance indicators, confirming the effectiveness of our\napproach in optimizing cloud resource utilization.", "AI": {"tldr": "A GA-based VM placement method reduces energy use and SLA violations while outperforming traditional heuristics like FFD and BFD.", "motivation": "Dynamic resource management in cloud computing is needed for performance, energy efficiency, and SLA adherence.", "method": "Uses a Genetic Algorithm for dynamic VM placement and consolidation, adapting to real-time workload changes.", "result": "Significant reductions in energy consumption, VM migrations, SLA violations, and execution time.", "conclusion": "The GA-based approach effectively optimizes cloud resource utilization, confirmed by performance indicators."}}
{"id": "2504.17664", "pdf": "https://arxiv.org/pdf/2504.17664", "abs": "https://arxiv.org/abs/2504.17664", "authors": ["Gr\u00e9gory Bournassenko"], "title": "On Multivariate Financial Time Series Classification", "categories": ["cs.LG"], "comment": null, "summary": "This article investigates the use of Machine Learning and Deep Learning\nmodels in multivariate time series analysis within financial markets. It\ncompares small and big data approaches, focusing on their distinct challenges\nand the benefits of scaling. Traditional methods such as SVMs are contrasted\nwith modern architectures like ConvTimeNet. The results show the importance of\nusing and understanding Big Data in depth in the analysis and prediction of\nfinancial time series.", "AI": {"tldr": "Comparison of ML/DL models in financial time series analysis, highlighting Big Data's role.", "motivation": "To explore the effectiveness of small vs. big data approaches and modern vs. traditional models in financial markets.", "method": "Comparison of SVM (traditional) and ConvTimeNet (modern) models, focusing on scalability and challenges.", "result": "Big Data is crucial for accurate financial time series analysis and prediction.", "conclusion": "Understanding and leveraging Big Data enhances financial market predictions."}}
{"id": "2502.05346", "pdf": "https://arxiv.org/pdf/2502.05346", "abs": "https://arxiv.org/abs/2502.05346", "authors": ["Christopher Nightingale", "Dominic Lavington", "Jonathan Thistlethwaite", "Sebastian Penhaligon", "Thomas Belinski", "David Boldo"], "title": "Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models", "categories": ["cs.CL"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship", "summary": "Representing token embeddings as probability distributions over learned\nmanifolds allows for more flexible contextual inference, reducing\nrepresentational rigidity while enhancing semantic granularity. Comparative\nevaluations demonstrate that probabilistic embeddings improve neighborhood\nconsistency and decrease redundancy, ensuring that token relationships remain\nmore structurally coherent across fine-tuning iterations. The integration of\nprobabilistic subspaces within attention mechanisms facilitates more adaptive\ncontextual weighting, enabling models to capture latent dependencies that would\notherwise be obscured in conventional embeddings. Experimental results\nhighlight increased robustness against adversarial modifications, with\nprobabilistic embeddings preserving contextual integrity even under\nperturbation-based evaluation scenarios. Performance assessments indicate that\nprobabilistic representations achieve greater adaptability in domain-specific\napplications, mitigating the need for extensive retraining when shifting across\nlinguistic domains. Computational trade-offs remain within operationally\nfeasible limits, with marginal increases in inference latency balanced against\nthe benefits of enhanced representation stability and contextual\nexpressiveness. The capacity to encode structured uncertainty provides\nadvantages in generative modeling tasks, particularly where maintaining\ncoherence across extended sequences requires a representation framework capable\nof handling ambiguous or context-dependent linguistic constructs.", "AI": {"tldr": "Probabilistic token embeddings improve semantic granularity and contextual coherence, enhancing robustness and adaptability in NLP tasks.", "motivation": "To address the rigidity of conventional embeddings and improve contextual inference and semantic granularity.", "method": "Represent token embeddings as probability distributions over learned manifolds and integrate probabilistic subspaces within attention mechanisms.", "result": "Improved neighborhood consistency, reduced redundancy, increased robustness against adversarial modifications, and better adaptability across domains.", "conclusion": "Probabilistic embeddings offer enhanced representation stability, contextual expressiveness, and advantages in generative modeling tasks."}}
{"id": "2504.17695", "pdf": "https://arxiv.org/pdf/2504.17695", "abs": "https://arxiv.org/abs/2504.17695", "authors": ["Alp\u00e1r Cseke", "Shashank Tripathi", "Sai Kumar Dwivedi", "Arjun Lakshmipathy", "Agniv Chatterjee", "Michael J. Black", "Dimitrios Tzionas"], "title": "PICO: Reconstructing 3D People In Contact with Objects", "categories": ["cs.CV"], "comment": "Accepted in CVPR'25. Project Page: https://pico.is.tue.mpg.de", "summary": "Recovering 3D Human-Object Interaction (HOI) from single color images is\nchallenging due to depth ambiguities, occlusions, and the huge variation in\nobject shape and appearance. Thus, past work requires controlled settings such\nas known object shapes and contacts, and tackles only limited object classes.\nInstead, we need methods that generalize to natural images and novel object\nclasses. We tackle this in two main ways: (1) We collect PICO-db, a new dataset\nof natural images uniquely paired with dense 3D contact on both body and object\nmeshes. To this end, we use images from the recent DAMON dataset that are\npaired with contacts, but these contacts are only annotated on a canonical 3D\nbody. In contrast, we seek contact labels on both the body and the object. To\ninfer these given an image, we retrieve an appropriate 3D object mesh from a\ndatabase by leveraging vision foundation models. Then, we project DAMON's body\ncontact patches onto the object via a novel method needing only 2 clicks per\npatch. This minimal human input establishes rich contact correspondences\nbetween bodies and objects. (2) We exploit our new dataset of contact\ncorrespondences in a novel render-and-compare fitting method, called PICO-fit,\nto recover 3D body and object meshes in interaction. PICO-fit infers contact\nfor the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db\nfor that object, and uses the contact to iteratively fit the 3D body and object\nmeshes to image evidence via optimization. Uniquely, PICO-fit works well for\nmany object categories that no existing method can tackle. This is crucial to\nenable HOI understanding to scale in the wild. Our data and code are available\nat https://pico.is.tue.mpg.de.", "AI": {"tldr": "The paper introduces PICO-db, a dataset for 3D Human-Object Interaction (HOI) recovery from natural images, and PICO-fit, a method leveraging this dataset for accurate 3D mesh fitting.", "motivation": "Existing methods for 3D HOI recovery are limited to controlled settings and specific object classes. The goal is to generalize to natural images and novel objects.", "method": "(1) Create PICO-db by pairing natural images with dense 3D contact labels on body and object meshes. (2) Develop PICO-fit, a render-and-compare method using contact correspondences to fit 3D meshes to images.", "result": "PICO-fit outperforms existing methods, handling diverse object categories and enabling scalable HOI understanding.", "conclusion": "The approach advances 3D HOI recovery by generalizing to natural scenes and novel objects, with publicly available data and code."}}
{"id": "2504.17677", "pdf": "https://arxiv.org/pdf/2504.17677", "abs": "https://arxiv.org/abs/2504.17677", "authors": ["Jarne Thys", "Sebe Vanbrabant", "Davy Vanacken", "Gustavo Rovelo Ruiz"], "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.", "AI": {"tldr": "The paper introduces INSIGHT, a modular AI tool to assist teaching staff and students by analyzing student questions, building dynamic FAQs, and enabling personalized support.", "motivation": "To address the challenges and opportunities of integrating AI, like LLMs, into education, balancing benefits (e.g., personalized teaching) with concerns (e.g., reduced student-teacher interaction).", "method": "INSIGHT uses a modular design to integrate AI tools, analyzes student questions via keyword extraction, and dynamically builds FAQs to support personalized teaching.", "result": "INSIGHT provides insights for teaching staff to offer tailored support and aims to enhance interactive and inclusive learning.", "conclusion": "Future work could expand INSIGHT for adaptive learning, adjusting content based on student progress and styles."}}
{"id": "2504.17703", "pdf": "https://arxiv.org/pdf/2504.17703", "abs": "https://arxiv.org/abs/2504.17703", "authors": ["Edward Collins", "Michel Wang"], "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems.", "AI": {"tldr": "A survey on Federated Learning (FL), covering its architecture, lifecycle, challenges, trends, applications, and future directions.", "motivation": "Address data privacy, security, and regulatory concerns by enabling collaborative model training without centralizing sensitive data.", "method": "Decentralized approach involving local training, model aggregation, and global updates, with techniques like differential privacy and secure aggregation.", "result": "FL is effective in domains like healthcare and finance, but faces challenges like non-IID data and communication overhead.", "conclusion": "FL is promising but requires further research to improve scalability, efficiency, and trustworthiness."}}
{"id": "2502.11569", "pdf": "https://arxiv.org/pdf/2502.11569", "abs": "https://arxiv.org/abs/2502.11569", "authors": ["Gaurav Srivastava", "Shuxiang Cao", "Xuan Wang"], "title": "Towards Reasoning Ability of Small Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "# fixed some typos, added public slm reasoning leaderboard", "summary": "Reasoning has long been viewed as an emergent property of large language\nmodels (LLMs), appearing at or above a certain scale ($\\sim$100B parameters).\nHowever, recent studies challenge this assumption, showing that small language\nmodels (SLMs) can also achieve competitive reasoning performance. SLMs are\nincreasingly favored for their efficiency and deployability. However, there is\na lack of systematic study on the reasoning abilities of diverse SLMs,\nincluding those trained from scratch or derived from LLMs through quantization,\npruning, and distillation. This raises a critical question: Can SLMs achieve\nreasoning abilities comparable to LLMs? In this work, we systematically survey,\nbenchmark, and analyze 72 SLMs from six model families across 14 reasoning\nbenchmarks. For reliable evaluation, we examine four evaluation methods and\ncompare four LLM judges against human evaluations on 800 data points. We repeat\nall experiments three times to ensure a robust performance assessment.\nAdditionally, we analyze the impact of different prompting strategies in small\nmodels. Beyond accuracy, we also evaluate model robustness under adversarial\nconditions and intermediate reasoning steps. Our findings challenge the\nassumption that scaling is the only way to achieve strong reasoning. Instead,\nwe foresee a future where SLMs with strong reasoning capabilities can be\ndeveloped through structured training or post-training compression. They can\nserve as efficient alternatives to LLMs for reasoning-intensive tasks.", "AI": {"tldr": "Small language models (SLMs) can achieve reasoning abilities comparable to large language models (LLMs) through structured training or compression, challenging the assumption that reasoning emerges only at scale.", "motivation": "To investigate whether SLMs can match LLMs in reasoning performance, given their efficiency and deployability advantages.", "method": "Systematically survey, benchmark, and analyze 72 SLMs from six families across 14 reasoning benchmarks, using four evaluation methods and comparing LLM judges to human evaluations.", "result": "SLMs can achieve reasoning abilities comparable to LLMs, with structured training or post-training compression enabling strong performance.", "conclusion": "Scaling is not the only path to strong reasoning; SLMs can serve as efficient alternatives for reasoning-intensive tasks."}}
{"id": "2504.17696", "pdf": "https://arxiv.org/pdf/2504.17696", "abs": "https://arxiv.org/abs/2504.17696", "authors": ["Ghazal Kaviani", "Yavuz Yarici", "Seulgi Kim", "Mohit Prabhushankar", "Ghassan AlRegib", "Mashhour Solh", "Ameya Patil"], "title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/", "AI": {"tldr": "DARai is a multimodal dataset with hierarchical annotations for understanding human activities, featuring 200+ hours of data from 20 sensors across 10 environments. It supports tasks like recognition, localization, and anticipation, highlighting sensor limitations.", "motivation": "To capture the complexity of human activities in real-world settings and enable research in human-centered AI applications.", "method": "Constructed a dataset with 50 participants in 10 environments, annotated hierarchically (L1-L3). Conducted unimodal/multimodal experiments for recognition, localization, and anticipation.", "result": "DARai reveals challenges in sensor fusion and domain variance, with 22.7% L2 and 14.2% L3 overlaps.", "conclusion": "DARai is a valuable resource for AI research, offering insights into human activity understanding and sensor limitations."}}
{"id": "2504.17717", "pdf": "https://arxiv.org/pdf/2504.17717", "abs": "https://arxiv.org/abs/2504.17717", "authors": ["\u00d3scar Escudero-Arnanz", "Antonio G. Marques", "Inmaculada Mora-Jim\u00e9nez", "Joaqu\u00edn \u00c1lvarez-Rodr\u00edguez", "Cristina Soguero-Ruiz"], "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care.", "AI": {"tldr": "An interpretable ML framework for MDR prediction using patient similarity metrics and graph-based analysis, achieving 81% AUC and outperforming baselines.", "motivation": "Addressing the global health challenge of MDR by improving prediction accuracy and explainability in critical care.", "method": "Modeling patients as MTS, using similarity measures (descriptive stats, DTW, Time Cluster Kernel) for classification (Logistic Regression, Random Forest, SVM), and applying spectral clustering/t-SNE for explainability.", "result": "Validated on ICU records, achieved 81% AUC, identified key risk factors, and revealed clinically meaningful clusters.", "conclusion": "Graph-based patient similarity enhances MDR prediction and interpretability, aiding early detection and risk stratification in critical care."}}
{"id": "2504.17709", "pdf": "https://arxiv.org/pdf/2504.17709", "abs": "https://arxiv.org/abs/2504.17709", "authors": ["Stefan Jonas", "Angela Meyer"], "title": "Fault Diagnosis in New Wind Turbines using Knowledge from Existing Turbines by Generative Domain Adaptation", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Intelligent condition monitoring of wind turbines is essential for reducing\ndowntimes. Machine learning models trained on wind turbine operation data are\ncommonly used to detect anomalies and, eventually, operation faults. However,\ndata-driven normal behavior models (NBMs) require a substantial amount of\ntraining data, as NBMs trained with scarce data may result in unreliable fault\ndiagnosis. To overcome this limitation, we present a novel generative deep\nlearning approach to make SCADA samples from one wind turbine lacking training\ndata resemble SCADA data from wind turbines with representative training data.\nThrough CycleGAN-based domain mapping, our method enables the application of an\nNBM trained on an existing wind turbine to one with severely limited data. We\ndemonstrate our approach on field data mapping SCADA samples across 7\nsubstantially different WTs. Our findings show significantly improved fault\ndiagnosis in wind turbines with scarce data. Our method achieves the most\nsimilar anomaly scores to an NBM trained with abundant data, outperforming NBMs\ntrained on scarce training data with improvements of +10.3% in F1-score when 1\nmonth of training data is available and +16.8% when 2 weeks are available. The\ndomain mapping approach outperforms conventional fine-tuning at all considered\ndegrees of data scarcity, ranging from 1 to 8 weeks of training data. The\nproposed technique enables earlier and more reliable fault diagnosis in newly\ninstalled wind farms, demonstrating a novel and promising research direction to\nimprove anomaly detection when faced with training data scarcity.", "AI": {"tldr": "A generative deep learning method using CycleGAN improves fault diagnosis in wind turbines with scarce training data by mapping SCADA samples from data-rich turbines.", "motivation": "Overcome the limitation of unreliable fault diagnosis in wind turbines due to scarce training data for normal behavior models (NBMs).", "method": "CycleGAN-based domain mapping to make SCADA samples from turbines with limited data resemble those from turbines with abundant data.", "result": "Significantly improved fault diagnosis, with F1-score improvements of +10.3% (1 month data) and +16.8% (2 weeks data). Outperforms conventional fine-tuning.", "conclusion": "The method enables earlier, more reliable fault diagnosis in new wind farms, offering a promising direction for anomaly detection under data scarcity."}}
{"id": "2502.13881", "pdf": "https://arxiv.org/pdf/2502.13881", "abs": "https://arxiv.org/abs/2502.13881", "authors": ["Jie Zou", "Mohammad Aliannejadi", "Evangelos Kanoulas", "Shuxi Han", "Heli Ma", "Zheng Wang", "Yang Yang", "Heng Tao Shen"], "title": "PSCon: Product Search Through Conversations", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "11 pages. Accepted by SIGIR 2025", "summary": "Conversational Product Search ( CPS ) systems interact with users via natural\nlanguage to offer personalized and context-aware product lists. However, most\nexisting research on CPS is limited to simulated conversations, due to the lack\nof a real CPS dataset driven by human-like language. Moreover, existing\nconversational datasets for e-commerce are constructed for a particular market\nor a particular language and thus can not support cross-market and\nmulti-lingual usage. In this paper, we propose a CPS data collection protocol\nand create a new CPS dataset, called PSCon, which assists product search\nthrough conversations with human-like language. The dataset is collected by a\ncoached human-human data collection protocol and is available for dual markets\nand two languages. By formulating the task of CPS, the dataset allows for\ncomprehensive and in-depth research on six subtasks: user intent detection,\nkeyword extraction, system action prediction, question selection, item ranking,\nand response generation. Moreover, we present a concise analysis of the dataset\nand propose a benchmark model on the proposed CPS dataset. Our proposed dataset\nand model will be helpful for facilitating future research on CPS.", "AI": {"tldr": "The paper introduces PSCon, a new dataset for Conversational Product Search (CPS) using human-like language, addressing gaps in existing datasets by supporting cross-market and multi-lingual usage. It also proposes a benchmark model.", "motivation": "Existing CPS research relies on simulated conversations and lacks real datasets with human-like language. Current datasets are limited to specific markets or languages.", "method": "A coached human-human data collection protocol was used to create PSCon, a dataset for CPS, supporting dual markets and two languages.", "result": "PSCon enables research on six CPS subtasks and includes a benchmark model.", "conclusion": "The dataset and model advance CPS research by providing a realistic, versatile resource."}}
{"id": "2504.17712", "pdf": "https://arxiv.org/pdf/2504.17712", "abs": "https://arxiv.org/abs/2504.17712", "authors": ["Zhuo He", "Paul Henderson", "Nicolas Pugeault"], "title": "Generative Fields: Uncovering Hierarchical Feature Control for StyleGAN via Inverted Receptive Fields", "categories": ["cs.CV"], "comment": null, "summary": "StyleGAN has demonstrated the ability of GANs to synthesize highly-realistic\nfaces of imaginary people from random noise. One limitation of GAN-based image\ngeneration is the difficulty of controlling the features of the generated\nimage, due to the strong entanglement of the low-dimensional latent space.\nPrevious work that aimed to control StyleGAN with image or text prompts\nmodulated sampling in W latent space, which is more expressive than Z latent\nspace. However, W space still has restricted expressivity since it does not\ncontrol the feature synthesis directly; also the feature embedding in W space\nrequires a pre-training process to reconstruct the style signal, limiting its\napplication. This paper introduces the concept of \"generative fields\" to\nexplain the hierarchical feature synthesis in StyleGAN, inspired by the\nreceptive fields of convolution neural networks (CNNs). Additionally, we\npropose a new image editing pipeline for StyleGAN using generative field theory\nand the channel-wise style latent space S, utilizing the intrinsic structural\nfeature of CNNs to achieve disentangled control of feature synthesis at\nsynthesis time.", "AI": {"tldr": "The paper introduces 'generative fields' to improve feature control in StyleGAN, proposing a new editing pipeline using the S latent space for disentangled control.", "motivation": "Controlling features in GAN-generated images is challenging due to latent space entanglement. Existing methods like W space are limited in expressivity and require pre-training.", "method": "The paper introduces 'generative fields' inspired by CNN receptive fields and proposes an editing pipeline using the S latent space for direct feature control.", "result": "The method achieves disentangled control of feature synthesis in StyleGAN, leveraging CNN structural features.", "conclusion": "Generative fields and the S latent space provide a more effective way to control StyleGAN's feature synthesis, overcoming limitations of previous approaches."}}
{"id": "2504.17721", "pdf": "https://arxiv.org/pdf/2504.17721", "abs": "https://arxiv.org/abs/2504.17721", "authors": ["Cheng Shen", "Yuewei Liu"], "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness.", "AI": {"tldr": "A method to improve steel defect detection reliability by statistically rigorous thresholds and prediction sets, ensuring controlled error rates.", "motivation": "Manual inspection is inefficient and costly, while automated CNN-based methods face reliability issues due to annotation uncertainties and overfitting.", "method": "Uses calibration data to define loss functions, derive thresholds, and construct prediction sets for defect regions, ensuring bounded error rates.", "result": "Achieves controlled error rates on test sets and shows adaptability across calibration-to-test ratios.", "conclusion": "The method provides statistically rigorous and adaptable defect detection, improving reliability over traditional and CNN-based approaches."}}
{"id": "2504.17723", "pdf": "https://arxiv.org/pdf/2504.17723", "abs": "https://arxiv.org/abs/2504.17723", "authors": ["Natan Levy", "Adiel Ashrov", "Guy Katz"], "title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework", "categories": ["cs.LG"], "comment": "17 pages, 5 figures", "summary": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment.", "AI": {"tldr": "The paper introduces the RoMA framework to measure LLM robustness against adversarial inputs, showing its accuracy and efficiency compared to formal verification. It highlights variability in robustness across models and tasks, advocating for task-specific evaluations.", "motivation": "Large Language Models (LLMs) are vulnerable to adversarial perturbations, but their robustness is under-explored compared to vision-based models. This gap undermines their reliability in critical applications.", "method": "The authors adapt the Robustness Measurement and Assessment (RoMA) framework to quantify LLM resilience without needing model parameters, comparing its results to formal verification methods.", "result": "RoMA proves accurate with minimal error margins and high efficiency. Robustness varies significantly across models, tasks, and perturbation types, indicating non-uniformity.", "conclusion": "Task-specific robustness evaluations are essential for selecting models based on application needs. The RoMA framework advances reliable LLM deployment by providing a systematic assessment methodology."}}
{"id": "2502.17086", "pdf": "https://arxiv.org/pdf/2502.17086", "abs": "https://arxiv.org/abs/2502.17086", "authors": ["Hyungyu Shin", "Jingyu Tang", "Yoonjoo Lee", "Nayoung Kim", "Hyunseung Lim", "Ji Yong Cho", "Hwajung Hong", "Moontae Lee", "Juho Kim"], "title": "Automatically Evaluating the Paper Reviewing Capability of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Peer review is essential for scientific progress, but it faces challenges\nsuch as reviewer shortages and growing workloads. Although Large Language\nModels (LLMs) show potential for providing assistance, research has reported\nsignificant limitations in the reviews they generate. While the insights are\nvaluable, conducting the analysis is challenging due to the considerable time\nand effort required, especially given the rapid pace of LLM developments. To\naddress the challenge, we developed an automatic evaluation pipeline to assess\nthe LLMs' paper review capability by comparing them with expert-generated\nreviews. By constructing a dataset consisting of 676 OpenReview papers, we\nexamined the agreement between LLMs and experts in their strength and weakness\nidentifications. The results showed that LLMs lack balanced perspectives,\nsignificantly overlook novelty assessment when criticizing, and produce poor\nacceptance decisions. Our automated pipeline enables a scalable evaluation of\nLLMs' paper review capability over time.", "AI": {"tldr": "An automated pipeline evaluates LLMs' paper review capabilities, finding they lack balanced perspectives, overlook novelty, and make poor acceptance decisions compared to expert reviews.", "motivation": "Peer review is critical but faces challenges like reviewer shortages. LLMs could help, but their review quality is limited, and evaluating them is time-consuming.", "method": "Developed an automatic evaluation pipeline using 676 OpenReview papers to compare LLMs' reviews with expert reviews, focusing on strengths, weaknesses, and agreement.", "result": "LLMs lack balanced perspectives, overlook novelty in criticisms, and perform poorly in acceptance decisions compared to experts.", "conclusion": "The automated pipeline provides a scalable way to assess LLMs' review capabilities over time, highlighting current limitations."}}
{"id": "2504.17732", "pdf": "https://arxiv.org/pdf/2504.17732", "abs": "https://arxiv.org/abs/2504.17732", "authors": ["Zhanwen Liu", "Sai Zhou", "Yuchao Dai", "Yang Wang", "Yisheng An", "Xiangmo Zhao"], "title": "DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt State Space Model", "categories": ["cs.CV", "I.4.4"], "comment": null, "summary": "All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, significantly reducing training costs and\ndeployment complexity compared to traditional methods that design dedicated\nmodels for each degradation type. Existing approaches typically rely on\nDegradation-specific models or coarse-grained degradation prompts to guide\nimage restoration. However, they lack fine-grained modeling of degradation\ninformation and face limitations in balancing multi-task conflicts. To overcome\nthese limitations, we propose DPMambaIR, a novel All-in-One image restoration\nframework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)\nand a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained\nmodeling of complex degradation information and efficient global integration,\nwhile mitigating the loss of high-frequency details caused by task competition.\nSpecifically, the DP-SSM utilizes a pre-trained degradation extractor to\ncapture fine-grained degradation features and dynamically incorporates them\ninto the state space modeling process, enhancing the model's adaptability to\ndiverse degradation types. Concurrently, the HEB supplements high-frequency\ninformation, effectively addressing the loss of critical details, such as edges\nand textures, in multi-task image restoration scenarios. Extensive experiments\non a mixed dataset containing seven degradation types show that DPMambaIR\nachieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,\nrespectively. These results highlight the potential and superiority of\nDPMambaIR as a unified solution for All-in-One image restoration.", "AI": {"tldr": "DPMambaIR is a novel All-in-One image restoration framework that integrates fine-grained degradation modeling and high-frequency enhancement, outperforming existing methods.", "motivation": "Existing All-in-One image restoration methods lack fine-grained degradation modeling and struggle with multi-task conflicts, limiting their effectiveness.", "method": "DPMambaIR combines a Degradation-Aware Prompt State Space Model (DP-SSM) for fine-grained degradation modeling and a High-Frequency Enhancement Block (HEB) to preserve details.", "result": "Experiments show DPMambaIR achieves 27.69dB PSNR and 0.893 SSIM, outperforming other methods on a mixed dataset with seven degradation types.", "conclusion": "DPMambaIR demonstrates superior performance as a unified solution for All-in-One image restoration, addressing key limitations of existing approaches."}}
{"id": "2504.17751", "pdf": "https://arxiv.org/pdf/2504.17751", "abs": "https://arxiv.org/abs/2504.17751", "authors": ["Enqi Zhang"], "title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps.To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling.", "AI": {"tldr": "The paper explores SNNs as binary-activated RNNs for sequence modeling, addressing challenges like memory mechanisms, biological components, and parallel training. It proposes a fixed-refractory-period SNN architecture.", "motivation": "To address fundamental challenges in SNNs for sequence modeling, such as lack of memory mechanisms, under-explored biological components, and parallel training limitations.", "method": "Systematic analysis of reset operations and refractory periods in binary-activated RNN-based SNNs, with theoretical insights and a proposed fixed-refractory-period architecture.", "result": "New theoretical explanations for biological mechanisms in SNNs and a proposed architecture for improved sequence modeling.", "conclusion": "The fixed-refractory-period SNN architecture offers a viable solution for sequence modeling challenges in SNNs."}}
{"id": "2504.17739", "pdf": "https://arxiv.org/pdf/2504.17739", "abs": "https://arxiv.org/abs/2504.17739", "authors": ["Lorenzo Simone", "Mauro Giuseppe Camporeale", "Vito Marco Rubino", "Vincenzo Gervasi", "Giovanni Dimauro"], "title": "Interpretable Early Detection of Parkinson's Disease through Speech Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Parkinson's disease is a progressive neurodegenerative disorder affecting\nmotor and non-motor functions, with speech impairments among its earliest\nsymptoms. Speech impairments offer a valuable diagnostic opportunity, with\nmachine learning advances providing promising tools for timely detection. In\nthis research, we propose a deep learning approach for early Parkinson's\ndisease detection from speech recordings, which also highlights the vocal\nsegments driving predictions to enhance interpretability. This approach seeks\nto associate predictive speech patterns with articulatory features, providing a\nbasis for interpreting underlying neuromuscular impairments. We evaluated our\napproach using the Italian Parkinson's Voice and Speech Database, containing\n831 audio recordings from 65 participants, including both healthy individuals\nand patients. Our approach showed competitive classification performance\ncompared to state-of-the-art methods, while providing enhanced interpretability\nby identifying key speech features influencing predictions.", "AI": {"tldr": "A deep learning approach for early Parkinson's disease detection from speech recordings, enhancing interpretability by identifying key vocal segments.", "motivation": "Speech impairments in Parkinson's disease offer early diagnostic opportunities, and machine learning can aid timely detection.", "method": "Proposed a deep learning model analyzing speech recordings, focusing on vocal segments to link predictive patterns with articulatory features.", "result": "Competitive classification performance on the Italian Parkinson's Voice and Speech Database, with improved interpretability.", "conclusion": "The approach effectively detects Parkinson's disease early and provides interpretable insights into speech impairments."}}
{"id": "2503.07269", "pdf": "https://arxiv.org/pdf/2503.07269", "abs": "https://arxiv.org/abs/2503.07269", "authors": ["Shamsuddeen Hassan Muhammad", "Nedjma Ousidhoum", "Idris Abdulmumin", "Seid Muhie Yimam", "Jan Philip Wahle", "Terry Ruas", "Meriem Beloucif", "Christine De Kock", "Tadesse Destaw Belay", "Ibrahim Said Ahmad", "Nirmal Surange", "Daniela Teodorescu", "David Ifeoluwa Adelani", "Alham Fikri Aji", "Felermino Ali", "Vladimir Araujo", "Abinew Ali Ayele", "Oana Ignat", "Alexander Panchenko", "Yi Zhou", "Saif M. Mohammad"], "title": "SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection", "categories": ["cs.CL"], "comment": "SemEval2025 Task11 (Task Description Paper). arXiv admin note: text\n  overlap with arXiv:2502.11926", "summary": "We present our shared task on text-based emotion detection, covering more\nthan 30 languages from seven distinct language families. These languages are\npredominantly low-resource and are spoken across various continents. The data\ninstances are multi-labeled with six emotional classes, with additional\ndatasets in 11 languages annotated for emotion intensity. Participants were\nasked to predict labels in three tracks: (a) multilabel emotion detection, (b)\nemotion intensity score detection, and (c) cross-lingual emotion detection.\n  The task attracted over 700 participants. We received final submissions from\nmore than 200 teams and 93 system description papers. We report baseline\nresults, along with findings on the best-performing systems, the most common\napproaches, and the most effective methods across different tracks and\nlanguages. The datasets for this task are publicly available. The dataset is\navailable at SemEval2025 Task 11 https://brighter-dataset.github.io", "AI": {"tldr": "A shared task on text-based emotion detection in over 30 low-resource languages, with multi-label and intensity annotations, attracted 700+ participants and 200+ teams.", "motivation": "To advance emotion detection in diverse, low-resource languages and provide a benchmark for cross-lingual and multi-label emotion analysis.", "method": "Participants predicted labels in three tracks: multi-label emotion detection, emotion intensity scoring, and cross-lingual detection. Baseline results and top-performing systems were analyzed.", "result": "Over 200 teams submitted results, with 93 system papers. Findings highlight effective methods and common approaches across tracks and languages.", "conclusion": "The task successfully created a benchmark dataset and insights for emotion detection in low-resource languages, with public data availability."}}
{"id": "2504.17735", "pdf": "https://arxiv.org/pdf/2504.17735", "abs": "https://arxiv.org/abs/2504.17735", "authors": ["Akhil Padmanabha", "Saravanan Govindarajan", "Hwanmun Kim", "Sergio Ortiz", "Rahul Rajan", "Doruk Senkal", "Sneha Kadetotad"], "title": "EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Human activity recognition (HAR) on smartglasses has various use cases,\nincluding health/fitness tracking and input for context-aware AI assistants.\nHowever, current approaches for egocentric activity recognition suffer from low\nperformance or are resource-intensive. In this work, we introduce a resource\n(memory, compute, power, sample) efficient machine learning algorithm,\nEgoCHARM, for recognizing both high level and low level activities using a\nsingle egocentric (head-mounted) Inertial Measurement Unit (IMU). Our\nhierarchical algorithm employs a semi-supervised learning strategy, requiring\nprimarily high level activity labels for training, to learn generalizable low\nlevel motion embeddings that can be effectively utilized for low level activity\nrecognition. We evaluate our method on 9 high level and 3 low level activities\nachieving 0.826 and 0.855 F1 scores on high level and low level activity\nrecognition respectively, with just 63k high level and 22k low level model\nparameters, allowing the low level encoder to be deployed directly on current\nIMU chips with compute. Lastly, we present results and insights from a\nsensitivity analysis and highlight the opportunities and limitations of\nactivity recognition using egocentric IMUs.", "AI": {"tldr": "EgoCHARM is a resource-efficient ML algorithm for egocentric activity recognition using a single IMU, achieving high F1 scores with minimal parameters.", "motivation": "Current HAR methods on smartglasses are either low-performing or resource-intensive, limiting practical applications.", "method": "EgoCHARM uses a hierarchical, semi-supervised learning strategy to recognize high and low level activities with minimal labeled data.", "result": "Achieved F1 scores of 0.826 (high level) and 0.855 (low level) with just 63k and 22k parameters, respectively.", "conclusion": "EgoCHARM is efficient and deployable on current IMU chips, though sensitivity analysis reveals limitations for egocentric IMU-based recognition."}}
{"id": "2504.17771", "pdf": "https://arxiv.org/pdf/2504.17771", "abs": "https://arxiv.org/abs/2504.17771", "authors": ["Haochen Wang", "Zhiwei Shi", "Chengxi Zhu", "Yafei Qiao", "Cheng Zhang", "Fan Yang", "Pengjie Ren", "Lan Lu", "Dong Xuan"], "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to ICRA 2025. Project page:\n  https://dreamstarring.github.io/HAMLET/", "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce \\ourmethod, a novel hybrid control system for agile\nbadminton robots. Specifically, we propose a model-based strategy for chassis\nlocomotion which provides a base for arm policy. We introduce a\nphysics-informed ``IL+RL'' training framework for learning-based arm policy. In\nthis train framework, a model-based strategy with privileged information is\nused to guide arm policy training during both IL and RL phases. In addition, we\ntrain the critic model during IL phase to alleviate the performance drop issue\nwhen transitioning from IL to RL. We present results on our self-engineered\nbadminton robot, achieving 94.5% success rate against the serving machine and\n90.7% success rate against human players. Our system can be easily generalized\nto other agile mobile manipulation tasks such as agile catching and table\ntennis. Our project website: https://dreamstarring.github.io/HAMLET/.", "AI": {"tldr": "A hybrid control system (HAMLET) combines model-based and learning-based methods for agile badminton robots, achieving high success rates and generalizability.", "motivation": "Existing learning-based methods lack integration with model-based approaches for safety and stability in agile robot tasks like badminton.", "method": "Proposes a model-based chassis strategy and a physics-informed IL+RL framework for arm policy, with a critic model to ease IL-to-RL transition.", "result": "Achieves 94.5% success against serving machines and 90.7% against humans, with potential for other agile tasks.", "conclusion": "HAMLET effectively harmonizes model-based and learning-based methods, ensuring safety and high performance in agile robotics."}}
{"id": "2504.17740", "pdf": "https://arxiv.org/pdf/2504.17740", "abs": "https://arxiv.org/abs/2504.17740", "authors": ["Mingchen Jiang", "Peng Xu", "Xichen Ye", "Xiaohui Chen", "Yun Yang", "Yifan Chen"], "title": "Embedding Empirical Distributions for Computing Optimal Transport Maps", "categories": ["cs.LG"], "comment": null, "summary": "Distributional data have become increasingly prominent in modern signal\nprocessing, highlighting the necessity of computing optimal transport (OT) maps\nacross multiple probability distributions. Nevertheless, recent studies on\nneural OT methods predominantly focused on the efficient computation of a\nsingle map between two distributions. To address this challenge, we introduce a\nnovel approach to learning transport maps for new empirical distributions.\nSpecifically, we employ the transformer architecture to produce embeddings from\ndistributional data of varying length; these embeddings are then fed into a\nhypernetwork to generate neural OT maps. Various numerical experiments were\nconducted to validate the embeddings and the generated OT maps. The model\nimplementation and the code are provided on\nhttps://github.com/jiangmingchen/HOTET.", "AI": {"tldr": "A novel transformer-based method for learning optimal transport (OT) maps across multiple distributions, validated through experiments.", "motivation": "Addressing the gap in neural OT methods, which focus on single maps between two distributions, by enabling OT map computation for new empirical distributions.", "method": "Uses transformer architecture to create embeddings from distributional data of varying lengths, then employs a hypernetwork to generate neural OT maps.", "result": "Validated through numerical experiments; model and code available on GitHub.", "conclusion": "Proposes an effective approach for multi-distribution OT map learning, with practical implementation provided."}}
{"id": "2503.10894", "pdf": "https://arxiv.org/pdf/2503.10894", "abs": "https://arxiv.org/abs/2503.10894", "authors": ["Jiuding Sun", "Jing Huang", "Sidharth Baskaran", "Karel D'Oosterlinck", "Christopher Potts", "Michael Sklar", "Atticus Geiger"], "title": "HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025", "summary": "Mechanistic interpretability has made great strides in identifying neural\nnetwork features (e.g., directions in hidden activation space) that mediate\nconcepts(e.g., the birth year of a person) and enable predictable manipulation.\nDistributed alignment search (DAS) leverages supervision from counterfactual\ndata to learn concept features within hidden states, but DAS assumes we can\nafford to conduct a brute force search over potential feature locations. To\naddress this, we present HyperDAS, a transformer-based hypernetwork\narchitecture that (1) automatically locates the token-positions of the residual\nstream that a concept is realized in and (2) constructs features of those\nresidual stream vectors for the concept. In experiments with Llama3-8B,\nHyperDAS achieves state-of-the-art performance on the RAVEL benchmark for\ndisentangling concepts in hidden states. In addition, we review the design\ndecisions we made to mitigate the concern that HyperDAS (like all powerful\ninterpretabilty methods) might inject new information into the target model\nrather than faithfully interpreting it.", "AI": {"tldr": "HyperDAS improves mechanistic interpretability by using a transformer-based hypernetwork to locate and construct concept features in neural networks, achieving state-of-the-art performance on the RAVEL benchmark.", "motivation": "Current methods like DAS require brute-force searches for concept features, which is inefficient. HyperDAS aims to automate this process while ensuring interpretability.", "method": "HyperDAS employs a transformer-based hypernetwork to automatically identify token-positions and construct features in the residual stream.", "result": "HyperDAS outperforms existing methods on the RAVEL benchmark with Llama3-8B, demonstrating superior concept disentanglement.", "conclusion": "HyperDAS advances interpretability by automating feature location and construction, though care is needed to avoid injecting new information into the model."}}
{"id": "2504.17761", "pdf": "https://arxiv.org/pdf/2504.17761", "abs": "https://arxiv.org/abs/2504.17761", "authors": ["Shiyu Liu", "Yucheng Han", "Peng Xing", "Fukun Yin", "Rui Wang", "Wei Cheng", "Jiaqi Liao", "Yingming Wang", "Honghao Fu", "Chunrui Han", "Guopeng Li", "Yuang Peng", "Quan Sun", "Jingwei Wu", "Yan Cai", "Zheng Ge", "Ranchen Ming", "Lei Xia", "Xianfang Zeng", "Yibo Zhu", "Binxing Jiao", "Xiangyu Zhang", "Gang Yu", "Daxin Jiang"], "title": "Step1X-Edit: A Practical Framework for General Image Editing", "categories": ["cs.CV"], "comment": "code: https://github.com/stepfun-ai/Step1X-Edit", "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.", "AI": {"tldr": "The paper introduces Step1X-Edit, an open-source image editing model that rivals proprietary models like GPT-4o and Gemini2 Flash, using Multimodal LLM and diffusion techniques.", "motivation": "To bridge the performance gap between open-source and closed-source image editing models.", "method": "Uses Multimodal LLM for processing images and instructions, integrates latent embeddings with a diffusion decoder, and trains on a high-quality generated dataset.", "result": "Step1X-Edit surpasses open-source baselines and nears proprietary model performance on the GEdit-Bench benchmark.", "conclusion": "Step1X-Edit advances open-source image editing, offering competitive performance against leading proprietary models."}}
{"id": "2308.04867", "pdf": "https://arxiv.org/pdf/2308.04867", "abs": "https://arxiv.org/abs/2308.04867", "authors": ["Daniel Tanneberg", "Michael Gienger"], "title": "Learning Type-Generalized Actions for Symbolic Planning", "categories": ["cs.AI", "cs.RO"], "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2023", "summary": "Symbolic planning is a powerful technique to solve complex tasks that require\nlong sequences of actions and can equip an intelligent agent with complex\nbehavior. The downside of this approach is the necessity for suitable symbolic\nrepresentations describing the state of the environment as well as the actions\nthat can change it. Traditionally such representations are carefully\nhand-designed by experts for distinct problem domains, which limits their\ntransferability to different problems and environment complexities. In this\npaper, we propose a novel concept to generalize symbolic actions using a given\nentity hierarchy and observed similar behavior. In a simulated grid-based\nkitchen environment, we show that type-generalized actions can be learned from\nfew observations and generalize to novel situations. Incorporating an\nadditional on-the-fly generalization mechanism during planning, unseen task\ncombinations, involving longer sequences, novel entities and unexpected\nenvironment behavior, can be solved.", "AI": {"tldr": "The paper introduces a method to generalize symbolic actions using entity hierarchies and observed behavior, enabling transferability to novel tasks and environments.", "motivation": "Symbolic planning requires hand-designed representations, limiting transferability. The goal is to generalize actions for broader applicability.", "method": "Proposes type-generalized actions learned from few observations, using an entity hierarchy and on-the-fly generalization during planning.", "result": "Demonstrated in a grid-based kitchen environment, the method generalizes to novel tasks, entities, and longer sequences.", "conclusion": "The approach enhances symbolic planning by reducing dependency on hand-designed representations and improving adaptability."}}
{"id": "2504.17749", "pdf": "https://arxiv.org/pdf/2504.17749", "abs": "https://arxiv.org/abs/2504.17749", "authors": ["Steven E. Wilson", "Sina Khanmohammadi"], "title": "MSGCN: Multiplex Spatial Graph Convolution Network for Interlayer Link Weight Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have been widely used for various learning\ntasks, ranging from node classification to link prediction. They have\ndemonstrated excellent performance in multiple domains involving\ngraph-structured data. However, an important category of learning tasks, namely\nlink weight prediction, has received less emphasis due to its increased\ncomplexity compared to binary link classification. Link weight prediction\nbecomes even more challenging when considering multilayer networks, where nodes\ncan be interconnected across multiple layers. To address these challenges, we\npropose a new method named Multiplex Spatial Graph Convolution Network (MSGCN),\nwhich spatially embeds information across multiple layers to predict interlayer\nlink weights. The MSGCN model generalizes spatial graph convolution to\nmultiplex networks and captures the geometric structure of nodes across\nmultiple layers. Extensive experiments using data with known interlayer link\ninformation show that the MSGCN model has robust, accurate, and generalizable\nlink weight prediction performance across a wide variety of multiplex network\nstructures.", "AI": {"tldr": "The paper proposes MSGCN, a method for predicting link weights in multilayer networks, addressing the complexity of such tasks compared to binary link classification.", "motivation": "Link weight prediction in multilayer networks is complex and understudied, especially compared to simpler tasks like binary link classification.", "method": "The authors introduce MSGCN, a model that generalizes spatial graph convolution to multiplex networks, capturing geometric node structures across layers.", "result": "MSGCN demonstrates robust, accurate, and generalizable performance in predicting interlayer link weights across diverse multiplex network structures.", "conclusion": "MSGCN effectively addresses the challenges of link weight prediction in multilayer networks, offering a scalable and accurate solution."}}
{"id": "2503.21073", "pdf": "https://arxiv.org/pdf/2503.21073", "abs": "https://arxiv.org/abs/2503.21073", "authors": ["Andrew Lee", "Melanie Weber", "Fernanda Vi\u00e9gas", "Martin Wattenberg"], "title": "Shared Global and Local Geometry of Language Model Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Researchers have recently suggested that models share common representations.\nIn our work, we find that token embeddings of language models exhibit common\ngeometric structure. First, we find ``global'' similarities: token embeddings\noften share similar relative orientations. Next, we characterize local geometry\nin two ways: (1) by using Locally Linear Embeddings, and (2) by defining a\nsimple measure for the intrinsic dimension of each token embedding. Our\nintrinsic dimension demonstrates that token embeddings lie on a lower\ndimensional manifold. We qualitatively show that tokens with lower intrinsic\ndimensions often have semantically coherent clusters, while those with higher\nintrinsic dimensions do not. Both characterizations allow us to find\nsimilarities in the local geometry of token embeddings. Perhaps most\nsurprisingly, we find that alignment in token embeddings persists through the\nhidden states of language models, allowing us to develop an application for\ninterpretability. Namely, we introduce Emb2Emb, a simple method to transfer\nsteering vectors from one language model to another, despite the two models\nhaving different dimensions.", "AI": {"tldr": "Token embeddings in language models share common geometric structures, including global similarities and local geometry, revealing lower-dimensional manifolds and semantic coherence. This alignment enables applications like Emb2Emb for interpretability.", "motivation": "To explore and characterize the geometric structure of token embeddings in language models, uncovering shared representations and their implications for interpretability.", "method": "Analyzed global similarities in token embeddings and local geometry using Locally Linear Embeddings and intrinsic dimension measures. Developed Emb2Emb for transferring steering vectors between models.", "result": "Token embeddings lie on lower-dimensional manifolds, with semantically coherent clusters for lower intrinsic dimensions. Alignment persists through hidden states, enabling cross-model applications.", "conclusion": "Common geometric structures in token embeddings reveal shared representations and enable practical tools like Emb2Emb for model interpretability."}}
{"id": "2504.17787", "pdf": "https://arxiv.org/pdf/2504.17787", "abs": "https://arxiv.org/abs/2504.17787", "authors": ["Anton Obukhov", "Matteo Poggi", "Fabio Tosi", "Ripudaman Singh Arora", "Jaime Spencer", "Chris Russell", "Simon Hadfield", "Richard Bowden", "Shuaihang Wang", "Zhenxin Ma", "Weijie Chen", "Baobei Xu", "Fengyu Sun", "Di Xie", "Jiang Zhu", "Mykola Lavreniuk", "Haining Guan", "Qun Wu", "Yupei Zeng", "Chao Lu", "Huanran Wang", "Guangyuan Zhou", "Haotian Zhang", "Jianxiong Wang", "Qiang Rao", "Chunjie Wang", "Xiao Liu", "Zhiqiang Lou", "Hualie Jiang", "Yihao Chen", "Rui Xu", "Minglang Tan", "Zihan Qin", "Yifan Mao", "Jiayang Liu", "Jialei Xu", "Yifan Yang", "Wenbo Zhao", "Junjun Jiang", "Xianming Liu", "Mingshuai Zhao", "Anlong Ming", "Wu Chen", "Feng Xue", "Mengying Yu", "Shida Gao", "Xiangfeng Wang", "Gbenga Omotara", "Ramy Farag", "Jacket Demby", "Seyed Mohamad Ali Tousi", "Guilherme N DeSouza", "Tuan-Anh Yang", "Minh-Quang Nguyen", "Thien-Phuc Tran", "Albert Luginov", "Muhammad Shahzad"], "title": "The Fourth Monocular Depth Estimation Challenge", "categories": ["cs.CV"], "comment": "To appear in CVPRW2025", "summary": "This paper presents the results of the fourth edition of the Monocular Depth\nEstimation Challenge (MDEC), which focuses on zero-shot generalization to the\nSYNS-Patches benchmark, a dataset featuring challenging environments in both\nnatural and indoor settings. In this edition, we revised the evaluation\nprotocol to use least-squares alignment with two degrees of freedom to support\ndisparity and affine-invariant predictions. We also revised the baselines and\nincluded popular off-the-shelf methods: Depth Anything v2 and Marigold. The\nchallenge received a total of 24 submissions that outperformed the baselines on\nthe test set; 10 of these included a report describing their approach, with\nmost leading methods relying on affine-invariant predictions. The challenge\nwinners improved the 3D F-Score over the previous edition's best result,\nraising it from 22.58% to 23.05%.", "AI": {"tldr": "The fourth MDEC focused on zero-shot generalization to SYNS-Patches, revised evaluation protocols, and included new baselines. Winning submissions improved 3D F-Score from 22.58% to 23.05%.", "motivation": "To advance monocular depth estimation by testing zero-shot generalization in challenging environments and refining evaluation methods.", "method": "Revised evaluation protocol with least-squares alignment, included off-the-shelf methods (Depth Anything v2, Marigold), and analyzed 24 submissions.", "result": "10 submissions with reports outperformed baselines, with top methods using affine-invariant predictions. 3D F-Score improved to 23.05%.", "conclusion": "The challenge demonstrated progress in monocular depth estimation, with affine-invariant methods leading to better performance."}}
{"id": "2406.10479", "pdf": "https://arxiv.org/pdf/2406.10479", "abs": "https://arxiv.org/abs/2406.10479", "authors": ["Wenjun Li", "Changyu Chen", "Pradeep Varakantham"], "title": "Unlocking Large Language Model's Planning Capabilities with Maximum Diversity Fine-tuning", "categories": ["cs.AI"], "comment": "8 pages of main paper, 2 pages of references", "summary": "Large language models (LLMs) have demonstrated impressive task-solving\ncapabilities through prompting techniques and system designs, including solving\nplanning tasks (e.g., math proofs, basic travel planning) when sufficient data\nis available online and used during pre-training. However, for planning tasks\nwith limited prior data (e.g., blocks world, advanced travel planning), the\nperformance of LLMs, including proprietary models like GPT and Gemini, is poor.\nThis paper investigates the impact of fine-tuning on the planning capabilities\nof LLMs, revealing that LLMs can achieve strong performance in planning through\nsubstantial (tens of thousands of specific examples) fine-tuning. Yet, this\nprocess incurs high economic, time, and computational costs for each planning\nproblem variation. To address this, we propose Clustering-Based Maximum\nDiversity Sampling (CMDS), which selects diverse and representative data to\nenhance sample efficiency and the model's generalization capability. Extensive\nevaluations demonstrate that CMDS-l, a baseline method combining CMDS with\nlanguage embeddings, outperforms random sampling. Furthermore, we introduce a\nnovel algorithm, CMDS-g, which encodes planning task instances with their graph\nrepresentations into the embedding space. Empirical results show that CMDS-g\nconsistently outperforms baseline methods across various scales and multiple\nbenchmark domains.", "AI": {"tldr": "The paper explores fine-tuning LLMs for planning tasks with limited data, proposing CMDS for efficient data sampling and introducing CMDS-g for improved performance.", "motivation": "LLMs struggle with planning tasks lacking sufficient pre-training data, and fine-tuning is costly. The paper aims to improve efficiency and generalization.", "method": "Proposes Clustering-Based Maximum Diversity Sampling (CMDS) for data selection and introduces CMDS-g, which uses graph representations for better embeddings.", "result": "CMDS-g outperforms baselines, showing consistent improvements in performance across various scales and domains.", "conclusion": "Fine-tuning with CMDS and CMDS-g enhances LLM planning capabilities efficiently and effectively."}}
{"id": "2504.17752", "pdf": "https://arxiv.org/pdf/2504.17752", "abs": "https://arxiv.org/abs/2504.17752", "authors": ["Zhihui Gao", "Sri Krishna Vadlamani", "Kfir Sulimany", "Dirk Englund", "Tingjun Chen"], "title": "Disaggregated Deep Learning via In-Physics Computing at Radio Frequency", "categories": ["cs.LG", "cs.ET", "eess.SP", "physics.app-ph"], "comment": "11 pages, 4 figures. Supplementary Information: 54 pages, 20 figures,\n  1 table", "summary": "Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,\nrely on deep learning to enable a wide range of intelligent applications,\nincluding object recognition, environment perception, and autonomous\nnavigation. However, deploying deep learning models directly on the often\nresource-constrained edge devices demands significant memory footprints and\ncomputational power for real-time inference using traditional digital computing\narchitectures. In this paper, we present WISE, a novel computing architecture\nfor wireless edge networks designed to overcome energy constraints in deep\nlearning inference. WISE achieves this goal through two key innovations:\ndisaggregated model access via wireless broadcasting and in-physics computation\nof general complex-valued matrix-vector multiplications directly at radio\nfrequency. Using a software-defined radio platform with wirelessly broadcast\nmodel weights over the air, we demonstrate that WISE achieves 95.7% image\nclassification accuracy with ultra-low operation power of 6.0 fJ/MAC per\nclient, corresponding to a computation efficiency of 165.8 TOPS/W. This\napproach enables energy-efficient deep learning inference on wirelessly\nconnected edge devices, achieving more than two orders of magnitude improvement\nin efficiency compared to traditional digital computing.", "AI": {"tldr": "WISE is a novel computing architecture for wireless edge networks, enabling energy-efficient deep learning inference through wireless broadcasting and in-physics computation.", "motivation": "Resource constraints on edge devices make traditional deep learning deployment inefficient; WISE aims to overcome energy limitations.", "method": "Uses disaggregated model access via wireless broadcasting and in-physics computation of complex-valued matrix-vector multiplications at radio frequency.", "result": "Achieves 95.7% image classification accuracy with ultra-low power (6.0 fJ/MAC) and high efficiency (165.8 TOPS/W).", "conclusion": "WISE significantly improves energy efficiency for deep learning on edge devices, outperforming traditional methods by over 100x."}}
{"id": "2504.02441", "pdf": "https://arxiv.org/pdf/2504.02441", "abs": "https://arxiv.org/abs/2504.02441", "authors": ["Lianlei Shan", "Shixian Luo", "Zezhou Zhu", "Yu Yuan", "Yong Wu"], "title": "Cognitive Memory in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "37 pages, 9 figures", "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.", "AI": {"tldr": "The paper analyzes memory mechanisms in LLMs, categorizing them into sensory, short-term, and long-term memory, and explores methods for acquisition, management, and utilization. It also discusses KV cache-based and parameter-based memory techniques, concluding with future research directions.", "motivation": "To understand and improve LLM memory mechanisms for better context-rich responses, reduced hallucinations, and enhanced efficiency.", "method": "Categorizes memory into sensory, short-term, and long-term, and details text-based, KV cache-based, parameter-based, and hidden-state-based memory methods.", "result": "Provides a comprehensive analysis of memory mechanisms, including acquisition, management, and utilization strategies, and highlights efficiency improvements.", "conclusion": "Emphasizes the importance of memory mechanisms in LLMs and suggests future research directions."}}
{"id": "2504.17788", "pdf": "https://arxiv.org/pdf/2504.17788", "abs": "https://arxiv.org/abs/2504.17788", "authors": ["Chris Rockwell", "Joseph Tung", "Tsung-Yi Lin", "Ming-Yu Liu", "David F. Fouhey", "Chen-Hsuan Lin"], "title": "Dynamic Camera Poses and Where to Find Them", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://research.nvidia.com/labs/dir/dynpose-100k", "summary": "Annotating camera poses on dynamic Internet videos at scale is critical for\nadvancing fields like realistic video generation and simulation. However,\ncollecting such a dataset is difficult, as most Internet videos are unsuitable\nfor pose estimation. Furthermore, annotating dynamic Internet videos present\nsignificant challenges even for state-of-theart methods. In this paper, we\nintroduce DynPose-100K, a large-scale dataset of dynamic Internet videos\nannotated with camera poses. Our collection pipeline addresses filtering using\na carefully combined set of task-specific and generalist models. For pose\nestimation, we combine the latest techniques of point tracking, dynamic\nmasking, and structure-from-motion to achieve improvements over the\nstate-of-the-art approaches. Our analysis and experiments demonstrate that\nDynPose-100K is both large-scale and diverse across several key attributes,\nopening up avenues for advancements in various downstream applications.", "AI": {"tldr": "DynPose-100K is a large-scale dataset of dynamic Internet videos annotated with camera poses, addressing challenges in pose estimation for such videos.", "motivation": "Annotating camera poses on dynamic Internet videos is difficult but crucial for fields like realistic video generation and simulation.", "method": "The pipeline combines task-specific and generalist models for filtering, and integrates point tracking, dynamic masking, and structure-from-motion for pose estimation.", "result": "DynPose-100K is large-scale and diverse, outperforming state-of-the-art methods.", "conclusion": "The dataset enables advancements in downstream applications by providing high-quality annotated dynamic videos."}}
{"id": "2409.17685", "pdf": "https://arxiv.org/pdf/2409.17685", "abs": "https://arxiv.org/abs/2409.17685", "authors": ["Yasaman Haghbin", "Hadi Moradi", "Reshad Hosseini"], "title": "Feature-to-Image Data Augmentation: Improving Model Feature Extraction with Cluster-Guided Synthetic Samples", "categories": ["cs.AI", "cs.LG"], "comment": "10 pages, 6 figures, 6 table", "summary": "One of the growing trends in machine learning is the use of data generation\ntechniques, since the performance of machine learning models is dependent on\nthe quantity of the training dataset. However, in many real-world applications,\nparticularly in medical and low-resource domains, collecting large datasets is\nchallenging due to resource constraints, which leads to overfitting and poor\ngeneralization. This study introduces FICAug, a novel feature-to-image data\naugmentation framework designed to improve model generalization under limited\ndata conditions by generating structured synthetic samples.\n  FICAug first operates in the feature space, where original data are clustered\nusing the k-means algorithm. Within pure-label clusters, synthetic data are\ngenerated through Gaussian sampling to increase diversity while maintaining\nlabel consistency. These synthetic features are then projected back into the\nimage domain using a generative neural network, and a convolutional neural\nnetwork is trained on the reconstructed images to learn enhanced\nrepresentations.\n  Experimental results demonstrate that FICAug significantly improves\nclassification accuracy. In feature space, it achieved a cross-validation\naccuracy of 84.09%, while training a ResNet-18 model on the reconstructed\nimages further boosted performance to 88.63%, illustrating the effectiveness of\nthe proposed framework in extracting new and task-relevant features.", "AI": {"tldr": "FICAug is a feature-to-image data augmentation framework that improves model generalization under limited data by generating structured synthetic samples.", "motivation": "Addressing challenges in collecting large datasets, especially in medical and low-resource domains, to prevent overfitting and poor generalization.", "method": "Clusters data in feature space using k-means, generates synthetic samples via Gaussian sampling, projects them back to image domain with a generative neural network, and trains a CNN on reconstructed images.", "result": "Achieved 84.09% cross-validation accuracy in feature space and 88.63% with ResNet-18 on reconstructed images.", "conclusion": "FICAug effectively enhances classification accuracy and feature extraction in limited-data scenarios."}}
{"id": "2504.17780", "pdf": "https://arxiv.org/pdf/2504.17780", "abs": "https://arxiv.org/abs/2504.17780", "authors": ["Sneh Pillai"], "title": "Replay to Remember: Retaining Domain Knowledge in Streaming Language Models", "categories": ["cs.LG"], "comment": "8 pages 3 figures, 3 tables", "summary": "Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios.", "AI": {"tldr": "A lightweight method combining LoRA and minimal replay mitigates catastrophic forgetting in LLMs under real-time domain adaptation constraints.", "motivation": "Address catastrophic forgetting in LLMs during continual learning, especially in resource-limited, real-time settings.", "method": "Combines LoRA (parameter-efficient tuning) with minimal replay in a streaming setup across medical, genetics, and law domains.", "result": "Minimal replay stabilizes and partially restores domain knowledge, reducing catastrophic forgetting.", "conclusion": "Offers practical insights for deploying adaptable LLMs in constrained real-world scenarios."}}
{"id": "2504.05058", "pdf": "https://arxiv.org/pdf/2504.05058", "abs": "https://arxiv.org/abs/2504.05058", "authors": ["Aravind Krishnan", "Siva Reddy", "Marius Mosbach"], "title": "Not All Data Are Unlearned Equally", "categories": ["cs.CL"], "comment": null, "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.", "AI": {"tldr": "The paper explores machine unlearning in LLMs, showing that the success of unlearning depends on the frequency of the knowledge in pre-training data, with more frequent knowledge being harder to remove. It also highlights misalignment in evaluation methods and calls for better practices.", "motivation": "To address the challenge of unlearning specific knowledge in LLMs, particularly for privacy, and to investigate the assumption that all data points are equally unlearnable.", "method": "The study examines how unlearning success varies with the frequency of knowledge in pre-training data and evaluates misalignment between probability and generation-based methods.", "result": "Frequent knowledge is harder to unlearn, and evaluation misalignment worsens with larger models.", "conclusion": "Better evaluation practices and methods considering training data are needed for effective LLM unlearning."}}
{"id": "2504.17789", "pdf": "https://arxiv.org/pdf/2504.17789", "abs": "https://arxiv.org/abs/2504.17789", "authors": ["Xu Ma", "Peize Sun", "Haoyu Ma", "Hao Tang", "Chih-Yao Ma", "Jialiang Wang", "Kunpeng Li", "Xiaoliang Dai", "Yujun Shi", "Xuan Ju", "Yushi Hu", "Artsiom Sanakoyeu", "Felix Juefei-Xu", "Ji Hou", "Junjiao Tian", "Tao Xu", "Tingbo Hou", "Yen-Cheng Liu", "Zecheng He", "Zijian He", "Matt Feiszli", "Peizhao Zhang", "Peter Vajda", "Sam Tsai", "Yun Fu"], "title": "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.", "AI": {"tldr": "Token-Shuffle reduces image tokens in AR models for efficient high-resolution image synthesis, outperforming AR and diffusion models.", "motivation": "AR models for image synthesis are limited by high token counts, reducing efficiency and resolution. Token-Shuffle addresses this by leveraging visual vocabulary redundancy in MLLMs.", "method": "Token-Shuffle merges local tokens along channels (token-shuffle) and restores spatial arrangement (token-unshuffle) in Transformer blocks, enabling efficient high-resolution synthesis.", "result": "Achieves 2048x2048 resolution, outperforming AR (LlamaGen) and diffusion (LDM) models in benchmarks and human evaluations.", "conclusion": "Token-Shuffle is a foundational design for efficient high-resolution image generation in MLLMs."}}
{"id": "2412.02868", "pdf": "https://arxiv.org/pdf/2412.02868", "abs": "https://arxiv.org/abs/2412.02868", "authors": ["Yixiang Qu", "Yifan Dai", "Shilin Yu", "Pradham Tanikella", "Travis Schrank", "Trevor Hackman", "Didong Li", "Di Wu"], "title": "Enhancing LLMs with Smart Preprocessing for EHR Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nnatural language processing; however, their application in sensitive domains\nsuch as healthcare, especially in processing Electronic Health Records (EHRs),\nis constrained by limited computational resources and privacy concerns. This\npaper introduces a compact LLM framework optimized for local deployment in\nenvironments with stringent privacy requirements and restricted access to\nhigh-performance GPUs. Our approach leverages simple yet powerful preprocessing\ntechniques, including regular expressions (regex) and Retrieval-Augmented\nGeneration (RAG), to extract and highlight critical information from clinical\nnotes. By pre-filtering long, unstructured text, we enhance the performance of\nsmaller LLMs on EHR-related tasks. Our framework is evaluated using zero-shot\nand few-shot learning paradigms on both private and publicly available datasets\n(MIMIC-IV), with additional comparisons against fine-tuned LLMs on MIMIC-IV.\nExperimental results demonstrate that our preprocessing strategy significantly\nsupercharges the performance of smaller LLMs, making them well-suited for\nprivacy-sensitive and resource-constrained applications. This study offers\nvaluable insights into optimizing LLM performance for local, secure, and\nefficient healthcare applications. It provides practical guidance for\nreal-world deployment for LLMs while tackling challenges related to privacy,\ncomputational feasibility, and clinical applicability.", "AI": {"tldr": "A compact LLM framework for EHR processing is introduced, optimized for privacy and resource constraints, using preprocessing techniques like regex and RAG to enhance smaller LLMs' performance.", "motivation": "To address privacy and computational limitations in deploying LLMs for sensitive healthcare applications like EHR processing.", "method": "Leverages preprocessing (regex and RAG) to filter and highlight critical EHR data, evaluated via zero-shot and few-shot learning on datasets like MIMIC-IV.", "result": "Preprocessing significantly boosts smaller LLMs' performance, making them viable for privacy-sensitive, resource-limited settings.", "conclusion": "The framework offers practical guidance for deploying LLMs in healthcare, balancing privacy, computational feasibility, and clinical utility."}}
{"id": "2504.16941", "pdf": "https://arxiv.org/pdf/2504.16941", "abs": "https://arxiv.org/abs/2504.16941", "authors": ["Zakaria Lamine", "Abdelatif Hafid", "Mohamed Rahouti"], "title": "Mathematical Modeling of Protein Structures: A Cohomology-Based Approach to the Flagellar Motor", "categories": ["q-bio.BM", "cs.LG", "math.AT"], "comment": null, "summary": "This study presents a novel mathematical model derived from cohomology,\nleveraging the KEEL-proven theorem that establishes cohomology as tautological,\ngenerated by boundary classes of curves with fixed dual graphs. Simplicial\ncomplexes are constructed using skew-commutative graded algebra, and the\nstructure theorem is applied to connect distinct homologies, enabling precise\ninterpretations of the resulting geometric forms. The proposed model is\nutilized for protein structure analysis and prediction, with a specific\napplication to the Flagellar Motor structure. This approach offers new insights\ninto the geometric and algebraic foundations of biological macromolecular\nmodeling, highlighting its potential for advancement in structural biology.", "AI": {"tldr": "A novel mathematical model based on cohomology and simplicial complexes is developed for protein structure analysis, specifically applied to the Flagellar Motor, advancing structural biology.", "motivation": "To leverage cohomology and geometric algebra for precise modeling of biological macromolecules, addressing gaps in structural biology.", "method": "Derives a model from cohomology using skew-commutative graded algebra and applies it to protein structures, focusing on the Flagellar Motor.", "result": "The model provides new geometric and algebraic insights into macromolecular structures, demonstrating potential for structural biology.", "conclusion": "The approach enhances understanding of biological macromolecules and shows promise for future advancements in structural biology."}}
{"id": "2504.07315", "pdf": "https://arxiv.org/pdf/2504.07315", "abs": "https://arxiv.org/abs/2504.07315", "authors": ["Alessio Tosolini", "Claire Bowern"], "title": "Multilingual MFA: Forced Alignment on Low-Resource Related Languages", "categories": ["cs.CL"], "comment": null, "summary": "We compare the outcomes of multilingual and crosslingual training for related\nand unrelated Australian languages with similar phonological inventories. We\nuse the Montreal Forced Aligner to train acoustic models from scratch and adapt\na large English model, evaluating results against seen data, unseen data (seen\nlanguage), and unseen data and language. Results indicate benefits of adapting\nthe English baseline model for previously unseen languages.", "AI": {"tldr": "Comparing multilingual vs. crosslingual training for Australian languages, adapting an English model shows benefits for unseen languages.", "motivation": "To evaluate the effectiveness of multilingual and crosslingual training for related and unrelated Australian languages with similar phonology.", "method": "Using the Montreal Forced Aligner, acoustic models were trained from scratch and adapted from a large English model, tested on seen data, unseen data (same language), and unseen data/language.", "result": "Adapting the English baseline model improved performance for previously unseen languages.", "conclusion": "Crosslingual adaptation of an English model is advantageous for handling unseen languages."}}
{"id": "2504.17791", "pdf": "https://arxiv.org/pdf/2504.17791", "abs": "https://arxiv.org/abs/2504.17791", "authors": ["Tetiana Martyniuk", "Gilles Puy", "Alexandre Boulch", "Renaud Marlet", "Raoul de Charette"], "title": "LiDPM: Rethinking Point Diffusion for Lidar Scene Completion", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to IEEE IV 2025", "summary": "Training diffusion models that work directly on lidar points at the scale of\noutdoor scenes is challenging due to the difficulty of generating fine-grained\ndetails from white noise over a broad field of view. The latest works\naddressing scene completion with diffusion models tackle this problem by\nreformulating the original DDPM as a local diffusion process. It contrasts with\nthe common practice of operating at the level of objects, where vanilla DDPMs\nare currently used. In this work, we close the gap between these two lines of\nwork. We identify approximations in the local diffusion formulation, show that\nthey are not required to operate at the scene level, and that a vanilla DDPM\nwith a well-chosen starting point is enough for completion. Finally, we\ndemonstrate that our method, LiDPM, leads to better results in scene completion\non SemanticKITTI. The project page is https://astra-vision.github.io/LiDPM .", "AI": {"tldr": "LiDPM bridges the gap between local and object-level diffusion models for lidar scene completion, showing vanilla DDPMs with a good starting point suffice for better results.", "motivation": "Addressing the challenge of generating fine-grained details in lidar point clouds for outdoor scenes using diffusion models.", "method": "Uses a vanilla DDPM with a well-chosen starting point, avoiding approximations in local diffusion formulations.", "result": "Achieves better scene completion results on SemanticKITTI compared to prior methods.", "conclusion": "A vanilla DDPM approach is effective for lidar scene completion without needing local diffusion approximations."}}
{"id": "2501.03888", "pdf": "https://arxiv.org/pdf/2501.03888", "abs": "https://arxiv.org/abs/2501.03888", "authors": ["Kexin Gu Baugh", "Luke Dickens", "Alessandra Russo"], "title": "Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and Editable Policies", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": "AAMAS 2025 (with Appendix)", "summary": "Although deep reinforcement learning has been shown to be effective, the\nmodel's black-box nature presents barriers to direct policy interpretation. To\naddress this problem, we propose a neuro-symbolic approach called neural DNF-MT\nfor end-to-end policy learning. The differentiable nature of the neural DNF-MT\nmodel enables the use of deep actor-critic algorithms for training. At the same\ntime, its architecture is designed so that trained models can be directly\ntranslated into interpretable policies expressed as standard (bivalent or\nprobabilistic) logic programs. Moreover, additional layers can be included to\nextract abstract features from complex observations, acting as a form of\npredicate invention. The logic representations are highly interpretable, and we\nshow how the bivalent representations of deterministic policies can be edited\nand incorporated back into a neural model, facilitating manual intervention and\nadaptation of learned policies. We evaluate our approach on a range of tasks\nrequiring learning deterministic or stochastic behaviours from various forms of\nobservations. Our empirical results show that our neural DNF-MT model performs\nat the level of competing black-box methods whilst providing interpretable\npolicies.", "AI": {"tldr": "The paper introduces neural DNF-MT, a neuro-symbolic approach for interpretable deep reinforcement learning policies, combining deep learning with logic programs.", "motivation": "Address the black-box nature of deep reinforcement learning models by enabling interpretable policy learning.", "method": "Proposes neural DNF-MT, a differentiable model trained with actor-critic algorithms, translating into interpretable logic programs. Supports predicate invention for complex observations.", "result": "Performs comparably to black-box methods while providing interpretable, editable logic-based policies.", "conclusion": "Neural DNF-MT bridges deep learning and symbolic logic, offering interpretability without sacrificing performance."}}
{"id": "2504.16943", "pdf": "https://arxiv.org/pdf/2504.16943", "abs": "https://arxiv.org/abs/2504.16943", "authors": ["Chiara Fusar Bassini", "Alice Lixuan Xu", "Jorge S\u00e1nchez Canales", "Lion Hirth", "Lynn H. Kaack"], "title": "Flexibility of German gas-fired generation: evidence from clustering empirical operation", "categories": ["cs.CY", "cs.LG"], "comment": "29 pages, 6 figures, 6 tables", "summary": "A key input to energy models are assumptions about the flexibility of power\ngeneration units, i.e., how quickly and often they can start up. These\nassumptions are usually calibrated on the technical characteristics of the\nunits, such as installed capacity or technology type. However, even if power\ngeneration units technically can dispatch flexibly, service obligations and\nmarket incentives may constrain their operation. Here, we cluster over 60% of\nGerman national gas generation (generation units of 100 MWp or above) based on\ntheir empirical flexibility. We process the hourly dispatch of sample units\nbetween 2019 and 2023 using a novel deep learning approach, that transforms\ntime series into easy-to-cluster representations. We identify two clusters of\npeaker units and two clusters of non-peaker units, whose different empirical\nflexibility is quantified by cluster-level ramp rates. Non-peaker units, around\nhalf of the sample, are empirically less flexible than peakers, and make up for\nmore than 83% of sample must-run generation. Regulatory changes addressing the\nlow market responsiveness of non-peakers are needed to unlock their\nflexibility.", "AI": {"tldr": "The paper analyzes the flexibility of German gas power generation units, identifying clusters of peaker and non-peaker units based on empirical data, and highlights regulatory needs to improve their market responsiveness.", "motivation": "To understand how service obligations and market incentives, beyond technical capabilities, affect the flexibility of power generation units.", "method": "Clustering 60% of German gas generation units using a deep learning approach to analyze hourly dispatch data (2019-2023) and quantify flexibility via ramp rates.", "result": "Identified two peaker and two non-peaker clusters; non-peakers (50% of sample) are less flexible and account for 83% of must-run generation.", "conclusion": "Regulatory changes are necessary to enhance the flexibility of non-peaker units."}}
{"id": "2504.09818", "pdf": "https://arxiv.org/pdf/2504.09818", "abs": "https://arxiv.org/abs/2504.09818", "authors": ["Rong Yao", "Hailin Hu", "Yifei Fu", "Hanting Chen", "Wenyi Fang", "Fanyi Du", "Kai Han", "Yunhe Wang"], "title": "Transferable text data distillation by trajectory matching", "categories": ["cs.CL"], "comment": null, "summary": "In the realm of large language model (LLM), as the size of large models\nincreases, it also brings higher training costs. There is a urgent need to\nminimize the data size in LLM training. Compared with data selection method,\nthe data distillation method aims to synthesize a small number of data samples\nto achieve the training effect of the full data set and has better flexibility.\nDespite its successes in computer vision, the discreteness of text data has\nhitherto stymied its exploration in natural language processing (NLP). In this\nwork, we proposed a method that involves learning pseudo prompt data based on\ntrajectory matching and finding its nearest neighbor ID to achieve\ncross-architecture transfer. During the distillation process, we introduce a\nregularization loss to improve the robustness of our distilled data. To our\nbest knowledge, this is the first data distillation work suitable for text\ngeneration tasks such as instruction tuning. Evaluations on two benchmarks,\nincluding ARC-Easy and MMLU instruction tuning datasets, established the\nsuperiority of our distillation approach over the SOTA data selection method\nLESS. Furthermore, our method demonstrates a good transferability over LLM\nstructures (i.e., OPT to Llama).", "AI": {"tldr": "A novel data distillation method for LLMs is introduced, synthesizing small datasets to match full-data training effects, outperforming SOTA selection methods and showing cross-architecture transferability.", "motivation": "High training costs of large language models (LLMs) necessitate minimizing data size without compromising performance, especially in NLP where text discreteness complicates distillation.", "method": "The method involves learning pseudo prompt data via trajectory matching, nearest neighbor ID matching, and regularization loss for robustness.", "result": "Outperforms SOTA data selection (LESS) on ARC-Easy and MMLU benchmarks and shows transferability across LLM architectures (OPT to Llama).", "conclusion": "This is the first effective data distillation approach for text generation tasks like instruction tuning, offering flexibility and efficiency."}}
{"id": "2504.17062", "pdf": "https://arxiv.org/pdf/2504.17062", "abs": "https://arxiv.org/abs/2504.17062", "authors": ["Yu Guo", "Zhiqiang Lao", "Xiyun Song", "Yubin Zhou", "Zongfang Lin", "Heather Yu"], "title": "ePBR: Extended PBR Materials in Image Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "8 pages without references, 7 figures, accepted in CVPRW 2025", "summary": "Realistic indoor or outdoor image synthesis is a core challenge in computer\nvision and graphics. The learning-based approach is easy to use but lacks\nphysical consistency, while traditional Physically Based Rendering (PBR) offers\nhigh realism but is computationally expensive. Intrinsic image representation\noffers a well-balanced trade-off, decomposing images into fundamental\ncomponents (intrinsic channels) such as geometry, materials, and illumination\nfor controllable synthesis. However, existing PBR materials struggle with\ncomplex surface models, particularly high-specular and transparent surfaces. In\nthis work, we extend intrinsic image representations to incorporate both\nreflection and transmission properties, enabling the synthesis of transparent\nmaterials such as glass and windows. We propose an explicit intrinsic\ncompositing framework that provides deterministic, interpretable image\nsynthesis. With the Extended PBR (ePBR) Materials, we can effectively edit the\nmaterials with precise controls.", "AI": {"tldr": "The paper introduces an extended intrinsic image representation (ePBR) to improve synthesis of transparent materials like glass, balancing realism and control.", "motivation": "Existing methods lack physical consistency (learning-based) or are too costly (PBR), and struggle with complex surfaces like transparent materials.", "method": "Proposes an explicit intrinsic compositing framework incorporating reflection and transmission properties for controllable synthesis.", "result": "Enables precise editing and synthesis of transparent materials (e.g., glass) with deterministic, interpretable results.", "conclusion": "The ePBR framework offers a practical solution for realistic, controllable image synthesis of complex materials."}}
{"id": "2501.15857", "pdf": "https://arxiv.org/pdf/2501.15857", "abs": "https://arxiv.org/abs/2501.15857", "authors": ["Yutong Yin", "Zhaoran Wang"], "title": "Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Humans exhibit remarkable compositional reasoning by integrating knowledge\nfrom various sources. For example, if someone learns ( B = f(A) ) from one\nsource and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even\nwithout encountering ( ABC ) together, showcasing the generalization ability of\nhuman intelligence. In this paper, we introduce a synthetic learning task,\n\"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential\nof Transformers in replicating this skill and interpret its inner mechanism. In\nthe training phase, data consist of separated knowledge fragments from an\noverall causal graph. During testing, Transformers must infer complete causal\ngraph traces by integrating these fragments. Our findings demonstrate that\nfew-shot Chain-of-Thought prompting enables Transformers to perform\ncompositional reasoning on FTCT by revealing correct combinations of fragments,\neven if such combinations were absent in the training data. Furthermore, the\nemergence of compositional reasoning ability is strongly correlated with the\nmodel complexity and training-testing data similarity. We propose, both\ntheoretically and empirically, that Transformers learn an underlying\ngeneralizable program from training, enabling effective compositional reasoning\nduring testing.", "AI": {"tldr": "Transformers can perform compositional reasoning on the FTCT task, integrating fragmented knowledge during testing, enabled by few-shot Chain-of-Thought prompting.", "motivation": "To validate if Transformers can replicate human-like compositional reasoning by integrating fragmented knowledge, and to understand the underlying mechanisms.", "method": "Introduces the FTCT task, where models train on fragmented causal knowledge and test by inferring complete causal traces. Uses few-shot Chain-of-Thought prompting.", "result": "Transformers succeed in compositional reasoning, correlating with model complexity and data similarity. They learn a generalizable program.", "conclusion": "Transformers can achieve human-like compositional reasoning, with performance linked to model design and training-testing alignment."}}
{"id": "2504.16969", "pdf": "https://arxiv.org/pdf/2504.16969", "abs": "https://arxiv.org/abs/2504.16969", "authors": ["Mathias Hanson", "Gregory Lewkowicz", "Sam Verboven"], "title": "Engineering the Law-Machine Learning Translation Problem: Developing Legally Aligned Models", "categories": ["cs.CY", "cs.LG", "I.2.m; K.5.0"], "comment": "16 pages, 1 figure", "summary": "Organizations developing machine learning-based (ML) technologies face the\ncomplex challenge of achieving high predictive performance while respecting the\nlaw. This intersection between ML and the law creates new complexities. As ML\nmodel behavior is inferred from training data, legal obligations cannot be\noperationalized in source code directly. Rather, legal obligations require\n\"indirect\" operationalization. However, choosing context-appropriate\noperationalizations presents two compounding challenges: (1) laws often permit\nmultiple valid operationalizations for a given legal obligation-each with\nvarying degrees of legal adequacy; and, (2) each operationalization creates\nunpredictable trade-offs among the different legal obligations and with\npredictive performance. Evaluating these trade-offs requires metrics (or\nheuristics), which are in turn difficult to validate against legal obligations.\nCurrent methodologies fail to fully address these interwoven challenges as they\neither focus on legal compliance for traditional software or on ML model\ndevelopment without adequately considering legal complexities. In response, we\nintroduce a five-stage interdisciplinary framework that integrates legal and\nML-technical analysis during ML model development. This framework facilitates\ndesigning ML models in a legally aligned way and identifying high-performing\nmodels that are legally justifiable. Legal reasoning guides choices for\noperationalizations and evaluation metrics, while ML experts ensure technical\nfeasibility, performance optimization and an accurate interpretation of metric\nvalues. This framework bridges the gap between more conceptual analysis of law\nand ML models' need for deterministic specifications. We illustrate its\napplication using a case study in the context of anti-money laundering.", "AI": {"tldr": "A framework integrates legal and ML analysis to address challenges in operationalizing legal obligations in ML models, balancing compliance and performance.", "motivation": "The intersection of ML and law creates complexities in operationalizing legal obligations, requiring interdisciplinary solutions.", "method": "A five-stage interdisciplinary framework combines legal and ML-technical analysis to guide model development.", "result": "The framework enables legally aligned ML models with high performance, demonstrated via an anti-money laundering case study.", "conclusion": "The framework bridges the gap between legal conceptualization and ML's deterministic needs, offering a practical solution for compliance and performance."}}
{"id": "2504.12597", "pdf": "https://arxiv.org/pdf/2504.12597", "abs": "https://arxiv.org/abs/2504.12597", "authors": ["Liangyu Xu", "Yingxiu Zhao", "Jingyun Wang", "Yingyao Wang", "Bu Pi", "Chen Wang", "Mingliang Zhang", "Jihao Gu", "Xiang Li", "Xiaoyong Zhu", "Jun Song", "Bo Zheng"], "title": "GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning", "categories": ["cs.CL"], "comment": "10 pages, 8 figures", "summary": "Geometry problem-solving (GPS), a challenging task requiring both visual\ncomprehension and symbolic reasoning, effectively measures the reasoning\ncapabilities of multimodal large language models (MLLMs). Humans exhibit strong\nreasoning ability in this task through accurate identification and adaptive\napplication of geometric principles within visual contexts. However, existing\nbenchmarks fail to jointly assess both dimensions of the human-like geometric\nreasoning mechanism in MLLMs, remaining a critical gap in assessing their\nability to tackle GPS. To this end, we introduce GeoSense, the first\ncomprehensive bilingual benchmark designed to systematically evaluate the\ngeometric reasoning abilities of MLLMs through the lens of geometric\nprinciples. GeoSense features a five-level hierarchical framework of geometric\nprinciples spanning plane and solid geometry, an intricately annotated dataset\nof 1,789 problems, and an innovative evaluation strategy. Through extensive\nexperiments on GeoSense with various open-source and closed-source MLLMs, we\nobserve that Gemini-2.0-pro-flash performs best, achieving an overall score of\n$65.3$. Our in-depth analysis reveals that the identification and application\nof geometric principles remain a bottleneck for leading MLLMs, jointly\nhindering their reasoning abilities. These findings underscore GeoSense's\npotential to guide future advancements in MLLMs' geometric reasoning\ncapabilities, paving the way for more robust and human-like reasoning in\nartificial intelligence.", "AI": {"tldr": "GeoSense is a bilingual benchmark for evaluating MLLMs' geometric reasoning, revealing gaps in principle identification and application.", "motivation": "Existing benchmarks lack joint assessment of visual and symbolic reasoning in MLLMs for geometry problem-solving.", "method": "GeoSense introduces a hierarchical framework, annotated dataset, and evaluation strategy to test MLLMs.", "result": "Gemini-2.0-pro-flash scored highest (65.3), but principle identification and application remain challenging.", "conclusion": "GeoSense highlights MLLMs' limitations and guides future improvements in geometric reasoning."}}
{"id": "2504.02269", "pdf": "https://arxiv.org/pdf/2504.02269", "abs": "https://arxiv.org/abs/2504.02269", "authors": ["Jay Lee", "Hanqi Su", "Dai-Yan Ji", "Takanobu Minami"], "title": "Engineering Artificial Intelligence: Framework, Challenges, and Future Direction", "categories": ["cs.AI", "cs.LG"], "comment": "The paper submitted to the Journal Machine Learning: Engineering has\n  been accepted", "summary": "Over the past ten years, the application of artificial intelligence (AI) and\nmachine learning (ML) in engineering domains has gained significant popularity,\nshowcasing their potential in data-driven contexts. However, the complexity and\ndiversity of engineering problems often require the development of\ndomain-specific AI approaches, which are frequently hindered by a lack of\nsystematic methodologies, scalability, and robustness during the development\nprocess. To address this gap, this paper introduces the \"ABCDE\" as the key\nelements of Engineering AI and proposes a unified, systematic engineering AI\necosystem framework, including eight essential layers, along with attributes,\ngoals, and applications, to guide the development and deployment of AI\nsolutions for specific engineering needs. Additionally, key challenges are\nexamined, and eight future research directions are highlighted. By providing a\ncomprehensive perspective, this paper aims to advance the strategic\nimplementation of AI, fostering the development of next-generation engineering\nAI solutions.", "AI": {"tldr": "The paper proposes the 'ABCDE' framework for Engineering AI, introducing a systematic ecosystem with eight layers to address domain-specific challenges and guide AI development in engineering.", "motivation": "The complexity and diversity of engineering problems lack systematic methodologies for AI development, requiring domain-specific solutions.", "method": "Introduces the 'ABCDE' framework and an eight-layer ecosystem to guide AI development for engineering needs.", "result": "A unified framework with attributes, goals, and applications is presented, along with key challenges and future research directions.", "conclusion": "The paper aims to advance AI implementation in engineering, fostering next-generation solutions through a comprehensive perspective."}}
{"id": "2504.17102", "pdf": "https://arxiv.org/pdf/2504.17102", "abs": "https://arxiv.org/abs/2504.17102", "authors": ["Haoyu Li", "Xiangru Zhong", "Bin Hu", "Huan Zhang"], "title": "Neural Contraction Metrics with Formal Guarantees for Discrete-Time Nonlinear Dynamical Systems", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted by L4DC 2025", "summary": "Contraction metrics are crucial in control theory because they provide a\npowerful framework for analyzing stability, robustness, and convergence of\nvarious dynamical systems. However, identifying these metrics for complex\nnonlinear systems remains an open challenge due to the lack of scalable and\neffective tools. This paper explores the approach of learning verifiable\ncontraction metrics parametrized as neural networks (NNs) for discrete-time\nnonlinear dynamical systems. While prior works on formal verification of\ncontraction metrics for general nonlinear systems have focused on convex\noptimization methods (e.g. linear matrix inequalities, etc) under the\nassumption of continuously differentiable dynamics, the growing prevalence of\nNN-based controllers, often utilizing ReLU activations, introduces challenges\ndue to the non-smooth nature of the resulting closed-loop dynamics. To bridge\nthis gap, we establish a new sufficient condition for establishing formal\nneural contraction metrics for general discrete-time nonlinear systems assuming\nonly the continuity of the dynamics. We show that from a computational\nperspective, our sufficient condition can be efficiently verified using the\nstate-of-the-art neural network verifier $\\alpha,\\!\\beta$-CROWN, which scales\nup non-convex neural network verification via novel integration of symbolic\nlinear bound propagation and branch-and-bound. Built upon our analysis tool, we\nfurther develop a learning method for synthesizing neural contraction metrics\nfrom sampled data. Finally, our approach is validated through the successful\nsynthesis and verification of NN contraction metrics for various nonlinear\nexamples.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2504.13471", "pdf": "https://arxiv.org/pdf/2504.13471", "abs": "https://arxiv.org/abs/2504.13471", "authors": ["Jiliang Ni", "Jiachen Pu", "Zhongyi Yang", "Kun Zhou", "Hui Wang", "Xiaoliang Xiao", "Dakui Wang", "Xin Li", "Jingfeng Luo", "Conggang Hu"], "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combines techniques like rejection fine-tuning, reinforcement\nlearning, and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress models to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas.", "AI": {"tldr": "A three-stage pipeline (prototyping, knowledge transfer, model compression) is introduced to optimize cost and performance in LLM-based systems, yielding a tiny, efficient model.", "motivation": "Address the high costs and latency of one-stage LLM pipelines while maintaining performance.", "method": "1. Prototype with function call-based LLM pipeline. 2. Transfer knowledge to a smaller student model. 3. Compress via quantization and pruning.", "result": "A 0.4B model with ultra-low latency and cost, retaining performance.", "conclusion": "The modular, cost-efficient framework shows promise for broader NLP applications."}}
{"id": "2504.17693", "pdf": "https://arxiv.org/pdf/2504.17693", "abs": "https://arxiv.org/abs/2504.17693", "authors": ["Asier Bikandi", "Muhammad Shaheer", "Hriday Bavle", "Jayan Jevanesan", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "BIM-Constrained Optimization for Accurate Localization and Deviation Correction in Construction Monitoring", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Augmented reality (AR) applications for construction monitoring rely on\nreal-time environmental tracking to visualize architectural elements. However,\nconstruction sites present significant challenges for traditional tracking\nmethods due to featureless surfaces, dynamic changes, and drift accumulation,\nleading to misalignment between digital models and the physical world. This\npaper proposes a BIM-aware drift correction method to address these challenges.\nInstead of relying solely on SLAM-based localization, we align ``as-built\"\ndetected planes from the real-world environment with ``as-planned\"\narchitectural planes in BIM. Our method performs robust plane matching and\ncomputes a transformation (TF) between SLAM (S) and BIM (B) origin frames using\noptimization techniques, minimizing drift over time. By incorporating BIM as\nprior structural knowledge, we can achieve improved long-term localization and\nenhanced AR visualization accuracy in noisy construction environments. The\nmethod is evaluated through real-world experiments, showing significant\nreductions in drift-induced errors and optimized alignment consistency. On\naverage, our system achieves a reduction of 52.24% in angular deviations and a\nreduction of 60.8% in the distance error of the matched walls compared to the\ninitial manual alignment by the user.", "AI": {"tldr": "A BIM-aware drift correction method is proposed to improve AR visualization in construction by aligning real-world planes with BIM models, reducing drift errors.", "motivation": "Construction sites challenge traditional AR tracking due to featureless surfaces and dynamic changes, causing misalignment between digital models and reality.", "method": "The method aligns detected real-world planes with BIM architectural planes, using optimization to compute transformations between SLAM and BIM frames, minimizing drift.", "result": "Real-world experiments show a 52.24% reduction in angular deviations and 60.8% reduction in distance errors compared to manual alignment.", "conclusion": "Incorporating BIM as prior knowledge enhances AR accuracy and long-term localization in noisy construction environments."}}
{"id": "2504.14128", "pdf": "https://arxiv.org/pdf/2504.14128", "abs": "https://arxiv.org/abs/2504.14128", "authors": ["Christopher Zhang Cui", "Xingdi Yuan", "Ziang Xiao", "Prithviraj Ammanabrolu", "Marc-Alexandre C\u00f4t\u00e9"], "title": "TALES: Text Adventure Learning Environment Suite", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tale-suite.", "AI": {"tldr": "TALES is a collection of text-adventure games designed to evaluate diverse reasoning in LLMs, showing top models struggle with human-designed games.", "motivation": "To challenge and assess the reasoning capabilities of LLMs in complex, sequential decision-making tasks.", "method": "Introduces TALES, a mix of synthetic and human-written text-adventure games, and evaluates various LLMs on them.", "result": "Top LLMs perform well on synthetic games but score below 15% on human-designed games.", "conclusion": "Current LLMs lack the reasoning sophistication for human-level performance in complex, enjoyable tasks."}}
{"id": "2504.17112", "pdf": "https://arxiv.org/pdf/2504.17112", "abs": "https://arxiv.org/abs/2504.17112", "authors": ["Margherita Lampani", "Sabrina Guastavino", "Michele Piana", "Federico Benvenuto"], "title": "Physics-informed features in supervised machine learning", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "68Q32, 62J07, 65J20"], "comment": null, "summary": "Supervised machine learning involves approximating an unknown functional\nrelationship from a limited dataset of features and corresponding labels. The\nclassical approach to feature-based machine learning typically relies on\napplying linear regression to standardized features, without considering their\nphysical meaning. This may limit model explainability, particularly in\nscientific applications. This study proposes a physics-informed approach to\nfeature-based machine learning that constructs non-linear feature maps informed\nby physical laws and dimensional analysis. These maps enhance model\ninterpretability and, when physical laws are unknown, allow for the\nidentification of relevant mechanisms through feature ranking. The method aims\nto improve both predictive performance in regression tasks and classification\nskill scores by integrating domain knowledge into the learning process, while\nalso enabling the potential discovery of new physical equations within the\ncontext of explainable machine learning.", "AI": {"tldr": "A physics-informed approach to feature-based machine learning is proposed, enhancing interpretability and performance by integrating physical laws and dimensional analysis.", "motivation": "Classical feature-based machine learning lacks physical meaning, limiting explainability, especially in scientific applications.", "method": "Constructs non-linear feature maps informed by physical laws and dimensional analysis, enabling feature ranking for unknown mechanisms.", "result": "Improves predictive performance in regression and classification while enhancing model interpretability.", "conclusion": "The method integrates domain knowledge for better performance and explainability, potentially uncovering new physical equations."}}
{"id": "2504.14992", "pdf": "https://arxiv.org/pdf/2504.14992", "abs": "https://arxiv.org/abs/2504.14992", "authors": ["Bohong Wu", "Shen Yan", "Sijun Zhang", "Jianqiao Lu", "Yutao Zeng", "Ya Wang", "Xun Zhou"], "title": "Efficient Pretraining Length Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.", "AI": {"tldr": "The paper introduces the PHD-Transformer, a framework for efficient length scaling in pre-training, using a novel KV cache management strategy and optimized variants (PHD-SWA and PHD-CSWA) to improve performance.", "motivation": "To explore the underexplored potential of length scaling in pre-training and address inefficiencies in existing methods.", "method": "Proposes the PHD-Transformer with a KV cache management strategy and introduces two optimized variants (PHD-SWA and PHD-CSWA) for enhanced performance.", "result": "Demonstrates consistent improvements across multiple benchmarks.", "conclusion": "The PHD-Transformer effectively enables length scaling in pre-training while maintaining inference efficiency."}}
{"id": "2504.17710", "pdf": "https://arxiv.org/pdf/2504.17710", "abs": "https://arxiv.org/abs/2504.17710", "authors": ["Yoeri Poels", "Alessandro Pau", "Christian Donner", "Giulio Romanelli", "Olivier Sauter", "Cristina Venturini", "Vlado Menkovski", "the TCV team", "the WPTE team"], "title": "Plasma State Monitoring and Disruption Characterization using Multimodal VAEs", "categories": ["physics.plasm-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "When a plasma disrupts in a tokamak, significant heat and electromagnetic\nloads are deposited onto the surrounding device components. These forces scale\nwith plasma current and magnetic field strength, making disruptions one of the\nkey challenges for future devices. Unfortunately, disruptions are not fully\nunderstood, with many different underlying causes that are difficult to\nanticipate. Data-driven models have shown success in predicting them, but they\nonly provide limited interpretability. On the other hand, large-scale\nstatistical analyses have been a great asset to understanding disruptive\npatterns. In this paper, we leverage data-driven methods to find an\ninterpretable representation of the plasma state for disruption\ncharacterization. Specifically, we use a latent variable model to represent\ndiagnostic measurements as a low-dimensional, latent representation. We build\nupon the Variational Autoencoder (VAE) framework, and extend it for (1)\ncontinuous projections of plasma trajectories; (2) a multimodal structure to\nseparate operating regimes; and (3) separation with respect to disruptive\nregimes. Subsequently, we can identify continuous indicators for the disruption\nrate and the disruptivity based on statistical properties of measurement data.\nThe proposed method is demonstrated using a dataset of approximately 1600 TCV\ndischarges, selecting for flat-top disruptions or regular terminations. We\nevaluate the method with respect to (1) the identified disruption risk and its\ncorrelation with other plasma properties; (2) the ability to distinguish\ndifferent types of disruptions; and (3) downstream analyses. For the latter, we\nconduct a demonstrative study on identifying parameters connected to\ndisruptions using counterfactual-like analysis. Overall, the method can\nadequately identify distinct operating regimes characterized by varying\nproximity to disruptions in an interpretable manner.", "AI": {"tldr": "The paper proposes a data-driven method using a Variational Autoencoder (VAE) to create an interpretable representation of plasma states for disruption prediction and characterization in tokamaks.", "motivation": "Disruptions in tokamaks pose significant challenges due to their unpredictable nature and destructive potential. Existing data-driven models lack interpretability, while statistical analyses offer limited insights.", "method": "The authors extend the VAE framework to include continuous projections of plasma trajectories, multimodal structure for regime separation, and disruption-specific separation. They apply this to 1600 TCV discharges.", "result": "The method successfully identifies distinct operating regimes and provides continuous indicators for disruption risk and disruptivity. It also distinguishes disruption types and supports downstream analyses.", "conclusion": "The proposed approach offers an interpretable and effective way to characterize plasma states and predict disruptions, aiding future tokamak research and operation."}}
{"id": "2504.15364", "pdf": "https://arxiv.org/pdf/2504.15364", "abs": "https://arxiv.org/abs/2504.15364", "authors": ["Junyoung Park", "Dalton Jones", "Matt J Morse", "Raghavv Goel", "Mingu Lee", "Chris Lott"], "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments", "categories": ["cs.AI"], "comment": "8 pages, 14 figures", "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.", "AI": {"tldr": "KeyDiff is a training-free KV cache eviction method for LLMs, optimizing key diversity to handle long prompts efficiently under resource constraints.", "motivation": "To enable LLM deployment in resource-limited environments by addressing the challenge of processing long input prompts.", "method": "Proposes KeyDiff, a method based on key similarity for KV cache eviction, independent of attention scores.", "result": "KeyDiff achieves near-optimal performance (0.04% gap) with 23% KV cache reduction on LongBench for Llama models.", "conclusion": "KeyDiff is effective for resource-constrained LLM applications, offering theoretical and practical advantages."}}
{"id": "2504.17142", "pdf": "https://arxiv.org/pdf/2504.17142", "abs": "https://arxiv.org/abs/2504.17142", "authors": ["Siddharth Nair", "Timothy F. Walsh", "Greg Pickrell", "Fabio Semperlotti"], "title": "Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints", "categories": ["physics.comp-ph", "cs.CE", "cs.LG"], "comment": "27 pages of main text, 15 figures", "summary": "This study focuses on the development of reinforcement learning based\ntechniques for the design of microelectronic components under multiphysics\nconstraints. While traditional design approaches based on global optimization\napproaches are effective when dealing with a small number of design parameters,\nas the complexity of the solution space and of the constraints increases\ndifferent techniques are needed. This is an important reason that makes the\ndesign and optimization of microelectronic components (characterized by large\nsolution space and multiphysics constraints) very challenging for traditional\nmethods. By taking as prototypical elements an application-specific integrated\ncircuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and\nnumerically test an optimization framework based on reinforcement learning\n(RL). More specifically, we consider the optimization of the bonded\ninterconnect geometry for an ASIC chip as well as the placement of components\non a HI interposer while satisfying thermoelastic and design constraints. This\nplacement problem is particularly interesting because it features a\nhigh-dimensional solution space.", "AI": {"tldr": "The paper explores reinforcement learning (RL) for optimizing microelectronic component design under multiphysics constraints, addressing challenges traditional methods face with complex solution spaces.", "motivation": "Traditional design methods struggle with high-dimensional solution spaces and multiphysics constraints in microelectronics, necessitating advanced techniques like RL.", "method": "The study develops an RL-based optimization framework, tested numerically on ASIC and HI interposer designs, focusing on bonded interconnect geometry and component placement.", "result": "The RL framework effectively handles the high-dimensional solution space and multiphysics constraints, demonstrating its potential for microelectronic design.", "conclusion": "RL offers a promising alternative to traditional methods for optimizing complex microelectronic components under multiphysics constraints."}}
{"id": "2406.14088", "pdf": "https://arxiv.org/pdf/2406.14088", "abs": "https://arxiv.org/abs/2406.14088", "authors": ["Zhiyu Mei", "Wei Fu", "Kaiwei Li", "Guangju Wang", "Huanchen Zhang", "Yi Wu"], "title": "ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "comment": "11 pages (20 pages with references and the appendix), 17 figures.\n  Accepted by MLSys 25", "summary": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for\nempowering large language model (LLM) applications. Compared with the\nsupervised training process of LLMs, the RLHF training process is much more\nsophisticated, requiring a diverse range of computation workloads with\nintricate dependencies between multiple LLM instances. Therefore, simply\nadopting the fixed parallelization strategies from supervised training for LLMs\ncan be insufficient for RLHF and result in low training efficiency. To overcome\nthis limitation, we propose a novel technique named parameter ReaLlocation,\nwhich dynamically adapts the parallelization strategies for different workloads\nduring training by redistributing LLM parameters across the training cluster.\nBuilding upon this idea, we introduce ReaL, a pioneering system for efficient\nRLHF training. ReaL introduces the concept of an execution plan, which defines\na fine-grained resource allocation and parallelization strategy particularly\ndesigned for RLHF training. Based on this concept, ReaL employs a tailored\nsearch algorithm with a lightweight run-time estimator to automatically\ndiscover an efficient execution plan for an instance of RLHF experiment.\nSubsequently, the runtime engine deploys the selected plan by effectively\nparallelizing computations and redistributing parameters. We evaluate ReaL on\nthe LLaMA models with up to 70 billion parameters and 128 GPUs. The\nexperimental results demonstrate that ReaL achieves speedups of up to\n$3.58\\times$ compared to baseline methods. Furthermore, the execution plans\ngenerated by ReaL exhibit an average of $81\\%$ performance improvement over\nheuristic approaches based on Megatron-LM in the long-context scenario. The\nsource code of ReaL is publicly available at\nhttps://github.com/openpsi-project/ReaLHF .", "AI": {"tldr": "ReaL introduces dynamic parallelization for RLHF training, improving efficiency by redistributing LLM parameters and using tailored execution plans, achieving up to 3.58\u00d7 speedup.", "motivation": "Fixed parallelization strategies from supervised LLM training are inefficient for RLHF due to diverse workloads and dependencies.", "method": "Proposes ReaL, a system with parameter ReaLlocation and execution plans, using a search algorithm and runtime estimator to optimize resource allocation.", "result": "Achieves up to 3.58\u00d7 speedup on LLaMA models (70B params, 128 GPUs) and 81% improvement over heuristic methods.", "conclusion": "ReaL significantly enhances RLHF training efficiency through dynamic parallelization and execution plans."}}
{"id": "2308.12452", "pdf": "https://arxiv.org/pdf/2308.12452", "abs": "https://arxiv.org/abs/2308.12452", "authors": ["Wenzhao Li", "Tianhao Wu", "Fangcheng Zhong", "Cengiz Oztireli"], "title": "ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for 3D Scene Stylization", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted at WACV 2025. The published version is available at\n  https://ieeexplore.ieee.org/document/10944114", "summary": "The radiance fields style transfer is an emerging field that has recently\ngained popularity as a means of 3D scene stylization, thanks to the outstanding\nperformance of neural radiance fields in 3D reconstruction and view synthesis.\nWe highlight a research gap in radiance fields style transfer, the lack of\nsufficient perceptual controllability, motivated by the existing concept in the\n2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style\ntransfer framework offering manageable control over perceptual factors, to\nsystematically explore the perceptual controllability in 3D scene stylization.\nFour distinct types of controls - color preservation control, (style pattern)\nscale control, spatial (selective stylization area) control, and depth\nenhancement control - are proposed and integrated into this framework. Results\nfrom real-world datasets, both quantitative and qualitative, show that the four\ntypes of controls in our ARF-Plus framework successfully accomplish their\ncorresponding perceptual controls when stylizing 3D scenes. These techniques\nwork well for individual style inputs as well as for the simultaneous\napplication of multiple styles within a scene. This unlocks a realm of\nlimitless possibilities, allowing customized modifications of stylization\neffects and flexible merging of the strengths of different styles, ultimately\nenabling the creation of novel and eye-catching stylistic effects on 3D scenes.", "AI": {"tldr": "ARF-Plus introduces a 3D neural style transfer framework with four perceptual controls for enhanced stylization of 3D scenes.", "motivation": "Addresses the lack of perceptual controllability in radiance fields style transfer, inspired by 2D image style transfer concepts.", "method": "Proposes ARF-Plus, integrating four controls: color preservation, style pattern scale, spatial selective stylization, and depth enhancement.", "result": "Demonstrates successful perceptual control in 3D scene stylization, applicable to single or multiple styles.", "conclusion": "Enables customizable and novel stylistic effects in 3D scenes, expanding creative possibilities."}}
{"id": "2504.15903", "pdf": "https://arxiv.org/pdf/2504.15903", "abs": "https://arxiv.org/abs/2504.15903", "authors": ["Nikhil Khandalkar", "Pavan Yadav", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations", "categories": ["cs.AI"], "comment": "60 pages, 25 figures", "summary": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility.", "AI": {"tldr": "LLMs like GPT-4o perform well on ARC tasks under ideal conditions, but models like DeepSeek R1 and LLaMA 3.2 struggle, revealing sensitivity to noise and limitations in abstract reasoning.", "motivation": "To evaluate and compare the structured reasoning capabilities of LLMs under varying noise levels and temperature settings, identifying vulnerabilities in their generalization.", "method": "Systematic evaluation of models (GPT-4o, DeepSeek R1, LLaMA 3.2) across different noise levels and temperature settings using the ARC benchmark.", "result": "Noise consistently impairs performance across all models, exposing their fragility and lack of robustness in real-world scenarios.", "conclusion": "Current LLMs need improvement in robustness and adaptability to handle real-world ambiguity, guiding future research toward more human-like cognitive flexibility."}}
{"id": "2504.17166", "pdf": "https://arxiv.org/pdf/2504.17166", "abs": "https://arxiv.org/abs/2504.17166", "authors": ["Ke Wan", "Kensuke Tanioka", "Toshio Shimokawa"], "title": "Causal rule ensemble approach for multi-arm data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Heterogeneous treatment effect (HTE) estimation is critical in medical\nresearch. It provides insights into how treatment effects vary among\nindividuals, which can provide statistical evidence for precision medicine.\nWhile most existing methods focus on binary treatment situations, real-world\napplications often involve multiple interventions. However, current HTE\nestimation methods are primarily designed for binary comparisons and often rely\non black-box models, which limit their applicability and interpretability in\nmulti-arm settings. To address these challenges, we propose an interpretable\nmachine learning framework for HTE estimation in multi-arm trials. Our method\nemploys a rule-based ensemble approach consisting of rule generation, rule\nensemble, and HTE estimation, ensuring both predictive accuracy and\ninterpretability. Through extensive simulation studies and real data\napplications, the performance of our method was evaluated against\nstate-of-the-art multi-arm HTE estimation approaches. The results indicate that\nour approach achieved lower bias and higher estimation accuracy compared with\nthose of existing methods. Furthermore, the interpretability of our framework\nallows clearer insights into how covariates influence treatment effects,\nfacilitating clinical decision making. By bridging the gap between accuracy and\ninterpretability, our study contributes a valuable tool for multi-arm HTE\nestimation, supporting precision medicine.", "AI": {"tldr": "Proposes an interpretable machine learning framework for heterogeneous treatment effect (HTE) estimation in multi-arm trials, outperforming existing methods in accuracy and interpretability.", "motivation": "Addresses limitations of current HTE methods, which are binary-focused and lack interpretability in multi-arm settings, hindering precision medicine.", "method": "Uses a rule-based ensemble approach with rule generation, rule ensemble, and HTE estimation to balance accuracy and interpretability.", "result": "Achieves lower bias and higher accuracy than state-of-the-art methods, with clear insights into covariate effects on treatment.", "conclusion": "Provides a valuable tool for multi-arm HTE estimation, enhancing precision medicine by combining accuracy and interpretability."}}
{"id": "2410.04612", "pdf": "https://arxiv.org/pdf/2410.04612", "abs": "https://arxiv.org/abs/2410.04612", "authors": ["Zhaolin Gao", "Wenhao Zhan", "Jonathan D. Chang", "Gokul Swamy", "Kiant\u00e9 Brantley", "Jason D. Lee", "Wen Sun"], "title": "Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success at tasks like\nsummarization that involve a single turn of interaction. However, they can\nstill struggle with multi-turn tasks like dialogue that require long-term\nplanning. Previous works on multi-turn dialogue extend single-turn\nreinforcement learning from human feedback (RLHF) methods to the multi-turn\nsetting by treating all prior dialogue turns as a long context. Such approaches\nsuffer from covariate shift: the conversations in the training set have\nprevious turns generated by some reference policy, which means that low\ntraining error may not necessarily correspond to good performance when the\nlearner is actually in the conversation loop. In response, we introduce\nREgressing the RELative FUture (REFUEL), an efficient policy optimization\napproach designed to address multi-turn RLHF in LLMs. REFUEL employs a single\nmodel to estimate $Q$-values and trains on self-generated data, addressing the\ncovariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence\nof regression tasks on iteratively collected datasets, enabling ease of\nimplementation. Theoretically, we prove that REFUEL can match the performance\nof any policy covered by the training set. Empirically, we evaluate our\nalgorithm by using Llama-3.1-70B-it to simulate a user in conversation with our\nmodel. REFUEL consistently outperforms state-of-the-art methods such as DPO and\nREBEL across various settings. Furthermore, despite having only 8 billion\nparameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it\non long multi-turn dialogues. Implementation of REFUEL can be found at\nhttps://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be\nfound at https://huggingface.co/Cornell-AGI.", "AI": {"tldr": "REFUEL is a policy optimization method for multi-turn dialogue in LLMs, addressing covariate shift by training on self-generated data and outperforming state-of-the-art methods.", "motivation": "LLMs struggle with multi-turn tasks like dialogue due to covariate shift in RLHF methods.", "method": "REFUEL uses a single model for Q-value estimation and trains on self-generated data, framing multi-turn RLHF as regression tasks.", "result": "REFUEL outperforms DPO and REBEL, with smaller models (8B) surpassing larger ones (70B) in multi-turn dialogues.", "conclusion": "REFUEL is an efficient, theoretically sound solution for multi-turn RLHF, with strong empirical performance."}}
{"id": "2312.00113", "pdf": "https://arxiv.org/pdf/2312.00113", "abs": "https://arxiv.org/abs/2312.00113", "authors": ["Ziyun Wang", "Friedhelm Hamann", "Kenneth Chaney", "Wen Jiang", "Guillermo Gallego", "Kostas Daniilidis"], "title": "Event-based Continuous Color Video Decompression from Single Frames", "categories": ["cs.CV"], "comment": null, "summary": "We present ContinuityCam, a novel approach to generate a continuous video\nfrom a single static RGB image and an event camera stream. Conventional cameras\nstruggle with high-speed motion capture due to bandwidth and dynamic range\nlimitations. Event cameras are ideal sensors to solve this problem because they\nencode compressed change information at high temporal resolution. In this work,\nwe tackle the problem of event-based continuous color video decompression,\npairing single static color frames and event data to reconstruct temporally\ncontinuous videos. Our approach combines continuous long-range motion modeling\nwith a neural synthesis model, enabling frame prediction at arbitrary times\nwithin the events. Our method only requires an initial image, thus increasing\nthe robustness to sudden motions, light changes, minimizing the prediction\nlatency, and decreasing bandwidth usage. We also introduce a novel single-lens\nbeamsplitter setup that acquires aligned images and events, and a novel and\nchallenging Event Extreme Decompression Dataset (E2D2) that tests the method in\nvarious lighting and motion profiles. We thoroughly evaluate our method by\nbenchmarking color frame reconstruction, outperforming the baseline methods by\n3.61 dB in PSNR and by 33% decrease in LPIPS, as well as showing superior\nresults on two downstream tasks.", "AI": {"tldr": "ContinuityCam generates continuous video from a single RGB image and event camera data, outperforming baselines in quality and efficiency.", "motivation": "High-speed motion capture is limited by conventional cameras' bandwidth and dynamic range; event cameras offer a solution with compressed, high-temporal-resolution data.", "method": "Combines continuous long-range motion modeling with neural synthesis to predict frames at arbitrary times using an initial image and event data.", "result": "Outperforms baselines by 3.61 dB in PSNR and 33% in LPIPS, with superior performance in downstream tasks.", "conclusion": "The method is robust, efficient, and validated by a novel dataset and hardware setup."}}
{"id": "2310.07263", "pdf": "https://arxiv.org/pdf/2310.07263", "abs": "https://arxiv.org/abs/2310.07263", "authors": ["Frank Joublin", "Antonello Ceravola", "Pavel Smirnov", "Felix Ocker", "Joerg Deigmoeller", "Anna Belardinelli", "Chao Wang", "Stephan Hasler", "Daniel Tanneberg", "Michael Gienger"], "title": "CoPAL: Corrective Planning of Robot Actions with Large Language Models", "categories": ["cs.RO", "cs.AI"], "comment": "IEEE International Conference on Robotics and Automation (ICRA) 2024", "summary": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation.", "AI": {"tldr": "A system architecture for robotic task and motion planning using LLMs, featuring a novel replanning strategy to handle errors, tested in simulations and real-world scenarios.", "motivation": "To address the challenge of open-world complexity in autonomous robotic systems by improving task and motion planning.", "method": "Proposes a feedback architecture integrating reasoning, planning, and motion generation with a novel replanning strategy.", "result": "Demonstrated efficacy in executability, correctness, and time complexity in simulations and real-world tasks (blocks world, barman, pizza preparation).", "conclusion": "The system enhances robotic autonomy by effectively managing errors and improving plan quality in complex environments."}}
{"id": "2504.17173", "pdf": "https://arxiv.org/pdf/2504.17173", "abs": "https://arxiv.org/abs/2504.17173", "authors": ["Tianyu Zhang", "Dongheng Zhang", "Ruixu Geng", "Xuecheng Xie", "Shuai Yang", "Yan Chen"], "title": "Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "In recent years, Channel State Information (CSI), recognized for its\nfine-grained spatial characteristics, has attracted increasing attention in\nWiFi-based indoor localization. However, despite its potential, CSI-based\napproaches have yet to achieve the same level of deployment scale and\ncommercialization as those based on Received Signal Strength Indicator (RSSI).\nA key limitation lies in the fact that most existing CSI-based systems are\ndeveloped and evaluated in controlled, small-scale environments, limiting their\ngeneralizability. To bridge this gap, we explore the deployment of a\nlarge-scale CSI-based localization system involving over 400 Access Points\n(APs) in a real-world building under the Integrated Sensing and Communication\n(ISAC) paradigm. We highlight two critical yet often overlooked factors: the\nunderutilization of unlabeled data and the inherent heterogeneity of CSI\nmeasurements. To address these challenges, we propose a novel CSI-based\nlearning framework for WiFi localization, tailored for large-scale ISAC\ndeployments on the server side. Specifically, we employ a novel graph-based\nstructure to model heterogeneous CSI data and reduce redundancy. We further\ndesign a pretext pretraining task that incorporates spatial and temporal priors\nto effectively leverage large-scale unlabeled CSI data. Complementarily, we\nintroduce a confidence-aware fine-tuning strategy to enhance the robustness of\nlocalization results. In a leave-one-smartphone-out experiment spanning five\nfloors and 25, 600 m2, we achieve a median localization error of 2.17 meters\nand a floor accuracy of 99.49%. This performance corresponds to an 18.7%\nreduction in mean absolute error (MAE) compared to the best-performing\nbaseline.", "AI": {"tldr": "The paper proposes a novel CSI-based learning framework for WiFi localization, addressing challenges like unlabeled data and CSI heterogeneity, achieving improved accuracy in large-scale real-world deployments.", "motivation": "Existing CSI-based localization systems lack generalizability due to small-scale evaluations. The paper aims to bridge this gap by deploying a large-scale system under the ISAC paradigm.", "method": "A graph-based structure models heterogeneous CSI data, a pretext pretraining task leverages unlabeled data, and a confidence-aware fine-tuning strategy enhances robustness.", "result": "Achieved a median localization error of 2.17 meters and 99.49% floor accuracy, with an 18.7% reduction in MAE compared to baselines.", "conclusion": "The proposed framework effectively addresses scalability and heterogeneity challenges, demonstrating superior performance in real-world large-scale deployments."}}
{"id": "2501.05255", "pdf": "https://arxiv.org/pdf/2501.05255", "abs": "https://arxiv.org/abs/2501.05255", "authors": ["Yewei Song", "Xunzhu Tang", "Cedric Lothritz", "Saad Ezzini", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9", "Andrey Boytsov", "Ulrick Ble", "Anne Goujon"], "title": "CallNavi, A Challenge and Empirical Study on LLM Function Calling and Routing", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "API-driven chatbot systems are increasingly integral to software engineering\napplications, yet their effectiveness hinges on accurately generating and\nexecuting API calls. This is particularly challenging in scenarios requiring\nmulti-step interactions with complex parameterization and nested API\ndependencies. Addressing these challenges, this work contributes to the\nevaluation and assessment of AI-based software development through three key\nadvancements: (1) the introduction of a novel dataset specifically designed for\nbenchmarking API function selection, parameter generation, and nested API\nexecution; (2) an empirical evaluation of state-of-the-art language models,\nanalyzing their performance across varying task complexities in API function\ngeneration and parameter accuracy; and (3) a hybrid approach to API routing,\ncombining general-purpose large language models for API selection with\nfine-tuned models and prompt engineering for parameter generation. These\ninnovations significantly improve API execution in chatbot systems, offering\npractical methodologies for enhancing software design, testing, and operational\nworkflows in real-world software engineering contexts.", "AI": {"tldr": "The paper introduces a dataset and hybrid approach to improve API-driven chatbot systems, evaluating language models for API function and parameter accuracy.", "motivation": "The effectiveness of API-driven chatbots depends on accurate API call generation and execution, especially in complex, multi-step interactions.", "method": "The work includes: (1) a novel dataset for benchmarking, (2) empirical evaluation of language models, and (3) a hybrid API routing approach combining general-purpose and fine-tuned models.", "result": "The innovations enhance API execution in chatbots, improving software design, testing, and workflows.", "conclusion": "The proposed advancements offer practical solutions for real-world software engineering challenges in chatbot systems."}}
{"id": "2404.00146", "pdf": "https://arxiv.org/pdf/2404.00146", "abs": "https://arxiv.org/abs/2404.00146", "authors": ["Huiyuan Yu", "Jia He", "Maggie Cheng"], "title": "Fast OMP for Exact Recovery and Sparse Approximation", "categories": ["cs.CV", "math.OC"], "comment": "It has been published in ICPR 2024", "summary": "Orthogonal Matching Pursuit (OMP) has been a powerful method in sparse signal\nrecovery and approximation. However OMP suffers computational issue when the\nsignal has large number of non-zeros. This paper advances OMP in two fronts: it\noffers a fast algorithm for the orthogonal projection of the input signal at\neach iteration, and a new selection criterion for making the greedy choice,\nwhich reduces the number of iterations it takes to recover the signal. The\nproposed modifications to OMP directly reduce the computational complexity.\nExperiment results show significant improvement over the classical OMP in\ncomputation time. The paper also provided a sufficient condition for exact\nrecovery under the new greedy choice criterion. For general signals that may\nnot have sparse representations, the paper provides a bound for the\napproximation error. The approximation error is at the same order as OMP but is\nobtained within fewer iterations and less time.", "AI": {"tldr": "The paper improves Orthogonal Matching Pursuit (OMP) by introducing a faster projection method and a new greedy selection criterion, reducing computational complexity and iteration count while maintaining recovery guarantees.", "motivation": "OMP is computationally inefficient for signals with many non-zeros, limiting its practicality. The paper aims to enhance OMP's speed and efficiency.", "method": "Proposes a fast orthogonal projection algorithm and a new greedy selection criterion to reduce iterations and computation time.", "result": "Experiments show significant speed improvements over classical OMP, with maintained recovery guarantees and comparable approximation error.", "conclusion": "The modified OMP achieves faster signal recovery with fewer iterations, making it more practical for large-scale applications."}}
{"id": "2402.04869", "pdf": "https://arxiv.org/pdf/2402.04869", "abs": "https://arxiv.org/abs/2402.04869", "authors": ["Ruichu Cai", "Siyang Huang", "Jie Qiao", "Wei Chen", "Yan Zeng", "Keli Zhang", "Fuchun Sun", "Yang Yu", "Zhifeng Hao"], "title": "Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by Science China Information Sciences", "summary": "As a key component to intuitive cognition and reasoning solutions in human\nintelligence, causal knowledge provides great potential for reinforcement\nlearning (RL) agents' interpretability towards decision-making by helping\nreduce the searching space. However, there is still a considerable gap in\ndiscovering and incorporating causality into RL, which hinders the rapid\ndevelopment of causal RL. In this paper, we consider explicitly modeling the\ngeneration process of states with the causal graphical model, based on which we\naugment the policy. We formulate the causal structure updating into the RL\ninteraction process with active intervention learning of the environment. To\noptimize the derived objective, we propose a framework with theoretical\nperformance guarantees that alternates between two steps: using interventions\nfor causal structure learning during exploration and using the learned causal\nstructure for policy guidance during exploitation. Due to the lack of public\nbenchmarks that allow direct intervention in the state space, we design the\nroot cause localization task in our simulated fault alarm environment and then\nempirically show the effectiveness and robustness of the proposed method\nagainst state-of-the-art baselines. Theoretical analysis shows that our\nperformance improvement attributes to the virtuous cycle of causal-guided\npolicy learning and causal structure learning, which aligns with our\nexperimental results. Codes are available at\nhttps://github.com/DMIRLAB-Group/FaultAlarm_RL.", "AI": {"tldr": "The paper proposes a causal reinforcement learning (RL) framework that integrates causal graphical models to improve interpretability and decision-making by alternating between causal structure learning and policy guidance.", "motivation": "Causal knowledge is crucial for human-like reasoning in RL but is underutilized. The paper aims to bridge this gap by explicitly modeling causality to enhance RL agents' interpretability and efficiency.", "method": "The method involves modeling state generation with causal graphical models, updating causal structures through active intervention, and alternating between causal learning and policy guidance. A theoretical framework ensures performance guarantees.", "result": "The proposed method shows effectiveness and robustness in a simulated fault alarm environment, outperforming state-of-the-art baselines. Theoretical and empirical results confirm the virtuous cycle of causal-guided learning.", "conclusion": "The framework successfully integrates causality into RL, improving interpretability and performance. Future work may explore broader applications and benchmarks."}}
{"id": "2504.17203", "pdf": "https://arxiv.org/pdf/2504.17203", "abs": "https://arxiv.org/abs/2504.17203", "authors": ["Shivasankari Kannan", "Yeounoh Chung", "Amita Gondi", "Tristan Swadell", "Fatma Ozcan"], "title": "High-Fidelity And Complex Test Data Generation For Real-World SQL Code Generation Services", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically ``meaningful'' mock data for complex\nschema that includes columns with nested structures that we frequently\nencounter in Google SQL code generation workloads. We highlight the limitations\nof existing approaches used in production, particularly their inability to\nhandle large and complex schema, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate realistic high-fidelity test data that\nadheres to complex structural constraints and maintains semantic integrity to\nthe test targets (SQL queries/functions). This approach supports comprehensive\ntesting of complex SQL queries involving joins, aggregations, and even deeply\nnested subqueries, ensuring robust evaluation of SQL code generation services,\nlike NL2SQL and SQL Code Assistant services. Our results demonstrate the\npractical utility of an out-of-the-box LLM (\\textit{gemini}) based test data\ngeneration for industrial SQL code generation services where generating\nrealistic test data is essential due to the frequent unavailability of\nproduction datasets.", "AI": {"tldr": "The paper proposes using LLMs to generate high-fidelity test data for complex SQL schemas, addressing limitations of traditional methods.", "motivation": "The need for realistic test data in industrial settings, especially for SQL code generation services, where production data is often unavailable.", "method": "Leverages LLMs (e.g., Gemini) with pre- and post-processing to generate semantically coherent and structurally correct test data.", "result": "Demonstrates practical utility in generating test data for complex SQL queries, improving test coverage.", "conclusion": "LLM-based test data generation is effective for industrial SQL code generation services, ensuring robust evaluation."}}
{"id": "2503.10742", "pdf": "https://arxiv.org/pdf/2503.10742", "abs": "https://arxiv.org/abs/2503.10742", "authors": ["Yudong Liu", "Jingwei Sun", "Yueqian Lin", "Jingyang Zhang", "Ming Yin", "Qinsi Wang", "Jianyi Zhang", "Hai Li", "Yiran Chen"], "title": "Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Vision language models (VLMs) demonstrate strong capabilities in jointly\nprocessing visual and textual data. However, they often incur substantial\ncomputational overhead due to redundant visual information, particularly in\nlong-form video scenarios. Existing approaches predominantly focus on either\nvision token pruning, which may overlook spatio-temporal dependencies, or\nkeyframe selection, which identifies informative frames but discards others,\nthus disrupting contextual continuity. In this work, we propose KVTP\n(Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the\ndrawbacks of token pruning and keyframe selection. By adaptively assigning\npruning rates based on frame relevance to the query, KVTP effectively retains\nessential contextual information while significantly reducing redundant\ncomputation. To thoroughly evaluate the long-form video understanding\ncapacities of VLMs, we curated and reorganized subsets from VideoMME,\nEgoSchema, and NextQA into a unified benchmark named SparseKV-QA that\nhighlights real-world scenarios with sparse but crucial events. Our experiments\nwith VLMs of various scales show that KVTP can reduce token usage by 80%\nwithout compromising spatiotemporal and contextual consistency, significantly\ncutting computation while maintaining the performance. These results\ndemonstrate our approach's effectiveness in efficient long-video processing,\nfacilitating more scalable VLM deployment.", "AI": {"tldr": "KVTP (Keyframe-oriented Vision Token Pruning) is a novel framework that reduces redundant computation in VLMs for long-form videos by adaptively pruning tokens based on frame relevance, maintaining performance.", "motivation": "VLMs incur high computational overhead due to redundant visual data in long videos; existing methods either disrupt context or overlook dependencies.", "method": "KVTP combines keyframe selection and token pruning, adaptively assigning pruning rates to retain essential context while reducing redundancy.", "result": "KVTP reduces token usage by 80% without losing spatiotemporal or contextual consistency, cutting computation while maintaining performance.", "conclusion": "KVTP enables efficient long-video processing, making VLM deployment more scalable."}}
{"id": "2404.17230", "pdf": "https://arxiv.org/pdf/2404.17230", "abs": "https://arxiv.org/abs/2404.17230", "authors": ["Ziyue Zhang", "Mingbao Lin", "Quanjian Song", "Yuxin Zhang", "Rongrong Ji"], "title": "ObjectAdd: Adding Objects into Image via a Training-Free Diffusion Modification Fashion", "categories": ["cs.CV"], "comment": "13 pages in total", "summary": "We introduce ObjectAdd, a training-free diffusion modification method to add\nuser-expected objects into user-specified area. The motive of ObjectAdd stems\nfrom: first, describing everything in one prompt can be difficult, and second,\nusers often need to add objects into the generated image. To accommodate with\nreal world, our ObjectAdd maintains accurate image consistency after adding\nobjects with technical innovations in: (1) embedding-level concatenation to\nensure correct text embedding coalesce; (2) object-driven layout control with\nlatent and attention injection to ensure objects accessing user-specified area;\n(3) prompted image inpainting in an attention refocusing & object expansion\nfashion to ensure rest of the image stays the same. With a text-prompted image,\nour ObjectAdd allows users to specify a box and an object, and achieves: (1)\nadding object inside the box area; (2) exact content outside the box area; (3)\nflawless fusion between the two areas", "AI": {"tldr": "ObjectAdd is a training-free diffusion method for adding objects into specified areas of an image while maintaining consistency.", "motivation": "The method addresses the difficulty of describing everything in one prompt and the need to add objects post-generation.", "method": "Uses embedding-level concatenation, object-driven layout control, and prompted image inpainting for seamless object addition.", "result": "Achieves accurate object placement, unchanged content outside the box, and flawless fusion between areas.", "conclusion": "ObjectAdd effectively adds objects to images while preserving consistency and user control."}}
{"id": "2402.14781", "pdf": "https://arxiv.org/pdf/2402.14781", "abs": "https://arxiv.org/abs/2402.14781", "authors": ["Christian Toth", "Christian Knoll", "Franz Pernkopf", "Robert Peharz"], "title": "Effective Bayesian Causal Inference via Structural Marginalisation and Autoregressive Orders", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": "9 pages + references + appendices (37 pages total)", "summary": "The traditional two-stage approach to causal inference first identifies a\nsingle causal model (or equivalence class of models), which is then used to\nanswer causal queries. However, this neglects any epistemic model uncertainty.\nIn contrast, Bayesian causal inference does incorporate epistemic uncertainty\ninto query estimates via Bayesian marginalisation (posterior averaging) over\nall causal models. While principled, this marginalisation over entire causal\nmodels, i.e., both causal structures (graphs) and mechanisms, poses a\ntremendous computational challenge. In this work, we address this challenge by\ndecomposing structure marginalisation into the marginalisation over (i) causal\norders and (ii) directed acyclic graphs (DAGs) given an order. We can\nmarginalise the latter in closed form by limiting the number of parents per\nvariable and utilising Gaussian processes to model mechanisms. To marginalise\nover orders, we use a sampling-based approximation, for which we devise a novel\nauto-regressive distribution over causal orders (ARCO). Our method outperforms\nstate-of-the-art in structure learning on simulated non-linear additive noise\nbenchmarks, and yields competitive results on real-world data. Furthermore, we\ncan accurately infer interventional distributions and average causal effects.", "AI": {"tldr": "The paper introduces a Bayesian causal inference method that decomposes structure marginalization into causal orders and DAGs, using Gaussian processes and a novel ARCO distribution for efficient computation.", "motivation": "Traditional causal inference neglects epistemic model uncertainty, while Bayesian methods face computational challenges in marginalizing over entire causal models.", "method": "Decomposes structure marginalization into causal orders and DAGs, using Gaussian processes and a novel auto-regressive distribution (ARCO) for sampling-based approximation.", "result": "Outperforms state-of-the-art in structure learning on simulated benchmarks and yields competitive results on real-world data, accurately inferring interventional distributions and causal effects.", "conclusion": "The proposed method efficiently addresses computational challenges in Bayesian causal inference, improving accuracy and scalability."}}
{"id": "2504.17236", "pdf": "https://arxiv.org/pdf/2504.17236", "abs": "https://arxiv.org/abs/2504.17236", "authors": ["Xiqiang Qu", "Jun Chen", "Lei Yu", "Xiangyu Xu"], "title": "Rate-Distortion-Perception Theory for the Quadratic Wasserstein Space", "categories": ["cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "We establish a single-letter characterization of the fundamental\ndistortion-rate-perception tradeoff with limited common randomness under the\nsquared error distortion measure and the squared Wasserstein-2 perception\nmeasure. Moreover, it is shown that this single-letter characterization can be\nexplicitly evaluated for the Gaussian source. Various notions of universal\nrepresentation are also clarified.", "AI": {"tldr": "A single-letter characterization of the distortion-rate-perception tradeoff is established for Gaussian sources under specific distortion and perception measures.", "motivation": "To understand the fundamental limits of the distortion-rate-perception tradeoff with limited common randomness.", "method": "Single-letter characterization under squared error distortion and squared Wasserstein-2 perception measures, evaluated explicitly for Gaussian sources.", "result": "The tradeoff is characterized and evaluated for Gaussian sources.", "conclusion": "The study clarifies universal representation notions and provides a foundational understanding of the tradeoff."}}
{"id": "2504.11336", "pdf": "https://arxiv.org/pdf/2504.11336", "abs": "https://arxiv.org/abs/2504.11336", "authors": ["Abitha Thankaraj", "Yiding Jiang", "J. Zico Kolter", "Yonatan Bisk"], "title": "Looking beyond the next token", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm.", "AI": {"tldr": "Trelawney, a data sequence rearrangement method, improves model performance without architectural changes, enabling better planning, reasoning, and goal generation.", "motivation": "Address the mismatch between causal language model training (predicting tokens from context) and human writing (knowing goals beforehand).", "method": "Rearranges and processes training data sequences to align with the true data-generating process, avoiding architectural changes.", "result": "Improves performance on benchmarks for planning, algorithmic reasoning, and story generation; enables goal generation.", "conclusion": "Trelawney offers a simple yet effective solution, potentially expanding language model capabilities beyond current paradigms."}}
{"id": "2405.04605", "pdf": "https://arxiv.org/pdf/2405.04605", "abs": "https://arxiv.org/abs/2405.04605", "authors": ["Fakrul Islam Tushar", "Avivah Wang", "Lavsen Dahal", "Michael R. Harowicz", "Kyle J. Lafata", "Tina D. Tailor", "Joseph Y. Lo"], "title": "AI in Lung Health: Benchmarking Detection and Diagnostic Models Across Multiple CT Scan Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "2 tables, 6 figures", "summary": "Lung cancer remains the leading cause of cancer-related mortality worldwide,\nand early detection through low-dose computed tomography (LDCT) has shown\nsignificant promise in reducing death rates. With the growing integration of\nartificial intelligence (AI) into medical imaging, the development and\nevaluation of robust AI models require access to large, well-annotated\ndatasets. In this study, we introduce the utility of Duke Lung Cancer Screening\n(DLCS) Dataset, the largest open-access LDCT dataset with over 2,000 scans and\n3,000 expert-verified nodules. We benchmark deep learning models for both 3D\nnodule detection and lung cancer classification across internal and external\ndatasets including LUNA16, LUNA25, and NLST-3D+. For detection, we develop two\nMONAI-based RetinaNet models (DLCSDmD and LUNA16-mD), evaluated using the\nCompetition Performance Metric (CPM). For classification, we compare five\nmodels, including state-of-the-art pretrained models (Models Genesis, Med3D), a\nselfsupervised foundation model (FMCB), a randomly initialized ResNet50, and\nproposed a novel Strategic Warm-Start++ (SWS++) model. SWS++ uses curated\ncandidate patches to pretrain a classification backbone within the same\ndetection pipeline, enabling task-relevant feature learning. Our models\ndemonstrated strong generalizability, with SWS++ achieving comparable or\nsuperior performance to existing foundational models across multiple datasets\n(AUC: 0.71 to 0.90). All code, models, and data are publicly released to\npromote reproducibility and collaboration. This work establishes a standardized\nbenchmarking resource for lung cancer AI research, supporting future efforts in\nmodel development, validation, and clinical translation.", "AI": {"tldr": "The paper introduces the Duke Lung Cancer Screening (DLCS) Dataset, a large open-access LDCT dataset, and benchmarks deep learning models for nodule detection and lung cancer classification, achieving strong generalizability.", "motivation": "Early detection of lung cancer via LDCT is crucial, and robust AI models require large, annotated datasets. The DLCS Dataset addresses this need.", "method": "Developed MONAI-based RetinaNet models for nodule detection and compared five models for classification, including the novel SWS++ model.", "result": "Models showed strong generalizability, with SWS++ performing comparably or better than existing models (AUC: 0.71 to 0.90).", "conclusion": "The work provides a standardized benchmarking resource for lung cancer AI research, promoting reproducibility and future advancements."}}
{"id": "2403.12533", "pdf": "https://arxiv.org/pdf/2403.12533", "abs": "https://arxiv.org/abs/2403.12533", "authors": ["Daniel Tanneberg", "Felix Ocker", "Stephan Hasler", "Joerg Deigmoeller", "Anna Belardinelli", "Chao Wang", "Heiko Wersing", "Bernhard Sendhoff", "Michael Gienger"], "title": "To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions", "categories": ["cs.RO", "cs.AI", "I.2.8; I.2.9"], "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024", "summary": "How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed.", "AI": {"tldr": "Attentive Support is a robot interaction concept using LLMs to provide unobtrusive physical support in human groups, balancing assistance and discretion.", "motivation": "To enable robots to support human groups without being disruptive, combining perception, reasoning, and behavior generation.", "method": "Integrates scene perception, dialogue, situation understanding, and LLM-based reasoning to decide when and how to assist.", "result": "Demonstrates effective attentive behavior in diverse scenarios, aiding humans when needed while avoiding unnecessary disturbance.", "conclusion": "Attentive Support successfully balances proactive assistance and discretion in group interactions."}}
{"id": "2504.17321", "pdf": "https://arxiv.org/pdf/2504.17321", "abs": "https://arxiv.org/abs/2504.17321", "authors": ["Michael J. Smith", "Luke Fleming", "James E. Geach", "Ryan J. Roberts", "Freddie Kalaitzis", "James Banister"], "title": "Dargana: fine-tuning EarthPT for dynamic tree canopy mapping from space", "categories": ["physics.geo-ph", "cs.LG"], "comment": "9 pages, 6 figures, spotlight at `Tackling Climate Change with\n  Machine Learning', ICLR 2025", "summary": "We present Dargana, a fine-tuned variant of the EarthPT time-series\nfoundation model that achieves specialisation using <3% of its pre-training\ndata volume and 5% of its pre-training compute. Dargana is fine-tuned to\ngenerate regularly updated classification of tree canopy cover at 10m\nresolution, distinguishing conifer and broadleaved tree types. Using Cornwall,\nUK, as a test case, the model achieves a pixel-level ROC-AUC of 0.98 and a\nPR-AUC of 0.83 on unseen satellite imagery. Dargana can identify fine\nstructures like hedgerows and coppice below the training sample limit, and can\ntrack temporal changes to canopy cover such as new woodland establishment. Our\nresults demonstrate how pre-trained Large Observation Models like EarthPT can\nbe specialised for granular, dynamic land cover monitoring from space,\nproviding a valuable, scalable tool for natural capital management and\nconservation.", "AI": {"tldr": "Dargana, a fine-tuned EarthPT variant, achieves high accuracy in tree canopy classification with minimal data and compute, demonstrating scalable land cover monitoring.", "motivation": "To specialize pre-trained models like EarthPT for granular, dynamic land cover monitoring, aiding natural capital management and conservation.", "method": "Fine-tuned EarthPT with <3% of pre-training data and 5% of compute to classify tree canopy cover at 10m resolution, distinguishing conifer and broadleaved types.", "result": "Achieved 0.98 ROC-AUC and 0.83 PR-AUC on unseen data, identifying fine structures and temporal changes like new woodland establishment.", "conclusion": "Dargana shows how pre-trained models can be specialized for scalable, high-resolution land cover monitoring."}}
{"id": "2504.11364", "pdf": "https://arxiv.org/pdf/2504.11364", "abs": "https://arxiv.org/abs/2504.11364", "authors": ["Tianwei Ni", "Allen Nie", "Sapana Chaudhary", "Yao Liu", "Huzefa Rangwala", "Rasool Fakoor"], "title": "Teaching Large Language Models to Reason through Learning and Forgetting", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code: https://github.com/twni2016/llm-reasoning-uft", "summary": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$.", "AI": {"tldr": "Fine-tuning large language models with successful and failed reasoning paths improves search capability and reduces inference time by 180x.", "motivation": "Inference-time search enhances model performance but increases computational costs and time.", "method": "Fine-tune the model using successful and failed reasoning paths with a smaller learning rate to prevent degradation.", "result": "Outperforms standard fine-tuning and inference-time search, reducing inference time significantly.", "conclusion": "Integrating search capabilities via fine-tuning is effective and efficient for complex reasoning tasks."}}
{"id": "2405.13901", "pdf": "https://arxiv.org/pdf/2405.13901", "abs": "https://arxiv.org/abs/2405.13901", "authors": ["Hongyi Pan", "Emadeldeen Hamdan", "Xin Zhu", "Ahmet Enis Cetin", "Ulas Bagci"], "title": "Discrete Cosine Transform Based Decorrelated Attention for Vision Transformers", "categories": ["cs.CV", "cs.LG", "eess.SP"], "comment": null, "summary": "Central to the Transformer architectures' effectiveness is the self-attention\nmechanism, a function that maps queries, keys, and values into a\nhigh-dimensional vector space. However, training the attention weights of\nqueries, keys, and values is non-trivial from a state of random initialization.\nIn this paper, we propose two methods. (i) We first address the initialization\nproblem of Vision Transformers by introducing a simple, yet highly innovative,\ninitialization approach utilizing discrete cosine transform (DCT) coefficients.\nOur proposed DCT-based \\textit{attention} initialization marks a significant\ngain compared to traditional initialization strategies; offering a robust\nfoundation for the attention mechanism. Our experiments reveal that the\nDCT-based initialization enhances the accuracy of Vision Transformers in\nclassification tasks. (ii) We also recognize that since DCT effectively\ndecorrelates image information in the frequency domain, this decorrelation is\nuseful for compression because it allows the quantization step to discard many\nof the higher-frequency components. Based on this observation, we propose a\nnovel DCT-based compression technique for the attention function of Vision\nTransformers. Since high-frequency DCT coefficients usually correspond to\nnoise, we truncate the high-frequency DCT components of the input patches. Our\nDCT-based compression reduces the size of weight matrices for queries, keys,\nand values. While maintaining the same level of accuracy, our DCT compressed\nSwin Transformers obtain a considerable decrease in the computational overhead.", "AI": {"tldr": "The paper introduces DCT-based initialization and compression for Vision Transformers, improving accuracy and reducing computational overhead.", "motivation": "Training self-attention weights from random initialization is challenging, and traditional methods may not be optimal.", "method": "Proposes DCT-based initialization for attention weights and DCT-based compression for attention function, truncating high-frequency components.", "result": "DCT initialization boosts Vision Transformers' accuracy; DCT compression reduces weight matrix size and computational cost without sacrificing accuracy.", "conclusion": "DCT-based techniques offer robust initialization and efficient compression for Vision Transformers, enhancing performance and reducing overhead."}}
{"id": "2405.12519", "pdf": "https://arxiv.org/pdf/2405.12519", "abs": "https://arxiv.org/abs/2405.12519", "authors": ["Zhaoning Yu", "Hongyang Gao"], "title": "MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "arXiv admin note: text overlap with arXiv:2405.08419 The Thirteenth\n  International Conference on Learning Representations 2025", "summary": "Graph Neural Networks (GNNs) have shown remarkable success in molecular\ntasks, yet their interpretability remains challenging. Traditional model-level\nexplanation methods like XGNN and GNNInterpreter often fail to identify valid\nsubstructures like rings, leading to questionable interpretability. This\nlimitation stems from XGNN's atom-by-atom approach and GNNInterpreter's\nreliance on average graph embeddings, which overlook the essential structural\nelements crucial for molecules. To address these gaps, we introduce an\ninnovative \\textbf{M}otif-b\\textbf{A}sed \\textbf{G}NN \\textbf{E}xplainer (MAGE)\nthat uses motifs as fundamental units for generating explanations. Our approach\nbegins with extracting potential motifs through a motif decomposition\ntechnique. Then, we utilize an attention-based learning method to identify\nclass-specific motifs. Finally, we employ a motif-based graph generator for\neach class to create molecular graph explanations based on these class-specific\nmotifs. This novel method not only incorporates critical substructures into the\nexplanations but also guarantees their validity, yielding results that are\nhuman-understandable. Our proposed method's effectiveness is demonstrated\nthrough quantitative and qualitative assessments conducted on six real-world\nmolecular datasets.", "AI": {"tldr": "MAGE introduces a motif-based GNN explainer to improve interpretability by focusing on valid substructures like rings, addressing limitations of traditional methods.", "motivation": "Traditional GNN explanation methods fail to identify valid substructures, leading to questionable interpretability in molecular tasks.", "method": "MAGE extracts motifs via decomposition, identifies class-specific motifs with attention-based learning, and generates explanations using a motif-based graph generator.", "result": "Demonstrated effectiveness on six molecular datasets, yielding human-understandable and valid explanations.", "conclusion": "MAGE improves GNN interpretability by incorporating critical substructures, ensuring validity and clarity in explanations."}}
{"id": "2504.17376", "pdf": "https://arxiv.org/pdf/2504.17376", "abs": "https://arxiv.org/abs/2504.17376", "authors": ["Maoyang Xiang", "Ramesh Fernando", "Bo Wang"], "title": "On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Transformer-based Large Language Models (LLMs) have significantly advanced AI\ncapabilities but pose considerable challenges for deployment on edge devices\ndue to high computational demands, memory bandwidth constraints, and energy\nconsumption. This paper addresses these challenges by presenting an efficient\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\nboth model compression rate and system throughput. Additionally, we propose a\nhybrid execution strategy that intelligently offloads compute-intensive\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\nbalancing the computational workload and maximizing overall performance. Our\nframework achieves a model compression rate of 55.08% compared to the original\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\nbaseline performance of 2.8 tokens per second.", "AI": {"tldr": "The paper presents an efficient framework for deploying the Qwen2.5-0.5B LLM on edge devices using FPGA-accelerated pipelines and hybrid execution, achieving 55.08% compression and 5.1 tokens/sec throughput.", "motivation": "Transformer-based LLMs face deployment challenges on edge devices due to high computational demands and resource constraints.", "method": "Uses Activation-aware Weight Quantization (AWQ) and a hybrid execution strategy offloading compute-intensive tasks to FPGA while using CPU for lighter tasks.", "result": "Achieves 55.08% model compression and 5.1 tokens/sec throughput, outperforming the baseline of 2.8 tokens/sec.", "conclusion": "The proposed framework effectively balances workload and enhances performance for LLM deployment on edge platforms."}}
{"id": "2407.21266", "pdf": "https://arxiv.org/pdf/2407.21266", "abs": "https://arxiv.org/abs/2407.21266", "authors": ["Corn\u00e9 Verburg", "Alexander Heinlein", "Eric C. Cyr"], "title": "DDU-Net: A Domain Decomposition-Based CNN for High-Resolution Image Segmentation on Multiple GPUs", "categories": ["cs.CV", "cs.DC", "cs.LG", "68T07, 68W10, 68W15, 65N55, 68U10", "I.2.6; I.4.6"], "comment": null, "summary": "The segmentation of ultra-high resolution images poses challenges such as\nloss of spatial information or computational inefficiency. In this work, a\nnovel approach that combines encoder-decoder architectures with domain\ndecomposition strategies to address these challenges is proposed. Specifically,\na domain decomposition-based U-Net (DDU-Net) architecture is introduced, which\npartitions input images into non-overlapping patches that can be processed\nindependently on separate devices. A communication network is added to\nfacilitate inter-patch information exchange to enhance the understanding of\nspatial context. Experimental validation is performed on a synthetic dataset\nthat is designed to measure the effectiveness of the communication network.\nThen, the performance is tested on the DeepGlobe land cover classification\ndataset as a real-world benchmark data set. The results demonstrate that the\napproach, which includes inter-patch communication for images divided into\n$16\\times16$ non-overlapping subimages, achieves a $2-3\\,\\%$ higher\nintersection over union (IoU) score compared to the same network without\ninter-patch communication. The performance of the network which includes\ncommunication is equivalent to that of a baseline U-Net trained on the full\nimage, showing that our model provides an effective solution for segmenting\nultra-high-resolution images while preserving spatial context. The code is\navailable at https://github.com/corne00/DDU-Net.", "AI": {"tldr": "A novel DDU-Net architecture combines encoder-decoder with domain decomposition to segment ultra-high-resolution images efficiently, achieving higher IoU scores by enabling inter-patch communication.", "motivation": "Addressing challenges like spatial information loss and computational inefficiency in ultra-high-resolution image segmentation.", "method": "Proposes DDU-Net, partitioning images into non-overlapping patches processed independently, with a communication network for spatial context.", "result": "Achieves 2-3% higher IoU than no-communication networks and matches baseline U-Net performance.", "conclusion": "DDU-Net effectively segments ultra-high-resolution images while preserving spatial context."}}
{"id": "2406.04724", "pdf": "https://arxiv.org/pdf/2406.04724", "abs": "https://arxiv.org/abs/2406.04724", "authors": ["Roman Belaire", "Arunesh Sinha", "Pradeep Varakantham"], "title": "On Minimizing Adversarial Counterfactual Error in Adversarial RL", "categories": ["cs.LG", "cs.AI"], "comment": "Presented at ICLR 2025", "summary": "Deep Reinforcement Learning (DRL) policies are highly susceptible to\nadversarial noise in observations, which poses significant risks in\nsafety-critical scenarios. The challenge inherent to adversarial perturbations\nis that by altering the information observed by the agent, the state becomes\nonly partially observable. Existing approaches address this by either enforcing\nconsistent actions across nearby states or maximizing the worst-case value\nwithin adversarially perturbed observations. However, the former suffers from\nperformance degradation when attacks succeed, while the latter tends to be\noverly conservative, leading to suboptimal performance in benign settings. We\nhypothesize that these limitations stem from their failing to account for\npartial observability directly. To this end, we introduce a novel objective\ncalled Adversarial Counterfactual Error (ACoE), defined on the beliefs about\nthe true state and balancing value optimization with robustness. To make ACoE\nscalable in model-free settings, we propose the theoretically-grounded\nsurrogate objective Cumulative-ACoE (C-ACoE). Our empirical evaluations on\nstandard benchmarks (MuJoCo, Atari, and Highway) demonstrate that our method\nsignificantly outperforms current state-of-the-art approaches for addressing\nadversarial RL challenges, offering a promising direction for improving\nrobustness in DRL under adversarial conditions. Our code is available at\nhttps://github.com/romanbelaire/acoe-robust-rl.", "AI": {"tldr": "The paper introduces Adversarial Counterfactual Error (ACoE) to improve robustness in Deep Reinforcement Learning (DRL) against adversarial noise, outperforming existing methods.", "motivation": "DRL policies are vulnerable to adversarial noise, risking safety-critical scenarios. Current methods either degrade performance or are overly conservative.", "method": "Proposes ACoE, a novel objective balancing value optimization and robustness, and its scalable surrogate C-ACoE for model-free settings.", "result": "Empirical evaluations on MuJoCo, Atari, and Highway benchmarks show superior performance over state-of-the-art methods.", "conclusion": "ACoE offers a promising direction for robust DRL under adversarial conditions."}}
{"id": "2504.17420", "pdf": "https://arxiv.org/pdf/2504.17420", "abs": "https://arxiv.org/abs/2504.17420", "authors": ["Louisa Pawusch", "Stefania Scheurer", "Wolfgang Nowak", "Reed Maxwell"], "title": "HydroStartML: A combined machine learning and physics-based approach to reduce hydrological model spin-up time", "categories": ["physics.geo-ph", "cs.LG"], "comment": "13 pages, 14 figures. To be published in Advances in Water Resources", "summary": "Finding the initial depth-to-water table (DTWT) configuration of a catchment\nis a critical challenge when simulating the hydrological cycle with integrated\nmodels, significantly impacting simulation outcomes. Traditionally, this\ninvolves iterative spin-up computations, where the model runs under constant\natmospheric settings until steady-state is achieved. These so-called model\nspin-ups are computationally expensive, often requiring many years of simulated\ntime, particularly when the initial DTWT configuration is far from steady\nstate.\n  To accelerate the model spin-up process we developed HydroStartML, a machine\nlearning emulator trained on steady-state DTWT configurations across the\ncontiguous United States. HydroStartML predicts, based on available data like\nconductivity and surface slopes, a DTWT configuration of the respective\nwatershed, which can be used as an initial DTWT.\n  Our results show that initializing spin-up computations with HydroStartML\npredictions leads to faster convergence than with other initial configurations\nlike spatially constant DTWTs. The emulator accurately predicts configurations\nclose to steady state, even for terrain configurations not seen in training,\nand allows especially significant reductions in computational spin-up effort in\nregions with deep DTWTs. This work opens the door for hybrid approaches that\nblend machine learning and traditional simulation, enhancing predictive\naccuracy and efficiency in hydrology for improving water resource management\nand understanding complex environmental interactions.", "AI": {"tldr": "HydroStartML, a machine learning emulator, accelerates hydrological model spin-up by predicting initial depth-to-water table (DTWT) configurations, reducing computational effort and improving efficiency.", "motivation": "Traditional iterative spin-up computations for DTWT configurations are computationally expensive, especially when far from steady state.", "method": "Developed HydroStartML, trained on steady-state DTWT data across the U.S., to predict initial DTWT configurations using features like conductivity and surface slopes.", "result": "HydroStartML predictions lead to faster convergence than other initial configurations, even for unseen terrains, and significantly reduce computational effort in regions with deep DTWTs.", "conclusion": "Hybrid approaches combining machine learning and traditional simulation can enhance hydrological modeling efficiency and accuracy, benefiting water resource management."}}
{"id": "2408.02657", "pdf": "https://arxiv.org/pdf/2408.02657", "abs": "https://arxiv.org/abs/2408.02657", "authors": ["Dongyang Liu", "Shitian Zhao", "Le Zhuo", "Weifeng Lin", "Yi Xin", "Xinyue Li", "Qi Qin", "Yu Qiao", "Hongsheng Li", "Peng Gao"], "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining", "categories": ["cs.CV"], "comment": "Code available at: https://github.com/Alpha-VLLM/Lumina-mGPT", "summary": "We present Lumina-mGPT, a family of multimodal autoregressive models capable\nof various vision and language tasks, particularly excelling in generating\nflexible photorealistic images from text descriptions. By initializing from\nmultimodal Generative PreTraining (mGPT), we demonstrate that decoder-only\nAutoregressive (AR) model can achieve image generation performance comparable\nto modern diffusion models with high efficiency through Flexible Progressive\nSupervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image\nRepresentation (UniRep), Lumina-mGPT can flexibly generate high-quality images\nof varying aspect ratios. Building on the strong image generation capabilities,\nwe further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial\nattempt to elevate Lumina-mGPT into a unified multi-modal generalist. The\nresulting model demonstrates versatile multimodal capabilities, including\nvisual generation tasks like text-to-image/multiview generation and\ncontrollable generation, visual recognition tasks like segmentation and depth\nestimation, and vision-language tasks like multi-turn visual question\nanswering, showing the rosy potential of the technical direction. Codes and\ncheckpoints are available at https://github.com/Alpha-VLLM/Lumina-mGPT.", "AI": {"tldr": "Lumina-mGPT is a multimodal autoregressive model excelling in text-to-image generation and versatile vision-language tasks, outperforming diffusion models with high efficiency.", "motivation": "To demonstrate that decoder-only autoregressive models can achieve high-quality image generation and versatile multimodal capabilities comparable to modern diffusion models.", "method": "Uses Flexible Progressive Supervised Fine-tuning (FP-SFT) and Unambiguous image Representation (UniRep) for flexible image generation. Explores Omniponent Supervised Fine-tuning (Omni-SFT) for unified multimodal tasks.", "result": "Achieves photorealistic image generation, supports varying aspect ratios, and performs well in visual recognition and vision-language tasks.", "conclusion": "Lumina-mGPT shows strong potential as a unified multimodal generalist, with promising results in generation, recognition, and vision-language tasks."}}
{"id": "2407.14306", "pdf": "https://arxiv.org/pdf/2407.14306", "abs": "https://arxiv.org/abs/2407.14306", "authors": ["Daniel Bogdoll", "Finn Sartoris", "Vincent Geppert", "Svetlana Pavlitska", "J. Marius Z\u00f6llner"], "title": "Label-Free Model Failure Detection for Lidar-based Point Cloud Segmentation", "categories": ["cs.RO", "cs.AI"], "comment": "Daniel Bogdoll, Finn Sartoris, and Vincent Geppert contributed\n  equally. Accepted for publication at IV 2025", "summary": "Autonomous vehicles drive millions of miles on the road each year. Under such\ncircumstances, deployed machine learning models are prone to failure both in\nseemingly normal situations and in the presence of outliers. However, in the\ntraining phase, they are only evaluated on small validation and test sets,\nwhich are unable to reveal model failures due to their limited scenario\ncoverage. While it is difficult and expensive to acquire large and\nrepresentative labeled datasets for evaluation, large-scale unlabeled datasets\nare typically available. In this work, we introduce label-free model failure\ndetection for lidar-based point cloud segmentation, taking advantage of the\nabundance of unlabeled data available. We leverage different data\ncharacteristics by training a supervised and self-supervised stream for the\nsame task to detect failure modes. We perform a large-scale qualitative\nanalysis and present LidarCODA, the first publicly available dataset with\nlabeled anomalies in real-world lidar data, for an extensive quantitative\nanalysis.", "AI": {"tldr": "The paper introduces a label-free method for detecting model failures in lidar-based point cloud segmentation using unlabeled data, addressing the limitations of small labeled datasets.", "motivation": "Autonomous vehicles rely on machine learning models prone to failures, but traditional small labeled datasets lack scenario coverage to detect these failures. Large unlabeled datasets are more accessible.", "method": "The approach combines supervised and self-supervised learning streams to leverage unlabeled data for failure detection.", "result": "The paper presents LidarCODA, the first public dataset with labeled anomalies in real-world lidar data, enabling extensive analysis.", "conclusion": "The proposed method effectively detects model failures using unlabeled data, offering a scalable solution for improving autonomous vehicle safety."}}
{"id": "2504.17529", "pdf": "https://arxiv.org/pdf/2504.17529", "abs": "https://arxiv.org/abs/2504.17529", "authors": ["Youngjune Lee", "Haeyu Jeong", "Changgeon Lim", "Jeong Choi", "Hongjun Lim", "Hangon Kim", "Jiyoon Kwon", "Saehun Kim"], "title": "IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted to SIGIR 2025 Industry Track. First two authors contributed\n  equally", "summary": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform.", "AI": {"tldr": "The IRA framework dynamically adapts to evolving user interests and new documents in online communities, using Interest Units and semantic retrieval for personalized recommendations.", "motivation": "Addressing the challenge of real-time adaptation to changing user interests and new content in large-scale industrial settings.", "method": "IRA uses Interest Units to capture evolving user interests and a semantic retrieval process to measure relevance, avoiding click signal biases.", "result": "IRA effectively adapts to user preferences, validated through experiments and deployment in NAVER's CAFE platform.", "conclusion": "IRA offers a scalable, efficient solution for dynamic personalization in online communities."}}
{"id": "2408.03404", "pdf": "https://arxiv.org/pdf/2408.03404", "abs": "https://arxiv.org/abs/2408.03404", "authors": ["Athanasios Efthymiou", "Stevan Rudinac", "Monika Kackovic", "Nachoem Wijnberg", "Marcel Worring"], "title": "Set2Seq Transformer: Temporal and Positional-Aware Set Representations for Sequential Multiple-Instance Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Sequential multiple-instance learning involves learning representations of\nsets distributed across discrete timesteps. In many real-world applications,\nmodeling both the internal structure of sets and their temporal relationships\nacross time is essential for capturing complex underlying patterns. However,\nexisting methods either focus on learning set representations at a static\nlevel, ignoring temporal dynamics, or treat sequences as ordered lists of\nindividual elements, lacking explicit mechanisms to represent sets. In this\nwork, we propose Set2Seq Transformer, a novel architecture that jointly models\npermutation-invariant set structure and temporal dependencies by learning\ntemporal and positional-aware representations of sets within a sequence in an\nend-to-end multimodal manner. We evaluate our Set2Seq Transformer on two tasks\nthat require modeling both set structure alongside temporal and positional\npatterns, but differ significantly in domain, modality, and objective. First,\nwe consider a fine-art analysis task, modeling artists' oeuvres for predicting\nartistic success using a novel dataset, WikiArt-Seq2Rank. Second, we utilize\nour Set2Seq Transformer for a short-term wildfire danger forecasting task.\nThrough extensive experimentation, we show that our Set2Seq Transformer\nsignificantly improves over traditional static multiple-instance learning\nmethods by effectively learning permutation-invariant set, temporal, and\npositional-aware representations across diverse domains, modalities, and tasks.\nWe will release both the dataset and model implementations on GitHub.", "AI": {"tldr": "The paper introduces Set2Seq Transformer, a novel architecture for sequential multiple-instance learning that jointly models set structure and temporal dependencies, outperforming static methods in diverse tasks like art analysis and wildfire forecasting.", "motivation": "Existing methods either ignore temporal dynamics or lack set representation mechanisms, limiting their ability to capture complex patterns in sequential set data.", "method": "The proposed Set2Seq Transformer learns temporal and positional-aware set representations in an end-to-end multimodal manner.", "result": "The model significantly improves over static methods, demonstrating effectiveness in tasks like art success prediction and wildfire danger forecasting.", "conclusion": "Set2Seq Transformer successfully integrates set structure and temporal dynamics, showing broad applicability across domains and tasks."}}
{"id": "2408.14837", "pdf": "https://arxiv.org/pdf/2408.14837", "abs": "https://arxiv.org/abs/2408.14837", "authors": ["Dani Valevski", "Yaniv Leviathan", "Moab Arar", "Shlomi Fruchter"], "title": "Diffusion Models Are Real-Time Game Engines", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICLR 2025. Project page: https://gamengen.github.io/", "summary": "We present GameNGen, the first game engine powered entirely by a neural model\nthat also enables real-time interaction with a complex environment over long\ntrajectories at high quality. When trained on the classic game DOOM, GameNGen\nextracts gameplay and uses it to generate a playable environment that can\ninteractively simulate new trajectories. GameNGen runs at 20 frames per second\non a single TPU and remains stable over extended multi-minute play sessions.\nNext frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG\ncompression. Human raters are only slightly better than random chance at\ndistinguishing short clips of the game from clips of the simulation, even after\n5 minutes of auto-regressive generation. GameNGen is trained in two phases: (1)\nan RL-agent learns to play the game and the training sessions are recorded, and\n(2) a diffusion model is trained to produce the next frame, conditioned on the\nsequence of past frames and actions. Conditioning augmentations help ensure\nstable auto-regressive generation over long trajectories, and decoder\nfine-tuning improves the fidelity of visual details and text.", "AI": {"tldr": "GameNGen is a neural-powered game engine enabling real-time interaction with high-quality, stable gameplay, trained on DOOM and achieving near-human-indistinguishable results.", "motivation": "To create a game engine entirely powered by neural models, capable of real-time interaction and high-quality, stable gameplay over long trajectories.", "method": "Trained in two phases: (1) an RL-agent learns and records gameplay, and (2) a diffusion model generates next frames conditioned on past frames and actions, with conditioning augmentations and decoder fine-tuning for stability and fidelity.", "result": "Achieves 20 FPS on a single TPU, PSNR of 29.4, and human raters struggle to distinguish real clips from generated ones even after 5 minutes.", "conclusion": "GameNGen demonstrates the feasibility of neural-powered game engines for real-time, high-quality gameplay simulation."}}
{"id": "2504.17546", "pdf": "https://arxiv.org/pdf/2504.17546", "abs": "https://arxiv.org/abs/2504.17546", "authors": ["Wouter van Loon"], "title": "An introduction to R package `mvs`", "categories": ["stat.CO", "cs.LG", "stat.ME", "stat.ML"], "comment": "15 pages, 4 figures. Package vignette corresponding to\n  https://doi.org/10.32614/CRAN.package.mvs", "summary": "In biomedical science, a set of objects or persons can often be described by\nmultiple distinct sets of features obtained from different data sources or\nmodalities (called \"multi-view data\"). Classical machine learning methods\nignore the multi-view structure of such data, limiting model interpretability\nand performance. The R package `mvs` provides methods that were designed\nspecifically for dealing with multi-view data, based on the multi-view stacking\n(MVS) framework. MVS is a form of supervised (machine) learning used to train\nmulti-view classification or prediction models. MVS works by training a\nlearning algorithm on each view separately, estimating the predictive power of\neach view-specific model through cross-validation, and then using another\nlearning algorithm to assign weights to the view-specific models based on their\nestimated predictions. MVS is a form of ensemble learning, dividing the large\nmulti-view learning problem into smaller sub-problems. Most of these\nsub-problems can be solved in parallel, making it computationally attractive.\nAdditionally, the number of features of the sub-problems is greatly reduced\ncompared with the full multi-view learning problem. This makes MVS especially\nuseful when the total number of features is larger than the number of\nobservations (i.e., high-dimensional data). MVS can still be applied even if\nthe sub-problems are themselves high-dimensional by adding suitable penalty\nterms to the learning algorithms. Furthermore, MVS can be used to automatically\nselect the views which are most important for prediction. The R package `mvs`\nmakes fitting MVS models, including such penalty terms, easily and openly\naccessible. `mvs` allows for the fitting of stacked models with any number of\nlevels, with different penalty terms, different outcome distributions, and\nprovides several options for missing data handling.", "AI": {"tldr": "The paper introduces the R package `mvs` for multi-view data analysis using the Multi-View Stacking (MVS) framework, improving interpretability and performance over classical methods.", "motivation": "Classical machine learning methods fail to leverage multi-view data structure, limiting interpretability and performance. The `mvs` package addresses this gap.", "method": "MVS trains separate models on each data view, estimates their predictive power via cross-validation, and combines them using weighted ensemble learning. It supports parallel processing and handles high-dimensional data with penalty terms.", "result": "The `mvs` package efficiently handles multi-view data, reduces feature dimensionality, and allows automatic selection of important views, making it computationally attractive.", "conclusion": "The `mvs` package provides a scalable, interpretable, and flexible solution for multi-view data analysis, suitable for high-dimensional datasets."}}
{"id": "2408.03624", "pdf": "https://arxiv.org/pdf/2408.03624", "abs": "https://arxiv.org/abs/2408.03624", "authors": ["Senkang Hu", "Zhengru Fang", "Zihan Fang", "Yiqin Deng", "Xianhao Chen", "Yuguang Fang", "Sam Kwong"], "title": "AgentsCoMerge: Large Language Model Empowered Collaborative Decision Making for Ramp Merging", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Mobile Computing (TMC)", "summary": "Ramp merging is one of the bottlenecks in traffic systems, which commonly\ncause traffic congestion, accidents, and severe carbon emissions. In order to\naddress this essential issue and enhance the safety and efficiency of connected\nand autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel\ncollaborative decision-making framework, named AgentsCoMerge, to leverage large\nlanguage models (LLMs). Specifically, we first design a scene observation and\nunderstanding module to allow an agent to capture the traffic environment. Then\nwe propose a hierarchical planning module to enable the agent to make decisions\nand plan trajectories based on the observation and the agent's own state. In\naddition, in order to facilitate collaboration among multiple agents, we\nintroduce a communication module to enable the surrounding agents to exchange\nnecessary information and coordinate their actions. Finally, we develop a\nreinforcement reflection guided training paradigm to further enhance the\ndecision-making capability of the framework. Extensive experiments are\nconducted to evaluate the performance of our proposed method, demonstrating its\nsuperior efficiency and effectiveness for multi-agent collaborative\ndecision-making under various ramp merging scenarios.", "AI": {"tldr": "Proposes AgentsCoMerge, a collaborative decision-making framework using LLMs for CAVs to improve safety and efficiency in ramp merging.", "motivation": "Address traffic congestion, accidents, and emissions caused by ramp merging in CAV systems.", "method": "Combines scene observation, hierarchical planning, communication modules, and reinforcement reflection training.", "result": "Demonstrates superior efficiency and effectiveness in multi-agent collaborative decision-making for ramp merging.", "conclusion": "AgentsCoMerge enhances CAV performance in merging scenarios, offering a scalable solution for traffic bottlenecks."}}
{"id": "2409.02313", "pdf": "https://arxiv.org/pdf/2409.02313", "abs": "https://arxiv.org/abs/2409.02313", "authors": ["Ricardo Buitrago Ruiz", "Tanya Marwah", "Albert Gu", "Andrej Risteski"], "title": "On the Benefits of Memory for Modeling Time-Dependent PDEs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data-driven techniques have emerged as a promising alternative to traditional\nnumerical methods for solving PDEs. For time-dependent PDEs, many approaches\nare Markovian -- the evolution of the trained system only depends on the\ncurrent state, and not the past states. In this work, we investigate the\nbenefits of using memory for modeling time-dependent PDEs: that is, when past\nstates are explicitly used to predict the future. Motivated by the Mori-Zwanzig\ntheory of model reduction, we theoretically exhibit examples of simple (even\nlinear) PDEs, in which a solution that uses memory is arbitrarily better than a\nMarkovian solution. Additionally, we introduce Memory Neural Operator (MemNO),\na neural operator architecture that combines recent state space models\n(specifically, S4) and Fourier Neural Operators (FNOs) to effectively model\nmemory. We empirically demonstrate that when the PDEs are supplied in low\nresolution or contain observation noise at train and test time, MemNO\nsignificantly outperforms the baselines without memory -- with up to 6x\nreduction in test error. Furthermore, we show that this benefit is particularly\npronounced when the PDE solutions have significant high-frequency Fourier modes\n(e.g., low-viscosity fluid dynamics) and we construct a challenging benchmark\ndataset consisting of such PDEs.", "AI": {"tldr": "The paper explores using memory (past states) to improve predictions for time-dependent PDEs, introducing Memory Neural Operator (MemNO) which outperforms Markovian methods, especially in low-resolution or noisy scenarios.", "motivation": "To address the limitations of Markovian approaches in solving time-dependent PDEs by leveraging past states for better predictions, inspired by the Mori-Zwanzig theory.", "method": "Introduces MemNO, combining state space models (S4) and Fourier Neural Operators (FNOs) to model memory effectively.", "result": "MemNO reduces test error by up to 6x compared to Markovian baselines, particularly for PDEs with high-frequency modes or noise.", "conclusion": "Memory-based approaches like MemNO significantly enhance PDE solutions, especially in challenging scenarios like low resolution or noise."}}
{"id": "2504.17548", "pdf": "https://arxiv.org/pdf/2504.17548", "abs": "https://arxiv.org/abs/2504.17548", "authors": ["Kilian Tscharke", "Maximilian Wendlinger", "Afrae Ahouzi", "Pallavi Bhardwaj", "Kaweh Amoi-Taleghani", "Michael Schr\u00f6dl-Baumann", "Pascal Debus"], "title": "Quantum Autoencoder for Multivariate Time Series Anomaly Detection", "categories": ["quant-ph", "cs.CR", "cs.LG"], "comment": "Submitted to IEEE International Conference on Quantum Computing and\n  Engineering (QCE) 2025", "summary": "Anomaly Detection (AD) defines the task of identifying observations or events\nthat deviate from typical - or normal - patterns, a critical capability in IT\nsecurity for recognizing incidents such as system misconfigurations, malware\ninfections, or cyberattacks. In enterprise environments like SAP HANA Cloud\nsystems, this task often involves monitoring high-dimensional, multivariate\ntime series (MTS) derived from telemetry and log data. With the advent of\nquantum machine learning offering efficient calculations in high-dimensional\nlatent spaces, many avenues open for dealing with such complex data. One\napproach is the Quantum Autoencoder (QAE), an emerging and promising method\nwith potential for application in both data compression and AD. However, prior\napplications of QAEs to time series AD have been restricted to univariate data,\nlimiting their relevance for real-world enterprise systems. In this work, we\nintroduce a novel QAE-based framework designed specifically for MTS AD towards\nenterprise scale. We theoretically develop and experimentally validate the\narchitecture, demonstrating that our QAE achieves performance competitive with\nneural-network-based autoencoders while requiring fewer trainable parameters.\nWe evaluate our model on datasets that closely reflect SAP system telemetry and\nshow that the proposed QAE is a viable and efficient alternative for\nsemisupervised AD in real-world enterprise settings.", "AI": {"tldr": "A novel Quantum Autoencoder (QAE) framework is introduced for multivariate time series anomaly detection in enterprise systems, showing competitive performance with fewer parameters than neural-network-based autoencoders.", "motivation": "Anomaly detection in high-dimensional, multivariate time series is critical for IT security in enterprise environments like SAP HANA Cloud, but existing QAE methods are limited to univariate data.", "method": "The paper proposes a QAE-based framework specifically designed for multivariate time series anomaly detection, theoretically developing and experimentally validating the architecture.", "result": "The QAE achieves performance comparable to neural-network-based autoencoders with fewer trainable parameters, validated on datasets resembling SAP system telemetry.", "conclusion": "The proposed QAE is a viable and efficient alternative for semisupervised anomaly detection in real-world enterprise settings."}}
{"id": "2408.16965", "pdf": "https://arxiv.org/pdf/2408.16965", "abs": "https://arxiv.org/abs/2408.16965", "authors": ["Dewen Zeng", "Yawen Wu", "Xinrong Hu", "Xiaowei Xu", "Yiyu Shi"], "title": "Contrastive Learning with Synthetic Positives", "categories": ["cs.CV"], "comment": "8 pages, conference", "summary": "Contrastive learning with the nearest neighbor has proved to be one of the\nmost efficient self-supervised learning (SSL) techniques by utilizing the\nsimilarity of multiple instances within the same class. However, its efficacy\nis constrained as the nearest neighbor algorithm primarily identifies \"easy\"\npositive pairs, where the representations are already closely located in the\nembedding space. In this paper, we introduce a novel approach called\nContrastive Learning with Synthetic Positives (CLSP) that utilizes synthetic\nimages, generated by an unconditional diffusion model, as the additional\npositives to help the model learn from diverse positives. Through feature\ninterpolation in the diffusion model sampling process, we generate images with\ndistinct backgrounds yet similar semantic content to the anchor image. These\nimages are considered \"hard\" positives for the anchor image, and when included\nas supplementary positives in the contrastive loss, they contribute to a\nperformance improvement of over 2% and 1% in linear evaluation compared to the\nprevious NNCLR and All4One methods across multiple benchmark datasets such as\nCIFAR10, achieving state-of-the-art methods. On transfer learning benchmarks,\nCLSP outperforms existing SSL frameworks on 6 out of 8 downstream datasets. We\nbelieve CLSP establishes a valuable baseline for future SSL studies\nincorporating synthetic data in the training process.", "AI": {"tldr": "CLSP introduces synthetic positives via a diffusion model to improve contrastive learning, outperforming NNCLR and All4One by 2% and 1% in benchmarks.", "motivation": "Nearest neighbor contrastive learning lacks diversity in positives, limiting efficacy.", "method": "Uses synthetic images from a diffusion model as hard positives in contrastive loss.", "result": "Achieves SOTA performance, improving by 2% and 1% over NNCLR and All4One, and excels in transfer learning.", "conclusion": "CLSP sets a baseline for SSL with synthetic data, enhancing model performance."}}
{"id": "2410.01131", "pdf": "https://arxiv.org/pdf/2410.01131", "abs": "https://arxiv.org/abs/2410.01131", "authors": ["Ilya Loshchilov", "Cheng-Ping Hsieh", "Simeng Sun", "Boris Ginsburg"], "title": "nGPT: Normalized Transformer with Representation Learning on the Hypersphere", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a novel neural network architecture, the normalized Transformer\n(nGPT) with representation learning on the hypersphere. In nGPT, all vectors\nforming the embeddings, MLP, attention matrices and hidden states are unit norm\nnormalized. The input stream of tokens travels on the surface of a hypersphere,\nwith each layer contributing a displacement towards the target output\npredictions. These displacements are defined by the MLP and attention blocks,\nwhose vector components also reside on the same hypersphere. Experiments show\nthat nGPT learns much faster, reducing the number of training steps required to\nachieve the same accuracy by a factor of 4 to 20, depending on the sequence\nlength.", "AI": {"tldr": "The paper introduces nGPT, a normalized Transformer with hypersphere-based representation learning, showing faster training and improved efficiency.", "motivation": "To enhance training efficiency and accuracy in neural networks by normalizing vectors and leveraging hypersphere geometry.", "method": "Proposes nGPT, where embeddings, MLP, attention matrices, and hidden states are unit norm normalized, operating on a hypersphere.", "result": "nGPT reduces training steps by 4-20x while maintaining accuracy, depending on sequence length.", "conclusion": "Normalization and hypersphere-based learning in nGPT significantly improve training efficiency without compromising performance."}}
{"id": "2504.17584", "pdf": "https://arxiv.org/pdf/2504.17584", "abs": "https://arxiv.org/abs/2504.17584", "authors": ["Qingyuan Liu", "Liyan Chen", "Yanning Yang", "Haocheng Wang", "Dong Du", "Zhigang Mao", "Naifeng Jing", "Yubin Xia", "Haibo Chen"], "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference", "categories": ["cs.AR", "cs.LG"], "comment": "16 pages, 11 figures", "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.", "AI": {"tldr": "L3 is a hardware-software co-designed system using DIMM-PIM and GPU to optimize LLM inference, addressing memory bottlenecks in multi-head attention decoding. It achieves 6.1\u00d7 speedup over HBM-PIM solutions.", "motivation": "GPU memory limitations create trade-offs between capacity and bandwidth for LLMs, especially in multi-head attention decoding. DIMM-PIM architectures offer potential but require optimization.", "method": "L3 integrates DIMM-PIM and GPU with hardware redesigns, communication optimizations, and an adaptive scheduler to enhance parallelism and hide transfer overhead.", "result": "L3 achieves up to 6.1\u00d7 speedup over HBM-PIM solutions and improves batch sizes.", "conclusion": "L3 demonstrates effective hardware-software co-design for scalable and efficient LLM inference, leveraging DIMM-PIM's strengths."}}
{"id": "2409.09451", "pdf": "https://arxiv.org/pdf/2409.09451", "abs": "https://arxiv.org/abs/2409.09451", "authors": ["Yi-Chia Chang", "Adam J. Stewart", "Favyen Bastani", "Piper Wolters", "Shreya Kannan", "George R. Huber", "Jingtong Wang", "Arindam Banerjee"], "title": "On the Generalizability of Foundation Models for Crop Type Mapping", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Foundation models pre-trained using self-supervised learning have shown\npowerful transfer learning capabilities on various downstream tasks, including\nlanguage understanding, text generation, and image recognition. The Earth\nobservation (EO) field has produced several foundation models pre-trained\ndirectly on multispectral satellite imagery for applications like precision\nagriculture, wildfire and drought monitoring, and natural disaster response.\nHowever, few studies have investigated the ability of these models to\ngeneralize to new geographic locations, and potential concerns of geospatial\nbias -- models trained on data-rich developed nations not transferring well to\ndata-scarce developing nations -- remain. We investigate the ability of popular\nEO foundation models to transfer to new geographic regions in the agricultural\ndomain, where differences in farming practices and class imbalance make\ntransfer learning particularly challenging. We first select five crop\nclassification datasets across five continents, normalizing for dataset size\nand harmonizing classes to focus on four major cereal grains: maize, soybean,\nrice, and wheat. We then compare three popular foundation models, pre-trained\non SSL4EO-S12, SatlasPretrain, and ImageNet, using in-distribution (ID) and\nout-of-distribution (OOD) evaluation. Experiments show that pre-trained weights\ndesigned explicitly for Sentinel-2, such as SSL4EO-S12, outperform general\npre-trained weights like ImageNet. Furthermore, while only 100 labeled images\nare sufficient for achieving high overall accuracy, 900 images are required to\nachieve high average accuracy due to class imbalance. All harmonized datasets\nand experimental code are open-source and available for download.", "AI": {"tldr": "The paper evaluates the generalization of Earth observation (EO) foundation models to new geographic regions, focusing on agricultural crop classification. It compares three models and highlights the superiority of Sentinel-2-specific pre-training, while addressing challenges like class imbalance.", "motivation": "To investigate the transferability of EO foundation models to new regions, especially in agriculture, and assess geospatial bias concerns.", "method": "Five crop classification datasets across continents were used, harmonizing classes for four cereal grains. Three foundation models (SSL4EO-S12, SatlasPretrain, ImageNet) were evaluated in-distribution and out-of-distribution.", "result": "Sentinel-2-specific pre-trained models (SSL4EO-S12) outperformed general models (ImageNet). Class imbalance required 900 labeled images for high average accuracy, while 100 sufficed for overall accuracy.", "conclusion": "Specialized pre-training for EO data improves transferability, but class imbalance remains a challenge. Open-source datasets and code support further research."}}
{"id": "2411.16206", "pdf": "https://arxiv.org/pdf/2411.16206", "abs": "https://arxiv.org/abs/2411.16206", "authors": ["Dawei Zhan", "Zhaoxi Zeng", "Shuoxiao Wei", "Ping Wu"], "title": "A Simple and Efficient Approach to Batch Bayesian Optimization", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Extending Bayesian optimization to batch evaluation can enable the designer\nto make the most use of parallel computing technology. However, most of current\nbatch approaches do not scale well with the batch size. That is, their\nperformances deteriorate dramatically as the batch size increases. To address\nthis issue, we propose a simple and efficient approach to extend Bayesian\noptimization to large-scale batch evaluation in this work. Different from\nexisting batch approaches, the idea of the new approach is to draw a batch of\naxis-aligned subspaces of the original problem and select one acquisition point\nfrom each subspace. To achieve this, we propose the expected subspace\nimprovement criterion to measure the amount of the improvement that a candidate\npoint can achieve within a certain axis-aligned subspace. By optimizing these\nexpected subspace improvement functions simultaneously, we can get a batch of\nquery points for parallel evaluation. Numerical experiments show that our\nproposed approach can speedup the convergence significantly when compared with\nthe sequential Bayesian optimization algorithm, and performs very competitively\nwhen compared with seven batch Bayesian optimization algorithms. A Matlab\nimplementation of the proposed approach is available at\nhttps://github.com/zhandawei/Expected_Subspace_Improvement_Batch_Bayesian_Optimization.", "AI": {"tldr": "A scalable batch Bayesian optimization method using axis-aligned subspaces and expected subspace improvement for parallel evaluation.", "motivation": "Current batch Bayesian optimization methods perform poorly with large batch sizes, limiting their use in parallel computing.", "method": "Proposes drawing batch subspaces and selecting acquisition points from each using the expected subspace improvement criterion.", "result": "Outperforms sequential Bayesian optimization and competes with seven batch methods, significantly speeding up convergence.", "conclusion": "The approach is simple, efficient, and scalable for large-scale batch Bayesian optimization."}}
{"id": "2504.17622", "pdf": "https://arxiv.org/pdf/2504.17622", "abs": "https://arxiv.org/abs/2504.17622", "authors": ["Chen Xu", "Qiang Wang", "Lijun Sun"], "title": "Likelihood-Free Variational Autoencoders", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Variational Autoencoders (VAEs) typically rely on a probabilistic decoder\nwith a predefined likelihood, most commonly an isotropic Gaussian, to model the\ndata conditional on latent variables. While convenient for optimization, this\nchoice often leads to likelihood misspecification, resulting in blurry\nreconstructions and poor data fidelity, especially for high-dimensional data\nsuch as images. In this work, we propose \\textit{EnVAE}, a novel\nlikelihood-free generative framework that has a deterministic decoder and\nemploys the energy score -- a proper scoring rule -- to build the\nreconstruction loss. This enables likelihood-free inference without requiring\nexplicit parametric density functions. To address the computational\ninefficiency of the energy score, we introduce a fast variant, \\textit{FEnVAE},\nbased on the local smoothness of the decoder and the sharpness of the posterior\ndistribution of latent variables. This yields an efficient single-sample\ntraining objective that integrates seamlessly into existing VAE pipelines with\nminimal overhead. Empirical results on standard benchmarks demonstrate that\n\\textit{EnVAE} achieves superior reconstruction and generation quality compared\nto likelihood-based baselines. Our framework offers a general, scalable, and\nstatistically principled alternative for flexible and nonparametric\ndistribution learning in generative modeling.", "AI": {"tldr": "EnVAE introduces a likelihood-free VAE framework using energy scores for reconstruction, avoiding blurry outputs from traditional VAEs. FEnVAE optimizes this with a fast variant for efficiency.", "motivation": "Traditional VAEs use predefined likelihoods (e.g., Gaussian), leading to blurry reconstructions and poor data fidelity, especially in high-dimensional data like images.", "method": "EnVAE replaces the probabilistic decoder with a deterministic one, using the energy score for reconstruction loss. FEnVAE introduces a fast variant for computational efficiency.", "result": "EnVAE outperforms likelihood-based baselines in reconstruction and generation quality on standard benchmarks.", "conclusion": "EnVAE provides a scalable, principled alternative for flexible, nonparametric distribution learning in generative modeling."}}
{"id": "2410.03058", "pdf": "https://arxiv.org/pdf/2410.03058", "abs": "https://arxiv.org/abs/2410.03058", "authors": ["Chen Liu", "Danqi Liao", "Alejandro Parada-Mayorga", "Alejandro Ribeiro", "Marcello DiStasio", "Smita Krishnaswamy"], "title": "DiffKillR: Killing and Recreating Diffeomorphisms for Cell Annotation in Dense Microscopy Images", "categories": ["cs.CV"], "comment": "ICASSP 2025, Oral Presentation", "summary": "The proliferation of digital microscopy images, driven by advances in\nautomated whole slide scanning, presents significant opportunities for\nbiomedical research and clinical diagnostics. However, accurately annotating\ndensely packed information in these images remains a major challenge. To\naddress this, we introduce DiffKillR, a novel framework that reframes cell\nannotation as the combination of archetype matching and image registration\ntasks. DiffKillR employs two complementary neural networks: one that learns a\ndiffeomorphism-invariant feature space for robust cell matching and another\nthat computes the precise warping field between cells for annotation mapping.\nUsing a small set of annotated archetypes, DiffKillR efficiently propagates\nannotations across large microscopy images, reducing the need for extensive\nmanual labeling. More importantly, it is suitable for any type of pixel-level\nannotation. We will discuss the theoretical properties of DiffKillR and\nvalidate it on three microscopy tasks, demonstrating its advantages over\nexisting supervised, semi-supervised, and unsupervised methods. The code is\navailable at https://github.com/KrishnaswamyLab/DiffKillR.", "AI": {"tldr": "DiffKillR is a framework combining archetype matching and image registration to automate cell annotation in microscopy images, reducing manual labeling.", "motivation": "The challenge of accurately annotating densely packed information in digital microscopy images drives the need for automated solutions.", "method": "DiffKillR uses two neural networks: one for diffeomorphism-invariant feature space learning and another for warping field computation to map annotations.", "result": "Validated on three microscopy tasks, DiffKillR outperforms existing supervised, semi-supervised, and unsupervised methods.", "conclusion": "DiffKillR efficiently propagates annotations with minimal manual input, suitable for any pixel-level annotation task."}}
{"id": "2411.16718", "pdf": "https://arxiv.org/pdf/2411.16718", "abs": "https://arxiv.org/abs/2411.16718", "authors": ["S. P. Sharan", "Minkyu Choi", "Sahil Shah", "Harsh Goel", "Mohammad Omama", "Sandeep Chinchali"], "title": "Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen,\nand CogVideoX are pushing the boundaries of synthetic video generation, with\nadoption seen in fields like robotics, autonomous driving, and entertainment.\nAs these models become prevalent, various metrics and benchmarks have emerged\nto evaluate the quality of the generated videos. However, these metrics\nemphasize visual quality and smoothness, neglecting temporal fidelity and\ntext-to-video alignment, which are crucial for safety-critical applications. To\naddress this gap, we introduce NeuS-V, a novel synthetic video evaluation\nmetric that rigorously assesses text-to-video alignment using neuro-symbolic\nformal verification techniques. Our approach first converts the prompt into a\nformally defined Temporal Logic (TL) specification and translates the generated\nvideo into an automaton representation. Then, it evaluates the text-to-video\nalignment by formally checking the video automaton against the TL\nspecification. Furthermore, we present a dataset of temporally extended prompts\nto evaluate state-of-the-art video generation models against our benchmark. We\nfind that NeuS-V demonstrates a higher correlation by over 5x with human\nevaluations when compared to existing metrics. Our evaluation further reveals\nthat current video generation models perform poorly on these temporally complex\nprompts, highlighting the need for future work in improving text-to-video\ngeneration capabilities.", "AI": {"tldr": "NeuS-V introduces a neuro-symbolic metric for evaluating text-to-video alignment, outperforming existing metrics by 5x in human correlation.", "motivation": "Current metrics neglect temporal fidelity and text-to-video alignment, critical for safety-critical applications.", "method": "Converts prompts into Temporal Logic (TL) specifications and videos into automata, then formally checks alignment.", "result": "NeuS-V shows 5x higher correlation with human evaluations; current models perform poorly on temporally complex prompts.", "conclusion": "Highlights the need for improved text-to-video generation capabilities, especially for temporal fidelity."}}
{"id": "2504.17656", "pdf": "https://arxiv.org/pdf/2504.17656", "abs": "https://arxiv.org/abs/2504.17656", "authors": ["Ayush Jain", "Rampi Ramprasad"], "title": "polyGen: A Learning Framework for Atomic-level Polymer Structure Generation", "categories": ["cs.CE", "cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Synthetic polymeric materials underpin fundamental technologies in the\nenergy, electronics, consumer goods, and medical sectors, yet their development\nstill suffers from prolonged design timelines. Although polymer informatics\ntools have supported speedup, polymer simulation protocols continue to face\nsignificant challenges: on-demand generation of realistic 3D atomic structures\nthat respect the conformational diversity of polymer structures. Generative\nalgorithms for 3D structures of inorganic crystals, bio-polymers, and small\nmolecules exist, but have not addressed synthetic polymers. In this work, we\nintroduce polyGen, the first latent diffusion model designed specifically to\ngenerate realistic polymer structures from minimal inputs such as the repeat\nunit chemistry alone, leveraging a molecular encoding that captures polymer\nconnectivity throughout the architecture. Due to a scarce dataset of only 3855\nDFT-optimized polymer structures, we augment our training with DFT-optimized\nmolecular structures, showing improvement in joint learning between similar\nchemical structures. We also establish structure matching criteria to benchmark\nour approach on this novel problem. polyGen effectively generates diverse\nconformations of both linear chains and complex branched structures, though its\nperformance decreases when handling repeat units with a high atom count. Given\nthese initial results, polyGen represents a paradigm shift in atomic-level\nstructure generation for polymer science-the first proof-of-concept for\npredicting realistic atomic-level polymer conformations while accounting for\ntheir intrinsic structural flexibility.", "AI": {"tldr": "polyGen is a latent diffusion model for generating realistic 3D polymer structures from minimal inputs, addressing gaps in polymer informatics.", "motivation": "Current polymer simulation protocols lack efficient tools for generating diverse, realistic 3D atomic structures, hindering rapid polymer development.", "method": "polyGen uses a latent diffusion model with molecular encoding to generate polymer structures, augmented by DFT-optimized data for training.", "result": "polyGen successfully generates diverse conformations for linear and branched polymers, though performance drops with high atom count repeat units.", "conclusion": "polyGen is a pioneering tool for atomic-level polymer structure generation, offering a proof-of-concept for realistic conformation prediction."}}
{"id": "2411.14423", "pdf": "https://arxiv.org/pdf/2411.14423", "abs": "https://arxiv.org/abs/2411.14423", "authors": ["Zhuoman Liu", "Weicai Ye", "Yan Luximon", "Pengfei Wan", "Di Zhang"], "title": "PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation", "categories": ["cs.CV"], "comment": "CVPR 2025. Homepage: https://zhuomanliu.github.io/PhysFlow/", "summary": "Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce PhysFlow, a\nnovel approach that leverages multi-modal foundation models and video diffusion\nto achieve enhanced 4D dynamic scene simulation. Our method utilizes\nmulti-modal models to identify material types and initialize material\nparameters through image queries, while simultaneously inferring 3D Gaussian\nsplats for detailed scene representation. We further refine these material\nparameters using video diffusion with a differentiable Material Point Method\n(MPM) and optical flow guidance rather than render loss or Score Distillation\nSampling (SDS) loss. This integrated framework enables accurate prediction and\nrealistic simulation of dynamic interactions in real-world scenarios, advancing\nboth accuracy and flexibility in physics-based simulations.", "AI": {"tldr": "PhysFlow enhances 4D dynamic scene simulation using multi-modal foundation models and video diffusion for realistic material interactions.", "motivation": "Existing methods lack the ability to represent complex real-world materials and interactions accurately.", "method": "Combines multi-modal models for material identification and 3D Gaussian splats, refined via video diffusion with differentiable MPM and optical flow guidance.", "result": "Achieves realistic and accurate simulation of dynamic interactions in real-world scenarios.", "conclusion": "PhysFlow advances accuracy and flexibility in physics-based simulations."}}
{"id": "2411.17973", "pdf": "https://arxiv.org/pdf/2411.17973", "abs": "https://arxiv.org/abs/2411.17973", "authors": ["Zhenyu Yu", "Jinnian Wang", "Mohd Yamani Idna Idris"], "title": "Improved implicit diffusion model with knowledge distillation to estimate the spatial distribution density of carbon stock in remote sensing imagery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The forest serves as the most significant terrestrial carbon stock mechanism,\neffectively reducing atmospheric CO2 concentrations and mitigating climate\nchange. Remote sensing provides high data accuracy and enables large-scale\nobservations. Optical images facilitate long-term monitoring, which is crucial\nfor future carbon stock estimation studies. This study focuses on Huize County,\nQujing City, Yunnan Province, China, utilizing GF-1 WFV satellite imagery. The\nKD-VGG and KD-UNet modules were introduced for initial feature extraction, and\nthe improved implicit diffusion model (IIDM) was proposed. The results showed:\n(1) The VGG module improved initial feature extraction, improving accuracy, and\nreducing inference time with optimized model parameters. (2) The\nCross-attention + MLPs module enabled effective feature fusion, establishing\ncritical relationships between global and local features, achieving\nhigh-accuracy estimation. (3) The IIDM model, a novel contribution,\ndemonstrated the highest estimation accuracy with an RMSE of 12.17%,\nsignificantly improving by 41.69% to 42.33% compared to the regression model.\nIn carbon stock estimation, the generative model excelled in extracting deeper\nfeatures, significantly outperforming other models, demonstrating the\nfeasibility of AI-generated content in quantitative remote sensing. The\n16-meter resolution estimates provide a robust basis for tailoring forest\ncarbon sink regulations, enhancing regional carbon stock management.", "AI": {"tldr": "The study uses GF-1 WFV satellite imagery and AI models (KD-VGG, KD-UNet, IIDM) for high-accuracy forest carbon stock estimation, improving RMSE by 41.69-42.33% over regression models.", "motivation": "Forests are vital carbon sinks, and accurate monitoring via remote sensing is crucial for climate change mitigation.", "method": "Utilized GF-1 WFV imagery, introduced KD-VGG and KD-UNet for feature extraction, and proposed the IIDM model for estimation.", "result": "IIDM achieved the highest accuracy (RMSE 12.17%), outperforming other models by 41.69-42.33%.", "conclusion": "AI-generated models like IIDM enhance carbon stock estimation, supporting better forest management and climate policies."}}
{"id": "2504.17690", "pdf": "https://arxiv.org/pdf/2504.17690", "abs": "https://arxiv.org/abs/2504.17690", "authors": ["Petros Georgiou", "Aaron Mark Thomas", "Sharu Theresa Jose", "Osvaldo Simeone"], "title": "On the Generalization of Adversarially Trained Quantum Classifiers", "categories": ["quant-ph", "cs.LG"], "comment": "22 pages, 6 figures", "summary": "Quantum classifiers are vulnerable to adversarial attacks that manipulate\ntheir input classical or quantum data. A promising countermeasure is\nadversarial training, where quantum classifiers are trained by using an\nattack-aware, adversarial loss function. This work establishes novel bounds on\nthe generalization error of adversarially trained quantum classifiers when\ntested in the presence of perturbation-constrained adversaries. The bounds\nquantify the excess generalization error incurred to ensure robustness to\nadversarial attacks as scaling with the training sample size $m$ as\n$1/\\sqrt{m}$, while yielding insights into the impact of the quantum embedding.\nFor quantum binary classifiers employing \\textit{rotation embedding}, we find\nthat, in the presence of adversarial attacks on classical inputs $\\mathbf{x}$,\nthe increase in sample complexity due to adversarial training over conventional\ntraining vanishes in the limit of high dimensional inputs $\\mathbf{x}$. In\ncontrast, when the adversary can directly attack the quantum state\n$\\rho(\\mathbf{x})$ encoding the input $\\mathbf{x}$, the excess generalization\nerror depends on the choice of embedding only through its Hilbert space\ndimension. The results are also extended to multi-class classifiers. We\nvalidate our theoretical findings with numerical experiments.", "AI": {"tldr": "The paper analyzes adversarial training for quantum classifiers, establishing bounds on generalization error and exploring the impact of quantum embedding. It shows how adversarial robustness scales with training data and input dimensionality.", "motivation": "Quantum classifiers are susceptible to adversarial attacks, necessitating robust training methods like adversarial training to ensure reliability.", "method": "The study derives theoretical bounds on generalization error for adversarially trained quantum classifiers, focusing on perturbation-constrained adversaries and different embedding types.", "result": "Generalization error scales as $1/\\sqrt{m}$ with training size. For high-dimensional inputs, adversarial training's sample complexity matches conventional training. Quantum state attacks' error depends on embedding's Hilbert space dimension.", "conclusion": "Adversarial training effectively enhances quantum classifier robustness, with performance insights tied to embedding choice and input dimensionality, validated by experiments."}}
{"id": "2412.16195", "pdf": "https://arxiv.org/pdf/2412.16195", "abs": "https://arxiv.org/abs/2412.16195", "authors": ["Shekhar Madhav Khairnar", "Huu Phong Nguyen", "Alexis Desir", "Carla Holcomb", "Daniel J. Scott", "Ganesh Sankaranarayanan"], "title": "Machine Learning-Based Automated Assessment of Intracorporeal Suturing in Laparoscopic Fundoplication", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "17 pages", "summary": "Automated assessment of surgical skills using artificial intelligence (AI)\nprovides trainees with instantaneous feedback. After bimanual tool motions are\ncaptured, derived kinematic metrics are reliable predictors of performance in\nlaparoscopic tasks. Implementing automated tool tracking requires\ntime-intensive human annotation. We developed AI-based tool tracking using the\nSegment Anything Model (SAM) to eliminate the need for human annotators. Here,\nwe describe a study evaluating the usefulness of our tool tracking model in\nautomated assessment during a laparoscopic suturing task in the fundoplication\nprocedure. An automated tool tracking model was applied to recorded videos of\nNissen fundoplication on porcine bowel. Surgeons were grouped as novices\n(PGY1-2) and experts (PGY3-5, attendings). The beginning and end of each\nsuturing step were segmented, and motions of the left and right tools were\nextracted. A low-pass filter with a 24 Hz cut-off frequency removed noise.\nPerformance was assessed using supervised and unsupervised models, and an\nablation study compared results. Kinematic features--RMS velocity, RMS\nacceleration, RMS jerk, total path length, and Bimanual Dexterity--were\nextracted and analyzed using Logistic Regression, Random Forest, Support Vector\nClassifier, and XGBoost. PCA was performed for feature reduction. For\nunsupervised learning, a Denoising Autoencoder (DAE) model with classifiers,\nsuch as a 1-D CNN and traditional models, was trained. Data were extracted for\n28 participants (9 novices, 19 experts). Supervised learning with PCA and\nRandom Forest achieved an accuracy of 0.795 and an F1 score of 0.778. The\nunsupervised 1-D CNN achieved superior results with an accuracy of 0.817 and an\nF1 score of 0.806, eliminating the need for kinematic feature computation. We\ndemonstrated an AI model capable of automated performance classification,\nindependent of human annotation.", "AI": {"tldr": "AI-based tool tracking using SAM eliminates human annotation for automated surgical skill assessment. Supervised and unsupervised models achieved high accuracy in classifying novice vs. expert performance.", "motivation": "To provide instantaneous feedback for surgical trainees by automating skill assessment without time-intensive human annotation.", "method": "Used SAM for tool tracking, extracted kinematic features, and applied supervised (Random Forest, etc.) and unsupervised (1-D CNN) models for classification.", "result": "Supervised learning achieved 0.795 accuracy; unsupervised 1-D CNN outperformed with 0.817 accuracy, eliminating kinematic feature computation.", "conclusion": "AI models can automate surgical skill assessment effectively, removing reliance on human annotation."}}
{"id": "2412.10892", "pdf": "https://arxiv.org/pdf/2412.10892", "abs": "https://arxiv.org/abs/2412.10892", "authors": ["Haocheng Duan", "Hao Wu", "Sean Qian"], "title": "Know Unreported Roadway Incidents in Real-time: Early Traffic Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This research aims to know traffic anomalies as early as possible. A traffic\nanomaly refers to a generic incident on the road that influences traffic flow\nand calls for urgent traffic management measures. `Knowing'' the occurrence of\na traffic anomaly is twofold: the ability to detect this anomaly before it is\nreported anywhere, or it may be such that an anomaly can be predicted before it\nactually occurs on the road (e.g., non-recurrent traffic breakdown). In either\nway, the objective is to inform traffic operators of unreported incidents in\nreal time and as early as possible. The key is to stay ahead of the curve. Time\nis of the essence.\n  Conventional automatic incident detection (AID) methods often struggle with\nearly detection due to their limited consideration of spatial effects and\nearly-stage characteristics. Therefore, we propose a deep learning framework\nutilizing prior domain knowledge and model-designing strategies. This allows\nthe model to detect a broader range of anomalies, not only incidents that\nsignificantly influence traffic flow but also early characteristics of\nincidents along with historically unreported anomalies. We specially design the\nmodel to target the early-stage detection/prediction of an incident.\nAdditionally, unlike most conventional AID studies, our method is highly\nscalable and generalizable, as it is fully automated with no manual selection\nof historical reports required, relies solely on widely available low-cost\ndata, and requires no additional detectors. The experimental results across\nnumerous road segments on different maps demonstrate that our model leads to\nmore effective and early anomaly detection.", "AI": {"tldr": "A deep learning framework for early detection and prediction of traffic anomalies, outperforming conventional methods by leveraging spatial effects and early-stage characteristics.", "motivation": "To enable real-time, early detection of unreported traffic anomalies for urgent traffic management, addressing limitations of conventional AID methods.", "method": "Proposes a deep learning framework incorporating domain knowledge and scalable design, using low-cost data without manual intervention.", "result": "Demonstrates more effective and early anomaly detection across various road segments.", "conclusion": "The framework offers scalable, generalizable, and automated early anomaly detection, improving traffic management."}}
{"id": "2504.17719", "pdf": "https://arxiv.org/pdf/2504.17719", "abs": "https://arxiv.org/abs/2504.17719", "authors": ["Matthijs van der Lende", "Jeremias Lino Ferrao", "Niclas M\u00fcller-Hof"], "title": "Evaluating Uncertainty in Deep Gaussian Processes", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Reliable uncertainty estimates are crucial in modern machine learning. Deep\nGaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) extend GPs\nhierarchically, offering promising methods for uncertainty quantification\ngrounded in Bayesian principles. However, their empirical calibration and\nrobustness under distribution shift relative to baselines like Deep Ensembles\nremain understudied. This work evaluates these models on regression (CASP\ndataset) and classification (ESR dataset) tasks, assessing predictive\nperformance (MAE, Accu- racy), calibration using Negative Log-Likelihood (NLL)\nand Expected Calibration Error (ECE), alongside robustness under various\nsynthetic feature-level distribution shifts. Results indicate DSPPs provide\nstrong in-distribution calibration leveraging their sigma point approximations.\nHowever, compared to Deep Ensembles, which demonstrated superior robustness in\nboth per- formance and calibration under the tested shifts, the GP-based\nmethods showed vulnerabilities, exhibiting particular sensitivity in the\nobserved metrics. Our findings underscore ensembles as a robust baseline,\nsuggesting that while deep GP methods offer good in-distribution calibration,\ntheir practical robustness under distribution shift requires careful\nevaluation. To facilitate reproducibility, we make our code available at\nhttps://github.com/matthjs/xai-gp.", "AI": {"tldr": "The paper evaluates Deep Gaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) for uncertainty quantification, comparing them to Deep Ensembles. Results show DSPPs excel in-distribution but lack robustness under distribution shifts, where Deep Ensembles outperform.", "motivation": "To assess the empirical calibration and robustness of DGPs and DSPPs under distribution shifts, compared to Deep Ensembles, for reliable uncertainty estimates in machine learning.", "method": "Evaluation on regression (CASP) and classification (ESR) tasks using metrics like MAE, Accuracy, NLL, and ECE, alongside synthetic feature-level distribution shifts.", "result": "DSPPs provide strong in-distribution calibration but are less robust under shifts. Deep Ensembles outperform in both performance and calibration robustness.", "conclusion": "Deep GP methods offer good in-distribution calibration but require careful evaluation for practical robustness under shifts. Ensembles remain a robust baseline."}}
{"id": "2501.01163", "pdf": "https://arxiv.org/pdf/2501.01163", "abs": "https://arxiv.org/abs/2501.01163", "authors": ["Jiajun Deng", "Tianyu He", "Li Jiang", "Tianyu Wang", "Feras Dayoub", "Ian Reid"], "title": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential\nin 3D-vision-based dialogue and reasoning. However, how to further enhance 3D\nLMMs to achieve fine-grained scene understanding and facilitate flexible\nhuman-agent interaction remains a challenging problem. In this work, we\nintroduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an\nintelligent assistant in comprehending, reasoning, and interacting with the 3D\nworld. Unlike existing top-performing methods that rely on complicated\npipelines-such as offline multi-view feature extraction or additional\ntask-specific heads-3D-LLaVA adopts a minimalist design with integrated\narchitecture and only takes point clouds as input. At the core of 3D-LLaVA is a\nnew Omni Superpoint Transformer (OST), which integrates three functionalities:\n(1) a visual feature selector that converts and selects visual tokens, (2) a\nvisual prompt encoder that embeds interactive visual prompts into the visual\ntoken space, and (3) a referring mask decoder that produces 3D masks based on\ntext description. This versatile OST is empowered by the hybrid pretraining to\nobtain perception priors and leveraged as the visual connector that bridges the\n3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA\nreports impressive results on various benchmarks.", "AI": {"tldr": "3D-LLaVA is a simple yet powerful 3D Large Multimodal Model (LMM) for fine-grained scene understanding and human-agent interaction, using point clouds and a novel Omni Superpoint Transformer (OST).", "motivation": "Enhancing 3D LMMs for fine-grained understanding and flexible interaction, addressing limitations of complex pipelines in existing methods.", "method": "Minimalist design with OST, integrating visual feature selection, prompt encoding, and mask decoding, pretrained and tuned for 3D-LLM bridging.", "result": "Achieves impressive performance on benchmarks.", "conclusion": "3D-LLaVA demonstrates effectiveness in 3D scene comprehension and interaction, offering a streamlined alternative to complex pipelines."}}
{"id": "2412.15921", "pdf": "https://arxiv.org/pdf/2412.15921", "abs": "https://arxiv.org/abs/2412.15921", "authors": ["Guang Yang", "Yu Zhou", "Xiangyu Zhang", "Wei Cheng", "Ke Liu", "Xiang Chen", "Terry Yue Zhuo", "Taolue Chen"], "title": "Less is More: Towards Green Code Large Language Models via Unified Structural Pruning", "categories": ["cs.SE", "cs.AI"], "comment": "UNDER REVIEW", "summary": "The extensive application of Large Language Models (LLMs) in generative\ncoding tasks has raised concerns due to their high computational demands and\nenergy consumption. Unlike previous structural pruning methods designed for\nclassification models that deal with lowdimensional classification logits,\ngenerative Code LLMs produce high-dimensional token logit sequences, making\ntraditional pruning objectives inherently limited. Moreover, existing single\ncomponent pruning approaches further constrain the effectiveness when applied\nto generative Code LLMs. In response, we propose Flab-Pruner, an innovative\nunified structural pruning method that combines vocabulary, layer, and\nFeed-Forward Network (FFN) pruning. This approach effectively reduces model\nparameters while maintaining performance. Additionally, we introduce a\ncustomized code instruction data strategy for coding tasks to enhance the\nperformance recovery efficiency of the pruned model. Through extensive\nevaluations on three state-of-the-art Code LLMs across multiple generative\ncoding tasks, the results demonstrate that Flab-Pruner retains 97% of the\noriginal performance after pruning 22% of the parameters and achieves the same\nor even better performance after post-training. The pruned models exhibit\nsignificant improvements in storage, GPU usage, computational efficiency, and\nenvironmental impact, while maintaining well robustness. Our research provides\na sustainable solution for green software engineering and promotes the\nefficient deployment of LLMs in real-world generative coding intelligence\napplications.", "AI": {"tldr": "Flab-Pruner is a unified structural pruning method for Code LLMs, combining vocabulary, layer, and FFN pruning to reduce parameters while maintaining performance, achieving 97% original performance after pruning 22% of parameters.", "motivation": "Address high computational demands and energy consumption of generative Code LLMs, overcoming limitations of traditional pruning methods.", "method": "Proposes Flab-Pruner, integrating vocabulary, layer, and FFN pruning, with a customized code instruction data strategy for performance recovery.", "result": "Retains 97% performance after pruning 22% parameters; improves storage, GPU usage, efficiency, and environmental impact.", "conclusion": "Provides a sustainable solution for green software engineering and efficient LLM deployment in generative coding tasks."}}
{"id": "2311.07283", "pdf": "https://arxiv.org/pdf/2311.07283", "abs": "https://arxiv.org/abs/2311.07283", "authors": ["Elizabeth Williams", "Daniel Gartner", "Paul Harper"], "title": "Predictive and prescriptive analytics for multi-site modelling of frail and elderly patient services", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Many economies are challenged by the effects of an ageing population,\nparticularly in sectors where resource capacity planning is critical, such as\nhealthcare. This research addresses the operational challenges of bed and\nstaffing capacity planning in hospital wards by using predictive and\nprescriptive analytical methods, both individually and in tandem. We applied\nthese methodologies to a study of 165,000 patients across a network of 11\nhospitals in the UK. Predictive modelling, specifically Classification and\nRegression Trees, forecasts patient length of stay based on clinical and\ndemographic data. On the prescriptive side, deterministic and two-stage\nstochastic optimisation models determine optimal bed and staff planning\nstrategies to minimise costs. Linking the predictive models with the\nprescriptive optimisation models, generates demand forecasts that inform the\noptimisation process, providing accurate and practical solutions. The results\ndemonstrate that this integrated approach captures real-world variations in\npatient LOS and offers a 7% cost saving compared to average-based planning.\nThis approach helps healthcare managers make robust decisions by incorporating\npatient-specific characteristics, improving capacity allocation, and mitigating\nrisks associated with demand variability. Consequently, this combined\nmethodology can be broadly extended across various sectors facing similar\nchallenges, showcasing the versatility and effectiveness of integrating\npredictive and prescriptive analytics.", "AI": {"tldr": "The paper integrates predictive and prescriptive analytics to optimize bed and staffing capacity in hospitals, achieving a 7% cost saving.", "motivation": "Addressing operational challenges in healthcare due to ageing populations, focusing on resource capacity planning.", "method": "Uses Classification and Regression Trees for predictive modeling and deterministic/stochastic optimization for prescriptive planning.", "result": "7% cost saving compared to average-based planning, with improved capacity allocation.", "conclusion": "The combined methodology is versatile and effective for sectors facing similar resource planning challenges."}}
{"id": "2501.12489", "pdf": "https://arxiv.org/pdf/2501.12489", "abs": "https://arxiv.org/abs/2501.12489", "authors": ["Josh Bruegger", "Diana Ioana Catana", "Vanja Macovaz", "Matias Valdenegro-Toro", "Matthia Sabatelli", "Marco Zullich"], "title": "Large-image Object Detection for Fine-grained Recognition of Punches Patterns in Medieval Panel Painting", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The attribution of the author of an art piece is typically a laborious manual\nprocess, usually relying on subjective evaluations of expert figures. However,\nthere are some situations in which quantitative features of the artwork can\nsupport these evaluations. The extraction of these features can sometimes be\nautomated, for instance, with the use of Machine Learning (ML) techniques. An\nexample of these features is represented by repeated, mechanically impressed\npatterns, called punches, present chiefly in 13th and 14th-century panel\npaintings from Tuscany. Previous research in art history showcased a strong\nconnection between the shapes of punches and specific artists or workshops,\nsuggesting the possibility of using these quantitative cues to support the\nattribution. In the present work, we first collect a dataset of large-scale\nimages of these panel paintings. Then, using YOLOv10, a recent and popular\nobject detection model, we train a ML pipeline to perform object detection on\nthe punches contained in the images. Due to the large size of the images, the\ndetection procedure is split across multiple frames by adopting a\nsliding-window approach with overlaps, after which the predictions are combined\nfor the whole image using a custom non-maximal suppression routine. Our results\nindicate how art historians working in the field can reliably use our method\nfor the identification and extraction of punches.", "AI": {"tldr": "The paper proposes a ML-based method using YOLOv10 to automate the detection of punches in historical panel paintings, aiding art historians in attribution.", "motivation": "Manual attribution of art pieces is laborious and subjective. Quantitative features like punches can support this process, and ML can automate their extraction.", "method": "A dataset of large-scale images is collected, and YOLOv10 is trained for punch detection using a sliding-window approach with custom non-maximal suppression.", "result": "The method reliably identifies and extracts punches, supporting art historians in attribution.", "conclusion": "The ML pipeline effectively automates punch detection, offering a reliable tool for art historical research."}}
{"id": "2501.06141", "pdf": "https://arxiv.org/pdf/2501.06141", "abs": "https://arxiv.org/abs/2501.06141", "authors": ["Satchel Grant", "Noah D. Goodman", "James L. McClelland"], "title": "Emergent Symbol-like Number Variables in Artificial Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "What types of numeric representations emerge in neural systems? What would a\nsatisfying answer to this question look like? In this work, we interpret Neural\nNetwork (NN) solutions to sequence based counting tasks through a variety of\nlenses. We seek to understand how well we can understand NNs through the lens\nof interpretable Symbolic Algorithms (SAs), where SAs are defined by precise,\nabstract, mutable variables used to perform computations. We use GRUs, LSTMs,\nand Transformers trained using Next Token Prediction (NTP) on numeric tasks\nwhere the solutions to the tasks depend on numeric information only latent in\nthe task structure. We show through multiple causal and theoretical methods\nthat we can interpret NN's raw activity through the lens of simplified SAs when\nwe frame the neural activity in terms of interpretable subspaces rather than\nindividual neurons. Depending on the analysis, however, these interpretations\ncan be graded, existing on a continuum, highlighting the philosophical question\nof what it means to \"interpret\" neural activity, and motivating us to introduce\nAlignment Functions to add flexibility to the existing Distributed Alignment\nSearch (DAS) method. Through our specific analyses we show the importance of\ncausal interventions for NN interpretability; we show that recurrent models\ndevelop graded, symbol-like number variables within their neural activity; we\nintroduce a generalization of DAS to frame NN activity in terms of linear\nfunctions of interpretable variables; and we show that Transformers must use\nanti-Markovian solutions -- solutions that avoid using cumulative, Markovian\nhidden states -- in the absence of sufficient attention layers. We use our\nresults to encourage interpreting NNs at the level of neural subspaces through\nthe lens of SAs.", "AI": {"tldr": "The paper explores how neural networks (NNs) represent numeric information and interprets their solutions through symbolic algorithms (SAs). It uses GRUs, LSTMs, and Transformers on numeric tasks, showing that NN activity can be understood via interpretable subspaces, though interpretations are often graded. The work introduces Alignment Functions for flexibility and emphasizes causal interventions for interpretability.", "motivation": "To understand how NNs represent numeric information and whether their solutions can be interpreted through symbolic algorithms, providing insights into NN interpretability.", "method": "GRUs, LSTMs, and Transformers are trained using Next Token Prediction (NTP) on numeric tasks. The study uses causal and theoretical methods to interpret NN activity via symbolic algorithms and introduces Alignment Functions for Distributed Alignment Search (DAS).", "result": "Recurrent models develop symbol-like number variables, and Transformers use anti-Markovian solutions without sufficient attention layers. Interpretations are graded, highlighting the complexity of NN activity.", "conclusion": "NNs should be interpreted at the level of neural subspaces through symbolic algorithms, emphasizing causal interventions and flexible methods like Alignment Functions."}}
{"id": "2311.11762", "pdf": "https://arxiv.org/pdf/2311.11762", "abs": "https://arxiv.org/abs/2311.11762", "authors": ["Daniel Bogdoll", "Yitian Yang", "Tim Joseph", "Melih Yazgan", "J. Marius Z\u00f6llner"], "title": "MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations", "categories": ["cs.LG", "cs.RO"], "comment": "Daniel Bogdoll and Yitian Yang contributed equally. Accepted for\n  publication at IV 2025", "summary": "World models for autonomous driving have the potential to dramatically\nimprove the reasoning capabilities of today's systems. However, most works\nfocus on camera data, with only a few that leverage lidar data or combine both\nto better represent autonomous vehicle sensor setups. In addition, raw sensor\npredictions are less actionable than 3D occupancy predictions, but there are no\nworks examining the effects of combining both multimodal sensor data and 3D\noccupancy prediction. In this work, we perform a set of experiments with a\nMUltimodal World Model with Geometric VOxel representations (MUVO) to evaluate\ndifferent sensor fusion strategies to better understand the effects on sensor\ndata prediction. We also analyze potential weaknesses of current sensor fusion\napproaches and examine the benefits of additionally predicting 3D occupancy.", "AI": {"tldr": "The paper explores multimodal sensor fusion (camera and lidar) and 3D occupancy prediction in autonomous driving world models, introducing MUVO to evaluate their impact.", "motivation": "Most autonomous driving models focus on camera data, neglecting lidar or combined sensor setups. Raw sensor predictions are less actionable than 3D occupancy predictions, yet no prior work examines their combined effects.", "method": "The authors conduct experiments using MUVO (MUltimodal World Model with Geometric VOxel representations) to evaluate sensor fusion strategies and 3D occupancy prediction.", "result": "The study provides insights into the effects of multimodal sensor fusion and highlights the benefits of 3D occupancy prediction.", "conclusion": "Combining multimodal sensor data with 3D occupancy prediction improves autonomous driving world models, addressing gaps in current approaches."}}
{"id": "2501.16312", "pdf": "https://arxiv.org/pdf/2501.16312", "abs": "https://arxiv.org/abs/2501.16312", "authors": ["Nicolas von L\u00fctzow", "Matthias Nie\u00dfner"], "title": "LinPrim: Linear Primitives for Differentiable Volumetric Rendering", "categories": ["cs.CV"], "comment": "Project page: https://nicolasvonluetzow.github.io/LinPrim - Project\n  video: https://youtu.be/NRRlmFZj5KQ", "summary": "Volumetric rendering has become central to modern novel view synthesis\nmethods, which use differentiable rendering to optimize 3D scene\nrepresentations directly from observed views. While many recent works build on\nNeRF or 3D Gaussians, we explore an alternative volumetric scene\nrepresentation. More specifically, we introduce two new scene representations\nbased on linear primitives - octahedra and tetrahedra - both of which define\nhomogeneous volumes bounded by triangular faces. To optimize these primitives,\nwe present a differentiable rasterizer that runs efficiently on GPUs, allowing\nend-to-end gradient-based optimization while maintaining real-time rendering\ncapabilities. Through experiments on real-world datasets, we demonstrate\ncomparable performance to state-of-the-art volumetric methods while requiring\nfewer primitives to achieve similar reconstruction fidelity. Our findings\ndeepen the understanding of 3D representations by providing insights into the\nfidelity and performance characteristics of transparent polyhedra and suggest\nthat adopting novel primitives can expand the available design space.", "AI": {"tldr": "The paper introduces two new volumetric scene representations using octahedra and tetrahedra, optimized via a differentiable rasterizer, achieving comparable performance to state-of-the-art methods with fewer primitives.", "motivation": "To explore alternative volumetric scene representations beyond NeRF and 3D Gaussians, focusing on linear primitives for efficient and high-fidelity 3D reconstruction.", "method": "Proposes octahedra and tetrahedra as scene primitives, optimized using a GPU-efficient differentiable rasterizer for end-to-end gradient-based optimization.", "result": "Demonstrates comparable performance to state-of-the-art volumetric methods with fewer primitives, maintaining real-time rendering capabilities.", "conclusion": "Highlights the potential of novel primitives to expand the design space for 3D representations, offering insights into fidelity and performance trade-offs."}}
{"id": "2501.06164", "pdf": "https://arxiv.org/pdf/2501.06164", "abs": "https://arxiv.org/abs/2501.06164", "authors": ["Satchel Grant"], "title": "Model Alignment Search", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "When can we say that two neural systems are the same? The answer to this\nquestion is goal-dependent, and it is often addressed through correlative\nmethods such as Representational Similarity Analysis (RSA) and Centered Kernel\nAlignment (CKA). We find ourselves chiefly interested in the relationship\nbetween representations and behavior, asking ourselves how we can isolate\nspecific functional aspects of representational similarity to relate our\nmeasures to behavior -- avoiding cause vs. correlation pitfalls in the process.\nIn this work, we introduce Model Alignment Search (MAS), a method for causally\nexploring distributed representational similarity as it relates to behavior.\nThe method learns invertible linear transformations that find an aligned\nsubspace between two distributed networks' representations where functional\ninformation can be isolated and manipulated. We first show that the method can\nbe used to transfer values of specific causal variables -- such as the number\nof items in a counting task -- between networks with different training seeds\nand different architectures. We then explore open questions in number cognition\nby comparing different types of numeric representations in models trained on\nstructurally different tasks, we explore differences between MAS and\npreexisting functional similarity methods, and lastly, we introduce a\ncounterfactual latent auxiliary loss that helps shape functionally relevant\nalignments even in cases where we do not have causal access to one of the two\nmodels for training.", "AI": {"tldr": "The paper introduces Model Alignment Search (MAS), a method to causally explore representational similarity between neural systems and behavior, avoiding correlation pitfalls.", "motivation": "To isolate specific functional aspects of representational similarity and relate them to behavior, addressing cause vs. correlation issues.", "method": "MAS learns invertible linear transformations to align subspaces between two networks' representations, enabling isolation and manipulation of functional information.", "result": "MAS successfully transfers causal variables (e.g., item count in a task) between networks with different seeds/architectures and explores numeric representations in models.", "conclusion": "MAS offers a causal approach to functional similarity, with a counterfactual loss aiding alignment even without causal access to one model."}}
{"id": "2403.11743", "pdf": "https://arxiv.org/pdf/2403.11743", "abs": "https://arxiv.org/abs/2403.11743", "authors": ["Philip Matthias Winter", "Maria Wimmer", "David Major", "Dimitrios Lenis", "Astrid Berg", "Theresa Neubauer", "Gaia Romana De Paolis", "Johannes Novotny", "Sophia Ulonska", "Katja B\u00fchler"], "title": "PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks", "categories": ["cs.LG", "stat.ML"], "comment": "This is the author's accepted manuscript of a paper published in\n  Lecture Notes in Computer Science (LNCS), volume 15297, Proceedings of DAGM\n  GCPR 2024. 25 pages, 7 figures", "summary": "This work addresses flexibility in deep learning by means of transductive\nreasoning. For adaptation to new data and tasks, e.g., in continual learning,\nexisting methods typically involve tuning learnable parameters or complete\nre-training from scratch, rendering such approaches unflexible in practice. We\nargue that the notion of separating computation from memory by the means of\ntransduction can act as a stepping stone for solving these issues. We therefore\npropose PARMESAN (parameter-free memory search and transduction), a scalable\nmethod which leverages a memory module for solving dense prediction tasks. At\ninference, hidden representations in memory are being searched to find\ncorresponding patterns. In contrast to other methods that rely on continuous\ntraining of learnable parameters, PARMESAN learns via memory consolidation\nsimply by modifying stored contents. Our method is compatible with commonly\nused architectures and canonically transfers to 1D, 2D, and 3D grid-based data.\nThe capabilities of our approach are demonstrated at the complex task of\ncontinual learning. PARMESAN learns by 3-4 orders of magnitude faster than\nestablished baselines while being on par in terms of predictive performance,\nhardware-efficiency, and knowledge retention.", "AI": {"tldr": "PARMESAN introduces a parameter-free method for deep learning flexibility using transductive reasoning and memory search, outperforming baselines in speed and efficiency.", "motivation": "Addressing inflexibility in deep learning adaptation, especially in continual learning, by separating computation from memory.", "method": "PARMESAN leverages a memory module for dense prediction tasks, searching hidden representations at inference without continuous parameter training.", "result": "Learns 3-4 orders of magnitude faster than baselines while matching performance, hardware-efficiency, and knowledge retention.", "conclusion": "PARMESAN offers a scalable, efficient solution for continual learning, compatible with various architectures and data types."}}
{"id": "2502.02309", "pdf": "https://arxiv.org/pdf/2502.02309", "abs": "https://arxiv.org/abs/2502.02309", "authors": ["Ketan Kotwal", "Sebastien Marcel"], "title": "Review of Demographic Fairness in Face Recognition", "categories": ["cs.CV", "cs.CR"], "comment": "under review", "summary": "Demographic fairness in face recognition (FR) has emerged as a critical area\nof research, given its impact on fairness, equity, and reliability across\ndiverse applications. As FR technologies are increasingly deployed globally,\ndisparities in performance across demographic groups-- such as race, ethnicity,\nand gender-- have garnered significant attention. These biases not only\ncompromise the credibility of FR systems but also raise ethical concerns,\nespecially when these technologies are employed in sensitive domains. This\nreview consolidates extensive research efforts providing a comprehensive\noverview of the multifaceted aspects of demographic fairness in FR.\n  We systematically examine the primary causes, datasets, assessment metrics,\nand mitigation approaches associated with demographic disparities in FR. By\ncategorizing key contributions in these areas, this work provides a structured\napproach to understanding and addressing the complexity of this issue. Finally,\nwe highlight current advancements and identify emerging challenges that need\nfurther investigation. This article aims to provide researchers with a unified\nperspective on the state-of-the-art while emphasizing the critical need for\nequitable and trustworthy FR systems.", "AI": {"tldr": "A review on demographic fairness in face recognition (FR), covering causes, datasets, metrics, and mitigation methods, while highlighting advancements and future challenges.", "motivation": "Addressing biases in FR systems due to demographic disparities, which impact fairness, equity, and reliability, especially in sensitive applications.", "method": "Systematic examination of causes, datasets, assessment metrics, and mitigation approaches in FR research.", "result": "Comprehensive overview of demographic fairness in FR, categorizing key contributions and identifying gaps.", "conclusion": "Emphasizes the need for equitable and trustworthy FR systems, providing a unified perspective for researchers."}}
{"id": "2501.10100", "pdf": "https://arxiv.org/pdf/2501.10100", "abs": "https://arxiv.org/abs/2501.10100", "authors": ["Chenhao Li", "Andreas Krause", "Marco Hutter"], "title": "Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. This work advances model-based reinforcement learning by\naddressing the challenges of long-horizon prediction, error accumulation, and\nsim-to-real transfer. By providing a scalable and robust framework, the\nintroduced methods pave the way for adaptive and efficient robotic systems in\nreal-world applications.", "AI": {"tldr": "A novel framework for learning robust and generalizable world models for robotic control, using dual-autoregressive mechanisms and self-supervised training, enabling long-horizon predictions and efficient policy optimization.", "motivation": "To address the challenges of learning accurate world models for robotic control in complex, partially observable, and stochastic real-world environments.", "method": "Employs a dual-autoregressive mechanism and self-supervised training to ensure reliable long-horizon predictions without domain-specific biases. Includes a policy optimization framework for training in imagined environments and real-world deployment.", "result": "Advances model-based reinforcement learning by improving long-horizon prediction, reducing error accumulation, and enhancing sim-to-real transfer.", "conclusion": "The framework provides a scalable and robust solution for adaptive and efficient robotic systems in real-world applications."}}
{"id": "2404.16495", "pdf": "https://arxiv.org/pdf/2404.16495", "abs": "https://arxiv.org/abs/2404.16495", "authors": ["Evandro S. Ortigossa", "F\u00e1bio F. Dias", "Brian Barr", "Claudio T. Silva", "Luis Gustavo Nonato"], "title": "T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients", "categories": ["cs.LG"], "comment": "Copyright 2025 IEEE. All rights reserved, including rights for text,\n  data mining and training of artificial intelligence and similar technologies.\n  Personal use is permitted, but republication/redistribution requires IEEE\n  permission. Article accepted for publication in IEEE Intelligent Systems.\n  This author's version includes the supplementary material. Content may change\n  prior to final publication", "summary": "The development of machine learning applications has increased significantly\nin recent years, motivated by the remarkable ability of learning-powered\nsystems to discover and generalize intricate patterns hidden in massive\ndatasets. Modern learning models, while powerful, often exhibit a complexity\nlevel that renders them opaque black boxes, lacking transparency and hindering\nour understanding of their decision-making processes. Opacity challenges the\npractical application of machine learning, especially in critical domains\nrequiring informed decisions. Explainable Artificial Intelligence (XAI)\naddresses that challenge, unraveling the complexity of black boxes by providing\nexplanations. Feature attribution/importance XAI stands out for its ability to\ndelineate the significance of input features in predictions. However, most\nattribution methods have limitations, such as instability, when divergent\nexplanations result from similar or the same instance. This work introduces\nT-Explainer, a novel additive attribution explainer based on the Taylor\nexpansion that offers desirable properties such as local accuracy and\nconsistency. We demonstrate T-Explainer's effectiveness and stability over\nmultiple runs in quantitative benchmark experiments against well-known\nattribution methods. Additionally, we provide several tools to evaluate and\nvisualize explanations, turning T-Explainer into a comprehensive XAI framework.", "AI": {"tldr": "The paper introduces T-Explainer, a novel XAI method based on Taylor expansion, addressing instability in feature attribution methods and offering local accuracy and consistency.", "motivation": "The opacity of complex machine learning models hinders their practical application in critical domains, necessitating explainable AI (XAI) solutions.", "method": "T-Explainer, an additive attribution explainer using Taylor expansion, is proposed to provide stable and accurate feature importance explanations.", "result": "Benchmark experiments show T-Explainer's effectiveness and stability compared to existing methods, with tools for evaluation and visualization.", "conclusion": "T-Explainer serves as a comprehensive XAI framework, improving transparency and reliability in model explanations."}}
{"id": "2502.17196", "pdf": "https://arxiv.org/pdf/2502.17196", "abs": "https://arxiv.org/abs/2502.17196", "authors": ["Guillaume Jeanneret", "Lo\u00efc Simon", "Fr\u00e9d\u00e9ric Jurie"], "title": "Disentangling Visual Transformers: Patch-level Interpretability for Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025 official version. Main manuscript + supplementary", "summary": "Visual transformers have achieved remarkable performance in image\nclassification tasks, but this performance gain has come at the cost of\ninterpretability. One of the main obstacles to the interpretation of\ntransformers is the self-attention mechanism, which mixes visual information\nacross the whole image in a complex way. In this paper, we propose Hindered\nTransformer (HiT), a novel interpretable by design architecture inspired by\nvisual transformers. Our proposed architecture rethinks the design of\ntransformers to better disentangle patch influences at the classification\nstage. Ultimately, HiT can be interpreted as a linear combination of\npatch-level information. We show that the advantages of our approach in terms\nof explicability come with a reasonable trade-off in performance, making it an\nattractive alternative for applications where interpretability is paramount.", "AI": {"tldr": "HiT is an interpretable transformer architecture that disentangles patch influences for better explicability with a reasonable performance trade-off.", "motivation": "Transformers lack interpretability due to the complex self-attention mechanism, limiting their use in applications requiring transparency.", "method": "Proposes Hindered Transformer (HiT), redesigning transformers to disentangle patch influences, representing classification as a linear combination of patch-level information.", "result": "HiT offers improved interpretability with a reasonable performance trade-off, making it suitable for applications prioritizing explicability.", "conclusion": "HiT provides a viable, interpretable alternative to traditional transformers, balancing performance and transparency."}}
{"id": "2501.11695", "pdf": "https://arxiv.org/pdf/2501.11695", "abs": "https://arxiv.org/abs/2501.11695", "authors": ["Majid Farhadloo", "Arun Sharma", "Alexey Leontovich", "Svetomir N. Markovic", "Shashi Shekhar"], "title": "Spatially-Delineated Domain-Adapted AI Classification: An Application for Oncology Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Given multi-type point maps from different place-types (e.g., tumor regions),\nour objective is to develop a classifier trained on the source place-type to\naccurately distinguish between two classes of the target place-type based on\ntheir point arrangements. This problem is societally important for many\napplications, such as generating clinical hypotheses for designing new\nimmunotherapies for cancer treatment. The challenge lies in the spatial\nvariability, the inherent heterogeneity and variation observed in spatial\nproperties or arrangements across different locations (i.e., place-types).\nPrevious techniques focus on self-supervised tasks to learn domain-invariant\nfeatures and mitigate domain differences; however, they often neglect the\nunderlying spatial arrangements among data points, leading to significant\ndiscrepancies across different place-types. We explore a novel multi-task\nself-learning framework that targets spatial arrangements, such as spatial\nmix-up masking and spatial contrastive predictive coding, for\nspatially-delineated domain-adapted AI classification. Experimental results on\nreal-world datasets (e.g., oncology data) show that the proposed framework\nprovides higher prediction accuracy than baseline methods.", "AI": {"tldr": "A novel multi-task self-learning framework improves classification accuracy by focusing on spatial arrangements in multi-type point maps, outperforming baseline methods.", "motivation": "The study addresses the challenge of spatial variability and heterogeneity in point arrangements across different place-types, aiming to improve classification accuracy for applications like cancer immunotherapy design.", "method": "The proposed framework uses spatial mix-up masking and spatial contrastive predictive coding to target spatial arrangements, enhancing domain-adapted AI classification.", "result": "Experiments on real-world datasets (e.g., oncology data) demonstrate higher prediction accuracy compared to baseline methods.", "conclusion": "The framework effectively mitigates spatial discrepancies and improves classification performance, offering potential for applications in clinical and other domains."}}
{"id": "2404.18896", "pdf": "https://arxiv.org/pdf/2404.18896", "abs": "https://arxiv.org/abs/2404.18896", "authors": ["Xingyuan Zhang", "Philip Becker-Ehmck", "Patrick van der Smagt", "Maximilian Karl"], "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models", "categories": ["cs.LG"], "comment": "Accepted at TMLR", "summary": "Pretraining and finetuning models has become increasingly popular in\ndecision-making. But there are still serious impediments in Imitation Learning\nfrom Observation (ILfO) with pretrained models. This study identifies two\nprimary obstacles: the Embodiment Knowledge Barrier (EKB) and the Demonstration\nKnowledge Barrier (DKB). The EKB emerges due to the pretrained models'\nlimitations in handling novel observations, which leads to inaccurate action\ninference. Conversely, the DKB stems from the reliance on limited demonstration\ndatasets, restricting the model's adaptability across diverse scenarios. We\npropose separate solutions to overcome each barrier and apply them to Action\nInference by Maximising Evidence (AIME), a state-of-the-art algorithm. This new\nalgorithm, AIME-NoB, integrates online interactions and a data-driven\nregulariser to mitigate the EKB. Additionally, it uses a surrogate reward\nfunction to broaden the policy's supported states, addressing the DKB. Our\nexperiments on vision-based control tasks from the DeepMind Control Suite and\nMetaWorld benchmarks show that AIME-NoB significantly improves sample\nefficiency and converged performance, presenting a robust framework for\novercoming the challenges in ILfO with pretrained models. Code available at\nhttps://github.com/IcarusWizard/AIME-NoB.", "AI": {"tldr": "The paper addresses challenges in Imitation Learning from Observation (ILfO) with pretrained models, proposing solutions for the Embodiment Knowledge Barrier (EKB) and Demonstration Knowledge Barrier (DKB) via the AIME-NoB algorithm.", "motivation": "To overcome limitations in ILfO caused by pretrained models' inability to handle novel observations (EKB) and reliance on limited datasets (DKB).", "method": "Introduces AIME-NoB, integrating online interactions and a data-driven regulariser for EKB, and a surrogate reward function for DKB.", "result": "AIME-NoB improves sample efficiency and performance on vision-based control tasks in DeepMind Control Suite and MetaWorld benchmarks.", "conclusion": "AIME-NoB provides a robust framework for addressing ILfO challenges with pretrained models, enhancing adaptability and accuracy."}}
{"id": "2503.08585", "pdf": "https://arxiv.org/pdf/2503.08585", "abs": "https://arxiv.org/abs/2503.08585", "authors": ["Shehreen Azad", "Vibhav Vineet", "Yogesh Singh Rawat"], "title": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025", "summary": "Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis.", "AI": {"tldr": "HierarQ is a task-aware hierarchical Q-Former framework for video understanding, addressing frame sampling and context length issues in MLLMs by processing frames sequentially and using a two-stream feature modulator.", "motivation": "Current MLLMs struggle with medium-to-long video understanding due to frame sampling and context length limitations, risking loss of key information.", "method": "HierarQ uses a two-stream language-guided feature modulator (entity and scene streams) with memory banks to capture short and long-term context, bypassing frame sampling.", "result": "HierarQ achieves state-of-the-art performance on 10 video benchmarks for understanding, QA, and captioning.", "conclusion": "HierarQ is robust and efficient for comprehensive video analysis, outperforming existing methods."}}
{"id": "2501.14050", "pdf": "https://arxiv.org/pdf/2501.14050", "abs": "https://arxiv.org/abs/2501.14050", "authors": ["Jiacheng Liang", "Yuhui Wang", "Changjiang Li", "Rongyi Zhu", "Tanqiu Jiang", "Neil Gong", "Ting Wang"], "title": "GraphRAG under Fire", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "13 pages", "summary": "GraphRAG advances retrieval-augmented generation (RAG) by structuring\nexternal knowledge as multi-scale knowledge graphs, enabling language models to\nintegrate both broad context and granular details in their generation. While\nGraphRAG has demonstrated success across domains, its security implications\nremain largely unexplored. To bridge this gap, this work examines GraphRAG's\nvulnerability to poisoning attacks, uncovering an intriguing security paradox:\ncompared to conventional RAG, GraphRAG's graph-based indexing and retrieval\nenhance resilience against simple poisoning attacks; yet, the same features\nalso create new attack surfaces. We present GRAGPoison, a novel attack that\nexploits shared relations in the underlying knowledge graph to craft poisoning\ntext capable of compromising multiple queries simultaneously. GRAGPoison\nemploys three key strategies: i) relation injection to introduce false\nknowledge, ii) relation enhancement to amplify poisoning influence, and iii)\nnarrative generation to embed malicious content within coherent text. Empirical\nevaluation across diverse datasets and models shows that GRAGPoison\nsubstantially outperforms existing attacks in terms of effectiveness (up to\n98\\% success rate) and scalability (using less than 68\\% poisoning text) on\nvarious GraphRAG-based systems. We also explore potential defensive measures\nand their limitations, identifying promising directions for future research.", "AI": {"tldr": "GraphRAG improves RAG with multi-scale knowledge graphs but introduces new security vulnerabilities. GRAGPoison exploits these vulnerabilities with high success.", "motivation": "To explore GraphRAG's security implications, particularly its vulnerability to poisoning attacks, given its unexplored security risks.", "method": "The study introduces GRAGPoison, an attack leveraging shared relations in knowledge graphs, using relation injection, enhancement, and narrative generation.", "result": "GRAGPoison achieves up to 98% success rate and uses less than 68% poisoning text, outperforming existing attacks.", "conclusion": "GraphRAG's graph-based features enhance resilience but also create new attack surfaces, highlighting the need for further defensive research."}}
{"id": "2410.11539", "pdf": "https://arxiv.org/pdf/2410.11539", "abs": "https://arxiv.org/abs/2410.11539", "authors": ["M. Germ\u00e1n-Morales", "A. J. Rivera-Rivas", "M. J. del Jesus D\u00edaz", "C. J. Carmona"], "title": "Transfer Learning with Foundational Models for Time Series Forecasting using Low-Rank Adaptations", "categories": ["cs.LG"], "comment": null, "summary": "Foundational Models are an emerging widely used technique of GenAI. These\nmodels are distinguished by their scalability and the ease with which they can\nbe adapted through the exploitation of Transfer Learning. The availability of\nhigh computational power and large datasets have supported their development,\nachieving a high generalization capacity due to the enormous and heterogeneous\namounts of data used in their initial training. These characteristics\ncontribute to a solid base that can be adapted or adjusted to a wide range of\ntasks, increasing their applicability. This study proposes the methodology\nLLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for\nthe Time Series Forecasting task. An adequate time-series prompting schema and\nLow-Rank Adaptations are used to enhance the knowledge of the model with\ndiverse time series datasets, known as the fine-tuning phase. A study divided\nin two stages has been performed for evaluating the effectiveness of the\nproposed methodology. Initially, a comparison was made between the performance\nof LLIAM and different state-of-the-art DL algorithms, including Recurrent\nNeural Networks and Temporal Convolutional Networks, as well as a LLM-based\nmethod, TimeLLM. Following this, a zero-shot study is presented in order to\nevaluate the generalization capacity of the proposed methodology with time\nseries datasets from unknown domains not considered in the model training. The\noutcomes of this investigation demonstrate the efficacy of LLIAM, highlighting\nthat this straightforward and general approach can attain competent results\nwithout the necessity for applying complex modifications. This work also\nencourages the use of available resources (such as these pre-trained models)\nand efficient fine-tuning techniques to avoid unnecessary and costly training,\nnarrowing the gap between the goals of traditional AI and Green AI.", "AI": {"tldr": "LLIAM adapts Large Language Models for Time Series Forecasting using prompting and Low-Rank Adaptations, outperforming state-of-the-art methods and demonstrating strong generalization in zero-shot scenarios.", "motivation": "To leverage the scalability and adaptability of Foundational Models (FMs) for Time Series Forecasting, avoiding complex modifications and costly training.", "method": "Proposes LLIAM, combining time-series prompting and Low-Rank Adaptations for fine-tuning. Evaluated against DL algorithms and TimeLLM, followed by zero-shot testing.", "result": "LLIAM achieves competent results, outperforming state-of-the-art methods and generalizing well to unseen domains.", "conclusion": "LLIAM offers a straightforward, efficient approach for Time Series Forecasting, promoting resource reuse and aligning with Green AI goals."}}
{"id": "2503.20322", "pdf": "https://arxiv.org/pdf/2503.20322", "abs": "https://arxiv.org/abs/2503.20322", "authors": ["Hao Ai", "Kunyi Wang", "Zezhou Wang", "Hao Lu", "Jin Tian", "Yaxin Luo", "Peng Xing", "Jen-Yuan Huang", "Huaxia Li", "Gen luo"], "title": "Dynamic Pyramid Network for Efficient Multimodal Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated impressive\nperformance in various vision-language (VL) tasks, but their expensive\ncomputations still limit the real-world application. To address this issue,\nrecent efforts aim to compress the visual features to save the computational\ncosts of MLLMs. However, direct visual compression methods, e.g. efficient\nprojectors, inevitably destroy the visual semantics in MLLM, especially in\ndifficult samples. To overcome this shortcoming, we propose a novel dynamic\npyramid network (DPN) for efficient MLLMs. Specifically, DPN formulates MLLM as\na hierarchical structure where visual features are gradually compressed with\nincreasing depth. In this case, even with a high compression ratio,\nfine-grained visual information can still be perceived in shallow layers. To\nmaximize the benefit of DPN, we further propose an innovative Dynamic Pooling\nExperts (DPE) that can dynamically choose the optimal visual compression rate\naccording to input features. With this design, harder samples will be assigned\nlarger computations, thus preserving the model performance. To validate our\napproach, we conduct extensive experiments on two popular MLLMs and ten\nbenchmarks. Experimental results show that DPN can save up to 56% average FLOPs\non LLaVA while further achieving +0.74% performance gains. Besides, the\ngeneralization ability of DPN is also validated on the existing high-resolution\nMLLM called LLaVA-HR. The source code will be released at\nhttps://github.com/aihao2000/DPN-LLaVA.", "AI": {"tldr": "The paper introduces Dynamic Pyramid Network (DPN) and Dynamic Pooling Experts (DPE) to efficiently compress visual features in Multimodal Large Language Models (MLLMs) without losing fine-grained semantics, achieving significant computational savings and performance gains.", "motivation": "The high computational cost of MLLMs limits their real-world application, and existing visual compression methods degrade performance, especially in difficult samples.", "method": "DPN hierarchically compresses visual features, preserving fine-grained details in shallow layers. DPE dynamically adjusts compression rates based on input difficulty.", "result": "DPN saves 56% FLOPs on LLaVA while improving performance by +0.74%. It also generalizes well to high-resolution MLLMs like LLaVA-HR.", "conclusion": "DPN and DPE offer an efficient solution for MLLMs, balancing computational savings with performance retention."}}
{"id": "2501.18577", "pdf": "https://arxiv.org/pdf/2501.18577", "abs": "https://arxiv.org/abs/2501.18577", "authors": ["Dan M. Kluger", "Kerri Lu", "Tijana Zrnic", "Sherrie Wang", "Stephen Bates"], "title": "Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling", "categories": ["stat.ME", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Machine learning models are increasingly used to produce predictions that\nserve as input data in subsequent statistical analyses. For example, computer\nvision predictions of economic and environmental indicators based on satellite\nimagery are used in downstream regressions; similarly, language models are\nwidely used to approximate human ratings and opinions in social science\nresearch. However, failure to properly account for errors in the machine\nlearning predictions renders standard statistical procedures invalid. Prior\nwork uses what we call the Predict-Then-Debias estimator to give valid\nconfidence intervals when machine learning algorithms impute missing variables,\nassuming a small complete sample from the population of interest. We expand the\nscope by introducing bootstrap confidence intervals that apply when the\ncomplete data is a nonuniform (i.e., weighted, stratified, or clustered) sample\nand to settings where an arbitrary subset of features is imputed. Importantly,\nthe method can be applied to many settings without requiring additional\ncalculations. We prove that these confidence intervals are valid under no\nassumptions on the quality of the machine learning model and are no wider than\nthe intervals obtained by methods that do not use machine learning predictions.", "AI": {"tldr": "The paper introduces bootstrap confidence intervals for statistical analyses using machine learning predictions, addressing errors in predictions and expanding applicability to nonuniform samples and arbitrary feature imputation.", "motivation": "Machine learning predictions are increasingly used in statistical analyses, but standard procedures fail to account for prediction errors, leading to invalid results.", "method": "The paper proposes the Predict-Then-Debias estimator and extends it with bootstrap confidence intervals for nonuniform samples and arbitrary feature imputation.", "result": "The introduced confidence intervals are valid regardless of machine learning model quality and are no wider than intervals from methods ignoring predictions.", "conclusion": "The method provides a robust solution for valid statistical inference when using machine learning predictions, applicable broadly without additional computational cost."}}
{"id": "2412.11003", "pdf": "https://arxiv.org/pdf/2412.11003", "abs": "https://arxiv.org/abs/2412.11003", "authors": ["Changyu Gao", "Andrew Lowy", "Xingyu Zhou", "Stephen J. Wright"], "title": "Optimal Rates for Robust Stochastic Convex Optimization", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "The 6th annual Symposium on Foundations of Responsible Computing\n  (FORC 2025)", "summary": "Machine learning algorithms in high-dimensional settings are highly\nsusceptible to the influence of even a small fraction of structured outliers,\nmaking robust optimization techniques essential. In particular, within the\n$\\epsilon$-contamination model, where an adversary can inspect and replace up\nto an $\\epsilon$-fraction of the samples, a fundamental open problem is\ndetermining the optimal rates for robust stochastic convex optimization (SCO)\nunder such contamination. We develop novel algorithms that achieve\nminimax-optimal excess risk (up to logarithmic factors) under the\n$\\epsilon$-contamination model. Our approach improves over existing algorithms,\nwhich are not only suboptimal but also require stringent assumptions, including\nLipschitz continuity and smoothness of individual sample functions. By\ncontrast, our optimal algorithms do not require these stringent assumptions,\nassuming only population-level smoothness of the loss. Moreover, our algorithms\ncan be adapted to handle the case in which the covariance parameter is unknown,\nand can be extended to nonsmooth population risks via convolutional smoothing.\nWe complement our algorithmic developments with a tight information-theoretic\nlower bound for robust SCO.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2503.22093", "pdf": "https://arxiv.org/pdf/2503.22093", "abs": "https://arxiv.org/abs/2503.22093", "authors": ["Ximing Wen", "Mallika Mainali", "Anik Sen"], "title": "How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "4 pages, accepted by ToM@AAAI25", "summary": "Vision Language Models (VLMs) have demonstrated strong reasoning capabilities\nin Visual Question Answering (VQA) tasks; however, their ability to perform\nTheory of Mind (ToM) tasks, such as inferring human intentions, beliefs, and\nmental states, remains underexplored. We propose an open-ended question\nframework to evaluate VLMs' performance across diverse categories of ToM tasks.\nWe curated and annotated a benchmark dataset of 30 images and evaluated the\nperformance of four VLMs of varying sizes. Our results show that the GPT-4\nmodel outperformed all the others, with only one smaller model, GPT-4o-mini,\nachieving comparable performance. We observed that VLMs often struggle to infer\nintentions in complex scenarios such as bullying or cheating. Our findings\nreveal that smaller models can sometimes infer correct intentions despite\nrelying on incorrect visual cues. The dataset is available at\nhttps://github.com/ximingwen/ToM-AAAI25-Multimodal.", "AI": {"tldr": "The paper evaluates Vision Language Models (VLMs) on Theory of Mind (ToM) tasks, revealing GPT-4's superior performance and highlighting challenges in inferring intentions in complex scenarios.", "motivation": "To explore VLMs' underexplored ability in ToM tasks like inferring human intentions, beliefs, and mental states.", "method": "Proposed an open-ended question framework and curated a benchmark dataset of 30 images to evaluate four VLMs of varying sizes.", "result": "GPT-4 outperformed others, with GPT-4o-mini being comparable. VLMs struggled in complex scenarios like bullying or cheating, and smaller models sometimes inferred correct intentions despite incorrect visual cues.", "conclusion": "VLMs show promise in ToM tasks but face challenges in complex scenarios, with performance varying by model size. The dataset is publicly available for further research."}}
{"id": "2502.06425", "pdf": "https://arxiv.org/pdf/2502.06425", "abs": "https://arxiv.org/abs/2502.06425", "authors": ["Hiroki Watanabe", "Motonobu Uchikoshi"], "title": "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge Proofs and LLMs", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted to The ACM Web Conference (WWW) 2025 Short Paper Track", "summary": "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios.", "AI": {"tldr": "A framework combining zero-knowledge proof (ZKP) technology (zkVM) with LLM-based chatbots is proposed to enable privacy-preserving personalization without exposing sensitive data.", "motivation": "Personalized advice from LLMs often relies on sensitive data, raising privacy concerns. This work aims to minimize data exposure while maintaining personalization.", "method": "The framework integrates zkVM with LLM chatbots, using a novel prompting strategy to verify user traits without revealing sensitive information.", "result": "Empirical evaluation highlights the constraints and performance limitations of zkVM and the prompting strategy, showing practical feasibility.", "conclusion": "The proposed approach offers a viable solution for privacy-preserving personalization in LLM applications, though further refinement is needed."}}
{"id": "2502.00473", "pdf": "https://arxiv.org/pdf/2502.00473", "abs": "https://arxiv.org/abs/2502.00473", "authors": ["Lichen Bai", "Masashi Sugiyama", "Zeke Xie"], "title": "Weak-to-Strong Diffusion with Reflection", "categories": ["cs.LG", "cs.CV"], "comment": "23 pages, 23 figures, 15 tables", "summary": "The goal of diffusion generative models is to align the learned distribution\nwith the real data distribution through gradient score matching. However,\ninherent limitations in training data quality, modeling strategies, and\narchitectural design lead to inevitable gap between generated outputs and real\ndata. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel\nframework that utilizes the estimated difference between existing weak and\nstrong models (i.e., weak-to-strong difference) to bridge the gap between an\nideal model and a strong model. By employing a reflective operation that\nalternates between denoising and inversion with weak-to-strong difference, we\ntheoretically understand that W2SD steers latent variables along sampling\ntrajectories toward regions of the real data distribution. W2SD is highly\nflexible and broadly applicable, enabling diverse improvements through the\nstrategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5,\ngood experts vs. bad experts in MoE). Extensive experiments demonstrate that\nW2SD significantly improves human preference, aesthetic quality, and prompt\nadherence, achieving SOTA performance across various modalities (e.g., image,\nvideo), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For\nexample, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to\n90% over the original results. Moreover, the performance gains achieved by W2SD\nmarkedly outweigh its additional computational overhead, while the cumulative\nimprovements from different weak-to-strong difference further solidify its\npractical utility and deployability.", "AI": {"tldr": "W2SD reduces the gap between generated and real data by leveraging weak-to-strong model differences, improving outputs across modalities and benchmarks.", "motivation": "Existing diffusion models have a gap between generated and real data due to training and design limitations.", "method": "Proposes W2SD, a framework using weak-to-strong model differences to steer latent variables toward real data distribution via reflective operations.", "result": "W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance with minimal computational overhead.", "conclusion": "W2SD is a flexible, effective solution for enhancing diffusion models, with practical utility and deployability."}}
{"id": "2504.14450", "pdf": "https://arxiv.org/pdf/2504.14450", "abs": "https://arxiv.org/abs/2504.14450", "authors": ["Weizhi Nie", "Zichun Zhang", "Weijie Wang", "Bruno Lepri", "Anan Liu", "Nicu Sebe"], "title": "Causal Disentanglement for Robust Long-tail Medical Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Counterfactual medical image generation effectively addresses data scarcity\nand enhances the interpretability of medical images. However, due to the\ncomplex and diverse pathological features of medical images and the imbalanced\nclass distribution in medical data, generating high-quality and diverse medical\nimages from limited data is significantly challenging. Additionally, to fully\nleverage the information in limited data, such as anatomical structure\ninformation and generate more structurally stable medical images while avoiding\ndistortion or inconsistency. In this paper, in order to enhance the clinical\nrelevance of generated data and improve the interpretability of the model, we\npropose a novel medical image generation framework, which generates independent\npathological and structural features based on causal disentanglement and\nutilizes text-guided modeling of pathological features to regulate the\ngeneration of counterfactual images. First, we achieve feature separation\nthrough causal disentanglement and analyze the interactions between features.\nHere, we introduce group supervision to ensure the independence of pathological\nand identity features. Second, we leverage a diffusion model guided by\npathological findings to model pathological features, enabling the generation\nof diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging\na large language model to extract lesion severity and location from medical\nreports. Additionally, we improve the performance of the latent diffusion model\non long-tailed categories through initial noise optimization.", "AI": {"tldr": "A novel framework for generating high-quality, diverse counterfactual medical images using causal disentanglement and text-guided modeling to address data scarcity and improve interpretability.", "motivation": "To overcome challenges like complex pathological features, imbalanced data, and the need for structurally stable images while enhancing clinical relevance and model interpretability.", "method": "Uses causal disentanglement for feature separation, group supervision for independence, a diffusion model for pathological feature modeling, and a large language model for extracting lesion details. Also optimizes noise for long-tailed categories.", "result": "Generates diverse, high-quality counterfactual medical images with improved structural stability and clinical relevance.", "conclusion": "The proposed framework effectively addresses data scarcity and enhances interpretability, leveraging causal disentanglement and text-guided modeling for better medical image generation."}}
{"id": "2502.09884", "pdf": "https://arxiv.org/pdf/2502.09884", "abs": "https://arxiv.org/abs/2502.09884", "authors": ["Seo Taek Kong", "Sihan Zeng", "Thinh T. Doan", "R. Srikant"], "title": "Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We consider linear two-time-scale stochastic approximation algorithms driven\nby martingale noise. Recent applications in machine learning motivate the need\nto understand finite-time error rates, but conventional stochastic\napproximation analysis focus on either asymptotic convergence in distribution\nor finite-time bounds that are far from optimal. Prior work on asymptotic\ncentral limit theorems (CLTs) suggest that two-time-scale algorithms may be\nable to achieve $1/\\sqrt{n}$ error in expectation, with a constant given by the\nexpected norm of the limiting Gaussian vector. However, the best known\nfinite-time rates are much slower. We derive the first non-asymptotic central\nlimit theorem with respect to the Wasserstein-1 distance for two-time-scale\nstochastic approximation with Polyak-Ruppert averaging. As a corollary, we show\nthat expected error achieved by Polyak-Ruppert averaging decays at rate\n$1/\\sqrt{n}$, which significantly improves on the rates of convergence in prior\nworks.", "AI": {"tldr": "The paper analyzes finite-time error rates for two-time-scale stochastic approximation algorithms, improving prior rates to $1/\\sqrt{n}$ with Polyak-Ruppert averaging.", "motivation": "Recent machine learning applications require understanding finite-time error rates, but existing analyses are either asymptotic or suboptimal.", "method": "Derives a non-asymptotic central limit theorem for two-time-scale stochastic approximation with Polyak-Ruppert averaging.", "result": "Achieves $1/\\sqrt{n}$ error rate in expectation, significantly improving prior rates.", "conclusion": "Polyak-Ruppert averaging enables optimal finite-time convergence for two-time-scale algorithms."}}
{"id": "2502.01015", "pdf": "https://arxiv.org/pdf/2502.01015", "abs": "https://arxiv.org/abs/2502.01015", "authors": ["Siqi Zeng", "Yifei He", "Weiqiu You", "Yifan Hao", "Yao-Hung Hubert Tsai", "Makoto Yamada", "Han Zhao"], "title": "Efficient Model Editing with Task Vector Bases: A Theoretical Framework and Scalable Approach", "categories": ["cs.LG"], "comment": "27 pages, 11 figures", "summary": "Task vectors, which are derived from the difference between pre-trained and\nfine-tuned model weights, enable flexible task adaptation and model merging\nthrough arithmetic operations such as addition and negation. However, existing\napproaches often rely on heuristics with limited theoretical support, often\nleading to performance gaps comparing to direct task fine tuning. Meanwhile,\nalthough it is easy to manipulate saved task vectors with arithmetic for\ndifferent purposes, such compositional flexibility demands high memory usage,\nespecially when dealing with a huge number of tasks, limiting scalability. This\nwork addresses these issues with a theoretically grounded framework that\nexplains task vector arithmetic and introduces the task vector bases framework.\nBuilding upon existing task arithmetic literature, our method significantly\nreduces the memory cost for downstream arithmetic with little effort, while\nachieving competitive performance and maintaining compositional advantage,\nproviding a practical solution for large-scale task arithmetic. The code is\navailable at https://github.com/uiuctml/TaskVectorBasis.", "AI": {"tldr": "The paper introduces a theoretical framework for task vector arithmetic, addressing performance and scalability issues in existing methods.", "motivation": "Existing task vector approaches lack theoretical grounding, leading to performance gaps and high memory usage, especially for large-scale tasks.", "method": "The authors propose the Task Vector Bases framework, building on task arithmetic literature, to reduce memory costs and maintain performance.", "result": "The method achieves competitive performance with reduced memory usage, preserving the flexibility of task vector arithmetic.", "conclusion": "The framework provides a practical solution for large-scale task arithmetic, balancing performance and scalability."}}
{"id": "2504.15007", "pdf": "https://arxiv.org/pdf/2504.15007", "abs": "https://arxiv.org/abs/2504.15007", "authors": ["David C Wong", "Bin Wang", "Gorkem Durak", "Marouane Tliba", "Mohamed Amine Kerkouri", "Aladine Chetouani", "Ahmet Enis Cetin", "Cagdas Topel", "Nicolo Gennaro", "Camila Vendrami", "Tugce Agirlar Trabzonlu", "Amir Ali Rahsepar", "Laetitia Perronne", "Matthew Antalek", "Onural Ozturk", "Gokcan Okur", "Andrew C. Gordon", "Ayis Pyrros", "Frank H Miller", "Amir A Borhani", "Hatice Savas", "Eric M. Hart", "Elizabeth A Krupinski", "Ulas Bagci"], "title": "Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical Images", "categories": ["cs.CV", "cs.HC"], "comment": "This paper was accepted at ETRA 2025 Japan", "summary": "Eye-tracking analysis plays a vital role in medical imaging, providing key\ninsights into how radiologists visually interpret and diagnose clinical cases.\nIn this work, we first analyze radiologists' attention and agreement by\nmeasuring the distribution of various eye-movement patterns, including saccades\ndirection, amplitude, and their joint distribution. These metrics help uncover\npatterns in attention allocation and diagnostic strategies. Furthermore, we\ninvestigate whether and how doctors' gaze behavior shifts when viewing\nauthentic (Real) versus deep-learning-generated (Fake) images. To achieve this,\nwe examine fixation bias maps, focusing on first, last, short, and longest\nfixations independently, along with detailed saccades patterns, to quantify\ndifferences in gaze distribution and visual saliency between authentic and\nsynthetic images.", "AI": {"tldr": "The paper analyzes radiologists' eye-tracking patterns to understand attention and agreement in medical imaging, comparing gaze behavior between real and deep-learning-generated images.", "motivation": "To uncover insights into how radiologists visually interpret and diagnose clinical cases, and to explore differences in gaze behavior between real and synthetic images.", "method": "Analyzed eye-movement patterns (saccades direction, amplitude, joint distribution) and fixation bias maps (first, last, short, longest fixations) to quantify attention allocation and differences in gaze distribution.", "result": "Revealed patterns in radiologists' attention and diagnostic strategies, and quantified differences in gaze behavior between real and synthetic images.", "conclusion": "Eye-tracking analysis provides valuable insights into radiologists' visual interpretation and highlights distinct gaze behaviors for real versus synthetic images."}}
{"id": "2502.11658", "pdf": "https://arxiv.org/pdf/2502.11658", "abs": "https://arxiv.org/abs/2502.11658", "authors": ["Antoine Boutet", "Victor Morel"], "title": "\"I'm not for sale\" -- Perceptions and limited awareness of privacy risks by digital natives about location data", "categories": ["cs.CY", "cs.AI", "cs.CR"], "comment": "Accepted for publication at ICWSM2025", "summary": "Although mobile devices benefit users in their daily lives in numerous ways,\nthey also raise several privacy concerns. For instance, they can reveal\nsensitive information that can be inferred from location data. This location\ndata is shared through service providers as well as mobile applications.\nUnderstanding how and with whom users share their location data -- as well as\nusers' perception of the underlying privacy risks --, are important notions to\ngrasp in order to design usable privacy-enhancing technologies. In this work,\nwe perform a quantitative and qualitative analysis of smartphone users'\nawareness, perception and self-reported behavior towards location data-sharing\nthrough a survey of n=99 young adult participants (i.e., digital natives). We\ncompare stated practices with actual behaviors to better understand their\nmental models, and survey participants' understanding of privacy risks before\nand after the inspection of location traces and the information that can be\ninferred therefrom.\n  Our empirical results show that participants have risky privacy practices:\nabout 54% of participants underestimate the number of mobile applications to\nwhich they have granted access to their data, and 33% forget or do not think of\nrevoking access to their data. Also, by using a demonstrator to perform\ninferences from location data, we observe that slightly more than half of\nparticipants (57%) are surprised by the extent of potentially inferred\ninformation, and that 47% intend to reduce access to their data via permissions\nas a result of using the demonstrator. Last, a majority of participants have\nlittle knowledge of the tools to better protect themselves, but are nonetheless\nwilling to follow suggestions to improve privacy (51%). Educating people,\nincluding digital natives, about privacy risks through transparency tools seems\na promising approach.", "AI": {"tldr": "The paper examines smartphone users' awareness and behavior regarding location data-sharing, revealing gaps in privacy practices and the effectiveness of educational tools.", "motivation": "To understand users' awareness, perception, and behavior regarding location data-sharing and privacy risks, aiming to design better privacy-enhancing technologies.", "method": "A survey of 99 young adults (digital natives) combined with quantitative and qualitative analysis, comparing stated practices with actual behaviors and using a demonstrator to show inferred information.", "result": "Participants showed risky privacy practices: 54% underestimated app access to their data, 33% forgot to revoke access, and 57% were surprised by inferred data. 47% intended to reduce data access after using the demonstrator.", "conclusion": "Educating users, even digital natives, about privacy risks via transparency tools is promising, as many lack knowledge but are willing to improve privacy practices."}}
{"id": "2502.17060", "pdf": "https://arxiv.org/pdf/2502.17060", "abs": "https://arxiv.org/abs/2502.17060", "authors": ["Andreas Loizou", "Dimitrios Tsoumakos"], "title": "Data Analysis Prediction over Multiple Unseen Datasets: A Vector Embedding Approach", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The massive increase in the data volume and dataset availability for analysts\ncompels researchers to focus on data content and select high-quality datasets\nto enhance the performance of analytics operators. While selecting the highest\nquality data for analysis highly increases task accuracy and efficiency, it is\nstill a hard task, especially when the number of available inputs is very\nlarge. To address this issue, we propose a novel methodology that infers the\noutcome of analytics operators by creating a model from datasets similar to the\nqueried one. Dataset similarity is performed via projecting each dataset to a\nvector embedding representation. The vectorization process is performed using\nour proposed deep learning model NumTabData2Vec, which takes a whole dataset\nand projects it into a lower vector embedding representation space. Through\nexperimental evaluation, we compare the prediction performance and the\nexecution time of our framework to another state-of-the-art modelling operator\nframework, illustrating that our approach predicts analytics outcomes\naccurately. Furthermore, our vectorization model can project different\nreal-world scenarios to a lower vector embedding representation and distinguish\nbetween them.", "AI": {"tldr": "A novel deep learning method, NumTabData2Vec, is proposed to select high-quality datasets by projecting them into vector embeddings, improving analytics accuracy and efficiency.", "motivation": "The challenge of selecting high-quality datasets from large volumes to enhance analytics performance drives the need for an automated solution.", "method": "The approach involves dataset vectorization using NumTabData2Vec, a deep learning model, to infer analytics outcomes by comparing similar datasets.", "result": "The method accurately predicts analytics outcomes and efficiently distinguishes between real-world scenarios in lower-dimensional vector space.", "conclusion": "NumTabData2Vec effectively addresses dataset selection challenges, offering improved accuracy and efficiency in analytics tasks."}}
{"id": "2504.15681", "pdf": "https://arxiv.org/pdf/2504.15681", "abs": "https://arxiv.org/abs/2504.15681", "authors": ["Vidi Team", "Celong Liu", "Chia-Wen Kuo", "Dawei Du", "Fan Chen", "Guang Chen", "Jiamin Yuan", "Lingxi Zhang", "Lu Guo", "Lusha Li", "Longyin Wen", "Qingyu Chen", "Rachel Deng", "Sijie Zhu", "Stuart Siew", "Tong Jin", "Wei Lu", "Wen Zhong", "Xiaohui Shen", "Xin Gu", "Xing Mei", "Xueqiong Qu"], "title": "Vidi: Large Multimodal Models for Video Understanding and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than videos of existing temporal retrival\ndatasets, 2) Audio support: includes audio-based queries, 3) Query format:\ndiverse query lengths/formats, 4) Annotation quality: ground-truth time ranges\nare manually annotated. 5) Evaluation metric: a refined IoU metric to support\nevaluation over multiple time ranges. Remarkably, Vidi significantly\noutperforms leading proprietary models, e.g., GPT-4o and Gemini, on the\ntemporal retrieval task, indicating its superiority in video editing scenarios.", "AI": {"tldr": "Vidi, a Large Multimodal Model (LMM), excels in temporal retrieval for video editing, outperforming proprietary models like GPT-4o and Gemini.", "motivation": "To address challenges in processing multimodal, long-duration videos for intelligent editing, requiring strong temporal understanding.", "method": "Introduces Vidi, an LMM for temporal retrieval, and VUE-TR benchmark with longer videos, audio support, diverse queries, high-quality annotations, and refined metrics.", "result": "Vidi significantly outperforms leading models (e.g., GPT-4o, Gemini) in temporal retrieval tasks.", "conclusion": "Vidi demonstrates superior capability in video editing scenarios, supported by the comprehensive VUE-TR benchmark."}}
{"id": "2503.00234", "pdf": "https://arxiv.org/pdf/2503.00234", "abs": "https://arxiv.org/abs/2503.00234", "authors": ["Lukasz Sztukiewicz", "Ignacy St\u0119pka", "Micha\u0142 Wili\u0144ski", "Jerzy Stefanowski"], "title": "Investigating the Relationship Between Debiasing and Artifact Removal using Saliency Maps", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "The widespread adoption of machine learning systems has raised critical\nconcerns about fairness and bias, making mitigating harmful biases essential\nfor AI development. In this paper, we investigate the relationship between\ndebiasing and removing artifacts in neural networks for computer vision tasks.\nFirst, we introduce a set of novel XAI-based metrics that analyze saliency maps\nto assess shifts in a model's decision-making process. Then, we demonstrate\nthat successful debiasing methods systematically redirect model focus away from\nprotected attributes. Finally, we show that techniques originally developed for\nartifact removal can be effectively repurposed for improving fairness. These\nfindings provide evidence for the existence of a bidirectional connection\nbetween ensuring fairness and removing artifacts corresponding to protected\nattributes.", "AI": {"tldr": "The paper explores the link between debiasing and artifact removal in neural networks for computer vision, using XAI-based metrics to show that debiasing shifts focus away from protected attributes and that artifact removal techniques can enhance fairness.", "motivation": "Addressing fairness and bias in AI is critical, and understanding the relationship between debiasing and artifact removal can improve AI development.", "method": "Introduces XAI-based metrics to analyze saliency maps and evaluates debiasing methods that redirect model focus from protected attributes.", "result": "Debiasing methods shift model focus away from protected attributes, and artifact removal techniques can be repurposed to improve fairness.", "conclusion": "There is a bidirectional connection between fairness and artifact removal, suggesting shared solutions for both challenges."}}
{"id": "2503.21495", "pdf": "https://arxiv.org/pdf/2503.21495", "abs": "https://arxiv.org/abs/2503.21495", "authors": ["Timo Budszuhn", "Mark Joachim Krallmann", "Daniel Horn"], "title": "Adaptive Resampling with Bootstrap for Noisy Multi-Objective Optimization Problems", "categories": ["cs.LG", "cs.AI", "stat.ML", "90C29", "G.1.6"], "comment": "14 pages. 5 figures", "summary": "The challenge of noisy multi-objective optimization lies in the constant\ntrade-off between exploring new decision points and improving the precision of\nknown points through resampling. This decision should take into account both\nthe variability of the objective functions and the current estimate of a point\nin relation to the Pareto front. Since the amount and distribution of noise are\ngenerally unknown, it is desirable for a decision function to be highly\nadaptive to the properties of the optimization problem. This paper presents a\nresampling decision function that incorporates the stochastic nature of the\noptimization problem by using bootstrapping and the probability of dominance.\nThe distribution-free estimation of the probability of dominance is achieved\nusing bootstrap estimates of the means. To make the procedure applicable even\nwith very few observations, we transfer the distribution observed at other\ndecision points. The efficiency of this resampling approach is demonstrated by\napplying it in the NSGA-II algorithm with a sequential resampling procedure\nunder multiple noise variations.", "AI": {"tldr": "A resampling decision function for noisy multi-objective optimization is proposed, using bootstrapping and dominance probability, adaptable to unknown noise properties.", "motivation": "Address the trade-off between exploration and precision in noisy multi-objective optimization, considering unknown noise distribution.", "method": "Uses bootstrapping and dominance probability, transferring observed distributions from other points for adaptability with few observations.", "result": "Demonstrated efficiency when integrated into NSGA-II with sequential resampling under various noise conditions.", "conclusion": "The proposed method effectively balances exploration and precision in noisy optimization, adapting to unknown noise properties."}}
{"id": "2504.15929", "pdf": "https://arxiv.org/pdf/2504.15929", "abs": "https://arxiv.org/abs/2504.15929", "authors": ["Saban Ozturk", "Melih B. Yilmaz", "Muti Kara", "M. Talat Yavuz", "Aykut Ko\u00e7", "Tolga \u00c7ukur"], "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods.", "AI": {"tldr": "MedTrim enhances image-text alignment in medical vision-language models by leveraging multimodal triplet learning and structured meta-entity information, improving performance in retrieval and classification tasks.", "motivation": "The growing volume of diagnostic imaging data increases errors and workflow backlogs, highlighting the need for better alignment methods in medical vision-language models to capture fine-grained pathology attributes.", "method": "MedTrim uses ontology-based entity recognition to extract meta-entities from reports, a novel score function for refined triplet mining, and a multimodal triplet alignment objective for detailed pathology alignment.", "result": "MedTrim outperforms existing methods in downstream tasks like retrieval and classification by preserving clinically significant intra-class variations.", "conclusion": "MedTrim offers a robust framework for aligning image and text representations in medical imaging, addressing limitations of current contrastive learning methods."}}
{"id": "2504.13192", "pdf": "https://arxiv.org/pdf/2504.13192", "abs": "https://arxiv.org/abs/2504.13192", "authors": ["Liang-bo Ning", "Shijie Wang", "Wenqi Fan", "Qing Li", "Xin Xu", "Hao Chen", "Feiran Huang"], "title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted by KDD 2024;", "summary": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys)\nhave brought significant advances in personalized user experience and have\nattracted considerable attention. Despite the impressive progress, the research\nquestion regarding the safety vulnerability of LLM-empowered RecSys still\nremains largely under-investigated. Given the security and privacy concerns, it\nis more practical to focus on attacking the black-box RecSys, where attackers\ncan only observe the system's inputs and outputs. However, traditional attack\napproaches employing reinforcement learning (RL) agents are not effective for\nattacking LLM-empowered RecSys due to the limited capabilities in processing\ncomplex textual inputs, planning, and reasoning. On the other hand, LLMs\nprovide unprecedented opportunities to serve as attack agents to attack RecSys\nbecause of their impressive capability in simulating human-like decision-making\nprocesses. Therefore, in this paper, we propose a novel attack framework called\nCheatAgent by harnessing the human-like capabilities of LLMs, where an\nLLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our\nmethod first identifies the insertion position for maximum impact with minimal\ninput modification. After that, the LLM agent is designed to generate\nadversarial perturbations to insert at target positions. To further improve the\nquality of generated perturbations, we utilize the prompt tuning technique to\nimprove attacking strategies via feedback from the victim RecSys iteratively.\nExtensive experiments across three real-world datasets demonstrate the\neffectiveness of our proposed attacking method.", "AI": {"tldr": "The paper introduces CheatAgent, an LLM-based attack framework targeting LLM-empowered recommender systems, leveraging LLMs' human-like decision-making to generate adversarial perturbations.", "motivation": "To address the under-investigated safety vulnerabilities in LLM-empowered recommender systems, focusing on black-box attacks where attackers can only observe inputs and outputs.", "method": "Proposes CheatAgent, an LLM-based agent that identifies optimal insertion positions for adversarial perturbations and uses prompt tuning to refine attacks iteratively.", "result": "Demonstrates effectiveness through extensive experiments on three real-world datasets.", "conclusion": "CheatAgent successfully exploits LLMs' capabilities to attack recommender systems, highlighting the need for improved security measures."}}
{"id": "2504.01482", "pdf": "https://arxiv.org/pdf/2504.01482", "abs": "https://arxiv.org/abs/2504.01482", "authors": ["Qihao Ye", "Xiaochuan Tian", "Yuhua Zhu"], "title": "A Robust Model-Based Approach for Continuous-Time Policy Evaluation with Unknown L\u00e9vy Process Dynamics", "categories": ["cs.LG", "cs.NA", "math.NA", "65R20, 62M05, 35R09, 60H35, 93E35, 90C40, 68T05"], "comment": "28 pages, 9 figures", "summary": "This paper develops a model-based framework for continuous-time policy\nevaluation (CTPE) in reinforcement learning, incorporating both Brownian and\nL\\'evy noise to model stochastic dynamics influenced by rare and extreme\nevents. Our approach formulates the policy evaluation problem as solving a\npartial integro-differential equation (PIDE) for the value function with\nunknown coefficients. A key challenge in this setting is accurately recovering\nthe unknown coefficients in the stochastic dynamics, particularly when driven\nby L\\'evy processes with heavy tail effects. To address this, we propose a\nrobust numerical approach that effectively handles both unbiased and censored\ntrajectory datasets. This method combines maximum likelihood estimation with an\niterative tail correction mechanism, improving the stability and accuracy of\ncoefficient recovery. Additionally, we establish a theoretical bound for the\npolicy evaluation error based on coefficient recovery error. Through numerical\nexperiments, we demonstrate the effectiveness and robustness of our method in\nrecovering heavy-tailed L\\'evy dynamics and verify the theoretical error\nanalysis in policy evaluation.", "AI": {"tldr": "A model-based framework for continuous-time policy evaluation (CTPE) in reinforcement learning is proposed, incorporating Brownian and L\u00e9vy noise to handle rare and extreme events. The method solves a PIDE for the value function and uses a robust numerical approach for coefficient recovery.", "motivation": "To address the challenge of accurately recovering unknown coefficients in stochastic dynamics, especially under heavy-tailed L\u00e9vy processes, and improve policy evaluation in reinforcement learning.", "method": "The approach formulates policy evaluation as solving a PIDE and combines maximum likelihood estimation with an iterative tail correction mechanism for robust coefficient recovery.", "result": "Numerical experiments show the method's effectiveness in recovering heavy-tailed L\u00e9vy dynamics and validate the theoretical error analysis for policy evaluation.", "conclusion": "The proposed framework provides a robust and accurate solution for policy evaluation in stochastic environments with rare and extreme events."}}
{"id": "2504.16443", "pdf": "https://arxiv.org/pdf/2504.16443", "abs": "https://arxiv.org/abs/2504.16443", "authors": ["Duy-Tho Le", "Trung Pham", "Jianfei Cai", "Hamid Rezatofighi"], "title": "Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Optimizing the similarity between parametric shapes is crucial for numerous\ncomputer vision tasks, where Intersection over Union (IoU) stands as the\ncanonical measure. However, existing optimization methods exhibit significant\nshortcomings: regression-based losses like L1/L2 lack correlation with IoU,\nIoU-based losses are unstable and limited to simple shapes, and task-specific\nmethods are computationally intensive and not generalizable accross domains. As\na result, the current landscape of parametric shape objective functions has\nbecome scattered, with each domain proposing distinct IoU approximations. To\naddress this, we unify the parametric shape optimization objective functions by\nintroducing Marginalized Generalized IoU (MGIoU), a novel loss function that\novercomes these challenges by projecting structured convex shapes onto their\nunique shape Normals to compute one-dimensional normalized GIoU. MGIoU offers a\nsimple, efficient, fully differentiable approximation strongly correlated with\nIoU. We then extend MGIoU to MGIoU+ that supports optimizing unstructured\nconvex shapes. Together, MGIoU and MGIoU+ unify parametric shape optimization\nacross diverse applications. Experiments on standard benchmarks demonstrate\nthat MGIoU and MGIoU+ consistently outperform existing losses while reducing\nloss computation latency by 10-40x. Additionally, MGIoU and MGIoU+ satisfy\nmetric properties and scale-invariance, ensuring robustness as an objective\nfunction. We further propose MGIoU- for minimizing overlaps in tasks like\ncollision-free trajectory prediction. Code is available at\nhttps://ldtho.github.io/MGIoU", "AI": {"tldr": "The paper introduces Marginalized Generalized IoU (MGIoU) and its variants (MGIoU+, MGIoU-) to unify and improve parametric shape optimization, offering efficiency, strong correlation with IoU, and robustness across diverse applications.", "motivation": "Existing optimization methods for parametric shapes suffer from issues like lack of correlation with IoU, instability, computational intensity, and domain-specific limitations, leading to a fragmented landscape of objective functions.", "method": "The authors propose MGIoU, which projects structured convex shapes onto their unique shape Normals to compute one-dimensional normalized GIoU, and extend it to MGIoU+ for unstructured convex shapes and MGIoU- for overlap minimization.", "result": "Experiments show MGIoU and MGIoU+ outperform existing losses, reduce computation latency by 10-40x, and satisfy metric properties and scale-invariance.", "conclusion": "MGIoU and its variants provide a unified, efficient, and robust solution for parametric shape optimization across diverse applications."}}
{"id": "2504.13199", "pdf": "https://arxiv.org/pdf/2504.13199", "abs": "https://arxiv.org/abs/2504.13199", "authors": ["Mohammad Saleh", "Azadeh Tabatabaei"], "title": "Building Trustworthy Multimodal AI: A Review of Fairness, Transparency, and Ethics in Vision-Language Tasks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework.", "AI": {"tldr": "This review examines trustworthiness in multimodal AI systems for vision-language tasks, focusing on fairness, transparency, and ethics. It compares key tasks like VQA and image captioning, highlighting challenges and solutions.", "motivation": "The study aims to address critical concerns about fairness, transparency, and ethical implications in vision-language AI systems, which are increasingly important as these models advance.", "method": "The review analyzes research from 2017 to 2024 on vision-language tasks, using a comparative approach to evaluate trustworthiness, fairness, explainability, and ethics.", "result": "Key findings include the importance of explainability (e.g., attention maps), fairness in bias mitigation, and ethical data handling for responsible deployment.", "conclusion": "The study emphasizes integrating fairness, transparency, and ethics into vision-language models for trustworthy AI development."}}
{"id": "2504.04318", "pdf": "https://arxiv.org/pdf/2504.04318", "abs": "https://arxiv.org/abs/2504.04318", "authors": ["Mehmet Can Yavuz", "Berrin Yanikoglu"], "title": "Variational Self-Supervised Learning", "categories": ["cs.LG", "cs.CV"], "comment": "NeurIPS 2025 - SSL Workshop Submission", "summary": "We present Variational Self-Supervised Learning (VSSL), a novel framework\nthat combines variational inference with self-supervised learning to enable\nefficient, decoder-free representation learning. Unlike traditional VAEs that\nrely on input reconstruction via a decoder, VSSL symmetrically couples two\nencoders with Gaussian outputs. A momentum-updated teacher network defines a\ndynamic, data-dependent prior, while the student encoder produces an\napproximate posterior from augmented views. The reconstruction term in the ELBO\nis replaced with a cross-view denoising objective, preserving the analytical\ntractability of Gaussian KL divergence. We further introduce cosine-based\nformulations of KL and log-likelihood terms to enhance semantic alignment in\nhigh-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and\nImageNet-100 show that VSSL achieves competitive or superior performance to\nleading self-supervised methods, including BYOL and MoCo V3. VSSL offers a\nscalable, probabilistically grounded approach to learning transferable\nrepresentations without generative reconstruction, bridging the gap between\nvariational modeling and modern self-supervised techniques.", "AI": {"tldr": "VSSL combines variational inference and self-supervised learning for efficient, decoder-free representation learning, outperforming methods like BYOL and MoCo V3.", "motivation": "To bridge the gap between variational modeling and self-supervised learning by eliminating the need for generative reconstruction.", "method": "Uses two symmetrically coupled encoders with Gaussian outputs, a teacher-student setup, and replaces ELBO's reconstruction term with cross-view denoising.", "result": "Achieves competitive or superior performance on CIFAR-10, CIFAR-100, and ImageNet-100.", "conclusion": "VSSL provides a scalable, probabilistically grounded approach for learning transferable representations without generative reconstruction."}}
{"id": "2504.16727", "pdf": "https://arxiv.org/pdf/2504.16727", "abs": "https://arxiv.org/abs/2504.16727", "authors": ["Zhiyuan Fan", "Yumeng Wang", "Sandeep Polisetty", "Yi R. Fung"], "title": "V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision Language Models (LVLMs) excel in various vision-language tasks.\nYet, their robustness to visual variations in position, scale, orientation, and\ncontext that objects in natural scenes inevitably exhibit due to changes in\nviewpoint and environment remains largely underexplored. To bridge this gap, we\nintroduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating\nVisual Variation Robustness of LVLMs, which encompasses automated evaluation\ndataset generation and principled metrics for thorough robustness assessment.\nThrough extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability\nto visual variations, in which even advanced models that excel at complex\nvision-language tasks significantly underperform on simple tasks such as object\nrecognition. Interestingly, these models exhibit a distinct visual position\nbias that contradicts theories of effective receptive fields, and demonstrate a\nhuman-like visual acuity threshold. To identify the source of these\nvulnerabilities, we present a systematic framework for component-level\nanalysis, featuring a novel visualization approach for aligned visual features.\nResults show that these vulnerabilities stem from error accumulation in the\npipeline architecture and inadequate multimodal alignment. Complementary\nexperiments with synthetic data further demonstrate that these limitations are\nfundamentally architectural deficiencies, scoring the need for architectural\ninnovations in future LVLM designs.", "AI": {"tldr": "V\u00b2R-Bench is introduced to evaluate LVLMs' robustness to visual variations, revealing vulnerabilities in object recognition and position bias, attributed to pipeline errors and poor multimodal alignment.", "motivation": "To assess LVLMs' robustness to visual variations like position, scale, and orientation, which are underexplored despite their prevalence in natural scenes.", "method": "Developed V\u00b2R-Bench, a benchmark framework with automated dataset generation and metrics, evaluated 21 LVLMs, and analyzed vulnerabilities through component-level analysis and synthetic data experiments.", "result": "LVLMs show surprising vulnerability to visual variations, exhibiting position bias and human-like acuity thresholds, with issues traced to pipeline errors and inadequate multimodal alignment.", "conclusion": "The findings highlight architectural deficiencies in LVLMs, calling for innovations to improve robustness to visual variations."}}
{"id": "2504.13941", "pdf": "https://arxiv.org/pdf/2504.13941", "abs": "https://arxiv.org/abs/2504.13941", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "Matvei Novikov", "Seungju Han", "Ying Lin", "Evelina Bakhturina", "Eric Nyberg", "Yejin Choi", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "title": "Nemotron-CrossThink: Scaling Self-Learning beyond Math Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs.", "AI": {"tldr": "NEMOTRON-CROSSTHINK improves LLM reasoning by integrating multi-domain data and structured templates in RL training, achieving higher accuracy and efficiency.", "motivation": "Generalizing RL methods for LLMs beyond math is challenging due to limited data, unclear rewards, and diverse tasks.", "method": "Incorporates varied data sources, structured templates, verifiable answers, and optimized blending strategies.", "result": "Boosts accuracy on math (+30.1%) and non-math benchmarks (+12.8%) while reducing token usage by 28%.", "conclusion": "Multi-domain, multi-format RL training enhances LLM accuracy, efficiency, and generalization."}}
{"id": "2504.06398", "pdf": "https://arxiv.org/pdf/2504.06398", "abs": "https://arxiv.org/abs/2504.06398", "authors": ["Saber Malekmohammadi", "Hong kyu Lee", "Li Xiong"], "title": "Sharpness-Aware Parameter Selection for Machine Unlearning", "categories": ["cs.LG"], "comment": null, "summary": "It often happens that some sensitive personal information, such as credit\ncard numbers or passwords, are mistakenly incorporated in the training of\nmachine learning models and need to be removed afterwards. The removal of such\ninformation from a trained model is a complex task that needs to partially\nreverse the training process. There have been various machine unlearning\ntechniques proposed in the literature to address this problem. Most of the\nproposed methods revolve around removing individual data samples from a trained\nmodel. Another less explored direction is when features/labels of a group of\ndata samples need to be reverted. While the existing methods for these tasks do\nthe unlearning task by updating the whole set of model parameters or only the\nlast layer of the model, we show that there are a subset of model parameters\nthat have the largest contribution in the unlearning target features. More\nprecisely, the model parameters with the largest corresponding diagonal value\nin the Hessian matrix (computed at the learned model parameter) have the most\ncontribution in the unlearning task. By selecting these parameters and updating\nthem during the unlearning stage, we can have the most progress in unlearning.\nWe provide theoretical justifications for the proposed strategy by connecting\nit to sharpness-aware minimization and robust unlearning. We empirically show\nthe effectiveness of the proposed strategy in improving the efficacy of\nunlearning with a low computational cost.", "AI": {"tldr": "The paper proposes a method for efficient machine unlearning by identifying and updating the most influential model parameters, reducing computational cost while maintaining effectiveness.", "motivation": "Sensitive data mistakenly included in training models needs removal, but existing unlearning methods are inefficient or computationally expensive.", "method": "Identify model parameters with the largest diagonal Hessian values, as they contribute most to unlearning, and update only these.", "result": "The method improves unlearning efficacy with lower computational cost, supported by theoretical and empirical evidence.", "conclusion": "Targeted parameter updates based on Hessian values offer an efficient and effective solution for machine unlearning."}}
{"id": "2412.15576", "pdf": "https://arxiv.org/pdf/2412.15576", "abs": "https://arxiv.org/abs/2412.15576", "authors": ["Xinyang Tong", "Pengxiang Ding", "Yiguo Fan", "Donglin Wang", "Wenjie Zhang", "Can Cui", "Mingyang Sun", "Han Zhao", "Hongyin Zhang", "Yonghao Dang", "Siteng Huang", "Shangke Lyu"], "title": "QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to ICRA 2025; Github page: https://quart-online.github.io", "summary": "This paper addresses the inherent inference latency challenges associated\nwith deploying multimodal large language models (MLLM) in quadruped\nvision-language-action (QUAR-VLA) tasks. Our investigation reveals that\nconventional parameter reduction techniques ultimately impair the performance\nof the language foundation model during the action instruction tuning phase,\nmaking them unsuitable for this purpose. We introduce a novel latency-free\nquadruped MLLM model, dubbed QUART-Online, designed to enhance inference\nefficiency without degrading the performance of the language foundation model.\nBy incorporating Action Chunk Discretization (ACD), we compress the original\naction representation space, mapping continuous action values onto a smaller\nset of discrete representative vectors while preserving critical information.\nSubsequently, we fine-tune the MLLM to integrate vision, language, and\ncompressed actions into a unified semantic space. Experimental results\ndemonstrate that QUART-Online operates in tandem with the existing MLLM system,\nachieving real-time inference in sync with the underlying controller frequency,\nsignificantly boosting the success rate across various tasks by 65%. Our\nproject page is https://quart-online.github.io.", "AI": {"tldr": "QUART-Online is a latency-free MLLM model for QUAR-VLA tasks, improving inference efficiency without performance loss via Action Chunk Discretization and unified semantic space integration.", "motivation": "Addressing inference latency in MLLMs for QUAR-VLA tasks, as conventional parameter reduction harms performance.", "method": "Introduces QUART-Online with Action Chunk Discretization (ACD) to compress action space and fine-tunes MLLM for unified vision-language-action integration.", "result": "Achieves real-time inference, boosting task success rates by 65%.", "conclusion": "QUART-Online effectively balances efficiency and performance in QUAR-VLA tasks."}}
{"id": "2504.15773", "pdf": "https://arxiv.org/pdf/2504.15773", "abs": "https://arxiv.org/abs/2504.15773", "authors": ["Cong Liu", "Sharvaree Vadgama", "David Ruhe", "Erik Bekkers", "Patrick Forr\u00e9"], "title": "Clifford Group Equivariant Diffusion Models for 3D Molecular Generation", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 1 figure, 1 table", "summary": "This paper explores leveraging the Clifford algebra's expressive power for\n$\\E(n)$-equivariant diffusion models. We utilize the geometric products between\nClifford multivectors and the rich geometric information encoded in Clifford\nsubspaces in \\emph{Clifford Diffusion Models} (CDMs). We extend the diffusion\nprocess beyond just Clifford one-vectors to incorporate all higher-grade\nmultivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us\nto apply latent diffusion across complete multivectors. This enables CDMs to\ncapture the joint distribution across different subspaces of the algebra,\nincorporating richer geometric information through higher-order features. We\nprovide empirical results for unconditional molecular generation on the QM9\ndataset, showing that CDMs provide a promising avenue for generative modeling.", "AI": {"tldr": "The paper introduces Clifford Diffusion Models (CDMs), leveraging Clifford algebra for E(n)-equivariant diffusion models to capture richer geometric information through higher-order multivector subspaces.", "motivation": "To enhance generative modeling by incorporating richer geometric information from Clifford algebra's multivector subspaces into diffusion models.", "method": "Extends diffusion to higher-grade multivectors in Clifford algebra, embedding data in grade-k subspaces and applying latent diffusion across complete multivectors.", "result": "Empirical results on QM9 dataset show CDMs are promising for unconditional molecular generation.", "conclusion": "CDMs offer a novel approach for generative modeling by utilizing Clifford algebra's expressive power."}}
{"id": "2504.06768", "pdf": "https://arxiv.org/pdf/2504.06768", "abs": "https://arxiv.org/abs/2504.06768", "authors": ["Shutong Chen", "Tianyi Zhou", "Guodong Long", "Jing Jiang", "Chengqi Zhang"], "title": "FedMerge: Federated Personalization via Model Merging", "categories": ["cs.LG"], "comment": null, "summary": "One global model in federated learning (FL) might not be sufficient to serve\nmany clients with non-IID tasks and distributions. While there has been\nadvances in FL to train multiple global models for better personalization, they\nonly provide limited choices to clients so local finetuning is still\nindispensable. In this paper, we propose a novel ``FedMerge'' approach that can\ncreate a personalized model per client by simply merging multiple global models\nwith automatically optimized and customized weights. In FedMerge, a few global\nmodels can serve many non-IID clients, even without further local finetuning.\nWe formulate this problem as a joint optimization of global models and the\nmerging weights for each client. Unlike existing FL approaches where the server\nbroadcasts one or multiple global models to all clients, the server only needs\nto send a customized, merged model to each client. Moreover, instead of\nperiodically interrupting the local training and re-initializing it to a global\nmodel, the merged model aligns better with each client's task and data\ndistribution, smoothening the local-global gap between consecutive rounds\ncaused by client drift. We evaluate FedMerge on three different non-IID\nsettings applied to different domains with diverse tasks and data types, in\nwhich FedMerge consistently outperforms existing FL approaches, including\nclustering-based and mixture-of-experts (MoE) based methods.", "AI": {"tldr": "FedMerge proposes a personalized model per client in federated learning by merging multiple global models with optimized weights, eliminating the need for local finetuning.", "motivation": "Existing FL methods provide limited choices for non-IID clients, requiring local finetuning. FedMerge aims to address this by creating customized models.", "method": "FedMerge jointly optimizes global models and merging weights per client, sending a merged model to each client instead of broadcasting global models.", "result": "FedMerge outperforms existing FL approaches in non-IID settings across diverse tasks and data types.", "conclusion": "FedMerge effectively bridges the local-global gap, offering better personalization without local finetuning."}}
{"id": "2504.14634", "pdf": "https://arxiv.org/pdf/2504.14634", "abs": "https://arxiv.org/abs/2504.14634", "authors": ["Sahara Sheikholeslami", "Ladislau B\u00f6l\u00f6ni"], "title": "Latent Representations for Visual Proprioception in Inexpensive Robots", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robotic manipulation requires explicit or implicit knowledge of the robot's\njoint positions. Precise proprioception is standard in high-quality industrial\nrobots but is often unavailable in inexpensive robots operating in unstructured\nenvironments. In this paper, we ask: to what extent can a fast, single-pass\nregression architecture perform visual proprioception from a single external\ncamera image, available even in the simplest manipulation settings? We explore\nseveral latent representations, including CNNs, VAEs, ViTs, and bags of\nuncalibrated fiducial markers, using fine-tuning techniques adapted to the\nlimited data available. We evaluate the achievable accuracy through experiments\non an inexpensive 6-DoF robot.", "AI": {"tldr": "A study explores visual proprioception for inexpensive robots using single-camera images, testing various latent representations and achieving notable accuracy.", "motivation": "Precise proprioception is often unavailable in low-cost robots, limiting their use in unstructured environments. This paper investigates whether visual proprioception from a single external camera can bridge this gap.", "method": "The study employs fast, single-pass regression architectures, testing CNN, VAE, ViT, and fiducial marker representations. Fine-tuning techniques are adapted for limited data.", "result": "Experiments on a 6-DoF robot demonstrate achievable accuracy, validating the feasibility of visual proprioception for inexpensive robots.", "conclusion": "Visual proprioception from a single camera is viable for inexpensive robots, offering a practical alternative to traditional proprioceptive methods."}}
{"id": "2504.16026", "pdf": "https://arxiv.org/pdf/2504.16026", "abs": "https://arxiv.org/abs/2504.16026", "authors": ["Konstantin F. Pilz", "James Sanders", "Robi Rahman", "Lennart Heim"], "title": "Trends in AI Supercomputers", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Frontier AI development relies on powerful AI supercomputers, yet analysis of\nthese systems is limited. We create a dataset of 500 AI supercomputers from\n2019 to 2025 and analyze key trends in performance, power needs, hardware cost,\nownership, and global distribution. We find that the computational performance\nof AI supercomputers has doubled every nine months, while hardware acquisition\ncost and power needs both doubled every year. The leading system in March 2025,\nxAI's Colossus, used 200,000 AI chips, had a hardware cost of \\$7B, and\nrequired 300 MW of power, as much as 250,000 households. As AI supercomputers\nevolved from tools for science to industrial machines, companies rapidly\nexpanded their share of total AI supercomputer performance, while the share of\ngovernments and academia diminished. Globally, the United States accounts for\nabout 75% of total performance in our dataset, with China in second place at\n15%. If the observed trends continue, the leading AI supercomputer in 2030 will\nachieve $2\\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a\nhardware cost of \\$200 billion, and require 9 GW of power. Our analysis\nprovides visibility into the AI supercomputer landscape, allowing policymakers\nto assess key AI trends like resource needs, ownership, and national\ncompetitiveness.", "AI": {"tldr": "The paper analyzes trends in AI supercomputers from 2019 to 2025, revealing rapid performance growth, rising costs, and shifting ownership from academia/governments to companies. The US dominates globally.", "motivation": "To address the lack of analysis on AI supercomputers, providing insights into performance, costs, power needs, ownership, and global distribution for policymakers.", "method": "Created a dataset of 500 AI supercomputers (2019-2025) and analyzed trends in performance, power, cost, ownership, and distribution.", "result": "Performance doubles every 9 months; cost and power needs double yearly. Companies dominate ownership; US leads globally (75% performance). Predicts extreme growth by 2030.", "conclusion": "The study highlights rapid AI supercomputer evolution, emphasizing the need for policy attention to resource needs, ownership, and competitiveness."}}
{"id": "2504.14732", "pdf": "https://arxiv.org/pdf/2504.14732", "abs": "https://arxiv.org/abs/2504.14732", "authors": ["Muhammad Qasim Elahi", "Somtochukwu Oguchienti", "Maheed H. Ahmed", "Mahsa Ghasemi"], "title": "Reinforcement Learning from Multi-level and Episodic Human Feedback", "categories": ["cs.LG"], "comment": null, "summary": "Designing an effective reward function has long been a challenge in\nreinforcement learning, particularly for complex tasks in unstructured\nenvironments. To address this, various learning paradigms have emerged that\nleverage different forms of human input to specify or refine the reward\nfunction. Reinforcement learning from human feedback is a prominent approach\nthat utilizes human comparative feedback, expressed as a preference for one\nbehavior over another, to tackle this problem. In contrast to comparative\nfeedback, we explore multi-level human feedback, which is provided in the form\nof a score at the end of each episode. This type of feedback offers more coarse\nbut informative signals about the underlying reward function than binary\nfeedback. Additionally, it can handle non-Markovian rewards, as it is based on\nthe evaluation of an entire episode. We propose an algorithm to efficiently\nlearn both the reward function and the optimal policy from this form of\nfeedback. Moreover, we show that the proposed algorithm achieves sublinear\nregret and demonstrate its empirical effectiveness through extensive\nsimulations.", "AI": {"tldr": "The paper explores multi-level human feedback for reward function design in reinforcement learning, proposing an algorithm to learn rewards and policies from episodic scores, with proven sublinear regret and empirical validation.", "motivation": "Designing effective reward functions in reinforcement learning is challenging, especially for complex tasks. Human feedback paradigms, like comparative feedback, are limited. Multi-level feedback offers richer signals and handles non-Markovian rewards.", "method": "The authors propose an algorithm to learn reward functions and optimal policies from multi-level human feedback, which is provided as episodic scores. The method handles non-Markovian rewards and leverages coarse but informative signals.", "result": "The algorithm achieves sublinear regret, demonstrating theoretical guarantees. Empirical simulations confirm its effectiveness in learning from episodic feedback.", "conclusion": "Multi-level human feedback is a viable alternative to comparative feedback, offering advantages for reward function design. The proposed algorithm is effective and theoretically sound."}}
{"id": "2504.15975", "pdf": "https://arxiv.org/pdf/2504.15975", "abs": "https://arxiv.org/abs/2504.15975", "authors": ["Peter Fletcher"], "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition", "categories": ["cs.FL", "cs.CV", "F.4.2; F.4.3"], "comment": "64 pages, 23 figures. Version 2: mathematical supplement added, 98\n  pages, 1 figure", "summary": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches.", "AI": {"tldr": "A formalism for representing graph-like patterns without production rules, using networks for grammar and pattern representation, enabling parallel parsing and error-tolerant recognition.", "motivation": "To provide a more direct and declarative way to represent syntactic structure in graph-like patterns, integrating all aspects of pattern recognition into a unified process.", "method": "Represents grammar and patterns as networks, parsing as homomorphism construction, supporting iterative, hierarchical, and nested recursive structures.", "result": "Demonstrates error-tolerant parsing of complex patterns (50-1000 symbols) with variability in geometry, blurriness, overlaps, clutter, and erasures.", "conclusion": "The approach integrates feature detection, segmentation, and inference into a single parallel process, offering theoretical and practical benefits for pattern recognition."}}
{"id": "2504.16113", "pdf": "https://arxiv.org/pdf/2504.16113", "abs": "https://arxiv.org/abs/2504.16113", "authors": ["Xin Wang", "Xiaoqi Li"], "title": "AI-Based Vulnerability Analysis of NFT Smart Contracts", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "With the rapid growth of the NFT market, the security of smart contracts has\nbecome crucial. However, existing AI-based detection models for NFT contract\nvulnerabilities remain limited due to their complexity, while traditional\nmanual methods are time-consuming and costly. This study proposes an AI-driven\napproach to detect vulnerabilities in NFT smart contracts.\n  We collected 16,527 public smart contract codes, classifying them into five\nvulnerability categories: Risky Mutable Proxy, ERC-721 Reentrancy, Unlimited\nMinting, Missing Requirements, and Public Burn. Python-processed data was\nstructured into training/test sets. Using the CART algorithm with Gini\ncoefficient evaluation, we built initial decision trees for feature extraction.\nA random forest model was implemented to improve robustness through random\ndata/feature sampling and multitree integration. GridSearch hyperparameter\ntuning further optimized the model, with 3D visualizations demonstrating\nparameter impacts on vulnerability detection.\n  Results show the random forest model excels in detecting all five\nvulnerabilities. For example, it identifies Risky Mutable Proxy by analyzing\nauthorization mechanisms and state modifications, while ERC-721 Reentrancy\ndetection relies on external call locations and lock mechanisms. The ensemble\napproach effectively reduces single-tree overfitting, with stable performance\nimprovements after parameter tuning. This method provides an efficient\ntechnical solution for automated NFT contract detection and lays groundwork for\nscaling AI applications.", "AI": {"tldr": "An AI-driven approach using random forest models effectively detects five types of NFT smart contract vulnerabilities, outperforming traditional methods.", "motivation": "The rapid growth of the NFT market highlights the need for efficient security solutions, as existing AI models are complex and manual methods are costly.", "method": "Collected 16,527 smart contracts, classified into five vulnerabilities, processed data with Python, and used CART and random forest models with GridSearch tuning.", "result": "The random forest model excelled in detecting all five vulnerabilities, reducing overfitting and improving performance post-tuning.", "conclusion": "This method offers an efficient, scalable solution for automated NFT contract vulnerability detection."}}
{"id": "2504.16450", "pdf": "https://arxiv.org/pdf/2504.16450", "abs": "https://arxiv.org/abs/2504.16450", "authors": ["Rubing Yang", "Pratik Chaudhari"], "title": "An Effective Gram Matrix Characterizes Generalization in Deep Networks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We derive a differential equation that governs the evolution of the\ngeneralization gap when a deep network is trained by gradient descent. This\ndifferential equation is controlled by two quantities, a contraction factor\nthat brings together trajectories corresponding to slightly different datasets,\nand a perturbation factor that accounts for them training on different\ndatasets. We analyze this differential equation to compute an ``effective Gram\nmatrix'' that characterizes the generalization gap after training in terms of\nthe alignment between this Gram matrix and a certain initial ``residual''.\nEmpirical evaluations on image classification datasets indicate that this\nanalysis can predict the test loss accurately. Further, at any point during\ntraining, the residual predominantly lies in the subspace of the effective Gram\nmatrix with the smallest eigenvalues. This indicates that the training process\nis benign, i.e., it does not lead to significant deterioration of the\ngeneralization gap (which is zero at initialization). The alignment between the\neffective Gram matrix and the residual is different for different datasets and\narchitectures. The match/mismatch of the data and the architecture is primarily\nresponsible for good/bad generalization.", "AI": {"tldr": "The paper derives a differential equation to model the generalization gap in deep networks trained by gradient descent, identifying key factors like contraction and perturbation. It introduces an \"effective Gram matrix\" to predict test loss and shows training is benign, with generalization influenced by data-architecture alignment.", "motivation": "To understand and predict the generalization gap in deep networks by analyzing the dynamics of gradient descent training.", "method": "Derives a differential equation for the generalization gap, controlled by contraction and perturbation factors, and analyzes an \"effective Gram matrix\" to characterize the gap.", "result": "Empirical evaluations show accurate test loss prediction. Training is benign, and generalization depends on data-architecture alignment.", "conclusion": "The generalization gap is governed by the alignment between the effective Gram matrix and the residual, with data-architecture match being key."}}
{"id": "2504.16173", "pdf": "https://arxiv.org/pdf/2504.16173", "abs": "https://arxiv.org/abs/2504.16173", "authors": ["Pedro Antunes", "Artur Podobas"], "title": "FPGA-Based Neural Network Accelerators for Space Applications: A Survey", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Space missions are becoming increasingly ambitious, necessitating\nhigh-performance onboard spacecraft computing systems. In response,\nfield-programmable gate arrays (FPGAs) have garnered significant interest due\nto their flexibility, cost-effectiveness, and radiation tolerance potential.\nConcurrently, neural networks (NNs) are being recognized for their capability\nto execute space mission tasks such as autonomous operations, sensor data\nanalysis, and data compression. This survey serves as a valuable resource for\nresearchers aiming to implement FPGA-based NN accelerators in space\napplications. By analyzing existing literature, identifying trends and gaps,\nand proposing future research directions, this work highlights the potential of\nthese accelerators to enhance onboard computing systems.", "AI": {"tldr": "Survey on FPGA-based NN accelerators for space missions, highlighting their potential to enhance onboard computing.", "motivation": "Space missions require high-performance computing; FPGAs and NNs offer flexible, cost-effective solutions.", "method": "Analyzes literature, identifies trends and gaps, and proposes future research directions.", "result": "FPGA-based NN accelerators show promise for improving space mission computing systems.", "conclusion": "FPGA-NN accelerators are a valuable resource for advancing onboard computing in space applications."}}
{"id": "2504.16580", "pdf": "https://arxiv.org/pdf/2504.16580", "abs": "https://arxiv.org/abs/2504.16580", "authors": ["Ignacio Peis", "Batuhan Koyuncu", "Isabel Valera", "Jes Frellsen"], "title": "Hyper-Transforming Latent Diffusion Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We introduce a novel generative framework for functions by integrating\nImplicit Neural Representations (INRs) and Transformer-based hypernetworks into\nlatent variable models. Unlike prior approaches that rely on MLP-based\nhypernetworks with scalability limitations, our method employs a\nTransformer-based decoder to generate INR parameters from latent variables,\naddressing both representation capacity and computational efficiency. Our\nframework extends latent diffusion models (LDMs) to INR generation by replacing\nstandard decoders with a Transformer-based hypernetwork, which can be trained\neither from scratch or via hyper-transforming-a strategy that fine-tunes only\nthe decoder while freezing the pre-trained latent space. This enables efficient\nadaptation of existing generative models to INR-based representations without\nrequiring full retraining.", "AI": {"tldr": "A novel generative framework combines INRs and Transformer-based hypernetworks for scalable and efficient function generation, improving upon MLP-based methods.", "motivation": "Prior methods using MLP-based hypernetworks face scalability issues. This work aims to enhance representation capacity and computational efficiency.", "method": "Integrates Transformer-based hypernetworks into latent variable models, replacing standard decoders in LDMs for INR generation. Training can be from scratch or via hyper-transforming.", "result": "The framework efficiently adapts existing generative models to INR-based representations without full retraining.", "conclusion": "The proposed method offers a scalable and efficient solution for generating functions using INRs and Transformers."}}
{"id": "2504.16748", "pdf": "https://arxiv.org/pdf/2504.16748", "abs": "https://arxiv.org/abs/2504.16748", "authors": ["Yanan Zhao", "Feng Ji", "Kai Zhao", "Xuhao Li", "Qiyu Kang", "Wenfei Liang", "Yahya Alkhatib", "Xingchao Jian", "Wee Peng Tay"], "title": "Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks", "categories": ["cs.LG"], "comment": "Submitted to ICML", "summary": "Graph Contrastive Learning (GCL) has recently made progress as an\nunsupervised graph representation learning paradigm. GCL approaches can be\ncategorized into augmentation-based and augmentation-free methods. The former\nrelies on complex data augmentations, while the latter depends on encoders that\ncan generate distinct views of the same input. Both approaches may require\nnegative samples for training. In this paper, we introduce a novel\naugmentation-free GCL framework based on graph neural diffusion models.\nSpecifically, we utilize learnable encoders governed by Fractional Differential\nEquations (FDE). Each FDE is characterized by an order parameter of the\ndifferential operator. We demonstrate that varying these parameters allows us\nto produce learnable encoders that generate diverse views, capturing either\nlocal or global information, for contrastive learning. Our model does not\nrequire negative samples for training and is applicable to both homophilic and\nheterophilic datasets. We demonstrate its effectiveness across various\ndatasets, achieving state-of-the-art performance.", "AI": {"tldr": "A novel augmentation-free Graph Contrastive Learning (GCL) framework using graph neural diffusion models with Fractional Differential Equations (FDE) is introduced, eliminating the need for negative samples and achieving state-of-the-art performance.", "motivation": "To address the limitations of existing GCL methods, which rely on complex augmentations or negative samples, by proposing an augmentation-free approach that generates diverse views using FDE-based encoders.", "method": "Utilizes learnable encoders governed by FDEs, where varying order parameters of the differential operator produces diverse views (local or global) for contrastive learning without negative samples.", "result": "The framework achieves state-of-the-art performance across various datasets, including homophilic and heterophilic ones.", "conclusion": "The proposed augmentation-free GCL framework with FDE-based encoders is effective, versatile, and eliminates the need for negative samples, advancing unsupervised graph representation learning."}}
{"id": "2504.16871", "pdf": "https://arxiv.org/pdf/2504.16871", "abs": "https://arxiv.org/abs/2504.16871", "authors": ["Mirian Hipolito Garcia", "Camille Couturier", "Daniel Madrigal Diaz", "Ankur Mallick", "Anastasios Kyrillidis", "Robert Sim", "Victor Ruhle", "Saravan Rajmohan"], "title": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge", "categories": ["cs.LG"], "comment": null, "summary": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks", "AI": {"tldr": "LLMs can distinguish domain-specific queries using hidden states, and fine-tuned models aren't always the best performers.", "motivation": "To investigate if LLMs inherently capture domain-specific nuances and how robust these representations are to prompt variations.", "method": "Probing domain sensitivity by analyzing hidden states during prefill, studying robustness to prompt styles, and using domain representations for model selection.", "result": "LLMs can differentiate queries from related domains, and fine-tuned models aren't always the most accurate.", "conclusion": "LLMs inherently recognize domain nuances, and model selection based on domain traces can improve performance."}}
{"id": "2305.04281", "pdf": "https://arxiv.org/pdf/2305.04281", "abs": "https://arxiv.org/abs/2305.04281", "authors": ["Juni Schindler", "Mauricio Barahona"], "title": "Analysing Multiscale Clusterings with Persistent Homology", "categories": ["math.AT", "cs.LG", "Primary 55N31, Secondary 62H30"], "comment": "This work was presented at the Dagstuhl Seminar (23192) on\n  \"Topological Data Analysis and Applications\"", "summary": "In data clustering, it is often desirable to find not just a single partition\ninto clusters but a sequence of partitions that describes the data at different\nscales (or levels of coarseness). A natural problem then is to analyse and\ncompare the (not necessarily hierarchical) sequences of partitions that\nunderpin such multiscale descriptions. Here, we use tools from topological data\nanalysis and introduce the Multiscale Clustering Filtration (MCF), a\nwell-defined and stable filtration of abstract simplicial complexes that\nencodes arbitrary cluster assignments in a sequence of partitions across scales\nof increasing coarseness. We show that the zero-dimensional persistent homology\nof the MCF measures the degree of hierarchy of this sequence, and the\nhigher-dimensional persistent homology tracks the emergence and resolution of\nconflicts between cluster assignments across the sequence of partitions. To\nbroaden the theoretical foundations of the MCF, we provide an equivalent\nconstruction via a nerve complex filtration, and we show that, in the\nhierarchical case, the MCF reduces to a Vietoris-Rips filtration of an\nultrametric space. Using synthetic data, we then illustrate how the persistence\ndiagram of the MCF provides a feature map that can serve to characterise and\nclassify multiscale clusterings.", "AI": {"tldr": "The paper introduces the Multiscale Clustering Filtration (MCF), a topological tool to analyze sequences of data partitions at varying scales, measuring hierarchy and conflicts via persistent homology.", "motivation": "To address the need for analyzing and comparing sequences of partitions in multiscale clustering, which are not necessarily hierarchical.", "method": "Uses topological data analysis to define the MCF, a stable filtration of simplicial complexes, and explores its properties via persistent homology.", "result": "MCF's zero-dimensional persistent homology measures hierarchy, while higher-dimensional tracks conflicts. It also reduces to Vietoris-Rips filtration in hierarchical cases.", "conclusion": "MCF provides a feature map for characterizing and classifying multiscale clusterings, demonstrated with synthetic data."}}
{"id": "2310.16975", "pdf": "https://arxiv.org/pdf/2310.16975", "abs": "https://arxiv.org/abs/2310.16975", "authors": ["Zheyu Oliver Wang", "Ricardo Baptista", "Youssef Marzouk", "Lars Ruthotto", "Deepanshu Verma"], "title": "Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference", "categories": ["stat.ML", "cs.LG", "62F15, 62M45"], "comment": "26 pages, 7 tables, 8 figures", "summary": "We present two neural network approaches that approximate the solutions of\nstatic and dynamic\n$\\unicode{x1D450}\\unicode{x1D45C}\\unicode{x1D45B}\\unicode{x1D451}\\unicode{x1D456}\\unicode{x1D461}\\unicode{x1D456}\\unicode{x1D45C}\\unicode{x1D45B}\\unicode{x1D44E}\\unicode{x1D459}\\unicode{x0020}\\unicode{x1D45C}\\unicode{x1D45D}\\unicode{x1D461}\\unicode{x1D456}\\unicode{x1D45A}\\unicode{x1D44E}\\unicode{x1D459}\\unicode{x0020}\\unicode{x1D461}\\unicode{x1D45F}\\unicode{x1D44E}\\unicode{x1D45B}\\unicode{x1D460}\\unicode{x1D45D}\\unicode{x1D45C}\\unicode{x1D45F}\\unicode{x1D461}$\n(COT) problems. Both approaches enable conditional sampling and conditional\ndensity estimation, which are core tasks in Bayesian\ninference$\\unicode{x2013}$particularly in the simulation-based\n($\\unicode{x201C}$likelihood-free$\\unicode{x201D}$) setting. Our methods\nrepresent the target conditional distribution as a transformation of a\ntractable reference distribution. Obtaining such a transformation, chosen here\nto be an approximation of the COT map, is computationally challenging even in\nmoderate dimensions. To improve scalability, our numerical algorithms use\nneural networks to parameterize candidate maps and further exploit the\nstructure of the COT problem. Our static approach approximates the map as the\ngradient of a partially input-convex neural network. It uses a novel numerical\nimplementation to increase computational efficiency compared to\nstate-of-the-art alternatives. Our dynamic approach approximates the\nconditional optimal transport via the flow map of a regularized neural ODE;\ncompared to the static approach, it is slower to train but offers more modeling\nchoices and can lead to faster sampling. We demonstrate both algorithms\nnumerically, comparing them with competing state-of-the-art approaches, using\nbenchmark datasets and simulation-based Bayesian inverse problems.", "AI": {"tldr": "Two neural network methods for approximating solutions to static and dynamic COT problems, enabling conditional sampling and density estimation in Bayesian inference.", "motivation": "Address computational challenges in approximating COT maps, particularly in moderate dimensions, for simulation-based Bayesian inference.", "method": "Static approach uses gradient of a partially input-convex neural network; dynamic approach employs neural ODE flow maps. Both leverage neural networks and exploit COT structure.", "result": "Static method is computationally efficient; dynamic method offers flexibility and faster sampling despite slower training. Both outperform state-of-the-art alternatives in benchmarks.", "conclusion": "Neural network-based approaches effectively approximate COT solutions, with trade-offs between efficiency and flexibility, advancing simulation-based Bayesian inference."}}
{"id": "2401.11679", "pdf": "https://arxiv.org/pdf/2401.11679", "abs": "https://arxiv.org/abs/2401.11679", "authors": ["Jinghuai Yao", "Puyuan Du", "Yucheng Zhao", "Yubo Wang"], "title": "Simulating Nighttime Visible Satellite Imagery of Tropical Cyclones Using Conditional Generative Adversarial Networks", "categories": ["physics.ao-ph", "cs.LG"], "comment": null, "summary": "Visible (VIS) imagery is important for monitoring Tropical Cyclones (TCs) but\nis unavailable at night. This study presents a Conditional Generative\nAdversarial Networks (CGAN) model to generate nighttime VIS imagery with\nsignificantly enhanced accuracy and spatial resolution. Our method offers three\nkey improvements compared to existing models. First, we replaced the L1 loss in\nthe pix2pix framework with the Structural Similarity Index Measure (SSIM) loss,\nwhich significantly reduced image blurriness. Second, we selected multispectral\ninfrared (IR) bands as input based on a thorough examination of their spectral\nproperties, providing essential physical information for accurate simulation.\nThird, we incorporated the direction parameters of the sun and the satellite,\nwhich addressed the dependence of VIS images on sunlight directions and enabled\na much larger training set from continuous daytime data. The model was trained\nand validated using data from the Advanced Himawari Imager (AHI) in the\ndaytime, achieving statistical results of SSIM = 0.923 and Root Mean Square\nError (RMSE) = 0.0299, which significantly surpasses existing models. We also\nperformed a cross-satellite nighttime model validation using the Day/Night Band\n(DNB) of the Visible/Infrared Imager Radiometer Suite (VIIRS), which yields\noutstanding results compared to existing models. Our model is operationally\napplied to generate accurate VIS imagery with arbitrary virtual sunlight\ndirections, significantly contributing to the nighttime monitoring of various\nmeteorological phenomena.", "AI": {"tldr": "A CGAN model generates nighttime VIS imagery for TCs with improved accuracy and resolution by using SSIM loss, multispectral IR bands, and sunlight direction parameters.", "motivation": "VIS imagery is unavailable at night, limiting TC monitoring. This study aims to overcome this by generating accurate nighttime VIS imagery.", "method": "The model replaces L1 loss with SSIM loss, selects multispectral IR bands, and incorporates sunlight direction parameters for training.", "result": "Achieved SSIM = 0.923 and RMSE = 0.0299, outperforming existing models. Cross-satellite validation with VIIRS DNB also showed outstanding results.", "conclusion": "The model enables accurate nighttime VIS imagery generation, enhancing meteorological monitoring."}}
{"id": "2403.10671", "pdf": "https://arxiv.org/pdf/2403.10671", "abs": "https://arxiv.org/abs/2403.10671", "authors": ["James McInerney", "Nathan Kallus"], "title": "Variation Due to Regularization Tractably Recovers Bayesian Deep Learning", "categories": ["stat.ML", "cs.LG"], "comment": "16 pages, 9 figures", "summary": "Uncertainty quantification in deep learning is crucial for safe and reliable\ndecision-making in downstream tasks. Existing methods quantify uncertainty at\nthe last layer or other approximations of the network which may miss some\nsources of uncertainty in the model. To address this gap, we propose an\nuncertainty quantification method for large networks based on variation due to\nregularization. Essentially, predictions that are more (less) sensitive to the\nregularization of network parameters are less (more, respectively) certain.\nThis principle can be implemented by deterministically tweaking the training\nloss during the fine-tuning phase and reflects confidence in the output as a\nfunction of all layers of the network. We show that regularization variation\n(RegVar) provides rigorous uncertainty estimates that, in the infinitesimal\nlimit, exactly recover the Laplace approximation in Bayesian deep learning. We\ndemonstrate its success in several deep learning architectures, showing it can\nscale tractably with the network size while maintaining or improving\nuncertainty quantification quality. Our experiments across multiple datasets\nshow that RegVar not only identifies uncertain predictions effectively but also\nprovides insights into the stability of learned representations.", "AI": {"tldr": "Proposes RegVar, a method for uncertainty quantification in deep learning by analyzing sensitivity to regularization, improving reliability and scalability.", "motivation": "Existing methods miss some uncertainty sources in deep networks; RegVar addresses this by leveraging regularization sensitivity.", "method": "Deterministically tweaks training loss during fine-tuning to reflect confidence based on all network layers.", "result": "RegVar provides rigorous uncertainty estimates, recovers Laplace approximation, and scales well with network size.", "conclusion": "RegVar effectively identifies uncertain predictions and offers insights into representation stability, outperforming existing methods."}}
{"id": "2405.05235", "pdf": "https://arxiv.org/pdf/2405.05235", "abs": "https://arxiv.org/abs/2405.05235", "authors": ["Hossein Mehri", "Hao Chen", "Hani Mehrpouyan"], "title": "RACH Traffic Prediction in Massive Machine Type Communications", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "Traffic pattern prediction has emerged as a promising approach for\nefficiently managing and mitigating the impacts of event-driven bursty traffic\nin massive machine-type communication (mMTC) networks. However, achieving\naccurate predictions of bursty traffic remains a non-trivial task due to the\ninherent randomness of events, and these challenges intensify within live\nnetwork environments. Consequently, there is a compelling imperative to design\na lightweight and agile framework capable of assimilating continuously\ncollected data from the network and accurately forecasting bursty traffic in\nmMTC networks. This paper addresses these challenges by presenting a machine\nlearning-based framework tailored for forecasting bursty traffic in\nmulti-channel slotted ALOHA networks. The proposed machine learning network\ncomprises long-term short-term memory (LSTM) and a DenseNet with feed-forward\nneural network (FFNN) layers, where the residual connections enhance the\ntraining ability of the machine learning network in capturing complicated\npatterns. Furthermore, we develop a new low-complexity online prediction\nalgorithm that updates the states of the LSTM network by leveraging frequently\ncollected data from the mMTC network. Simulation results and complexity\nanalysis demonstrate the superiority of our proposed algorithm in terms of both\naccuracy and complexity, making it well-suited for time-critical live\nscenarios. We evaluate the performance of the proposed framework in a network\nwith a single base station and thousands of devices organized into groups with\ndistinct traffic-generating characteristics. Comprehensive evaluations and\nsimulations indicate that our proposed machine learning approach achieves a\nremarkable $52\\%$ higher accuracy in long-term predictions compared to\ntraditional methods, without imposing additional processing load on the system.", "AI": {"tldr": "The paper proposes a machine learning-based framework for predicting bursty traffic in mMTC networks, combining LSTM and DenseNet with FFNN layers, achieving 52% higher accuracy than traditional methods.", "motivation": "Accurate prediction of bursty traffic in mMTC networks is challenging due to event randomness, necessitating a lightweight, agile framework for live environments.", "method": "Uses LSTM and DenseNet with FFNN layers, enhanced by residual connections, and a low-complexity online prediction algorithm for real-time updates.", "result": "Simulations show 52% higher accuracy in long-term predictions compared to traditional methods, with no added processing load.", "conclusion": "The proposed framework is effective for time-critical live scenarios, outperforming conventional approaches in accuracy and complexity."}}
{"id": "2406.06225", "pdf": "https://arxiv.org/pdf/2406.06225", "abs": "https://arxiv.org/abs/2406.06225", "authors": ["Samhruth Ananthanarayanan", "Girish Kulathumani", "Ganesh Narayanan"], "title": "Siren -- Advancing Cybersecurity through Deception and Adaptive Analysis", "categories": ["cs.CR", "cs.LG", "C.2.0; I.2.7"], "comment": "14 pages, 5 figures, 13th Computing Conference 2025 - London, United\n  Kingdom", "summary": "Siren represents a pioneering research effort aimed at fortifying\ncybersecurity through strategic integration of deception, machine learning, and\nproactive threat analysis. Drawing inspiration from mythical sirens, this\nproject employs sophisticated methods to lure potential threats into controlled\nenvironments. The system features a dynamic machine learning model for realtime\nanalysis and classification, ensuring continuous adaptability to emerging cyber\nthreats. The architectural framework includes a link monitoring proxy, a\npurpose-built machine learning model for dynamic link analysis, and a honeypot\nenriched with simulated user interactions to intensify threat engagement. Data\nprotection within the honeypot is fortified with probabilistic encryption.\nAdditionally, the incorporation of simulated user activity extends the system's\ncapacity to capture and learn from potential attackers even after user\ndisengagement. Overall, Siren introduces a paradigm shift in cybersecurity,\ntransforming traditional defense mechanisms into proactive systems that\nactively engage and learn from potential adversaries. The research strives to\nenhance user protection while yielding valuable insights for ongoing refinement\nin response to the evolving landscape of cybersecurity threats.", "AI": {"tldr": "Siren enhances cybersecurity by integrating deception, machine learning, and proactive threat analysis, using a dynamic system to lure and study threats.", "motivation": "To transform traditional cybersecurity defenses into proactive systems that actively engage and learn from adversaries, improving user protection and threat insights.", "method": "Uses a dynamic machine learning model, link monitoring proxy, honeypot with simulated user interactions, and probabilistic encryption for data protection.", "result": "Introduces a proactive cybersecurity system capable of real-time threat analysis and continuous adaptation.", "conclusion": "Siren represents a paradigm shift in cybersecurity, offering a proactive approach to threat engagement and learning."}}
{"id": "2409.01444", "pdf": "https://arxiv.org/pdf/2409.01444", "abs": "https://arxiv.org/abs/2409.01444", "authors": ["Wouter A. C. van Amsterdam"], "title": "A causal viewpoint on prediction model performance under changes in case-mix: discrimination and calibration respond differently for prognosis and diagnosis predictions", "categories": ["stat.ME", "cs.LG"], "comment": null, "summary": "Prediction models need reliable predictive performance as they inform\nclinical decisions, aiding in diagnosis, prognosis, and treatment planning. The\npredictive performance of these models is typically assessed through\ndiscrimination and calibration. Changes in the distribution of the data impact\nmodel performance and there may be important changes between a model's current\napplication and when and where its performance was last evaluated. In\nhealth-care, a typical change is a shift in case-mix. For example, for\ncardiovascular risk management, a general practitioner sees a different mix of\npatients than a specialist in a tertiary hospital.\n  This work introduces a novel framework that differentiates the effects of\ncase-mix shifts on discrimination and calibration based on the causal direction\nof the prediction task. When prediction is in the causal direction (often the\ncase for prognosis predictions), calibration remains stable under case-mix\nshifts, while discrimination does not. Conversely, when predicting in the\nanti-causal direction (often with diagnosis predictions), discrimination\nremains stable, but calibration does not.\n  A simulation study and empirical validation using cardiovascular disease\nprediction models demonstrate the implications of this framework. The causal\ncase-mix framework provides insights for developing, evaluating and deploying\nprediction models across different clinical settings, emphasizing the\nimportance of understanding the causal structure of the prediction task.", "AI": {"tldr": "The paper introduces a causal case-mix framework to analyze how case-mix shifts affect discrimination and calibration in prediction models, depending on the causal direction of the task.", "motivation": "Prediction models must maintain reliable performance across different clinical settings, but case-mix shifts can impact their discrimination and calibration. Understanding these effects is crucial for model deployment.", "method": "The study proposes a causal case-mix framework, validated through simulations and empirical analysis using cardiovascular disease prediction models.", "result": "Calibration remains stable under case-mix shifts for causal (prognosis) predictions, while discrimination does not. The opposite holds for anti-causal (diagnosis) predictions.", "conclusion": "The framework highlights the importance of causal structure in prediction tasks, guiding model development and deployment across diverse clinical settings."}}
{"id": "2409.18804", "pdf": "https://arxiv.org/pdf/2409.18804", "abs": "https://arxiv.org/abs/2409.18804", "authors": ["Iskander Azangulov", "George Deligiannidis", "Judith Rousseau"], "title": "Convergence of Diffusion Models Under the Manifold Hypothesis in High-Dimensions", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPM) are powerful state-of-the-art\nmethods used to generate synthetic data from high-dimensional data\ndistributions and are widely used for image, audio, and video generation as\nwell as many more applications in science and beyond. The \\textit{manifold\nhypothesis} states that high-dimensional data often lie on lower-dimensional\nmanifolds within the ambient space, and is widely believed to hold in provided\nexamples. While recent results have provided invaluable insight into how\ndiffusion models adapt to the manifold hypothesis, they do not capture the\ngreat empirical success of these models, making this a very fruitful research\ndirection.\n  In this work, we study DDPMs under the manifold hypothesis and prove that\nthey achieve rates independent of the ambient dimension in terms of score\nlearning. In terms of sampling complexity, we obtain rates independent of the\nambient dimension w.r.t. the Kullback-Leibler divergence, and $O(\\sqrt{D})$\nw.r.t. the Wasserstein distance. We do this by developing a new framework\nconnecting diffusion models to the well-studied theory of extrema of Gaussian\nProcesses.", "AI": {"tldr": "DDPMs achieve dimension-independent rates in score learning and sampling under the manifold hypothesis, leveraging Gaussian Process theory.", "motivation": "To understand why DDPMs perform well empirically despite the manifold hypothesis, and to provide theoretical guarantees for their success.", "method": "Develop a new framework linking DDPMs to Gaussian Process theory, analyzing score learning and sampling complexity under the manifold hypothesis.", "result": "Proved dimension-independent rates for score learning and KL divergence, and O(\u221aD) for Wasserstein distance.", "conclusion": "The work provides theoretical insights into DDPMs' empirical success, bridging gaps in understanding their performance under the manifold hypothesis."}}
{"id": "2410.09046", "pdf": "https://arxiv.org/pdf/2410.09046", "abs": "https://arxiv.org/abs/2410.09046", "authors": ["Peter Potaptchik", "Iskander Azangulov", "George Deligiannidis"], "title": "Linear Convergence of Diffusion Models Under the Manifold Hypothesis", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Score-matching generative models have proven successful at sampling from\ncomplex high-dimensional data distributions. In many applications, this\ndistribution is believed to concentrate on a much lower $d$-dimensional\nmanifold embedded into $D$-dimensional space; this is known as the manifold\nhypothesis. The current best-known convergence guarantees are either linear in\n$D$ or polynomial (superlinear) in $d$. The latter exploits a novel integration\nscheme for the backward SDE. We take the best of both worlds and show that the\nnumber of steps diffusion models require in order to converge in\nKullback-Leibler~(KL) divergence is linear (up to logarithmic terms) in the\nintrinsic dimension $d$. Moreover, we show that this linear dependency is\nsharp.", "AI": {"tldr": "Score-matching generative models achieve linear convergence in intrinsic dimension $d$ for KL divergence, improving over previous linear-in-$D$ or polynomial-in-$d$ guarantees.", "motivation": "The manifold hypothesis suggests data lies on a lower-dimensional manifold in high-dimensional space, motivating the need for efficient convergence guarantees in such settings.", "method": "The paper introduces an integration scheme for backward SDEs, combining strengths of existing methods to optimize convergence.", "result": "The method achieves linear (up to logarithmic terms) convergence in $d$, proving this dependency is sharp.", "conclusion": "The work advances theoretical guarantees for diffusion models, aligning convergence rates with intrinsic data dimensionality."}}
{"id": "2410.24117", "pdf": "https://arxiv.org/pdf/2410.24117", "abs": "https://arxiv.org/abs/2410.24117", "authors": ["Ali Reza Ibrahimzada", "Kaiyao Ke", "Mrigank Pawagi", "Muhammad Salman Abid", "Rangeet Pan", "Saurabh Sinha", "Reyhaneh Jabbarvand"], "title": "AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation", "categories": ["cs.SE", "cs.LG"], "comment": "Published in FSE 2025", "summary": "Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc. We propose\nAlphaTrans, a neuro-symbolic approach to automate repository-level code\ntranslation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans breaks down these projects into 17874\nfragments and translates the entire repository. 96.40% of the translated\nfragments are syntactically correct, and AlphaTrans validates the translations'\nruntime behavior and functional correctness for 27.03% and 25.14% of fragments.\nOn average, the integrated translation and validation take 34 hours to\ntranslate a project, showing its scalability in practice. For the incorrect\ntranslations, AlphaTrans generates a report including existing translation,\nstack trace, test errors, or assertion failures. We provided these artifacts to\ntwo developers to fix the translation bugs in four projects. They were able to\nfix the issues in 20.1 hours on average and achieve all passing tests.", "AI": {"tldr": "AlphaTrans is a neuro-symbolic approach for repository-level code translation, combining LLMs and program analysis to handle real-world projects with high accuracy and scalability.", "motivation": "Existing rule-based transpilers become obsolete as PLs evolve and struggle with real-world project complexity. LLMs alone fail to generalize to large-scale projects.", "method": "AlphaTrans decomposes programs into fragments, translates them in reverse call order, and validates functionality through multiple levels of testing.", "result": "AlphaTrans achieved 96.40% syntactic correctness, validated runtime behavior for 27.03% of fragments, and translated projects in ~34 hours on average.", "conclusion": "AlphaTrans demonstrates scalability and effectiveness for real-world code translation, with developer-friendly bug reporting."}}
{"id": "2411.13922", "pdf": "https://arxiv.org/pdf/2411.13922", "abs": "https://arxiv.org/abs/2411.13922", "authors": ["Bhupender Singh", "Ananth Ram Rajagopalan", "Srikrishna Bhashyam"], "title": "Exponentially Consistent Nonparametric Linkage-Based Clustering of Data Sequences", "categories": ["stat.ML", "cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": null, "summary": "In this paper, we consider nonparametric clustering of $M$ independent and\nidentically distributed (i.i.d.) data sequences generated from {\\em unknown}\ndistributions. The distributions of the $M$ data sequences belong to $K$\nunderlying distribution clusters. Existing results on exponentially consistent\nnonparametric clustering algorithms, like single linkage-based (SLINK)\nclustering and $k$-medoids distribution clustering, assume that the maximum\nintra-cluster distance ($d_L$) is smaller than the minimum inter-cluster\ndistance ($d_H$). First, in the fixed sample size (FSS) setting, we show that\nexponential consistency can be achieved for SLINK clustering under a less\nstrict assumption, $d_I < d_H$, where $d_I$ is the maximum distance between any\ntwo sub-clusters of a cluster that partition the cluster. Note that $d_I < d_L$\nin general. Thus, our results show that SLINK is exponentially consistent for a\nlarger class of problems than previously known. In our simulations, we also\nidentify examples where $k$-medoids clustering is unable to find the true\nclusters, but SLINK is exponentially consistent. Then, we propose a sequential\nclustering algorithm, named SLINK-SEQ, based on SLINK and prove that it is also\nexponentially consistent. Simulation results show that the SLINK-SEQ algorithm\nrequires fewer expected number of samples than the FSS SLINK algorithm for the\nsame probability of error.", "AI": {"tldr": "The paper extends the conditions for exponential consistency in nonparametric clustering, showing SLINK works under looser assumptions than previously known, and introduces a sequential SLINK-SEQ algorithm with better sample efficiency.", "motivation": "To improve nonparametric clustering by relaxing assumptions for exponential consistency and developing a more sample-efficient sequential algorithm.", "method": "Analyzes SLINK clustering under relaxed assumptions and proposes SLINK-SEQ, a sequential version, proving its exponential consistency.", "result": "SLINK is exponentially consistent under weaker conditions; SLINK-SEQ outperforms FSS SLINK in sample efficiency.", "conclusion": "The work broadens the applicability of SLINK and introduces a superior sequential variant, SLINK-SEQ."}}
{"id": "2501.12314", "pdf": "https://arxiv.org/pdf/2501.12314", "abs": "https://arxiv.org/abs/2501.12314", "authors": ["Xueqiong Yuan", "Jipeng Li", "Ercan Engin Kuruoglu"], "title": "Uncertainty Quantification With Noise Injection in Neural Networks: A Bayesian Perspective", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Model uncertainty quantification involves measuring and evaluating the\nuncertainty linked to a model's predictions, helping assess their reliability\nand confidence. Noise injection is a technique used to enhance the robustness\nof neural networks by introducing randomness. In this paper, we establish a\nconnection between noise injection and uncertainty quantification from a\nBayesian standpoint. We theoretically demonstrate that injecting noise into the\nweights of a neural network is equivalent to Bayesian inference on a deep\nGaussian process. Consequently, we introduce a Monte Carlo Noise Injection\n(MCNI) method, which involves injecting noise into the parameters during\ntraining and performing multiple forward propagations during inference to\nestimate the uncertainty of the prediction. Through simulation and experiments\non regression and classification tasks, our method demonstrates superior\nperformance compared to the baseline model.", "AI": {"tldr": "The paper connects noise injection in neural networks to Bayesian uncertainty quantification, proposing a Monte Carlo Noise Injection (MCNI) method for improved model robustness and uncertainty estimation.", "motivation": "To bridge the gap between noise injection techniques and Bayesian uncertainty quantification, enhancing model reliability and confidence in predictions.", "method": "Theoretical proof linking noise injection to Bayesian inference, followed by the introduction of MCNI\u2014noise injection during training and multiple forward passes during inference for uncertainty estimation.", "result": "MCNI outperforms baseline models in regression and classification tasks, demonstrating superior uncertainty quantification.", "conclusion": "Noise injection, framed as Bayesian inference, effectively quantifies uncertainty, with MCNI offering a practical and high-performing approach."}}
{"id": "2502.09395", "pdf": "https://arxiv.org/pdf/2502.09395", "abs": "https://arxiv.org/abs/2502.09395", "authors": ["Jaime Maldonado", "Jonas Krumme", "Christoph Zetzsche", "Vanessa Didelez", "Kerstin Schill"], "title": "Robot Pouring: Identifying Causes of Spillage and Selecting Alternative Action Parameters Using Probabilistic Actual Causation", "categories": ["cs.RO", "cs.LG"], "comment": "20 pages, 13 figures", "summary": "In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a\nlarge variety of objects and goals. When confronted with an unexpected or\nunwanted outcome, we take corrective actions and try again until achieving the\ndesired result. The reasoning performed to identify a cause of the observed\noutcome and to select an appropriate corrective action is a crucial aspect of\nhuman reasoning for successful task execution. Central to this reasoning is the\nassumption that a factor is responsible for producing the observed outcome. In\nthis paper, we investigate the use of probabilistic actual causation to\ndetermine whether a factor is the cause of an observed undesired outcome.\nFurthermore, we show how the actual causation probabilities can be used to find\nalternative actions to change the outcome. We apply the probabilistic actual\ncausation analysis to a robot pouring task. When spillage occurs, the analysis\nindicates whether a task parameter is the cause and how it should be changed to\navoid spillage. The analysis requires a causal graph of the task and the\ncorresponding conditional probability distributions. To fulfill these\nrequirements, we perform a complete causal modeling procedure (i.e., task\nanalysis, definition of variables, determination of the causal graph structure,\nand estimation of conditional probability distributions) using data from a\nrealistic simulation of the robot pouring task, covering a large combinatorial\nspace of task parameters. Based on the results, we discuss the implications of\nthe variables' representation and how the alternative actions suggested by the\nactual causation analysis would compare to the alternative solutions proposed\nby a human observer. The practical use of the analysis of probabilistic actual\ncausation to select alternative action parameters is demonstrated.", "AI": {"tldr": "The paper explores probabilistic actual causation to identify causes of undesired outcomes in tasks (e.g., robot pouring) and suggests corrective actions.", "motivation": "Human reasoning involves identifying causes of unexpected outcomes and taking corrective actions. The paper aims to formalize this using probabilistic actual causation.", "method": "The study uses causal graphs and conditional probability distributions to model tasks. It applies this to a robot pouring task, analyzing spillage causes and corrective actions.", "result": "The method successfully identifies causes of spillage and suggests alternative actions. It compares these with human-proposed solutions.", "conclusion": "Probabilistic actual causation is practical for selecting corrective actions, with implications for variable representation and human-like reasoning."}}
{"id": "2503.03659", "pdf": "https://arxiv.org/pdf/2503.03659", "abs": "https://arxiv.org/abs/2503.03659", "authors": ["Liang Hong"], "title": "Conformal prediction of future insurance claims in the regression problem", "categories": ["stat.ML", "cs.LG", "stat.AP", "62P05, 91G05"], "comment": null, "summary": "In the current insurance literature, prediction of insurance claims in the\nregression problem is often performed with a statistical model. This\nmodel-based approach may potentially suffer from several drawbacks: (i) model\nmisspecification, (ii) selection effect, and (iii) lack of finite-sample\nvalidity. This article addresses these three issues simultaneously by employing\nconformal prediction -- a general machine learning strategy for valid\npredictions. The proposed method is both model-free and tuning-parameter-free.\nIt also guarantees finite-sample validity at a pre-assigned coverage\nprobability level. Examples, based on both simulated and real data, are\nprovided to demonstrate the excellent performance of the proposed method and\nits applications in insurance, especially regarding meeting the solvency\ncapital requirement of European insurance regulation, Solvency II.", "AI": {"tldr": "The paper proposes a conformal prediction method to address model misspecification, selection effect, and finite-sample validity issues in insurance claim predictions, ensuring valid predictions without relying on models or tuning parameters.", "motivation": "To overcome drawbacks of traditional statistical models in insurance claim prediction, such as model misspecification, selection effect, and lack of finite-sample validity.", "method": "Uses conformal prediction, a model-free and tuning-parameter-free machine learning strategy, to ensure valid predictions.", "result": "Demonstrates excellent performance in simulations and real data, meeting Solvency II regulatory requirements.", "conclusion": "Conformal prediction effectively addresses key issues in insurance claim prediction, offering valid and practical solutions."}}
{"id": "2503.09829", "pdf": "https://arxiv.org/pdf/2503.09829", "abs": "https://arxiv.org/abs/2503.09829", "authors": ["Joohwan Seo", "Soochul Yoo", "Junwoo Chang", "Hyunseok An", "Hyunwoo Ryu", "Soomi Lee", "Arvind Kruthiventy", "Jongeun Choi", "Roberto Horowitz"], "title": "SE(3)-Equivariant Robot Learning and Control: A Tutorial Survey", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted to International Journcal of Control, Automation and Systems\n  (IJCAS)", "summary": "Recent advances in deep learning and Transformers have driven major\nbreakthroughs in robotics by employing techniques such as imitation learning,\nreinforcement learning, and LLM-based multimodal perception and\ndecision-making. However, conventional deep learning and Transformer models\noften struggle to process data with inherent symmetries and invariances,\ntypically relying on large datasets or extensive data augmentation. Equivariant\nneural networks overcome these limitations by explicitly integrating symmetry\nand invariance into their architectures, leading to improved efficiency and\ngeneralization. This tutorial survey reviews a wide range of equivariant deep\nlearning and control methods for robotics, from classic to state-of-the-art,\nwith a focus on SE(3)-equivariant models that leverage the natural 3D\nrotational and translational symmetries in visual robotic manipulation and\ncontrol design. Using unified mathematical notation, we begin by reviewing key\nconcepts from group theory, along with matrix Lie groups and Lie algebras. We\nthen introduce foundational group-equivariant neural network design and show\nhow the group-equivariance can be obtained through their structure. Next, we\ndiscuss the applications of SE(3)-equivariant neural networks in robotics in\nterms of imitation learning and reinforcement learning. The SE(3)-equivariant\ncontrol design is also reviewed from the perspective of geometric control.\nFinally, we highlight the challenges and future directions of equivariant\nmethods in developing more robust, sample-efficient, and multi-modal real-world\nrobotic systems.", "AI": {"tldr": "A survey on equivariant deep learning and control methods in robotics, focusing on SE(3)-equivariant models for improved efficiency and generalization by leveraging symmetries.", "motivation": "Conventional deep learning and Transformers struggle with data symmetries, requiring large datasets or augmentation. Equivariant networks address this by embedding symmetry into architectures.", "method": "Reviews SE(3)-equivariant models, group theory, and Lie algebras, then applies these to robotics via imitation and reinforcement learning, and geometric control.", "result": "Equivariant methods enhance robustness, sample efficiency, and generalization in robotic tasks.", "conclusion": "Highlights challenges and future directions for equivariant methods in multi-modal, real-world robotic systems."}}
{"id": "2503.17430", "pdf": "https://arxiv.org/pdf/2503.17430", "abs": "https://arxiv.org/abs/2503.17430", "authors": ["Yi-Meng Huang", "Zi-Ran Zhao", "Shun-Cai Zhao"], "title": "Long-term excitation energy transfer predicted by a modified convolutional neural networks in the FMO complexes", "categories": ["physics.chem-ph", "cs.LG", "quant-ph", "2020: 05C70"], "comment": "11 pages, 10figures", "summary": "In machine learning (ML), the risk of recursive strategies overfitting\nhistorical data has driven the development of convolutional neural networks\n(CNNs) in simulating quantum dissipative dynamics. In this work, we propose an\nefficient CNNs scheme incorporating novel redundant time-functions to predict\n100 picosecond (ps) excitation energy transfer (EET) in Fenna-Matthews-Olson\n(FMO) complexes, in which the original time $t$ is normalized by mapping it to\nthe [0, 1] range, allowing different functions focus on distinct time\nintervals, thereby effectively capturing the multi-timescale characteristics of\nEET dynamics. This method simplifies optimization and enhances learning\nefficiency, and demonstrate the accuracy, robustness, and efficiency of our\napproach in predicting quantum dissipative dynamics.", "AI": {"tldr": "A CNN-based method with redundant time-functions is proposed to predict EET in FMO complexes, improving accuracy and efficiency in quantum dissipative dynamics.", "motivation": "To address overfitting risks in recursive ML strategies for simulating quantum dissipative dynamics.", "method": "Normalizes time to [0, 1], uses redundant time-functions to capture multi-timescale EET dynamics, and simplifies optimization.", "result": "Demonstrates accuracy, robustness, and efficiency in predicting 100 ps EET in FMO complexes.", "conclusion": "The proposed CNN scheme effectively captures multi-timescale dynamics and enhances learning efficiency."}}
{"id": "2504.01650", "pdf": "https://arxiv.org/pdf/2504.01650", "abs": "https://arxiv.org/abs/2504.01650", "authors": ["Tommy Rochussen", "Vincent Fortuin"], "title": "Sparse Gaussian Neural Processes", "categories": ["stat.ML", "cs.LG"], "comment": "Proceedings of the 7th Symposium on Advances in Approximate Bayesian\n  Inference, PMLR, 2025. 25 pages, 6 figures, 5 tables", "summary": "Despite significant recent advances in probabilistic meta-learning, it is\ncommon for practitioners to avoid using deep learning models due to a\ncomparative lack of interpretability. Instead, many practitioners simply use\nnon-meta-models such as Gaussian processes with interpretable priors, and\nconduct the tedious procedure of training their model from scratch for each\ntask they encounter. While this is justifiable for tasks with a limited number\nof data points, the cubic computational cost of exact Gaussian process\ninference renders this prohibitive when each task has many observations. To\nremedy this, we introduce a family of models that meta-learn sparse Gaussian\nprocess inference. Not only does this enable rapid prediction on new tasks with\nsparse Gaussian processes, but since our models have clear interpretations as\nmembers of the neural process family, it also allows manual elicitation of\npriors in a neural process for the first time. In meta-learning regimes for\nwhich the number of observed tasks is small or for which expert domain\nknowledge is available, this offers a crucial advantage.", "AI": {"tldr": "A new family of models meta-learns sparse Gaussian process inference, combining interpretability with efficiency for tasks with many observations.", "motivation": "Practitioners avoid deep learning models due to lack of interpretability, opting for non-meta-models like Gaussian processes, which are computationally expensive for large datasets.", "method": "Introduces models that meta-learn sparse Gaussian process inference, enabling rapid prediction and interpretable priors in neural processes.", "result": "Enables efficient and interpretable meta-learning, especially useful when tasks are few or domain knowledge is available.", "conclusion": "The approach bridges the gap between interpretability and computational efficiency in probabilistic meta-learning."}}
{"id": "2504.03515", "pdf": "https://arxiv.org/pdf/2504.03515", "abs": "https://arxiv.org/abs/2504.03515", "authors": ["Shan An", "Ziyu Meng", "Chao Tang", "Yuning Zhou", "Tengyu Liu", "Fangqiang Ding", "Shufang Zhang", "Yao Mu", "Ran Song", "Wei Zhang", "Zeng-Guang Hou", "Hong Zhang"], "title": "Dexterous Manipulation through Imitation Learning: A Survey", "categories": ["cs.RO", "cs.LG"], "comment": "22pages, 5 figures", "summary": "Dexterous manipulation, which refers to the ability of a robotic hand or\nmulti-fingered end-effector to skillfully control, reorient, and manipulate\nobjects through precise, coordinated finger movements and adaptive force\nmodulation, enables complex interactions similar to human hand dexterity. With\nrecent advances in robotics and machine learning, there is a growing demand for\nthese systems to operate in complex and unstructured environments. Traditional\nmodel-based approaches struggle to generalize across tasks and object\nvariations due to the high dimensionality and complex contact dynamics of\ndexterous manipulation. Although model-free methods such as reinforcement\nlearning (RL) show promise, they require extensive training, large-scale\ninteraction data, and carefully designed rewards for stability and\neffectiveness. Imitation learning (IL) offers an alternative by allowing robots\nto acquire dexterous manipulation skills directly from expert demonstrations,\ncapturing fine-grained coordination and contact dynamics while bypassing the\nneed for explicit modeling and large-scale trial-and-error. This survey\nprovides an overview of dexterous manipulation methods based on imitation\nlearning, details recent advances, and addresses key challenges in the field.\nAdditionally, it explores potential research directions to enhance IL-driven\ndexterous manipulation. Our goal is to offer researchers and practitioners a\ncomprehensive introduction to this rapidly evolving domain.", "AI": {"tldr": "The paper surveys imitation learning (IL) for dexterous manipulation in robotics, highlighting its advantages over traditional model-based and model-free methods, recent advances, challenges, and future directions.", "motivation": "Dexterous manipulation is crucial for robots operating in unstructured environments, but traditional methods struggle with generalization and complexity. IL offers a promising alternative by leveraging expert demonstrations.", "method": "The survey reviews IL-based approaches for dexterous manipulation, focusing on how they capture fine-grained coordination and contact dynamics without explicit modeling or extensive trial-and-error.", "result": "IL shows promise in enabling robots to perform complex manipulation tasks by learning directly from demonstrations, though challenges like scalability and robustness remain.", "conclusion": "The paper provides a comprehensive overview of IL for dexterous manipulation, identifies key challenges, and suggests future research directions to advance the field."}}
{"id": "2504.04002", "pdf": "https://arxiv.org/pdf/2504.04002", "abs": "https://arxiv.org/abs/2504.04002", "authors": ["Abigail R. Hering", "Mansha Dubey", "Elahe Hosseini", "Meghna Srivastava", "Yu An", "Juan-Pablo Correa-Baena", "Houman Homayoun", "Marina S. Leite"], "title": "Machine Learning Reveals Composition Dependent Thermal Stability in Halide Perovskites", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": "21 pages, 5 figures", "summary": "Halide perovskites exhibit unpredictable properties in response to\nenvironmental stressors, due to several composition-dependent degradation\nmechanisms. In this work, we apply data visualization and machine learning (ML)\ntechniques to reveal unexpected correlations between composition, temperature,\nand material properties while using high throughput, in situ environmental\nphotoluminescence (PL) experiments. Correlation heatmaps show the strong\ninfluence of Cs content on film degradation, and dimensionality reduction\nvisualization methods uncover clear composition-based data clusters. An extreme\ngradient boosting algorithm (XGBoost) effectively forecasts PL features for ten\nperovskite films with both composition-agnostic (>85% accuracy) and\ncomposition-dependent (>75% accuracy) model approaches, while elucidating the\nrelative feature importance of composition (up to 99%). This model validates a\npreviously unseen anti-correlation between Cs content and material thermal\nstability. Our ML-based framework can be expanded to any perovskite family,\nsignificantly reducing the analysis time currently employed to identify stable\noptions for photovoltaics.", "AI": {"tldr": "The paper uses ML and data visualization to analyze perovskite degradation, revealing Cs content's impact and achieving high-accuracy predictions for material properties.", "motivation": "To understand and predict the unpredictable degradation of halide perovskites under environmental stressors.", "method": "High-throughput in situ PL experiments, correlation heatmaps, dimensionality reduction, and XGBoost for forecasting PL features.", "result": "Strong influence of Cs content on degradation, high model accuracy (>85% agnostic, >75% dependent), and validation of Cs-thermal stability anti-correlation.", "conclusion": "The ML framework accelerates stable perovskite identification for photovoltaics and is adaptable to other perovskite families."}}
{"id": "2504.07347", "pdf": "https://arxiv.org/pdf/2504.07347", "abs": "https://arxiv.org/abs/2504.07347", "authors": ["Yueying Li", "Jim Dai", "Tianyi Peng"], "title": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": null, "summary": "As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.", "AI": {"tldr": "The paper develops queuing fundamentals for LLM inference, proving work-conserving algorithms achieve maximum throughput in single-engine systems but require advanced strategies in multi-agent networks.", "motivation": "Optimizing LLM inference systems is critical, yet queuing theory's role remains underexplored. This paper bridges the gap between queuing theory and LLM systems.", "method": "The study analyzes throughput in LLM inference, proving work-conserving scheduling's efficacy for single engines and identifying needs for advanced strategies in networks.", "result": "Work-conserving scheduling maximizes throughput in single-engine systems, but multi-agent networks require more sophisticated approaches. Orca and Sarathi-serve are throughput-optimal, unlike FasterTransformer and vLLM.", "conclusion": "Queuing theory significantly benefits LLM inference optimization, calling for interdisciplinary collaboration to enhance system performance."}}
{"id": "2504.08469", "pdf": "https://arxiv.org/pdf/2504.08469", "abs": "https://arxiv.org/abs/2504.08469", "authors": ["Khrystyna Semkiv", "Jia Zhang", "Maria Laura Ferster", "Walter Karlen"], "title": "Artifact detection and localization in single-channel mobile EEG for sleep research using deep learning and attention mechanisms", "categories": ["eess.SP", "cs.LG", "J.3"], "comment": null, "summary": "Artifacts in the electroencephalogram (EEG) degrade signal quality and impact\nthe analysis of brain activity. Current methods for detecting artifacts in\nsleep EEG rely on simple threshold-based algorithms that require manual\nintervention, which is time-consuming and impractical due to the vast volume of\ndata that novel mobile recording systems generate. We propose a convolutional\nneural network (CNN) model incorporating a convolutional block attention module\n(CNN-CBAM) to detect and identify the location of artifacts in the sleep EEG\nwith attention maps. We benchmarked this model against six other machine\nlearning and signal processing approaches. We trained/tuned all models on 72\nmanually annotated EEG recordings obtained during home-based monitoring from 18\nhealthy participants with a mean (SD) age of 68.05 y ($\\pm$5.02). We tested\nthem on 26 separate recordings from 6 healthy participants with a mean (SD) age\nof 68.33 y ($\\pm$4.08), with contained artifacts in 4\\% of epochs. CNN-CBAM\nachieved the highest area under the receiver operating characteristic curve\n(0.88), sensitivity (0.81), and specificity (0.86) when compared to the other\napproaches. The attention maps from CNN-CBAM localized artifacts within the\nepoch with a sensitivity of 0.71 and specificity of 0.67. This work\ndemonstrates the feasibility of automating the detection and localization of\nartifacts in wearable sleep EEG.", "AI": {"tldr": "A CNN-CBAM model is proposed for automated artifact detection and localization in sleep EEG, outperforming other methods with high accuracy.", "motivation": "Current artifact detection methods for sleep EEG are manual and impractical for large datasets from mobile recording systems.", "method": "A convolutional neural network with a convolutional block attention module (CNN-CBAM) is developed and benchmarked against six other approaches using annotated EEG recordings.", "result": "CNN-CBAM achieved the highest AUC (0.88), sensitivity (0.81), and specificity (0.86), with attention maps localizing artifacts effectively.", "conclusion": "The study demonstrates the feasibility of automating artifact detection and localization in wearable sleep EEG, improving efficiency and accuracy."}}
{"id": "2504.15284", "pdf": "https://arxiv.org/pdf/2504.15284", "abs": "https://arxiv.org/abs/2504.15284", "authors": ["Weichen Li", "Albert Jan", "Baishakhi Ray", "Chengzhi Mao", "Junfeng Yang", "Kexin Pei"], "title": "EditLord: Learning Code Transformation Rules for Code Editing", "categories": ["cs.SE", "cs.CR", "cs.LG"], "comment": null, "summary": "Code editing is a foundational task in software development, where its\neffectiveness depends on whether it introduces desired code property changes\nwithout changing the original code's intended functionality. Existing\napproaches often formulate code editing as an implicit end-to-end task,\nomitting the fact that code-editing procedures inherently consist of discrete\nand explicit steps. Thus, they suffer from suboptimal performance and lack of\nrobustness and generalization. We introduce EditLord, a code editing framework\nthat makes the code transformation steps explicit. Our key insight is to employ\na language model (LM) as an inductive learner to extract code editing rules\nfrom the training code pairs as concise meta-rule sets. Such rule sets will be\nmanifested for each training sample to augment them for finetuning or assist in\nprompting- and iterative-based code editing. EditLordoutperforms the\nstate-of-the-art by an average of 22.7% in editing performance and 58.1% in\nrobustness while achieving 20.2% higher functional correctness across critical\nsoftware engineering and security applications, LM models, and editing modes.", "AI": {"tldr": "EditLord is a code editing framework that makes transformation steps explicit, outperforming state-of-the-art methods in performance, robustness, and functional correctness.", "motivation": "Existing code editing approaches treat the task as implicit and end-to-end, leading to suboptimal performance, lack of robustness, and poor generalization.", "method": "EditLord uses a language model to extract explicit code editing rules from training pairs, forming meta-rule sets for finetuning or prompting-based editing.", "result": "EditLord improves editing performance by 22.7%, robustness by 58.1%, and functional correctness by 20.2% over existing methods.", "conclusion": "Explicitly modeling code editing steps with EditLord significantly enhances performance, robustness, and correctness in software engineering and security applications."}}
{"id": "2504.15632", "pdf": "https://arxiv.org/pdf/2504.15632", "abs": "https://arxiv.org/abs/2504.15632", "authors": ["Seyed Shayan Daneshvar", "Da Tan", "Shaowei Wang", "Carson Leung"], "title": "A Study on Mixup-Inspired Augmentation Methods for Software Vulnerability Detection", "categories": ["cs.SE", "cs.CR", "cs.LG"], "comment": "Accepted at EASE 2025, Istanbul, Turkey", "summary": "Various deep learning (DL) methods have recently been utilized to detect\nsoftware vulnerabilities. Real-world software vulnerability datasets are rare\nand hard to acquire, as there is no simple metric for classifying\nvulnerability. Such datasets are heavily imbalanced, and none of the current\ndatasets are considered huge for DL models. To tackle these problems, a recent\nwork has tried to augment the dataset using the source code and generate\nrealistic single-statement vulnerabilities, which is not quite practical and\nrequires manual checking of the generated vulnerabilities. In this paper, we\naim to explore the augmentation of vulnerabilities at the representation level\nto help current models learn better, which has never been done before to the\nbest of our knowledge. We implement and evaluate five augmentation techniques\nthat augment the embedding of the data and have recently been used for code\nsearch, which is a completely different software engineering task. We also\nintroduced a conditioned version of those augmentation methods, which ensures\nthe augmentation does not change the vulnerable section of the vector\nrepresentation. We show that such augmentation methods can be helpful and\nincrease the F1-score by up to 9.67%, yet they cannot beat Random Oversampling\nwhen balancing datasets, which increases the F1-score by 10.82%.", "AI": {"tldr": "The paper explores representation-level augmentation for software vulnerability detection, evaluating five techniques to improve model performance, though they don't outperform Random Oversampling.", "motivation": "Real-world vulnerability datasets are rare, imbalanced, and small for DL models, prompting the need for better augmentation methods.", "method": "Five embedding-level augmentation techniques are implemented and evaluated, including a conditioned version to preserve vulnerable sections.", "result": "Augmentation improves F1-score by up to 9.67%, but Random Oversampling performs better with a 10.82% increase.", "conclusion": "Representation-level augmentation is beneficial but less effective than Random Oversampling for balancing datasets."}}
{"id": "2504.16098", "pdf": "https://arxiv.org/pdf/2504.16098", "abs": "https://arxiv.org/abs/2504.16098", "authors": ["Tianning Feng", "Junting Ni", "Ezequiel Gleichgerrcht", "Wei Jin"], "title": "SeizureFormer: A Transformer Model for IEA-Based Seizure Risk Forecasting", "categories": ["eess.SP", "cs.LG", "I.5.1; I.2.6"], "comment": "9 pages, 2 figures. Submitted as an undergraduate honors thesis at\n  Emory University", "summary": "We present SeizureFormer, a Transformer-based model for long-term seizure\nrisk forecasting using interictal epileptiform activity (IEA) surrogate\nbiomarkers and long episode (LE) biomarkers from responsive neurostimulation\n(RNS) systems. Unlike raw scalp EEG-based models, SeizureFormer leverages\nstructured, clinically relevant features and integrates CNN-based patch\nembedding, multi-head self-attention, and squeeze-and-excitation blocks to\nmodel both short-term dynamics and long-term seizure cycles. Tested across five\npatients and multiple prediction windows (1 to 14 days), SeizureFormer achieved\nstate-of-the-art performance with mean ROC AUC of 79.44 percent and mean PR AUC\nof 76.29 percent. Compared to statistical, machine learning, and deep learning\nbaselines, it demonstrates enhanced generalizability and seizure risk\nforecasting performance under class imbalance. This work supports future\nclinical integration of interpretable and robust seizure forecasting tools for\npersonalized epilepsy management.", "AI": {"tldr": "SeizureFormer is a Transformer-based model for long-term seizure risk forecasting using IEA and LE biomarkers, outperforming baselines with ROC AUC of 79.44% and PR AUC of 76.29%.", "motivation": "To improve seizure risk forecasting by leveraging structured biomarkers and advanced deep learning techniques for better generalizability and performance under class imbalance.", "method": "Uses CNN-based patch embedding, multi-head self-attention, and squeeze-and-excitation blocks to model short-term dynamics and long-term seizure cycles.", "result": "Achieved state-of-the-art performance with mean ROC AUC of 79.44% and PR AUC of 76.29% across five patients and multiple prediction windows.", "conclusion": "Supports clinical integration of interpretable and robust seizure forecasting tools for personalized epilepsy management."}}
{"id": "2504.16688", "pdf": "https://arxiv.org/pdf/2504.16688", "abs": "https://arxiv.org/abs/2504.16688", "authors": ["Nahshon Mokua Obiri", "Kristof Van Laerhoven"], "title": "A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation for 6G: MLR, ANOVA, and Residual Distribution Analysis", "categories": ["cs.NI", "cs.LG", "eess.SP"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media. This is the accepted version of the article: To appear in the\n  2025 Joint European Conference on Networks and Communications & 6G Summit\n  (EuCNC/6G Summit)", "summary": "Modeling path loss in indoor LoRaWAN technology deployments is inherently\nchallenging due to structural obstructions, occupant density and activities,\nand fluctuating environmental conditions. This study proposes a two-stage\napproach to capture and analyze these complexities using an extensive dataset\nof 1,328,334 field measurements collected over six months in a single-floor\noffice at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,\nwe implement a multiple linear regression model that includes traditional\npropagation metrics (distance, structural walls) and an extension with proposed\nenvironmental variables (relative humidity, temperature, carbon dioxide,\nparticulate matter, and barometric pressure). Using analysis of variance, we\ndemonstrate that adding these environmental factors can reduce unexplained\nvariance by 42.32 percent. Secondly, we examine residual distributions by\nfitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,\nStudent's t, and Gaussian Mixture Models with one to five components. Our\nresults show that a four-component Gaussian Mixture Model captures the residual\nheterogeneity of indoor signal propagation most accurately, significantly\noutperforming single-distribution approaches. Given the push toward\nultra-reliable, context-aware communications in 6G networks, our analysis shows\nthat environment-aware modeling can substantially improve LoRaWAN network\ndesign in dynamic indoor IoT deployments.", "AI": {"tldr": "The paper proposes a two-stage approach to model indoor LoRaWAN path loss, using environmental variables and Gaussian Mixture Models for better accuracy.", "motivation": "Challenges in modeling indoor LoRaWAN path loss due to structural and environmental factors.", "method": "Two-stage approach: multiple linear regression with environmental variables and residual analysis using Gaussian Mixture Models.", "result": "Adding environmental factors reduces unexplained variance by 42.32%; a four-component Gaussian Mixture Model best captures residual heterogeneity.", "conclusion": "Environment-aware modeling improves LoRaWAN network design for dynamic indoor IoT deployments, aligning with 6G goals."}}
