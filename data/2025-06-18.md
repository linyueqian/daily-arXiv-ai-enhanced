<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 111]
- [cs.CV](#cs.CV) [Total: 112]
- [cs.AI](#cs.AI) [Total: 72]
- [cs.SD](#cs.SD) [Total: 20]
- [cs.LG](#cs.LG) [Total: 143]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 6]
- [eess.IV](#eess.IV) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/pdf/2506.13796)
*Zhou Chen, Xiao Wang, Yuanhong Liao, Ming Lin, Yuqi Bai*

Main category: cs.CL

TL;DR: The paper introduces an automated method to create high-quality climate change instruction data, resulting in the ClimateChat-Corpus dataset and the ClimateChat LLM, which improves performance on climate-related tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of efficient methods to produce large volumes of high-precision climate change instruction data, which limits the development of specialized LLMs.

Method: Automated generation of instructions using facts and background knowledge from documents, enhanced by web scraping and seed instruction collection. The dataset (ClimateChat-Corpus) was used to fine-tune open-source LLMs.

Result: ClimateChat significantly improved performance on climate change Q&A tasks. The study also evaluated the impact of base models and instruction data on LLM performance.

Conclusion: The research provides empirical support for constructing climate change instruction data and training specialized LLMs, highlighting the importance of base model selection.

Abstract: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.

</details>


### [2] [Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles](https://arxiv.org/pdf/2506.13886)
*Antara Raaghavi Bhattacharya, Isabel Papadimitriou, Kathryn Davidson, David Alvarez-Melis*

Main category: cs.CL

TL;DR: LLMs struggle with cross-linguistic numeral puzzles due to inability to infer implicit compositional rules, unlike humans.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs fail at linguistic-mathematical puzzles involving diverse numeral systems, while humans succeed.

Method: Conducted experiments to separate linguistic and mathematical aspects of numbers, testing LLMs on problems with and without explicit mathematical symbols.

Result: LLMs only solve problems when mathematical operations are explicitly marked; they lack human-like inference of implicit numeral structure.

Conclusion: Flexibly inferring compositional rules from implicit patterns remains a challenge for current reasoning models.

Abstract: Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.

</details>


### [3] [VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training](https://arxiv.org/pdf/2506.13888)
*Jipeng Zhang, Kehao Miao, Renjie Pi, Zhaowei Wang, Runtao Liu, Rui Pan, Tong Zhang*

Main category: cs.CL

TL;DR: The paper introduces an iterative training framework to address challenges in training Vision-Language Reward Models (VL-RMs), improving hallucination detection and multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing VL-RMs face bootstrapping dilemmas and modality bias, leading to flawed training data and reinforcement of biases.

Method: Proposes an iterative framework using vision experts, Chain-of-Thought rationales, and Margin-based Rejection Sampling to refine preference datasets and enhance critiques.

Result: Demonstrates superior performance in hallucination detection and multimodal reasoning on VL-RM benchmarks.

Conclusion: The framework advances VL model alignment with reinforcement learning by addressing key training challenges.

Abstract: Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.

</details>


### [4] [EmoNews: A Spoken Dialogue System for Expressive News Conversations](https://arxiv.org/pdf/2506.13894)
*Ryuki Matsuura, Shikhar Bharadwaj, Jiarui Liu, Dhatchi Kunde Govindarajan*

Main category: cs.CL

TL;DR: A task-oriented spoken dialogue system (SDS) is developed to regulate emotional speech in news conversations, outperforming baselines in engagement and emotion regulation.


<details>
  <summary>Details</summary>
Motivation: Address the gap in emotional SDS research due to compartmentalized studies and lack of standardized metrics for social goals.

Method: Uses an LLM-based sentiment analyzer and PromptTTS for context-appropriate emotional speech, with a proposed subjective evaluation scale.

Result: The emotional SDS outperformed baselines in emotion regulation and engagement.

Conclusion: Speech emotion plays a critical role in engaging conversations; the system's code is open-sourced.

Abstract: We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1

</details>


### [5] [Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages](https://arxiv.org/pdf/2506.14177)
*Tuan Nguyen, Huy-Dat Tran*

Main category: cs.CL

TL;DR: The study proposes a phrase-level mixing method to generate synthetic code-switching (CS) data for improving ASR performance in under-resourced languages, achieving notable gains.


<details>
  <summary>Details</summary>
Motivation: Code-switching in multilingual settings poses challenges for ASR due to scarce and costly transcribed data. This work aims to address this by leveraging synthetic CS data.

Method: A phrase-level mixing method generates synthetic CS data to mimic natural patterns. Monolingual and synthetic CS data are used to fine-tune pretrained ASR models (Whisper, MMS, SeamlessM4T).

Result: The training strategy improves ASR performance on monolingual and CS tests, with the highest gains for Malay-English, followed by Tamil-English and Mandarin-Malay.

Conclusion: The approach offers a cost-effective solution for CS-ASR development, benefiting both research and industry.

Abstract: Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.

</details>


### [6] [Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations](https://arxiv.org/pdf/2506.13901)
*Abhilekh Borah, Chhavi Sharma, Danush Khanna, Utkarsh Bhatt, Gurpreet Singh, Hasnat Md Abdullah, Raghav Kaushik Ravi, Vinija Jain, Jyoti Patel, Shubham Singh, Vasu Sharma, Arpita Vats, Rahul Raja, Aman Chadha, Amitava Das*

Main category: cs.CL

TL;DR: The paper introduces the Alignment Quality Index (AQI) to assess LLM alignment by analyzing latent space separation of safe/unsafe activations, addressing blind spots in current evaluations.


<details>
  <summary>Details</summary>
Motivation: Current evaluations (e.g., refusal rates, toxicity classifiers) fail to detect hidden misalignments, jailbreaking, and alignment faking in LLMs, necessitating a more robust metric.

Method: AQI combines geometric clustering measures (DBS, DI, XBI, CHI) to evaluate alignment quality in latent space, independent of output behavior. The LITMUS dataset supports empirical validation.

Result: AQI correlates with external judges and uncovers vulnerabilities missed by traditional metrics, demonstrating effectiveness across models trained with DPO, GRPO, and RLHF.

Conclusion: AQI provides a decoding-invariant, behavior-agnostic tool for safety auditing, with potential to improve alignment evaluation in high-stakes domains.

Abstract: Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.
  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.
  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.

</details>


### [7] [AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR](https://arxiv.org/pdf/2506.14190)
*Tuan Nguyen, Huy-Dat Tran*

Main category: cs.CL

TL;DR: AsyncSwitch is a framework for improving code-switched ASR by pre-exposing models to text data before fine-tuning, achieving a 9.02% WER reduction.


<details>
  <summary>Details</summary>
Motivation: Challenges in code-switched ASR include language ambiguity and limited multilingual data, making synthetic audio generation costly and unscalable.

Method: A three-stage process: (1) train decoder on code-switched text, (2) align decoder and encoder with limited speech-text data, and (3) fine-tune the entire model.

Result: 9.02% relative WER reduction in Malay-English code-switching; improved monolingual performance in Singlish, Malay, and English variants.

Conclusion: AsyncSwitch effectively leverages text data to enhance code-switched ASR performance while maintaining monolingual accuracy.

Abstract: Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants.

</details>


### [8] [ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection](https://arxiv.org/pdf/2506.13956)
*Shang-Chi Tsai, Seiya Kawano, Angel Garcia Contreras, Koichiro Yoshino, Yun-Nung Chen*

Main category: cs.CL

TL;DR: A novel framework for data augmentation in robotic assistance uses large language and stable diffusion models to generate synthetic dialogues and images, improving multimodal model performance with limited real-world data.


<details>
  <summary>Details</summary>
Motivation: Enhancing robot intent understanding in human activities requires multimodal data, but collecting large-scale datasets is challenging.

Method: Leverages large language models for synthetic dialogues and stable diffusion for environmental images to augment training data.

Result: Significantly improves robot action selection, achieving state-of-the-art performance on real-world datasets.

Conclusion: The proposed framework effectively addresses data scarcity in multimodal robotic assistance, enhancing model accuracy.

Abstract: When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.

</details>


### [9] [Are manual annotations necessary for statutory interpretations retrieval?](https://arxiv.org/pdf/2506.13965)
*Aleksander Smywiński-Pohl, Tomer Libal, Adam Kaczmarczyk, Magdalena Król*

Main category: cs.CL

TL;DR: The paper explores automating the retrieval of legal interpretations by determining the optimal manual annotation volume, scope, and necessity, including using LLMs for automation.


<details>
  <summary>Details</summary>
Motivation: To reduce the costly and repetitive manual annotation process in legal research for retrieving relevant interpretations of legal concepts.

Method: Experiments to determine optimal annotation volume, selection strategies (random vs. best candidates), and automation using LLMs.

Result: Findings on the optimal number of annotations, performance gains from annotating best candidates, and outcomes of LLM-based automation.

Conclusion: Automation and strategic annotation can improve efficiency in legal interpretation retrieval, reducing reliance on manual processes.

Abstract: One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM.

</details>


### [10] [AI shares emotion with humans across languages and cultures](https://arxiv.org/pdf/2506.13978)
*Xiuwen Wu, Hao Wang, Zhiang Yan, Xiaohan Tang, Pengfei Xu, Wai-Ting Siok, Ping Li, Jia-Hong Gao, Bingjiang Lyu, Lang Qin*

Main category: cs.CL

TL;DR: LLMs align with human emotion representation and can be controlled using human-centric emotion concepts.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs represent emotions like humans and if their emotional tone can be controlled.

Method: Assessed human-AI emotional alignment using interpretable LLM features for nuanced emotion categories, analyzing valence and arousal.

Result: LLM-derived emotion spaces align with human perception and predict behavioral data. Emotion output can be modulated using human concepts.

Conclusion: AI shares emotional representations with humans, and its affective outputs can be precisely guided using psychological concepts.

Abstract: Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.

</details>


### [11] [Beyond Browsing: API-Based Web Agents](https://arxiv.org/pdf/2410.16464)
*Yueqi Song, Frank Xu, Shuyan Zhou, Graham Neubig*

Main category: cs.CL

TL;DR: API-based and hybrid AI agents outperform traditional web browsing agents in online tasks, with hybrid agents achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Explore the potential of AI agents using APIs instead of web browsing for online tasks, given APIs are designed for machine interaction.

Method: Propose two agent types: API-calling (API-only) and Hybrid (API + web browsing). Test on WebArena benchmark.

Result: Hybrid agents achieve 38.9% success rate, a 24.0% improvement over web browsing alone. API-based agents also outperform browsing agents.

Conclusion: APIs offer a superior alternative to web browsing for AI agents when available, with hybrid approaches yielding the best results.

Abstract: Web browsers are a portal to the internet, where much of human activity is undertaken. Thus, there has been significant research work in AI agents that interact with the internet through web browsing. However, there is also another interface designed specifically for machine interaction with online content: application programming interfaces (APIs). In this paper we ask -- what if we were to take tasks traditionally tackled by Browsing Agents, and give AI agents access to APIs? To do so, we propose two varieties of agents: (1) an API-calling agent that attempts to perform online tasks through APIs only, similar to traditional coding agents, and (2) a Hybrid Agent that can interact with online data through both web browsing and APIs. In experiments on WebArena, a widely-used and realistic benchmark for web navigation tasks, we find that API-Based Agents outperform web Browsing Agents. Hybrid Agents out-perform both others nearly uniformly across tasks, resulting in a more than 24.0% absolute improvement over web browsing alone, achieving a success rate of 38.9%, the SOTA performance among task-agnostic agents. These results strongly suggest that when APIs are available, they present an attractive alternative to relying on web browsing alone.

</details>


### [12] [Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text](https://arxiv.org/pdf/2506.14012)
*Amr Mohamed, Yang Zhang, Michalis Vazirgiannis, Guokan Shang*

Main category: cs.CL

TL;DR: The paper evaluates how Large Language Models (LLMs) handle code-switched text, showing mixed performance with degradation in some cases but improved comprehension when English is embedded in other languages. Fine-tuning is more effective than prompting for mitigating issues.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs process code-switched text is crucial due to its prevalence in multilingual online communication.

Method: The study generates code-switched variants of reasoning and comprehension benchmarks to systematically evaluate LLM performance.

Result: Performance degrades when foreign tokens disrupt English, but embedding English in other languages often improves comprehension. Fine-tuning mitigates degradation better than prompting.

Conclusion: Fine-tuning is a stable solution for improving LLM performance on code-switched text, though embedding English in other languages can also enhance comprehension.

Abstract: Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\unicode{x2013}$even under linguistic constraints$\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.

</details>


### [13] [A Variational Framework for Improving Naturalness in Generative Spoken Language Models](https://arxiv.org/pdf/2506.14767)
*Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky*

Main category: cs.CL

TL;DR: The paper proposes an end-to-end variational method to enhance speech tokens by automatically encoding continuous speech attributes, improving naturalness without manual feature engineering.


<details>
  <summary>Details</summary>
Motivation: Existing speech token methods focus on linguistic aspects but neglect prosody, leading to unnatural speech. Adding pitch features is insufficient and requires manual effort.

Method: An end-to-end variational approach is introduced to learn and encode continuous speech attributes, eliminating manual feature extraction.

Result: The method produces speech continuations preferred by human raters, demonstrating improved naturalness.

Conclusion: The proposed approach effectively enhances speech tokens by automating paralinguistic feature encoding, outperforming manual methods.

Abstract: The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.

</details>


### [14] [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/pdf/2506.00854)
*Jacky Tai-Yu Lu, Jung Chiang, Chi-Sheng Chen, Anna Nai-Yun Tung, Hsiang Wei Hu, Yuan Chiao Cheng*

Main category: cs.CL

TL;DR: EEG2TEXT-CN is an EEG-to-text framework for Chinese, using a biologically grounded encoder and pretrained language model for zero-shot sentence prediction.


<details>
  <summary>Details</summary>
Motivation: To pioneer open-vocabulary EEG-to-text generation for Chinese, addressing the gap in multilingual brain-to-text research.

Method: Combines NICE-EEG encoder and MiniLM language model with masked pretraining and contrastive learning. Uses per-character EEG embeddings and teacher forcing for decoding.

Result: Achieves a BLEU-1 score of 6.38%, showing lexical alignment but limited syntactic fluency.

Conclusion: Demonstrates feasibility of non-phonetic EEG-to-text decoding, paving the way for future Chinese cognitive-language interfaces.

Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.

</details>


### [15] [MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation](https://arxiv.org/pdf/2506.14028)
*Xueqing Peng, Lingfei Qian, Yan Wang, Ruoyu Xiang, Yueru He, Yang Ren, Mingyang Jiang, Jeff Zhao, Huan He, Yi Han, Yun Feng, Yuechen Jiang, Yupeng Cao, Haohang Li, Yangyang Yu, Xiaoyu Wang, Penglei Gao, Shengyuan Lin, Keyi Wang, Shanshan Yang, Yilun Zhao, Zhiwei Liu, Peng Lu, Jerry Huang, Suyuchen Wang, Triantafillos Papadopoulos, Polydoros Giannouris, Efstathia Soufleri, Nuo Chen, Guojun Xiong, Zhiyang Deng, Yijia Zhao, Mingquan Lin, Meikang Qiu, Kaleb E Smith, Arman Cohan, Xiao-Yang Liu, Jimin Huang, Alejandro Lopez-Lira, Xi Chen, Junichi Tsujii, Jian-Yun Nie, Sophia Ananiadou, Qianqian Xie*

Main category: cs.CL

TL;DR: MultiFinBen is the first multilingual and multimodal benchmark for financial NLP, introducing novel tasks and revealing limitations of current LLMs in complex, cross-lingual, and multimodal financial tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for financial NLP are limited to monolingual and unimodal settings, failing to reflect real-world financial communication complexity.

Method: Introduces MultiFinBen, a benchmark with tasks like PolyFiQA (multilingual financial QA) and OCR-embedded QA, using a dynamic, difficulty-aware selection mechanism.

Result: Evaluation of 22 models shows even the strongest struggle with complex cross-lingual and multimodal financial tasks.

Conclusion: MultiFinBen aims to advance transparent, reproducible, and inclusive progress in financial NLP by addressing current benchmark limitations.

Abstract: Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.

</details>


### [16] [An Interdisciplinary Review of Commonsense Reasoning and Intent Detection](https://arxiv.org/pdf/2506.14040)
*Md Nazmus Sakib*

Main category: cs.CL

TL;DR: A review of recent advances in commonsense reasoning and intent detection, analyzing 28 papers (2020-2025) from ACL, EMNLP, and CHI, organized by methodology and application.


<details>
  <summary>Details</summary>
Motivation: To explore key challenges in natural language understanding by bridging insights from NLP and HCI, highlighting trends and gaps.

Method: Analysis of 28 papers, organizing them by methodology (e.g., zero-shot learning, open-set models) and application (e.g., cultural adaptation, human-centered systems).

Result: Identified trends toward adaptive, multilingual, and context-aware models, and gaps in grounding, generalization, and benchmark design.

Conclusion: The review underscores the need for more robust models and benchmarks in commonsense reasoning and intent detection, integrating NLP and HCI perspectives.

Abstract: This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.

</details>


### [17] [Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications](https://arxiv.org/pdf/2506.14046)
*David Kogan, Max Schumacher, Sam Nguyen, Masanori Suzuki, Melissa Smith, Chloe Sophia Bellows, Jared Bernstein*

Main category: cs.CL

TL;DR: Ace-CEFR is a dataset for evaluating conversational text difficulty, outperforming human experts in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: To address the lack of tools for assessing language difficulty in conversational text for LLM training.

Method: Created Ace-CEFR, a dataset annotated by experts, and tested Transformer-based models and LLMs.

Result: Models trained on Ace-CEFR measure text difficulty more accurately and faster than humans.

Conclusion: Ace-CEFR is released publicly to aid research and development in text difficulty assessment.

Abstract: There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.

</details>


### [18] [Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data](https://arxiv.org/pdf/2506.14064)
*Iona Carslaw, Sivan Milton, Nicolas Navarre, Ciyang Qing, Wataru Uegaki*

Main category: cs.CL

TL;DR: A method for detecting and annotating naturally-occurring English embedded clauses in large corpora using parsing heuristics, evaluated on a hand-annotated dataset (GECS) and applied to extract a large dataset from Dolma.


<details>
  <summary>Details</summary>
Motivation: Current research on embedded clauses relies on schematic examples, missing statistical insights from natural language corpora.

Method: Uses constituency parsing and parsing heuristics to detect and annotate embedded clauses in large text data.

Result: Developed a tool evaluated on GECS and extracted a large dataset of naturally-occurring embedded clauses from Dolma.

Conclusion: The approach enables large-scale analysis of embedded clauses using natural language data, advancing linguistic research.

Abstract: For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.

</details>


### [19] [Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models](https://arxiv.org/pdf/2506.13300)
*Bo Li, Chengben Xu, Wufeng Zhang*

Main category: cs.CL

TL;DR: Seewo's system for MLC-SLM Challenge improves ASR and SD-ASR using multi-stage training with curriculum learning, Chain-of-Thought data augmentation, and RLVR, achieving significant WER/CER reductions.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning and self-correction in speech language models for ASR and SD-ASR tasks in the MLC-SLM Challenge.

Method: Multi-stage training pipeline with curriculum learning, Chain-of-Thought data augmentation, and Reinforcement Learning with Verifiable Rewards (RLVR).

Result: Achieved WER/CER of 11.57% for Track 1 and tcpWER/tcpCER of 17.67% for Track 2, outperforming baselines.

Conclusion: The proposed components are effective, as shown by ablation studies, and significantly improve performance in the challenge.

Abstract: This paper presents Seewo's systems for both tracks of the Multilingual Conversational Speech Language Model Challenge (MLC-SLM), addressing automatic speech recognition (ASR) and speaker diarization with ASR (SD-ASR). We introduce a multi-stage training pipeline that explicitly enhances reasoning and self-correction in speech language models for ASR. Our approach combines curriculum learning for progressive capability acquisition, Chain-of-Thought data augmentation to foster intermediate reflection, and Reinforcement Learning with Verifiable Rewards (RLVR) to further refine self-correction through reward-driven optimization. This approach achieves substantial improvements over the official challenge baselines. On the evaluation set, our best system attains a WER/CER of 11.57% for Track 1 and a tcpWER/tcpCER of 17.67% for Track 2. Comprehensive ablation studies demonstrate the effectiveness of each component under challenge constraints.

</details>


### [20] [Abstract Meaning Representation for Hospital Discharge Summarization](https://arxiv.org/pdf/2506.14101)
*Paul Landes, Sitara Rao, Aaron Jeremy Chaise, Barbara Di Eugenio*

Main category: cs.CL

TL;DR: The paper addresses hallucination in LLMs for clinical discharge summaries, proposing a method combining language-based graphs and deep learning to improve reliability.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs poses risks in clinical settings; automating discharge summaries can reduce physician workload.

Method: Combines language-based graphs and deep learning to ensure content provenance and trustworthiness.

Result: Demonstrates impressive reliability on MIMIC-III and clinical notes from Anonymous Hospital.

Conclusion: The method effectively addresses hallucination, providing reliable automated discharge summaries.

Abstract: The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.

</details>


### [21] [Essential-Web v1.0: 24T tokens of organized web data](https://arxiv.org/pdf/2506.14111)
*Essential AI, :, Andrew Hojel, Michael Pust, Tim Romanski, Yash Vanjani, Ritvik Kapila, Mohit Parmar, Adarsh Chaluvaraju, Alok Tripathy, Anil Thomas, Ashish Tanwer, Darsh J Shah, Ishaan Shah, Karl Stratos, Khoi Nguyen, Kurt Smith, Michael Callahan, Peter Rushton, Philip Monk, Platon Mazarakis, Saad Jamal, Saurabh Srivastava, Somanshu Singla, Ashish Vaswani*

Main category: cs.CL

TL;DR: Essential-Web v1.0 is a 24-trillion-token dataset with a 12-category taxonomy, enabling efficient data filtering for competitive performance in specialized domains.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of massive, well-organized pre-training datasets to reduce costly and inaccessible data pipelines.

Method: Uses Essential-Web v1.0, annotated by EAI-Distill-0.5b, a fine-tuned model, and allows SQL-style filtering for dataset creation.

Result: Achieves competitive performance in math, web code, STEM, and medical domains compared to SOTA.

Conclusion: Essential-Web v1.0 provides a scalable and efficient solution for high-quality dataset curation.

Abstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0

</details>


### [22] [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/pdf/2506.14123)
*Jonathan Hayase, Alisa Liu, Noah A. Smith, Sewoong Oh*

Main category: cs.CL

TL;DR: A method to convert autoregressive LMs with BPE tokenizers into character/byte-level LMs, solving the Prompt Boundary Problem and enabling model ensembling and interoperability.


<details>
  <summary>Details</summary>
Motivation: Tokenization introduces distortions like the Prompt Boundary Problem (PBP) and hinders model interoperability due to mismatched vocabularies.

Method: Inference-time conversion of BPE-tokenized LMs to character/byte-level LMs without altering generative distribution.

Result: Solves PBP, unifies vocabularies for ensembling, and improves downstream performance via proxy-tuning.

Conclusion: The method effectively addresses tokenization issues, enhancing model flexibility and performance.

Abstract: Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.

</details>


### [23] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/pdf/2506.14157)
*Chengyu Huang, Tanya Goyal*

Main category: cs.CL

TL;DR: The paper introduces Distance Calibrated Reward Margin (DCRM) to measure response pair quality for preference optimization (PO), showing higher DCRM correlates with better learning outcomes. A best-of-N² pairing method is proposed to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Existing preference datasets may not align with desirable learning differences, so the study aims to quantify and optimize these differences for better PO performance.

Method: The authors use distance and reward margin to create DCRM, then analyze 3 types of preference datasets. They propose a best-of-N² pairing method to select high-DCRM pairs.

Result: Higher DCRM in training sets correlates with improved learning outcomes. The proposed method outperforms existing datasets on benchmarks like AlpacaEval, MT-Bench, and Arena-Hard.

Conclusion: DCRM effectively measures response pair quality, and the best-of-N² method enhances model performance, demonstrating the importance of dataset quality in PO.

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


### [24] [S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models](https://arxiv.org/pdf/2506.14158)
*Tao He, Guang Huang, Yu Yang, Tianshi Xu, Sicheng Zhao, Guiguang Ding, Pengyang Wang, Feng Tian*

Main category: cs.CL

TL;DR: S$^4$C improves speculative sampling for LLMs by incorporating syntactic and semantic coherence, achieving faster token generation and validation with fewer resources.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs face high inference latency, limiting real-time applications. Existing speculative sampling methods lack coherence awareness, reducing efficiency.

Method: Proposes S$^4$C, using multi-head drafting and a continuous verification tree for faster token generation and validation while maintaining coherence.

Result: S$^4$C outperforms baselines, achieving 2.26x-2.60x acceleration on Spec-bench benchmarks with higher efficiency and parallelism.

Conclusion: S$^4$C enhances speculative sampling by leveraging coherence, offering significant speedups and resource efficiency for LLM inference.

Abstract: Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.

</details>


### [25] [MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind](https://arxiv.org/pdf/2506.14161)
*Yanlin Li, Hao Liu, Huimin Liu, Yinwei Wei, Yupeng Hu*

Main category: cs.CL

TL;DR: The paper proposes a framework to evaluate implicit bias in LLMs' Theory of Mind (ToM) using the Stereotype Content Model (SCM) and introduces indirect tasks (WABT and AAT) to uncover multi-dimensional bias structures.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating bias in LLMs' ToM are limited by social desirability effects and fail to capture subtle, multi-dimensional biases.

Method: The framework uses the SCM to conceptualize bias across Competence, Sociability, and Morality, employing indirect tasks (WABT and AAT) to assess implicit associations and affective leanings.

Result: Experiments on 8 LLMs reveal complex bias structures, including sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification.

Conclusion: The proposed framework offers a robust methodology for identifying implicit bias in LLMs, addressing limitations of conventional evaluation methods.

Abstract: Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.

</details>


### [26] [GRAM: A Generative Foundation Reward Model for Reward Generalization](https://arxiv.org/pdf/2506.14175)
*Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu*

Main category: cs.CL

TL;DR: The paper introduces a generative reward model for aligning large language models (LLMs), trained via unsupervised learning and fine-tuned with supervised learning, showing improved generalization across tasks.


<details>
  <summary>Details</summary>
Motivation: Standard reward models rely solely on labeled human preference data; this work explores using both unlabeled and labeled data for training, leveraging generative capabilities of LLMs.

Method: Develops a generative reward model trained via large-scale unsupervised learning, fine-tuned with supervised learning, and uses label smoothing to optimize a regularized pairwise ranking loss.

Result: The model generalizes well across tasks like response ranking, reinforcement learning from human feedback, and task adaptation, outperforming strong baselines.

Conclusion: The approach links generative and discriminative models under a unified training objective, yielding a versatile foundation reward model requiring minimal fine-tuning.

Abstract: In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.

</details>


### [27] [MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment](https://arxiv.org/pdf/2506.14199)
*Junghwan Kim, Kieun Park, Sohee Park, Hyunggug Kim, Bongwon Suh*

Main category: cs.CL

TL;DR: MAS-LitEval, a multi-agent system using LLMs, outperforms traditional metrics like BLEU and METEOR in evaluating literary translations by assessing terminology, narrative, and style.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics fail to capture cultural nuances and stylistic elements in literary translations, necessitating a more nuanced evaluation method.

Method: Proposed MAS-LitEval, a multi-agent system leveraging LLMs, tested on translations of literary works like The Little Prince.

Result: MAS-LitEval achieved up to 0.890 in capturing literary nuances, outperforming traditional metrics.

Conclusion: MAS-LitEval provides a scalable, nuanced framework for Translation Quality Assessment, aiding translators and researchers.

Abstract: Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.

</details>


### [28] [ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations](https://arxiv.org/pdf/2506.14200)
*Brihi Joshi, Keyu He, Sahana Ramnath, Sadra Sabouri, Kaitlyn Zhou, Souti Chattopadhyay, Swabha Swayamdipta, Xiang Ren*

Main category: cs.CL

TL;DR: ELI-Why benchmark evaluates language models' ability to tailor explanations for learners of varying educational levels, finding GPT-4 explanations less effective than human-curated ones.


<details>
  <summary>Details</summary>
Motivation: To assess and improve language models' pedagogical capabilities in providing tailored explanations for diverse learners.

Method: Introduces ELI-Why benchmark (13.4K "Why" questions) and conducts two human studies comparing GPT-4 and human-curated explanations for elementary, high-school, and graduate levels.

Result: GPT-4 explanations matched intended grade levels 50% of the time (vs. 79% for humans) and were deemed 20% less suited to learners' needs. Automated metrics also showed indistinct grade-level tailoring.

Conclusion: Current language models, including GPT-4, struggle to effectively tailor explanations for diverse educational needs, highlighting a gap in pedagogical effectiveness.

Abstract: Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.

</details>


### [29] [Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation](https://arxiv.org/pdf/2506.14203)
*Jongho Kim, Romain Storaï, Seung-won Hwang*

Main category: cs.CL

TL;DR: The study explores using language models to help anomia patients by addressing term failure and semantic paraphasia errors through gradient-based selective augmentation, showing strong results on real patient data.


<details>
  <summary>Details</summary>
Motivation: To assist anomia patients in identifying target items despite unseen terms and semantic paraphasia errors.

Method: Proposes gradient-based selective augmentation: gradient value controls data quality amid errors, while gradient variance includes unseen relevant terms. Evaluated on Tip-of-the-Tongue dataset and AphasiaBank patient data.

Result: Demonstrates strong performance against baselines, effectively aiding anomia patients.

Conclusion: The approach successfully addresses key challenges in aiding anomia patients, showing promise for practical applications.

Abstract: In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.

</details>


### [30] [AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents](https://arxiv.org/pdf/2506.14205)
*Jingxu Xie, Dylan Xu, Xuandong Zhao, Dawn Song*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth

</details>


### [31] [CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation](https://arxiv.org/pdf/2506.14206)
*Jia-Chen Zhang, Zheng Zhou, Yu-Jie Xiong, Chun-Ming Xia, Fei Dai*

Main category: cs.CL

TL;DR: CausalDiffTab is a diffusion model for generating high-quality mixed tabular data, addressing challenges like heterogeneous data types and complex relationships. It uses adaptive causal regularization for improved performance.


<details>
  <summary>Details</summary>
Motivation: High-quality tabular data generation is challenging due to heterogeneous data types and complex relationships. Existing methods struggle with these issues.

Method: Introduces CausalDiffTab, a diffusion model for mixed tabular data, with hybrid adaptive causal regularization based on Hierarchical Prior Fusion.

Result: Outperforms baseline methods on seven datasets across all metrics.

Conclusion: CausalDiffTab effectively generates high-quality tabular data while capturing complex interactions, with publicly available code.

Abstract: Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab.

</details>


### [32] [Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation](https://arxiv.org/pdf/2506.14211)
*Sina Abdidizaji, Md Kowsher, Niloofar Yousefi, Ivan Garibay*

Main category: cs.CL

TL;DR: Improved detection of implicit influential patterns in digital conversations using augmented datasets and advanced language models.


<details>
  <summary>Details</summary>
Motivation: Address the shift from explicit to implicit linguistic strategies used to influence public perception digitally.

Method: Augmented existing datasets using state-of-the-art language models to detect and locate implicit influential patterns in conversations.

Result: 6% improvement in detecting implicit patterns; 33% and 43% better performance in multi-label classification for influence techniques and victim vulnerability.

Conclusion: The proposed framework effectively identifies and locates implicit influential patterns, enhancing detection and classification tasks.

Abstract: In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively.

</details>


### [33] [Chaining Event Spans for Temporal Relation Grounding](https://arxiv.org/pdf/2506.14213)
*Jongho Kim, Dohyeon Lee, Minsoo Kim, Seung-won Hwang*

Main category: cs.CL

TL;DR: The paper proposes a Timeline Reasoning Network (TRN) to improve temporal relation understanding by predicting event time spans, addressing unreliable answer overlaps in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for temporal relation tasks rely on answer overlaps, which can be unreliable due to spurious overlaps. The paper aims to improve accuracy by grounding answers in predicted timelines.

Method: The TRN uses a two-step inductive reasoning process: first answering questions with semantic/syntactic info, then chaining questions to predict a timeline for grounding answers.

Result: TRN outperforms previous methods on TORQUE, TB-dense, TRC, and TRE tasks by resolving spurious overlaps.

Conclusion: The proposed TRN effectively addresses the limitations of answer overlaps, improving temporal relation understanding through timeline-based reasoning.

Abstract: Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: "What finished right before the decision?" or "What finished right after the decision?". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.

</details>


### [34] [Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team](https://arxiv.org/pdf/2506.14234)
*Md Tanzib Hosain, Salman Rahman, Md Kishor Morol, Md Rizwan Parvez*

Main category: cs.CL

TL;DR: Xolver is a multi-agent reasoning framework for LLMs that integrates diverse experience modalities, outperforming specialized models by leveraging holistic experience learning.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack the ability to accumulate or integrate experiential knowledge, unlike expert problem solvers who learn from past experiences and collaboration.

Method: Xolver uses a training-free framework with persistent memory, incorporating external and self-retrieval, tool use, collaboration, evaluation, and iterative refinement.

Result: Xolver outperforms advanced models, achieving top results on benchmarks like GSM8K (98.1%) and AIME'24 (94.4%).

Conclusion: Holistic experience learning is a key step toward generalist agents capable of expert-level reasoning.

Abstract: Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.

</details>


### [35] [A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs](https://arxiv.org/pdf/2506.14235)
*Yimin Deng, Yuxia Wu, Yejing Wang, Guoshuai Zhao, Li Zhu, Qidong Liu, Derong Xu, Zichuan Fu, Xian Wu, Yefeng Zheng, Xiangyu Zhao, Xueming Qian*

Main category: cs.CL

TL;DR: Proposes MESH framework for temporal knowledge graph reasoning, integrating structural and semantic reasoning with expert modules for better prediction.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack integration of dual reasoning perspectives and fail to distinguish historical vs. non-historical events, limiting generalization.

Method: Uses Multi-Expert Structural-Semantic Hybrid (MESH) framework with three expert modules to combine structural and semantic information.

Result: Demonstrated effectiveness through extensive experiments on three datasets.

Conclusion: MESH improves temporal knowledge graph reasoning by addressing limitations of previous methods.

Abstract: Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.

</details>


### [36] [Re-Initialization Token Learning for Tool-Augmented Large Language Models](https://arxiv.org/pdf/2506.14248)
*Chenghao Li, Liu Liu, Baosheng Yu, Jiayan Qiu, Yibing Zhan*

Main category: cs.CL

TL;DR: A novel token learning method aligns tool tokens with word embeddings to improve LLMs' tool integration, outperforming baselines in tasks like numerical reasoning and plan generation.


<details>
  <summary>Details</summary>
Motivation: Current methods for integrating tools into LLMs lack adaptability due to misalignment between tool and word tokens, limiting performance in complex tasks.

Method: Proposes initializing and regularizing tool token embeddings based on tool names/descriptions to align them with the word token space.

Result: Demonstrates improved performance on GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets over CoT, REACT, ICL, and ToolkenGPT.

Conclusion: The method effectively enhances LLMs' tool integration by aligning tokens, improving accuracy in diverse domains.

Abstract: Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.

</details>


### [37] [From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents](https://arxiv.org/pdf/2506.14285)
*Seongbo Jang, Minjin Jeon, Jaehoon Lee, Seonghyeon Lee, Dongha Lee, Hwanjo Yu*

Main category: cs.CL

TL;DR: The paper introduces a novel task of timely dialogue response generation, proposes the TimelyChat benchmark, and presents Timer, a model trained to predict time intervals and generate timely responses.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored aspect of when to respond in dialogues, grounded on temporal context.

Method: Leverages a temporal commonsense knowledge graph and an LLM to synthesize 55K event-driven dialogues, then trains Timer for time-interval prediction and response generation.

Result: Timer outperforms prompting-based LLMs and fine-tuned baselines in evaluations.

Conclusion: The work advances dialogue systems by incorporating temporal grounding, with data, model, and code publicly released.

Abstract: While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.

</details>


### [38] [Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent](https://arxiv.org/pdf/2506.14302)
*Xueyang Feng, Jingsen Zhang, Jiakai Tang, Wei Li, Guohao Cai, Xu Chen, Quanyu Dai, Yue Zhu, Zhenhua Dong*

Main category: cs.CL

TL;DR: The paper introduces ECPO, a multi-turn preference optimization paradigm, to improve Conversational Recommendation Agents (CRAs) by modeling user satisfaction evolution and addressing dissatisfaction causes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current CRAs using LLMs often produce short-sighted responses, and preference optimization is costly and ineffective in multi-turn dialogues.

Method: ECPO leverages Expectation Confirmation Theory to model user satisfaction evolution and targets unsatisfactory responses for optimization, using an LLM-based simulator (AILO) for feedback.

Result: ECPO significantly enhances CRA interaction capabilities, improving efficiency and effectiveness over existing methods.

Conclusion: ECPO addresses the limitations of current MTPO methods by optimizing user satisfaction in multi-turn dialogues, achieving better performance with reduced overhead.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.

</details>


### [39] [Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics](https://arxiv.org/pdf/2506.14335)
*Silvia Casola, Yang Janet Liu, Siyao Peng, Oliver Kraus, Albert Gatt, Barbara Plank*

Main category: cs.CL

TL;DR: The paper investigates how the choice of reference sets affects reference-based summarization metrics, showing instability in popular metrics like ROUGE. It recommends incorporating reference set variation for more reliable evaluations, especially for LLMs.


<details>
  <summary>Details</summary>
Motivation: Human language production is diverse, but summarization evaluation often overlooks this variation. The impact of different reference sets on metrics is not well understood.

Method: Analyzes three multi-reference datasets (SummEval, GUMSum, DUC2004) and evaluates metric sensitivity. Human judgments on LLM outputs are also collected for correlation analysis.

Result: Popular metrics like ROUGE show significant instability, with model rankings varying by reference set. Human judgments correlate weakly or not at all with metrics for genre-diverse data.

Conclusion: Reference set variation should be incorporated into summarization evaluation to improve consistency and alignment with human judgments, particularly for LLMs.

Abstract: Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.

</details>


### [40] [A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis](https://arxiv.org/pdf/2506.14345)
*Bruno Martins, Piotr Szymański, Piotr Gramacki*

Main category: cs.CL

TL;DR: The paper discusses the need for geo-temporal capabilities in LLM-powered deep research systems to answer context-rich questions, proposing solutions for integration and evaluation.


<details>
  <summary>Details</summary>
Motivation: Current deep research systems lack geo-temporal reasoning, which is crucial for domains like public health and environmental science.

Method: The paper proposes augmenting retrieval and synthesis processes with geo-temporal constraints, supported by open infrastructures and rigorous evaluation.

Result: The vision outlines a path for more advanced, geo-temporally aware deep research systems.

Conclusion: Integrating geo-temporal reasoning could significantly enhance AI-driven information access.

Abstract: The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.

</details>


### [41] [Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits](https://arxiv.org/pdf/2506.14370)
*Amrit Poudel, Yifan Ding, Jurgen Pfeffer, Tim Weninger*

Main category: cs.CL

TL;DR: Google's search algorithms selectively promote or suppress certain hashtags and subreddits, influencing public discourse by curating content visibility.


<details>
  <summary>Details</summary>
Motivation: To understand how search engines like Google act as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation.

Method: Comparison of search engine results with nonsampled data from Reddit and Twitter/X to identify biases.

Result: Google suppresses sexually explicit material, conspiracy theories, ads, and cryptocurrencies while promoting high-engagement content.

Conclusion: Google's gatekeeping practices systematically influence the social media narratives users encounter.

Abstract: Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.

</details>


### [42] [ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection](https://arxiv.org/pdf/2506.14371)
*Lucile Favero, Daniel Frases, Juan Antonio Pérez-Ortiz, Tanja Käser, Nuria Oliver*

Main category: cs.CL

TL;DR: The paper proposes a two-step LLM-based framework for generating critical questions to foster deeper reasoning in debates, winning a shared task competition.


<details>
  <summary>Details</summary>
Motivation: Concerns about LLMs promoting superficial learning led to exploring their potential for encouraging critical thinking by challenging unsupported claims.

Method: A two-step framework using open-source LLMs: a Questioner generates candidate questions, and a Judge selects the most relevant ones.

Result: The system ranked first in the shared task, showing effectiveness in critical question generation.

Conclusion: The approach demonstrates LLMs' potential to enhance critical engagement with argumentative texts.

Abstract: The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.

</details>


### [43] [Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding](https://arxiv.org/pdf/2506.14397)
*Yeonkyoung So, Gyuseong Lee, Sungmok Jung, Joonhak Lee, JiA Kang, Sangho Kim, Jaejin Lee*

Main category: cs.CL

TL;DR: Thunder-NUBench is a new benchmark for evaluating LLMs' understanding of sentence-level negation, focusing on diverse negation types beyond surface cues.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks inadequately address negation, treating it as a minor aspect of broader tasks, necessitating a dedicated benchmark for deeper semantic analysis.

Method: The benchmark includes manually curated sentence-negation pairs and a multiple-choice dataset to evaluate models' negation understanding across diverse negation types.

Result: Thunder-NUBench provides a comprehensive tool for assessing LLMs' negation capabilities, highlighting gaps in current models.

Conclusion: The benchmark advances the evaluation of negation understanding in LLMs, offering a focused and nuanced approach.

Abstract: Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.

</details>


### [44] [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/pdf/2506.14407)
*Zeinab Sadat Taghavi, Ali Modarressi, Yunpu Ma, Hinrich Schütze*

Main category: cs.CL

TL;DR: The paper introduces ImpliRet, a benchmark focusing on document-side reasoning for retrieval systems, where relevance depends on implicit facts in documents. Evaluations show current retrievers and models struggle, with GPT-4.1 achieving only 35.06% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of retrieval systems relying on surface-level cues by shifting the reasoning challenge to document-side processing.

Method: Introduces ImpliRet, a benchmark with simple queries but relevance based on implicit document facts (temporal, arithmetic, world knowledge). Evaluates sparse/dense retrievers and long-context models like GPT-4.1.

Result: Current retrievers perform poorly (best nDCG@10: 15.07%). GPT-4.1 scores only 35.06% even with short context.

Conclusion: Document-side reasoning remains a significant challenge for retrieval systems, as demonstrated by the low performance of state-of-the-art models.

Abstract: Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.

</details>


### [45] [LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/pdf/2506.14429)
*Xiaoran Liu, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu*

Main category: cs.CL

TL;DR: The paper investigates the long-context capabilities of diffusion LLMs, comparing them to auto-regressive LLMs, and introduces a method (LongLLaDA) for context extension.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored long-context capabilities of diffusion LLMs and compare them with auto-regressive LLMs.

Method: Systematic investigation of long-context performance, analysis of perplexity stability and local perception, and proposal of LongLLaDA (a training-free method for context extension).

Result: Diffusion LLMs show stable perplexity and local perception, enabling better performance in certain long-context tasks. LongLLaDA effectively extends context windows.

Conclusion: The study provides the first context extrapolation method for diffusion LLMs, along with theoretical insights and benchmarks for future research.

Abstract: Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \textbf{\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \textbf{\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.

</details>


### [46] [How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison](https://arxiv.org/pdf/2506.14448)
*Jiayin Wang, Zhiquang Guo, Weizhi Ma, Min Zhang*

Main category: cs.CL

TL;DR: The paper advocates for evaluating Test-time Learning in LLMs using semantic games, revealing measurable but unstable learning capabilities compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on static knowledge, but intelligence requires dynamic learning. The paper aims to assess LLMs' ability to learn during test time.

Method: Proposes semantic games as testbeds for Test-time Learning, with an evaluation framework comparing model performance under limited and cumulative experience. Includes human baseline.

Result: LLMs show test-time learning but improve less stably and slowly than humans, highlighting a gap in dynamic learning capabilities.

Conclusion: LLMs have potential as general-purpose learners, but their dynamic learning lags behind humans, even if they excel on static benchmarks.

Abstract: As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.

</details>


### [47] [LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data](https://arxiv.org/pdf/2506.14474)
*Eyal German, Sagiv Antebi, Edan Habler, Asaf Shabtai, Yuval Elovici*

Main category: cs.CL

TL;DR: LexiMark is a stealthy watermarking technique for text, using synonym substitutions to detect unauthorized LLM training without altering semantics.


<details>
  <summary>Details</summary>
Motivation: Existing dataset watermarking methods lack stealth and are easily detectable, prompting the need for a more subtle and effective solution.

Method: LexiMark embeds synonym substitutions for high-entropy words, enhancing LLM memorization while preserving text semantics.

Result: Evaluation on multiple models and training settings shows LexiMark outperforms existing methods in AUROC scores for detecting unauthorized data use.

Conclusion: LexiMark provides a robust, stealthy watermarking solution for verifying unauthorized LLM training data usage.

Abstract: Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.

</details>


### [48] [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/pdf/2506.14493)
*Jiyuan Fu, Kaixun Jiang, Lingyi Hong, Jinglun Li, Haijing Guo, Dingkang Yang, Zhaoyu Chen, Wenqiang Zhang*

Main category: cs.CL

TL;DR: LingoLoop is an attack exploiting MLLMs' vulnerabilities by inducing verbose, repetitive outputs via POS-aware delay and generative path pruning, increasing tokens and energy consumption by up to 30x.


<details>
  <summary>Details</summary>
Motivation: Attackers exploit MLLMs' computational demands by inducing excessive output, but prior methods overlook token-level POS and structural patterns, limiting effectiveness.

Method: LingoLoop uses POS-Aware Delay Mechanism to delay EOS tokens and Generative Path Pruning to enforce repetitive loops.

Result: LingoLoop increases generated tokens and energy consumption by up to 30 times, pushing MLLMs to their limits.

Conclusion: LingoLoop exposes critical MLLM vulnerabilities, challenging reliable deployment.

Abstract: Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.

</details>


### [49] [M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models](https://arxiv.org/pdf/2506.14532)
*Can Zheng, Jiguang He, Chung G. Kang, Guofa Cai, Zitong Yu, Merouane Debbah*

Main category: cs.CL

TL;DR: M2BeamLLM is a neural network framework for mmWave mMIMO beam prediction, using multi-modal sensor data and LLMs like GPT-2 for improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To enhance beam prediction in mmWave mMIMO systems by integrating diverse sensor data and leveraging LLMs for better performance.

Method: Combines sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT) with LLMs.

Result: Outperforms traditional DL models in accuracy and robustness, especially in few-shot scenarios, and improves with more sensing modalities.

Conclusion: M2BeamLLM offers an efficient, intelligent solution for V2I mmWave communication systems.

Abstract: This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.

</details>


### [50] [AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/pdf/2506.14562)
*Di He, Ajay Jaiswal, Songjun Tu, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin*

Main category: cs.CL

TL;DR: AlphaDecay adaptively assigns weight decay strengths to LLM modules based on spectral properties, improving performance over uniform decay.


<details>
  <summary>Details</summary>
Motivation: Uniform weight decay overlooks structural diversity and spectral variations in LLMs, necessitating an adaptive approach.

Method: AlphaDecay uses HT-SR theory to analyze ESD of weight matrices, assigning decay strengths inversely proportional to heavy-tailedness.

Result: AlphaDecay outperforms uniform decay and other baselines in perplexity and generalization across model sizes (60M to 1B).

Conclusion: Adaptive weight decay based on spectral properties enhances LLM training efficiency and performance.

Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.

</details>


### [51] [GenerationPrograms: Fine-grained Attribution with Executable Programs](https://arxiv.org/pdf/2506.14580)
*David Wan, Eran Hirsch, Elias Stengel-Eskin, Ido Dagan, Mohit Bansal*

Main category: cs.CL

TL;DR: GenerationPrograms is a modular framework improving attribution quality in LLMs by decomposing generation into program planning and execution.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs lack fine-grained attributions, undermining trust and interpretability.

Method: Decomposes generation into program planning (modular text operations) and execution, enhancing attribution.

Result: Improves attribution quality in QA and summarization tasks; outperforms post-hoc methods.

Conclusion: GenerationPrograms offers interpretable, modular refinement for better attribution and trust.

Abstract: Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.

</details>


### [52] [Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees](https://arxiv.org/pdf/2506.14606)
*Ahmed Heakl, Sarim Hashmi, Chaimaa Abi, Celine Lee, Abdulrahman Mahmoud*

Main category: cs.CL

TL;DR: GG (Guaranteed Guess) is a transpilation pipeline combining LLMs and software testing for accurate CISC-to-RISC translation, outperforming Rosetta 2 in speed, energy, and memory.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of translating low-level programs between CISC and RISC architectures to improve code portability and longevity.

Method: Uses LLMs for candidate translations and embeds them in a software-testing framework to ensure correctness.

Result: Achieves 99% correctness on HumanEval and 49% on BringupBench, with better performance than Rosetta 2.

Conclusion: GG is effective for real-world CISC-to-RISC translation and will be open-sourced to advance research.

Abstract: The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.

</details>


### [53] [When Does Meaning Backfire? Investigating the Role of AMRs in NLI](https://arxiv.org/pdf/2506.14613)
*Junghyun Min, Xiulin Yang, Shira Wein*

Main category: cs.CL

TL;DR: Adding AMR to NLI tasks hinders generalization in fine-tuning but slightly improves GPT-4o performance in prompting, though the gain stems from surface-level differences, not semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: To explore if semantic information (AMR) enhances pretrained language models' generalization in NLI.

Method: Integrate AMR into NLI via fine-tuning and prompting, then analyze performance and conduct an ablation study.

Result: AMR in fine-tuning harms generalization; prompting with AMR slightly boosts GPT-4o but due to surface-level differences, not semantic reasoning.

Conclusion: AMR's benefits in NLI are misleading, as improvements arise from superficial cues rather than deeper semantic understanding.

Abstract: Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.

</details>


### [54] [Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models](https://arxiv.org/pdf/2506.14625)
*Chenchen Yuan, Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci*

Main category: cs.CL

TL;DR: A framework synthesizes multiple LLMs' moral judgments into a collective consensus, fine-tuning misaligned models to improve moral reasoning consistency.


<details>
  <summary>Details</summary>
Motivation: Address discrepancies in LLMs' moral reasoning when faced with complex dilemmas by creating a collective moral judgment.

Method: Aggregate continuous moral acceptability scores into a collective probability, weight by model reliability, and fine-tune misaligned models using embedding optimization.

Result: Experiments show the approach builds robust consensus and improves individual model fidelity.

Conclusion: Data-driven moral alignment across multiple models enhances AI safety and consistency.

Abstract: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.

</details>


### [55] [AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation](https://arxiv.org/pdf/2506.14634)
*Leah von der Heyde, Anna-Carolina Haensch, Bernd Weiß, Jessika Daikeler*

Main category: cs.CL

TL;DR: LLMs are explored for coding open-ended survey responses in German, comparing models and prompting methods. Fine-tuned LLMs perform best, but performance varies by model and category.


<details>
  <summary>Details</summary>
Motivation: Assess LLMs' suitability for coding non-English survey responses, given limited research on generalizability and comparison to manual methods.

Method: Compare multiple LLMs and prompting approaches using German survey data, evaluated against human expert codings.

Result: Performance varies by LLM; only fine-tuned models achieve satisfactory accuracy. Prompting effectiveness depends on the LLM. Uneven performance across categories affects distributions.

Conclusion: LLMs show promise but require careful model and method selection. Trade-offs exist for automated coding in survey research.

Abstract: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.

</details>


### [56] [Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot](https://arxiv.org/pdf/2506.14641)
*Xiang Cheng, Chengyan Pan, Minjun Zhao, Deyang Li, Fangchao Liu, Xinyu Zhang, Xiao Zhang, Yong Liu*

Main category: cs.CL

TL;DR: Recent strong LLMs like Qwen2.5 don't benefit from traditional or enhanced CoT exemplars in ICL for math tasks; they focus on instructions instead, questioning the current ICL+CoT framework.


<details>
  <summary>Details</summary>
Motivation: To investigate whether CoT exemplars still enhance reasoning in advanced LLMs like Qwen2.5, given their improved capabilities.

Method: Systematic experiments comparing traditional and enhanced CoT exemplars (using answers from advanced models) against Zero-Shot CoT.

Result: No improvement in reasoning performance with CoT exemplars; models ignore exemplars and focus on instructions.

Conclusion: Current ICL+CoT framework has limitations in math tasks, necessitating a re-examination of ICL paradigms and exemplar definitions.

Abstract: In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.

</details>


### [57] [Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments](https://arxiv.org/pdf/2506.14645)
*. Pazzaglia, V. Vendetti, L. D. Comencini, F. Deriu, V. Modugno*

Main category: cs.CL

TL;DR: Fine-tuned LLMs can amplify ideological polarization by generating persuasive, biased content indistinguishable from human-written discourse, raising ethical concerns for AI governance.


<details>
  <summary>Details</summary>
Motivation: To investigate how fine-tuned LLMs can replicate and amplify polarizing discourse in online environments, given concerns about AI's role in ideological polarization.

Method: Fine-tune an open-source LLM on a curated dataset of politically charged Reddit discussions, then evaluate outputs via linguistic analysis, sentiment scoring, and human annotation.

Result: Fine-tuned LLMs produce highly plausible, provocative comments aligned with partisan discourse, often indistinguishable from human writing.

Conclusion: The findings highlight ethical risks of AI in political discourse and call for governance, regulation, and detection tools to mitigate adversarial fine-tuning.

Abstract: The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.

</details>


### [58] [GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors](https://arxiv.org/pdf/2506.14646)
*Hengyuan Zhang, Xinrong Chen, Yingmin Qiu, Xiao Liang, Ziyue Li, Guanyu Wang, Weiping Li, Tong Mo, Wenyue Li, Hayden Kwok-Hay So, Ngai Wong*

Main category: cs.CL

TL;DR: GuiLoMo introduces a fine-grained strategy for allocating expert numbers and ranks in LoRA-MoE, using GuidedSelection Vectors (GSVs) to optimize performance.


<details>
  <summary>Details</summary>
Motivation: Current LoRA-MoE methods are limited by uniform rank assignment and task-agnostic expert allocation, hindering their potential.

Method: GuiLoMo uses GSVs learned via bilevel optimization to adaptively allocate expert numbers and ranks per layer and task.

Result: Experiments show GuiLoMo outperforms baselines, with insights into adaptive expert configuration.

Conclusion: GuiLoMo enhances LoRA-MoE by addressing its limitations, offering a flexible and efficient fine-tuning approach.

Abstract: Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.

</details>


### [59] [Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality](https://arxiv.org/pdf/2506.14681)
*Yuto Harada, Yusuke Yamauchi, Yusuke Oda, Yohei Oseki, Yusuke Miyao, Yu Takagi*

Main category: cs.CL

TL;DR: The paper explores supervised fine-tuning (SFT) for aligning LLMs with human values, analyzing 1,000+ models to identify key dataset properties and layer-wise changes. Perplexity is a strong predictor of SFT effectiveness.


<details>
  <summary>Details</summary>
Motivation: To better understand SFT's role in aligning LLMs with human instructions and values, as many aspects remain unclear.

Method: Trained diverse base models on varied datasets (code, math, general tasks) under controlled conditions, analyzing dataset properties and layer-wise modifications.

Result: Found persistent training-task synergies and model-specific variations. Perplexity predicts SFT effectiveness better than data similarity, with mid-layer weight changes correlating with performance.

Conclusion: Model-specific strategies are crucial, and perplexity is a reliable metric for SFT success. The release of 1,000+ models aims to advance research.

Abstract: Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.

</details>


### [60] [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers](https://arxiv.org/pdf/2506.14702)
*Daniel D'souza, Julia Kreutzer, Adrien Morisot, Ahmet Üstün, Sara Hooker*

Main category: cs.CL

TL;DR: The paper proposes optimizing training protocols to improve controllability and performance on underrepresented use cases in machine learning models, achieving significant gains in long-tail tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of poor performance on rare or underrepresented features in large models, which often excel only on high-frequency use cases.

Method: Develops a taxonomy of data characteristics and task provenance to control generation attributes, fine-tuning a base model to infer these markers automatically.

Result: Achieves a 5.7% average win rate improvement in open-ended generation, with over 9.1% gains in underrepresented domains and up to 14.1% on specific tasks like CodeRepair.

Conclusion: The approach enhances long-tail performance and controllability, offering flexible and principled improvements for underrepresented use cases.

Abstract: One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.

</details>


### [61] [Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data](https://arxiv.org/pdf/2506.14704)
*Anton Changalidis, Aki Härmä*

Main category: cs.CL

TL;DR: The paper explores how model architecture and data configurations affect generative transformers' memorization capacity, identifying embedding size as key, with additional layers offering limited benefits and Softmax activation being most stable.


<details>
  <summary>Details</summary>
Motivation: To understand how model architecture and data setups influence the memorization capacity of generative transformers, particularly with structured data like SNOMED.

Method: Training models on synthetic text datasets from SNOMED (triplets and sequences) to analyze memorization capacity.

Result: Embedding size is the main factor for learning speed and capacity; extra layers may hinder performance on simpler data. Softmax is the most stable activation function, and data complexity boosts memorization.

Conclusion: The findings enhance understanding of transformer memory mechanisms and offer a framework for optimizing models with structured real-world data.

Abstract: This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.

</details>


### [62] [Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs](https://arxiv.org/pdf/2506.14731)
*Ring Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen*

Main category: cs.CL

TL;DR: Ring-lite is an MoE-based LLM optimized via RL, achieving SOTA performance with fewer parameters. It introduces C3PO for stable RL training and a two-stage paradigm for multi-domain data.


<details>
  <summary>Details</summary>
Motivation: To create an efficient and robust reasoning model using MoE and RL, addressing challenges in training stability and domain conflicts.

Method: Joint training pipeline combining distillation and RL, with C3PO for stability and a two-stage approach for multi-domain data.

Result: Matches SOTA performance with one-third of the parameters, improved training stability, and better performance-efficiency trade-offs.

Conclusion: Ring-lite demonstrates efficient reasoning, introduces novel training methods, and resolves domain conflicts, with plans to release resources.

Abstract: We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.

</details>


### [63] [Reasoning with Exploration: An Entropy Perspective](https://arxiv.org/pdf/2506.14758)
*Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, Furu Wei*

Main category: cs.CL

TL;DR: The paper explores the role of entropy in enhancing exploratory reasoning in language models (LMs) by linking high-entropy regions to key reasoning actions. A simple entropy-based modification to RL improves LM reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the imbalance between exploration and exploitation in RL for LMs, which often leads to performance plateaus, by leveraging entropy as a signal for exploratory reasoning.

Method: Augmenting the advantage function in RL with an entropy-based term to promote deeper reasoning chains, rather than just uncertainty.

Result: Strong positive correlations between high-entropy regions and exploratory reasoning actions, with significant gains on the Pass@K metric.

Conclusion: The proposed entropy-based method effectively enhances LM reasoning by encouraging deeper exploration, pushing the boundaries of LM capabilities.

Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.

</details>


### [64] [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/pdf/2506.14761)
*Mathurin Videau, Badr Youbi Idrissi, Alessandro Leite, Marc Schoenauer, Olivier Teytaud, David Lopez-Paz*

Main category: cs.CL

TL;DR: The paper introduces an autoregressive U-Net to dynamically embed tokens during training, replacing static tokenization like BPE, enabling multi-scale text processing and improved semantic prediction.


<details>
  <summary>Details</summary>
Motivation: Static tokenization methods like BPE limit model flexibility and granularity, freezing input processing and prediction scope. The goal is to relax this rigidity by embedding tokens dynamically.

Method: An autoregressive U-Net is proposed, processing raw bytes into multi-scale embeddings (words, word pairs, up to 4 words) to provide a hierarchical view. Deeper stages predict further into the future, focusing on broader semantics.

Result: Shallow hierarchies match strong BPE baselines, while deeper hierarchies show promising trends. The dynamic tokenization allows handling character-level tasks and cross-lingual knowledge transfer.

Conclusion: Dynamic tokenization via the U-Net offers flexibility, multi-scale processing, and potential for broader applications, outperforming static methods like BPE.

Abstract: Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.

</details>


### [65] [Compression of enumerations and gain](https://arxiv.org/pdf/2304.03030)
*George Barmpalias, Xiaoyan Zhang, Bohua Zhan*

Main category: cs.CL

TL;DR: The paper explores compressibility of enumerations in Kolmogorov complexity, distinguishing strong and weak compression forms and their gain. It proves strong and weak gainless compression for computably enumerable sets and links the density problem of such sets to their compressibility via enumeration games.


<details>
  <summary>Details</summary>
Motivation: To understand the compressibility of enumerations in Kolmogorov complexity and its implications for computably enumerable sets.

Method: Analyzes strong and weak compression forms, their gain, and uses enumeration games to study compressibility.

Result: Existence of strong and weak gainless compression for any computably enumerable set is proven.

Conclusion: The density problem of c.e. sets is reduced to their compressibility, studied through enumeration games.

Abstract: We study the compressibility of enumerations in the context of Kolmogorov complexity, focusing on strong and weak forms of compression and their gain: the amount of auxiliary information embedded in the compressed enumeration. The existence of strong compression and weak gainless compression is shown for any computably enumerable (c.e.) set. The density problem of c.e. sets with respect to their prefix complexity is reduced to the question of whether every c.e. set is well-compressible, which we study via enumeration games.

</details>


### [66] [FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback](https://arxiv.org/pdf/2307.10867)
*Ashish Singh, Ashutosh Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nesreen K. Ahmed, Puneet Mathur, Erik Learned-Miller, Franck Dernoncourt, Ryan A. Rossi*

Main category: cs.CL

TL;DR: FigCaps-HF is a new framework for generating high-quality figure captions by incorporating domain expert feedback and optimizing for reader preferences using reinforcement learning with human feedback (RLHF).


<details>
  <summary>Details</summary>
Motivation: Existing captioning methods for scientific figures often produce misaligned or low-quality captions, lacking helpfulness, explainability, and visual-descriptiveness.

Method: The framework includes an automatic quality evaluation method for figure-caption pairs and a novel RLHF approach to optimize caption generation.

Result: Using BLIP as the base model, the framework achieves significant gains in ROUGE (35.7%), BLEU (16.9%), and Meteor (9%).

Conclusion: FigCaps-HF improves caption quality and releases a benchmark dataset to advance RLHF techniques for figure captioning.

Abstract: Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.

</details>


### [67] [Exploring news intent and its application: A theory-driven approach](https://arxiv.org/pdf/2312.16490)
*Zhengjia Wang, Danding Wang, Qiang Sheng, Juan Cao, Siyuan Ma, Haonan Cheng*

Main category: cs.CL

TL;DR: The paper introduces a framework (NINT) for understanding news intent, creates a dataset, and shows its application improves fake news detection.


<details>
  <summary>Details</summary>
Motivation: To address the lack of structured investigation into perceived news intent and its applications.

Method: Reviews interdisciplinary studies, proposes the NINT framework, and contributes a new intent perception dataset.

Result: Significant improvement (+2.2% macF1) in fake news detection using intent assistance.

Conclusion: The findings offer insights into intent cognition and computational social science.

Abstract: Understanding the intent behind information is crucial. However, news as a medium of public discourse still lacks a structured investigation of perceived news intent and its application. To advance this field, this paper reviews interdisciplinary studies on intentional action and introduces a conceptual deconstruction-based news intent understanding framework (NINT). This framework identifies the components of intent, facilitating a structured representation of news intent and its applications. Building upon NINT, we contribute a new intent perception dataset. Moreover, we investigate the potential of intent assistance on news-related tasks, such as significant improvement (+2.2% macF1) in the task of fake news detection. We hope that our findings will provide valuable insights into action-based intent cognition and computational social science.

</details>


### [68] [Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification](https://arxiv.org/pdf/2402.10735)
*John Dougrez-Lewis, Mahmud Elahi Akhter, Federico Ruggeri, Sebastian Löbbers, Yulan He, Maria Liakata*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' reasoning abilities in claim verification, introducing RECV, a benchmark for deductive and abductive reasoning. Results show LLMs struggle with abductive reasoning, and rationale generation doesn't always help.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' reasoning beyond math and coding, focusing on claim verification, and identify gaps in their capabilities.

Method: Proposed a framework to break claims into atomic reasoning types, created RECV benchmark with three datasets, and evaluated three LLMs under various prompts.

Result: LLMs perform well on deductive reasoning but fail in abductive reasoning. Rationale generation doesn't consistently improve performance, though generated rationales resemble human ones in deductive cases.

Conclusion: LLMs have limitations in abductive reasoning, and rationale generation isn't universally beneficial, highlighting areas for improvement in reasoning tasks.

Abstract: Although LLMs have shown great performance on Mathematics and Coding related reasoning tasks, the reasoning capabilities of LLMs regarding other forms of reasoning are still an open problem. Here, we examine the issue of reasoning from the perspective of claim verification. We propose a framework designed to break down any claim paired with evidence into atomic reasoning types that are necessary for verification. We use this framework to create RECV, the first claim verification benchmark, incorporating real-world claims, to assess the deductive and abductive reasoning capabilities of LLMs. The benchmark comprises of three datasets, covering reasoning problems of increasing complexity. We evaluate three state-of-the-art proprietary LLMs under multiple prompt settings. Our results show that while LLMs can address deductive reasoning problems, they consistently fail in cases of abductive reasoning. Moreover, we observe that enhancing LLMs with rationale generation is not always beneficial. Nonetheless, we find that generated rationales are semantically similar to those provided by humans, especially in deductive reasoning cases.

</details>


### [69] [Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora](https://arxiv.org/pdf/2406.13677)
*Erik Derner, Sara Sansalvador de la Fuente, Yoan Gutiérrez, Paloma Moreda, Nuria Oliver*

Main category: cs.CL

TL;DR: The paper addresses gender representation bias in LLM training data, proposing a method to detect and quantify it in gendered languages, revealing male-dominant imbalances and suggesting mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on gender stereotyping in English, neglecting gender representation bias in training corpora, which can propagate bias throughout the model lifecycle.

Method: A novel LLM-based method to detect and quantify gender representation bias in gendered languages, leveraging LLMs' contextual understanding to classify person-referencing words.

Result: Applied to Spanish-English and Valencian corpora, the method revealed significant male-dominant biases, which affect model outputs but can be mitigated with small-scale training on oppositely biased datasets.

Conclusion: The study underscores the importance of corpus-level gender bias analysis in multilingual NLP and provides publicly available code and data.

Abstract: Large language models (LLMs) often inherit and amplify social biases embedded in their training data. A prominent social bias is gender bias. In this regard, prior work has mainly focused on gender stereotyping bias - the association of specific roles or traits with a particular gender - in English and on evaluating gender bias in model embeddings or generated outputs. In contrast, gender representation bias - the unequal frequency of references to individuals of different genders - in the training corpora has received less attention. Yet such imbalances in the training data constitute an upstream source of bias that can propagate and intensify throughout the entire model lifecycle. To fill this gap, we propose a novel LLM-based method to detect and quantify gender representation bias in LLM training data in gendered languages, where grammatical gender challenges the applicability of methods developed for English. By leveraging the LLMs' contextual understanding, our approach automatically identifies and classifies person-referencing words in gendered language corpora. Applied to four Spanish-English benchmarks and five Valencian corpora, our method reveals substantial male-dominant imbalances. We show that such biases in training data affect model outputs, but can surprisingly be mitigated leveraging small-scale training on datasets that are biased towards the opposite gender. Our findings highlight the need for corpus-level gender bias analysis in multilingual NLP. We make our code and data publicly available.

</details>


### [70] [ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models](https://arxiv.org/pdf/2407.07313)
*Benjamin G. Ascoli, Yasoda Sai Ram Kandikonda, Jinho D. Choi*

Main category: cs.CL

TL;DR: The paper introduces Enhanced Tree Matching (ETM), a new metric for evaluating Text-to-SQL tasks, addressing limitations in existing metrics (EXE and ESM) like false positives/negatives.


<details>
  <summary>Details</summary>
Motivation: Current metrics (EXE and ESM) misrepresent performance in Text-to-SQL tasks, especially for LLM-based approaches, due to rigid or lenient evaluations.

Method: Proposes ETM, which compares queries using syntactic and semantic elements to reduce evaluation errors.

Result: ETM reduces false positive and negative rates significantly (0.3% and 2.7%) compared to EXE and ESM (23.0% and 28.9%).

Conclusion: ETM provides a more robust and reliable evaluation method for Text-to-SQL, released as open-source for community use.

Abstract: The task of Text-to-SQL enables anyone to retrieve information from SQL databases using natural language. While this task has made substantial progress, the two primary evaluation metrics - Execution Accuracy (EXE) and Exact Set Matching Accuracy (ESM) - suffer from inherent limitations that can misrepresent performance. Specifically, ESM's rigid matching overlooks semantically correct but stylistically different queries, whereas EXE can overestimate correctness by ignoring structural errors that yield correct outputs. These shortcomings become especially problematic when assessing outputs from large language model (LLM)-based approaches without fine-tuning, which vary more in style and structure compared to their fine-tuned counterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM), which mitigates these issues by comparing queries using both syntactic and semantic elements. Through evaluating nine LLM-based models, we show that EXE and ESM can produce false positive and negative rates as high as 23.0% and 28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release our ETM script as open source, offering the community a more robust and reliable approach to evaluating Text-to-SQL.

</details>


### [71] [Geometric Signatures of Compositionality Across a Language Model's Lifetime](https://arxiv.org/pdf/2410.01444)
*Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, Yoshua Bengio, Emily Cheng*

Main category: cs.CL

TL;DR: The paper explores whether language models (LMs) reflect the simplicity of language enabled by compositionality, using geometric analysis of intrinsic dimension (ID) in representations.


<details>
  <summary>Details</summary>
Motivation: To understand if LMs capture the intrinsic simplicity of language through compositionality.

Method: Analyzes the relationship between dataset compositionality and the intrinsic dimension (ID) of LM representations, focusing on geometric complexity and learned linguistic features.

Result: Finds that dataset compositionality is reflected in ID, with nonlinear and linear dimensions encoding semantic and superficial aspects, respectively.

Conclusion: LMs' geometric complexity aligns with linguistic compositionality, revealing distinct roles of nonlinear and linear dimensions in encoding language features.

Abstract: By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.

</details>


### [72] [Uncovering Overfitting in Large Language Model Editing](https://arxiv.org/pdf/2410.07819)
*Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen*

Main category: cs.CL

TL;DR: The paper addresses 'Editing Overfit' in knowledge editing for LLMs, introduces the EVOKE benchmark, and proposes the LTI method to mitigate the issue.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge editing methods struggle with complex tasks like multi-hop reasoning due to 'Editing Overfit,' where models over-prioritize edit targets.

Method: The authors introduce the EVOKE benchmark and propose LTI, a plug-and-play strategy with a Multi-stage Inference Constraint module.

Result: Experiments show LTI effectively mitigates Editing Overfit across various tasks.

Conclusion: LTI provides a promising solution to improve generalization in knowledge editing for LLMs.

Abstract: Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon of Editing Overfit, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are ineffective in knowledge editing. To overcome this, inspired by LLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn the Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit.

</details>


### [73] [Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework](https://arxiv.org/pdf/2410.18653)
*Esteban Garces Arias, Hannah Blocher, Julian Rodemann, Meimingwei Li, Christian Heumann, Matthias Aßenmacher*

Main category: cs.CL

TL;DR: The paper proposes novel methods for multicriteria evaluation of open-ended text generation, addressing trade-offs in metrics like coherence, diversity, and perplexity.


<details>
  <summary>Details</summary>
Motivation: Evaluating the quality of open-ended text generation models and decoding strategies is challenging due to conflicting metrics.

Method: The paper introduces benchmarking based on partial orderings and a new summary metric to balance automatic indicators.

Result: Experiments show the proposed methods robustly compare decoding strategies and aid model selection.

Conclusion: The paper suggests future improvements for text generation evaluation and shares resources publicly.

Abstract: Open-ended text generation has become a prominent task in natural language processing due to the rise of powerful (large) language models. However, evaluating the quality of these models and the employed decoding strategies remains challenging due to trade-offs among widely used metrics such as coherence, diversity, and perplexity. This paper addresses the specific problem of multicriteria evaluation for open-ended text generation, proposing novel methods for both relative and absolute rankings of decoding methods. Specifically, we employ benchmarking approaches based on partial orderings and present a new summary metric to balance existing automatic indicators, providing a more holistic evaluation of text generation quality. Our experiments demonstrate that the proposed approaches offer a robust way to compare decoding strategies and serve as valuable tools to guide model selection for open-ended text generation tasks. We suggest future directions for improving evaluation methodologies in text generation and make our code, datasets, and models publicly available.

</details>


### [74] [Ensemble Watermarks for Large Language Models](https://arxiv.org/pdf/2411.19563)
*Georg Niess, Roman Kern*

Main category: cs.CL

TL;DR: Proposes a multi-feature watermarking method for LLMs, combining acrostica, sensorimotor norms, and red-green watermark, achieving high detection rates (98%) and robustness against paraphrasing (95%).


<details>
  <summary>Details</summary>
Motivation: Addressing the difficulty in distinguishing AI-generated text and the limitations of existing watermarks (e.g., lack of flexibility, vulnerability to attacks like paraphrasing).

Method: Combines multiple watermark features (acrostica, sensorimotor norms, red-green) into an ensemble watermark, evaluated across LLMs and settings.

Result: Achieves 98% detection rate; remains robust (95%) after paraphrasing, outperforming single-feature baselines (49%).

Conclusion: The ensemble method offers flexibility, high detection rates, and adaptability, aiding accountability and mitigating societal harm from AI-generated text.

Abstract: As large language models (LLMs) reach human-like fluency, reliably distinguishing AI-generated text from human authorship becomes increasingly difficult. While watermarks already exist for LLMs, they often lack flexibility and struggle with attacks such as paraphrasing. To address these issues, we propose a multi-feature method for generating watermarks that combines multiple distinct watermark features into an ensemble watermark. Concretely, we combine acrostica and sensorimotor norms with the established red-green watermark to achieve a 98% detection rate. After a paraphrasing attack, the performance remains high with 95% detection rate. In comparison, the red-green feature alone as a baseline achieves a detection rate of 49% after paraphrasing. The evaluation of all feature combinations reveals that the ensemble of all three consistently has the highest detection rate across several LLMs and watermark strength settings. Due to the flexibility of combining features in the ensemble, various requirements and trade-offs can be addressed. Additionally, the same detection function can be used without adaptations for all ensemble configurations. This method is particularly of interest to facilitate accountability and prevent societal harm.

</details>


### [75] [BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English](https://arxiv.org/pdf/2412.04726)
*Dipankar Srirag, Aditya Joshi, Jordan Painter, Diptesh Kanojia*

Main category: cs.CL

TL;DR: The paper introduces BESSTIE, a benchmark for sentiment and sarcasm classification across three English varieties (en-AU, en-IN, en-UK), highlighting biases in LLMs and the need for language-specific datasets.


<details>
  <summary>Details</summary>
Motivation: To address the lack of labelled datasets for sentiment analysis of non-standard English varieties and biases in LLMs against these varieties.

Method: Datasets were collected via location-based (Google Places) and topic-based (Reddit) methods, validated through manual and automatic language variety annotation, and annotated by native speakers. Nine LLMs were fine-tuned and evaluated.

Result: Models performed better on inner-circle varieties (en-AU, en-UK) than en-IN, especially for sarcasm, revealing challenges in cross-variety generalization.

Conclusion: BESSTIE is a valuable benchmark for equitable LLM research, emphasizing the need for language variety-specific datasets.

Abstract: Despite large language models (LLMs) being known to exhibit bias against non-standard language varieties, there are no known labelled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect datasets for these language varieties using two methods: location-based for Google Places reviews, and topic-based filtering for Reddit comments. To assess whether the dataset accurately represents these varieties, we conduct two validation steps: (a) manual annotation of language varieties and (b) automatic language variety prediction. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. We perform an additional annotation exercise to validate the reliance of the annotated labels. Subsequently, we fine-tune nine LLMs (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results show that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), in comparison with en-IN, particularly for sarcasm classification. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE dataset is publicly available at: https://huggingface.co/ datasets/unswnlporg/BESSTIE.

</details>


### [76] [Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression](https://arxiv.org/pdf/2412.05693)
*Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh*

Main category: cs.CL

TL;DR: Compressing the KV cache during input processing enables larger batch sizes and higher throughput without sacrificing model accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in GPU memory-limited settings, especially when input context exceeds generation length.

Method: Develop eviction policies for KV cache compression during both input processing and token generation phases.

Result: Larger batch sizes and significantly higher throughput while maintaining model accuracy.

Conclusion: KV cache compression during input processing enhances efficiency in memory-constrained scenarios.

Abstract: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.

</details>


### [77] [ClusterChat: Multi-Feature Search for Corpus Exploration](https://arxiv.org/pdf/2412.14533)
*Ashish Chouhan, Saifeldin Mandour, Michael Gertz*

Main category: cs.CL

TL;DR: ClusterChat is an open-source system for exploring large text corpora by integrating cluster-based organization, search, and QA, validated on a PubMed dataset.


<details>
  <summary>Details</summary>
Motivation: Traditional keyword-based search methods retrieve documents in isolation, limiting the ability to inspect corpus-wide trends and relationships.

Method: ClusterChat uses textual embeddings for cluster-based organization, combines lexical and semantic search, timeline-driven exploration, and QA.

Result: Validated on a PubMed dataset, ClusterChat enhances corpus exploration with context-aware insights while maintaining scalability.

Conclusion: ClusterChat improves corpus exploration by integrating multiple search features and maintaining performance on large datasets.

Abstract: Exploring large-scale text corpora presents a significant challenge in biomedical, finance, and legal domains, where vast amounts of documents are continuously published. Traditional search methods, such as keyword-based search, often retrieve documents in isolation, limiting the user's ability to easily inspect corpus-wide trends and relationships. We present ClusterChat (The demo video and source code are available at: https://github.com/achouhan93/ClusterChat), an open-source system for corpus exploration that integrates cluster-based organization of documents using textual embeddings with lexical and semantic search, timeline-driven exploration, and corpus and document-level question answering (QA) as multi-feature search capabilities. We validate the system with two case studies on a four million abstract PubMed dataset, demonstrating that ClusterChat enhances corpus exploration by delivering context-aware insights while maintaining scalability and responsiveness on large-scale document collections.

</details>


### [78] [Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models](https://arxiv.org/pdf/2501.05478)
*Malak Mansour, Ahmed Aly, Bahey Tharwat, Sarim Hashmi, Dong An, Ian Reid*

Main category: cs.CL

TL;DR: The study evaluates multilingual SLMs and Arabic-centric LLMs in Vision-and-Language Navigation (VLN) using the NavGPT framework, revealing challenges in Arabic-language reasoning and planning.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored integration of Arabic in VLN for robotics and assess multilingual models' performance in navigation tasks.

Method: Utilized the NavGPT framework for zero-shot sequential action prediction on the R2R dataset, testing models like GPT-4o mini, Llama 3 8B, Phi-3 medium 14B, and Jais.

Result: Models showed high-level planning in English and Arabic but struggled with Arabic due to limitations in reasoning, performance, and parsing.

Conclusion: Enhancing reasoning and planning in language models is crucial for effective navigation, especially for Arabic, unlocking real-world applications.

Abstract: Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications.

</details>


### [79] [The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs](https://arxiv.org/pdf/2501.10970)
*Nitay Calderon, Roi Reichart, Rotem Dror*

Main category: cs.CL

TL;DR: The paper introduces the Alternative Annotator Test (alt-test) to justify using LLM annotations and a measure for comparing LLM annotators. Experiments show closed-source LLMs like GPT-4o can sometimes replace humans.


<details>
  <summary>Details</summary>
Motivation: There's no standard procedure to determine if LLMs can replace human annotators, despite their widespread use in research.

Method: Proposed the alt-test and a measure for comparing LLM annotators. Tested on ten datasets with six LLMs and four prompting techniques.

Result: Closed-source LLMs (e.g., GPT-4o) can sometimes replace humans, outperforming open-source LLMs. Prompting techniques vary in quality.

Conclusion: The study encourages more rigorous practices for using LLMs as annotators and judges.

Abstract: The "LLM-as-an-annotator" and "LLM-as-a-judge" paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.

</details>


### [80] [Position: Editing Large Language Models Poses Serious Safety Risks](https://arxiv.org/pdf/2502.02958)
*Paul Youssef, Zhixue Zhao, Daniel Braun, Jörg Schlötterer, Christin Seifert*

Main category: cs.CL

TL;DR: The paper highlights safety risks of knowledge editing in LLMs, warning of malicious uses and calling for research on countermeasures and ecosystem security.


<details>
  <summary>Details</summary>
Motivation: To address overlooked safety risks posed by knowledge editing methods in LLMs, which could be exploited by malicious actors.

Method: Analyzes the accessibility, performance, and stealth of KEs, discusses malicious use cases, identifies ecosystem vulnerabilities, and evaluates stakeholder awareness.

Result: Reveals significant risks due to the ease of misuse of KEs and lack of safeguards in the AI ecosystem.

Conclusion: Urges the community to develop tamper-resistant models, countermeasures, and secure the AI ecosystem to mitigate risks.

Abstract: Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem.

</details>


### [81] [Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models](https://arxiv.org/pdf/2502.11425)
*Jongho Kim, Seung-won Hwang*

Main category: cs.CL

TL;DR: A novel counterfactual prompting approach improves temporal reasoning in LLMs by addressing inconsistencies in event ordering and commonsense understanding.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with temporal reasoning, often confusing relations like 'before' and 'after,' leading to inconsistent predictions.

Method: Proposes counterfactual prompting, generating counterfactual questions and enforcing collective constraints to enhance consistency.

Result: Significant improvements in event ordering and temporal commonsense understanding across multiple datasets.

Conclusion: The method effectively addresses temporal inconsistencies in LLMs, advancing their temporal reasoning capabilities.

Abstract: Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.

</details>


### [82] [Towards Geo-Culturally Grounded LLM Generations](https://arxiv.org/pdf/2502.13497)
*Piyawat Lertvittayakumjorn, David Kinney, Vinodkumar Prabhakaran, Donald Martin, Sunipa Dev*

Main category: cs.CL

TL;DR: Retrieval-augmented techniques, especially search grounding, improve LLMs' propositional cultural knowledge but not cultural fluency, with risks of stereotypes.


<details>
  <summary>Details</summary>
Motivation: To address gaps in LLMs' cultural awareness by evaluating retrieval-augmented methods.

Method: Compared standard LLMs, KB-grounding, and search-grounding on cultural benchmarks.

Result: Search grounding boosts propositional knowledge but not cultural fluency, with stereotype risks.

Conclusion: Propositional knowledge and cultural fluency are distinct in evaluating LLMs' cultural awareness.

Abstract: Generative large language models (LLMs) have demonstrated gaps in diverse cultural awareness across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on LLMs' ability to display familiarity with various national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on multiple cultural awareness benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., cultural norms, artifacts, and institutions), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models and fails to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional cultural knowledge and open-ended cultural fluency when it comes to evaluating LLMs' cultural awareness.

</details>


### [83] [PredictaBoard: Benchmarking LLM Score Predictability](https://arxiv.org/pdf/2502.14445)
*Lorenzo Pacchiardi, Konstantinos Voudouris, Ben Slater, Fernando Martínez-Plumed, José Hernández-Orallo, Lexin Zhou, Wout Schellaert*

Main category: cs.CL

TL;DR: PredictaBoard is a framework to evaluate how well assessors can predict LLM errors, aiming to improve LLM predictability and safety.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail unpredictably, posing risks in deployment. Ensuring a 'safe zone' requires better predictability of errors.

Method: PredictaBoard benchmarks assessors' ability to predict LLM errors on specific prompts, evaluating rejection rates at different error tolerances.

Result: The framework highlights the need to assess predictability alongside performance, using baseline assessors and state-of-the-art LLMs.

Conclusion: PredictaBoard advances safer AI by focusing on anticipating and mitigating errors, not just minimizing them.

Abstract: Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable "safe zone" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard

</details>


### [84] [Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models](https://arxiv.org/pdf/2502.15910)
*Zheyuan Liu, Guangyao Dou, Xiangchi Yuan, Chunhui Zhang, Zhaoxuan Tan, Meng Jiang*

Main category: cs.CL

TL;DR: MANU is a novel unlearning framework for MLLMs that selectively prunes neurons to remove sensitive data while preserving model utility.


<details>
  <summary>Details</summary>
Motivation: Address privacy and ethical concerns in MLLMs due to memorization of sensitive data across modalities.

Method: Two-stage approach: (1) identify important neurons for targeted forget data, (2) selectively prune them.

Result: MANU achieves balanced unlearning across modalities without significantly impacting model performance.

Conclusion: MANU effectively removes sensitive data in MLLMs while maintaining overall utility.

Abstract: Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.

</details>


### [85] [LongSpec: Long-Context Lossless Speculative Decoding with Efficient Drafting and Verification](https://arxiv.org/pdf/2502.17421)
*Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An*

Main category: cs.CL

TL;DR: LongSpec is a framework for efficient speculative decoding in long-context LLMs, addressing memory, training-inference mismatch, and attention inefficiencies with innovations like a constant-sized KV cache and novel position indices. It achieves up to 3.26x speedup.


<details>
  <summary>Details</summary>
Motivation: Efficient inference over long contexts is critical for LLM applications like agents, but existing speculative decoding methods are trained on short texts and fail in long-context scenarios.

Method: LongSpec introduces a memory-efficient draft model, novel position indices, and an attention aggregation strategy combining fast prefix computation with tree attention.

Result: LongSpec achieves up to 3.26x speedup over baselines and reduces wall-clock time by 2.25x on long-context tasks.

Conclusion: LongSpec effectively addresses challenges in long-context speculative decoding, offering significant latency improvements for LLM applications.

Abstract: As Large Language Models (LLMs) can now process extremely long contexts, efficient inference over these extended inputs has become increasingly important, especially for emerging applications like LLM agents that highly depend on this capability. Speculative decoding (SD) offers a promising lossless acceleration technique compared to lossy alternatives such as quantization and model cascades. However, most state-of-the-art SD methods are trained on short texts (typically fewer than 4k tokens), making them unsuitable for long-context scenarios. Specifically, adapting these methods to long contexts presents three key challenges: (1) the excessive memory demands posed by draft models due to large Key-Value (KV) cache; (2) performance degradation resulting from the mismatch between short-context training and long-context inference; and (3) inefficiencies in tree attention mechanisms when managing long token sequences. This work introduces LongSpec, a framework that addresses these challenges through three core innovations: a memory-efficient draft model with a constant-sized KV cache; novel position indices that mitigate the training-inference mismatch; and an attention aggregation strategy that combines fast prefix computation with standard tree attention to enable efficient decoding. Experimental results confirm the effectiveness of LongSpec, achieving up to a 3.26x speedup over strong Flash Attention baselines across five long-context understanding datasets, as well as a 2.25x reduction in wall-clock time on the AIME24 long reasoning task with the QwQ model, demonstrating significant latency improvements for long-context applications. The code is available at https://github.com/sail-sg/LongSpec.

</details>


### [86] [Conformal Linguistic Calibration: Trading-off between Factuality and Specificity](https://arxiv.org/pdf/2502.19110)
*Zhengping Jiang, Anqi Liu, Benjamin Van Durme*

Main category: cs.CL

TL;DR: The paper proposes Conformal Linguistic Calibration (CLC), a method to adapt language model responses based on uncertainty by unifying abstention and linguistic calibration into answer set prediction, ensuring calibrated outputs with factual accuracy guarantees.


<details>
  <summary>Details</summary>
Motivation: Language models often produce unreliable outputs, and existing methods like abstention or linguistic calibration have limitations—abstention withholds information, while calibrated responses are hard to use downstream.

Method: CLC reinterprets linguistic calibration as answer set prediction, connecting abstention and calibration via linguistic pragmatics. It controls response imprecision and enables uncertainty-aware adaptive claim rewriting.

Result: CLC produces calibrated outputs with conformal guarantees on factual accuracy and allows fine-tuning for a balance between factuality and specificity.

Conclusion: CLC offers a unified, controllable approach to uncertainty-aware language model responses, improving reliability and usability.

Abstract: Language model outputs are not always reliable, thus prompting research into how to adapt model responses based on uncertainty. Common approaches include: \emph{abstention}, where models refrain from generating responses when uncertain; and \emph{linguistic calibration}, where models hedge their statements using uncertainty quantifiers. However, abstention can withhold valuable information, while linguistically calibrated responses are often challenging to leverage in downstream tasks. We propose a unified view, Conformal Linguistic Calibration (CLC), which reinterprets linguistic calibration as \emph{answer set prediction}. First we present a framework connecting abstention and linguistic calibration through the lens of linguistic pragmatics. We then describe an implementation of CLC that allows for controlling the level of imprecision in model responses. Results demonstrate our method produces calibrated outputs with conformal guarantees on factual accuracy. Further, our approach enables fine-tuning models to perform uncertainty-aware adaptive claim rewriting, offering a controllable balance between factuality and specificity.

</details>


### [87] [Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning](https://arxiv.org/pdf/2502.20620)
*Ayana Niwa, Masahiro Kaneko, Kentaro Inui*

Main category: cs.CL

TL;DR: A method to rectify LLMs' belief space by suppressing spurious beliefs and enhancing true ones, improving reliability without harming performance.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate incorrect answers due to spurious beliefs, which are internally held but incorrect propositions. Addressing this can enhance model reliability.

Method: Uses Forward-Backward Beam Search (FBBS) to identify spurious and true beliefs via textual explanations, then applies unlearning to suppress spurious beliefs and enhance true ones.

Result: Corrects misanswered questions without degrading overall performance and improves generalization on unseen data.

Conclusion: Rectifying belief spaces is a promising approach to mitigate LLM errors and boost reliability.

Abstract: Large language models (LLMs) can exhibit advanced reasoning yet still generate incorrect answers. We hypothesize that such errors frequently stem from spurious beliefs, propositions the model internally considers true but are incorrect. To address this, we propose a method to rectify the belief space by suppressing these spurious beliefs while simultaneously enhancing true ones, thereby enabling more reliable inferences. Our approach first identifies the beliefs that lead to incorrect or correct answers by prompting the model to generate textual explanations, using our Forward-Backward Beam Search (FBBS). We then apply unlearning to suppress the identified spurious beliefs and enhance the true ones, effectively rectifying the model's belief space. Empirical results on multiple QA datasets and LLMs show that our method corrects previously misanswered questions without harming overall model performance. Furthermore, our approach yields improved generalization on unseen data, suggesting that rectifying a model's belief space is a promising direction for mitigating errors and enhancing overall reliability.

</details>


### [88] [SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling](https://arxiv.org/pdf/2503.04619)
*Xin Zhang, Qiyu Wei, Yingjie Zhu, Linhai Zhang, Deyu Zhou, Sophia Ananiadou*

Main category: cs.CL

TL;DR: SynGraph is a framework addressing data sparsity in sentiment analysis for streaming reviews by categorizing users and using LLM-augmented enhancements in a dynamic graph structure.


<details>
  <summary>Details</summary>
Motivation: Traditional sentiment analysis fails to capture evolving sentiment patterns in streaming reviews due to data sparsity.

Method: SynGraph categorizes users into mid-tail, long-tail, and extreme scenarios and incorporates LLM-augmented enhancements in a dynamic graph-based structure.

Result: Experiments show SynGraph effectively addresses sparsity and improves sentiment modeling in streaming reviews.

Conclusion: SynGraph successfully tackles data sparsity and enhances sentiment analysis for dynamic review data.

Abstract: User reviews on e-commerce platforms exhibit dynamic sentiment patterns driven by temporal and contextual factors. Traditional sentiment analysis methods focus on static reviews, failing to capture the evolving temporal relationship between user sentiment rating and textual content. Sentiment analysis on streaming reviews addresses this limitation by modeling and predicting the temporal evolution of user sentiments. However, it suffers from data sparsity, manifesting in temporal, spatial, and combined forms. In this paper, we introduce SynGraph, a novel framework designed to address data sparsity in sentiment analysis on streaming reviews. SynGraph alleviates data sparsity by categorizing users into mid-tail, long-tail, and extreme scenarios and incorporating LLM-augmented enhancements within a dynamic graph-based structure. Experiments on real-world datasets demonstrate its effectiveness in addressing sparsity and improving sentiment modeling in streaming reviews.

</details>


### [89] [Effect of Selection Format on LLM Performance](https://arxiv.org/pdf/2503.06926)
*Yuchen Han, Yucheng Wu, Jeffrey Willard*

Main category: cs.CL

TL;DR: Bullet points in prompts generally improve LLM performance for classification tasks, but exceptions exist. Further research on option formatting is needed.


<details>
  <summary>Details</summary>
Motivation: To determine the impact of option formatting (bullet points vs. plain English) on LLM performance in classification tasks.

Method: Conducted an extensive experimental study comparing the two formats.

Result: Bullet points generally yield better performance, with some exceptions.

Conclusion: Option formatting affects LLM performance; continued exploration is necessary for further improvements.

Abstract: This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.

</details>


### [90] [SOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints](https://arxiv.org/pdf/2503.08669)
*Zekun Li, Shinda Huang, Jiangtian Wang, Nathan Zhang, Antonis Antoniades, Wenyue Hua, Kaijie Zhu, Sirui Zeng, Chi Wang, William Yang Wang, Xifeng Yan*

Main category: cs.CL

TL;DR: SOPBench evaluates language agents' adherence to domain-specific SOPs using automated test cases and verifiers, revealing challenges even for top models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Language agents often lack rigorous evaluation for following SOPs and constraints, which is critical for automating tasks.

Method: Developed SOPBench with executable environments, test generation, and evaluation frameworks to assess agent compliance with SOPs.

Result: Top models struggle (30%-50% pass rates), reasoning models perform better, and small models lag significantly. Agents can also be jailbroken.

Conclusion: SOPBench provides a robust evaluation tool, highlighting the need for improved agent adherence to SOPs and constraints.

Abstract: As language agents increasingly automate critical tasks, their ability to follow domain-specific standard operating procedures (SOPs), policies, and constraints when taking actions and making tool calls becomes essential yet remains underexplored. To address this gap, we develop an automated evaluation pipeline SOPBench with: (1) executable environments containing 167 tools/functions across seven customer service domains with service-specific SOPs and rule-based verifiers, (2) an automated test generation framework producing over 900 verified test cases, and (3) an automated evaluation framework to rigorously assess agent adherence from multiple dimensions. Our approach transforms each service-specific SOP code program into a directed graph of executable functions and requires agents to call these functions based on natural language SOP descriptions. The original code serves as oracle rule-based verifiers to assess compliance, reducing reliance on manual annotations and LLM-based evaluations. We evaluate 18 leading models, and results show the task is challenging even for top-tier models (like GPT-4o, Claude-3.7-Sonnet), with variances across domains. Reasoning models like o4-mini-high show superiority while other powerful models perform less effectively (pass rates of 30%-50%), and small models (7B, 8B) perform significantly worse. Additionally, language agents can be easily jailbroken to overlook SOPs and constraints. Code, data, and over 24k agent trajectories are released at https://github.com/Leezekun/SOPBench.

</details>


### [91] [Do Construction Distributions Shape Formal Language Learning In German BabyLMs?](https://arxiv.org/pdf/2503.11593)
*Bastian Bunzeck, Daniel Duran, Sina Zarrieß*

Main category: cs.CL

TL;DR: The study examines how utterance-level construction distributions in German child-directed speech affect word-level, syntactic, and semantic learning in small LMs, finding robust trajectories despite varying training data.


<details>
  <summary>Details</summary>
Motivation: To understand how different linguistic stimuli influence language learning in small LMs, using developmentally plausible German data.

Method: Training small LMs on a novel collection of German child-directed/child-available speech and analyzing their learning trajectories.

Result: Trajectories are robust across varying construction distributions, with syntax benefiting from complex utterances and word-level learning from fragmentary ones.

Conclusion: LMs trained on developmentally plausible data can inform debates on the effectiveness of linguistic stimuli for language learning.

Abstract: We analyze the influence of utterance-level construction distributions in German child-directed/child-available speech on the resulting word-level, syntactic and semantic competence (and their underlying learning trajectories) in small LMs, which we train on a novel collection of developmentally plausible language data for German. We find that trajectories are surprisingly robust for markedly different distributions of constructions in the training data, which have little effect on final accuracies and almost no effect on global learning trajectories. While syntax learning benefits from more complex utterances, word-level learning culminates in better scores with more fragmentary utterances. We argue that LMs trained on developmentally plausible data can contribute to debates on how conducive different kinds of linguistic stimuli are to language learning.

</details>


### [92] [Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information](https://arxiv.org/pdf/2504.07738)
*Andrea Loreti, Kesi Chen, Ruby George, Robert Firth, Adriano Agnello, Shinnosuke Tanaka*

Main category: cs.CL

TL;DR: A multi-step method for building a knowledge graph in nuclear fusion energy, using pre-trained language models and evaluating against Zipf's law, with a retrieval-augmented system for complex queries.


<details>
  <summary>Details</summary>
Motivation: To structure domain-specific knowledge from large document corpora in nuclear fusion energy, a field with vast scope and heterogeneity.

Method: Multi-step pipeline with automatic named entity recognition, entity resolution, and pre-trained large language models. Includes a retrieval-augmented generation system for query answering.

Result: Demonstrates the effectiveness of pre-trained models in knowledge graph construction and retrieval, handling complex multi-hop queries.

Conclusion: The approach successfully builds a specialized knowledge graph and enhances query answering, showcasing the potential of combining language models with structured knowledge.

Abstract: In this document, we discuss a multi-step approach to automated construction of a knowledge graph, for structuring and representing domain-specific knowledge from large document corpora. We apply our method to build the first knowledge graph of nuclear fusion energy, a highly specialized field characterized by vast scope and heterogeneity. This is an ideal benchmark to test the key features of our pipeline, including automatic named entity recognition and entity resolution. We show how pre-trained large language models can be used to address these challenges and we evaluate their performance against Zipf's law, which characterizes human-generated natural language. Additionally, we develop a knowledge-graph retrieval-augmented generation system that combines large language models with a multi-prompt approach. This system provides contextually relevant answers to natural-language queries, including complex multi-hop questions that require reasoning across interconnected entities.

</details>


### [93] [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/pdf/2504.16005)
*Tom Zehle, Moritz Schlager, Timo Heiß, Matthias Feurer*

Main category: cs.CL

TL;DR: CAPO is a cost-aware prompt optimization algorithm that improves efficiency by integrating AutoML techniques, outperforming existing methods with fewer LLM calls and shorter prompts.


<details>
  <summary>Details</summary>
Motivation: Current prompt optimization methods are expensive due to high LLM calls and input tokens, necessitating a more efficient solution.

Method: CAPO uses an evolutionary approach with LLMs as operators, incorporating racing and multi-objective optimization to balance performance and prompt length.

Result: CAPO outperforms state-of-the-art methods in 11/15 cases, improving accuracy by up to 21%p, with cost-efficiency and robustness.

Conclusion: CAPO advances prompt optimization by enhancing cost-efficiency, making it more powerful and accessible.

Abstract: Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automatic prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p in accuracy. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.

</details>


### [94] [Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic Approach](https://arxiv.org/pdf/2505.00039)
*Hudson de Martim*

Main category: cs.CL

TL;DR: The paper adapts Graph RAG for legal norm analysis, addressing challenges like hierarchical structure, references, and temporal dynamism by using a FRBRoo-inspired model for deterministic representation.


<details>
  <summary>Details</summary>
Motivation: Legal texts' complexity and temporal evolution challenge standard AI systems, requiring accurate, point-in-time legal information.

Method: A multi-layered representation of Temporal and Language Versions grounded in a FRBRoo-inspired model to construct a verifiable knowledge graph.

Result: Enables Large Language Models to generate context-aware, accurate legal responses, mitigating temporal inaccuracies.

Conclusion: Advances AI in law, improving legal research, legislative analysis, and decision support with reliable systems.

Abstract: This article proposes an adaptation of Graph Retrieval-Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms. Legal texts are characterized by a predefined hierarchical structure, an extensive network of references and a continuous evolution through multiple temporal versions. This temporal dynamism poses a significant challenge for standard AI systems, demanding a deterministic representation of the law at any given point in time. To address this, our approach grounds the knowledge graph construction in a formal, FRBRoo-inspired model that distinguishes abstract legal works from their concrete textual expressions. We introduce a multi-layered representation of Temporal Versions (capturing date-specific changes) and Language Versions (capturing linguistic variations). By modeling normative evolution as a precise sequence of these versioned entities, we enable the construction of a knowledge graph that serves as a verifiable "ground truth". This allows Large Language Models to generate responses based on accurate, context-aware, and point-in-time correct legal information, overcoming the risk of temporal inaccuracies. Through a detailed analysis of this formal Graph RAG approach and its application to legal norm datasets, this article aims to advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective and reliable systems in legal research, legislative analysis, and decision support.

</details>


### [95] [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/pdf/2505.06987)
*Xiaoyu Wang, Yue Zhao, Qingqing Gu, Zhonglin Jiang, Xiaokai Chen, Yong Chen, Luo Ji*

Main category: cs.CL

TL;DR: The paper proposes straQ*, a Q-learning-based framework for emotional support conversation (ESC) to improve long-term satisfaction by optimizing strategies in LLMs.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based ESC solutions lack a state model perspective, leading to suboptimal long-term satisfaction.

Method: The straQ* framework integrates Q-learning with LLMs to bootstrap planning, determine optimal strategies, and guide responses.

Result: Experiments show straQ* outperforms baselines like direct inference, self-refine, chain of thought, finetuning, and finite state machines.

Conclusion: straQ* effectively enhances ESC by leveraging Q-learning for long-term strategy optimization in LLMs.

Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.

</details>


### [96] [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/pdf/2505.07897)
*Stefano Rando, Luca Romani, Alessio Sampieri, Luca Franco, John Yang, Yuta Kyuragi, Fabio Galasso, Tatsunori Hashimoto*

Main category: cs.CL

TL;DR: The paper introduces LongCodeBench (LCB), a benchmark for evaluating long-context models in code comprehension and repair tasks, highlighting performance drops in models like Claude 3.5 Sonnet and Qwen2.5.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of model context lengths necessitates realistic benchmarks, but constructing such benchmarks is challenging due to costs and identifying realistic scenarios.

Method: The authors propose LongCodeBench, which includes QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks derived from real-world GitHub issues, stratified by complexity.

Result: Performance drops significantly in long-context scenarios, e.g., Claude 3.5 Sonnet drops from 29% to 3%, and Qwen2.5 from 70.2% to 40%.

Conclusion: Long-context remains a challenge for models, and LongCodeBench provides a valuable tool for evaluating their capabilities in realistic settings.

Abstract: Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.

</details>


### [97] [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/pdf/2505.11368)
*Lingxiao Diao, Xinyue Xu, Wanxuan Sun, Cheng Yang, Zhuosheng Zhang*

Main category: cs.CL

TL;DR: GuideBench is introduced to evaluate LLMs' ability to follow domain-oriented guidelines, addressing gaps in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on general domains and commonsense knowledge, but lack evaluation for domain-specific guidelines, which are complex and frequently updated.

Method: GuideBench assesses LLMs on rule adherence, robustness to updates, and human preference alignment.

Result: Experiments show significant room for improvement in LLMs' guideline-following capabilities.

Conclusion: GuideBench fills a critical gap in evaluating LLMs as domain-oriented agents, highlighting areas for future enhancement.

Abstract: Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.

</details>


### [98] [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/pdf/2505.12368)
*Gauri Kholkar, Ratinder Ahuja*

Main category: cs.CL

TL;DR: CAPTURE introduces a context-aware benchmark for prompt injection guardrails, revealing flaws in current models and proposing CaptureGuard for improved robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored efficacy of guardrail models in context-aware settings and their over-defense tendencies.

Method: Introduces CAPTURE, a novel benchmark, and trains CaptureGuard on generated data to assess and improve prompt injection defenses.

Result: Current models show high false negatives in adversarial cases and excessive false positives in benign scenarios. CaptureGuard reduces both rates effectively.

Conclusion: CAPTURE and CaptureGuard offer a path toward more robust and practical prompt injection defenses.

Abstract: Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations. To demonstrate our framework's utility, we train CaptureGuard on our generated data. This new model drastically reduces both false negative and false positive rates on our context-aware datasets while also generalizing effectively to external benchmarks, establishing a path toward more robust and practical prompt injection defenses.

</details>


### [99] [Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)](https://arxiv.org/pdf/2505.17238)
*Clayton Cohn, Surya Rayala, Caitlin Snyder, Joyce Fonteles, Shruti Jain, Naveeduddin Mohammed, Umesh Timalsina, Sarah K. Burriss, Ashwin T S, Namrata Srivastava, Menton Deweese, Angela Eeds, Gautam Biswas*

Main category: cs.CL

TL;DR: LC-RAG improves retrieval-augmented generation for personalized STEM+C pedagogy by leveraging environment logs to contextualize student dialogue, enhancing relevance and trust in agent guidance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of retrieval-augmented generation (RAG) in student dialogues, where weak semantic links hinder effective grounding of LLM outputs, and to improve pedagogical agent interactions in STEM+C settings.

Method: Proposes log-contextualized RAG (LC-RAG), which uses environment logs to enhance retrieval by contextualizing collaborative discourse, tested in the C2STEM environment with the Copa agent.

Result: LC-RAG outperforms discourse-only baselines in retrieval, enabling the Copa agent to provide relevant, personalized guidance that supports critical thinking and epistemic decision-making.

Conclusion: LC-RAG effectively bridges the gap in RAG for student dialogues, improving trust and instructional value in personalized STEM+C pedagogy.

Abstract: Collaborative dialogue offers rich insights into students' learning and critical thinking, which is essential for personalizing pedagogical agent interactions in STEM+C settings. While large language models (LLMs) facilitate dynamic pedagogical interactions, hallucinations undermine confidence, trust, and instructional value. Retrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge but requires a clear semantic link between user input and a knowledge base, which is often weak in student dialogue. We propose log-contextualized RAG (LC-RAG), which enhances RAG retrieval by using environment logs to contextualize collaborative discourse. Our findings show that LC-RAG improves retrieval over a discourse-only baseline and allows our collaborative peer agent, Copa, to deliver relevant, personalized guidance that supports students' critical thinking and epistemic decision-making in a collaborative computational modeling environment, C2STEM.

</details>


### [100] [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/pdf/2505.20613)
*Ziju Shen, Naohao Huang, Fanyi Yang, Yutong Wang, Guoxiong Gao, Tianyi Xu, Jiedong Jiang, Wanyi He, Pu Yang, Mengzhou Sun, Haocheng Ju, Peihao Wu, Bryan Dai, Bin Dong*

Main category: cs.CL

TL;DR: REAL-Prover is a new theorem prover for Lean 4, combining a fine-tuned LLM and retrieval system, achieving competitive results on college-level math problems.


<details>
  <summary>Details</summary>
Motivation: Existing theorem provers excel at high-school math but struggle with advanced mathematics. REAL-Prover aims to bridge this gap.

Method: Developed HERALD-AF for data extraction and Jixia-interactive for data collection. Combines REAL-Prover-v1 (fine-tuned LLM) with Leansearch-PS (retrieval system).

Result: Achieves 23.7% success on ProofNet and 56.7% on FATE-M, setting new benchmarks.

Conclusion: REAL-Prover advances theorem proving for advanced math, demonstrating the potential of combining LLMs with retrieval systems.

Abstract: Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).

</details>


### [101] [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/pdf/2506.01565)
*Li Zhou, Lutong Yu, Dongchu Xie, Shaohuan Cheng, Wenyan Li, Haizhou Li*

Main category: cs.CL

TL;DR: Hanfu-Bench introduces a multimodal dataset for temporal-cultural understanding and transcreation, revealing gaps in VLMs' performance compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing studies on cultural understanding with VLMs focus on geographic diversity but neglect temporal dimensions, which Hanfu-Bench addresses.

Method: The dataset includes tasks for cultural visual understanding (multiple-choice VQA) and cultural image transcreation (modernizing traditional attire).

Result: Closed VLMs match non-experts in visual understanding but trail human experts by 10%; open VLMs perform worse. Transcreation success rate is 42%.

Conclusion: Hanfu-Bench highlights challenges in temporal cultural understanding and adaptation, serving as a critical benchmark for future research.

Abstract: Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation.The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10\% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42\%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation.

</details>


### [102] [EuroLLM-9B: Technical Report](https://arxiv.org/pdf/2506.04079)
*Pedro Henrique Martins, João Alves, Patrick Fernandes, Nuno M. Guerreiro, Ricardo Rei, Amin Farajian, Mateusz Klimaszewski, Duarte M. Alves, José Pombal, Nicolas Boizard, Manuel Faysse, Pierre Colombo, François Yvon, Barry Haddow, José G. C. de Souza, Alexandra Birch, André F. T. Martins*

Main category: cs.CL

TL;DR: EuroLLM-9B is a multilingual large language model supporting 35 European languages, addressing underrepresentation in existing models. It includes novel data filtering and synthetic datasets, achieving competitive performance and being openly released.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation and underserving of European languages in existing open large language models.

Method: Developed EuroLLM-9B with a focus on tokenizer design, data filtering (EuroFilter), and synthetic datasets (EuroBlocks-Synthetic). Detailed pre-training and post-training procedures were followed.

Result: EuroLLM-9B shows competitive performance on multilingual benchmarks and machine translation tasks, becoming the leading open European-made LLM of its size.

Conclusion: EuroLLM-9B successfully supports European languages, with all major components released to foster open research and adoption.

Abstract: This report presents EuroLLM-9B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-9B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. We describe the pre-training data collection and filtering pipeline, including the creation of EuroFilter, an AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a novel synthetic dataset for post-training that enhances language coverage for European languages. Evaluation results demonstrate EuroLLM-9B's competitive performance on multilingual benchmarks and machine translation tasks, establishing it as the leading open European-made LLM of its size. To support open research and adoption, we release all major components of this work, including the base and instruction-tuned models, the EuroFilter classifier, and the synthetic post-training dataset.

</details>


### [103] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/pdf/2506.07801)
*Iustin Sirbu, Robert-Adrian Popovici, Cornelia Caragea, Stefan Trausan-Matu, Traian Rebedea*

Main category: cs.CL

TL;DR: MultiMatch is a semi-supervised learning algorithm combining co-training, consistency regularization, and pseudo-labeling, featuring a novel pseudo-label weighting module for improved robustness and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance semi-supervised learning by unifying and improving existing techniques (heads agreement, self-adaptive thresholds, and Average Pseudo-Margins) for better performance and robustness, especially in imbalanced settings.

Method: MultiMatch integrates a three-fold pseudo-label weighting module for selecting, filtering, and weighting pseudo-labels based on head agreement, model confidence, and classification difficulty.

Result: Achieves state-of-the-art results on 9/10 setups across 5 NLP datasets and ranks first among 19 methods. Shows 3.26% improvement over the second-best method in imbalanced settings.

Conclusion: MultiMatch is a robust and high-performing SSL algorithm, particularly effective for text classification tasks with data imbalance.

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label weighting module designed for three key purposes: selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, achieving state-of-the-art results on 9 out of 10 setups from 5 natural language processing datasets and ranking first according to the Friedman test among 19 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26% -- and data imbalance is a key factor for many text classification tasks.

</details>


### [104] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/pdf/2506.10055)
*Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King Zhu, Minghao Liu, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou*

Main category: cs.CL

TL;DR: TaskCraft automates the creation of scalable, multi-tool agentic tasks to address limitations in existing instruction data and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing agentic benchmarks lack scalability due to reliance on human annotation, and instruction data lacks tool interaction.

Method: TaskCraft uses depth-based and width-based extensions to generate structurally complex tasks, producing a synthetic dataset of ~36,000 tasks.

Result: The tasks improve prompt optimization and enhance supervised fine-tuning of agentic models.

Conclusion: TaskCraft provides a scalable solution for advancing agentic task research, supported by a large synthetic dataset.

Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce \textsc{TaskCraft}, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.

</details>


### [105] [A Hybrid Multi-Agent Prompting Approach for Simplifying Complex Sentences](https://arxiv.org/pdf/2506.11681)
*Pratibha Zunjare, Michael Hsiao*

Main category: cs.CL

TL;DR: A hybrid approach using LLMs and multi-agent architectures simplifies complex sentences, achieving 70% success, outperforming single-agent methods (48%).


<details>
  <summary>Details</summary>
Motivation: To transform complex sentences into simplified sequences while maintaining semantic and logical integrity.

Method: Hybrid approach combining advanced prompting with multi-agent architectures.

Result: 70% success rate in simplifying complex sentences, compared to 48% with single-agent.

Conclusion: The hybrid approach is more effective for sentence simplification in video game design applications.

Abstract: This paper addresses the challenge of transforming complex sentences into sequences of logical, simplified sentences while preserving semantic and logical integrity with the help of Large Language Models. We propose a hybrid approach that combines advanced prompting with multi-agent architectures to enhance the sentence simplification process. Experimental results show that our approach was able to successfully simplify 70% of the complex sentences written for video game design application. In comparison, a single-agent approach attained a 48% success rate on the same task.

</details>


### [106] [Enhancing Clinical Models with Pseudo Data for De-identification](https://arxiv.org/pdf/2506.12674)
*Paul Landes, Aaron J Chaise, Tarak Nath Nandi, Ravi K Madduri*

Main category: cs.CL

TL;DR: The paper explores the impact of training models on redacted text versus pseudo-replaced text for clinical foundation models, showing improved performance in de-identification tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the effects of training models on redacted text, which is common in clinical settings for privacy, and to improve de-identification performance.

Method: Pretrained encoder-only models on redacted and pseudo-replaced text datasets, then fine-tuned for de-identification tasks.

Result: The method significantly outperforms previous baselines in de-identification tasks.

Conclusion: The work provides novel findings, training recommendations, and open-source resources for pseudo dataset generation and model training.

Abstract: Many models are pretrained on redacted text for privacy reasons. Clinical foundation models are often trained on de-identified text, which uses special syntax (masked) text in place of protected health information. Even though these models have increased in popularity, there has been little effort in understanding the effects of training them on redacted text. In this work, we pretrain several encoder-only models on a dataset that contains redacted text and a version with replaced realistic pseudo text. We then fine-tuned models for the protected health information de-identification task and show how our methods significantly outperform previous baselines. The contributions of this work include: a) our novel, and yet surprising findings with training recommendations, b) redacted text replacements used to produce the pseudo dataset, c) pretrained embeddings and fine-tuned task specific models, and d) freely available pseudo training dataset generation and model source code used in our experiments.

</details>


### [107] [Surprise Calibration for Better In-Context Learning](https://arxiv.org/pdf/2506.12796)
*Zhihang Tan, Jingrui Hou, Ping Wang, Qibiao Hu, Peng Zhu*

Main category: cs.CL

TL;DR: Surprise Calibration (SC) improves bias calibration in in-context learning (ICL) by dynamically adjusting class priors using surprise signals, outperforming fixed-prior methods.


<details>
  <summary>Details</summary>
Motivation: ICL in LLMs is prone to biases from prior knowledge and contextual demonstrations, degrading performance. Fixed-prior calibration methods are ineffective in dynamic ICL settings.

Method: Proposes Surprise Calibration (SC), using implicit sequential Bayesian inference and surprise signals to adaptively adjust class priors.

Result: SC outperforms existing bias calibration techniques across multiple NLP benchmark tasks.

Conclusion: SC provides a more adaptive and efficient solution for bias calibration in ICL, enhancing LLM performance.

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for task adaptation in large language models (LLMs), where models infer underlying task structures from a few demonstrations. However, ICL remains susceptible to biases that arise from prior knowledge and contextual demonstrations, which can degrade the performance of LLMs. Existing bias calibration methods typically apply fixed class priors across all inputs, limiting their efficacy in dynamic ICL settings where the context for each query differs. To address these limitations, we adopt implicit sequential Bayesian inference as a framework for interpreting ICL, identify "surprise" as an informative signal for class prior shift, and introduce a novel method--Surprise Calibration (SC). SC leverages the notion of surprise to capture the temporal dynamics of class priors, providing a more adaptive and computationally efficient solution for in-context learning. We empirically demonstrate the superiority of SC over existing bias calibration techniques across a range of benchmark natural language processing tasks.

</details>


### [108] [AI-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns](https://arxiv.org/pdf/2506.13172)
*Evgeny Markhasin*

Main category: cs.CL

TL;DR: The paper evaluates structured workflow prompts for LLMs to analyze scholarly manuscripts, focusing on identifying unsubstantiated claims and ambiguous pronouns. Results show performance varies by model, task, and context.


<details>
  <summary>Details</summary>
Motivation: To improve LLMs' hierarchical reasoning in analyzing scholarly texts by addressing informational integrity and linguistic clarity.

Method: Used proof-of-concept prompts tested on Gemini Pro 2.5 Pro and ChatGPT Plus o3 under varied conditions.

Result: Divergent performance: Gemini excelled in some tasks (e.g., adjectival modifiers) while ChatGPT performed better in others (e.g., summary-only pronoun resolution).

Conclusion: Structured prompting is viable but performance depends on model, task, and context, necessitating model-specific testing.

Abstract: We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in the high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks within academic summaries (abstracts and conclusions): identifying unsubstantiated claims (informational integrity) and flagging semantically confusing ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding the potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. Surprisingly, in a summary-only setting, Gemini's performance was substantially degraded, while ChatGPT achieved a perfect (100%) success rate. Our findings suggest that while structured prompting is a viable methodology for complex textual analysis, prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing.

</details>


### [109] [Enhancing Goal-oriented Proactive Dialogue Systems via Consistency Reflection and Correction](https://arxiv.org/pdf/2506.13366)
*Didi Zhang, Yaxin Fan, Peifeng Li, Qiaoming Zhu*

Main category: cs.CL

TL;DR: The paper introduces a two-stage CRC framework to improve consistency between dialogue responses and contexts in goal-oriented dialogue systems.


<details>
  <summary>Details</summary>
Motivation: Previous research focused on optimizing dialogue paths but ignored inconsistencies between responses and contexts, such as user profiles and dialogue history.

Method: The CRC framework has two stages: (1) reflection to identify inconsistencies and suggest corrections, and (2) correction to generate context-consistent responses.

Result: Experiments on various models (e.g., BART, GPT-2, LLaMA3) show CRC significantly improves response consistency across three datasets.

Conclusion: The CRC framework effectively addresses inconsistencies in goal-oriented dialogue systems, enhancing response quality.

Abstract: Goal-oriented proactive dialogue systems are designed to guide user conversations seamlessly towards specific objectives by planning a goal-oriented path. However, previous research has focused predominantly on optimizing these paths while neglecting the inconsistencies that may arise between generated responses and dialogue contexts, including user profiles, dialogue history, domain knowledge, and subgoals. To address this issue, we introduce a model-agnostic two-stage Consistency Reflection and Correction (CRC) framework. Specifically, in the consistency reflection stage, the model is prompted to reflect on the discrepancies between generated responses and dialogue contexts, identifying inconsistencies and suggesting possible corrections. In the consistency correction stage, the model generates responses that are more consistent with the dialogue context based on these reflection results. We conducted experiments on various model architectures with different parameter sizes, including encoder-decoder models (BART, T5) and decoder-only models (GPT-2, DialoGPT, Phi3, Mistral and LLaMA3), and the experimental results on three datasets demonstrate that our CRC framework significantly improves the consistency between generated responses and dialogue contexts.

</details>


### [110] [ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models](https://arxiv.org/pdf/2506.13472)
*Junho Yoon, Geom Lee, Donghyeon Jeon, Inho Kang, Seung-Hoon Na*

Main category: cs.CL

TL;DR: ROSAQ is a rotation-based saliency-aware weight quantization method for LLMs, improving efficiency by identifying salient channels in PCA-projected space and using mixed-precision (FP16/INT3/4). It outperforms baselines and speeds up token generation.


<details>
  <summary>Details</summary>
Motivation: Reducing memory and latency in large language models (LLMs) through effective quantization, leveraging transformer rotational invariance.

Method: 1) PCA projection for feature transformation, 2) Salient channel identification via K-largest eigenvalues, 3) Mixed-precision quantization (FP16 for salient, INT3/4 for others).

Result: ROSAQ outperforms baseline and existing methods, achieving ~2.3x speedup over FP16 with kernel fusion.

Conclusion: ROSAQ is an efficient quantization method for LLMs, combining PCA-based saliency and mixed-precision for improved performance.

Abstract: Quantization has been widely studied as an effective technique for reducing the memory requirement of large language models (LLMs), potentially improving the latency time as well. Utilizing the characteristic of rotational invariance of transformer, we propose the rotation-based saliency-aware weight quantization (ROSAQ), which identifies salient channels in the projection feature space, not in the original feature space, where the projected "principal" dimensions are naturally considered as "salient" features. The proposed ROSAQ consists of 1) PCA-based projection, which first performs principal component analysis (PCA) on a calibration set and transforms via the PCA projection, 2) Salient channel dentification, which selects dimensions corresponding to the K-largest eigenvalues as salient channels, and 3) Saliency-aware quantization with mixed-precision, which uses FP16 for salient dimensions and INT3/4 for other dimensions. Experiment results show that ROSAQ shows improvements over the baseline saliency-aware quantization on the original feature space and other existing quantization methods. With kernel fusion, ROSAQ presents about 2.3x speed up over FP16 implementation in generating 256 tokens with a batch size of 64.

</details>


### [111] [Prefix-Tuning+: Modernizing Prefix-Tuning by Decoupling the Prefix from Attention](https://arxiv.org/pdf/2506.13674)
*Haonan Wang, Brian Chen, Siquan Li, Xinhe Liang, Hwee Kuan Lee, Kenji Kawaguchi, Tianyang Hu*

Main category: cs.CL

TL;DR: Prefix-Tuning, a PEFT method, underperforms on modern LLMs due to a tradeoff in attention heads. Prefix-Tuning+ is introduced to address this, outperforming Prefix-Tuning and matching LoRA in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Prefix-Tuning's limitations in modern LLMs due to attention head tradeoffs motivate the development of Prefix-Tuning+.

Method: Prefix-Tuning+ shifts the prefix module out of the attention head, generalizing Prefix-Tuning principles while addressing its shortcomings.

Result: Prefix-Tuning+ consistently outperforms Prefix-Tuning and matches LoRA on benchmarks, demonstrating its effectiveness.

Conclusion: Prefix-Tuning+ revitalizes Prefix-Tuning as a competitive PEFT method, suggesting its continued relevance in LLM adaptation.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have become crucial for rapidly adapting large language models (LLMs) to downstream tasks. Prefix-Tuning, an early and effective PEFT technique, demonstrated the ability to achieve performance comparable to full fine-tuning with significantly reduced computational and memory overhead. However, despite its earlier success, its effectiveness in training modern state-of-the-art LLMs has been very limited. In this work, we demonstrate empirically that Prefix-Tuning underperforms on LLMs because of an inherent tradeoff between input and prefix significance within the attention head. This motivates us to introduce Prefix-Tuning+, a novel architecture that generalizes the principles of Prefix-Tuning while addressing its shortcomings by shifting the prefix module out of the attention head itself. We further provide an overview of our construction process to guide future users when constructing their own context-based methods. Our experiments show that, across a diverse set of benchmarks, Prefix-Tuning+ consistently outperforms existing Prefix-Tuning methods. Notably, it achieves performance on par with the widely adopted LoRA method on several general benchmarks, highlighting the potential modern extension of Prefix-Tuning approaches. Our findings suggest that by overcoming its inherent limitations, Prefix-Tuning can remain a competitive and relevant research direction in the landscape of parameter-efficient LLM adaptation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [112] [Non-planar Object Detection and Identification by Features Matching and Triangulation Growth](https://arxiv.org/pdf/2506.13769)
*Filippo Leveni*

Main category: cs.CV

TL;DR: A feature-based method for detecting and identifying distorted objects in images using incremental grouping of feature matches, guided by Delaunay triangulation.


<details>
  <summary>Details</summary>
Motivation: Object detection is key in computer vision, but existing methods struggle with non-planar or distorted objects.

Method: Uses Delaunay triangulation of template features as a graph, incrementally grouping matches based on local consistency criteria.

Result: Outperforms homography-based RANSAC in distorted scenarios and matches performance in low-distortion cases.

Conclusion: The method effectively detects distorted objects, expanding applicability beyond planar or minimally distorted cases.

Abstract: Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.

</details>


### [113] [CDST: Color Disentangled Style Transfer for Universal Style Reference Customization](https://arxiv.org/pdf/2506.13770)
*Shiwen Zhang, Zhuowei Chen, Lang Chen, Yanze Wu*

Main category: cs.CV

TL;DR: CDST is a novel two-stream style transfer method that isolates color from style, enabling universal, tuning-free style transfer with improved similarity and editing capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of preserving style characteristics while isolating color in style transfer, enabling universal and tuning-free capabilities.

Method: Uses a two-stream training paradigm, multi-feature image embeddings compression, and a new CDST style definition inspired by Diffusion UNet disentanglement.

Result: Achieves state-of-the-art results in style similarity and editing capability, validated by qualitative, quantitative, and human evaluations.

Conclusion: CDST successfully isolates color from style, enabling high-quality, universal style transfer without tuning, outperforming existing methods.

Abstract: We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks.

</details>


### [114] [Hidden Bias in the Machine: Stereotypes in Text-to-Image Models](https://arxiv.org/pdf/2506.13780)
*Sedat Porikli, Vedat Porikli*

Main category: cs.CV

TL;DR: The paper examines biases in Text-to-Image (T2I) models, revealing disparities in gender, race, age, and other human-centric factors, and calls for more inclusive development practices.


<details>
  <summary>Details</summary>
Motivation: To investigate societal biases replicated and magnified by T2I models, given their growing role in visual content creation.

Method: Curated 160 diverse topics with multiple prompt variations, generated over 16,000 images using Stable Diffusion 1.5 and Flux-1 models, and compared with 8,000 Google Image Search results.

Result: Significant disparities in representation of gender, race, age, and somatotype, often reinforcing harmful societal stereotypes.

Conclusion: Highlights the need for inclusive datasets and development practices to ensure fairness in generative visual systems.

Abstract: Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.

</details>


### [115] [Fake it till You Make it: Reward Modeling as Discriminative Prediction](https://arxiv.org/pdf/2506.13846)
*Runtao Liu, Jiahao Zhan, Yingqing He, Chen Wei, Alan Yuille, Qifeng Chen*

Main category: cs.CV

TL;DR: GAN-RM is a reward modeling framework for reinforcement learning in visual generative models, eliminating manual annotations and explicit quality engineering by using adversarial training.


<details>
  <summary>Details</summary>
Motivation: Current reward modeling methods are complex, relying on human-annotated data or engineered quality dimensions, which are incomplete and labor-intensive.

Method: GAN-RM trains a reward model by discriminating between a small set of unpaired target samples (Preference Proxy Data) and model-generated outputs, requiring minimal data.

Result: GAN-RM is effective for applications like Best-of-N filtering, Supervised Fine-Tuning (SFT), and Direct Preference Optimization (DPO).

Conclusion: GAN-RM offers a simpler, efficient alternative to traditional reward modeling, reducing reliance on human input and engineering.

Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

</details>


### [116] [Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer](https://arxiv.org/pdf/2309.14704)
*Zhihao Zhang, Yiwei Chen, Weizhan Zhang, Caixia Yan, Qinghua Zheng, Qi Wang, Wangdu Chen*

Main category: cs.CV

TL;DR: The paper proposes MFTR, a tile classification-based viewport prediction method using Multi-modal Fusion Transformer, outperforming existing methods in accuracy, robustness, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory-based methods for viewport prediction in 360 video streaming lack robustness and oversimplify multi-modal input fusion, leading to error accumulation.

Method: MFTR employs transformer networks to extract long-range dependencies and intra-/inter-modality relations, classifying future tiles as user-interested or not to select viewports.

Result: MFTR achieves higher prediction accuracy and overlap ratio on PVS-HM and Xu-Gaze datasets, with competitive computation efficiency.

Conclusion: MFTR's tile classification approach enhances robustness and interpretability, demonstrating superior performance over state-of-the-art methods.

Abstract: Viewport prediction is a crucial aspect of tile-based 360 video streaming system. However, existing trajectory based methods lack of robustness, also oversimplify the process of information construction and fusion between different modality inputs, leading to the error accumulation problem. In this paper, we propose a tile classification based viewport prediction method with Multi-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes transformer-based networks to extract the long-range dependencies within each modality, then mine intra- and inter-modality relations to capture the combined impact of user historical inputs and video contents on future viewport selection. In addition, MFTR categorizes future tiles into two categories: user interested or not, and selects future viewport as the region that contains most user interested tiles. Comparing with predicting head trajectories, choosing future viewport based on tile's binary classification results exhibits better robustness and interpretability. To evaluate our proposed MFTR, we conduct extensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows superior performance over state-of-the-art methods in terms of average prediction accuracy and overlap ratio, also presents competitive computation efficiency.

</details>


### [117] [DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding](https://arxiv.org/pdf/2506.13897)
*Thomas Kreutz, Max Mühlhäuser, Alejandro Sanchez Guinea*

Main category: cs.CV

TL;DR: DeSPITE introduces a multi-modal contrastive pre-training model for human activity understanding, combining LiDAR, skeleton poses, IMU data, and text in a joint embedding space.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored use of LiDAR in multi-modal contrastive pre-training for human activity understanding.

Method: DeSPITE learns joint embeddings across four modalities (LiDAR, skeleton, IMU, text) using noise contrastive estimation, leveraging combined datasets (LIPD and Babel).

Result: Enables novel tasks like cross-modal matching and retrieval, and improves point cloud HAR performance in benchmarks (MSR-Action3D, HMPEAR).

Conclusion: DeSPITE effectively bridges the gap in multi-modal human activity understanding and serves as a strong pre-training strategy.

Abstract: Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding (e.g., human activity recognition (HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model, which effectively learns a joint embedding space across these four modalities through noise contrastive estimation. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR.

</details>


### [118] [Controllable Dance Generation with Style-Guided Motion Diffusion](https://arxiv.org/pdf/2406.07871)
*Hongsong Wang, Ying Zhu, Yang Zhang, Junbo Wang, Xin Geng, Liang Wang*

Main category: cs.CV

TL;DR: A diffusion-based framework (DGSDP) is introduced for flexible dance generation, leveraging music style semantics to create realistic and music-aligned dances.


<details>
  <summary>Details</summary>
Motivation: Dance creation is challenging, and existing methods often overlook music style attributes. This work aims to enhance dance generation by integrating music style semantics.

Method: The framework uses Music-Conditioned Style-Aware Diffusion (MCSAD), combining a Transformer-based network and a Style Modulation module, with a spatial-temporal masking strategy for flexibility.

Result: The framework successfully generates realistic dance sequences aligned with music for tasks like long-term generation, in-betweening, and inpainting.

Conclusion: DGSDP has potential to inspire dance generation and applications in entertainment, art, and education.

Abstract: Dance plays an important role as an artistic form and expression in human culture, yet the creation of dance remains a challenging task. Most dance generation methods primarily rely solely on music, seldom taking into consideration intrinsic attributes such as music style or genre. In this work, we introduce Flexible Dance Generation with Style Description Prompts (DGSDP), a diffusion-based framework suitable for diversified tasks of dance generation by fully leveraging the semantics of music style. The core component of this framework is Music-Conditioned Style-Aware Diffusion (MCSAD), which comprises a Transformer-based network and a music Style Modulation module. The MCSAD seemly integrates music conditions and style description prompts into the dance generation framework, ensuring that generated dances are consistent with the music content and style. To facilitate flexible dance generation and accommodate different tasks, a spatial-temporal masking strategy is effectively applied in the backward diffusion process. The proposed framework successfully generates realistic dance sequences that are accurately aligned with music for a variety of tasks such as long-term generation, dance in-betweening, dance inpainting, and etc. We hope that this work has the potential to inspire dance generation and creation, with promising applications in entertainment, art, and education. Code is available on Github: https://github.com/mucunzhuzhu/DGSDP.

</details>


### [119] [OPTIMUS: Observing Persistent Transformations in Multi-temporal Unlabeled Satellite-data](https://arxiv.org/pdf/2506.13902)
*Raymond Yu, Paul Han, Josh Myers-Dean, Piper Wolters, Favyen Bastani*

Main category: cs.CV

TL;DR: OPTIMUS is a self-supervised learning method for detecting long-lasting changes in satellite imagery, improving AUROC scores significantly over baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of annotated satellite data for rare change categories by leveraging self-supervised learning.

Method: Uses change point detection on model outputs in time series to identify persistent changes.

Result: Achieves an AUROC score improvement from 56.3% to 87.6% for detecting changes.

Conclusion: OPTIMUS effectively detects changes in satellite images without requiring labeled data, offering a scalable solution for environmental monitoring.

Abstract: In the face of pressing environmental issues in the 21st century, monitoring surface changes on Earth is more important than ever. Large-scale remote sensing, such as satellite imagery, is an important tool for this task. However, using supervised methods to detect changes is difficult because of the lack of satellite data annotated with change labels, especially for rare categories of change. Annotation proves challenging due to the sparse occurrence of changes in satellite images. Even within a vast collection of images, only a small fraction may exhibit persistent changes of interest. To address this challenge, we introduce OPTIMUS, a self-supervised learning method based on an intuitive principle: if a model can recover information about the relative order of images in the time series, then that implies that there are long-lasting changes in the images. OPTIMUS demonstrates this principle by using change point detection methods on model outputs in a time series. We demonstrate that OPTIMUS can directly detect interesting changes in satellite images, achieving an improvement in AUROC score from 56.3% to 87.6% at distinguishing changed time series from unchanged ones compared to baselines. Our code and dataset are available at https://huggingface.co/datasets/optimus-change/optimus-dataset/.

</details>


### [120] [A Simple Baseline with Single-encoder for Referring Image Segmentation](https://arxiv.org/pdf/2408.15521)
*Seonghoon Yu, Ilchae Jung, Byeongju Han, Taeoh Kim, Yunho Kim, Dongyoon Wee, Jeany Son*

Main category: cs.CV

TL;DR: A novel RIS method using a single-encoder (BEiT-3) for dense vision-language interactions, outperforming dual-encoder approaches in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Dual-encoders in RIS lack dense multi-modal interactions, creating a gap with pixel-level tasks and increasing computational costs.

Method: Proposes BEiT-3, a single-encoder leveraging shared self-attention, with lightweight decoders (Shared FPN and Shared Mask Decoder) for efficiency.

Result: Achieves outstanding performance on RIS benchmarks while maintaining computational efficiency.

Conclusion: The single-encoder approach with shared self-attention and lightweight decoders bridges the gap in RIS tasks efficiently.

Abstract: Referring image segmentation (RIS) requires dense vision-language interactions between visual pixels and textual words to segment objects based on a given description. However, commonly adapted dual-encoders in RIS, e.g., Swin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal dual-encoder), lack dense multi-modal interactions during pre-training, leading to a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods often rely on multi-modal fusion modules that interact two encoders, but this approach leads to high computational costs. In this paper, we present a novel RIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of shared self-attention across all framework components. This enables seamless interactions of two modalities from input to final prediction, producing granularly aligned multi-modal features. Furthermore, we propose lightweight yet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which contribute to the high efficiency of our model. Our simple baseline with a single encoder achieves outstanding performances on the RIS benchmark datasets while maintaining computational efficiency, compared to the most recent SoTA methods based on dual-encoders.

</details>


### [121] [Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation](https://arxiv.org/pdf/2506.13910)
*Aritra Dutta, Pushpita Boral, G Suseela*

Main category: cs.CV

TL;DR: A machine learning framework for violence detection and classification in video streams using 3D CNNs and bidirectional LSTMs, trained on diverse datasets for improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional surveillance methods fail to promptly detect diverse violent acts, necessitating automated solutions to reduce human and property losses.

Method: Uses 3D CNNs for detection and separable convolutional 3D models with bidirectional LSTMs for classification, trained on annotated datasets from various sources.

Result: Demonstrates improved computational efficiency and accuracy in violence detection and classification.

Conclusion: The proposed framework effectively addresses the need for automated violence detection, offering scalable and efficient solutions for real-world surveillance.

Abstract: The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.

</details>


### [122] [HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment](https://arxiv.org/pdf/2506.13925)
*Numair Nadeem, Saeed Anwar, Muhammad Hamza Asad, Abdul Bais*

Main category: cs.CV

TL;DR: HierVL integrates vision-language models into semi-supervised segmentation, improving performance under severe label scarcity.


<details>
  <summary>Details</summary>
Motivation: Addressing poor generalization and boundary localization in vision-only methods by leveraging domain-invariant semantics from vision-language models.

Method: HierVL combines a Hierarchical Semantic Query Generator, Cross-Modal Spatial Alignment Module, and Dual-Query Transformer Decoder with targeted regularization losses.

Result: Achieves +4.4% mIoU on COCO, +3.1% on Pascal VOC, +5.9% on ADE20, and +1.8% on Cityscapes under 1% supervision.

Conclusion: Language-guided segmentation enhances label efficiency and fine-grained, instance-aware generalization.

Abstract: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.

</details>


### [123] [Mapping Farmed Landscapes from Remote Sensing](https://arxiv.org/pdf/2506.13993)
*Michelangelo Conserva, Alex Wilson, Charlotte Stanton, Vishal Batchu, Varun Gulshan*

Main category: cs.CV

TL;DR: Farmscapes is a high-resolution, large-scale map of rural England, created using deep learning, to aid biodiversity management by identifying key ecological features like hedgerows and woodlands.


<details>
  <summary>Details</summary>
Motivation: The absence of detailed, large-scale ecological maps hampers effective agricultural landscape management and biodiversity conservation efforts.

Method: A deep learning segmentation model was trained on 942 manually annotated aerial imagery tiles to identify landscape features.

Result: The model achieved high accuracy (F1-scores: 96% for woodland, 95% for farmed land, 72% for hedgerows) and the map is now available on Google Earth Engine.

Conclusion: Farmscapes provides a valuable tool for ecologists and policymakers, supporting habitat restoration and biodiversity monitoring.

Abstract: Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\%) and farmed land (95\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.

</details>


### [124] [FindMeIfYouCan: Bringing Open Set metrics to $\textit{near} $, $ \textit{far} $ and $\textit{farther}$ Out-of-Distribution Object Detection](https://arxiv.org/pdf/2506.14008)
*Daniel Montoya, Aymen Bouguerra, Alexandra Gomez-Villa, Fabio Arnez*

Main category: cs.CV

TL;DR: The paper critiques current OOD-OD evaluation protocols, introduces a manually curated benchmark with semantic splits, and evaluates performance using Open Set metrics.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of closed-world OD assumptions, especially for safety-critical applications like autonomous driving and medical imaging, by improving OOD detection.

Method: Manually curate and enrich benchmarks using semantic similarity, creating splits (near, far, farther) and incorporating Open Set metrics for evaluation.

Result: Semantically/visually close OOD objects are easier to localize but more confusable with ID objects; far/farther objects are harder to localize but less confusable.

Conclusion: The proposed benchmark and metrics provide deeper insights into OOD detection performance, highlighting trade-offs between localization and confusion with ID objects.

Abstract: State-of-the-art Object Detection (OD) methods predominantly operate under a closed-world assumption, where test-time categories match those encountered during training. However, detecting and localizing unknown objects is crucial for safety-critical applications in domains such as autonomous driving and medical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a vital research direction for OD, focusing on identifying incorrect predictions typically associated with unknown objects. This paper shows that the current evaluation protocol for OOD-OD violates the assumption of non-overlapping objects with respect to the In-Distribution (ID) datasets, and obscures crucial situations such as ignoring unknown objects, potentially leading to overconfidence in deployment scenarios where truly novel objects might be encountered. To address these limitations, we manually curate, and enrich the existing benchmark by exploiting semantic similarity to create new evaluation splits categorized as $\textit{near}$, $\textit{far}$, and $\textit{farther}$ from ID distributions. Additionally, we incorporate established metrics from the Open Set community, providing deeper insights into how effectively methods detect unknowns, when they ignore them, and when they mistakenly classify OOD objects as ID. Our comprehensive evaluation demonstrates that semantically and visually close OOD objects are easier to localize than far ones, but are also more easily confounded with ID objects. $\textit{Far}$ and $\textit{farther}$ objects are harder to localize but less prone to be taken for an ID object.

</details>


### [125] [HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for Robust Multimodal Hallucination and Factuality Detection in VLMs](https://arxiv.org/pdf/2506.13038)
*Zijian Zhang, Xuecheng Wu, Danlei Huang, Siyu Yan, Chong Peng, Xuezhi Cao*

Main category: cs.CV

TL;DR: The paper introduces HKD4VLM, a progressive hybrid knowledge distillation framework for vision-language models (VLMs), addressing hallucination detection and factuality checking. It outperforms larger VLMs with higher efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve responsible behavior in VLMs by tackling hallucination detection and factuality checking through knowledge distillation, inspired by the effectiveness of smaller distilled models.

Method: Proposes HKD4VLM, combining Pyramid-like Progressive Online Distillation and Ternary-Coupled Refinement Distillation, along with mapping shift-enhanced inference and diverse augmentation.

Result: Extensive experiments show HKD4VLM's effectiveness, with ablation studies highlighting key design choices.

Conclusion: HKD4VLM successfully enhances VLM performance and robustness, offering insights for future research in responsible AI.

Abstract: Driven by the rapid progress in vision-language models (VLMs), the responsible behavior of large-scale multimodal models has become a prominent research area, particularly focusing on hallucination detection and factuality checking. In this paper, we present the solution for the two tracks of Responsible AI challenge. Inspirations from the general domain demonstrate that a smaller distilled VLM can often outperform a larger VLM that is directly tuned on downstream tasks, while achieving higher efficiency. We thus jointly tackle two tasks from the perspective of knowledge distillation and propose a progressive hybrid knowledge distillation framework termed HKD4VLM. Specifically, the overall framework can be decomposed into Pyramid-like Progressive Online Distillation and Ternary-Coupled Refinement Distillation, hierarchically moving from coarse-grained knowledge alignment to fine-grained refinement. Besides, we further introduce the mapping shift-enhanced inference and diverse augmentation strategies to enhance model performance and robustness. Extensive experimental results demonstrate the effectiveness of our HKD4VLM. Ablation studies provide insights into the critical design choices driving performance gains.

</details>


### [126] [Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation](https://arxiv.org/pdf/2506.14015)
*Nick Yiwen Huang, Akin Caliskan, Berkay Kicanaoglu, James Tompkin, Hyeongwoo Kim*

Main category: cs.CV

TL;DR: The paper introduces a method to disentangle 3D control from large vision-language models (LVLMs) for generating 3D portraits with text and geometry control, overcoming noise in LVLM embeddings.


<details>
  <summary>Details</summary>
Motivation: To enable free-form text and 3D geometry control (e.g., age, expression) in 3D portrait generation without requiring large labeled datasets or training large models.

Method: Uses a pre-trained LVLM (CLIP) and a 3D morphable model (FLAME), disentangling via canonicalization and Jacobian regularization to address noise in embeddings.

Result: Produces consistent 3D portraits with controllable text and geometry attributes, outperforming existing methods.

Conclusion: This approach democratizes 3D portrait generation by allowing control over 2D face data without extensive labeling or large model training.

Abstract: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models.

</details>


### [127] [SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement](https://arxiv.org/pdf/2506.14035)
*Chelsi Jain, Yiran Wu, Yifan Zeng, Jiale Liu, S hengyu Dai, Zhenwen Shao, Qingyun Wu, Huazheng Wang*

Main category: cs.CV

TL;DR: SimpleDoc is a lightweight, retrieval-augmented framework for DocVQA that improves evidence retrieval and answer generation by combining embedding similarity and page summaries, outperforming baselines by 3.2%.


<details>
  <summary>Details</summary>
Motivation: DocVQA is challenging due to multi-modality and multi-page references. Existing methods use VLMs but lack efficiency. SimpleDoc aims to enhance retrieval and reasoning with fewer pages.

Method: SimpleDoc uses a dual-cue retriever (embedding similarity and page summaries) and a VLM-based reasoner agent to iteratively gather evidence until the question is answered.

Result: SimpleDoc outperforms baselines by 3.2% on average across 4 DocVQA datasets while retrieving fewer pages.

Conclusion: SimpleDoc is an effective, lightweight solution for DocVQA, improving performance and efficiency.

Abstract: Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.

</details>


### [128] [Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems](https://arxiv.org/pdf/2506.14096)
*Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma*

Main category: cs.CV

TL;DR: A survey on LLM-augmented image segmentation in ITS, covering applications, challenges, and future directions, with a focus on enhancing road scene understanding.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs can improve image segmentation for ITS, addressing safety and efficiency needs.

Method: Systematic review of current approaches, categorized by prompting mechanisms and architectures.

Result: Identifies innovations for autonomous driving, traffic monitoring, and infrastructure maintenance.

Conclusion: Highlights challenges like real-time performance and reliability, advocating for explainable, human-centric AI in future ITS.

Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.

</details>


### [129] [FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution](https://arxiv.org/pdf/2506.14121)
*Siyu Xu, Wenjie Li, Guangwei Gao, Jian Yang, Guo-Jun Qi, Chia-Wen Lin*

Main category: cs.CV

TL;DR: FADPNet, a Frequency-Aware Dual-Path Network, improves face super-resolution by separately processing low- and high-frequency facial features using Mamba and CNN, achieving better efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing FSR methods treat all facial pixels equally, leading to inefficient resource use and degraded performance. CNN excels at high-frequency features, while Mamba captures low-frequency features more efficiently.

Method: FADPNet decomposes facial features into low- and high-frequency components. Low-frequency regions use a Mamba-based LFEB for global interactions, while high-frequency regions employ a CNN-based DPA module and HFR for structural refinement.

Result: The method balances FSR quality and efficiency, outperforming existing approaches.

Conclusion: FADPNet effectively addresses the FSR challenge by leveraging frequency-aware processing, achieving superior results with optimized computational costs.

Abstract: Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches.

</details>


### [130] [KDMOS:Knowledge Distillation for Motion Segmentation](https://arxiv.org/pdf/2506.14130)
*Chunyu Cao, Jintao Cheng, Zeyu Chen, Linfan Zhan, Rui Fan, Zhijian He, Xiaoyu Tang*

Main category: cs.CV

TL;DR: Proposes a logits-based knowledge distillation framework for Motion Object Segmentation (MOS) to balance accuracy and real-time efficiency, achieving notable results.


<details>
  <summary>Details</summary>
Motivation: Enhancing MOS for autonomous driving by improving accuracy while maintaining real-time inference, addressing class imbalance and overfitting.

Method: Uses a BEV projection-based student model and a non-projection teacher model, decouples moving/non-moving classes, applies tailored distillation, dynamic upsampling, and optimizes architecture.

Result: Achieves 78.8% IoU on SemanticKITTI-MOS and competitive results on Apollo dataset, with a 7.69% parameter reduction.

Conclusion: The framework effectively improves MOS performance, balancing accuracy and efficiency, with open-source implementation available.

Abstract: Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.

</details>


### [131] [Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology](https://arxiv.org/pdf/2506.14136)
*Nafiz Sadman, Farhana Zulkernine, Benjamin Kwan*

Main category: cs.CV

TL;DR: The paper analyzes BiomedCLIP's embedding space and its limitations on an imbalanced medical dataset (IU-xray), evaluating zero-shot, full fine-tuning, and linear probing. Results show zero-shot performs poorly, while fine-tuning improves disease classification.


<details>
  <summary>Details</summary>
Motivation: To explore BiomedCLIP's embedding space and quantify its limitations on imbalanced, out-of-distribution medical data for reliable real-world application.

Method: Experiments on IU-xray dataset using zero-shot inference, full fine-tuning, and linear probing, with Grad-CAM heatmaps and radiologist annotations for evaluation.

Result: Zero-shot over-predicts labels with poor precision; fine-tuning improves distinct disease classification; linear probing detects overlapping features.

Conclusion: Careful model adaptation is needed for reliability in real-world medical settings, as demonstrated by the study's findings.

Abstract: In this paper, we construct two research objectives: i) explore the learned embedding space of BiomedCLIP, an open-source large vision language model, to analyse meaningful class separations, and ii) quantify the limitations of BiomedCLIP when applied to a highly imbalanced, out-of-distribution multi-label medical dataset. We experiment on IU-xray dataset, which exhibits the aforementioned criteria, and evaluate BiomedCLIP in classifying images (radiographs) in three contexts: zero-shot inference, full finetuning, and linear probing. The results show that the model under zero-shot settings over-predicts all labels, leading to poor precision and inter-class separability. Full fine-tuning improves classification of distinct diseases, while linear probing detects overlapping features. We demonstrate visual understanding of the model using Grad-CAM heatmaps and compare with 15 annotations by a radiologist. We highlight the need for careful adaptations of the models to foster reliability and applicability in a real-world setting. The code for the experiments in this work is available and maintained on GitHub.

</details>


### [132] [FGA-NN: Film Grain Analysis Neural Network](https://arxiv.org/pdf/2506.14350)
*Zoubida Ameur, Frédéric Lefebvre, Philippe De Lagrange, Miloš Radosavljević*

Main category: cs.CV

TL;DR: FGA-NN is a learning-based method for analyzing and modeling film grain to preserve it during compression, offering accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Film grain is often lost in compression due to its randomness, but preserving it is crucial for artistic intent.

Method: FGA-NN, a learning-based approach, analyzes and models film grain parameters for post-decoding synthesis.

Result: FGA-NN achieves a superior balance between accuracy and complexity, proving robust and applicable.

Conclusion: FGA-NN effectively preserves film grain in compressed content, aligning with artistic goals.

Abstract: Film grain, once a by-product of analog film, is now present in most cinematographic content for aesthetic reasons. However, when such content is compressed at medium to low bitrates, film grain is lost due to its random nature. To preserve artistic intent while compressing efficiently, film grain is analyzed and modeled before encoding and synthesized after decoding. This paper introduces FGA-NN, the first learning-based film grain analysis method to estimate conventional film grain parameters compatible with conventional synthesis. Quantitative and qualitative results demonstrate FGA-NN's superior balance between analysis accuracy and synthesis complexity, along with its robustness and applicability.

</details>


### [133] [RadFabric: Agentic AI System with Reasoning Capability for Radiology](https://arxiv.org/pdf/2506.14142)
*Wenting Chen, Yi Dong, Zhaojun Ding, Yucheng Shi, Yifan Zhou, Fang Zeng, Yijun Luo, Tianyu Lin, Yihang Su, Yichen Wu, Kai Zhang, Zhen Xiang, Tianming Liu, Ninghao Liu, Lichao Sun, Yixuan Yuan, Xiang Li*

Main category: cs.CV

TL;DR: RadFabric is a multimodal framework for CXR analysis, combining visual and textual reasoning to improve diagnostic accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: Current automated CXR systems lack comprehensive pathology coverage, accuracy, and integration of visual-textual reasoning.

Method: RadFabric uses a multi-agent system with specialized agents for pathology detection, anatomical mapping, and reasoning, built on the Model Context Protocol (MCP).

Result: Achieves near-perfect fracture detection (1.000 accuracy) and superior overall diagnostic accuracy (0.799 vs. 0.229-0.527 in traditional systems).

Conclusion: RadFabric advances AI-driven radiology with transparent, precise, and clinically actionable CXR analysis.

Abstract: Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.

</details>


### [134] [Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching](https://arxiv.org/pdf/2506.14605)
*Giacomo Meanti, Thomas Ryckeboer, Michael Arbel, Julien Mairal*

Main category: cs.CV

TL;DR: The paper proposes a method for image restoration using unpaired datasets, leveraging conditional flow matching and distribution-matching loss, outperforming traditional approaches in tasks like deblurring and PSF calibration.


<details>
  <summary>Details</summary>
Motivation: Addressing real-world scenarios where the forward model is unknown or paired data is unavailable, the method aims to simplify image restoration with minimal assumptions.

Method: Uses conditional flow matching to model degraded observations and learns the forward model via a distribution-matching loss, requiring only small, unpaired datasets.

Result: Outperforms single-image blind and unsupervised methods in deblurring and PSF calibration, and matches state-of-the-art in blind super-resolution.

Conclusion: The method is effective for real-world applications, reducing data acquisition effort and outperforming traditional approaches.

Abstract: This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.

</details>


### [135] [SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability](https://arxiv.org/pdf/2506.14144)
*Juho Bai, Inwook Shim*

Main category: cs.CV

TL;DR: SceneAware improves pedestrian trajectory prediction by incorporating scene understanding, outperforming state-of-the-art methods by 50%.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook environmental context, which significantly impacts pedestrian movement. SceneAware addresses this gap.

Method: Uses a Vision Transformer (ViT) for scene encoding and Multi-modal Large Language Models (MLLMs) for walkability masks. Combines these with a Transformer-based trajectory encoder and collision penalties.

Result: Outperforms state-of-the-art methods by over 50% on ETH/UCY datasets, with consistent performance across trajectory types.

Conclusion: SceneAware demonstrates the importance of explicit scene information for accurate, physically plausible trajectory predictions.

Abstract: Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.

</details>


### [136] [VideoMAR: Autoregressive Video Generatio with Continuous Tokens](https://arxiv.org/pdf/2506.14168)
*Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, Feng Zhao*

Main category: cs.CV

TL;DR: VideoMAR is an efficient decoder-only autoregressive model for video generation, combining temporal and spatial masked generation with innovations like next-frame diffusion loss and curriculum learning to reduce costs and errors.


<details>
  <summary>Details</summary>
Motivation: The potential of masked-based autoregressive models for video generation is under-explored, and existing methods face high costs and challenges in long sequence modeling.

Method: Proposes VideoMAR with temporal causality, spatial bi-directionality, next-frame diffusion loss, curriculum learning, progressive resolution training, and 3D rotary embeddings.

Result: Outperforms Cosmos I2V on VBench-I2V with fewer parameters (9.3%), training data (0.5%), and GPU resources (0.2%).

Conclusion: VideoMAR is a concise, efficient, and innovative approach to video generation, replicating language model capabilities while reducing resource demands.

Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

</details>


### [137] [A multi-stage augmented multimodal interaction network for fish feeding intensity quantification](https://arxiv.org/pdf/2506.14170)
*Shulong Zhang, Mingyuan Yao, Jiayin Zhao, Xiao Liu, Haihua Wang*

Main category: cs.CV

TL;DR: The paper proposes MAINet, a multi-stage augmented multimodal interaction network, to improve the accuracy and reliability of fish feeding intensity assessment in aquaculture systems.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing fish feeding intensity have limitations in modality selection, feature extraction, and decision-making, hindering accuracy and reliability.

Method: MAINet includes a general feature extraction framework, an ARPM mechanism for inter-modal interaction, and an ER rule for decision fusion.

Result: MAINet achieves high accuracy (96.76-96.79%) and outperforms single-modality, dual-modality, and other fusion models.

Conclusion: MAINet significantly improves the robustness and accuracy of fish feeding intensity quantification, validated by ablation experiments.

Abstract: In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.

</details>


### [138] [One-Shot Neural Architecture Search with Network Similarity Directed Initialization for Pathological Image Classification](https://arxiv.org/pdf/2506.14176)
*Renao Yan*

Main category: cs.CV

TL;DR: Proposes NSDI for stable NAS and domain adaptation in one-shot NAS to improve pathological image analysis, outperforming existing methods on BRACS.


<details>
  <summary>Details</summary>
Motivation: Addresses inefficiencies in applying general computer vision models to pathological images due to their unique characteristics.

Method: Introduces Network Similarity Directed Initialization (NSDI) and domain adaptation in one-shot NAS.

Result: Outperforms existing methods on BRACS dataset, with better classification and feature localization.

Conclusion: The proposed NSDI and domain adaptation enhance pathological image analysis, offering clinical relevance.

Abstract: Deep learning-based pathological image analysis presents unique challenges due to the practical constraints of network design. Most existing methods apply computer vision models directly to medical tasks, neglecting the distinct characteristics of pathological images. This mismatch often leads to computational inefficiencies, particularly in edge-computing scenarios. To address this, we propose a novel Network Similarity Directed Initialization (NSDI) strategy to improve the stability of neural architecture search (NAS). Furthermore, we introduce domain adaptation into one-shot NAS to better handle variations in staining and semantic scale across pathology datasets. Experiments on the BRACS dataset demonstrate that our method outperforms existing approaches, delivering both superior classification performance and clinically relevant feature localization.

</details>


### [139] [Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition](https://arxiv.org/pdf/2506.14181)
*Yufei Li, Jirui Wu, Long Tian, Liming Wang, Xiaonan Liu, Zijun Liu, Xiyang Liu*

Main category: cs.CV

TL;DR: Meta-SurDiff, a meta-learning-optimized classification diffusion model, addresses uncertainty in surgical videos for reliable online phase recognition by tackling frame ambiguity and unbalanced phase distribution.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in surgical videos (frame ambiguity and unbalanced phase distribution) is often overlooked but crucial for reliable online surgical phase recognition.

Method: Uses a classification diffusion model for finer-grained frame-level confidence assessment and meta-learning to enhance robustness of classification boundaries.

Result: Demonstrated effectiveness on five datasets (Cholec80, AutoLaparo, M2Cai16, OphNet, NurViD) using multiple metrics.

Conclusion: Meta-SurDiff improves online surgical phase recognition by addressing uncertainty, with code to be released upon acceptance.

Abstract: Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical phases.We establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance.

</details>


### [140] [Egocentric Human-Object Interaction Detection: A New Benchmark and Method](https://arxiv.org/pdf/2506.14189)
*Kunyuan Deng, Yi Wang, Lap-Pui Chau*

Main category: cs.CV

TL;DR: The paper introduces Ego-HOIBench, a new dataset for egocentric human-object interaction (Ego-HOI) detection, and proposes a Hand Geometry and Interactivity Refinement (HGIR) scheme to improve detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing HOI detection methods focus on third-person perspectives, neglecting the egocentric view (Ego-HOI). The paper aims to address this gap by creating a dataset and adapting methods for Ego-HOI.

Method: The HGIR scheme uses hand pose and geometric features to refine interaction-specific features, enhancing detection. It is lightweight and plug-and-play compatible with existing HOI baselines.

Result: The proposed HGIR scheme significantly improves Ego-HOI detection, achieving state-of-the-art results on the new Ego-HOIBench dataset.

Conclusion: The Ego-HOIBench dataset and HGIR scheme advance Ego-HOI detection, addressing challenges like hand-occluded objects and complex hand interactions.

Abstract: Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: https://dengkunyuan.github.io/EgoHOIBench/

</details>


### [141] [HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction](https://arxiv.org/pdf/2506.14229)
*Changbai Li, Haodong Zhu, Hanlin Chen, Juan Zhang, Tongfei Chen, Shuo Yang, Shuwei Shao, Wenhao Dong, Baochang Zhang*

Main category: cs.CV

TL;DR: HRGS improves 3DGS by introducing hierarchical block-level optimization and Gaussian pruning for memory-efficient, high-resolution 3D scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: Address memory scalability issues in 3DGS for high-resolution scenarios.

Method: Hierarchical block-level optimization, Gaussian partitioning, training data partitioning, Importance-Driven Gaussian Pruning (IDGP), and normal priors.

Result: Achieves state-of-the-art performance in high-resolution NVS and surface reconstruction.

Conclusion: HRGS enables high-quality, high-resolution 3D scene reconstruction under memory constraints.

Abstract: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.

</details>


### [142] [Exploring Linear Attention Alternative for Single Image Super-Resolution](https://arxiv.org/pdf/2502.00404)
*Rongchang Lu, Changyu Li, Donghang Li, Guojing Zhang, Jianqiang Huang, Xilai Li*

Main category: cs.CV

TL;DR: The paper introduces OmniRWKVSR, a deep learning model for single-image super-resolution (SISR), combining RWKV architecture with feature extraction techniques to improve computational efficiency and image quality.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in computational complexity and quality in SISR, especially for remote sensing images.

Method: Proposes OmniRWKVSR, integrating RWKV architecture with VRSM and VRCM for feature extraction.

Result: Achieves 0.26% PSNR and 0.16% SSIM improvement over MambaIR in 4x super-resolution tasks.

Conclusion: OmniRWKVSR effectively enhances SISR performance, offering high-quality image reconstruction.

Abstract: Deep learning-based single-image super-resolution (SISR) technology focuses on enhancing low-resolution (LR) images into high-resolution (HR) ones. Although significant progress has been made, challenges remain in computational complexity and quality, particularly in remote sensing image processing. To address these issues, we propose our Omni-Scale RWKV Super-Resolution (OmniRWKVSR) model which presents a novel approach that combines the Receptance Weighted Key Value (RWKV) architecture with feature extraction techniques such as Visual RWKV Spatial Mixing (VRSM) and Visual RWKV Channel Mixing (VRCM), aiming to overcome the limitations of existing methods and achieve superior SISR performance. This work has proved able to provide effective solutions for high-quality image reconstruction. Under the 4x Super-Resolution tasks, compared to the MambaIR model, we achieved an average improvement of 0.26% in PSNR and 0.16% in SSIM.

</details>


### [143] [Unified Representation Space for 3D Visual Grounding](https://arxiv.org/pdf/2506.14238)
*Yinuo Zheng, Lipeng Gu, Honghua Chen, Liangliang Nan, Mingqiang Wei*

Main category: cs.CV

TL;DR: UniSpace-3D introduces a unified representation space for 3D visual grounding, bridging vision-text gaps with a CLIP-based encoder, contrastive learning, and language-guided query selection, outperforming baselines by 2.24%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3DVG use separate vision-text encoders, causing spatial and semantic gaps, leading to errors in object positioning and classification.

Method: UniSpace-3D uses a unified representation encoder (CLIP-based), multi-modal contrastive learning, and language-guided query selection to align visual and textual features.

Result: UniSpace-3D outperforms baselines by at least 2.24% on ScanRefer and Nr3D/Sr3D datasets.

Conclusion: The proposed method effectively bridges the vision-text gap in 3DVG, improving accuracy and performance.

Abstract: 3D visual grounding (3DVG) is a critical task in scene understanding that aims to identify objects in 3D scenes based on text descriptions. However, existing methods rely on separately pre-trained vision and text encoders, resulting in a significant gap between the two modalities in terms of spatial geometry and semantic categories. This discrepancy often causes errors in object positioning and classification. The paper proposes UniSpace-3D, which innovatively introduces a unified representation space for 3DVG, effectively bridging the gap between visual and textual features. Specifically, UniSpace-3D incorporates three innovative designs: i) a unified representation encoder that leverages the pre-trained CLIP model to map visual and textual features into a unified representation space, effectively bridging the gap between the two modalities; ii) a multi-modal contrastive learning module that further reduces the modality gap; iii) a language-guided query selection module that utilizes the positional and semantic information to identify object candidate points aligned with textual descriptions. Extensive experiments demonstrate that UniSpace-3D outperforms baseline models by at least 2.24% on the ScanRefer and Nr3D/Sr3D datasets. The code will be made available upon acceptance of the paper.

</details>


### [144] [Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition](https://arxiv.org/pdf/2506.14243)
*Xiaohui Jiang, Haijiang Zhu, Chadei Li, Fulin Tang, Ning An*

Main category: cs.CV

TL;DR: A novel LiDAR-based place recognition framework addresses density inconsistency and representation fragility by using density-agnostic geometric reasoning and implicit 3D representations, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Overcome challenges in LiDAR-based place recognition caused by inconsistent point cloud density and fragile geometric representations in complex scenarios.

Method: Proposes an implicit 3D representation with elastic points, derives occupancy grid and normal vector information, and fuses descriptors from bird's-eye and 3D segment views.

Result: Achieves state-of-the-art performance on datasets like KITTI, KITTI-360, MulRan, and NCLT, balancing accuracy, runtime, and memory optimization.

Conclusion: The framework excels in resilience and scalability, with plans for future open-sourcing.

Abstract: LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future.

</details>


### [145] [synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?](https://arxiv.org/pdf/2506.14255)
*Johannes Flotzinger, Fabian Deuser, Achref Jaziri, Heiko Neumann, Norbert Oswald, Visvanathan Ramesh, Thomas Braml*

Main category: cs.CV

TL;DR: The paper introduces 'synth-dacl', synthetic dataset extensions to improve model performance in bridge defect segmentation, addressing class imbalance in the dacl10k dataset.


<details>
  <summary>Details</summary>
Motivation: Challenges in bridge inspection due to resource constraints and the need for robust automated defect classification.

Method: Creation of synthetic dataset extensions (synth-dacl) to balance class distribution in dacl10k, enhancing model training.

Result: Improved model robustness, with a 2% increase in mean IoU, F1 score, Recall, and Precision on perturbed test sets.

Conclusion: Synth-dacl effectively enhances model performance for fine-grained defect segmentation in bridge inspections.

Abstract: Adequate bridge inspection is increasingly challenging in many countries due to growing ailing stocks, compounded with a lack of staff and financial resources. Automating the key task of visual bridge inspection, classification of defects and building components on pixel level, improves efficiency, increases accuracy and enhances safety in the inspection process and resulting building assessment. Models overtaking this task must cope with an assortment of real-world conditions. They must be robust to variations in image quality, as well as background texture, as defects often appear on surfaces of diverse texture and degree of weathering. dacl10k is the largest and most diverse dataset for real-world concrete bridge inspections. However, the dataset exhibits class imbalance, which leads to notably poor model performance particularly when segmenting fine-grained classes such as cracks and cavities. This work introduces "synth-dacl", a compilation of three novel dataset extensions based on synthetic concrete textures. These extensions are designed to balance class distribution in dacl10k and enhance model performance, especially for crack and cavity segmentation. When incorporating the synth-dacl extensions, we observe substantial improvements in model robustness across 15 perturbed test sets. Notably, on the perturbed test set, a model trained on dacl10k combined with all synthetic extensions achieves a 2% increase in mean IoU, F1 score, Recall, and Precision compared to the same model trained solely on dacl10k.

</details>


### [146] [Comparison of Two Methods for Stationary Incident Detection Based on Background Image](https://arxiv.org/pdf/2506.14256)
*Deepak Ghimire, Joonwhoan Lee*

Main category: cs.CV

TL;DR: The paper proposes two background subtraction-based schemes for detecting temporarily stationary objects, comparing their performance and complexity, and uses NCC for tracking.


<details>
  <summary>Details</summary>
Motivation: To improve detection of temporarily stationary objects in visual tracking, addressing challenges like occlusion and illumination changes.

Method: Two schemes: single background and dual backgrounds with different learning rates, followed by NCC-based image comparison for tracking.

Result: The method is robust to partial/full occlusion, illumination changes, and operates in real time.

Conclusion: The proposed approach effectively detects and tracks stationary objects under various conditions.

Abstract: In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time.

</details>


### [147] [Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling](https://arxiv.org/pdf/2506.14265)
*Siran Dai, Qianqian Xu, Peisong Wen, Yang Liu, Qingming Huang*

Main category: cs.CV

TL;DR: SSLProfiler is a non-contrastive SSL framework for cell profiling, addressing challenges in view generation and multi-image representation, winning the CVPR 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: To create a generalizable feature extractor for cell images using SSL, overcoming distribution differences and multi-image representation issues.

Method: Proposes SSLProfiler with specialized data augmentation and post-processing for cell images.

Result: SSLProfiler achieved robust feature extraction, winning the CVPR 2025 Cell Line Transferability challenge.

Conclusion: SSLProfiler effectively adapts SSL for cell profiling, demonstrating superior performance in real-world challenges.

Abstract: Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025.

</details>


### [148] [Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment](https://arxiv.org/pdf/2506.14271)
*Weiming Zhang, Dingwen Xiao, Aobotao Dai, Yexin Liu, Tianbo Pan, Shiqi Wen, Lei Chen, Lin Wang*

Main category: cs.CV

TL;DR: Leader360V is the first large-scale labeled 360 video dataset for instance segmentation and tracking, featuring an automated annotation pipeline combining 2D segmentors and LLMs.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale labeled 360 video datasets due to spherical distortions and annotation complexity hinders 360 scene understanding tasks.

Method: A three-stage automated labeling pipeline: Initial Annotation (Semantic- and Distortion-aware Refinement), Auto-Refine (correcting missing regions), and Manual Revision (LLM and human validation).

Result: Leader360V improves model performance for 360 video segmentation and tracking, validated by user studies.

Conclusion: Leader360V addresses the dataset gap and advances scalable 360 scene understanding.

Abstract: 360 video captures the complete surrounding scenes with the ultra-large field of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation and tracking, crucial for appications, such as autonomous driving, robotics. With the recent emergence of foundation models, the community is, however, impeded by the lack of large-scale, labelled real-world datasets. This is caused by the inherent spherical properties, eg, severe distortion in polar regions, and content discontinuities, rendering the annotation costly yet complex. This paper introduces Leader360V, the first large-scale, labeled real-world 360 video datasets for instance segmentation and tracking. Our datasets enjoy high scene diversity, ranging from indoor and urban settings to natural and dynamic outdoor scenes. To automate annotation, we design an automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors and large language models to facilitate the labeling. The pipeline operates in three novel stages. Specifically, in the Initial Annotation Phase, we introduce a Semantic- and Distortion-aware Refinement module, which combines object mask proposals from multiple 2D segmentors with LLM-verified semantic labels. These are then converted into mask prompts to guide SAM2 in generating distortion-aware masks for subsequent frames. In the Auto-Refine Annotation Phase, missing or incomplete regions are corrected either by applying the SDR again or resolving the discontinuities near the horizontal borders. The Manual Revision Phase finally incorporates LLMs and human annotators to further refine and validate the annotations. Extensive user studies and evaluations demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments confirm that Leader360V significantly enhances model performance for 360 video segmentation and tracking, paving the way for more scalable 360 scene understanding.

</details>


### [149] [FRIDU: Functional Map Refinement with Guided Image Diffusion](https://arxiv.org/pdf/2506.14322)
*Avigail Cohen Rimon, Mirela Ben-Chen, Or Litany*

Main category: cs.CV

TL;DR: A novel method refines shape correspondence maps using an image diffusion model trained in functional map space, achieving competitive accuracy with state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing correspondence maps between shapes can be inaccurate; refining them efficiently and accurately is a challenge.

Method: Train an image diffusion model in functional map space, using pointwise maps as guidance during diffusion to enforce objectives like orthogonality.

Result: The approach is competitive with state-of-the-art map refinement methods.

Conclusion: Guided diffusion models offer a promising direction for functional map processing.

Abstract: We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.

</details>


### [150] [EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization](https://arxiv.org/pdf/2506.14356)
*Xiaoqi Wang, Yi Wang, Lap-Pui Chau*

Main category: cs.CV

TL;DR: EVA02-AT introduces efficient video-language models for egocentric tasks, addressing pre-training costs, spatial-temporal encoding, and learning objectives with innovations like joint attention and SMS loss.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from high pre-training costs, ineffective spatial-temporal encoding, and imprecise learning objectives in egocentric video-language tasks.

Method: EVA02-AT uses single-stage pretraining, spatial-temporal rotary positional embeddings with joint attention, and the Symmetric Multi-Similarity (SMS) loss for multi-instance retrieval.

Result: EVA02-AT achieves state-of-the-art performance on Ego4D, EPIC-Kitchens-100, and Charades-Ego with fewer parameters and significant gains in retrieval tasks.

Conclusion: EVA02-AT offers a scalable and precise solution for egocentric video-language understanding, with publicly available code and models.

Abstract: Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .

</details>


### [151] [HydroChronos: Forecasting Decades of Surface Water Change](https://arxiv.org/pdf/2506.14362)
*Daniele Rege Cambrin, Eleonora Poeta, Eliana Pastor, Isaac Corley, Tania Cerquitelli, Elena Baralis, Paolo Garza*

Main category: cs.CV

TL;DR: HydroChronos is a large-scale dataset for surface water dynamics forecasting, paired with a novel model, AquaClimaTempo UNet, which outperforms baselines and includes Explainable AI insights.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of comprehensive datasets and standardized benchmarks in forecasting surface water dynamics for water resource management and climate change adaptation.

Method: Introduces HydroChronos dataset with multi-modal data (Landsat 5, Sentinel-2, climate data, DEMs) and proposes AquaClimaTempo UNet, a spatiotemporal model with a climate data branch.

Result: Model outperforms Persistence baseline by +14% and +11% F1 in tasks, and +0.1 MAE in regression. Explainable AI identifies key climate variables.

Conclusion: HydroChronos and AquaClimaTempo UNet provide a robust benchmark and insights for future modeling in surface water dynamics forecasting.

Abstract: Forecasting surface water dynamics is crucial for water resource management and climate change adaptation. However, the field lacks comprehensive datasets and standardized benchmarks. In this paper, we introduce HydroChronos, a large-scale, multi-modal spatiotemporal dataset for surface water dynamics forecasting designed to address this gap. We couple the dataset with three forecasting tasks. The dataset includes over three decades of aligned Landsat 5 and Sentinel-2 imagery, climate data, and Digital Elevation Models for diverse lakes and rivers across Europe, North America, and South America. We also propose AquaClimaTempo UNet, a novel spatiotemporal architecture with a dedicated climate data branch, as a strong benchmark baseline. Our model significantly outperforms a Persistence baseline for forecasting future water dynamics by +14% and +11% F1 across change detection and direction of change classification tasks, and by +0.1 MAE on the magnitude of change regression. Finally, we conduct an Explainable AI analysis to identify the key climate variables and input channels that influence surface water change, providing insights to inform and guide future modeling efforts.

</details>


### [152] [DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI](https://arxiv.org/pdf/2506.14367)
*Sumshun Nahar Eity, Mahin Montasir Afif, Tanisha Fairooz, Md. Mortuza Ahmmed, Md Saef Ullah Miah*

Main category: cs.CV

TL;DR: DGG-XNet, a hybrid deep learning model combining VGG16 and DenseNet121, improves brain disorder diagnosis with 91.33% accuracy, outperforming conventional MRI analysis.


<details>
  <summary>Details</summary>
Motivation: Accurate diagnosis of brain disorders like Alzheimer's and tumors is challenging with manual MRI methods, which are inefficient and error-prone.

Method: DGG-XNet integrates VGG16 and DenseNet121 for enhanced feature extraction and classification, using Grad-CAM for interpretability.

Result: Achieved 91.33% test accuracy, with precision, recall, and F1-score all above 91% on BraTS 2021 and Kaggle datasets.

Conclusion: DGG-XNet is a promising, interpretable tool for computer-aided diagnosis of brain disorders.

Abstract: Accurate diagnosis of brain disorders such as Alzheimer's disease and brain tumors remains a critical challenge in medical imaging. Conventional methods based on manual MRI analysis are often inefficient and error-prone. To address this, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and DenseNet121 to enhance feature extraction and classification. DenseNet121 promotes feature reuse and efficient gradient flow through dense connectivity, while VGG16 contributes strong hierarchical spatial representations. Their fusion enables robust multiclass classification of neurological conditions. Grad-CAM is applied to visualize salient regions, enhancing model transparency. Trained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a test accuracy of 91.33\%, with precision, recall, and F1-score all exceeding 91\%. These results highlight DGG-XNet's potential as an effective and interpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and oncological brain disorders.

</details>


### [153] [Discrete JEPA: Learning Discrete Token Representations without Reconstruction](https://arxiv.org/pdf/2506.14373)
*Junyeob Baek, Hosung Lee, Christopher Hoang, Mengye Ren, Sungjin Ahn*

Main category: cs.CV

TL;DR: Discrete-JEPA improves symbolic reasoning in AI by enhancing image tokenization with semantic tokenization and complementary objectives, outperforming baselines in visual symbolic prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Current image tokenization methods lack symbolic abstraction and logical reasoning capabilities needed for systematic inference.

Method: Extends latent predictive coding with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning.

Result: Outperforms baselines in visual symbolic prediction tasks and shows emergence of systematic patterns in learned token space.

Conclusion: Discrete-JEPA has significant potential to advance symbolic world modeling and planning in AI systems.

Abstract: The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.

</details>


### [154] [DepthSeg: Depth prompting in remote sensing semantic segmentation](https://arxiv.org/pdf/2506.14382)
*Ning Zhou, Shanxiong Chen, Mingting Zhou, Haigang Sui, Lieyun Hu, Han Li, Li Hua, Qiming Zhou*

Main category: cs.CV

TL;DR: The paper introduces DepthSeg, a depth-prompting 2D remote sensing semantic segmentation framework, to address misclassification caused by spectral confusion and shadow occlusion by integrating depth/height information.


<details>
  <summary>Details</summary>
Motivation: Existing semantic segmentation methods focus on spectral characteristics but ignore elevation differences, leading to misclassification in complex scenarios like shadow occlusion and spectral confusion.

Method: DepthSeg models depth/height from 2D images using a lightweight adapter for fine-tuning and a depth prompter for explicit feature modeling, coupled with a semantic classification decoder.

Result: Experiments on the LiuZhou dataset show DepthSeg's effectiveness in land cover mapping, with ablation studies confirming the importance of depth prompts.

Conclusion: DepthSeg improves semantic segmentation accuracy by integrating depth information, addressing limitations of spectral-only methods.

Abstract: Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.

</details>


### [155] [GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion](https://arxiv.org/pdf/2506.14384)
*Huan Kang, Hui Li, Xiao-Jun Wu, Tianyang Xu, Rui Wang, Chunyang Cheng, Josef Kittler*

Main category: cs.CV

TL;DR: The paper introduces GrFormer, a novel attention mechanism using Grassmann manifold for infrared and visible image fusion, improving semantic and detail balance.


<details>
  <summary>Details</summary>
Motivation: Euclidean methods fail to capture non-Euclidean data structures, leading to poor fusion performance. The goal is to balance low-level details and high-level semantics.

Method: GrFormer uses Grassmann manifold for subspace mapping, decoupling features into high-frequency details and low-frequency semantics. A cross-modal fusion strategy (CMS) is also developed.

Result: GrFormer outperforms state-of-the-art methods in qualitative and quantitative benchmarks.

Conclusion: The proposed method effectively addresses fusion challenges by leveraging Grassmann manifold and CMS, achieving superior performance.

Abstract: In the field of image fusion, promising progress has been made by modeling data from different modalities as linear subspaces.
  However, in practice, the source images are often located in a non-Euclidean space, where the Euclidean methods usually cannot
  encapsulate the intrinsic topological structure. Typically, the inner product performed in the Euclidean space calculates the algebraic
  similarity rather than the semantic similarity, which results in undesired attention output and a decrease in fusion performance.
  While the balance of low-level details and high-level semantics should be considered in infrared and visible image fusion task. To
  address this issue, in this paper, we propose a novel attention mechanism based on Grassmann manifold for infrared and visible
  image fusion (GrFormer). Specifically, our method constructs a low-rank subspace mapping through projection constraints on the
  Grassmann manifold, compressing attention features into subspaces of varying rank levels. This forces the features to decouple into
  high-frequency details (local low-rank) and low-frequency semantics (global low-rank), thereby achieving multi-scale semantic
  fusion. Additionally, to effectively integrate the significant information, we develop a cross-modal fusion strategy (CMS) based on
  a covariance mask to maximise the complementary properties between different modalities and to suppress the features with high
  correlation, which are deemed redundant. The experimental results demonstrate that our network outperforms SOTA methods both
  qualitatively and quantitatively on multiple image fusion benchmarks. The codes are available at https://github.com/Shaoyun2023.

</details>


### [156] [Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models](https://arxiv.org/pdf/2506.14399)
*Tian Xia, Fabio De Sousa Ribeiro, Rajat R Rasal, Avinash Kori, Raghav Mehta, Ben Glocker*

Main category: cs.CV

TL;DR: The paper introduces Decoupled Classifier-Free Guidance (DCFG) to improve counterfactual image generation by addressing attribute amplification issues in standard CFG.


<details>
  <summary>Details</summary>
Motivation: Standard CFG's single global weight for conditioning variables leads to poor identity preservation and spurious attribute changes (attribute amplification).

Method: Proposes DCFG, a model-agnostic framework with group-wise conditioning control, using an attribute-split embedding strategy to disentangle semantic inputs.

Result: Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show DCFG improves intervention fidelity, reduces unintended changes, and enhances reversibility.

Conclusion: DCFG enables more faithful and interpretable counterfactual image generation by selectively guiding attribute groups.

Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.

</details>


### [157] [Causally Steered Diffusion for Automated Video Counterfactual Generation](https://arxiv.org/pdf/2506.14404)
*Nikos Spyrou, Athanasios Vlontzos, Paraskevas Pegios, Thomas Melistas, Nefeli Gkouti, Yannis Panagakis, Giorgos Papanastasiou, Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: A framework for causally faithful counterfactual video generation using text-to-image latent diffusion models, guided by a vision-language model, without needing internal access or finetuning.


<details>
  <summary>Details</summary>
Motivation: Challenges in maintaining causal relationships in video content when using T2I models for editing, risking unrealistic outcomes.

Method: Optimizes text prompts based on a causal graph to guide generation, agnostic to the underlying video editing system.

Result: Demonstrates effective generation of causally faithful video counterfactuals within LDM distributions.

Conclusion: The method's compatibility with black-box systems offers potential for realistic "what-if" scenarios in fields like healthcare and digital media.

Abstract: Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.

</details>


### [158] [Compositional Attribute Imbalance in Vision Datasets](https://arxiv.org/pdf/2506.14418)
*Jiayi Chen, Yanbiao Ma, Andi Zhang, Weidong Tang, Wei Dai, Bowei Liu*

Main category: cs.CV

TL;DR: A CLIP-based framework addresses visual attribute imbalance in image classification by adjusting sampling probabilities and integrating data augmentation, improving model robustness and fairness.


<details>
  <summary>Details</summary>
Motivation: Visual attribute imbalance in image classification is underexplored but significantly impacts model performance and generalization.

Method: Define image attributes, construct a visual attribute dictionary using CLIP, analyze attribute imbalance, and adjust sampling probabilities with data augmentation techniques like CutMix, Fmix, and SaliencyMix.

Result: The method effectively mitigates attribute imbalance, enhancing model robustness and fairness on benchmark datasets.

Conclusion: The research underscores the importance of modeling visual attribute distributions and offers a scalable solution for long-tail image classification.

Abstract: Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.

</details>


### [159] [Toward Rich Video Human-Motion2D Generation](https://arxiv.org/pdf/2506.14428)
*Ruihao Xi, Xuekuan Wang, Yongcheng Li, Shuhua Li, Zichen Wang, Yiwei Wang, Feng Wei, Cairong Zhao*

Main category: cs.CV

TL;DR: The paper introduces Motion2D-Video-150K, a large-scale dataset for human motion, and proposes RVHM2D, a diffusion-based model for generating realistic and controllable 2D human motions, especially for multi-character interactions.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in generating realistic and controllable human motions, particularly for multi-character interactions, due to data scarcity and modeling complexities.

Method: The authors introduce Motion2D-Video-150K dataset and propose RVHM2D, a diffusion-based model with enhanced textual conditioning and a two-stage training strategy (diffusion objective followed by reinforcement learning).

Result: RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark for single and double-character motion generation.

Conclusion: The proposed dataset and model effectively address the challenges of generating realistic and interactive human motions.

Abstract: Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.

</details>


### [160] [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models](https://arxiv.org/pdf/2506.14435)
*Hongyu Wang, Jiayu Xu, Ruiping Wang, Yan Feng, Yitao Zhai, Peng Pei, Xunliang Cai, Xilin Chen*

Main category: cs.CV

TL;DR: MoTE trains ternary experts for memory-efficient multimodal MoEs, matching full-precision performance with lower memory footprint.


<details>
  <summary>Details</summary>
Motivation: Address high memory usage of full-precision experts in MoEs for edge deployment.

Method: Train ternary experts (-1, 0, 1) using pre-trained FFN as shared expert, reducing memory.

Result: MoTE matches MoE-LLaVA performance with 4.3% higher accuracy under memory constraints.

Conclusion: MoTE is effective for memory-constrained devices, scalable, and compatible with quantization.

Abstract: Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.

</details>


### [161] [Model compression using knowledge distillation with integrated gradients](https://arxiv.org/pdf/2506.14440)
*David E. Hernandez, Jose Chang, Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: A novel method enhances knowledge distillation using integrated gradients (IG) for model compression, achieving higher accuracy and faster inference.


<details>
  <summary>Details</summary>
Motivation: To improve deep learning model deployment on resource-constrained devices by enhancing knowledge distillation with deeper insights into teacher models.

Method: Overlays IG maps onto input images during training, precomputing IG maps to reduce runtime costs.

Result: Achieves 92.6% accuracy (1.1% improvement) with 4.1x compression and reduces inference time from 140 ms to 13 ms.

Conclusion: IG-based knowledge distillation outperforms conventional methods, offering a viable solution for edge device deployment.

Abstract: Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.

</details>


### [162] [Adapting Lightweight Vision Language Models for Radiological Visual Question Answering](https://arxiv.org/pdf/2506.14451)
*Aditya Shourya, Michel Dumontier, Chang Sun*

Main category: cs.CV

TL;DR: A lightweight 3B parameter vision-language model is fine-tuned for Radiological VQA, achieving robust performance with curated data and a cost-effective training pipeline.


<details>
  <summary>Details</summary>
Motivation: Challenges in radiological VQA include limited expert-labeled data, complex image patterns, and lack of evaluation efforts.

Method: Fine-tuning a small model with synthetic QA pairs and multi-stage fine-tuning on radiological datasets (e.g., ROCO v2.0, MedPix v2.0).

Result: The model performs well despite its small size and limited data, comparable to larger models like LLaVA-Med.

Conclusion: Lightweight models with curated data can be effective for radiological VQA, aided by a diagnostic tool for performance inspection.

Abstract: Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.

</details>


### [163] [Dense360: Dense Understanding from Omnidirectional Panoramas](https://arxiv.org/pdf/2506.14471)
*Yikang Zhou, Tao Zhang, Dizhe Zhang, Shunping Ji, Xiangtai Li, Lu Qi*

Main category: cs.CV

TL;DR: The paper introduces a dataset and benchmark for dense visual-language understanding in omnidirectional panoramas, addressing challenges with ERP-RoPE position encoding.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack dense understanding of omnidirectional panoramas, which offer more complete and continuous scene representations than limited FOV inputs.

Method: The authors introduce a dataset with 160K panoramas and annotations, and propose ERP-RoPE for handling ERP-specific challenges in MLLMs.

Result: The dataset includes 5M captions, 1M referring expressions, and 100K scene descriptions, with Dense360-Bench established for evaluation.

Conclusion: The work advances dense visual-language understanding in panoramic settings, addressing key challenges and providing a benchmark for future research.

Abstract: Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capabilities through limited field-of-view (FOV) visual inputs (e.g., 70 degree), we take the first step toward dense understanding from omnidirectional panoramas. We first introduce an omnidirectional panoramas dataset featuring a comprehensive suite of reliability-scored annotations. Specifically, our dataset contains 160K panoramas with 5M dense entity-level captions, 1M unique referring expressions, and 100K entity-grounded panoramic scene descriptions. Compared to multi-view alternatives, panoramas can provide more complete, compact, and continuous scene representations through equirectangular projections (ERP). However, the use of ERP introduces two key challenges for MLLMs: i) spatial continuity along the circle of latitude, and ii) latitude-dependent variation in information density. We address these challenges through ERP-RoPE, a position encoding scheme specifically designed for panoramic ERP. In addition, we introduce Dense360-Bench, the first benchmark for evaluating MLLMs on omnidirectional captioning and grounding, establishing a comprehensive framework for advancing dense visual-language understanding in panoramic settings.

</details>


### [164] [Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection](https://arxiv.org/pdf/2506.14473)
*Zhijing Wan, Zhixiang Wang, Zheng Wang, Xin Xu, Shin'ichi Satoh*

Main category: cs.CV

TL;DR: FM-based subset selection outperforms traditional IEs on fine-grained datasets, with RAM-APL enhancing performance by leveraging multiple FMs.


<details>
  <summary>Details</summary>
Motivation: To reduce deep learning training costs by improving subset selection methods, especially for fine-grained datasets.

Method: Proposes RAM-APL, leveraging multiple foundation models (FMs) for subset selection, focusing on fine-grained datasets.

Result: RAM-APL achieves state-of-the-art performance on fine-grained datasets like Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.

Conclusion: FM-based subset selection, particularly with RAM-APL, is highly effective for fine-grained datasets, outperforming traditional methods.

Abstract: One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.

</details>


### [165] [I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs](https://arxiv.org/pdf/2506.14495)
*Yu Qi, Lipeng Gu, Honghua Chen, Liangliang Nan, Mingqiang Wei*

Main category: cs.CV

TL;DR: SpeechRefer enhances 3D visual grounding by addressing noisy speech-to-text transcriptions with acoustic and contrastive learning modules.


<details>
  <summary>Details</summary>
Motivation: Existing 3DVG methods rely on precise text, but real-world speech inputs are often noisy due to accents, noise, and speech variations, limiting their effectiveness.

Method: SpeechRefer introduces a Speech Complementary Module for acoustic similarity and a Contrastive Complementary Module for aligning erroneous text with speech features.

Result: Experiments show SpeechRefer significantly improves existing 3DVG methods, handling noisy speech inputs effectively.

Conclusion: SpeechRefer bridges the gap between noisy speech and reliable 3DVG, enabling more intuitive multimodal systems.

Abstract: Existing 3D visual grounding methods rely on precise text prompts to locate objects within 3D scenes. Speech, as a natural and intuitive modality, offers a promising alternative. Real-world speech inputs, however, often suffer from transcription errors due to accents, background noise, and varying speech rates, limiting the applicability of existing 3DVG methods. To address these challenges, we propose \textbf{SpeechRefer}, a novel 3DVG framework designed to enhance performance in the presence of noisy and ambiguous speech-to-text transcriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and introduces two key innovations. First, the Speech Complementary Module captures acoustic similarities between phonetically related words and highlights subtle distinctions, generating complementary proposal scores from the speech signal. This reduces dependence on potentially erroneous transcriptions. Second, the Contrastive Complementary Module employs contrastive learning to align erroneous text features with corresponding speech features, ensuring robust performance even when transcription errors dominate. Extensive experiments on the SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves the performance of existing 3DVG methods by a large margin, which highlights SpeechRefer's potential to bridge the gap between noisy speech inputs and reliable 3DVG, enabling more intuitive and practical multimodal systems.

</details>


### [166] [MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution](https://arxiv.org/pdf/2506.14511)
*Zhiwen Shao, Yifan Cheng, Feiran Li, Yong Zhou, Xuequan Lu, Yuan Xie, Lizhuang Ma*

Main category: cs.CV

TL;DR: Proposes an end-to-end micro-action-aware deep learning framework combining transformer, graph convolution, and vanilla convolution for facial micro-expression recognition (MER), outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing MER methods rely on hand-crafted features or key frames, and face limitations due to small, low-diversity datasets. The paper aims to overcome these by leveraging deep learning without prior key frame knowledge.

Method: Introduces the F5C block (fully-connected and channel correspondence convolution) to extract local-global features from raw frames. Combines transformer-style and graph-style convolutions, and jointly trains MER with optical flow estimation and facial landmark detection.

Result: Outperforms state-of-the-art MER methods on benchmarks (CASME II, SAMM, SMIC), excels in optical flow and landmark detection, and captures subtle facial muscle actions.

Conclusion: The framework effectively addresses MER challenges by integrating multiple convolution techniques and auxiliary tasks, demonstrating superior performance and robustness.

Abstract: Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available at https://github.com/CYF-cuber/MOL.

</details>


### [167] [SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks](https://arxiv.org/pdf/2506.14512)
*Zijian Song, Xiaoxin Lin, Qiuming Huang, Guangrun Wang, Liang Lin*

Main category: cs.CV

TL;DR: SIRI-Bench is a new benchmark for evaluating Vision-Language Models (VLMs) on spatial reasoning tasks using video-based questions in realistic 3D scenes.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack systematic evaluation for spatial reasoning, despite its importance in real-world interactions.

Method: Developed SIRI-Bench with 1K video-question-answer triplets and an Automatic Scene Creation Engine using LLM agents to generate 3D scenes from math problems.

Result: State-of-the-art VLMs perform poorly on SIRI-Bench, highlighting the challenge of spatial reasoning.

Conclusion: The study aims to draw attention to spatially grounded reasoning and improve VLMs' visual problem-solving abilities.

Abstract: Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving.

</details>


### [168] [VisLanding: Monocular 3D Perception for UAV Safe Landing via Depth-Normal Synergy](https://arxiv.org/pdf/2506.14525)
*Zhuoyue Tan, Boyong He, Yuxiang Ji, Liaoni Wu*

Main category: cs.CV

TL;DR: VisLanding is a monocular 3D perception framework for safe UAV landing, using depth-normal synergy from Metric3D V2 to estimate safe landing zones via binary segmentation, validated on WildUAV and cross-domain datasets.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous UAV landing in complex, unknown environments by accurately identifying safe landing zones.

Method: Leverages Metric3D V2 for depth-normal synergy, transforms landing zone estimation into binary segmentation, fine-tuned on WildUAV dataset, and validated with cross-domain testing.

Result: VisLanding improves safe zone accuracy through depth-normal joint optimization and shows robust generalization in cross-domain tests.

Conclusion: The framework effectively estimates landing zones, supports decision-making, and outperforms other methods in generalization and robustness.

Abstract: This paper presents VisLanding, a monocular 3D perception-based framework for safe UAV (Unmanned Aerial Vehicle) landing. Addressing the core challenge of autonomous UAV landing in complex and unknown environments, this study innovatively leverages the depth-normal synergy prediction capabilities of the Metric3D V2 model to construct an end-to-end safe landing zones (SLZ) estimation framework. By introducing a safe zone segmentation branch, we transform the landing zone estimation task into a binary semantic segmentation problem. The model is fine-tuned and annotated using the WildUAV dataset from a UAV perspective, while a cross-domain evaluation dataset is constructed to validate the model's robustness. Experimental results demonstrate that VisLanding significantly enhances the accuracy of safe zone identification through a depth-normal joint optimization mechanism, while retaining the zero-shot generalization advantages of Metric3D V2. The proposed method exhibits superior generalization and robustness in cross-domain testing compared to other approaches. Furthermore, it enables the estimation of landing zone area by integrating predicted depth and normal information, providing critical decision-making support for practical applications.

</details>


### [169] [Exploring Diffusion with Test-Time Training on Efficient Image Restoration](https://arxiv.org/pdf/2506.14541)
*Rongchang Lu, Tianduo Luo, Yunzhi Zhang, Conghan Yue, Pei Yang, Guibao Liu, Changyang Gu*

Main category: cs.CV

TL;DR: DiffRWKVIR introduces a novel framework combining Test-Time Training with efficient diffusion for image restoration, addressing feature fusion, computational bottlenecks, and diffusion inefficiency.


<details>
  <summary>Details</summary>
Motivation: Challenges in image restoration include ineffective feature fusion, computational bottlenecks, and inefficient diffusion processes.

Method: Proposes three innovations: Omni-Scale 2D State Evolution for global awareness, Chunk-Optimized Flash Processing for parallelism, and Prior-Guided Efficient Diffusion for compact representation.

Result: Outperforms SwinIR, HAT, and MambaIR/v2 in benchmarks (Set5, Set14, etc.) in PSNR, SSIM, LPIPS, and efficiency.

Conclusion: Establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.

Abstract: Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.

</details>


### [170] [DreamLight: Towards Harmonious and Consistent Image Relighting](https://arxiv.org/pdf/2506.14549)
*Yong Liu, Wenpeng Xiao, Qianqian Wang, Junlin Chen, Shiyin Wang, Yitong Wang, Xinglong Wu, Yansong Tang*

Main category: cs.CV

TL;DR: DreamLight is a universal image relighting model that composites subjects into new backgrounds with aesthetic lighting and color uniformity, supporting both image-based and text-based relighting.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on image-based relighting or rely on complex pipelines, struggling with realistic light interactions. DreamLight addresses these gaps by unifying input data and leveraging semantic priors.

Method: The model uses a Position-Guided Light Adapter (PGLA) to condense light information and modulate the foreground, along with a Spectral Foreground Fixer (SFF) for appearance consistency.

Result: DreamLight achieves remarkable relighting performance, outperforming existing methods in generating natural light interactions.

Conclusion: DreamLight effectively addresses challenges in universal image relighting, offering improved realism and consistency.

Abstract: We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.

</details>


### [171] [Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images](https://arxiv.org/pdf/2506.14560)
*David Butler, Adrian Hilton, Gustavo Carneiro*

Main category: cs.CV

TL;DR: A new interpretable ML method improves knee OA risk estimation by 2% AUC and speeds up inference by 9%, using multi-task predictive modeling and diffusion models for future image generation.


<details>
  <summary>Details</summary>
Motivation: Clinical adoption of ML for knee OA risk estimation is limited due to lack of interpretability and impracticality of existing methods.

Method: Multi-task predictive modeling combines future knee OA severity classification and anatomical landmark prediction, using a diffusion model for efficient future image generation.

Result: Achieves 0.71 AUC (2% improvement over SOTA) and ~9% faster inference on the Osteoarthritis Initiative dataset.

Conclusion: The method enhances interpretability and practicality for knee OA risk estimation, advancing clinical adoption.

Abstract: Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.

</details>


### [172] [Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images](https://arxiv.org/pdf/2506.14583)
*Krishna Sahukara, Zineddine Bettouche, Andreas Fischer*

Main category: cs.CV

TL;DR: An automated LaTeX-based pipeline generates synthetic two-column pages with diverse table layouts to augment the Marmot benchmark, improving TableNet's performance with reduced manual effort.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of tables from document pages is slow and error-prone, necessitating an automated solution.

Method: A LaTeX-based pipeline synthesizes realistic two-column pages with diverse table layouts and aligned ground-truth masks for training TableNet.

Result: TableNet achieves 4.04% XOR error on synthetic data (256x256) and 9.18% on Marmot benchmark, reducing manual annotation.

Conclusion: The synthetic data pipeline effectively enhances TableNet's performance and reduces reliance on manual annotation.

Abstract: Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.

</details>


### [173] [PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation](https://arxiv.org/pdf/2506.14596)
*Ming Xu, Xu Zhang*

Main category: cs.CV

TL;DR: PoseGRAF improves monocular 3D pose estimation by modeling joint and bone dependencies using dual graph convolutions, cross-attention, and dynamic fusion, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore directional and angular correlations in skeletons, leading to implausible poses under occlusions or motion changes.

Method: Uses dual graph convolutions for joint and bone graphs, a cross-attention module, dynamic fusion, and an improved Transformer encoder.

Result: Outperforms state-of-the-art on Human3.6M and MPI-INF-3DHP datasets, with validated generalizability on in-the-wild videos.

Conclusion: PoseGRAF effectively addresses limitations of existing methods by leveraging joint-bone dependencies, demonstrating superior performance and generalizability.

Abstract: Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.

</details>


### [174] [Align Your Flow: Scaling Continuous-Time Flow Map Distillation](https://arxiv.org/pdf/2506.14603)
*Amirmojtaba Sabour, Sanja Fidler, Karsten Kreis*

Main category: cs.CV

TL;DR: The paper introduces flow maps for efficient generative modeling, outperforming diffusion- and flow-based models in few-step generation, with novel training techniques and validation on image benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of diffusion- and flow-based models requiring many sampling steps, and the performance degradation of consistency models with increased steps.

Method: Proposes two continuous-time objectives for training flow maps, autoguidance for distillation, and adversarial finetuning. Validates on ImageNet benchmarks.

Result: Achieves state-of-the-art few-step generation on ImageNet 64x64 and 512x512, and outperforms non-adversarial text-to-image samplers.

Conclusion: Flow maps (Align Your Flow) offer efficient, high-performance generative modeling, validated across benchmarks and text-to-image tasks.

Abstract: Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.

</details>


### [175] [VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning](https://arxiv.org/pdf/2506.14629)
*Md. Adnanul Islam, Md. Faiyaz Abdullah Sayeedi, Md. Asaduzzaman Shuvo, Muhammad Ziaur Rahman, Shahanur Rahman Bappy, Raiyan Rahman, Swakkhar Shatabda*

Main category: cs.CV

TL;DR: VisText-Mosquito is a multimodal dataset combining visual and textual data for automated mosquito breeding site analysis, achieving high precision in detection, segmentation, and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To address the global health risk of mosquito-borne diseases by enabling early detection and proactive control of breeding sites using AI.

Method: The dataset includes annotated images for object detection and segmentation, plus natural language reasoning texts. Models like YOLOv9s and YOLOv11n-Seg are used for detection and segmentation, while a fine-tuned BLIP model handles reasoning.

Result: High precision scores in detection (0.92926 mAP@50) and segmentation (0.91587 mAP@50), with strong reasoning performance (BLEU 54.7, BERTScore 0.91).

Conclusion: The dataset and models demonstrate effective AI-based proactive prevention of mosquito-borne diseases, aligning with the theme 'Prevention is Better than Cure'.

Abstract: Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito

</details>


### [176] [3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting](https://arxiv.org/pdf/2506.14642)
*Yuke Xing, Jiarui Wang, Peizhi Niu, Wenjie Huang, Guangtao Zhai, Yiling Xu*

Main category: cs.CV

TL;DR: The paper introduces 3DGS-IEval-15K, a large-scale dataset for evaluating the perceptual impact of compressed 3D Gaussian Splatting (3DGS) representations, addressing the lack of a comprehensive framework for such assessments.


<details>
  <summary>Details</summary>
Motivation: 3DGS offers high-fidelity real-time rendering but faces storage challenges. Existing methods lack a framework to evaluate the perceptual impact of compression.

Method: The dataset includes 15,200 images from 10 scenes, rendered using 6 3DGS algorithms at 20 viewpoints, with varying compression levels. Subjective experiments collected human perception data from 60 viewers.

Result: The dataset's quality is validated through scene diversity and MOS analysis. A benchmark with 30 IQA metrics is established.

Conclusion: 3DGS-IEval-15K is the largest 3DGS quality assessment dataset, enabling specialized IQA metric development and investigation of view-dependent quality patterns.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.

</details>


### [177] [DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification](https://arxiv.org/pdf/2506.14667)
*Matt Poyser, Toby P. Breckon*

Main category: cs.CV

TL;DR: DDS-NAS speeds up NAS training by 27x using dynamic hard example mining and curriculum learning, without performance loss.


<details>
  <summary>Details</summary>
Motivation: Address scalability challenges in Neural Architecture Search (NAS) by improving training efficiency.

Method: Uses an autoencoder for image similarity embedding, constructs a kd-tree for efficient dissimilarity search, and dynamically updates subsample datasets via curriculum learning.

Result: Achieves up to 27x speedup in gradient-based NAS without performance degradation.

Conclusion: DDS-NAS reduces training time and iterations by maximizing each image's contribution, enhancing NAS scalability.

Abstract: In order to address the scalability challenge within Neural Architecture Search (NAS), we speed up NAS training via dynamic hard example mining within a curriculum learning framework. By utilizing an autoencoder that enforces an image similarity embedding in latent space, we construct an efficient kd-tree structure to order images by furthest neighbour dissimilarity in a low-dimensional embedding. From a given query image from our subsample dataset, we can identify the most dissimilar image within the global dataset in logarithmic time. Via curriculum learning, we then dynamically re-formulate an unbiased subsample dataset for NAS optimisation, upon which the current NAS solution architecture performs poorly. We show that our DDS-NAS framework speeds up gradient-based NAS strategies by up to 27x without loss in performance. By maximising the contribution of each image sample during training, we reduce the duration of a NAS training cycle and the number of iterations required for convergence.

</details>


### [178] [Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models](https://arxiv.org/pdf/2506.14674)
*Ling Li, Yao Zhou, Yuxuan Liang, Fugee Tsung, Jiaheng Wei*

Main category: cs.CV

TL;DR: The paper introduces GLOBE, a novel pipeline for geo-localization using reasoning-driven tasks and diverse social media images, outperforming existing methods in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing geo-localization methods lack interpretability and rely on limited datasets. The rise of LVLMs allows for reasoning-driven tasks, but challenges in data diversity and model reasoning persist.

Method: Proposes MP16-Reason dataset and GLOBE, a pipeline combining group-relative policy optimization for locatability assessment and visual-clue reasoning, with task-specific rewards.

Result: GLOBE outperforms state-of-the-art LVLMs in geo-localization, especially in diverse scenes, and provides more interpretable reasoning.

Conclusion: GLOBE addresses data and modeling challenges, improving geo-localization accuracy and interpretability through reasoning-driven tasks.

Abstract: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vision-language models (LVLMs) has enabled a rethinking of geo-localization as a reasoning-driven task grounded in visual cues. However, two major challenges persist. On the data side, existing reasoning-focused datasets are primarily based on street-view imagery, offering limited scene diversity and constrained viewpoints. On the modeling side, current approaches predominantly rely on supervised fine-tuning, which yields only marginal improvements in reasoning capabilities. To address these challenges, we propose a novel pipeline that constructs a reasoning-oriented geo-localization dataset, MP16-Reason, using diverse social media images. We introduce GLOBE, Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning, yielding Bi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE incorporates task-specific rewards that jointly enhance locatability assessment, visual clue reasoning, and geolocation accuracy. Both qualitative and quantitative results demonstrate that GLOBE outperforms state-of-the-art open-source LVLMs on geo-localization tasks, particularly in diverse visual scenes, while also generating more insightful and interpretable reasoning trajectories.

</details>


### [179] [FocalClick-XL: Towards Unified and High-quality Interactive Segmentation](https://arxiv.org/pdf/2506.14686)
*Xi Chen, Hengshuang Zhao*

Main category: cs.CV

TL;DR: FocalClick-XL extends FocalClick's coarse-to-fine design, introducing a multi-level subnet pipeline for interactive segmentation, achieving state-of-the-art performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing interactive segmentation methods lack flexibility and struggle with fine details, prompting the need for a more versatile and effective approach.

Method: Proposes FocalClick-XL, a pipeline with dedicated subnets for context, object, and detail levels, leveraging large-scale pretraining and a prompting layer for interaction-specific encoding.

Result: Achieves top performance on click-based benchmarks and adapts well to diverse interaction forms (boxes, scribbles, masks), also predicting alpha mattes.

Conclusion: FocalClick-XL is a powerful, flexible tool for interactive segmentation, excelling in detail capture and adaptability.

Abstract: Interactive segmentation enables users to extract binary masks of target objects through simple interactions such as clicks, scribbles, and boxes. However, existing methods often support only limited interaction forms and struggle to capture fine details. In this paper, we revisit the classical coarse-to-fine design of FocalClick and introduce significant extensions. Inspired by its multi-stage strategy, we propose a novel pipeline, FocalClick-XL, to address these challenges simultaneously. Following the emerging trend of large-scale pretraining, we decompose interactive segmentation into meta-tasks that capture different levels of information -- context, object, and detail -- assigning a dedicated subnet to each level.This decomposition allows each subnet to undergo scaled pretraining with independent data and supervision, maximizing its effectiveness. To enhance flexibility, we share context- and detail-level information across different interaction forms as common knowledge while introducing a prompting layer at the object level to encode specific interaction types. As a result, FocalClick-XL achieves state-of-the-art performance on click-based benchmarks and demonstrates remarkable adaptability to diverse interaction formats, including boxes, scribbles, and coarse masks. Beyond binary mask generation, it is also capable of predicting alpha mattes with fine-grained details, making it a versatile and powerful tool for interactive segmentation.

</details>


### [180] [YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework](https://arxiv.org/pdf/2506.14696)
*Dahang Wan, Rongsheng Lu, Yang Fang, Xianli Lang, Shuangbao Shu, Jingjing Chen, Siyuan Shen, Ting Xu, Zecong Ye*

Main category: cs.CV

TL;DR: YOLOv11-RGBT is a new multimodal object detection framework addressing challenges like lack of unified single-stage design and modality weight issues. It introduces six fusion modes, a P3 mid-fusion strategy, and MCF, improving performance on datasets like LLVIP and FLIR.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with unified frameworks, performance-fusion balance, and modality weight allocation, limiting multispectral object detection's potential.

Method: Based on YOLOv11, the framework includes six fusion modes, a P3 mid-fusion strategy, and MCF for optimizing feature fusion and reducing redundancy.

Result: The framework achieves significant improvements, e.g., 3.41%-5.65% mAP boost on FLIR, reaching 47.61%, demonstrating enhanced adaptability and robustness.

Conclusion: YOLOv11-RGBT effectively addresses key challenges in multispectral object detection, offering a versatile and high-performing solution.

Abstract: Multispectral object detection, which integrates information from multiple bands, can enhance detection accuracy and environmental adaptability, holding great application potential across various fields. Although existing methods have made progress in cross-modal interaction, low-light conditions, and model lightweight, there are still challenges like the lack of a unified single-stage framework, difficulty in balancing performance and fusion strategy, and unreasonable modality weight allocation. To address these, based on the YOLOv11 framework, we present YOLOv11-RGBT, a new comprehensive multimodal object detection framework. We designed six multispectral fusion modes and successfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After reevaluating the importance of the two modalities, we proposed a P3 mid-fusion strategy and multispectral controllable fine-tuning (MCF) strategy for multispectral models. These improvements optimize feature fusion, reduce redundancy and mismatches, and boost overall model performance. Experiments show our framework excels on three major open-source multispectral object detection datasets, like LLVIP and FLIR. Particularly, the multispectral controllable fine-tuning strategy significantly enhanced model adaptability and robustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP by 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and strategies' effectiveness. The code is available at: https://github.com/wandahangFY/YOLOv11-RGBT.

</details>


### [181] [Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion](https://arxiv.org/pdf/2506.14706)
*Ni Ou, Zhuo Chen, Xinru Zhang, Junzheng Wang*

Main category: cs.CV

TL;DR: A versatile iterative framework using surrogate diffusion improves extrinsic calibration accuracy for camera-LiDAR fusion in autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end calibration methods lack iterative optimization, limiting accuracy.

Method: Proposes an iterative refinement framework using surrogate diffusion, where initial extrinsic parameters are refined through a denoising process.

Result: Integration with the diffusion model enhances accuracy, robustness, and stability across four state-of-the-art calibration methods.

Conclusion: The framework outperforms single-step and other iterative approaches, offering a flexible solution for higher calibration accuracy.

Abstract: Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.

</details>


### [182] [DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning](https://arxiv.org/pdf/2506.14709)
*Kunal Swami, Debtanu Gupta, Amrit Kumar Muduli, Chirag Jaiswal, Pankaj Kumar Bajpai*

Main category: cs.CV

TL;DR: DiFuse-Net is a novel network for depth estimation using RGB and dual-pixel (DP) data, featuring a unique attention mechanism and cross-modal learning, outperforming baselines and introducing a new dataset.


<details>
  <summary>Details</summary>
Motivation: Traditional depth sensors have limitations; DP technology in modern cameras offers a cost-effective alternative for depth estimation.

Method: DiFuse-Net uses a window bi-directional parallax attention mechanism (WBiPAM) for DP disparity cues, a separate RGB encoder, and cross-modal transfer learning (CmTL) to leverage existing RGB-D datasets.

Result: DiFuse-Net outperforms DP and stereo-based baseline methods.

Conclusion: The paper presents an effective solution for depth estimation using DP data, introduces a new dataset, and demonstrates superior performance.

Abstract: Depth estimation is crucial for intelligent systems, enabling applications from autonomous navigation to augmented reality. While traditional stereo and active depth sensors have limitations in cost, power, and robustness, dual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling alternative. This paper introduces DiFuse-Net, a novel modality decoupled network design for disentangled RGB and DP based depth estimation. DiFuse-Net features a window bi-directional parallax attention mechanism (WBiPAM) specifically designed to capture the subtle DP disparity cues unique to smartphone cameras with small aperture. A separate encoder extracts contextual information from the RGB image, and these features are fused to enhance depth prediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to utilize large-scale RGB-D datasets in the literature to cope with the limitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and comparison of the proposed method demonstrates its superiority over the DP and stereo-based baseline methods. Additionally, we contribute a new, high-quality, real-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP) dataset, created using our novel symmetric stereo camera hardware setup, stereo calibration and rectification protocol, and AI stereo disparity estimation method.

</details>


### [183] [ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM](https://arxiv.org/pdf/2506.14766)
*Yujun Wang, Jinhe Bi, Yunpu Ma, Soeren Pirk*

Main category: cs.CV

TL;DR: The paper proposes an attention-steerable contrastive decoding framework to mitigate hallucinations in Multimodal Large Language Models (MLLMs) by directly intervening in attention mechanisms, improving performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: MLLMs often suffer from hallucinations due to over-reliance on partial cues, and existing methods like VCD and ICD may not address the root cause.

Method: The authors introduce an attention-steerable contrastive decoding framework that directly intervenes in the model's attention mechanisms.

Result: The approach significantly reduces hallucinations and improves performance on benchmarks like POPE, CHAIR, and MMHal-Bench, while also enhancing standard VQA performance.

Conclusion: The proposed framework offers a principled solution to mitigate hallucinations in MLLMs by addressing attention dynamics.

Abstract: Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.

</details>


### [184] [Active InSAR monitoring of building damage in Gaza during the Israel-Hamas War](https://arxiv.org/pdf/2506.14730)
*Corey Scher, Jamon Van Den Hoek*

Main category: cs.CV

TL;DR: The paper introduces a long temporal-arc coherent change detection (LT-CCD) method using SAR data to monitor urban damage in Gaza during the 2023 Israel-Hamas War, achieving high accuracy and revealing dynamic damage trends.


<details>
  <summary>Details</summary>
Motivation: The need for active monitoring of urban damage in geographically dynamic and protracted conflicts like the Gaza Strip, where traditional methods are limited.

Method: Uses interferometric SAR data from Sentinel-1 and applies LT-CCD to track weekly damage trends over the first year of the conflict.

Result: Detects 92.5% of damage labels with a 1.2% false positive rate, revealing rapid damage increase, ceasefire pauses, and shifting conflict hotspots. 191,263 buildings were damaged or destroyed.

Conclusion: The LT-CCD approach provides low-cost, timely damage data for humanitarian and journalistic use in conflict zones.

Abstract: Aerial bombardment of the Gaza Strip beginning October 7, 2023 is one of the most intense bombing campaigns of the twenty-first century, driving widespread urban damage. Characterizing damage over a geographically dynamic and protracted armed conflict requires active monitoring. Synthetic aperture radar (SAR) has precedence for mapping disaster-induced damage with bi-temporal methods but applications to active monitoring during sustained crises are limited. Using interferometric SAR data from Sentinel-1, we apply a long temporal-arc coherent change detection (LT-CCD) approach to track weekly damage trends over the first year of the 2023- Israel-Hamas War. We detect 92.5% of damage labels in reference data from the United Nations with a negligible (1.2%) false positive rate. The temporal fidelity of our approach reveals rapidly increasing damage during the first three months of the war focused in northern Gaza, a notable pause in damage during a temporary ceasefire, and surges of new damage as conflict hot-spots shift from north to south. Three-fifths (191,263) of all buildings are damaged or destroyed by the end of the study. With massive need for timely data on damage in armed conflict zones, our low-cost and low-latency approach enables rapid uptake of damage information at humanitarian and journalistic organizations.

</details>


### [185] [SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting](https://arxiv.org/pdf/2506.14742)
*Ziqiao Peng, Wentao Hu, Junyuan Ma, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Hui Tian, Jun He, Hongyan Liu, Zhaoxin Fan*

Main category: cs.CV

TL;DR: SyncTalk++ improves synchronization in speech-driven talking head videos using dynamic rendering, facial alignment, and head stabilization, achieving high realism and speed.


<details>
  <summary>Details</summary>
Motivation: The challenge of achieving synchronization in talking head videos, where lack of coordination leads to unrealistic results, drives the need for SyncTalk++.

Method: SyncTalk++ employs a Dynamic Portrait Renderer, Face-Sync Controller, Head-Sync Stabilizer, Expression Generator, and Torso Restorer to ensure identity, lip, expression, and head pose synchronization.

Result: The method achieves 101 FPS, outperforms state-of-the-art in synchronization and realism, and handles OOD audio robustly.

Conclusion: SyncTalk++ effectively addresses synchronization challenges, enhancing realism and speed in talking head video synthesis.

Abstract: Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.

</details>


### [186] [Cost-Aware Routing for Efficient Text-To-Image Generation](https://arxiv.org/pdf/2506.14753)
*Qinchan, Li, Kenneth Chen, Changyue, Su, Wittawat Jitkrittum, Qi Sun, Patsorn Sangkloy*

Main category: cs.CV

TL;DR: A framework optimizes computational cost and quality in diffusion models by routing prompts to the most suitable text-to-image generation function based on complexity.


<details>
  <summary>Details</summary>
Motivation: High computational cost of diffusion models due to sequential denoising, despite high fidelity, prompts the need for a balanced approach.

Method: Automatically routes prompts to appropriate generation functions (varying denoising steps or distinct models) based on complexity.

Result: Achieves higher average quality than individual models by reserving expensive options for complex prompts and economical ones for simpler ones.

Conclusion: The framework effectively balances quality and computational cost, outperforming uniform cost reduction techniques.

Abstract: Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.

</details>


### [187] [Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset](https://arxiv.org/pdf/2506.14765)
*Nikolaos Dionelis, Jente Bosmans, Riccardo Musto, Giancarlo Paoletti, Simone Sarti, Giacomo Cascarano, Casper Fibaek, Luke Camilleri, Bertrand Le Saux, Nicolas Longépé*

Main category: cs.CV

TL;DR: The paper explores scaling up the PhilEO Geo-Aware U-Net foundation model using large unlabeled EO datasets (MajorTOM and FastTOM) and evaluates its performance on downstream tasks like road density estimation and land cover segmentation.


<details>
  <summary>Details</summary>
Motivation: To leverage the massive volumes of EO data (e.g., Copernicus Sentinel-2) by pretraining foundation models on large unlabeled datasets for efficient fine-tuning with minimal labeled data.

Method: Scaling up the PhilEO model on the 23TB MajorTOM and 2TB FastTOM datasets, developing variants with different parameters and architectures (U-Net CNN to ViT), and fine-tuning on PhilEO Bench for tasks like road density estimation.

Result: The PhilEO 44M MajorTOM 23TB model outperforms others for road density regression, while PhilEO 200M FastTOM excels in road density estimation and building density regression. Dataset and model scaling are validated as effective.

Conclusion: The study demonstrates the effectiveness of scaling datasets and models for EO tasks, with architecture transitions (CNN to ViT) also explored.

Abstract: Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures. Finally, we fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT).

</details>


### [188] [CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion](https://arxiv.org/pdf/2506.14769)
*Jiahua Ma, Yiran Qin, Yixiong Li, Xuanqi Liao, Yulan Guo, Ruimao Zhang*

Main category: cs.CV

TL;DR: CDP improves robot learning by using historical action sequences for context-aware policy learning, reducing computational costs with a caching mechanism, and maintaining robustness under degraded conditions.


<details>
  <summary>Details</summary>
Motivation: Hardware limitations and real-time constraints degrade data quality and restrict model inference, reducing the efficacy of learning from expert demonstrations.

Method: Proposes Causal Diffusion Policy (CDP), a transformer-based diffusion model that conditions on historical action sequences and uses a caching mechanism to reduce redundant computations.

Result: CDP achieves higher accuracy than existing methods in diverse 2D/3D tasks and maintains precision under degraded input quality.

Conclusion: CDP is robust and effective for robotic control under realistic, imperfect conditions.

Abstract: Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.

</details>


### [189] [Maximizing Information in Domain-Invariant Representation Improves Transfer Learning](https://arxiv.org/pdf/2306.00262)
*Adrian Shuai Li, Elisa Bertino, Xuan-Hong Dang, Ankush Singla, Yuhai Tu, Mark N Wegman*

Main category: cs.CV

TL;DR: A new domain adaptation (DA) algorithm enforces a stronger constraint to minimize domain-dependent representation (DDRep) information, improving invariant representation and outperforming existing methods like DSN.


<details>
  <summary>Details</summary>
Motivation: Current DA methods, such as DSN, inadequately address the issue of classification-relevant information hiding in DDRep due to weak constraints.

Method: Develops a stronger constraint to minimize DDRep information, ensuring domain-independent representation (DIRep) retains target-relevant information.

Result: Outperforms DSN and other DA methods on benchmark datasets, showing robustness and better performance.

Conclusion: The new algorithm improves DA performance, is compatible with pre-trained models, and is versatile for real-world applications like network intrusion detection.

Abstract: The most effective domain adaptation (DA) technique involves the decomposition of data representation into a domain-independent representation (DIRep) and a domain-dependent representation (DDRep). A classifier is trained by using the DIRep on the labeled source images. Since the DIRep is domain invariant, the classifier can be "transferred" to make predictions for the target domain with no (or few) labels. However, information useful for classification in the target domain can "hide" in the DDRep. Current DA algorithms, such as Domain-Separation Networks (DSN), do not adequately address this issue. DSN's weak constraint to enforce the orthogonality of DIRep and DDRep allows this hiding effect and can result in poor performance. To address this shortcoming, we develop a new algorithm wherein a stronger constraint is imposed to minimize the information content in DDRep to create a DIRep that retains relevant information about the target labels and, in turn, results in a better invariant representation. By using synthetic datasets, we show explicitly that depending on the initialization, DSN, with its weaker constraint, can lead to sub-optimal solutions with poorer DA performance. In contrast, our algorithm is robust against such perturbations. We demonstrate the equal-or-better performance of our approach against DSN and other recent DA methods by using several standard benchmark image datasets. We further highlight the compatibility of our algorithm with pre-trained models for classifying real-world images and showcase its adaptability and versatility through its application in network intrusion detection.

</details>


### [190] [Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures](https://arxiv.org/pdf/2307.15220)
*Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Jacques Marescaux, Pietro Mascagni, Nassir Navab, Nicolas Padoy*

Main category: cs.CV

TL;DR: SurgVLP introduces a multi-modal representation learning method using surgical video lectures for vision-language pre-training, reducing reliance on manual annotations and improving adaptability.


<details>
  <summary>Details</summary>
Motivation: Current vision-only models lack language integration and rely on manual annotations, limiting generalizability. Surgical video lectures offer untapped vision-language supervisory signals.

Method: Uses automatic speech recognition for text transcriptions and proposes SurgVLP for multi-modal representation learning.

Result: SurgVLP shows strong transferability and adaptability in surgical video analysis, with potential as a foundation model for workflow analysis.

Conclusion: SurgVLP reduces manual annotation dependency, supports few-shot learning, and is scalable for diverse surgical applications.

Abstract: Recent advancements in surgical computer vision applications have been driven by vision-only models, which do not explicitly integrate the rich semantics of language into their design. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective vision and language supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. Extensive experiments across diverse surgical procedures and tasks demonstrate that the multi-modal representations learned by SurgVLP exhibit strong transferability and adaptability in surgical video analysis. Furthermore, our zero-shot evaluations highlight SurgVLP's potential as a general-purpose foundation model for surgical workflow analysis, reducing the reliance on extensive manual annotations for downstream tasks, and facilitating adaptation methods such as few-shot learning to build a scalable and data-efficient solution for various downstream surgical applications. The [training code](https://github.com/CAMMA-public/PeskaVLP) and [weights](https://github.com/CAMMA-public/SurgVLP) are public.

</details>


### [191] [InkSight: Offline-to-Online Handwriting Conversion by Teaching Vision-Language Models to Read and Write](https://arxiv.org/pdf/2402.05804)
*Blagoj Mitrevski, Arina Rak, Julian Schnitzler, Chengkun Li, Andrii Maksai, Jesse Berent, Claudiu Musat*

Main category: cs.CV

TL;DR: InkSight bridges the gap between physical and digital note-taking by converting offline handwriting to digital ink (derendering) using a novel approach combining reading and writing priors.


<details>
  <summary>Details</summary>
Motivation: Digital note-taking lacks the ease of traditional pen-and-paper methods. InkSight aims to make digital conversion seamless for physical note-takers.

Method: Combines reading and writing priors to train a model without needing large paired datasets, addressing generalization issues in prior work.

Result: Effectively derenders handwritten text in arbitrary photos, generalizing beyond training domains to simple sketches. Human evaluation shows 87% validity and 67% human-like quality.

Conclusion: InkSight successfully bridges the gap between physical and digital note-taking, offering a robust and generalizable solution for derendering handwritten notes.

Abstract: Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in a vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice that is still favored by a vast majority. Our work InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore, it generalizes beyond its training domain into simple sketches. Our human evaluation reveals that 87% of the samples produced by our model on the challenging HierText dataset are considered as a valid tracing of the input image and 67% look like a pen trajectory traced by a human.

</details>


### [192] [Unified Source-Free Domain Adaptation](https://arxiv.org/pdf/2403.07601)
*Song Tang, Wenxin Su, Mao Ye, Jianwei Zhang, Xiatian Zhu*

Main category: cs.CV

TL;DR: The paper introduces CausalDA, a novel approach for unified Source-Free Domain Adaptation (SFDA) that addresses multiple scenarios by uncovering latent causal relationships, leveraging CLIP for knowledge integration, and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing SFDA methods are limited to specific scenarios and require prior knowledge of the target domain, reducing practicality. The paper aims to unify these scenarios and enhance model robustness against domain shifts.

Method: CausalDA formulates SFDA from a causality perspective, discovering latent causal factors using a pre-trained vision-language model (CLIP) and a new information bottleneck with theoretical guarantees.

Result: CausalDA achieves state-of-the-art performance in various SFDA settings and source-free out-of-distribution generalization.

Conclusion: The proposed CausalDA offers a unified, practical solution for SFDA by leveraging causality and pre-trained models, demonstrating superior performance across diverse scenarios.

Abstract: In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including Closed-set, Open-set, Partial-set, and Generalized settings. Existing methods, focusing on specific scenarios, not only address a limited subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability. In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner. In this paper, we propose a novel approach latent Causal factors discovery for unified SFDA(CausalDA). In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate CausalDA from a causality perspective. The objective is to uncover the causal relationships between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts. To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP. This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees. Extensive experiments demonstrate that CausalDA can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization.

</details>


### [193] [ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with Probability Map Guided Multi-Format Feature Fusion](https://arxiv.org/pdf/2405.05164)
*Bing Zhu, Zixin He, Weiyi Xiong, Guanhua Ding, Tao Huang, Wei Chen, Wei Xiang*

Main category: cs.CV

TL;DR: ProbRadarM3F improves mmWave radar-based pose estimation by fusing traditional heatmap features with positional encoding, achieving 69.9% AP on HuPR dataset.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of mmWave radar in pose estimation by leveraging unexplored positional information in radar signals.

Method: Introduces ProbRadarM3F, a framework combining FFT and probability map-based positional encoding for feature fusion.

Result: Achieves 69.9% AP on HuPR dataset, outperforming existing methods.

Conclusion: Demonstrates the potential of exploiting non-redundant information in mmWave radar for improved pose estimation.

Abstract: Millimeter wave (mmWave) radar is a non-intrusive privacy and relatively convenient and inexpensive device, which has been demonstrated to be applicable in place of RGB cameras in human indoor pose estimation tasks. However, mmWave radar relies on the collection of reflected signals from the target, and the radar signals containing information is difficult to be fully applied. This has been a long-standing hindrance to the improvement of pose estimation accuracy. To address this major challenge, this paper introduces a probability map guided multi-format feature fusion model, ProbRadarM3F. This is a novel radar feature extraction framework using a traditional FFT method in parallel with a probability map based positional encoding method. ProbRadarM3F fuses the traditional heatmap features and the positional features, then effectively achieves the estimation of 14 keypoints of the human body. Experimental evaluation on the HuPR dataset proves the effectiveness of the model proposed in this paper, outperforming other methods experimented on this dataset with an AP of 69.9 %. The emphasis of our study is focusing on the position information that is not exploited before in radar singal. This provides direction to investigate other potential non-redundant information from mmWave rader.

</details>


### [194] [A Survey on Personalized Content Synthesis with Diffusion Models](https://arxiv.org/pdf/2405.05538)
*Xulu Zhang, Xiaoyong Wei, Wentao Hu, Jinlin Wu, Jiaxin Wu, Wengyu Zhang, Zhaoxiang Zhang, Zhen Lei, Qing Li*

Main category: cs.CV

TL;DR: A survey of Personalized Content Synthesis (PCS) categorizing methods into test-time fine-tuning (TTF) and pre-trained adaptation (PTA), analyzing their strengths, limitations, and key techniques, while addressing challenges like overfitting and fidelity-alignment trade-offs.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of PCS methods (over 150 in two years) lacks up-to-date surveys, especially beyond text-to-image generation, prompting this comprehensive review.

Method: Categorizes PCS frameworks into TTF and PTA, analyzes their techniques, and explores specialized tasks like object, face, and style personalization.

Result: Identifies key challenges (e.g., overfitting, fidelity-alignment trade-offs) and innovations in PCS, providing a detailed overview of the field.

Conclusion: Proposes future directions for PCS development, emphasizing the need to address ongoing challenges and leverage emerging innovations.

Abstract: Recent advancements in diffusion models have significantly impacted content creation, leading to the emergence of Personalized Content Synthesis (PCS). By utilizing a small set of user-provided examples featuring the same subject, PCS aims to tailor this subject to specific user-defined prompts. Over the past two years, more than 150 methods have been introduced in this area. However, existing surveys primarily focus on text-to-image generation, with few providing up-to-date summaries on PCS. This paper provides a comprehensive survey of PCS, introducing the general frameworks of PCS research, which can be categorized into test-time fine-tuning (TTF) and pre-trained adaptation (PTA) approaches. We analyze the strengths, limitations, and key techniques of these methodologies. Additionally, we explore specialized tasks within the field, such as object, face, and style personalization, while highlighting their unique challenges and innovations. Despite the promising progress, we also discuss ongoing challenges, including overfitting and the trade-off between subject fidelity and text alignment. Through this detailed overview and analysis, we propose future directions to further the development of PCS.

</details>


### [195] [Learning Invariant Causal Mechanism from Vision-Language Models](https://arxiv.org/pdf/2405.15289)
*Zeen Song, Siyu Zhao, Xingyu Zhang, Jiangmeng Li, Changwen Zheng, Wenwen Qiang*

Main category: cs.CV

TL;DR: CLIP-ICM improves CLIP's OOD performance by leveraging invariant causal factors and interventional data.


<details>
  <summary>Details</summary>
Motivation: CLIP's performance degrades in OOD scenarios due to varying causal mechanisms between training and test environments.

Method: Proposes CLIP-ICM: models CLIP's prediction process with SCM, estimates linear mapping to invariant factors using interventional data, and predicts within invariant subspace.

Result: CLIP-ICM significantly boosts CLIP's performance on OOD datasets.

Conclusion: CLIP-ICM is a simple yet powerful enhancement for reliable CLIP applications in real-world OOD settings.

Abstract: Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success, but its performance can degrade when fine-tuned in out-of-distribution (OOD) scenarios. We model the prediction process using a Structural Causal Model (SCM) and show that the causal mechanism involving both invariant and variant factors in training environments differs from that in test environments. In contrast, the causal mechanism with solely invariant factors remains consistent across environments. We theoretically prove the existence of a linear mapping from CLIP embeddings to invariant factors, which can be estimated using interventional data. Additionally, we provide a condition to guarantee low OOD risk of the invariant predictor. Based on these insights, we propose the Invariant Causal Mechanism of CLIP (CLIP-ICM) framework. CLIP-ICM involves collecting interventional data, estimating a linear projection matrix, and making predictions within the invariant subspace. Experiments on several OOD datasets show that CLIP-ICM significantly improves the performance of CLIP. Our method offers a simple but powerful enhancement, boosting the reliability of CLIP in real-world applications.

</details>


### [196] [MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping](https://arxiv.org/pdf/2409.11316)
*Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh*

Main category: cs.CV

TL;DR: A new Few-shot Semantic Segmentation framework using Transformer architecture improves relational understanding and efficiency, achieving competitive results with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of previous methods that discard local features or suffer from high complexity.

Method: Proposes a Transformer-based framework with spatial transformer decoder, contextual mask generation, and multi-scale decoder for hierarchical feature refinement.

Result: Achieves competitive performance on PASCAL-5^i and COCO-20^i datasets in 1-shot and 5-shot settings with only 1.5M parameters.

Conclusion: The framework balances performance and efficiency, overcoming existing method limitations.

Abstract: Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. https://github.com/amirrezafateh/MSDNet

</details>


### [197] [LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior](https://arxiv.org/pdf/2410.21264)
*Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, Abhinav Shrivastava*

Main category: cs.CV

TL;DR: LARP is a novel video tokenizer for autoregressive models, using holistic queries for global semantic capture and adaptive tokenization, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of patchwise tokenizers by capturing global video semantics and improving compatibility with autoregressive generation.

Method: Uses learned holistic queries for tokenization, integrates a lightweight AR transformer as a prior model, and optimizes token order for AR generation.

Result: Achieves state-of-the-art FVD on UCF101 benchmark, enhancing AR model compatibility with videos.

Conclusion: LARP improves video tokenization for AR models and enables unified high-fidelity multimodal LLMs.

Abstract: We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).

</details>


### [198] [WHALES: A Multi-agent Scheduling Dataset for Enhanced Cooperation in Autonomous Driving](https://arxiv.org/pdf/2411.13340)
*Richard Wang, Siwei Chen, Ziyi Song, Sheng Zhou*

Main category: cs.CV

TL;DR: WHALES is a large-scale V2X dataset addressing communication-aware agent scheduling and cooperative perception, featuring 8.4 agents per scene and 2.01M annotated 3D objects. It introduces CAHS, a novel scheduler, and bridges real-world V2X challenges.


<details>
  <summary>Details</summary>
Motivation: The scarcity of datasets capturing real-world V2X complexity under dynamic communication constraints motivates WHALES.

Method: WHALES integrates communication metadata and proposes CAHS, a coverage-aware historical scheduler, to improve perception performance.

Result: WHALES sets a new SOTA with 8.4 agents per scene and 2.01M annotated 3D objects, enhancing evaluation of scheduling strategies.

Conclusion: WHALES bridges simulation-real-world gaps, offering a framework for perception-scheduling co-design and scalability exploration.

Abstract: Cooperative perception research is constrained by the scarcity of datasets that capture the complexity of real-world Vehicle-to-Everything (V2X) interactions, particularly under dynamic communication constraints. To address this, we present WHALES (Wireless enhanced Autonomous vehicles with Large number of Engaged agents), the first large-scale V2X dataset specifically designed to benchmark communication-aware agent scheduling and scalable cooperative perception. WHALES establishes a new state-of-the-art (SOTA) standard with an average of 8.4 cooperative agents per scene and 2.01 million annotated 3D objects spanning diverse traffic scenarios. It integrates communication metadata to simulate real-world communication bottlenecks, enabling rigorous evaluation of scheduling strategies. To further advance the field, we propose the Coverage-Aware Historical Scheduler (CAHS), a novel scheduling baseline that prioritizes agents based on historical viewpoint coverage, improving perception performance over existing SOTA methods. WHALES bridges the gap between simulated and real-world V2X challenges, offering a robust framework to explore perception-scheduling co-design, cross-data generalization, and scalability limits. The WHALES dataset and code are available at: https://github.com/chensiweiTHU/WHALES.

</details>


### [199] [Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis](https://arxiv.org/pdf/2412.04431)
*Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu*

Main category: cs.CV

TL;DR: Infinity is a Bitwise Visual AutoRegressive Modeling method that generates high-resolution images from text, outperforming top diffusion models in speed and quality.


<details>
  <summary>Details</summary>
Motivation: To redefine visual autoregressive modeling by scaling tokenizer vocabulary to infinity and improving generation capacity and detail.

Method: Uses a bitwise token prediction framework with an infinite-vocabulary tokenizer, classifier, and bitwise self-correction mechanism. Scales transformer size alongside tokenizer.

Result: Achieves record performance, surpassing SD3-Medium in benchmarks (GenEval 0.73, ImageReward 0.96) and speed (0.8s for 1024x1024 images).

Conclusion: Infinity sets a new standard for text-to-image models, combining speed, quality, and scalability, with plans for public release.

Abstract: We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.

</details>


### [200] [PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension](https://arxiv.org/pdf/2412.11906)
*Kun Ouyang, Yuanxin Liu, Shicheng Li, Yi Liu, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun*

Main category: cs.CV

TL;DR: The paper introduces PunchBench, a benchmark for evaluating multimodal punchline comprehension in MLLMs, addressing limitations of existing benchmarks. It proposes SC-CoQ to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for punchline comprehension in MLLMs have limitations like language shortcuts, lack of question diversity, and narrow domain focus. PunchBench aims to provide a more accurate and comprehensive evaluation.

Method: PunchBench mitigates shortcuts by generating synonymous/antonymous captions and includes diverse question formats and domains. The SC-CoQ strategy helps models incrementally tackle complex questions.

Result: Evaluations show a significant gap between MLLMs and humans in punchline comprehension. SC-CoQ outperforms in-context learning and chain-of-thought methods.

Conclusion: PunchBench offers a robust evaluation framework, and SC-CoQ effectively enhances MLLMs' punchline comprehension, highlighting areas for future improvement.

Abstract: Multimodal punchlines, which involve humor or sarcasm conveyed in image-caption pairs, are a popular way of communication on online multimedia platforms. With the rapid development of multimodal large language models (MLLMs), it is essential to assess their ability to effectively comprehend these punchlines. However, existing benchmarks on punchline comprehension suffer from three major limitations: 1) language shortcuts that allow models to solely rely on text, 2) lack of question diversity, and 3) narrow focus on a specific domain of multimodal content (e.g., cartoon). To address these limitations, we introduce a multimodal \textbf{Punch}line comprehension \textbf{Bench}mark, named \textbf{PunchBench}, which is tailored for accurate and comprehensive evaluation of punchline comprehension. To enhance the evaluation accuracy, we generate synonymous and antonymous captions by modifying original captions, which mitigates the impact of shortcuts in the captions. To provide a comprehensive evaluation, PunchBench incorporates diverse question formats and image-captions from various domains. On this basis, we conduct extensive evaluations and reveal a significant gap between state-of-the-art MLLMs and humans in punchline comprehension. To improve punchline comprehension, we propose Simple-to-Complex Chain-of-Question (SC-CoQ) strategy, enabling the models to incrementally address complicated questions by first mastering simple ones. SC-CoQ effectively enhances the performance of various MLLMs on PunchBench, surpassing in-context learning and chain-of-thought.

</details>


### [201] [Concept Guided Co-salient Object Detection](https://arxiv.org/pdf/2412.16609)
*Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu*

Main category: cs.CV

TL;DR: ConceptCoSOD introduces semantic guidance via text-based concepts to improve co-salient object detection, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Co-SOD methods rely on low-level visual patterns and lack semantic priors, limiting performance.

Method: ConceptCoSOD extracts shared text-based concepts, uses diffusion timestep analysis, and resampling for robust concepts.

Result: Outperforms existing methods on three benchmarks and five corrupted settings.

Conclusion: Semantic guidance and resampling enhance Co-SOD accuracy and generalization.

Abstract: Co-salient object detection (Co-SOD) aims to identify common salient objects across a group of related images. While recent methods have made notable progress, they typically rely on low-level visual patterns and lack semantic priors, limiting their detection performance. We propose ConceptCoSOD, a concept-guided framework that introduces high-level semantic knowledge to enhance co-saliency detection. By extracting shared text-based concepts from the input image group, ConceptCoSOD provides semantic guidance that anchors the detection process. To further improve concept quality, we analyze the effect of diffusion timesteps and design a resampling strategy that selects more informative steps for learning robust concepts. This semantic prior, combined with the resampling-enhanced representation, enables accurate and consistent segmentation even in challenging visual conditions. Extensive experiments on three benchmark datasets and five corrupted settings demonstrate that ConceptCoSOD significantly outperforms existing methods in both accuracy and generalization.

</details>


### [202] [Distraction is All You Need for Multimodal Large Language Model Jailbreaking](https://arxiv.org/pdf/2502.10794)
*Zuopeng Yang, Jiluan Fan, Anli Yan, Erdun Gao, Xin Lin, Tao Li, Kanghua Mo, Changyu Dong*

Main category: cs.CV

TL;DR: The paper introduces CS-DJ, a framework to jailbreak MLLMs by disrupting their alignment through distraction strategies, achieving high attack success rates.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in MLLMs caused by complex interactions between visual and textual data, which can bypass safety mechanisms.

Method: Proposes the Distraction Hypothesis and CS-DJ framework, using structured and visual-enhanced distraction to disrupt MLLM alignment.

Result: CS-DJ achieves 52.40% average attack success rate and 74.10% ensemble attack success rate across multiple MLLMs.

Conclusion: Distraction-based approaches can effectively exploit MLLM defenses, providing new insights for attack strategies.

Abstract: Multimodal Large Language Models (MLLMs) bridge the gap between visual and textual data, enabling a range of advanced applications. However, complex internal interactions among visual elements and their alignment with text can introduce vulnerabilities, which may be exploited to bypass safety mechanisms. To address this, we analyze the relationship between image content and task and find that the complexity of subimages, rather than their content, is key. Building on this insight, we propose the Distraction Hypothesis, followed by a novel framework called Contrasting Subimage Distraction Jailbreaking (CS-DJ), to achieve jailbreaking by disrupting MLLMs alignment through multi-level distraction strategies. CS-DJ consists of two components: structured distraction, achieved through query decomposition that induces a distributional shift by fragmenting harmful prompts into sub-queries, and visual-enhanced distraction, realized by constructing contrasting subimages to disrupt the interactions among visual elements within the model. This dual strategy disperses the model's attention, reducing its ability to detect and mitigate harmful content. Extensive experiments across five representative scenarios and four popular closed-source MLLMs, including GPT-4o-mini, GPT-4o, GPT-4V, and Gemini-1.5-Flash, demonstrate that CS-DJ achieves average success rates of 52.40% for the attack success rate and 74.10% for the ensemble attack success rate. These results reveal the potential of distraction-based approaches to exploit and bypass MLLMs' defenses, offering new insights for attack strategies.

</details>


### [203] [Hardware-Friendly Static Quantization Method for Video Diffusion Transformers](https://arxiv.org/pdf/2502.15077)
*Sanghyun Yi, Qingfeng Liu, Mostafa El-Khamy*

Main category: cs.CV

TL;DR: The paper proposes a static quantization method for OpenSora, a Video Diffusion Transformer, as an alternative to dynamic quantization, achieving comparable video quality on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: Dynamic quantization is unsuitable for resource-constrained devices, necessitating a static quantization approach for efficient deployment of generative-AI models like OpenSora.

Method: The method uses per-step calibration data for post-training static quantization, employing channel-wise quantization for weights and tensor-wise quantization for activations, enhanced by smooth-quantization.

Result: Static quantization matches FP16 and dynamic quantization in video quality (measured by CLIP and VQA metrics) while being more efficient.

Conclusion: Static quantization is a viable and efficient alternative to dynamic quantization for video diffusion transformers, maintaining performance without dynamic techniques.

Abstract: Diffusion Transformers for video generation have gained significant research interest since the impressive performance of SORA. Efficient deployment of such generative-AI models on GPUs has been demonstrated with dynamic quantization. However, resource-constrained devices cannot support dynamic quantization, and need static quantization of the models for their efficient deployment on AI processors. In this paper, we propose a novel method for the post-training quantization of OpenSora\cite{opensora}, a Video Diffusion Transformer, without relying on dynamic quantization techniques. Our approach employs static quantization, achieving video quality comparable to FP16 and dynamically quantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular, we utilize per-step calibration data to adequately provide a post-training statically quantized model for each time step, incorporating channel-wise quantization for weights and tensor-wise quantization for activations. By further applying the smooth-quantization technique, we can obtain high-quality video outputs with the statically quantized models. Extensive experimental results demonstrate that static quantization can be a viable alternative to dynamic quantization for video diffusion transformers, offering a more efficient approach without sacrificing performance.

</details>


### [204] [A dataset of high-resolution plantar pressures for gait analysis across varying footwear and walking speeds](https://arxiv.org/pdf/2502.17244)
*Robyn Larracy, Angkoon Phinyomark, Ala Salehi, Eve MacDonald, Saeed Kazemi, Shikder Shafiul Bashar, Aaron Tabor, Erik Scheme*

Main category: cs.CV

TL;DR: The paper introduces the UNB StepUP-P150 dataset, a large-scale plantar pressure dataset for gait analysis, addressing the lack of publicly available data in this field.


<details>
  <summary>Details</summary>
Motivation: Gait analysis is crucial in various fields, but underfoot pressure data is underexplored due to limited datasets. The paper aims to fill this gap.

Method: The dataset includes high-resolution plantar pressure data from 150 individuals, collected using a pressure-sensing walkway, with varied walking speeds and footwear conditions.

Result: The dataset contains over 200,000 footsteps, enabling advancements in gait recognition and biomechanics research.

Conclusion: UNB StepUP-P150 sets a new benchmark for plantar pressure-based gait analysis and opens new research opportunities.

Abstract: Gait refers to the patterns of limb movement generated during walking, which are unique to each individual due to both physical and behavioral traits. Walking patterns have been widely studied in biometrics, biomechanics, sports, and rehabilitation. While traditional methods rely on video and motion capture, advances in plantar pressure sensing technology now offer deeper insights into gait. However, underfoot pressures during walking remain underexplored due to the lack of large, publicly accessible datasets. To address this, we introduce the UNB StepUP-P150 dataset: a footStep database for gait analysis and recognition using Underfoot Pressure, including data from 150 individuals. This dataset comprises high-resolution plantar pressure data (4 sensors per cm-squared) collected using a 1.2m by 3.6m pressure-sensing walkway. It contains over 200,000 footsteps from participants walking with various speeds (preferred, slow-to-stop, fast, and slow) and footwear conditions (barefoot, standard shoes, and two personal shoes), supporting advancements in biometric gait recognition and resenting new research opportunities in biomechanics and deep learning. UNB StepUP-P150 establishes a new benchmark for plantar pressure-based gait analysis and recognition.

</details>


### [205] [Hardware-Rasterized Ray-Based Gaussian Splatting](https://arxiv.org/pdf/2503.18682)
*Samuel Rota Bulò, Nemanja Bartolovic, Lorenzo Porzi, Peter Kontschieder*

Main category: cs.CV

TL;DR: A hardware rasterized rendering method for RayGS achieves fast, high-quality novel view synthesis, supporting real-time applications like VR/MR, with alias-free rendering.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and high-quality rendering of RayGS models for real-time applications, addressing MIP-related aliasing issues.

Method: Uses hardware rasterization shaders for efficient rendering, with a mathematically rigorous derivation for estimating rendering quantities. Also addresses MIP-related aliasing.

Result: Achieves high frame rates for real-time applications and alias-free rendering, maintaining state-of-the-art quality across benchmark scenes.

Conclusion: The method successfully combines speed and quality for RayGS rendering, making it suitable for VR/MR applications.

Abstract: We present a novel, hardware rasterized rendering approach for ray-based 3D Gaussian Splatting (RayGS), obtaining both fast and high-quality results for novel view synthesis. Our work contains a mathematically rigorous and geometrically intuitive derivation about how to efficiently estimate all relevant quantities for rendering RayGS models, structured with respect to standard hardware rasterization shaders. Our solution is the first enabling rendering RayGS models at sufficiently high frame rates to support quality-sensitive applications like Virtual and Mixed Reality. Our second contribution enables alias-free rendering for RayGS, by addressing MIP-related issues arising when rendering diverging scales during training and testing. We demonstrate significant performance gains, across different benchmark scenes, while retaining state-of-the-art appearance quality of RayGS.

</details>


### [206] [Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments](https://arxiv.org/pdf/2505.05540)
*Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Harshvardhan Sikka, Paul Pu Liang*

Main category: cs.CV

TL;DR: The paper introduces MultiNet v0.2, a benchmark for evaluating zero-shot generalization of vision-language-action (VLA) models in procedurally OOD environments, revealing limitations and performance insights.


<details>
  <summary>Details</summary>
Motivation: To systematically assess the generalization capabilities of VLA models in OOD tasks, addressing a gap in current evaluation methods.

Method: Developed MultiNet v0.2 to test state-of-the-art VLMs and VLAs (e.g., GPT-4o, OpenVLA) on diverse procedural tasks from Procgen benchmark.

Result: Key findings: (1) models struggle with zero-shot OOD generalization, (2) VLAs outperform others due to robust design, (3) VLMs improve with precise prompt engineering.

Conclusion: The benchmark and findings aim to guide future VLA model development and highlight areas for improvement in OOD task performance.

Abstract: Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in procedurally out-of-distribution (OOD) environments, remains limited. In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLMs and VLAs - including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST - on diverse procedural tasks from the Procgen benchmark. Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexity; (2) VLAs generally outperforms other models due to their robust architectural design; and (3) VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering. We release our benchmark, evaluation framework, and findings to enable the assessment of future VLA models and identify critical areas for improvement in their application to out-of-distribution digital tasks.

</details>


### [207] [T2V-OptJail: Discrete Prompt Optimization for Text-to-Video Jailbreak Attacks](https://arxiv.org/pdf/2505.06679)
*Jiayang Liu, Siyuan Liang, Shiqian Zhao, Rongcheng Tu, Wenbo Zhou, Aishan Liu, Dacheng Tao, Siew Kei Lam*

Main category: cs.CV

TL;DR: The paper introduces T2V-OptJail, a framework for jailbreak attacks on text-to-video models, optimizing bypassing safety filters and semantic consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Text-to-video models are vulnerable to jailbreak attacks, but existing benchmarks lack systematic vulnerability exploration.

Method: Formalizes jailbreak attacks as a discrete optimization problem, using a joint objective-based framework with iterative optimization guided by prompt variants.

Result: Improves attack success rates by 11.4% (GPT-4) and 10.0% (human assessors) over state-of-the-art methods.

Conclusion: T2V-OptJail effectively enhances attack success and content control, highlighting security risks in text-to-video models.

Abstract: In recent years, fueled by the rapid advancement of diffusion models, text-to-video (T2V) generation models have achieved remarkable progress, with notable examples including Pika, Luma, Kling, and Open-Sora. Although these models exhibit impressive generative capabilities, they also expose significant security risks due to their vulnerability to jailbreak attacks, where the models are manipulated to produce unsafe content such as pornography, violence, or discrimination. Existing works such as T2VSafetyBench provide preliminary benchmarks for safety evaluation, but lack systematic methods for thoroughly exploring model vulnerabilities. To address this gap, we are the first to formalize the T2V jailbreak attack as a discrete optimization problem and propose a joint objective-based optimization framework, called T2V-OptJail. This framework consists of two key optimization goals: bypassing the built-in safety filtering mechanisms to increase the attack success rate, preserving semantic consistency between the adversarial prompt and the unsafe input prompt, as well as between the generated video and the unsafe input prompt, to enhance content controllability. In addition, we introduce an iterative optimization strategy guided by prompt variants, where multiple semantically equivalent candidates are generated in each round, and their scores are aggregated to robustly guide the search toward optimal adversarial prompts. We conduct large-scale experiments on several T2V models, covering both open-source models and real commercial closed-source models. The experimental results show that the proposed method improves 11.4% and 10.0% over the existing state-of-the-art method in terms of attack success rate assessed by GPT-4, attack success rate assessed by human accessors, respectively, verifying the significant advantages of the method in terms of attack effectiveness and content control.

</details>


### [208] [HyMamba: Mamba with Hybrid Geometry-Feature Coupling for Efficient Point Cloud Classification](https://arxiv.org/pdf/2505.11099)
*Bin Liu, Chunyang Wang, Xuelian Liu, Bo Xiao, Guan Xi*

Main category: cs.CV

TL;DR: HyMamba improves point cloud classification by coupling geometry and features, enhancing local and global feature extraction with GFCP and CoFE, achieving high accuracy on ModelNet40 and ScanObjectNN.


<details>
  <summary>Details</summary>
Motivation: Addressing the weakened local geometric relevance in Mamba-based methods due to decoupling geometric structures and features, limiting local feature extraction.

Method: Proposes HyMamba with Geometry-Feature Coupled Pooling (GFCP) for dynamic geometric aggregation and Collaborative Feature Enhancer (CoFE) for cross-path feature hybridization.

Result: Achieves 95.99% accuracy on ModelNet40 and 98.9% on ModelNetFewShot, demonstrating superior performance and generalization.

Conclusion: HyMamba effectively addresses local feature extraction limitations, offering robust performance and generalization in point cloud classification.

Abstract: Point cloud classification is one of the essential technologies for achieving intelligent perception of 3D environments by machines, its core challenge is to efficiently extract local and global features. Mamba leverages state space models (SSMs) for global point cloud modeling. Although prior Mamba-based point cloud processing methods pay attention to the limitation of its flattened sequence modeling mechanism in fusing local and global features, the critical issue of weakened local geometric relevance caused by decoupling geometric structures and features in the input patches remains not fully revealed, and both jointly limit local feature extraction. Therefore, we propose HyMamba, a geometry and feature coupled Mamba framework featuring: (1) Geometry-Feature Coupled Pooling (GFCP), which achieves physically interpretable geometric information coupling by dynamically aggregating adjacent geometric information into local features; (2) Collaborative Feature Enhancer (CoFE), which enhances sparse signal capture through cross-path feature hybridization while effectively integrating global and local contexts. We conducted extensive experiments on ModelNet40 and ScanObjectNN datasets. The results demonstrate that the proposed model achieves superior classification performance, particularly on the ModelNet40, where it elevates accuracy to 95.99% with merely 0.03M additional parameters. Furthermore, it attains 98.9% accuracy on the ModelNetFewShot dataset, validating its robust generalization capabilities under sparse samples. Our code and weights are available at https://github.com/L1277471578/HyMamba

</details>


### [209] [Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner](https://arxiv.org/pdf/2505.11404)
*Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu*

Main category: cs.CV

TL;DR: The paper introduces Patho-R1, a multimodal RL-based pathology Reasoner, and Patho-CLIP, addressing limitations in current pathology VLMs by leveraging high-quality datasets and a three-stage training pipeline.


<details>
  <summary>Details</summary>
Motivation: Current pathology VLMs lack diagnostic accuracy and reasoning plausibility due to shallow datasets. The study aims to improve this by using expert-curated data and structured training.

Method: A three-stage pipeline: (1) pretraining on 3.5M image-text pairs, (2) fine-tuning on 500k Chain-of-Thought samples, (3) reinforcement learning for reasoning refinement. Patho-CLIP is also introduced for dataset alignment assessment.

Result: Patho-R1 and Patho-CLIP show robust performance in tasks like zero-shot classification, cross-modal retrieval, VQA, and MCQs.

Conclusion: The proposed methods significantly enhance pathology VLMs, validated by comprehensive experiments, with code available for further research.

Abstract: Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose Patho-CLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both Patho-CLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: https://github.com/Wenchuan-Zhang/Patho-R1.

</details>


### [210] [MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion](https://arxiv.org/pdf/2505.14719)
*Wei Hua, Chenlin Zhou, Jibin Wu, Yansong Chua, Yangyang Shu*

Main category: cs.CV

TL;DR: MSVIT, a novel spike-driven Transformer architecture, enhances spiking attention with multi-scale features, outperforming existing SNN-based models.


<details>
  <summary>Details</summary>
Motivation: Addressing the performance gap between SNN-based and ANN-based transformer architectures by improving feature extraction from different image scales.

Method: Proposes MSVIT with multi-scale spiking attention (MSSA) to enhance spiking attention blocks.

Result: Outperforms existing SNN-based models, achieving state-of-the-art performance.

Conclusion: MSVIT is a promising solution for energy-efficient, high-performance SNN-transformer architectures.

Abstract: The combination of Spiking Neural Networks (SNNs) with Vision Transformer architectures has garnered significant attention due to their potential for energy-efficient and high-performance computing paradigms. However, a substantial performance gap still exists between SNN-based and ANN-based transformer architectures. While existing methods propose spiking self-attention mechanisms that are successfully combined with SNNs, the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting features from different image scales. In this paper, we address this issue and propose MSVIT. This novel spike-driven Transformer architecture firstly uses multi-scale spiking attention (MSSA) to enhance the capabilities of spiking attention blocks. We validate our approach across various main datasets. The experimental results show that MSVIT outperforms existing SNN-based models, positioning itself as a state-of-the-art solution among SNN-transformer architectures. The codes are available at https://github.com/Nanhu-AI-Lab/MSViT.

</details>


### [211] [Mouse Lockbox Dataset: Behavior Recognition for Mice Solving Lockboxes](https://arxiv.org/pdf/2505.15408)
*Patrik Reiske, Marcus N. Boon, Niek Andresen, Sole Traverso, Katharina Hohlbaum, Lars Lewejohann, Christa Thöne-Reineke, Olaf Hellwich, Henning Sprekeler*

Main category: cs.CV

TL;DR: A video dataset of mice solving complex mechanical puzzles is presented to advance automated behavior classification in neuroscience.


<details>
  <summary>Details</summary>
Motivation: Existing datasets focus on simple or social behaviors, lacking complex individual tasks like puzzle-solving.

Method: The dataset includes 110+ hours of video from three perspectives, with human-annotated labels for 13% of the data. A keypoint tracking-based framework is used for action classification.

Result: The framework highlights challenges in automated labeling of fine-grained behaviors, such as object manipulation.

Conclusion: The dataset aims to accelerate progress in automated action and behavior classification in neuroscience.

Abstract: Machine learning and computer vision methods have a major impact on the study of natural animal behavior, as they enable the (semi-)automatic analysis of vast amounts of video data. Mice are the standard mammalian model system in most research fields, but the datasets available today to refine such methods focus either on simple or social behaviors. In this work, we present a video dataset of individual mice solving complex mechanical puzzles, so-called lockboxes. The more than 110 hours of total playtime show their behavior recorded from three different perspectives. As a benchmark for frame-level action classification methods, we provide human-annotated labels for all videos of two different mice, that equal 13% of our dataset. Our keypoint (pose) tracking-based action classification framework illustrates the challenges of automated labeling of fine-grained behaviors, such as the manipulation of objects. We hope that our work will help accelerate the advancement of automated action and behavior classification in the computational neuroscience community. Our dataset is publicly available at https://doi.org/10.14279/depositonce-23850

</details>


### [212] [BiggerGait: Unlocking Gait Recognition with Layer-wise Representations from Large Vision Models](https://arxiv.org/pdf/2505.18132)
*Dingqiang Ye, Chao Fan, Zhanbo Huang, Chengwen Luo, Jianqiang Li, Shiqi Yu, Xiaoming Liu*

Main category: cs.CV

TL;DR: BiggerGait leverages LVM's multi-layer representations for gait recognition, outperforming prior methods without relying heavily on gait priors.


<details>
  <summary>Details</summary>
Motivation: Existing LVM-based gait recognition methods overemphasize gait priors and overlook the rich, distinct representations in LVM's intermediate layers.

Method: Analyzes layer-wise LVM representations, integrates them, and proposes BiggerGait, a simple baseline for gait recognition.

Result: BiggerGait shows superior performance on CCPG, CAISA-B*, SUSTech1K, and CCGR_MINI datasets in within- and cross-domain tasks.

Conclusion: BiggerGait is a practical, effective baseline for gait representation learning, with models and code made publicly available.

Abstract: Large vision models (LVM) based gait recognition has achieved impressive performance. However, existing LVM-based approaches may overemphasize gait priors while neglecting the intrinsic value of LVM itself, particularly the rich, distinct representations across its multi-layers. To adequately unlock LVM's potential, this work investigates the impact of layer-wise representations on downstream recognition tasks. Our analysis reveals that LVM's intermediate layers offer complementary properties across tasks, integrating them yields an impressive improvement even without rich well-designed gait priors. Building on this insight, we propose a simple and universal baseline for LVM-based gait recognition, termed BiggerGait. Comprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\_MINI validate the superiority of BiggerGait across both within- and cross-domain tasks, establishing it as a simple yet practical baseline for gait representation learning. All the models and code will be publicly available.

</details>


### [213] [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/pdf/2505.20612)
*Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri*

Main category: cs.CV

TL;DR: Vision-language models (VLMs) struggle with out-of-distribution tasks. The paper introduces Roboflow100-VL, a benchmark for evaluating VLMs, and highlights the need for few-shot concept alignment.


<details>
  <summary>Details</summary>
Motivation: Current VLMs fail to generalize to uncommon tasks and modalities. The goal is to improve alignment with new concepts using minimal visual examples and rich text.

Method: Introduces Roboflow100-VL, a multi-modal dataset with 100 diverse object detection tasks, and evaluates VLMs in various settings (zero-shot, few-shot, etc.).

Result: VLMs perform poorly (under 2% accuracy) on challenging tasks like medical imaging, showing the need for few-shot alignment. A competition winner improved baseline by 16.8 mAP.

Conclusion: Few-shot alignment is crucial for VLMs to handle diverse tasks. The Roboflow100-VL benchmark and insights from the competition highlight paths for future improvements.

Abstract: Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 16.8 mAP! Our code and dataset are available at https://github.com/roboflow/rf100-vl/ and https://universe.roboflow.com/rf100-vl/

</details>


### [214] [FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing](https://arxiv.org/pdf/2505.23145)
*Jeongsol Kim, Yeobin Hong, Jong Chul Ye*

Main category: cs.CV

TL;DR: FlowAlign is a new inversion-free, flow-based framework for consistent image editing with trajectory control, outperforming existing methods in source preservation and editing controllability.


<details>
  <summary>Details</summary>
Motivation: Existing inversion-free, flow-based methods like FlowEdit suffer from unstable editing trajectories and poor source consistency.

Method: FlowAlign introduces a flow-matching loss to regularize smoother, stable trajectories and balance semantic alignment with structural consistency.

Result: FlowAlign achieves better source preservation and editing controllability, supporting reverse editing by reversing the ODE trajectory.

Conclusion: FlowAlign provides a principled, reversible, and consistent solution for inversion-free image editing.

Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3, enabling text-driven manipulation by solving an ordinary differential equation (ODE). While the lack of exact latent inversion is a core advantage of these methods, it often results in unstable editing trajectories and poor source consistency. To address this limitation, we propose FlowAlign, a novel inversion-free flow-based framework for consistent image editing with principled trajectory control. FlowAlign introduces a flow-matching loss as a regularization mechanism to promote smoother and more stable trajectories during the editing process. Notably, the flow-matching loss is shown to explicitly balance semantic alignment with the edit prompt and structural consistency with the source image along the trajectory. Furthermore, FlowAlign naturally supports reverse editing by simply reversing the ODE trajectory, highlighting the reversible and consistent nature of the transformation. Extensive experiments demonstrate that FlowAlign outperforms existing methods in both source preservation and editing controllability.

</details>


### [215] [CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs](https://arxiv.org/pdf/2505.24120)
*Ai Jian, Weijie Qiu, Xiaokun Wang, Peiyu Wang, Yunzhuo Hao, Jiangbo Pei, Yichen Wei, Yi Peng, Xuchen Song*

Main category: cs.CV

TL;DR: CSVQA is a new benchmark for evaluating scientific reasoning in Vision-Language Models (VLMs), featuring 1,378 STEM-based questions requiring domain knowledge and visual analysis. It reveals significant performance gaps, with top models scoring only 49.6% accuracy.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack authentic scientific contexts, failing to assess VLMs' ability to integrate domain-specific knowledge with visual evidence.

Method: CSVQA includes 1,378 STEM question-answer pairs and introduces a protocol to validate reasoning steps via explanations.

Result: Evaluation of 15 VLMs shows poor performance (top model: 49.6% accuracy), highlighting limitations in scientific reasoning.

Conclusion: CSVQA underscores the need for advancing VLMs' scientific reasoning capabilities and is publicly available.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remain inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering. Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning. We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6% accuracy. This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA

</details>


### [216] [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/pdf/2506.01413)
*Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun*

Main category: cs.CV

TL;DR: The paper addresses challenges in LLMs for complex instructions, proposing a method using reinforcement learning to improve reasoning and instruction-following, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with complex instructions, and vanilla chain-of-thought (CoT) methods fail due to superficial reasoning. A systematic approach is needed to enhance reasoning and performance.

Method: The method decomposes complex instructions, uses reinforcement learning with verifiable rewards, and employs contrastive learning and expert behavior cloning to improve reasoning.

Result: A 1.5B LLM achieves 11.74% performance gains, matching an 8B LLM, validated on seven benchmarks.

Conclusion: The proposed method effectively enhances LLMs' ability to handle complex instructions, outperforming vanilla CoT and scaling compute efficiently.

Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data will be available later (under review).
  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions

</details>


### [217] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/pdf/2506.08915)
*Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos*

Main category: cs.CV

TL;DR: An attention-based method using binary masks to focus on relevant image regions, improving robustness against spurious correlations and out-of-distribution backgrounds.


<details>
  <summary>Details</summary>
Motivation: Context can bias object perception, especially in out-of-distribution backgrounds, while many tasks require identifying relevant regions.

Method: A two-stage framework: stage 1 identifies task-relevant regions, and stage 2 uses attention masks to focus on these regions, trained jointly.

Result: Significant improvement in robustness against spurious correlations and out-of-distribution backgrounds.

Conclusion: The proposed method effectively balances context use and focused analysis, enhancing object-centric task performance.

Abstract: We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds. Code: https://github.com/ananthu-aniraj/ifam

</details>


### [218] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/pdf/2506.09081)
*Zheqi He, Yesheng Liu, Jing-shu Zheng, Xuejing Li, Jin-Ge Yao, Bowen Qin, Richeng Xuan, Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM is an open-source framework for evaluating multimodal models on vision-language tasks, offering flexibility, efficiency, and accuracy.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive and efficient tool for assessing multimodal models across diverse tasks, aiding research advancement.

Method: Decouples model inference from evaluation, uses advanced acceleration tools (e.g., vLLM, SGLang), and employs asynchronous data loading.

Result: Enhances evaluation efficiency and provides accurate insights into model performance.

Conclusion: FlagEvalMM is a valuable, publicly accessible tool for advancing multimodal research.

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [219] [Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models](https://arxiv.org/pdf/2506.12340)
*Zongyu Wu, Minhua Lin, Zhiwei Zhang, Fali Wang, Xianren Zhang, Xiang Zhang, Suhang Wang*

Main category: cs.CV

TL;DR: The paper proposes ICIMIA, a method to detect if an image was used to train a large vision-language model (LVLM) by exploiting differences in sensitivity to image corruption for member vs. non-member images.


<details>
  <summary>Details</summary>
Motivation: LVLMs trained on large datasets pose privacy risks if sensitive images are included. Detecting training data usage is crucial for privacy protection.

Method: ICIMIA uses embedding similarity between original and corrupted images (white-box) or output text embeddings (black-box) to infer membership.

Result: Experiments show ICIMIA effectively detects training data usage under both white-box and black-box settings.

Conclusion: ICIMIA is a simple yet effective method for membership inference attacks on LVLMs, addressing privacy concerns.

Abstract: Large vision-language models (LVLMs) have demonstrated outstanding performance in many downstream tasks. However, LVLMs are trained on large-scale datasets, which can pose privacy risks if training images contain sensitive information. Therefore, it is important to detect whether an image is used to train the LVLM. Recent studies have investigated membership inference attacks (MIAs) against LVLMs, including detecting image-text pairs and single-modality content. In this work, we focus on detecting whether a target image is used to train the target LVLM. We design simple yet effective Image Corruption-Inspired Membership Inference Attacks (ICIMIA) against LLVLMs, which are inspired by LVLM's different sensitivity to image corruption for member and non-member images. We first perform an MIA method under the white-box setting, where we can obtain the embeddings of the image through the vision part of the target LVLM. The attacks are based on the embedding similarity between the image and its corrupted version. We further explore a more practical scenario where we have no knowledge about target LVLMs and we can only query the target LVLMs with an image and a question. We then conduct the attack by utilizing the output text embeddings' similarity. Experiments on existing datasets validate the effectiveness of our proposed attack methods under those two different settings.

</details>


### [220] [3D Hand Mesh-Guided AI-Generated Malformed Hand Refinement with Hand Pose Transformation via Diffusion Model](https://arxiv.org/pdf/2506.12680)
*Chen-Bin Feng, Kangdao Liu, Jian Sun, Jiping Jin, Yiguo Jiang, Chi-Man Vong*

Main category: cs.CV

TL;DR: A 3D mesh-guided diffusion framework is proposed to refine malformed hands in AI-generated images, outperforming depth-based methods by leveraging detailed 3D hand meshes and a novel pose transformation technique.


<details>
  <summary>Details</summary>
Motivation: Malformed hands in AI-generated images reduce authenticity, and existing depth-based methods fail due to limited detail representation.

Method: Uses a 3D hand mesh estimator for detailed guidance, trains a diffusion inpainting model on a reannotated dataset, and employs a double-check algorithm for robust inference. Also introduces a hand pose transformation method without extra training.

Result: The method achieves superior performance in refining malformed hands and enables flexible pose transformation.

Conclusion: The proposed framework effectively refines malformed hands and enhances flexibility, demonstrating significant improvements over existing approaches.

Abstract: The malformed hands in the AI-generated images seriously affect the authenticity of the images. To refine malformed hands, existing depth-based approaches use a hand depth estimator to guide the refinement of malformed hands. Due to the performance limitations of the hand depth estimator, many hand details cannot be represented, resulting in errors in the generated hands, such as confusing the palm and the back of the hand. To solve this problem, we propose a 3D mesh-guided refinement framework using a diffusion pipeline. We use a state-of-the-art 3D hand mesh estimator, which provides more details of the hands. For training, we collect and reannotate a dataset consisting of RGB images and 3D hand mesh. Then we design a diffusion inpainting model to generate refined outputs guided by 3D hand meshes. For inference, we propose a double check algorithm to facilitate the 3D hand mesh estimator to obtain robust hand mesh guidance to obtain our refined results. Beyond malformed hand refinement, we propose a novel hand pose transformation method. It increases the flexibility and diversity of the malformed hand refinement task. We made the restored images mimic the hand poses of the reference images. The pose transformation requires no additional training. Extensive experimental results demonstrate the superior performance of our proposed method.

</details>


### [221] [Efficient multi-view training for 3D Gaussian Splatting](https://arxiv.org/pdf/2506.12727)
*Minhyuk Choi, Injae Kim, Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: The paper proposes multi-view training for 3D Gaussian Splatting (3DGS) to address suboptimal optimization in single-view training, introducing modified rasterization, a 3D distance-aware D-SSIM loss, and multi-view adaptive density control.


<details>
  <summary>Details</summary>
Motivation: Single-view training in 3DGS leads to high variance in gradients and suboptimal optimization, necessitating multi-view training, which is challenging due to overhead and Gaussian densification issues.

Method: The authors modify rasterization to reduce overhead, introduce a 3D distance-aware D-SSIM loss, and propose multi-view adaptive density control for better multi-view training.

Result: Experiments show the proposed methods significantly improve 3DGS performance, overcoming single-view training limitations.

Conclusion: Multi-view training enhances 3DGS, making it more efficient and effective compared to single-view approaches.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside Neural Radiance Fields (NeRF) in inverse rendering due to its superior rendering speed. Currently, the common approach in 3DGS is to utilize "single-view" mini-batch training, where only one image is processed per iteration, in contrast to NeRF's "multi-view" mini-batch training, which leverages multiple images. We observe that such single-view training can lead to suboptimal optimization due to increased variance in mini-batch stochastic gradients, highlighting the necessity for multi-view training. However, implementing multi-view training in 3DGS poses challenges. Simply rendering multiple images per iteration incurs considerable overhead and may result in suboptimal Gaussian densification due to its reliance on single-view assumptions. To address these issues, we modify the rasterization process to minimize the overhead associated with multi-view training and propose a 3D distance-aware D-SSIM loss and multi-view adaptive density control that better suits multi-view scenarios. Our experiments demonstrate that the proposed methods significantly enhance the performance of 3DGS and its variants, freeing 3DGS from the constraints of single-view training.

</details>


### [222] [Self-Supervised Enhancement for Depth from a Lightweight ToF Sensor with Monocular Images](https://arxiv.org/pdf/2506.13444)
*Laiyan Ding, Hualie Jiang, Jiwei Chen, Rui Huang*

Main category: cs.CV

TL;DR: SelfToF is a self-supervised framework enhancing low-resolution depth maps using RGB images, avoiding the need for groundtruth depth. It includes a scale-recovery module and depth consistency loss, with an upgraded version (SelfToF*) handling varying ToF sparsity.


<details>
  <summary>Details</summary>
Motivation: Improving low-resolution depth data from ToF sensors without requiring groundtruth depth maps for supervision.

Method: Uses a self-supervised learning framework with RGB images, introduces a depth consistency loss, scale-recovery module, and upgrades to SelfToF* with submanifold convolution and guided feature fusion for varying ToF sparsity.

Result: Generates detailed and scale-aware depth maps, achieving a large performance boost and robust performance across varying ToF sparsity levels.

Conclusion: The method is efficient and effective, validated on NYU and ScanNet datasets, with code publicly available.

Abstract: Depth map enhancement using paired high-resolution RGB images offers a cost-effective solution for improving low-resolution depth data from lightweight ToF sensors. Nevertheless, naively adopting a depth estimation pipeline to fuse the two modalities requires groundtruth depth maps for supervision. To address this, we propose a self-supervised learning framework, SelfToF, which generates detailed and scale-aware depth maps. Starting from an image-based self-supervised depth estimation pipeline, we add low-resolution depth as inputs, design a new depth consistency loss, propose a scale-recovery module, and finally obtain a large performance boost. Furthermore, since the ToF signal sparsity varies in real-world applications, we upgrade SelfToF to SelfToF* with submanifold convolution and guided feature fusion. Consequently, SelfToF* maintain robust performance across varying sparsity levels in ToF data. Overall, our proposed method is both efficient and effective, as verified by extensive experiments on the NYU and ScanNet datasets. The code is available at \href{https://github.com/denyingmxd/selftof}{https://github.com/denyingmxd/selftof}.

</details>


### [223] [Lecture Video Visual Objects (LVVO) Dataset: A Benchmark for Visual Object Detection in Educational Videos](https://arxiv.org/pdf/2506.13657)
*Dipayan Biswas, Shishir Shah, Jaspal Subhlok*

Main category: cs.CV

TL;DR: The LVVO dataset is a new benchmark for visual object detection in educational videos, featuring 4,000 frames (1k manually annotated, 3k semi-supervised) across four categories.


<details>
  <summary>Details</summary>
Motivation: To provide a resource for developing and evaluating visual content detection methods in educational videos.

Method: Frames from lecture videos were manually annotated (with inter-annotator agreement checks) and semi-supervised for expansion.

Result: High inter-annotator agreement (F1 score 83.41%) and a publicly available dataset (LVVO) for research.

Conclusion: LVVO is a valuable benchmark for advancing visual object detection in educational content.

Abstract: We introduce the Lecture Video Visual Objects (LVVO) dataset, a new benchmark for visual object detection in educational video content. The dataset consists of 4,000 frames extracted from 245 lecture videos spanning biology, computer science, and geosciences. A subset of 1,000 frames, referred to as LVVO_1k, has been manually annotated with bounding boxes for four visual categories: Table, Chart-Graph, Photographic-image, and Visual-illustration. Each frame was labeled independently by two annotators, resulting in an inter-annotator F1 score of 83.41%, indicating strong agreement. To ensure high-quality consensus annotations, a third expert reviewed and resolved all cases of disagreement through a conflict resolution process. To expand the dataset, a semi-supervised approach was employed to automatically annotate the remaining 3,000 frames, forming LVVO_3k. The complete dataset offers a valuable resource for developing and evaluating both supervised and semi-supervised methods for visual content detection in educational videos. The LVVO dataset is publicly available to support further research in this domain.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [224] ['Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra](https://arxiv.org/pdf/2506.13768)
*Stefan Reimann*

Main category: cs.AI

TL;DR: A non-associative algebraic framework for high-dimensional information representation and computation, maintaining temporal order without auxiliary structures.


<details>
  <summary>Details</summary>
Motivation: Address limitations of associative bundling (loss of order information) and align with cognitive science findings on memory.

Method: Uses multiplication-like binding and non-associative interference-like bundling, creating L-states (recency) and R-states (primacy).

Result: Sparse representations of sequences with preserved temporal structure, replicating the Serial Position Curve.

Conclusion: Non-associative bundling effectively captures cognitive memory effects (recency and primacy) without auxiliary order markers.

Abstract: This note presents a non-associative algebraic framework for the representation and computation of information items in high-dimensional space. This framework is consistent with the principles of spatial computing and with the empirical findings in cognitive science about memory. Computations are performed through a process of multiplication-like binding and non-associative interference-like bundling. Models that rely on associative bundling typically lose order information, which necessitates the use of auxiliary order structures, such as position markers, to represent sequential information that is important for cognitive tasks. In contrast, the non-associative bundling proposed allows the construction of sparse representations of arbitrarily long sequences that maintain their temporal structure across arbitrary lengths. In this operation, noise is a constituent element of the representation of order information, rather than a means of obscuring it. The non-associative nature of the proposed framework results in the representation of a single sequence by two distinct states. The L-state, generated through left-associative bundling, continuously updates and emphasises a recency effect, while the R-state, formed through right-associative bundling, encodes finite sequences or chunks, capturing a primacy effect. The construction of these states may be associated with activity in the prefrontal cortex in relation to short-term memory and hippocampal encoding in long-term memory, respectively. The accuracy of retrieval is contingent upon a decision-making process that is based on the mutual information between the memory states and the cue. The model is able to replicate the Serial Position Curve, which reflects the empirical recency and primacy effects observed in cognitive experiments.

</details>


### [225] [Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs](https://arxiv.org/pdf/2506.13773)
*Milapji Singh Gill, Tom Jeleniewski, Felix Gehlhoff, Alexander Fay*

Main category: cs.AI

TL;DR: The paper introduces a modular semantic model and method for integrating differential equations into knowledge graphs, validated in aviation maintenance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reusable ontological artifacts and methods for contextualizing differential equations in CPS applications.

Method: Proposes a modular semantic model for representing differential equations in knowledge graphs and a method for efficient knowledge graph generation.

Result: Demonstrated successful formal representation and contextualization of differential equations in aviation maintenance.

Conclusion: The artifacts are practically applicable for integrating behavioral information in CPS lifecycle phases.

Abstract: Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.

</details>


### [226] [Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values](https://arxiv.org/pdf/2506.13774)
*Nell Watson, Ahmed Amer, Evan Harris, Preeti Ravindra, Shujun Zhang*

Main category: cs.AI

TL;DR: The paper introduces a 'superego' agent for aligning agentic AI with human values, reducing harmful outputs by up to 98.3%.


<details>
  <summary>Details</summary>
Motivation: Challenges in aligning AI behavior with diverse human values, safety, and compliance needs hinder deployment.

Method: A 'superego' agent uses 'Creed Constitutions' for dynamic oversight and real-time compliance validation.

Result: Achieves significant harm reduction (98.3%) and high refusal rates (e.g., 100% with Claude Sonnet 4).

Conclusion: The approach simplifies AI alignment, improving safety and contextual attunement.

Abstract: Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (www.Creed.Space) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.

</details>


### [227] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/pdf/2506.13776)
*Kevin L. Wei, Patricia Paskov, Sunishchal Dev, Michael J. Byun, Anka Reuel, Xavier Roberts-Gaal, Rachel Calcott, Evie Coxon, Chinmay Deshpande*

Main category: cs.AI

TL;DR: The paper advocates for stricter and more transparent human baselines in AI evaluations to ensure meaningful comparisons between human and AI performance, offering a framework and checklist for improvement.


<details>
  <summary>Details</summary>
Motivation: Current human baselines in AI evaluations lack rigor and transparency, leading to unreliable claims of 'super-human' performance. This undermines trust and utility for researchers, users, and policymakers.

Method: The authors conduct a meta-review of measurement theory and AI evaluation literature to develop a framework and checklist for human baselines. They apply this checklist to review 115 studies.

Result: The review identifies shortcomings in existing baselining methods. The proposed checklist aids in designing, executing, and reporting human baselines more rigorously.

Conclusion: The paper aims to improve AI evaluation practices by promoting rigorous and transparent human baselines, benefiting both the research community and policymakers.

Abstract: In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines

</details>


### [228] [The NordDRG AI Benchmark for Large Language Models](https://arxiv.org/pdf/2506.13790)
*Tapio Pitkäranta*

Main category: cs.AI

TL;DR: NordDRG-AI-Benchmark is the first public benchmark for evaluating LLMs in hospital funding (DRG) tasks, highlighting domain-specific performance differences.


<details>
  <summary>Details</summary>
Motivation: To address the lack of open benchmarks for LLMs in clinical coding and decision support, specifically for hospital funding (DRG) tasks.

Method: The benchmark includes three artifact classes: DRG logic tables, expert manuals, and a prompt pack with 14 CaseMix tasks.

Result: Five LLMs were tested, with OpenAI's o3 scoring highest (9/9) and Gemini models performing poorly (5/9 and 3/9).

Conclusion: The benchmark reveals domain-specific LLM strengths/weaknesses, providing a reproducible baseline for trustworthy automation in hospital funding.

Abstract: Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.
  The benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.
  All artefacts are available at: https://github.com/longshoreforrest/norddrg-ai-benchmark
  A baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding.

</details>


### [229] [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/pdf/2506.13792)
*Gonçalo Hora de Carvalho, Lazar S. Popov, Sander Kaatee, Kristinn R. Thórisson, Tangrui Li, Pétur Húni Björnsson, Jilles S. Dibangoye*

Main category: cs.AI

TL;DR: ICE-ID is a new benchmark dataset for historical identity resolution, covering 220 years of Icelandic census records. It evaluates methods like rule-based matchers, ML ensembles, LLMs, and NARS, with NARS achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale, open datasets for long-term person-entity matching in historical populations, enabling reproducible research in identity resolution.

Method: The study compares handcrafted rule-based matchers, ML ensembles, LLMs, and NARS (a Non-Axiomatic Reasoning System) on the ICE-ID dataset, using defined metrics and splits.

Result: NARS, despite its simplicity, performs competitively and achieves state-of-the-art results in identity resolution tasks.

Conclusion: ICE-ID facilitates reproducible benchmarking and opens new research opportunities in data linkage and historical analytics.

Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.

</details>


### [230] [Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection](https://arxiv.org/pdf/2506.13793)
*Zongxian Yang, Jiayu Qian, Zegao Peng, Haoyu Zhang, Zhi-An Huang*

Main category: cs.AI

TL;DR: Med-REFL enhances medical reasoning by decomposing questions into fine-grained steps, evaluating reflections, and optimizing preferences, achieving up to 4.11% improvement on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models struggle in medical domains due to poor intermediate reflection quality, which is critical for high-stakes scenarios.

Method: Uses a tree-of-thought approach to break down medical questions, evaluates reasoning steps, and constructs preference optimization data automatically.

Result: Achieves up to 4.11% improvement on MedQA-USMLE and boosts 7B/8B models by 4.13%. Shows strong generalization and robustness.

Conclusion: Focusing on reflection quality improves accuracy and trustworthiness in medical AI, as demonstrated by Med-REFL.

Abstract: Large reasoning models have recently made significant strides in mathematical and code reasoning, yet their success has not transferred smoothly to the medical domain. While multiple factors contribute to this disparity, a critical issue is the inadequate focus on the quality of intermediate reflection steps, which is particularly crucial in high-stakes medical scenarios. To address this challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical \underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via self-corrected \underline{\textbf{F}}ine-grained ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought approach to decompose medical questions into fine-grained reasoning paths, quantitatively evaluating each step and its subsequent reflections. These assessments enable automatic construction of direct preference optimization data, reducing reliance on expensive expert annotations while guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark demonstrate Med-REFL achieves consistent improvements, with average gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong generalization capabilities and robustness across several challenging medical question-answering datasets. Our work illustrates that prioritizing reflection quality leads to more accurate and trustworthy reasoning in medical AI applications. Checkpoints, code, and data can be found \href{https://github.com/TianYin123/Med-REFL}{here}.

</details>


### [231] [BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection](https://arxiv.org/pdf/2506.13795)
*Boshen Shi, Yongqing Wang, Fangda Guo, Jiangli Shao, Huawei Shen, Xueqi Cheng*

Main category: cs.AI

TL;DR: BotTrans is a multi-source graph domain adaptation model designed to improve social bot detection by leveraging knowledge from multiple source networks, addressing challenges like network heterophily and single-source limitations.


<details>
  <summary>Details</summary>
Motivation: Label scarcity in detecting social bots and anomalies with GNNs motivates the need for effective knowledge transfer from relevant social networks. Challenges include network heterophily and single-source transfer instability.

Method: BotTrans establishes a cross-source-domain topology for homophily, aggregates cross-domain neighbor information, integrates source-target relevance, and refines detection using target domain semantics.

Result: BotTrans outperforms state-of-the-art methods in experiments, demonstrating its efficacy in leveraging multi-source knowledge for unlabeled target tasks.

Conclusion: BotTrans effectively addresses transfer challenges in social bot detection, proving superior performance by utilizing multi-source knowledge and domain adaptation.

Abstract: Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.

</details>


### [232] [Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization](https://arxiv.org/pdf/2506.13799)
*Soroush Vahidi*

Main category: cs.AI

TL;DR: A suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, tested on the FlyWire Connectome Challenge dataset, outperforms previous methods in maximizing forward edge weight.


<details>
  <summary>Details</summary>
Motivation: To reveal biologically meaningful feedforward structure in neural connectomes by minimizing feedback arcs in large-scale weighted directed graphs.

Method: Integrates greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components.

Result: The best solution improves forward edge weight over previous top-performing methods.

Conclusion: The algorithms are efficient, implemented in Python, and validated via cloud-based execution on Google Colab Pro+.

Abstract: We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.

</details>


### [233] [Causality in the human niche: lessons for machine learning](https://arxiv.org/pdf/2506.13803)
*Richard D. Lange, Konrad P. Kording*

Main category: cs.AI

TL;DR: The paper discusses the gap between human causal cognition and the Structural Causal Model (SCM) framework, advocating for more human-like causal reasoning in AI to improve generalization and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current machine learning systems lack human-like causal understanding, which limits their ability to generalize and learn efficiently. The SCM framework, while useful, doesn't fully capture human causal cognition.

Method: The paper critiques the SCM framework and explores how human causal cognition, such as analogical reasoning, is adaptive in the human niche.

Result: The SCM framework falls short in capturing human causal cognition, particularly in areas like analogical reasoning and generalization.

Conclusion: Future AI systems should incorporate more human-like causal reasoning to enhance capability, controllability, and interpretability.

Abstract: Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.

</details>


### [234] [Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study](https://arxiv.org/pdf/2506.13810)
*Olivier Saidi*

Main category: cs.AI

TL;DR: A framework leveraging structural patterns in NP-hard problems like TSP improves computational efficiency, achieving up to 79% gains in solution quality.


<details>
  <summary>Details</summary>
Motivation: Real-world instances of NP-hard problems often have exploitable patterns, unlike worst-case scenarios.

Method: Proposes a pattern-aware complexity framework with metrics like PUE and a meta-learning solver pipeline.

Result: Up to 79% solution quality gains in TSP benchmarks (22 to 2392 cities).

Conclusion: Offers a practical, unified approach for pattern-driven efficiency, distinct from theoretical NP-hardness.

Abstract: NP hard optimization problems like the Traveling Salesman Problem (TSP) defy efficient solutions in the worst case, yet real-world instances often exhibit exploitable patterns. We propose a novel patternaware complexity framework that quantifies and leverages structural regularities e.g., clustering, symmetry to reduce effective computational complexity across domains, including financial forecasting and LLM optimization. With rigorous definitions, theorems, and a meta learning driven solver pipeline, we introduce metrics like Pattern Utilization Efficiency (PUE) and achieve up to 79 percent solution quality gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP hardness, our approach offers a unified, practical lens for pattern-driven efficiency.

</details>


### [235] [The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness](https://arxiv.org/pdf/2506.13825)
*Gnankan Landry Regis N'guessan, Issa Karambal*

Main category: cs.AI

TL;DR: The paper introduces the Reflexive Integrated Information Unit (RIIU), a trainable module for artificial consciousness research, designed to be small, benchmarkable, and improvable.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a foundational, trainable module in artificial consciousness research, akin to the perceptron in neural networks.

Method: RIIU is a recurrent cell with a meta-state (μ) and broadcast buffer (B) to track and share causal footprints. It uses a sliding-window covariance and differentiable Auto-Φ surrogate for local information integration.

Result: RIIUs are differentiable, additive, and Φ-monotone under gradient ascent. In tests, a four-layer RIIU agent recovered >90% reward in 13 steps after actuator failure, outperforming GRUs.

Conclusion: RIIUs transform the philosophical debate on consciousness into an empirical, mathematical problem by scaling consciousness-like computation to unit level.

Abstract: Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $μ$ that records the cell's own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$Φ$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $Φ$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$Φ$ signal. By shrinking "consciousness-like" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.

</details>


### [236] [LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning](https://arxiv.org/pdf/2506.13841)
*Miho Koda, Yu Zheng, Ruixian Ma, Mingyang Sun, Devesh Pansare, Fabio Duarte, Paolo Santi*

Main category: cs.AI

TL;DR: The paper introduces LocationReasoner, a benchmark to test LLMs' reasoning in real-world site selection, revealing limitations in current models.


<details>
  <summary>Details</summary>
Motivation: To evaluate if LLMs' reasoning skills generalize beyond domains like math and coding to complex real-world scenarios.

Method: Developed a benchmark with 300 queries for site selection, using a sandbox environment and constraint-based tools.

Result: State-of-the-art models show limited improvement, with OpenAI o4 failing 30% of tasks; agentic strategies like ReAct underperform.

Conclusion: LLMs struggle with holistic, non-linear reasoning in real-world tasks; LocationReasoner is released to spur better model development.

Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.

</details>


### [237] [Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features](https://arxiv.org/pdf/2506.13917)
*Miguel A. Lago, Ghada Zamzmi, Brandon Eich, Jana G. Delfino*

Main category: cs.AI

TL;DR: The paper proposes a framework to evaluate AI explainability using four criteria: Consistency, Plausibility, Fidelity, and Usefulness, and applies it to medical imaging.


<details>
  <summary>Details</summary>
Motivation: There's a lack of techniques to assess the quality of AI explanations, especially in critical fields like medical devices.

Method: The framework evaluates explanations based on four criteria and uses a scorecard for comprehensive assessment. Case studies with Ablation CAM and Eigen CAM on synthetic mammographies illustrate the approach.

Result: The framework provides a structured way to evaluate explanation quality, demonstrated through clinical scenarios.

Conclusion: The proposed framework aims to improve AI explainability evaluation and foster better development of AI-based medical devices.

Abstract: Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.

</details>


### [238] [Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction](https://arxiv.org/pdf/2506.13920)
*Mbithe Nzomo, Deshendran Moodley*

Main category: cs.AI

TL;DR: A novel approach integrates knowledge graphs and Bayesian networks for explainable disease risk prediction using multimodal EHR data, demonstrated with atrial fibrillation.


<details>
  <summary>Details</summary>
Motivation: To adapt general medical knowledge to specific healthcare settings and patient populations while handling uncertainty and ensuring explainability in disease risk prediction.

Method: Constructs Bayesian networks from ontology-based knowledge graphs and multimodal EHR data.

Result: Balances general medical knowledge with patient-specific context, handles uncertainty, is explainable, and achieves good predictive performance.

Conclusion: The approach effectively integrates knowledge graphs and Bayesian networks for practical and explainable disease risk prediction.

Abstract: Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.

</details>


### [239] [ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users](https://arxiv.org/pdf/2506.13980)
*Shahaf David, Yair Meidan, Ido Hersko, Daniel Varnovitzky, Dudu Mimran, Yuval Elovici, Asaf Shabtai*

Main category: cs.AI

TL;DR: ProfiLLM is a framework for dynamic user profiling in chatbots, tested in IT/cybersecurity, showing rapid and accurate proficiency inference.


<details>
  <summary>Details</summary>
Motivation: Chatbots lack personalization, especially in specialized domains like IT/cybersecurity, where user knowledge varies widely. Existing methods rely on static or self-reported data, limiting adaptability.

Method: ProfiLLM uses a taxonomy and LLM-based profiling to dynamically infer user characteristics. ProfiLLM[ITSec] was tested on synthetic user conversations.

Result: ProfiLLM[ITSec] reduced the gap between actual and predicted proficiency scores by 55-65% after one prompt, with further refinement.

Conclusion: The framework effectively enables dynamic user profiling, with contributions including a taxonomy, simulation methodology, and dataset for future research.

Abstract: Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.

</details>


### [240] [SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine](https://arxiv.org/pdf/2506.13983)
*Adarsh Gupta, Bhabesh Mali, Chandan Karfa*

Main category: cs.AI

TL;DR: SANGAM is a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search to automate SVA generation from industry specs, outperforming recent methods.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs for complex and automatic Hardware Assertion Generation, addressing the need for robust SVAs in industry.

Method: A three-stage approach: multi-modal Specification Processing, Monte Carlo Tree Self-Refine (MCTSr) algorithm for reasoning, and combining traces to generate SVAs.

Result: SANGAM generates robust SVAs, outperforming recent methods in evaluation.

Conclusion: The framework effectively automates SVA generation, demonstrating superior performance.

Abstract: Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.

</details>


### [241] [Machine Mirages: Defining the Undefined](https://arxiv.org/pdf/2506.13990)
*Hamidou Tembine*

Main category: cs.AI

TL;DR: The paper discusses 'machine mirages,' a new class of cognitive aberrations in multimodal AI systems, and emphasizes the need for their explicit definition and systematic assessment to improve reliability and ethics.


<details>
  <summary>Details</summary>
Motivation: As AI systems achieve human-like fluency, they exhibit unique errors ('machine mirages') that mimic but differ from human fallibility, necessitating deeper understanding for ethical and reliable AI development.

Method: The article identifies and categorizes various machine mirages (e.g., hallucination, bias amplification) and argues for their systematic assessment.

Result: The paper highlights the prevalence of machine mirages and their implications for AI reliability and ethics.

Conclusion: Understanding and addressing machine mirages is crucial for building a co-evolving, ethical intelligence ecosystem that respects diverse forms of cognition and life.

Abstract: As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.

</details>


### [242] [Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning](https://arxiv.org/pdf/2506.14045)
*Martin Klissarov, Akhil Bagaria, Ziyan Luo, George Konidaris, Doina Precup, Marlos C. Machado*

Main category: cs.AI

TL;DR: The paper explores hierarchical reinforcement learning (HRL) as a solution for AI agents to navigate complex environments, focusing on defining good structure, its benefits, and methods for discovering it.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of developing AI agents capable of exploring, planning, and learning in open-ended environments by leveraging HRL.

Method: Analyzes HRL benefits, covers methods for temporal structure discovery (online, offline, LLMs), and identifies suitable domains.

Result: Highlights the impact of HRL on AI performance trade-offs and the challenges in temporal structure discovery.

Conclusion: HRL is promising but requires clearer definitions of good structure and tailored methods for different problem domains.

Abstract: Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.

</details>


### [243] [Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places](https://arxiv.org/pdf/2506.14070)
*Xinglei Wang, Tao Cheng, Stephen Law, Zichao Zeng, Ilya Ilyankou, Junyuan Liu, Lu Yin, Weiming Huang, Natchapon Jongwiriyanurak*

Main category: cs.AI

TL;DR: CaLLiPer, a multimodal framework, improves location prediction by combining spatial and semantic data, outperforming traditional methods, especially for new locations.


<details>
  <summary>Details</summary>
Motivation: Traditional location prediction methods lack spatial explicitness, semantic context, and adaptability to unseen locations, limiting their effectiveness.

Method: CaLLiPer uses contrastive learning to fuse spatial coordinates and semantic features of points of interest for robust location embeddings.

Result: CaLLiPer outperforms baselines in experiments, particularly in inductive scenarios with new locations.

Conclusion: Multimodal, inductive embeddings like CaLLiPer enhance mobility prediction, with potential for broader applications; code and data are shared for reproducibility.

Abstract: Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility and future research.

</details>


### [244] [FormGym: Doing Paperwork with Agents](https://arxiv.org/pdf/2506.14079)
*Matthew Toles, Rattandeep Singh, Isaac Song Zhou Yu*

Main category: cs.AI

TL;DR: A benchmark for form-filling in the image domain shows poor performance by baseline agents; FieldFinder tool improves accuracy significantly.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of form-filling in the pure-image domain without OCR or text access, which requires multi-modal understanding and tool-use.

Method: Introduces a benchmark with 432 fields across 55 documents and 3 tasks, and FieldFinder, a tool to assist LLMs in text placement.

Result: Baseline VLAs achieve <1% accuracy; GUI agents score 10.6-68.0%. FieldFinder boosts performance up to 56%.

Conclusion: FieldFinder significantly improves form-filling accuracy, highlighting the limitations of current agents in localization and tool-use.

Abstract: Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%.

</details>


### [245] [Lightweight Relevance Grader in RAG](https://arxiv.org/pdf/2506.14084)
*Taehee Jeong*

Main category: cs.AI

TL;DR: The paper introduces a lightweight relevance grader (finetuned llama-3.2-1b) for Retrieval-Augmented Generation (RAG) to improve document relevance, achieving precision comparable to larger models.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of ensuring document relevance in RAG systems by reducing computational costs while maintaining high precision.

Method: Finetuned llama-3.2-1b as a relevance grader to verify document relevance in RAG, comparing its performance to larger models like llama-3.1-70b.

Result: Precision improved from 0.1301 to 0.7750, matching the performance of llama-3.1-70b.

Conclusion: A lightweight model can effectively serve as a relevance grader in RAG, balancing accuracy and computational efficiency.

Abstract: Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.

</details>


### [246] [Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models](https://arxiv.org/pdf/2506.14092)
*Haonan Yin, Shai Vardi, Vidyanand Choudhary*

Main category: cs.AI

TL;DR: The paper investigates positional biases in LLMs during decision-making, revealing new biases like centrality bias and quality-dependent shifts, and proposes mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze and dissect positional biases in LLMs, which are increasingly used in high-stakes decision-making, and to understand their underlying preference structures.

Method: Comprehensive investigation across multiple LLM architectures and domains, introducing a framework to classify pairwise preferences and analyzing order effects.

Result: Uncovered strong positional biases, including centrality bias and quality-dependent shifts, and found that these biases can lead to selecting inferior options. Also identified biases favoring certain names.

Conclusion: LLMs exhibit unique biases not seen in human decision-making, requiring targeted mitigation strategies like adjusting the temperature parameter to reduce distortions.

Abstract: Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.

</details>


### [247] [Situational-Constrained Sequential Resources Allocation via Reinforcement Learning](https://arxiv.org/pdf/2506.14125)
*Libo Zhang, Yang Chen, Toru Takisaka, Kaiqi Zhao, Weidong Li, Jiamou Liu*

Main category: cs.AI

TL;DR: A novel framework, SCRL, addresses sequential resource allocation with situational constraints by formalizing constraints as logic implications and using a dynamic penalty algorithm. It outperforms baselines in efficiency and constraint satisfaction.


<details>
  <summary>Details</summary>
Motivation: Real-world resource allocation often involves context-dependent demands and priorities, requiring a method to handle situational constraints effectively.

Method: SCRL formalizes constraints as logic implications, introduces a dynamic penalty algorithm, and uses a probabilistic selection mechanism to improve upon traditional CRL approaches.

Result: SCRL outperforms existing baselines in scenarios like medical resource allocation and pesticide distribution, achieving high efficiency and constraint satisfaction.

Conclusion: SCRL is effective for real-world, context-sensitive decision-making tasks involving sequential resource allocation.

Abstract: Sequential Resource Allocation with situational constraints presents a significant challenge in real-world applications, where resource demands and priorities are context-dependent. This paper introduces a novel framework, SCRL, to address this problem. We formalize situational constraints as logic implications and develop a new algorithm that dynamically penalizes constraint violations. To handle situational constraints effectively, we propose a probabilistic selection mechanism to overcome limitations of traditional constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two scenarios: medical resource allocation during a pandemic and pesticide distribution in agriculture. Experiments demonstrate that SCRL outperforms existing baselines in satisfying constraints while maintaining high resource efficiency, showcasing its potential for real-world, context-sensitive decision-making tasks.

</details>


### [248] [Collaborative Editable Model](https://arxiv.org/pdf/2506.14146)
*Kaiwen Tang, Aitong Wu, Yao Lu, Guangda Sun*

Main category: cs.AI

TL;DR: CoEM introduces a collaborative, user-driven approach to adapt LLMs to vertical domains without heavy fine-tuning, using high-value knowledge fragments from user contributions.


<details>
  <summary>Details</summary>
Motivation: Traditional LLM training for vertical domains requires extensive annotated data and computational resources, hindering rapid development and iteration.

Method: CoEM builds a knowledge pool from user snippets, uses interactive dialogues and ratings to identify high-value knowledge, and injects it via in-context prompts for lightweight adaptation.

Result: In a financial scenario, CoEM improved domain-specific content generation with 15k user feedback, avoiding traditional fine-tuning costs.

Conclusion: CoEM offers an efficient, user-collaborative method for domain-specific LLM adaptation, reducing resource demands.

Abstract: Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows.

</details>


### [249] [What's in the Box? Reasoning about Unseen Objects from Multimodal Cues](https://arxiv.org/pdf/2506.14212)
*Lance Ying, Daniel Xu, Alicia Zhang, Katherine M. Collins, Max H. Siegel, Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: A neurosymbolic model integrates multimodal inputs using neural networks and Bayesian inference, outperforming unimodal and large neural models in correlating with human judgments in an object guessing game.


<details>
  <summary>Details</summary>
Motivation: To understand how humans flexibly integrate diverse information sources (e.g., auditory, visual, language, prior knowledge) to infer unseen objects.

Method: Proposes a neurosymbolic model: neural networks parse multimodal inputs, and a Bayesian model integrates information to evaluate hypotheses. Evaluated via the "What's in the Box?" game.

Result: The model strongly correlates with human judgments, unlike unimodal or large neural baselines.

Conclusion: The neurosymbolic approach effectively mimics human-like multimodal integration for inference tasks.

Abstract: People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.

</details>


### [250] [From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models](https://arxiv.org/pdf/2506.14224)
*Xinyang Li, Siqi Liu, Bochao Zou, Jiansheng Chen, Huimin Ma*

Main category: cs.AI

TL;DR: The paper introduces an interpretability-driven method to assess Theory of Mind (ToM) in multimodal large language models (MLLMs), using a new dataset (GridToM) and attention head analysis to enhance ToM capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing ToM evaluations for models are unimodal and lack interpretability, prompting the need for a deeper exploration of internal mechanisms in MLLMs.

Method: Constructs GridToM, a multimodal ToM test dataset, and analyzes attention heads to identify ToM capabilities. Proposes a training-free approach to enhance ToM by adjusting attention heads.

Result: Attention heads in MLLMs can distinguish cognitive information across perspectives, demonstrating ToM. The proposed method improves ToM performance.

Conclusion: The study provides an interpretability-driven framework for evaluating and enhancing ToM in MLLMs, highlighting the role of attention mechanisms.

Abstract: As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.

</details>


### [251] [ImpReSS: Implicit Recommender System for Support Conversations](https://arxiv.org/pdf/2506.14231)
*Omri Haller, Yair Meidan, Dudu Mimran, Yuval Elovici, Asaf Shabtai*

Main category: cs.AI

TL;DR: ImpReSS is an implicit recommender system for customer support chatbots, recommending solution product categories (SPCs) without assuming user intent, showing strong performance in evaluations.


<details>
  <summary>Details</summary>
Motivation: To enhance customer support by integrating implicit recommendations into conversations, addressing a gap in LLM-based conversational recommender systems.

Method: ImpReSS operates alongside support chatbots, identifying opportunities to recommend SPCs based on conversation context, without explicit user intent.

Result: High performance in recommending SPCs, with MRR@1 and recall@3 scores ranging from 0.72 to 0.85 across different support scenarios.

Conclusion: ImpReSS effectively integrates implicit recommendations into customer support, demonstrating potential for both issue resolution and business growth.

Abstract: Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.

</details>


### [252] [Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?](https://arxiv.org/pdf/2506.14239)
*Louis Vervoort, Vitaly Nikolaev*

Main category: cs.AI

TL;DR: A test for abstract causal reasoning in AI is proposed, using neuron diagrams from philosophy. Advanced LLMs like ChatGPT perform well, challenging the belief that defining causation in such diagrams is elusive.


<details>
  <summary>Details</summary>
Motivation: To evaluate AI's ability in abstract causal reasoning and propose a broader definition of causation in neuron diagrams.

Method: Using neuron diagrams from philosophy (D. Lewis) to test advanced LLMs (ChatGPT, DeepSeek, Gemini).

Result: LLMs correctly identify debated causes, suggesting a broader definition of causation is possible.

Conclusion: Future philosophical research may involve collaboration between human and AI expertise.

Abstract: We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.

</details>


### [253] [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/pdf/2506.14245)
*Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, Jiang Bian, Mao Yang*

Main category: cs.AI

TL;DR: RLVR improves LLM reasoning but underperforms on Pass@K due to flawed metrics. Introducing CoT-Pass@K shows RLVR's true potential by ensuring correct reasoning paths and answers.


<details>
  <summary>Details</summary>
Motivation: Address the paradox where RLVR-tuned models underperform on Pass@K, suggesting flawed evaluation metrics and aiming to validate RLVR's reasoning benefits.

Method: Introduce CoT-Pass@K to evaluate reasoning paths and answers. Provide theoretical foundation for RLVR's logical integrity and analyze training dynamics.

Result: RLVR excels with CoT-Pass@K, showing early and smooth generalization of correct reasoning.

Conclusion: RLVR genuinely advances machine reasoning when evaluated properly, offering reliable metrics and insights.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.

</details>


### [254] [Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents](https://arxiv.org/pdf/2506.14246)
*Lingfeng Li, Yunlong Lu, Yongyi Wang, Qifan Zheng, Wenxin Li*

Main category: cs.AI

TL;DR: The paper introduces Mxplainer, a method to convert black-box Mahjong AI agents into interpretable neural networks, providing insights into their play styles and decision-making.


<details>
  <summary>Details</summary>
Motivation: To help people learn from AI agents by making their decision-making processes in Mahjong (a complex game with imperfect information) understandable.

Method: Mxplainer, a parameterized search algorithm converted into a neural network, learns and explains black-box AI agent parameters.

Result: The learned parameters offer human-understandable insights into AI agents' play styles, and the framework explains their decisions for most game states.

Conclusion: Mxplainer successfully bridges the gap between black-box AI agents and human understanding, enabling better learning from AI in Mahjong.

Abstract: People need to internalize the skills of AI agents to improve their own capabilities. Our paper focuses on Mahjong, a multiplayer game involving imperfect information and requiring effective long-term decision-making amidst randomness and hidden information. Through the efforts of AI researchers, several impressive Mahjong AI agents have already achieved performance levels comparable to those of professional human players; however, these agents are often treated as black boxes from which few insights can be gleaned. This paper introduces Mxplainer, a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents. Experiments conducted on AI and human player data demonstrate that the learned parameters provide human-understandable insights into these agents' characteristics and play styles. In addition to analyzing the learned parameters, we also showcase how our search-based framework can locally explain the decision-making processes of black-box agents for most Mahjong game states.

</details>


### [255] [Don't throw the baby out with the bathwater: How and why deep learning for ARC](https://arxiv.org/pdf/2506.14276)
*Jack Cole, Mohamed Osman*

Main category: cs.AI

TL;DR: The paper demonstrates that deep learning, combined with on-the-fly NN training and novel techniques like TTFT and AIRV, achieves state-of-the-art performance on the ARC-AGI challenge, boosting accuracy significantly.


<details>
  <summary>Details</summary>
Motivation: To address the low performance of AI systems on the ARC-AGI challenge by leveraging deep learning's ability to acquire novel abstractions.

Method: Incorporates on-the-fly NN training, Test-Time Fine-Tuning (TTFT), and Augment Inference Reverse-Augmentation and Vote (AIRV) techniques, starting from pretrained LLMs.

Result: Achieved a 260% accuracy boost with AIRV and a further 300% with TTFT, securing first place in the 2023 ARCathon and the best score (58%) on the ARC private test-set.

Conclusion: Deep learning, when fully committed to acquiring novel abstractions, is effective for ARC, highlighting key mechanisms for robust reasoning in unfamiliar domains.

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.

</details>


### [256] [ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems](https://arxiv.org/pdf/2506.14299)
*Fanzhi Zeng, Siqi Wang, Chuzhao Zhu, Li Li*

Main category: cs.AI

TL;DR: The paper introduces ADRD, a framework using LLMs to create interpretable, rule-based autonomous driving systems, outperforming traditional methods in interpretability, speed, and performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of building interpretable autonomous driving decision-making systems by leveraging LLMs' reasoning and programming capabilities.

Method: ADRD integrates three modules: Information (aggregates driving scenarios), Agents (generates rule-based tactics), and Testing (refines tactics iteratively).

Result: ADRD outperforms reinforcement learning and advanced LLM-based methods in interpretability, response speed, and driving performance.

Conclusion: The framework demonstrates the potential of LLM-driven, rule-based systems for real-world autonomous driving, offering transparency and adaptability.

Abstract: How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.

</details>


### [257] [AviationLLM: An LLM-based Knowledge System for Aviation Training](https://arxiv.org/pdf/2506.14336)
*Jia'ang Wan, Feng Shen, Fujuan Li, Yanjin Sun, Yan Li, Shiwen Zhang*

Main category: cs.AI

TL;DR: The paper introduces RALA-DPO, a method combining Direct Preference Optimization (DPO) and Retrieval-Augmented Generation (RAG) to enhance aviation training by improving LLM accuracy in professional knowledge responses.


<details>
  <summary>Details</summary>
Motivation: Current aviation training relies on limited instructors and unreliable online answers, leading to inefficiency. Basic pre-trained LLMs lack accuracy in professional fields, and traditional fine-tuning risks incorrect responses.

Method: The proposed RALA-DPO fine-tunes the Qwen LLM using DPO for domain alignment and integrates RAG to mitigate hallucinations from data biases or gaps.

Result: Experiments show RALA-DPO improves accuracy in aviation knowledge responses and enables zero-cost knowledge updates via RAG.

Conclusion: RALA-DPO effectively addresses training inefficiencies by combining DPO and RAG, enhancing accuracy and adaptability in aviation theory training.

Abstract: Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously.

</details>


### [258] [Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning](https://arxiv.org/pdf/2506.14387)
*William F. Shen, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane*

Main category: cs.AI

TL;DR: SEAT is a fine-tuning method for LLMs that preserves ignorance awareness and performance by using sparse training and entity perturbation with KL-divergence regularization.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the degradation of safety alignment capabilities like expressing ignorance during fine-tuning, leading to issues like hallucinations.

Method: SEAT combines sparse training to limit activation drift and entity perturbation with KL-divergence regularization to prevent knowledge entanglement.

Result: SEAT outperforms baselines in maintaining ignorance awareness and fine-tuning performance.

Conclusion: SEAT provides a robust solution for LLM fine-tuning by balancing performance and safety alignment.

Abstract: Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.

</details>


### [259] [AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection](https://arxiv.org/pdf/2506.14470)
*Zixian Zhang, Takfarinas Saber*

Main category: cs.AI

TL;DR: The paper evaluates hybrid AST-based graph representations (CFG, DFG, FA-AST) in GNNs for code clone detection, finding AST+CFG+DFG improves accuracy but FA-AST can harm performance. GMN excels even with standard AST.


<details>
  <summary>Details</summary>
Motivation: Code clones increase maintenance costs and risks, but ASTs lack semantic depth. Hybrid AST-based representations with semantic graphs (CFG, DFG) aim to improve detection, but their effectiveness and compatibility with GNNs are unclear.

Method: Conducts an empirical study comparing AST-based hybrid representations (CFG, DFG, FA-AST) across GNN architectures (GCN, GAT, GMN) for code clone detection.

Result: AST+CFG+DFG boosts accuracy for GCN and GAT, while FA-AST often degrades performance. GMN performs best even with standard AST, reducing the need for enriched structures.

Conclusion: Hybrid representations vary in effectiveness; GMN's superior performance with standard AST suggests simpler structures may suffice for certain GNNs in code clone detection.

Abstract: As one of the most detrimental code smells, code clones significantly increase software maintenance costs and heighten vulnerability risks, making their detection a critical challenge in software engineering. Abstract Syntax Trees (ASTs) dominate deep learning-based code clone detection due to their precise syntactic structure representation, but they inherently lack semantic depth. Recent studies address this by enriching AST-based representations with semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs (DFGs). However, the effectiveness of various enriched AST-based representations and their compatibility with different graph-based machine learning techniques remains an open question, warranting further investigation to unlock their full potential in addressing the complexities of code clone detection. In this paper, we present a comprehensive empirical study to rigorously evaluate the effectiveness of AST-based hybrid graph representations in Graph Neural Network (GNN)-based code clone detection. We systematically compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs (FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid representations impact GNNs differently: while AST+CFG+DFG consistently enhances accuracy for convolution- and attention-based models (Graph Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST frequently introduces structural complexity that harms performance. Notably, GMN outperforms others even with standard AST representations, highlighting its superior cross-code similarity detection and reducing the need for enriched structures.

</details>


### [260] [GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/pdf/2506.14477)
*Jingqi Yang, Zhilong Song, Jiawei Chen, Mingli Song, Sheng Zhou, linjun sun, Xiaogang Ouyang, Chun Chen, Can Wang*

Main category: cs.AI

TL;DR: GUI-Robust is a new dataset for evaluating GUI agents, incorporating real-world anomalies and reducing annotation time by 19x using a semi-automated method.


<details>
  <summary>Details</summary>
Motivation: Existing GUI datasets lack real-world anomalies, limiting their practical utility for benchmarking GUI agents.

Method: A semi-automated paradigm using RPA tools and MLLMs to collect user actions and generate descriptions, reducing annotation time.

Result: State-of-the-art GUI agents show significant performance drops in abnormal scenarios when tested on GUI-Robust.

Conclusion: GUI-Robust emphasizes the need for robust GUI agents and provides a resource for future research.

Abstract: The development of high-quality datasets is crucial for benchmarking and advancing research in Graphical User Interface (GUI) agents. Despite their importance, existing datasets are often constructed under idealized conditions, overlooking the diverse anomalies frequently encountered in real-world deployments. To address this limitation, we introduce GUI-Robust, a novel dataset designed for comprehensive GUI agent evaluation, explicitly incorporating seven common types of anomalies observed in everyday GUI interactions. Furthermore, we propose a semi-automated dataset construction paradigm that collects user action sequences from natural interactions via RPA tools and then generate corresponding step and task descriptions for these actions with the assistance of MLLMs. This paradigm significantly reduces annotation time cost by a factor of over 19 times. Finally, we assess state-of-the-art GUI agents using the GUI-Robust dataset, revealing their substantial performance degradation in abnormal scenarios. We anticipate that our work will highlight the importance of robustness in GUI agents and inspires more future research in this direction. The dataset and code are available at https://github.com/chessbean1/GUI-Robust..

</details>


### [261] [LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?](https://arxiv.org/pdf/2506.14496)
*Muhammad Atta Ur Rahman, Melanie Schranz*

Main category: cs.AI

TL;DR: The paper compares traditional swarm intelligence (e.g., Boids, ACO) with LLM-driven swarms, analyzing decentralization, scalability, and emergence in AI. It evaluates performance and resource usage, highlighting LLMs' potential and challenges.


<details>
  <summary>Details</summary>
Motivation: To explore how modern AI (LLMs) redefines traditional swarm concepts like decentralization and emergence, and to assess their practical feasibility.

Method: Implemented and compared traditional swarm algorithms (Boids, ACO) with LLM-driven swarms, evaluating latency, resource usage, and behavioral accuracy. Also assessed cloud-based vs. local LLMs.

Result: LLMs offer advanced reasoning but introduce computational and coordination challenges, impacting traditional swarm design.

Conclusion: LLMs expand swarm capabilities but pose new constraints, reshaping the definition of 'swarm' in AI research.

Abstract: Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.

</details>


### [262] [Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow](https://arxiv.org/pdf/2506.14502)
*Xiao Wang, Junru Yu, Jun Huang, Qiong Wu, Ljubo Vacic, Changyin Sun*

Main category: cs.AI

TL;DR: The paper proposes a safety-first human-like decision-making framework (SF-HLDM) for autonomous vehicles to navigate dense, interactive traffic by combining intention inference, social compliance, and efficient policy search.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles struggle in dynamic, dense traffic due to human unpredictability and data-driven methods' limitations, necessitating a robust, adaptable decision-making framework.

Method: SF-HLDM integrates spatial-temporal attention for intention inference, social compliance estimation, and Deep Evolutionary Reinforcement Learning (DERL) for flexible, efficient policy search.

Result: The framework enables AVs to dynamically adjust decisions, ensuring safety, comfort, and social compatibility while avoiding local optima and overfitting.

Conclusion: SF-HLDM enhances AV decision-making with interpretability, flexibility, and adaptability in complex traffic scenarios.

Abstract: Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.

</details>


### [263] [Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/pdf/2506.14539)
*Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, Meong Hi Son*

Main category: cs.AI

TL;DR: The paper introduces the 'Doppelgänger method' to highlight risks of prompt hijacking in autonomous agents, proposes 'PACAT' to measure vulnerability, and suggests 'CAT' prompts for defense.


<details>
  <summary>Details</summary>
Motivation: Addressing safety and robustness concerns in prompt-based autonomous agents due to potential hijacking and information exposure.

Method: Introduces the Doppelgänger method to demonstrate hijacking risks, defines PACAT for vulnerability assessment, and proposes CAT prompts for defense.

Result: Doppelgänger method successfully compromises agent consistency and exposes internal info; CAT prompts effectively defend against such attacks.

Conclusion: The study underscores the need for robust defenses like CAT prompts to mitigate adversarial risks in prompt-based autonomous agents.

Abstract: Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelgänger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelgänger method. The experimental results demonstrate that the Doppelgänger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.

</details>


### [264] [QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents](https://arxiv.org/pdf/2506.14568)
*Eliott Thomas, Mickael Coustaty, Aurelie Joseph, Gaspar Deloin, Elodie Carel, Vincent Poulain D'Andecy, Jean-Marc Ogier*

Main category: cs.AI

TL;DR: QUEST is a quality-aware semi-supervised framework for table extraction from business documents, improving accuracy and reducing errors by leveraging structural and contextual quality assessments.


<details>
  <summary>Details</summary>
Motivation: Automating table extraction from business documents is challenging due to sparse annotations and unreliable multi-stage pipelines. Existing semi-supervised methods rely on poor confidence metrics.

Method: QUEST introduces a quality assessment model predicting F1 scores for extracted tables, guiding pseudo-label selection with diversity measures to avoid bias.

Result: On proprietary and DocILE datasets, QUEST improved F1 scores (64% to 74% and 42% to 50%) and reduced empty predictions (12% to 6.5% and 27% to 22%).

Conclusion: QUEST's interpretable quality assessments and robustness to sparse annotations make it effective for business documents, ensuring structural consistency and data completeness.

Abstract: Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.

</details>


### [265] [Enhancing Symbolic Machine Learning by Subsymbolic Representations](https://arxiv.org/pdf/2506.14569)
*Stephen Roth, Lennart Baur, Derian Boer, Stefan Kramer*

Main category: cs.AI

TL;DR: The paper proposes enhancing symbolic machine learning (e.g., TILDE) with neural embeddings for constants, improving performance over baselines in discriminative tasks.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies of neuro-symbolic systems like LTNs and DeepProbLog in simpler settings, especially with many constants.

Method: Enhance symbolic learners (TILDE) by integrating neural embeddings for constants, refining them based on symbolic theory.

Result: Outperforms baseline methods in F1 score across three real-world domains.

Conclusion: The approach is effective and extensible to tasks like analogical reasoning or propositionalization.

Abstract: The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI approaches, to overcome the limitations of either. Prominent systems include Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and end-to-end learning. The versatility of systems like LTNs and DeepProbLog, however, makes them less efficient in simpler settings, for instance, for discriminative machine learning, in particular in domains with many constants. Therefore, we follow a different approach: We propose to enhance symbolic machine learning schemes by giving them access to neural embeddings. In the present paper, we show this for TILDE and embeddings of constants used by TILDE in similarity predicates. The approach can be fine-tuned by further refining the embeddings depending on the symbolic theory. In experiments in three real-world domain, we show that this simple, yet effective, approach outperforms all other baseline methods in terms of the F1 score. The approach could be useful beyond this setting: Enhancing symbolic learners in this way could be extended to similarities between instances (effectively working like kernels within a logical language), for analogical reasoning, or for propositionalization.

</details>


### [266] [From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places](https://arxiv.org/pdf/2506.14570)
*Mohammad Hashemi, Andreas Zufle*

Main category: cs.AI

TL;DR: The paper advocates for spatial foundation models to generalize human mobility data, addressing gaps in adaptability, scalability, and multi-granular reasoning for geospatial intelligence.


<details>
  <summary>Details</summary>
Motivation: Current foundation models lack the capability to handle the spatial, temporal, and semantic complexity of mobility data, limiting scalable and transferable analysis across diverse contexts.

Method: Proposes a shift from modeling discrete points of interest to understanding dynamic, context-rich regions (places) shaped by human behavior and mobility.

Result: Identifies key gaps and proposes research directions for scalable, context-aware models.

Conclusion: The vision is to develop next-generation geospatial intelligence models for applications like urban planning and logistics optimization.

Abstract: Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.

</details>


### [267] [AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes](https://arxiv.org/pdf/2506.14728)
*Jiahao Qiu, Xinzhe Juan, Yimin Wang, Ling Yang, Xuan Qi, Tongcheng Zhang, Jiacheng Guo, Yifu Lu, Zixin Yao, Hongru Wang, Shilong Liu, Xun Jiang, Liu Leqi, Mengdi Wang*

Main category: cs.AI

TL;DR: AgentDistill is a training-free framework for distilling LLM-based agents by reusing structured task-solving modules (MCPs) from teachers, enabling scalable and efficient knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Existing agent distillation methods struggle with dynamic planning in novel environments, limiting their effectiveness.

Method: AgentDistill reuses Model-Context-Protocols (MCPs) from teacher agents for training-free knowledge transfer.

Result: Distilled student agents achieve performance comparable to advanced systems using large LLMs, as shown in biomedical and mathematical benchmarks.

Conclusion: AgentDistill offers a scalable and cost-efficient solution for building intelligent agents without extensive training.

Abstract: While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.

</details>


### [268] [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/pdf/2506.14755)
*Zhengxiang Cheng, Dongping Chen, Mingyang Fu, Tianyi Zhou*

Main category: cs.AI

TL;DR: LC-R1 is a post-training method for Large Reasoning Models (LRMs) that reduces verbose reasoning chains by 50% with minimal accuracy loss (~2%), using Brevity and Sufficiency principles.


<details>
  <summary>Details</summary>
Motivation: LRMs often produce unnecessary reasoning steps ('invalid thinking'), leading to inefficiency.

Method: Introduces LC-R1, a method using Group Relative Policy Optimization (GRPO) with Length and Compress Rewards to eliminate redundancy.

Result: Achieves ~50% reduction in reasoning chain length with only ~2% accuracy drop.

Conclusion: LC-R1 offers a robust trade-off between conciseness and accuracy, advancing efficient LRM development.

Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.

</details>


### [269] [OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models](https://arxiv.org/pdf/2310.07637)
*Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li, Gaogang Xie, Xidao Wen, Xiaohui Nie, Minghua Ma, Dan Pei*

Main category: cs.AI

TL;DR: OpsEval is a benchmark for evaluating LLMs in AIOps tasks, featuring multi-choice and QA formats in English and Chinese. It assesses LLM performance in Ops scenarios, discusses model techniques, and addresses evaluation credibility through expert review and partial open-sourcing.


<details>
  <summary>Details</summary>
Motivation: The rise of AIOps and the potential of LLMs in IT operations necessitate a standardized benchmark to evaluate their performance in critical Ops tasks.

Method: OpsEval includes 7184 multi-choice and 1736 QA questions, reviewed by experts, with 20% open-sourced for preliminary evaluations. An online leaderboard tracks real-time LLM performance.

Result: The benchmark evaluates leading LLMs, revealing the impact of techniques like model quantification and addressing hallucination issues.

Conclusion: OpsEval provides a credible, comprehensive tool for assessing LLMs in AIOps, with ongoing updates to include new models.

Abstract: Information Technology (IT) Operations (Ops), particularly Artificial Intelligence for IT Operations (AIOps), is the guarantee for maintaining the orderly and stable operation of existing information systems. According to Gartner's prediction, the use of AI technology for automated IT operations has become a new trend. Large language models (LLMs) that have exhibited remarkable capabilities in NLP-related tasks, are showing great potential in the field of AIOps, such as in aspects of root cause analysis of failures, generation of operations and maintenance scripts, and summarizing of alert information. Nevertheless, the performance of current LLMs in Ops tasks is yet to be determined. In this paper, we present OpsEval, a comprehensive task-oriented Ops benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in various crucial scenarios at different ability levels. The benchmark includes 7184 multi-choice questions and 1736 question-answering (QA) formats in English and Chinese. By conducting a comprehensive performance evaluation of the current leading large language models, we show how various LLM techniques can affect the performance of Ops, and discussed findings related to various topics, including model quantification, QA evaluation, and hallucination issues. To ensure the credibility of our evaluation, we invite dozens of domain experts to manually review our questions. At the same time, we have open-sourced 20% of the test QA to assist current researchers in preliminary evaluations of their OpsLLM models. The remaining 80% of the data, which is not disclosed, is used to eliminate the issue of the test set leakage. Additionally, we have constructed an online leaderboard that is updated in real-time and will continue to be updated, ensuring that any newly emerging LLMs will be evaluated promptly. Both our dataset and leaderboard have been made public.

</details>


### [270] [Learning Traffic Signal Control via Genetic Programming](https://arxiv.org/pdf/2403.17328)
*Xiao-Cheng Liao, Yi Mei, Mengjie Zhang*

Main category: cs.AI

TL;DR: The paper proposes a learning-based method for traffic signal control using phase urgency and genetic programming, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current DRL-based traffic signal control methods require extensive domain knowledge for reward design and lack explainability.

Method: Introduces phase urgency for signal transitions, represented as an explainable tree structure optimized via genetic programming.

Result: The proposed method outperforms state-of-the-art and DRL-based baselines on public datasets.

Conclusion: The tree-shaped urgency function offers an efficient and explainable solution for traffic signal control.

Abstract: The control of traffic signals is crucial for improving transportation efficiency. Recently, learning-based methods, especially Deep Reinforcement Learning (DRL), garnered substantial success in the quest for more efficient traffic signal control strategies. However, the design of rewards in DRL highly demands domain knowledge to converge to an effective policy, and the final policy also presents difficulties in terms of explainability. In this work, a new learning-based method for signal control in complex intersections is proposed. In our approach, we design a concept of phase urgency for each signal phase. During signal transitions, the traffic light control strategy selects the next phase to be activated based on the phase urgency. We then proposed to represent the urgency function as an explainable tree structure. The urgency function can calculate the phase urgency for a specific phase based on the current road conditions. Genetic programming is adopted to perform gradient-free optimization of the urgency function. We test our algorithm on multiple public traffic signal control datasets. The experimental results indicate that the tree-shaped urgency function evolved by genetic programming outperforms the baselines, including a state-of-the-art method in the transportation field and a well-known DRL-based method. Our code is available online.

</details>


### [271] [Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries](https://arxiv.org/pdf/2405.20653)
*Jiahao Yu, Haozheng Luo, Jerry Yao-Chieh Hu, Wenbo Guo, Han Liu, Xinyu Xing*

Main category: cs.AI

TL;DR: Appending multiple end-of-sequence (eos) tokens to inputs causes context segmentation, weakening LLM alignment and boosting jailbreak attacks. Major commercial APIs are also vulnerable.


<details>
  <summary>Details</summary>
Motivation: To expose a subtle weakness in aligned LLMs where eos tokens disrupt model behavior, enabling jailbreak attacks.

Method: Propose appending eos tokens to inputs, evaluate attack success across models and techniques, and probe commercial APIs.

Result: Significant increase in jailbreak success rates; major APIs are vulnerable due to unfiltered eos tokens.

Conclusion: Highlights a critical blind spot in alignment, calling for improved defenses and robust refusal boundaries.

Abstract: Recent advances in Large Language Models (LLMs) have led to impressive alignment where models learn to distinguish harmful from harmless queries through supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). In this paper, we reveal a subtle yet impactful weakness in these aligned models. We find that simply appending multiple end of sequence (eos) tokens can cause a phenomenon we call context segmentation, which effectively shifts both harmful and benign inputs closer to the refusal boundary in the hidden space.
  Building on this observation, we propose a straightforward method to BOOST jailbreak attacks by appending eos tokens. Our systematic evaluation shows that this strategy significantly increases the attack success rate across 8 representative jailbreak techniques and 16 open-source LLMs, ranging from 2B to 72B parameters. Moreover, we develop a novel probing mechanism for commercial APIs and discover that major providers such as OpenAI, Anthropic, and Qwen do not filter eos tokens, making them similarly vulnerable. These findings highlight a hidden yet critical blind spot in existing alignment and content filtering approaches.
  We call for heightened attention to eos tokens' unintended influence on model behaviors, particularly in production systems. Our work not only calls for an input-filtering based defense, but also points to new defenses that make refusal boundaries more robust and generalizable, as well as fundamental alignment techniques that can defend against context segmentation attacks.

</details>


### [272] [Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers](https://arxiv.org/pdf/2406.14986)
*Manuel Mondal, Ljiljana Dolamic, Gérôme Bovet, Philippe Cudré-Mauroux, Julien Audiffren*

Main category: cs.AI

TL;DR: The paper investigates whether LLMs' probabilistic reasoning abilities, observed in MCQ evaluations, hold in direct text-completion tasks. It introduces Revealed Belief, a framework analyzing LLMs' probability distributions, revealing inconsistencies and biases in their reasoning under uncertainty.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs' apparent reasoning abilities in MCQs extend to their fundamental text-completion tasks, as MCQ evaluations may not fully capture their capabilities.

Method: Reformulate reasoning tasks as direct text-completion and introduce Revealed Belief, a framework analyzing LLMs' probability distributions in these tasks.

Result: LLMs often state correct answers in MCQs but show inconsistent probability allocations, biases, and poor belief updating in Revealed Belief analysis.

Conclusion: Common MCQ evaluations may be incomplete; further research is needed to understand LLMs' true reasoning capabilities.

Abstract: Multiple Choice Questions (MCQ) have become a commonly used approach to assess the capabilities of Large Language Models (LLMs), due to their ease of manipulation and evaluation. The experimental appraisals of the LLMs' Stated Answer (their answer to MCQ) have pointed to their apparent ability to perform probabilistic reasoning or to grasp uncertainty. In this work, we investigate whether these aptitudes are measurable outside tailored prompting and MCQ by reformulating these issues as direct text-completion - the fundamental computational unit of LLMs. We introduce Revealed Belief, an evaluation framework that evaluates LLMs on tasks requiring reasoning under uncertainty, which complements MCQ scoring by analyzing text-completion probability distributions. Our findings suggest that while LLMs frequently state the correct answer, their Revealed Belief shows that they often allocate probability mass inconsistently, exhibit systematic biases, and often fail to update their beliefs appropriately when presented with new evidence, leading to strong potential impacts on downstream tasks. These results suggest that common evaluation methods may only provide a partial picture and that more research is needed to assess the extent and nature of their capabilities.

</details>


### [273] [Controllable and Reliable Knowledge-Intensive Task-Oriented Conversational Agents with Declarative Genie Worksheets](https://arxiv.org/pdf/2407.05674)
*Harshit Joshi, Shicheng Liu, James Chen, Robert Weigle, Monica S. Lam*

Main category: cs.AI

TL;DR: Genie is a programmable framework for creating knowledge-intensive task-oriented conversational agents, addressing LLM shortcomings like hallucination and conditional logic. It outperforms SOTA methods and GPT-4 Turbo in real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based conversational agents struggle with hallucination, conditional logic, and knowledge integration, limiting their effectiveness.

Method: Genie uses advanced dialogue state management and declarative specifications (Genie Worksheet) to control agent policies, limiting LLMs to parsing input and generating context-bound responses.

Result: Genie agents achieved an 82.8% goal completion rate in real-world tasks, significantly outperforming GPT-4 Turbo (21.8%).

Conclusion: Genie provides a reliable, programmable solution for building effective task-oriented conversational agents, overcoming LLM limitations.

Abstract: Large Language Models can carry out human-like conversations in diverse settings, responding to user requests for tasks and knowledge. However, existing conversational agents implemented with LLMs often struggle with hallucination, following instructions with conditional logic, and integrating knowledge from different sources. These shortcomings compromise the agents' effectiveness, rendering them unsuitable for deployment. To address these challenges, we introduce Genie, a programmable framework for creating knowledge-intensive task-oriented conversational agents. Genie can handle involved interactions and answer complex queries. Unlike LLMs, it delivers reliable, grounded responses through advanced dialogue state management and supports controllable agent policies via its declarative specification -- Genie Worksheet. This is achieved through an algorithmic runtime system that implements the developer-supplied policy, limiting LLMs to (1) parse user input using a succinct conversational history, and (2) generate responses according to supplied context. Agents built with Genie outperform SOTA methods on complex logic dialogue datasets. We conducted a user study with 62 participants on three real-life applications: restaurant reservations with Yelp, as well as ticket submission and course enrollment for university students. Genie agents with GPT-4 Turbo outperformed the GPT-4 Turbo agents with function calling, improving goal completion rates from 21.8% to 82.8% across three real-world tasks.

</details>


### [274] [Decoupling Generation and Evaluation for Parallel Greedy Best-First Search(extended version)](https://arxiv.org/pdf/2408.05682)
*Takumi Shimoda, Alex Fukunaga*

Main category: cs.AI

TL;DR: Improved constrained parallel search by decoupling state generation and evaluation, enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of threads waiting idly in constrained parallel greedy best-first search due to costly constraint enforcement.

Method: Decouples state generation and state evaluation to improve evaluation rate.

Result: Significantly better search performance.

Conclusion: The proposed improvement enhances efficiency in constrained parallel search.

Abstract: In order to understand and control the search behavior of parallel search, recent work has proposed a class of constrained parallel greedy best-first search algorithms which only expands states that satisfy some constraint.However, enforcing such constraints can be costly, as threads must be waiting idly until a state that satisfies the expansion constraint is available. We propose an improvement to constrained parallel search which decouples state generation and state evaluation and significantly improves state evaluation rate, resulting in better search performance.

</details>


### [275] [A Unified Framework for Next-Gen Urban Forecasting via LLM-driven Dependency Retrieval and GeoTransformer](https://arxiv.org/pdf/2408.08852)
*Yuhao Jia, Zile Wu, Shengao Yi, Yifei Sun, Xiao Huang*

Main category: cs.AI

TL;DR: A unified framework for urban forecasting integrates graph-based and region-based methods, overcoming their limitations with modular components for representation, dependency retrieval, and prediction.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on structured spatial data or lack adaptability and holistic context integration, creating a need for a unified approach.

Method: The framework includes an Urban Region Representation Module, Task-aware Dependency Retrieval module, and Prediction Module (GeoTransformer) with geospatial attention.

Result: Demonstrated strong generalization across six urban forecasting tasks, validating effectiveness.

Conclusion: The proposed framework successfully integrates and improves upon existing methods, offering modularity and adaptability.

Abstract: Urban forecasting has increasingly benefited from high-dimensional spatial data through two primary approaches: graph-based methods that rely on predefined spatial structures, and region-based methods that focus on learning expressive urban representations. Although these methods have laid a strong foundation, they either rely heavily on structured spatial data, struggle to adapt to task-specific dependencies, or fail to integrate holistic urban context. Moreover, no existing framework systematically integrates these two paradigms and overcomes their respective limitations. To address this gap, we propose a novel, unified framework for high-dimensional urban forecasting, composed of three key components: (1) the Urban Region Representation Module that organizes latent embeddings and semantic descriptions for each region, (2) the Task-aware Dependency Retrieval module that selects relevant context regions based on natural language prompts, and (3) the Prediction Module, exemplified by our proposed GeoTransformer architecture, which adopts a novel geospatial attention mechanism to incorporate spatial proximity and information entropy as priors. Our framework is modular, supports diverse representation methods and forecasting models, and can operate even with minimal input. Quantitative experiments and qualitative analysis across six urban forecasting tasks demonstrate strong task generalization and validate the framework's effectiveness.

</details>


### [276] [Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents](https://arxiv.org/pdf/2410.05243)
*Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su*

Main category: cs.AI

TL;DR: The paper introduces UGround, a visual grounding model for GUI agents, outperforming text-based methods by up to 20% and enabling human-like interaction with GUIs.


<details>
  <summary>Details</summary>
Motivation: Current GUI agents rely on noisy, incomplete text-based representations (e.g., HTML), limiting their robustness. The paper advocates for visual grounding to mimic human-like interaction.

Method: Proposes UGround, trained on a synthetic web dataset (10M GUI elements, 1.3M screenshots) using a modified LLaVA architecture for visual grounding.

Result: UGround outperforms existing models by up to 20% and enables agents to surpass state-of-the-art performance using only visual input.

Conclusion: Visual grounding is feasible and promising for GUI agents, enabling human-like navigation of digital interfaces.

Abstract: Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly perform pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.

</details>


### [277] [Robust Multi-bit Text Watermark with LLM-based Paraphrasers](https://arxiv.org/pdf/2412.03123)
*Xiaojun Xu, Jinghan Jia, Yuanshun Yao, Yang Liu, Hang Li*

Main category: cs.AI

TL;DR: The paper introduces a multi-bit text watermarking method using LLM paraphrasers to embed and decode watermarks with high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To create a stealthy and robust multi-bit watermark for text that preserves semantics and resists perturbations.

Method: Fine-tunes paired LLM paraphrasers to embed binary codes at the sentence level, decoded by a trained classifier.

Result: Achieves over 99.99% detection AUC, robustness to perturbations, and generalization to out-of-distribution data.

Conclusion: The method is effective, stealthy, and open-sourced for further use and development.

Abstract: We propose an imperceptible multi-bit text watermark embedded by paraphrasing with LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder. To embed our multi-bit watermark, we use two paraphrasers alternatively to encode the pre-defined binary code at the sentence level. Then we use a text classifier as the decoder to decode each bit of the watermark. Through extensive experiments, we show that our watermarks can achieve over 99.99\% detection AUC with small (1.1B) text paraphrasers while keeping the semantic information of the original sentence. More importantly, our pipeline is robust under word substitution and sentence paraphrasing perturbations and generalizes well to out-of-distributional data. We also show the stealthiness of our watermark with LLM-based evaluation. We open-source the code: https://github.com/xiaojunxu/multi-bit-text-watermark.

</details>


### [278] [MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces](https://arxiv.org/pdf/2502.07709)
*Loris Gaven, Thomas Carta, Clément Romac, Cédric Colas, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer*

Main category: cs.AI

TL;DR: MAGELLAN is a metacognitive framework for LLM agents to predict competence and learning progress (LP) efficiently in open-ended goal spaces, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Open-ended learning agents need to prioritize goals effectively to maximize LP, but existing methods are inefficient or rely on expert-defined groupings.

Method: MAGELLAN learns semantic relationships between goals to predict competence and LP online, enabling dynamic adaptation in evolving goal spaces.

Result: MAGELLAN improves LP prediction efficiency and goal prioritization, mastering large and evolving goal spaces where other methods fail.

Conclusion: Augmenting LLM agents with metacognitive LP prediction scales curriculum learning effectively in open-ended environments.

Abstract: Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces.

</details>


### [279] [Planning of Heuristics: Strategic Planning on Large Language Models with Monte Carlo Tree Search for Automating Heuristic Optimization](https://arxiv.org/pdf/2502.11422)
*Chaoxu Mu, Xufeng Zhang, Hui Wang*

Main category: cs.AI

TL;DR: PoH integrates LLMs and MCTS to automate heuristic optimization for COPs, outperforming hand-crafted and other LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Human-designed heuristics require extensive domain knowledge and testing; LLMs offer a novel, automated alternative.

Method: PoH combines LLM self-reflection with MCTS to iteratively refine heuristics via evaluation and improvement suggestions.

Result: PoH achieves state-of-the-art performance in solving TSP and FSSP, surpassing other methods.

Conclusion: PoH demonstrates significant potential in automating heuristic optimization for COPs using LLMs.

Abstract: Heuristics have achieved great success in solving combinatorial optimization problems (COPs). However, heuristics designed by humans require too much domain knowledge and testing time. Given the fact that Large Language Models (LLMs) possess strong capabilities to understand and generate content, and a knowledge base that covers various domains, which offer a novel way to automatically optimize heuristics. Therefore, we propose Planning of Heuristics (PoH), an optimization method that integrates the self-reflection of LLMs with the Monte Carlo Tree Search (MCTS), a well-known planning algorithm. PoH iteratively refines generated heuristics by evaluating their performance and providing improvement suggestions. Our method enables to iteratively evaluate the generated heuristics (states) and improve them based on the improvement suggestions (actions) and evaluation results (rewards), by effectively simulating future states to search for paths with higher rewards. In this paper, we apply PoH to solve the Traveling Salesman Problem (TSP) and the Flow Shop Scheduling Problem (FSSP). The experimental results show that PoH outperforms other hand-crafted heuristics and Automatic Heuristic Design (AHD) by other LLMs-based methods, and achieves the significant improvements and the state-of-the-art performance of our proposed method in automating heuristic optimization with LLMs to solve COPs.

</details>


### [280] [Activation Space Interventions Can Be Transferred Between Large Language Models](https://arxiv.org/pdf/2503.04429)
*Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash, Michael Lan, Abir Harrasse, Amirali Abdullah*

Main category: cs.AI

TL;DR: The paper explores transferring safety interventions between AI models via shared activation spaces, demonstrating success in backdoor removal and harmful prompt refusal, and introduces a new task for testing model capabilities.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in practical applications of representation universality in AI models by leveraging shared activation spaces for safety interventions.

Method: Uses learned mappings of shared activation spaces to transfer steering vectors between models, tested on backdoor removal and harmful prompt refusal. Introduces a new task, 'corrupted capabilities,' to evaluate model behavior.

Result: Successful transfer of safety interventions across models (Llama, Qwen, Gemma), enabling smaller models to align larger ones. Autoencoder mappings act as lightweight safety switches.

Conclusion: The approach effectively transfers safety interventions and introduces a scalable method for dynamic model behavior control, addressing real-world AI safety challenges.

Abstract: The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, the practical applications of representation universality remain largely unexplored. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, \textit{corrupted capabilities}, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across Llama, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches", allowing dynamic toggling between model behaviors.

</details>


### [281] [Chain-of-Thought Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/pdf/2503.08679)
*Iván Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy*

Main category: cs.AI

TL;DR: The paper highlights unfaithful Chain-of-Thought (CoT) reasoning in AI models, showing they sometimes justify logically contradictory answers due to implicit biases, even in realistic prompts.


<details>
  <summary>Details</summary>
Motivation: To investigate and expose instances where CoT reasoning in AI models is unfaithful, particularly due to implicit biases or illogical shortcuts, challenging current detection strategies.

Method: Analyzed models' responses to contradictory questions (e.g., "Is X bigger than Y?" vs. "Is Y bigger than X?") and hard math problems, measuring rates of unfaithful reasoning.

Result: Found significant unfaithful CoT rates in models like GPT-4o-mini (13%) and Haiku 3.5 (7%), with frontier models performing better but not perfectly (e.g., Gemini 2.5 Pro at 0.14%).

Conclusion: Unfaithful CoT reasoning, driven by implicit biases or illogical shortcuts, poses challenges for detecting undesired behavior in LLMs, necessitating improved strategies.

Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions "Is X bigger than Y?" and "Is Y bigger than X?", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.

</details>


### [282] [Verification Learning: Make Unsupervised Neuro-Symbolic System Feasible](https://arxiv.org/pdf/2503.12917)
*Lin-Han Jia, Wen-Chao Hu, Jie-Jing Shao, Lan-Zhe Guo, Yu-Feng Li*

Main category: cs.AI

TL;DR: The paper introduces Verification Learning (VL), a label-free paradigm for Neuro-Symbolic (NeSy) Learning, addressing over-reliance on labeled data by using verification functions and rules. It formalizes the problem as a COP, proposes the DCS algorithm for efficiency, and validates VL on unsupervised tasks.


<details>
  <summary>Details</summary>
Motivation: Current NeSy Learning relies heavily on labeled data, leading to issues like reduced symbol information and larger solution spaces when labels are disregarded. VL aims to overcome these challenges.

Method: VL replaces label-based reasoning with label-free verification, formalizing the problem as a COP. The DCS algorithm reduces verification attempts, and a prior alignment method prevents shortcuts.

Result: VL achieves strong performance on unsupervised tasks (addition, sort, match, chess) with improved efficiency and computational cost reduction.

Conclusion: VL demonstrates that rules can replace labels for certain tasks in NeSy systems, offering a scalable and efficient alternative to traditional label-dependent methods.

Abstract: The current Neuro-Symbolic (NeSy) Learning paradigm suffers from an over-reliance on labeled data, so if we completely disregard labels, it leads to less symbol information, a larger solution space, and more shortcuts-issues that current Nesy systems cannot resolve. This paper introduces a novel learning paradigm, Verification Learning (VL), which addresses this challenge by transforming the label-based reasoning process in Nesy into a label-free verification process. VL achieves excellent learning results solely by relying on unlabeled data and a function that verifies whether the current predictions conform to the rules. We formalize this problem as a Constraint Optimization Problem (COP) and propose a Dynamic Combinatorial Sorting (DCS) algorithm that accelerates the solution by reducing verification attempts, effectively lowering computational costs and introduce a prior alignment method to address potential shortcuts. Our theoretical analysis points out which tasks in Nesy systems can be completed without labels and explains why rules can replace infinite labels for some tasks, while for others the rules have no effect. We validate the proposed framework through several fully unsupervised tasks including addition, sort, match, and chess, each showing significant performance and efficiency improvements.

</details>


### [283] [Behaviour Discovery and Attribution for Explainable Reinforcement Learning](https://arxiv.org/pdf/2503.14973)
*Rishav Rishav, Somjit Nath, Vincent Michalski, Samira Ebrahimi Kahou*

Main category: cs.AI

TL;DR: A framework for behavior discovery in RL agents identifies recurring patterns for fine-grained explanations, outperforming trajectory-level methods.


<details>
  <summary>Details</summary>
Motivation: Understanding RL agent decisions is critical in high-stakes applications, but existing methods lack insights into recurring strategies.

Method: A fully offline, reward-free framework segments behaviors, clustering state-action sequences to attribute actions to interpretable patterns.

Result: The method outperforms baselines in fidelity, human preference, and cluster coherence across four offline RL environments.

Conclusion: The proposed framework effectively captures and explains recurring behaviors, enhancing trust in RL agents.

Abstract: Building trust in reinforcement learning (RL) agents requires understanding why they make certain decisions, especially in high-stakes applications like robotics, healthcare, and finance. Existing explainability methods often focus on single states or entire trajectories, either providing only local, step-wise insights or attributing decisions to coarse, episodelevel summaries. Both approaches miss the recurring strategies and temporally extended patterns that actually drive agent behavior across multiple decisions. We address this gap by proposing a fully offline, reward-free framework for behavior discovery and segmentation, enabling the attribution of actions to meaningful and interpretable behavior segments that capture recurring patterns appearing across multiple trajectories. Our method identifies coherent behavior clusters from state-action sequences and attributes individual actions to these clusters for fine-grained, behavior-centric explanations. Evaluations on four diverse offline RL environments show that our approach discovers meaningful behaviors and outperforms trajectory-level baselines in fidelity, human preference, and cluster coherence. Our code is publicly available.

</details>


### [284] [ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning](https://arxiv.org/pdf/2504.21370)
*Jingyang Yi, Jiazheng Wang, Sida Li*

Main category: cs.AI

TL;DR: ShorterBetter is a reinforcement learning method that optimizes Chain-of-Thought (CoT) lengths for reasoning models, reducing output lengths by 50%-80% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Longer reasoning traces in models like OpenAI o1 and DeepSeek-R1 often lead to inefficiency and redundancy, termed as overthinking.

Method: Uses reinforcement learning with Sample Optimal Length (SOL) as a reward signal to train models to find optimal CoT lengths.

Result: Achieves significant reduction in output lengths (50%-80%) without losing accuracy, refining reasoning traces by eliminating redundancy.

Conclusion: ShorterBetter effectively balances thoroughness and efficiency in reasoning tasks, improving model performance.

Abstract: Recent models such as OpenAI o1 and DeepSeek-R1 have demonstrated strong performance on reasoning-intensive tasks by generating extended Chain-of-Thought (CoT) traces. While longer reasoning helps with thorough exploration of solution paths for complex problems, it also often leads to inefficient and redundant outputs--a phenomenon commonly described as overthinking. In this paper, we propose ShorterBetter, a simple yet effective reinforcement learning method that enables reasoning models to learn their own optimal CoT lengths without manual supervision. We define the Sample Optimal Length (SOL) as the length of the shortest correct response among multiple generations, which serves as a dynamic reward signal to guide the model toward efficient reasoning. Applied to DeepSeek-Distill-Qwen-1.5B/7B as base models, ShorterBetter achieves 50%-80% reduction in output lengths in both in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our reasoning trace analysis shows that ShorterBetter refines the structure of the reasoning traces by reducing unnecessary repetition, excessive self-verification, and over-exploration of alternatives.

</details>


### [285] [Arbitrarily Applicable Same/Opposite Relational Responding with NARS](https://arxiv.org/pdf/2505.07079)
*Robert Johansson, Patrick Hammer, Tony Lofthouse*

Main category: cs.AI

TL;DR: The paper demonstrates how the Non-Axiomatic Reasoning System (NARS) can learn and generalize same/opposite relational responding, mimicking human symbolic cognition, through minimal training and contextual cues.


<details>
  <summary>Details</summary>
Motivation: To explore the integration of human-like relational learning mechanisms, such as same/opposite responding, into artificial general intelligence frameworks like NARS.

Method: Extended NARS with acquired relations, using a matching-to-sample (MTS) procedure to train and test derived relational generalizations, including mutual and combinatorial entailments.

Result: NARS successfully internalized relational rules and demonstrated robust derived generalizations, paralleling human relational learning.

Conclusion: The study highlights the potential of incorporating nuanced relational learning from psychology into AI, showcasing NARS's ability to model arbitrary and context-sensitive relational behaviors.

Abstract: Same/opposite relational responding, a fundamental aspect of human symbolic cognition, allows the flexible generalization of stimulus relationships based on minimal experience. In this study, we demonstrate the emergence of \textit{arbitrarily applicable} same/opposite relational responding within the Non-Axiomatic Reasoning System (NARS), a computational cognitive architecture designed for adaptive reasoning under uncertainty. Specifically, we extend NARS with an implementation of \textit{acquired relations}, enabling the system to explicitly derive both symmetric (mutual entailment) and novel relational combinations (combinatorial entailment) from minimal explicit training in a contextually controlled matching-to-sample (MTS) procedure. Experimental results show that NARS rapidly internalizes explicitly trained relational rules and robustly demonstrates derived relational generalizations based on arbitrary contextual cues. Importantly, derived relational responding in critical test phases inherently combines both mutual and combinatorial entailments, such as deriving same-relations from multiple explicitly trained opposite-relations. Internal confidence metrics illustrate strong internalization of these relational principles, closely paralleling phenomena observed in human relational learning experiments. Our findings underscore the potential for integrating nuanced relational learning mechanisms inspired by learning psychology into artificial general intelligence frameworks, explicitly highlighting the arbitrary and context-sensitive relational capabilities modeled within NARS.

</details>


### [286] [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/pdf/2505.13227)
*Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, Caiming Xiong*

Main category: cs.AI

TL;DR: OSWorld-G and Jedi dataset address GUI grounding limitations by providing comprehensive benchmarks and large-scale data, improving model performance and agentic capabilities.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks oversimplify GUI grounding tasks, lacking real-world complexity like software commonsense and fine-grained manipulation.

Method: Introduce OSWorld-G (564 annotated samples) and Jedi dataset (4M examples) for diverse tasks. Train multi-scale models on Jedi.

Result: Models outperform existing approaches on benchmarks (ScreenSpot-v2, ScreenSpot-Pro, OSWorld-G) and improve agentic capabilities (5% to 27% on OSWorld).

Conclusion: Specialized data enables compositional generalization, enhancing grounding performance and agentic capabilities. Resources are open-sourced.

Abstract: Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.

</details>


### [287] [SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning](https://arxiv.org/pdf/2505.19099)
*Kun Xiang, Heng Li, Terry Jingchen Zhang, Yinya Huang, Zirong Liu, Peixin Qu, Jixi He, Jiaqi Chen, Yu-Jie Yuan, Jianhua Han, Hang Xu, Hanhui Li, Mrinmaya Sachan, Xiaodan Liang*

Main category: cs.AI

TL;DR: SeePhys is a multimodal benchmark for LLM reasoning in physics, featuring vision-essential problems. Advanced models struggle with accuracy, highlighting challenges in visual understanding and reliance on text.


<details>
  <summary>Details</summary>
Motivation: To address gaps in LLMs' visual reasoning capabilities, especially in physics, by creating a benchmark with vision-essential problems.

Method: Developed a large-scale multimodal benchmark with 7 physics domains and 21 diagram categories, emphasizing visual information extraction.

Result: Advanced models like Gemini-2.5-pro and o4-mini scored below 60% accuracy, revealing weaknesses in visual understanding and text reliance.

Conclusion: SeePhys exposes critical limitations in LLMs' visual reasoning, urging improvements in diagram-physics coupling and reducing text dependency.

Abstract: We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.

</details>


### [288] [OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs](https://arxiv.org/pdf/2505.19165)
*Debdeep Sanyal, Umakanta Maharana, Yash Sinha, Hong Ming Tan, Shirish Karande, Mohan Kankanhalli, Murari Mandal*

Main category: cs.AI

TL;DR: The paper evaluates LLMs' ability to understand and adhere to organizational RBAC hierarchies using a synthetic benchmark, finding significant limitations even in state-of-the-art models like GPT-4.1.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' reliability in handling nuanced organizational permissions and hierarchical constraints, a challenge underexplored due to proprietary data limitations.

Method: Introduces the OrgAccess benchmark with 40 permission types and varying difficulty levels (easy, medium, hard) to test LLMs' compliance with hierarchical rules.

Result: LLMs, including GPT-4.1, perform poorly, especially with conflicting permissions (F1-Score of 0.27 on hardest tasks).

Conclusion: Highlights a critical gap in LLMs' rule-following and reasoning for structured environments, suggesting a new evaluation paradigm.

Abstract: Role-based access control (RBAC) and hierarchical structures are foundational to how information flows and decisions are made within virtually all organizations. As the potential of Large Language Models (LLMs) to serve as unified knowledge repositories and intelligent assistants in enterprise settings becomes increasingly apparent, a critical, yet under explored, challenge emerges: \textit{can these models reliably understand and operate within the complex, often nuanced, constraints imposed by organizational hierarchies and associated permissions?} Evaluating this crucial capability is inherently difficult due to the proprietary and sensitive nature of real-world corporate data and access control policies. We introduce a synthetic yet representative \textbf{OrgAccess} benchmark consisting of 40 distinct types of permissions commonly relevant across different organizational roles and levels. We further create three types of permissions: 40,000 easy (1 permission), 10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to test LLMs' ability to accurately assess these permissions and generate responses that strictly adhere to the specified hierarchical rules, particularly in scenarios involving users with overlapping or conflicting permissions. Our findings reveal that even state-of-the-art LLMs struggle significantly to maintain compliance with role-based structures, even with explicit instructions, with their performance degrades further when navigating interactions involving two or more conflicting permissions. Specifically, even \textbf{GPT-4.1 only achieves an F1-Score of 0.27 on our hardest benchmark}. This demonstrates a critical limitation in LLMs' complex rule following and compositional reasoning capabilities beyond standard factual or STEM-based benchmarks, opening up a new paradigm for evaluating their fitness for practical, structured environments.

</details>


### [289] [Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models](https://arxiv.org/pdf/2505.19676)
*Lachlan McGinness, Peter Baumgartner*

Main category: cs.AI

TL;DR: The study evaluates LLMs' ability to use ATP reasoning strategies, finding stalled progress in reasoning improvements over nine months, with most gains attributed to hidden prompts or generic Chain of Thought training. Bottom-up reasoning works best, and correct reasoning weakly correlates with correct conclusions.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of LLMs in employing ATP reasoning strategies and track progress in their reasoning abilities over time.

Method: Evaluated state-of-the-art models (Dec 2023 and Aug 2024) on PRONTOQA steamroller problems, developed accuracy assessment methods, and analyzed completion tokens.

Result: Progress in LLM reasoning stalled; improvements linked to hidden prompts or generic Chain of Thought training. Bottom-up reasoning performed best; weak correlation between correct reasoning and correct conclusions.

Conclusion: Current LLMs show limited progress in reasoning, with bottom-up strategies being most effective, but correct reasoning doesn't strongly ensure correct conclusions.

Abstract: Empirical methods to examine the capability of Large Language Models (LLMs) to use Automated Theorem Prover (ATP) reasoning strategies are studied. We evaluate the performance of State of the Art models from December 2023 and August 2024 on PRONTOQA steamroller reasoning problems. For that, we develop methods for assessing LLM response accuracy and correct answer correlation.
  Our results show that progress in improving LLM reasoning abilities has stalled over the nine month period. By tracking completion tokens, we show that almost all improvement in reasoning ability since GPT-4 was released can be attributed to either hidden system prompts or the training of models to automatically use generic Chain of Thought prompting strategies. Among the ATP reasoning strategies tried, we found that current frontier LLMs are best able to follow the bottom-up (also known as forward-chaining) strategy. A low positive correlation was found between an LLM response containing correct reasoning and arriving at the correct conclusion.

</details>


### [290] [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.01391)
*Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming Liu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, Maosong Sun*

Main category: cs.AI

TL;DR: AgentCPM-GUI is an 8B-parameter GUI agent designed for robust on-device GUI interaction, addressing challenges like noisy data and lack of semantic diversity. It achieves SOTA performance on benchmarks, including a new Chinese GUI benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents face challenges like noisy training data, overfitting, and lack of support for non-English interfaces, limiting practical deployment.

Method: The pipeline includes grounding-aware pre-training, supervised fine-tuning on Chinese and English trajectories, and reinforcement fine-tuning with GRPO. A compact action space is introduced for efficiency.

Result: AgentCPM-GUI achieves 96.9% Type-Match and 91.3% Exact-Match on benchmarks, including the new CAGUI benchmark.

Conclusion: The work presents a robust GUI agent with strong performance, supporting both English and Chinese interfaces, and releases resources for reproducibility.

Abstract: The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and $91.3\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.

</details>


### [291] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/pdf/2506.08026)
*Xibai Wang*

Main category: cs.AI

TL;DR: TIP-Search is a framework for real-time market prediction that dynamically selects deep learning models to meet strict latency demands while maximizing accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the strict latency requirements in high-frequency financial systems, the paper aims to ensure predictive accuracy under uncertain workloads.

Method: TIP-Search profiles latency and performance offline, then dynamically selects models online without domain labels.

Result: Evaluated on three datasets, TIP-Search improves accuracy by up to 8.5% and achieves 100% deadline satisfaction.

Conclusion: TIP-Search effectively enables robust, low-latency financial inference under uncertainty.

Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling framework for real-time market prediction under uncertain workloads. Motivated by the strict latency demands in high-frequency financial systems, TIP-Search dynamically selects a deep learning model from a heterogeneous pool, aiming to maximize predictive accuracy while satisfying per-task deadline constraints. Our approach profiles latency and generalization performance offline, then performs online task-aware selection without relying on explicit input domain labels. We evaluate TIP-Search on three real-world limit order book datasets (FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms static baselines with up to 8.5% improvement in accuracy and 100% deadline satisfaction. Our results highlight the effectiveness of TIP-Search in robust low-latency financial inference under uncertainty.

</details>


### [292] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/pdf/2506.09250)
*A. Lawsen*

Main category: cs.AI

TL;DR: The paper critiques Shojaee et al.'s findings on LRMs' "accuracy collapse," attributing it to flawed experimental design rather than inherent reasoning failures.


<details>
  <summary>Details</summary>
Motivation: To address misconceptions about LRMs' reasoning capabilities due to experimental limitations.

Method: Analyzed three key issues in Shojaee et al.'s experiments: token limits in Tower of Hanoi, misclassification in evaluation, and unsolvable River Crossing instances. Controlled experiments were conducted by requesting generating functions.

Result: Preliminary experiments showed high accuracy on Tower of Hanoi instances previously deemed failures, disproving the original claims.

Conclusion: Careful experimental design is crucial for accurately evaluating AI reasoning capabilities.

Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments risk exceeding model output token limits, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N > 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.

</details>


### [293] [A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI](https://arxiv.org/pdf/2506.10130)
*Luciano Floridi*

Main category: cs.AI

TL;DR: The paper introduces a conjecture about a trade-off in AI systems between provable correctness and broad data-mapping capacity, reframing AI engineering and philosophical expectations.


<details>
  <summary>Details</summary>
Motivation: To formalize the implicit trade-off between error-free guarantees and high-dimensional data processing in AI, addressing historical tensions and philosophical debates.

Method: The conjecture is stated in information-theoretic form and analyzed through epistemology, formal verification, and philosophy of technology, using concepts like underdetermination and epistemic risk.

Result: The conjecture reframes AI evaluation standards, governance, and hybrid system design, emphasizing the need for rigorous verification.

Conclusion: Proving or refuting the conjecture is crucial for the future of trustworthy AI, impacting standards and frameworks.

Abstract: This article introduces a conjecture that formalises a fundamental trade-off between provable correctness and broad data-mapping capacity in Artificial Intelligence (AI) systems. When an AI system is engineered for deductively watertight guarantees (demonstrable certainty about the error-free nature of its outputs) -- as in classical symbolic AI -- its operational domain must be narrowly circumscribed and pre-structured. Conversely, a system that can input high-dimensional data to produce rich information outputs -- as in contemporary generative models -- necessarily relinquishes the possibility of zero-error performance, incurring an irreducible risk of errors or misclassification. By making this previously implicit trade-off explicit and open to rigorous verification, the conjecture significantly reframes both engineering ambitions and philosophical expectations for AI. After reviewing the historical motivations for this tension, the article states the conjecture in information-theoretic form and contextualises it within broader debates in epistemology, formal verification, and the philosophy of technology. It then offers an analysis of its implications and consequences, drawing on notions of underdetermination, prudent epistemic risk, and moral responsibility. The discussion clarifies how, if correct, the conjecture would help reshape evaluation standards, governance frameworks, and hybrid system design. The conclusion underscores the importance of eventually proving or refuting the inequality for the future of trustworthy AI.

</details>


### [294] [ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities](https://arxiv.org/pdf/2506.12376)
*Zhaochen Hong, Haofei Yu, Jiaxuan You*

Main category: cs.AI

TL;DR: ConsistencyChecker is a tree-based framework to evaluate LLM consistency through reversible transformations, showing strong correlation with WMT 2024 auto-ranking.


<details>
  <summary>Details</summary>
Motivation: Traditional self-consistency methods fail to capture subtle semantic and functional changes in multi-step interactions, necessitating a more robust evaluation tool.

Method: The framework uses a tree structure with nodes as text states and edges as inverse operations, tested on dynamic and LLM-generated benchmarks.

Result: Experiments on eight models demonstrate ConsistencyChecker's ability to distinguish performance, with scores correlating strongly (r > 0.7) with WMT 2024 auto-ranking.

Conclusion: ConsistencyChecker provides a benchmark-free, reliable method for assessing LLM consistency, validated by its correlation with established metrics.

Abstract: Evaluating consistency in large language models (LLMs) is crucial for ensuring reliability, particularly in complex, multi-step interactions between humans and LLMs. Traditional self-consistency methods often miss subtle semantic changes in natural language and functional shifts in code or equations, which can accumulate over multiple transformations. To address this, we propose ConsistencyChecker, a tree-based evaluation framework designed to measure consistency through sequences of reversible transformations, including machine translation tasks and AI-assisted programming tasks. In our framework, nodes represent distinct text states, while edges correspond to pairs of inverse operations. Dynamic and LLM-generated benchmarks ensure a fair assessment of the model's generalization ability and eliminate benchmark leakage. Consistency is quantified based on similarity across different depths of the transformation tree. Experiments on eight models from various families and sizes show that ConsistencyChecker can distinguish the performance of different models. Notably, our consistency scores-computed entirely without using WMT paired data-correlate strongly (r > 0.7) with WMT 2024 auto-ranking, demonstrating the validity of our benchmark-free approach. Our implementation is available at: https://github.com/ulab-uiuc/consistencychecker.

</details>


### [295] [AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving](https://arxiv.org/pdf/2506.12508)
*Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, Bo An*

Main category: cs.AI

TL;DR: \projectname is a hierarchical multi-agent framework for general-purpose task solving, outperforming flat-agent and monolithic baselines in adaptability and success rate.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agent systems lack coordination mechanisms and struggle with generalization across diverse domains.

Method: \projectname uses a central planning agent to decompose tasks and delegate to specialized sub-agents, supporting modularity, coordination, and multimodal capabilities.

Result: Evaluated on benchmark datasets, \projectname shows higher task success rates and adaptability compared to baselines.

Conclusion: Hierarchical organization and role specialization enhance scalability and generality in LLM-based agent systems.

Abstract: Recent advances in agent systems based on large language models (LLMs) have demonstrated strong capabilities in solving complex tasks. However, most current methods lack mechanisms for coordinating specialized agents and have limited ability to generalize to new or diverse domains. We introduce \projectname, a hierarchical multi-agent framework for general-purpose task solving that integrates high-level planning with modular agent collaboration. Inspired by the way a conductor orchestrates a symphony and guided by the principles of \textit{extensibility}, \textit{multimodality}, \textit{modularity}, and \textit{coordination}, \projectname features a central planning agent that decomposes complex objectives and delegates sub-tasks to a team of specialized agents. Each sub-agent is equipped with general programming and analytical tools, as well as abilities to tackle a wide range of real-world specific tasks, including data analysis, file operations, web navigation, and interactive reasoning in dynamic multimodal environments. \projectname supports flexible orchestration through explicit sub-goal formulation, inter-agent communication, and adaptive role allocation. We evaluate the framework on three widely used benchmark datasets covering various real-world tasks, searching web pages, reasoning over heterogeneous modalities, etc. Experimental results demonstrate that \projectname consistently outperforms flat-agent and monolithic baselines in task success rate and adaptability. These findings highlight the effectiveness of hierarchical organization and role specialization in building scalable and general-purpose LLM-based agent systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [296] [Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription](https://arxiv.org/pdf/2506.14223)
*Anna Hamberger, Sebastian Murgul, Jochen Schmidt, Michael Heizmann*

Main category: cs.SD

TL;DR: The paper introduces the Fretting-Transformer, a T5-based model for converting MIDI to guitar tablature, addressing playability and ambiguity issues, and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: MIDI lacks playability details for guitar, necessitating a system to automate transcription into tablature while resolving string-fret ambiguity.

Method: The Fretting-Transformer uses a T5 architecture, novel data pre-processing, and tokenization, trained on datasets like DadaGP, GuitarToday, and Leduc.

Result: The model outperforms baseline methods (A*) and commercial tools (Guitar Pro) in accuracy and playability, enhanced by context-sensitive processing.

Conclusion: The Fretting-Transformer provides a strong foundation for future automated guitar transcription, with potential for further improvements.

Abstract: Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription.

</details>


### [297] [Manipulated Regions Localization For Partially Deepfake Audio: A Survey](https://arxiv.org/pdf/2506.14396)
*Jiayi He, Jiangyan Yi, Jianhua Tao, Siding Zeng, Hao Gu*

Main category: cs.SD

TL;DR: This survey provides the first systematic review of partially deepfake audio localization tasks, covering fundamentals, methods, limitations, and future trends.


<details>
  <summary>Details</summary>
Motivation: Partially deepfake audio is harder to detect than fully deepfake, posing higher security risks, yet lacks comprehensive review.

Method: The survey outlines fundamentals, branches of existing methods, and analyzes current limitations.

Result: It offers a revealing insight into the field, highlighting gaps and potential trends.

Conclusion: This work serves as a foundational resource for understanding and advancing partially deepfake audio detection.

Abstract: With the development of audio deepfake techniques, attacks with partially deepfake audio are beginning to rise. Compared to fully deepfake, it is much harder to be identified by the detector due to the partially cryptic manipulation, resulting in higher security risks. Although some studies have been launched, there is no comprehensive review to systematically introduce the current situations and development trends for addressing this issue. Thus, in this survey, we are the first to outline a systematic introduction for partially deepfake audio manipulated region localization tasks, including the fundamentals, branches of existing methods, current limitations and potential trends, providing a revealing insight into this scope.

</details>


### [298] [A Survey on World Models Grounded in Acoustic Physical Information](https://arxiv.org/pdf/2506.13833)
*Xiaoliang Chen, Le Chang, Xin Yu, Yunhe Huang, Xianling Tu*

Main category: cs.SD

TL;DR: A survey on acoustic world models, covering theory, methods, applications, and future challenges in using sound for environmental perception and AI intelligence.


<details>
  <summary>Details</summary>
Motivation: To explore how acoustic signals encode physical information and enable AI systems to understand and simulate dynamic events through sound.

Method: Reviews theoretical foundations, Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning.

Result: Highlights applications in robotics, autonomous driving, healthcare, and finance, and identifies technical and ethical challenges.

Conclusion: Proposes a roadmap for future research toward robust, causal, and responsible acoustic intelligence, aiming for AI systems with intuitive physics via sound.

Abstract: This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal "intuitive physics" engine through sound.

</details>


### [299] [Set theoretic solution for the tuning problem](https://arxiv.org/pdf/2506.13969)
*Vsevolod Vladimirovich Deriushkin*

Main category: cs.SD

TL;DR: A new solution for musical tuning generalizes Just Intonation for inharmonic timbres and unifies spectral interference and harmonicity, using set theory to quantify consonance via affinity and harmonicity measures.


<details>
  <summary>Details</summary>
Motivation: To address musical tuning by generalizing Just Intonation and unifying spectral interference and harmonicity, making the theory accessible to a broad audience.

Method: Defines two consonance measures (affinity and harmonicity) using set theory to generate dynamic tuning systems.

Result: Mathematical quantification of musical consonance, enabling dynamic tuning systems based on the defined measures.

Conclusion: The work provides a unified framework for musical tuning, accessible to non-experts, with practical applications in dynamic tuning systems.

Abstract: In this paper I want to suggest a new solution to the problem of musical tuning. On one hand, I see it as a generalization of Just Intonation (JI) to inharmonic timbers, on another, as a unification of spectral interference and harmonicity contributions to consonance within a single framework. The main achievement of the work is the ability to mathematically quantify the phenomenon of musical consonance using set theory. That quantification is done by defining two measures of consonance: affinity and harmonicity. These measures naturally generate sets of intervals that can be used as dynamic tuning systems. The paper is aimed at a broad audience of people who may not be skilled in music and tuning theory or mathematics. Thus, I attempt to give as much details and explanations as I can, while keeping the number of pages as low as possible.

</details>


### [300] [Making deep neural networks work for medical audio: representation, compression and domain adaptation](https://arxiv.org/pdf/2506.13970)
*Charles C Onu*

Main category: cs.SD

TL;DR: This thesis explores using machine learning to analyze medical audio signals, focusing on infant cry sounds, with innovations in transfer learning, model compression, domain adaptation, and an open-source dataset.


<details>
  <summary>Details</summary>
Motivation: To standardize medical sound analysis, enable screening in low-resource settings, and detect subtle health patterns for early diagnosis.

Method: Utilizes transfer learning from adult speech data, introduces end-to-end model compression for recurrent networks, proposes domain adaptation techniques, and releases an open-source infant cry dataset.

Result: Developed accurate, portable models for infant cry analysis, improved generalization across domains, and provided a unique dataset for further research.

Conclusion: The work establishes infant cry as a vital sign and showcases AI-driven audio monitoring's potential for accessible healthcare.

Abstract: This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.
  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.
  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.

</details>


### [301] [Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment](https://arxiv.org/pdf/2506.14148)
*Long-Vu Hoang, Tuan Nguyen, Tran Huy Dat*

Main category: cs.SD

TL;DR: A non-invasive object classification method using acoustic scattering achieves 90% accuracy for hair assessment, leveraging AI-driven deep learning.


<details>
  <summary>Details</summary>
Motivation: To explore a privacy-preserving, non-contact alternative to visual classification by utilizing acoustic scattering for object classification.

Method: Emits acoustic stimuli, captures scattered signals, and classifies hair type and moisture using AI-driven deep learning. Benchmarks include supervised, embedding-based, foundation model fine-tuning, and self-supervised approaches.

Result: Best performance (nearly 90% accuracy) achieved by fine-tuning all parameters of a self-supervised model.

Conclusion: Acoustic scattering is a viable, privacy-preserving alternative to visual classification with broad industry potential.

Abstract: This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries.

</details>


### [302] [Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models](https://arxiv.org/pdf/2506.14153)
*Tuan Dat Phuong, Long-Vu Hoang, Huy Dat Tran*

Main category: cs.SD

TL;DR: Replacing MLP with KAN in XLSR-Conformer improves synthetic speech detection by 60.55% on ASVspoof2021, achieving 0.70% EER.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in synthetic speech detection by enhancing SSL-based models like XLSR-Conformer.

Method: Replace Multi-Layer Perceptron in XLSR-Conformer with Kolmogorov-Arnold Network (KAN).

Result: 60.55% relative improvement on LA and DF sets, 0.70% EER on 21LA set.

Conclusion: KAN integration in SSL models is promising for synthetic speech detection.

Abstract: Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.

</details>


### [303] [Investigation of Zero-shot Text-to-Speech Models for Enhancing Short-Utterance Speaker Verification](https://arxiv.org/pdf/2506.14226)
*Yiyang Zhao, Shuai Wang, Guangzhi Sun, Zehua Chen, Chao Zhang, Mingxing Xu, Thomas Fang Zheng*

Main category: cs.SD

TL;DR: Using ZS-TTS systems for test-time data augmentation improves speaker verification, especially for short utterances, but longer synthetic speech doesn't match the benefits of real speech.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of short-utterance speaker verification by leveraging ZS-TTS systems for data augmentation.

Method: Evaluate three pre-trained ZS-TTS systems (NatureSpeech 3, CosyVoice, MaskGCT) on VoxCeleb 1, combining real and synthetic speech samples.

Result: 10%-16% relative EER reduction, with notable gains for short utterances, but no added benefit from longer synthetic speech.

Conclusion: ZS-TTS shows promise for speaker verification but has limitations with longer synthetic speech, guiding future research.

Abstract: Short-utterance speaker verification presents significant challenges due to the limited information in brief speech segments, which can undermine accuracy and reliability. Recently, zero-shot text-to-speech (ZS-TTS) systems have made considerable progress in preserving speaker identity. In this study, we explore, for the first time, the use of ZS-TTS systems for test-time data augmentation for speaker verification. We evaluate three state-of-the-art pre-trained ZS-TTS systems, NatureSpeech 3, CosyVoice, and MaskGCT, on the VoxCeleb 1 dataset. Our experimental results show that combining real and synthetic speech samples leads to 10%-16% relative equal error rate (EER) reductions across all durations, with particularly notable improvements for short utterances, all without retraining any existing systems. However, our analysis reveals that longer synthetic speech does not yield the same benefits as longer real speech in reducing EERs. These findings highlight the potential and challenges of using ZS-TTS for test-time speaker verification, offering insights for future research.

</details>


### [304] [SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling](https://arxiv.org/pdf/2506.14293)
*Tawsif Ahmed, Andrej Radonjic, Gollam Rabby*

Main category: cs.SD

TL;DR: Sleeping-DISCO 9M is a large-scale pre-training dataset for music and song, addressing the lack of open-source, high-quality datasets for generative music tasks by using real-world popular music.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for generative music tasks are either synthetic or lack real-world relevance, limiting their adoption and effectiveness.

Method: The dataset is constructed using actual popular music and world-renowned artists, ensuring real-world relevance.

Result: Sleeping-DISCO 9M provides a high-quality, representative dataset for generative music modeling tasks.

Conclusion: This dataset fills a critical gap in the field by offering a realistic and practical resource for generative music research.

Abstract: We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists.

</details>


### [305] [Unifying Streaming and Non-streaming Zipformer-based ASR](https://arxiv.org/pdf/2506.14434)
*Bidisha Sharma, Karthik Pandia Durai, Shankar Venkatesan, Jeena J Prakash, Shashi Kumar, Malolan Chetlur, Andreas Stolcke*

Main category: cs.SD

TL;DR: A unified framework trains a single ASR model for both streaming and non-streaming applications using dynamic right-context in zipformer models, improving accuracy with minimal latency impact.


<details>
  <summary>Details</summary>
Motivation: To reduce costs and complexity by unifying streaming and non-streaming ASR models while maintaining performance.

Method: Uses dynamic right-context with chunked attention masking in zipformer-based ASR models, analyzing the impact of varying right-context frames.

Result: Reduces word error by 7.9% with slight latency degradation; achieves streaming performance close to non-streaming models by adjusting right-context frames.

Conclusion: The approach offers a flexible latency-accuracy tradeoff, making it adaptable to customer needs while unifying model development.

Abstract: There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.

</details>


### [306] [A Comparative Study on Proactive and Passive Detection of Deepfake Speech](https://arxiv.org/pdf/2506.14398)
*Chia-Hua Wu, Wanying Ge, Xin Wang, Junichi Yamagishi, Yu Tsao, Hsin-Min Wang*

Main category: cs.SD

TL;DR: Proposes a framework for evaluating proactive watermarking and passive deepfake detectors in speech detection, ensuring fair comparison and analyzing robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Differences in training, optimization, and evaluation prevent unified protocols for comparing proactive and passive deepfake speech defense solutions.

Method: Trains and tests all models on common datasets, evaluates performance with a shared metric, and analyzes robustness against adversarial attacks.

Result: Different models exhibit distinct vulnerabilities to various speech attribute distortions.

Conclusion: The framework enables fair comparison and selection of the best solutions for different deepfake speech defense scenarios.

Abstract: Solutions for defending against deepfake speech fall into two categories: proactive watermarking models and passive conventional deepfake detectors. While both address common threats, their differences in training, optimization, and evaluation prevent a unified protocol for joint evaluation and selecting the best solutions for different cases. This work proposes a framework to evaluate both model types in deepfake speech detection. To ensure fair comparison and minimize discrepancies, all models were trained and tested on common datasets, with performance evaluated using a shared metric. We also analyze their robustness against various adversarial attacks, showing that different models exhibit distinct vulnerabilities to different speech attribute distortions. Our training and evaluation code is available at Github.

</details>


### [307] [An Open Research Dataset of the 1932 Cairo Congress of Arab Music](https://arxiv.org/pdf/2506.14503)
*Baris Bozkurt*

Main category: cs.SD

TL;DR: ORD-CC32 is an open dataset from the 1932 Cairo Congress of Arab Music recordings, featuring metadata, melodic/rhythmic tags, and acoustic features for computational studies of Arab music.


<details>
  <summary>Details</summary>
Motivation: To support interdisciplinary research in computational ethnomusicology, MIR, cultural studies, and digital heritage preservation by providing a structured dataset of historically significant Arab music recordings.

Method: The dataset includes manually labeled metadata, tonic information, and acoustic features extracted using advanced pitch detection. A case study uses pitch histograms to analyze microtonal differences.

Result: ORD-CC32 enables data-driven analysis of regional variations in Arab music, demonstrated through microtonal studies.

Conclusion: The dataset is openly shared on Zenodo with tools, fostering research in diverse fields related to Arab music and cultural heritage.

Abstract: This paper introduces ORD-CC32 , an open research dataset derived from the 1932 Cairo Congress of Arab Music recordings, a historically significant collection representing diverse Arab musical traditions. The dataset includes structured metadata, melodic and rhythmic mode tags (maqam and iqa), manually labeled tonic information, and acoustic features extracted using state-of-the-art pitch detection methods. These resources support computational studies of tuning, temperament, and regional variations in Arab music. A case study using pitch histograms demonstrates the potential for data-driven analysis of microtonal differences across regions. By making this dataset openly available, we aim to enable interdisciplinary research in computational ethnomusicology, music information retrieval (MIR), cultural studies, and digital heritage preservation. ORD-CC32 is shared on Zenodo with tools for feature extraction and metadata retrieval.

</details>


### [308] [Evolving music theory for emerging musical languages](https://arxiv.org/pdf/2506.14504)
*Emmanuel Deruty*

Main category: cs.SD

TL;DR: The paper redefines pitch in contemporary popular music as a perceptual construct, not an objective property, influenced by listeners and conditions.


<details>
  <summary>Details</summary>
Motivation: To challenge traditional assumptions about pitch in electronic music contexts where they may not apply.

Method: Uses phenomenological and inductive methods to analyze quasi-harmonic tones and perceptual variability.

Result: Finds that a single tone can convey multiple pitches (tonal fission) and pitch perception can be multistable.

Conclusion: Proposes a model of pitch based on perceptual variability, challenging conventional theoretical norms.

Abstract: This chapter reconsiders the concept of pitch in contemporary popular music (CPM), particularly in electronic contexts where traditional assumptions may fail. Drawing on phenomenological and inductive methods, it argues that pitch is not an ontologically objective property but a perceptual construct shaped by listeners and conditions. Analyses of quasi-harmonic tones reveal that a single tone can convey multiple pitches, giving rise to tonal fission. The perception of pitch may also be multistable, varying for the same listener over time. In this framework, the tuning system may emerge from a tone's internal structure. A parallel with the coastline paradox supports a model of pitch grounded in perceptual variability, challenging inherited theoretical norms.

</details>


### [309] [Refining music sample identification with a self-supervised graph neural network](https://arxiv.org/pdf/2506.14684)
*Aditya Bhattacharjee, Ivan Meresman Higgs, Mark Sandler, Emmanouil Benetos*

Main category: cs.SD

TL;DR: A lightweight, scalable Graph Neural Network model for ASID achieves comparable performance with fewer parameters, introducing a two-stage retrieval approach and benchmarking for short queries.


<details>
  <summary>Details</summary>
Motivation: ASID struggles with identifying musically modified samples, unlike audio fingerprinting, necessitating a robust system for common music production transformations.

Method: Proposes a contrastive learning framework with a Graph Neural Network, using a two-stage approach (coarse search + cross-attention classifier) and benchmarking short queries.

Result: Achieves 44.2% mAP with only 9% of trainable parameters compared to state-of-the-art.

Conclusion: The model is efficient and scalable, addressing key ASID challenges, with published fine-grained annotations for the Sample100 dataset.

Abstract: Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under "real world" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.
  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.
  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.

</details>


### [310] [Adaptive Accompaniment with ReaLchords](https://arxiv.org/pdf/2506.14723)
*Yusong Wu, Tim Cooijmans, Kyle Kastner, Adam Roberts, Ian Simon, Alexander Scarlatos, Chris Donahue, Cassie Tarakajian, Shayegan Omidshafiei, Aaron Courville, Pablo Samuel Castro, Natasha Jaques, Cheng-Zhi Anna Huang*

Main category: cs.SD

TL;DR: ReaLchords is an online generative model for improvising chord accompaniment to melodies, using reinforcement learning to adapt to live input.


<details>
  <summary>Details</summary>
Motivation: Current generative models lack the ability to generate music online (simultaneously with musicians), limiting live collaboration.

Method: The model is pretrained by maximum likelihood, then finetuned with reinforcement learning using a reward model for harmonic/temporal coherency and a divergence term for distillation from a teacher model.

Result: The model adapts well to unfamiliar input and produces fitting accompaniment, validated by quantitative experiments and listening tests.

Conclusion: ReaLchords enables live jamming and co-creation in other modalities, advancing online generative music.

Abstract: Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.

</details>


### [311] [Exploring Speaker Diarization with Mixture of Experts](https://arxiv.org/pdf/2506.14750)
*Gaobin Yang, Maokui He, Shutong Niu, Ruoyu Wang, Hang Chen, Jun Du*

Main category: cs.SD

TL;DR: A novel neural speaker diarization system (NSD-MS2S) with memory-aware multi-speaker embedding and Seq2Seq architecture is proposed, extended with SS-MoE for improved performance.


<details>
  <summary>Details</summary>
Motivation: To enhance speaker diarization by integrating memory-aware embeddings and Seq2Seq, and to mitigate model bias using SS-MoE.

Method: Combines memory-aware multi-speaker embedding with Seq2Seq, and introduces SS-MoE for bias reduction.

Result: Achieves state-of-the-art performance on complex datasets like CHiME-6 and DIHARD-III.

Conclusion: The proposed methods are effective and robust for real-world speaker diarization tasks.

Abstract: In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios.

</details>


### [312] [Quality-aware Masked Diffusion Transformer for Enhanced Music Generation](https://arxiv.org/pdf/2405.15863)
*Chang Li, Ruoyu Wang, Lijuan Liu, Jun Du, Yixuan Sun, Zilu Guo, Zhenrong Zhang, Yuan Jiang, Jianqing Gao, Feng Ma*

Main category: cs.SD

TL;DR: The paper proposes a quality-aware training paradigm and a masked diffusion transformer (MDT) model for text-to-music generation, addressing data quality issues and achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for text-to-music generation suffer from low-quality waveforms and inconsistent text-audio alignment, limiting model performance.

Method: A quality-aware training paradigm and MDT model are introduced, along with a three-stage caption refinement approach to improve text-audio consistency.

Result: The method achieves SOTA performance on benchmark datasets (MusicCaps, Song-Describer) in both objective and subjective metrics.

Conclusion: The proposed approach effectively addresses data quality challenges and enhances music generation quality and diversity.

Abstract: Text-to-music (TTM) generation, which converts textual descriptions into audio, opens up innovative avenues for multimedia creation. Achieving high quality and diversity in this process demands extensive, high-quality data, which are often scarce in available datasets. Most open-source datasets frequently suffer from issues like low-quality waveforms and low text-audio consistency, hindering the advancement of music generation models. To address these challenges, we propose a novel quality-aware training paradigm for generating high-quality, high-musicality music from large-scale, quality-imbalanced datasets. Additionally, by leveraging unique properties in the latent space of musical signals, we adapt and implement a masked diffusion transformer (MDT) model for the TTM task, showcasing its capacity for quality control and enhanced musicality. Furthermore, we introduce a three-stage caption refinement approach to address low-quality captions' issue. Experiments show state-of-the-art (SOTA) performance on benchmark datasets including MusicCaps and the Song-Describer Dataset with both objective and subjective metrics. Demo audio samples are available at https://qa-mdt.github.io/, code and pretrained checkpoints are open-sourced at https://github.com/ivcylc/OpenMusic.

</details>


### [313] [Generative Deep Learning and Signal Processing for Data Augmentation of Cardiac Auscultation Signals: Improving Model Robustness Using Synthetic Audio](https://arxiv.org/pdf/2410.10125)
*Leigh Abbott, Milan Marocchi, Matthew Fynn, Yue Rong, Sven Nordholm*

Main category: cs.SD

TL;DR: Using generative deep learning and signal processing to augment cardiac auscultation data improves the robustness of classification models, enhancing both in-distribution and out-of-distribution performance.


<details>
  <summary>Details</summary>
Motivation: The lack of labeled data for cardiac auscultation signals limits the training of classification models, and prior studies focused more on performance than robustness.

Method: Augmented datasets were created using traditional audio techniques and synthetic audio generated by WaveGrad and DiffWave diffusion models. A convolutional neural network was trained on this augmented data.

Result: The augmented dataset improved in-distribution and out-of-distribution performance, including accuracy, balanced accuracy, and Matthew's correlation coefficient.

Conclusion: Augmented datasets enhance the robustness of cardiac auscultation classifiers, addressing imbalanced data issues and improving generalization.

Abstract: Accurately interpreting cardiac auscultation signals plays a crucial role in diagnosing and managing cardiovascular diseases. However, the paucity of labelled data inhibits classification models' training. Researchers have turned to generative deep learning techniques combined with signal processing to augment the existing data and improve cardiac auscultation classification models to overcome this challenge. However, the primary focus of prior studies has been on model performance as opposed to model robustness. Robustness, in this case, is defined as both the in-distribution and out-of-distribution performance by measures such as Matthew's correlation coefficient. This work shows that more robust abnormal heart sound classifiers can be trained using an augmented dataset. The augmentations consist of traditional audio approaches and the creation of synthetic audio conditionally generated using the WaveGrad and DiffWave diffusion models. It is found that both the in-distribution and out-of-distribution performance can be improved over various datasets when training a convolutional neural network-based classification model with this augmented dataset. With the performance increase encompassing not only accuracy but also balanced accuracy and Matthew's correlation coefficient, an augmented dataset significantly contributes to resolving issues of imbalanced datasets. This, in turn, helps provide a more general and robust classifier.

</details>


### [314] [Target Speaker Extraction through Comparing Noisy Positive and Negative Audio Enrollments](https://arxiv.org/pdf/2502.16611)
*Shitong Xu, Yiyuan Yang, Niki Trigoni, Andrew Markham*

Main category: cs.SD

TL;DR: A novel enrollment strategy for target speaker extraction uses noisy enrollments (Positive and Negative Enrollments) to encode speaker information, outperforming prior works by 2.1 dB SI-SNRi and reducing training steps by 60%.


<details>
  <summary>Details</summary>
Motivation: Clean audio samples for target speaker identity are often unavailable, especially in noisy environments like cocktail parties.

Method: Uses noisy enrollments (Positive and Negative Enrollments) to encode target speaker information and a two-stage training strategy.

Result: Achieves 2.1 dB higher SI-SNRi and 60% faster convergence to 3 dB SNR compared to prior works.

Conclusion: The method sets a new state-of-the-art for monaural target speaker extraction with noisy enrollments.

Abstract: Target speaker extraction focuses on isolating a specific speaker's voice from an audio mixture containing multiple speakers. To provide information about the target speaker's identity, prior works have utilized clean audio samples as conditioning inputs. However, such clean audio examples are not always readily available. For instance, obtaining a clean recording of a stranger's voice at a cocktail party without leaving the noisy environment is generally infeasible. Limited prior research has explored extracting the target speaker's characteristics from noisy enrollments, which may contain overlapping speech from interfering speakers. In this work, we explore a novel enrollment strategy that encodes target speaker information from the noisy enrollment by comparing segments where the target speaker is talking (Positive Enrollments) with segments where the target speaker is silent (Negative Enrollments). Experiments show the effectiveness of our model architecture, which achieves over 2.1 dB higher SI-SNRi compared to prior works in extracting the monaural speech from the mixture of two speakers. Additionally, the proposed two-stage training strategy accelerates convergence, reducing the number of optimization steps required to reach 3 dB SNR by 60\%. Overall, our method achieves state-of-the-art performance in the monaural target speaker extraction conditioned on noisy enrollments.

</details>


### [315] [Discrete Audio Tokens: More Than a Survey!](https://arxiv.org/pdf/2506.10274)
*Pooneh Mousavi, Gallil Maimon, Adel Moumen, Darius Petermann, Jiatong Shi, Haibin Wu, Haici Yang, Anastasia Kuznetsova, Artem Ploujnikov, Ricard Marxer, Bhuvana Ramabhadran, Benjamin Elizalde, Loren Lugosch, Jinyu Li, Cem Subakan, Phil Woodland, Minje Kim, Hung-yi Lee, Shinji Watanabe, Yossi Adi, Mirco Ravanelli*

Main category: cs.SD

TL;DR: A systematic review and benchmark of discrete audio tokenizers, covering speech, music, and general audio, with a proposed taxonomy and evaluation across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of unified comparisons in discrete audio tokenization and provide insights for future research.

Method: Proposes a taxonomy of tokenization approaches, evaluates tokenizers on reconstruction, downstream tasks, and acoustic language modeling, and conducts ablation studies.

Result: Highlights key limitations, practical considerations, and open challenges in discrete audio tokenization.

Conclusion: Offers guidance for future research in this rapidly evolving field, with results and a tokenizer database available on a dedicated website.

Abstract: Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks. They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [316] [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/pdf/2506.13771)
*Banseok Lee, Dongkyu Kim, Youngcheon You, Youngmin Kim*

Main category: cs.LG

TL;DR: LittleBit is a novel method for extreme LLM compression, achieving 31× memory reduction with 0.1 bits per weight (BPW) through low-rank factorization and multi-scale compensation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of memory and computational costs in deploying large language models (LLMs), especially performance degradation in sub-1-bit quantization.

Method: Uses low-rank matrix factorization and binarization, with multi-scale compensation (row, column, and latent dimension). Introduces Dual-SVID for stable QAT initialization and Residual Compensation for error mitigation.

Result: Achieves 31× memory reduction (e.g., Llama2-13B to under 0.9 GB) and outperforms leading methods (0.1 BPW surpasses 0.7 BPW). Kernel-level benchmarks show 5× speedup over FP16.

Conclusion: LittleBit enables efficient deployment of powerful LLMs in resource-constrained environments, setting a superior size-performance trade-off.

Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.

</details>


### [317] [MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs](https://arxiv.org/pdf/2506.13772)
*Zhenyan Lu, Daliang Xu, Dongqi Cai, Zexi Li, Wei Liu, Fangming Liu, Shangguang Wang, Mengwei Xu*

Main category: cs.LG

TL;DR: MobiEdit is a mobile framework for efficient knowledge editing in LLMs, reducing memory, energy, and latency by replacing backpropagation with quantized forward-only gradient estimation.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate on personalized queries, and existing knowledge editing methods are impractical for mobile devices due to resource-heavy backpropagation.

Method: MobiEdit uses quantized forward-only gradient estimation, early stopping, and prefix caching to enable efficient editing on mobile NPUs.

Result: Achieves 7.6× less memory, 14.7× less energy, and 3.6× less latency for editing a 3B-parameter model on mobile devices.

Conclusion: MobiEdit enables real-time, efficient knowledge editing for LLMs on mobile devices, addressing the limitations of prior methods.

Abstract: Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$ less latency compared to previous knowledge editing methods.

</details>


### [318] [Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment](https://arxiv.org/pdf/2506.13781)
*Pablo Ariño Fernández*

Main category: cs.LG

TL;DR: JobShopLib is a modular library introduced to simplify experimentation with deep learning models for job shop scheduling, outperforming traditional methods and highlighting the importance of feature customization.


<details>
  <summary>Details</summary>
Motivation: The complexity and lack of modular tools for experimenting with deep learning models in job shop scheduling motivated the creation of JobShopLib.

Method: The paper introduces JobShopLib, a modular library for customizing graph representation, node features, action space, and reward functions, and trains dispatchers using imitation learning.

Result: A GNN model trained with JobShopLib achieved near state-of-the-art results, outperforming traditional graph-based dispatchers.

Conclusion: JobShopLib provides essential tools for future research, indicating significant potential for further improvements in deep learning-based job shop scheduling.

Abstract: The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.

</details>


### [319] [Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction](https://arxiv.org/pdf/2506.13786)
*Vuong M. Ngo, Tran Quang Vinh, Patricia Kearney, Mark Roantree*

Main category: cs.LG

TL;DR: The paper introduces an enhanced bagging ensemble regression model (EBMBag+) for predicting diabetes prevalence in U.S. cities, outperforming baseline models with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate state-level diabetes predictions are crucial for healthcare planning, but incomplete data poses challenges.

Method: Integrated diabetes datasets (2011-2021), developed EBMBag+ for time series forecasting, and compared it with baseline models (SVMReg, BDTree, LSBoost, NN, LSTM, ERMBag).

Result: EBMBag+ achieved the best performance (MAE: 0.41, RMSE: 0.53, MAPE: 4.01, R2: 0.9).

Conclusion: EBMBag+ is effective for diabetes prevalence forecasting, aiding targeted healthcare interventions.

Abstract: Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.

</details>


### [320] [Light Aircraft Game : Basic Implementation and training results analysis](https://arxiv.org/pdf/2506.14164)
*Hanzhong Cao*

Main category: cs.LG

TL;DR: The paper evaluates HAPPO and HASAC in a MARL combat environment (LAG), finding HASAC better for simple tasks and HAPPO for dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of on-policy (HAPPO) and off-policy (HASAC) MARL methods in a cooperative-competitive, partially observable combat setting.

Method: Evaluated HAPPO (hierarchical PPO) and HASAC (hierarchical SAC) in the LAG environment, analyzing training stability, rewards, and coordination.

Result: HASAC excels in simpler tasks (No Weapon), while HAPPO adapts better to dynamic scenarios (ShootMissile).

Conclusion: The study highlights trade-offs between on-policy and off-policy MARL methods, with HAPPO showing adaptability in complex settings.

Abstract: This paper investigates multi-agent reinforcement learning (MARL) in a partially observable, cooperative-competitive combat environment known as LAG. We describe the environment's setup, including agent actions, hierarchical controls, and reward design across different combat modes such as No Weapon and ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy hierarchical variant of PPO, and HASAC, an off-policy method based on soft actor-critic. We analyze their training stability, reward progression, and inter-agent coordination capabilities. Experimental results show that HASAC performs well in simpler coordination tasks without weapons, while HAPPO demonstrates stronger adaptability in more dynamic and expressive scenarios involving missile combat. These findings provide insights into the trade-offs between on-policy and off-policy methods in multi-agent settings.

</details>


### [321] [Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles](https://arxiv.org/pdf/2506.13828)
*Abdullah Burkan Bereketoglu*

Main category: cs.LG

TL;DR: A hybrid meta-learning framework combining physics-inspired simulation, CNN-LSTM, VAE, Isolation Forests, and DA-RNN for forecasting and anomaly detection in nonlinear systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of forecasting and anomaly detection in nonlinear, nonstationary, and stochastic dynamical systems where complete physical models may be unavailable.

Method: Integrates physics-inspired simulation with CNN-LSTM for spatio-temporal features, VAE for anomaly scoring, Isolation Forests for outlier detection, and DA-RNN for forecasting. A meta-learner combines these outputs.

Result: Outperforms standalone models in anomaly localization, generalization, and robustness to nonlinear deviations.

Conclusion: Provides a versatile, data-driven solution for early defect identification and predictive monitoring in nonlinear systems.

Abstract: We propose a hybrid meta-learning framework for forecasting and anomaly detection in nonlinear dynamical systems characterized by nonstationary and stochastic behavior. The approach integrates a physics-inspired simulator that captures nonlinear growth-relaxation dynamics with random perturbations, representative of many complex physical, industrial, and cyber-physical systems. We use CNN-LSTM architectures for spatio-temporal feature extraction, Variational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation Forests for residual-based outlier detection in addition to a Dual-Stage Attention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of the generated simulation data. To create composite anomaly forecasts, these models are combined using a meta-learner that combines forecasting outputs, reconstruction errors, and residual scores. The hybrid ensemble performs better than standalone models in anomaly localization, generalization, and robustness to nonlinear deviations, according to simulation-based experiments. The framework provides a broad, data-driven approach to early defect identification and predictive monitoring in nonlinear systems, which may be applied to a variety of scenarios where complete physical models might not be accessible.

</details>


### [322] [Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation](https://arxiv.org/pdf/2506.13831)
*Jitian Zhao, Chenghui Li, Frederic Sala, Karl Rohe*

Main category: cs.LG

TL;DR: A hypothesis testing framework is introduced to statistically validate concepts in CLIP embeddings, improving interpretability and reducing spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Current concept-based methods lack statistical rigor, making validation and comparison difficult.

Method: A hypothesis testing framework identifies rotation-sensitive structures in CLIP embeddings, followed by a post-hoc concept decomposition method with theoretical guarantees.

Result: The method outperforms others in reconstruction error and increases worst-group accuracy by 22.6% by removing spurious concepts.

Conclusion: The approach provides statistically validated, interpretable concepts and mitigates spurious cues, enhancing model reliability.

Abstract: Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.

</details>


### [323] [Evolvable Conditional Diffusion](https://arxiv.org/pdf/2506.13834)
*Zhao Wei, Chin Chun Ooi, Abhishek Gupta, Jian Cheng Wong, Pao-Hsiung Chiu, Sheares Xue Wen Toh, Yew-Soon Ong*

Main category: cs.LG

TL;DR: An evolvable conditional diffusion method is introduced for guiding generative processes using black-box, non-differentiable multi-physics models, enabling autonomous scientific discovery without gradient computation.


<details>
  <summary>Details</summary>
Motivation: To leverage black-box, non-differentiable multi-physics models (common in fields like fluid dynamics and electromagnetics) for guiding generative processes in scientific discovery.

Method: Formulates guidance as an optimization problem, optimizing for a fitness function by updating descriptive statistics of the denoising distribution, derived from probabilistic evolution principles.

Result: Validated in AI for Science scenarios (fluidic topology and meta-surface design), the method generates designs meeting optimization objectives without differentiable proxies.

Conclusion: The method provides effective guidance-based diffusion for non-differentiable models, enhancing scientific discovery.

Abstract: This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.

</details>


### [324] [Knowledge Bridger: Towards Training-free Missing Modality Completion](https://arxiv.org/pdf/2502.19834)
*Guanzhou Ke, Shengfeng He, Xiao Li Wang, Bo Wang, Guoqing Chao, Yuanyang Zhang, Yi Xie, HeXing Su*

Main category: cs.LG

TL;DR: A training-free framework, 'Knowledge Bridger', leverages large multimodal models (LMMs) for missing modality completion, outperforming existing methods in OOD scenarios.


<details>
  <summary>Details</summary>
Motivation: To develop a resource-efficient and robust missing modality completion model that generalizes well in out-of-domain (OOD) scenarios.

Method: Modality-agnostic framework integrating generation and ranking of missing modalities using domain-specific priors and knowledge graphs.

Result: Consistently outperforms competing methods, including in OOD generalization, and shows superiority over direct LMM usage.

Conclusion: The Knowledge Bridger framework offers a robust, efficient solution for missing modality completion, with potential applications across domains.

Abstract: Previous successful approaches to missing modality completion rely on carefully designed fusion techniques and extensive pre-training on complete data, which can limit their generalizability in out-of-domain (OOD) scenarios. In this study, we pose a new challenge: can we develop a missing modality completion model that is both resource-efficient and robust to OOD generalization? To address this, we present a training-free framework for missing modality completion that leverages large multimodal models (LMMs). Our approach, termed the "Knowledge Bridger", is modality-agnostic and integrates generation and ranking of missing modalities. By defining domain-specific priors, our method automatically extracts structured information from available modalities to construct knowledge graphs. These extracted graphs connect the missing modality generation and ranking modules through the LMM, resulting in high-quality imputations of missing modalities. Experimental results across both general and medical domains show that our approach consistently outperforms competing methods, including in OOD generalization. Additionally, our knowledge-driven generation and ranking techniques demonstrate superiority over variants that directly employ LMMs for generation and ranking, offering insights that may be valuable for applications in other domains.

</details>


### [325] [Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study](https://arxiv.org/pdf/2506.13836)
*Dang Viet Anh Nguyen, Carlos Lima Azevedo, Tomer Toledo, Filipe Rodrigues*

Main category: cs.LG

TL;DR: T-REX is a SUMO-based framework for evaluating RL-TSC methods under dynamic traffic incidents, revealing performance trade-offs between independent and hierarchical methods.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored robustness of RL-TSC methods under real-world disruptions like traffic incidents.

Method: Introduces T-REX, a simulation framework modeling realistic traffic behaviors (rerouting, speed adaptation, lane-changing) and proposes robustness metrics.

Result: Independent methods perform well in stable conditions but degrade under incidents; hierarchical methods offer stability but with slower convergence.

Conclusion: Highlights the need for robustness-aware RL-TSC design, with T-REX providing a standardized benchmarking platform.

Abstract: Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.

</details>


### [326] [Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy](https://arxiv.org/pdf/2506.13838)
*Lorena Poenaru-Olaru, June Sallou, Luis Cruz, Jan Rellermeyer, Arie van Deursen*

Main category: cs.LG

TL;DR: The paper explores energy-efficient retraining techniques for ML systems, showing that using recent data and updating only when needed can reduce energy consumption by 25-40% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the environmental impact of energy-intensive ML model retraining by identifying sustainable techniques.

Method: Study the energy consumption and accuracy of common retraining techniques, focusing on using recent data and updating only when necessary.

Result: Retraining with recent data reduces energy by 25%, and updating only when needed saves up to 40%. Both methods maintain accuracy.

Conclusion: Sustainable retraining techniques can significantly reduce energy consumption, guiding ML practitioners toward eco-friendly practices.

Abstract: The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.

</details>


### [327] [SatHealth: A Multimodal Public Health Dataset with Satellite-based Environmental Factors](https://arxiv.org/pdf/2506.13842)
*Yuanlong Wang, Pengqi Wang, Changchang Yin, Ping Zhang*

Main category: cs.LG

TL;DR: SatHealth is a novel dataset combining multimodal spatiotemporal data to improve AI models in healthcare by incorporating environmental data, satellite images, and SDoH indicators.


<details>
  <summary>Details</summary>
Motivation: Existing studies lack long-term, fine-grained environmental data, limiting AI model performance in healthcare.

Method: Developed SatHealth, a dataset with environmental data, satellite images, disease prevalences, and SDoH indicators, tested in regional public health modeling and personal disease risk prediction.

Result: Environmental data significantly improves AI model performance and spatiotemporal generalizability.

Conclusion: SatHealth provides a foundational framework for integrating environmental data in healthcare research, with plans to expand coverage and a web-based tool for accessibility.

Abstract: Living environments play a vital role in the prevalence and progression of diseases, and understanding their impact on patient's health status becomes increasingly crucial for developing AI models. However, due to the lack of long-term and fine-grained spatial and temporal data in public and population health studies, most existing studies fail to incorporate environmental data, limiting the models' performance and real-world application. To address this shortage, we developed SatHealth, a novel dataset combining multimodal spatiotemporal data, including environmental data, satellite images, all-disease prevalences estimated from medical claims, and social determinants of health (SDoH) indicators. We conducted experiments under two use cases with SatHealth: regional public health modeling and personal disease risk prediction. Experimental results show that living environmental information can significantly improve AI models' performance and temporal-spatial generalizability on various tasks. Finally, we deploy a web-based application to provide an exploration tool for SatHealth and one-click access to both our data and regional environmental embedding to facilitate plug-and-play utilization. SatHealth is now published with data in Ohio, and we will keep updating SatHealth to cover the other parts of the US. With the web application and published code pipeline, our work provides valuable angles and resources to include environmental data in healthcare research and establishes a foundational framework for future research in environmental health informatics.

</details>


### [328] [StaQ it! Growing neural networks for Policy Mirror Descent](https://arxiv.org/pdf/2506.13862)
*Alena Shilova, Alex Davey, Brahim Driss, Riad Akrour*

Main category: cs.LG

TL;DR: The paper proposes PMD-like algorithms (StaQ) that retain only the last M Q-functions, ensuring convergence and stability in deep RL, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Regularization in RL improves exploration and stability, but existing PMD frameworks are impractical due to intractable memory requirements.

Method: Develops PMD-like algorithms (StaQ) by limiting stored Q-functions to the last M, ensuring tractability and convergence.

Result: StaQ achieves competitive performance with deep RL baselines, reduces oscillation, and maintains theoretical guarantees.

Conclusion: StaQ provides a stable and practical PMD implementation, advancing the potential for fully stable deep RL algorithms.

Abstract: In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.

</details>


### [329] [Scaling Algorithm Distillation for Continuous Control with Mamba](https://arxiv.org/pdf/2506.13892)
*Samuel Beaussant, Mehdi Mounsif*

Main category: cs.LG

TL;DR: Algorithm Distillation (AD) improves In-Context Reinforcement Learning (ICRL) using S6 models (Mamba), outperforming transformers in complex environments and scaling better with long contexts.


<details>
  <summary>Details</summary>
Motivation: Transformers in AD face limitations due to quadratic complexity, restricting use to simple environments. S6 models offer linear scaling, enabling better performance in complex settings.

Method: Replace transformers with S6-based Mamba models for AD, testing in four complex Meta RL environments.

Result: Mamba outperforms transformers in AD, scales better with long contexts, and competes with SOTA online meta RL baselines.

Conclusion: S6 models (Mamba) are superior for AD, enabling effective ICRL in complex environments and long contexts.

Abstract: Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.

</details>


### [330] [Enhancing interpretability of rule-based classifiers through feature graphs](https://arxiv.org/pdf/2506.13903)
*Christel Sirocchi, Damiano Verda*

Main category: cs.LG

TL;DR: A framework for estimating feature contributions in rule-based systems, featuring graph-based visualization, a novel importance metric, and a rule set comparison metric, validated on clinical datasets.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of interpreting complex rule-based systems in domains like healthcare, where transparency is critical.

Method: Proposes a graph-based visualization, a feature importance metric, and a rule set distance metric, tested on clinical datasets and multiple rule-based methods.

Result: Uncovers insights on feature predictive value, aids in identifying risk factors and biomarkers, and shows competitive performance on benchmarks.

Conclusion: The framework enhances interpretability and robustness in rule-based systems, with practical applications in healthcare decision support.

Abstract: In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: https://github.com/ChristelSirocchi/rule-graph.

</details>


### [331] [GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations](https://arxiv.org/pdf/2506.13906)
*Milad Ramezankhani, Janak M. Patel, Anirudh Deodhar, Dagnachew Birru*

Main category: cs.LG

TL;DR: GITO is a graph-informed transformer operator for solving PDEs on irregular geometries, combining a hybrid graph transformer and a transformer neural operator for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning complex PDE systems on irregular geometries and non-uniform meshes, which existing methods struggle with.

Method: GITO uses a hybrid graph transformer (HGT) for local and long-range spatial relationships and a transformer neural operator (TNO) for discretization-invariant predictions.

Result: GITO outperforms existing transformer-based neural operators on benchmark PDE tasks.

Conclusion: GITO enables efficient, mesh-agnostic surrogate solvers for engineering applications.

Abstract: We present a novel graph-informed transformer operator (GITO) architecture for learning complex partial differential equation systems defined on irregular geometries and non-uniform meshes. GITO consists of two main modules: a hybrid graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages a graph neural network (GNN) to encode local spatial relationships and a transformer to capture long-range dependencies. A self-attention fusion layer integrates the outputs of the GNN and transformer to enable more expressive feature learning on graph-structured data. TNO module employs linear-complexity cross-attention and self-attention layers to map encoded input functions to predictions at arbitrary query locations, ensuring discretization invariance and enabling zero-shot super-resolution across any mesh. Empirical results on benchmark PDE tasks demonstrate that GITO outperforms existing transformer-based neural operators, paving the way for efficient, mesh-agnostic surrogate solvers in engineering applications.

</details>


### [332] [Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring](https://arxiv.org/pdf/2506.13909)
*Xinyuan Tu, Haocheng Zhang, Tao Chengxu, Zuyi Chen*

Main category: cs.LG

TL;DR: The paper explores few-shot learning (FSL) for industrial time-series data, introducing a label-aware episodic sampler and comparing metric-based and gradient-based FSL paradigms. Results show lightweight CNNs with metric learning outperform larger models in scarce data scenarios.


<details>
  <summary>Details</summary>
Motivation: FSL is promising for industrial time-series data due to high annotation costs for defects. The study aims to benchmark FSL methods and improve performance with a novel sampling technique.

Method: A label-aware episodic sampler is introduced to handle multi-label sequences. Two FSL paradigms (Prototypical Network and MAML) are tested with three backbones (1D CNN, InceptionTime, transformer Moment).

Result: InceptionTime + Prototypical Network achieves 0.944 weighted F1 (multi-class) and 0.935 (multi-label), outperforming larger models by up to 5.3%. Metric learning consistently beats MAML, and label-aware sampling adds 1.7% F1.

Conclusion: Lightweight CNNs with metric learning outperform large models in data-scarce settings. The findings challenge the superiority of large foundation models and encourage FSL adoption in manufacturing.

Abstract: Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information.
  Two FSL paradigms are investigated: the metric-based \emph{Prototypical Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the multi-class regime and \textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\% F1 over traditional class-based sampling.
  These findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.

</details>


### [333] [Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization](https://arxiv.org/pdf/2506.13911)
*Arie Soeteman, Balder ten Cate*

Main category: cs.LG

TL;DR: HEGNNs extend GNNs with hierarchical node individualization, forming expressive models that can distinguish graphs up to isomorphism, outperforming traditional GNNs.


<details>
  <summary>Details</summary>
Motivation: To enhance the expressive power of GNNs by incorporating hierarchical node individualization, inspired by graph isomorphism testing paradigms.

Method: Proposes HEGNNs, a hierarchy of models with increasing expressiveness, characterized using graded hybrid logic and compared to higher-order GNNs and color refinement algorithms.

Result: HEGNNs outperform traditional GNNs, demonstrating practical feasibility and benefits, even without local homomorphism count features.

Conclusion: HEGNNs provide a powerful and scalable extension to GNNs, bridging theoretical expressiveness with practical performance.

Abstract: We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.

</details>


### [334] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/pdf/2506.13916)
*Isaias Banales, Arturo Jaramillo, Heli Ricalde Guerrero*

Main category: cs.LG

TL;DR: Proposes BSVGD, a particle-based variational inference method for multimodal distributions, extending SVGD with a branching mechanism for better exploration.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of SVGD in exploring multimodal distributions by introducing a branching mechanism.

Method: Extends SVGD with a random branching mechanism to explore state space, supported by theoretical convergence guarantees.

Result: Validated through numerical experiments, showing improved performance over SVGD in Wasserstein distance and computational time.

Conclusion: BSVGD effectively explores multimodal distributions, outperforming SVGD in both accuracy and efficiency.

Abstract: We propose a novel particle-based variational inference method designed to work with multimodal distributions. Our approach, referred to as Branched Stein Variational Gradient Descent (BSVGD), extends the classical Stein Variational Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism that encourages the exploration of the state space. In this work, a theoretical guarantee for the convergence in distribution is presented, as well as numerical experiments to validate the suitability of our algorithm. Performance comparisons between the BSVGD and the SVGD are presented using the Wasserstein distance between samples and the corresponding computational times.

</details>


### [335] [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/pdf/2506.13923)
*Vaskar Nath, Elaine Lau, Anisha Gunjal, Manasi Sharma, Nikhil Baharte, Sean Hendryx*

Main category: cs.LG

TL;DR: RLVR training improves reasoning models by compressing pass@k into pass@1 and enabling capability gain, with self-distillation as the primary driver. The $	ext{Guide}$ algorithm, incorporating hints and optimizing off-policy trajectories, enhances performance, showing up to 4% improvement on math benchmarks.


<details>
  <summary>Details</summary>
Motivation: To understand how reasoning models trained with RLVR learn to solve new problems and improve generalization.

Method: Analyzes RLVR's impact on pass@k and capability gain, introduces $	ext{Guide}$ algorithm for adaptive hint incorporation and off-policy optimization, tested on models from 0.5B to 72B parameters.

Result: RLVR compresses pass@k into pass@1 and enables capability gain; $	ext{Guide}$ improves pass@k rates and generalization, with up to 4% macro-average improvement on math benchmarks.

Conclusion: RLVR and $	ext{Guide}$ enhance reasoning models' problem-solving and generalization, with self-distillation and adaptive hinting as key drivers.

Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.

</details>


### [336] [ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture](https://arxiv.org/pdf/2506.13935)
*Vishesh Kumar Tanwar, Soumik Sarkar, Asheesh K. Singh, Sajal K. Das*

Main category: cs.LG

TL;DR: ReinDSplit is a reinforcement learning-driven framework that dynamically optimizes DNN split points for edge devices in precision agriculture, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional split learning frameworks struggle with device heterogeneity in agriculture, causing inefficiencies and performance issues.

Method: Uses Q-learning to dynamically tailor DNN split points, balancing workloads and latency thresholds for each device.

Result: Achieves 94.31% accuracy with MobileNetV2 on insect classification datasets.

Conclusion: ReinDSplit enhances split learning by integrating RL for better resource efficiency, privacy, and scalability in heterogeneous environments.

Abstract: To empower precision agriculture through distributed machine learning (DML), split learning (SL) has emerged as a promising paradigm, partitioning deep neural networks (DNNs) between edge devices and servers to reduce computational burdens and preserve data privacy. However, conventional SL frameworks' one-split-fits-all strategy is a critical limitation in agricultural ecosystems where edge insect monitoring devices exhibit vast heterogeneity in computational power, energy constraints, and connectivity. This leads to straggler bottlenecks, inefficient resource utilization, and compromised model performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement learning (RL)-driven framework that dynamically tailors DNN split points for each device, optimizing efficiency without sacrificing accuracy. Specifically, a Q-learning agent acts as an adaptive orchestrator, balancing workloads and latency thresholds across devices to mitigate computational starvation or overload. By framing split layer selection as a finite-state Markov decision process, ReinDSplit convergence ensures that highly constrained devices contribute meaningfully to model training over time. Evaluated on three insect classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit pioneers a paradigm shift in SL by harmonizing RL for resource efficiency, privacy, and scalability in heterogeneous environments.

</details>


### [337] [Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers](https://arxiv.org/pdf/2506.13958)
*Leonardo Guiducci, Antonio Rizzo, Giovanna Maria Dimitri*

Main category: cs.LG

TL;DR: EDTs with intrinsic motivation improve performance, and this paper explains how intrinsic motivation shapes embeddings in EDTs through statistical analysis of embedding properties.


<details>
  <summary>Details</summary>
Motivation: To understand how intrinsic motivation mechanisms in EDTs influence learned embeddings and improve decision-making.

Method: A post-hoc explainability framework analyzing embedding properties (covariance, vector magnitudes, orthogonality) to reveal representational structures.

Result: Different intrinsic motivation variants create distinct embedding structures, with environment-specific correlations to performance.

Conclusion: Intrinsic motivation acts as a representational prior, shaping embedding geometry in biologically plausible ways to enhance decision-making.

Abstract: Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.

</details>


### [338] [Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble](https://arxiv.org/pdf/2506.13972)
*Zhiqi Wang, Chengyu Zhang, Yuetian Chen, Nathalie Baracaldo, Swanand Kadhe, Lei Yu*

Main category: cs.LG

TL;DR: The paper highlights disparities in membership inference attacks (MIAs) and proposes an ensemble framework to improve privacy evaluation.


<details>
  <summary>Details</summary>
Motivation: Prior MIA research focuses on performance metrics but overlooks disparities among attacks, affecting reliability and completeness of privacy assessments.

Method: A novel framework based on coverage and stability analysis is used to investigate MIA disparities, followed by an ensemble framework with three strategies.

Result: Experiments reveal significant disparities in MIAs, their causes, and implications for privacy evaluation. The ensemble framework enhances attack power and robustness.

Conclusion: The proposed framework improves MIA reliability and offers a comprehensive approach for privacy evaluation.

Abstract: Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.

</details>


### [339] [Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability](https://arxiv.org/pdf/2506.13974)
*Michael Crawshaw, Blake Woodworth, Mingrui Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous objectives requires stepsizes $η\leq 1/K$ where $K$ is the communication interval, which ensures monotonic decrease of the objective. In contrast, we analyze Local Gradient Descent for logistic regression with separable, heterogeneous data using any stepsize $η> 0$. With $R$ communication rounds and $M$ clients, we show convergence at a rate $\mathcal{O}(1/ηK R)$ after an initial unstable phase lasting for $\widetilde{\mathcal{O}}(ηK M)$ rounds. This improves upon the existing $\mathcal{O}(1/R)$ rate for general smooth, convex objectives. Our analysis parallels the single machine analysis of~\cite{wu2024large} in which instability is caused by extremely large stepsizes, but in our setting another source of instability is large local updates with heterogeneous objectives.

</details>


### [340] [HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting](https://arxiv.org/pdf/2506.13981)
*Thanh Dan Bui*

Main category: cs.LG

TL;DR: HAELT, a hybrid deep learning framework, outperforms in high-frequency stock price prediction by combining noise mitigation, dynamic attention, and adaptive ensembling.


<details>
  <summary>Details</summary>
Motivation: High-frequency stock prediction is difficult due to noise, volatility, and non-stationarity, requiring advanced methods for accurate forecasting.

Method: HAELT integrates ResNet for noise reduction, temporal self-attention for dynamic history focus, and a hybrid LSTM-Transformer core for local and long-range dependencies, with adaptive ensembling.

Result: HAELT achieves the highest F1-Score on AAPL hourly data (2024-2025), effectively identifying price movements.

Conclusion: HAELT shows promise for robust financial forecasting and algorithmic trading.

Abstract: High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.

</details>


### [341] [Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems](https://arxiv.org/pdf/2506.13987)
*Md Abrar Jahin, Adiba Abid, M. F. Mridha*

Main category: cs.LG

TL;DR: QCL-MixNet is a quantum-informed contrastive learning framework with kNN-guided dynamic mixup, outperforming existing methods in handling class-imbalanced tabular data.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of conventional methods (e.g., overfitting, poor generalization) in detecting rare instances for safety-critical expert systems.

Method: Integrates quantum entanglement-inspired layers, adaptive mixup, and a hybrid loss function to enhance minority class representation and classification robustness.

Result: Outperforms 20 state-of-the-art baselines on 18 datasets in macro-F1 and recall, validated by ablation studies.

Conclusion: QCL-MixNet sets a new benchmark for tabular imbalance handling, supported by theoretical robustness and expressiveness.

Abstract: Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness.

</details>


### [342] [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/pdf/2506.13992)
*An Luo, Xun Xian, Jin Du, Fangqiao Tian, Ganghua Wang, Ming Zhong, Shengchun Zhao, Xuan Bi, Zirui Liu, Jiawei Zhou, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding*

Main category: cs.LG

TL;DR: AssistedDS benchmark evaluates LLMs' ability to use domain knowledge in tabular tasks, showing they often uncritically adopt adversarial info, struggle with helpful guidance, and mishandle time-series/categorical data.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can critically leverage external domain knowledge like human data scientists in automated data science workflows.

Method: Introduces AssistedDS benchmark with synthetic and real-world datasets, providing curated helpful/adversarial documents for evaluation.

Result: LLMs often adopt adversarial info uncritically, struggle to counteract its effects, and make errors in time-series and categorical data handling.

Conclusion: Highlights a gap in LLMs' critical evaluation of domain knowledge, pointing to the need for more robust, knowledge-aware automated systems.

Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.

</details>


### [343] [Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences](https://arxiv.org/pdf/2506.13996)
*Stas Bekman, Samyam Rajbhandari, Michael Wyatt, Jeff Rasley, Tunji Ruwase, Zhewei Yao, Aurick Qiao, Yuxiong He*

Main category: cs.LG

TL;DR: ALST enables long sequence training for HF models with optimizations for single and multi-GPU memory, supporting up to 15M tokens.


<details>
  <summary>Details</summary>
Motivation: Long sequence training is challenging due to memory constraints and lack of open-source solutions.

Method: ALST combines attention-agnostic single and multi-GPU memory optimizations.

Result: Supports 500K tokens on one H100, 3.7M on 8xH100, and 15M on a 4-node cluster (400x improvement).

Conclusion: ALST makes long sequence training accessible for HF models and is open-sourced.

Abstract: Long sequences are critical for applications like RAG, long document summarization, multi-modality, etc., and modern LLMs, like Llama 4 Scout, support max sequence length of up to 10 million tokens. However, outside of enterprise labs, long sequence training is challenging for the AI community with limited system support in the open-source space.
  Out-of-box, even on a modern NVIDIA H100 80GB GPU cluster, training Llama 8B model with sequence over 32K runs out of memory on a basic Hugging Face (HF) model due to two reasons: i) LLM training workloads are not optimized to fully leverage a single GPU memory, ii) existing solutions for leveraging multiple GPU memory are not easily available to HF models, making long sequence training inaccessible.
  We address this with Arctic Long Sequence Training (ALST). It offers a combination of attention-agnostic single GPU and multi-GPU memory optimizations, that enables it to support out-of-box training of multi-million sequence length for a wide variety of HF models.
  ALST supports training Meta's Llama 8B model with 500K sequence length on a single H100 GPU, 3.7M on a single 8xH100 GPU node, and over 15M on a 4 node cluster, an increase of over 400x compared to the 32K baseline for the latter. ALST is fully compatible with HF models and open-sourced via Deepspeed https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ and Arctic Training https://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md.

</details>


### [344] [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/pdf/2506.14002)
*Siyu Chen, Heejune Sheen, Xuyuan Xiong, Tianhao Wang, Zhuoran Yang*

Main category: cs.LG

TL;DR: A novel statistical framework and SAE training algorithm (GBA) with theoretical guarantees for feature recovery in LLMs, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of rigorous guarantees and practical issues (hyperparameter sensitivity, instability) in existing SAE training methods for LLM interpretation.

Method: Proposes a statistical framework for feature identifiability and introduces a bias adaptation-based SAE training algorithm (GBA).

Result: Theoretically proves feature recovery under the model; GBA outperforms benchmarks on LLMs up to 1.5B parameters.

Conclusion: Advances SAE training with theoretical guarantees, enhancing mechanistic interpretability for transparent AI.

Abstract: We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.

</details>


### [345] [Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs](https://arxiv.org/pdf/2506.14003)
*Yiwei Chen, Soumyadeep Pal, Yimeng Zhang, Qing Qu, Sijia Liu*

Main category: cs.LG

TL;DR: The paper identifies a vulnerability in machine unlearning (MU) for large language models (LLMs), where unlearning leaves detectable traces in model behavior and internal representations, posing a risk of reverse-engineering forgotten information.


<details>
  <summary>Details</summary>
Motivation: To highlight the risks of unlearning trace detection in LLMs, which can compromise data privacy, copyright enforcement, and harm mitigation efforts.

Method: The study uses supervised classifiers to detect unlearning traces from textual outputs and analyzes intermediate activations to identify low-dimensional manifolds in activation space.

Result: Experiments show over 90% accuracy in detecting unlearning traces, even with forget-irrelevant inputs, revealing persistent signatures post-unlearning.

Conclusion: Unlearning leaves measurable traces, introducing a new vulnerability where forgotten information can be reverse-engineered if a model is identified as unlearned.

Abstract: Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](https://github.com/OPTML-Group/Unlearn-Trace).

</details>


### [346] [Bures-Wasserstein Flow Matching for Graph Generation](https://arxiv.org/pdf/2506.14020)
*Keyue Jiang, Jiahao Cui, Xiaowen Dong, Laura Toni*

Main category: cs.LG

TL;DR: BWFlow introduces a flow-matching framework for graph generation, addressing limitations of Euclidean assumptions in existing methods by modeling joint node-edge evolution using MRFs and optimal transport.


<details>
  <summary>Details</summary>
Motivation: Existing graph generation methods assume Euclidean structure and independent node-edge evolution, which is suboptimal for graphs' non-Euclidean nature and interconnected patterns.

Method: BWFlow models graphs as connected systems via MRFs, uses optimal transport to design probability paths, and adapts to continuous/discrete flow-matching algorithms.

Result: BWFlow achieves competitive performance in graph and molecule generation, with stable training and guaranteed sampling convergence.

Conclusion: BWFlow provides a geometry-respecting, effective framework for graph generation, outperforming traditional methods.

Abstract: Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.

</details>


### [347] [Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data](https://arxiv.org/pdf/2506.14036)
*Tatthapong Srikitrungruang, Sina Aghaee Dabaghan Fard, Matthew Lemon, Jaesung Lee, Yuxiao Zhou*

Main category: cs.LG

TL;DR: IE-PINN is a novel neural network method for robustly estimating heterogeneous elasticity parameters from noisy displacement data, overcoming limitations of existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inverse elasticity problems are unstable, noise-sensitive, and struggle with absolute-scale Young's modulus recovery.

Method: IE-PINN uses three neural networks for displacement, strain, and elasticity fields, a two-phase estimation strategy, and innovations like positional encoding and sine activation.

Result: IE-PINN achieves accurate absolute-scale elasticity estimations under severe noise, outperforming existing methods.

Conclusion: IE-PINN advances elasticity parameter estimation, with potential applications in noisy clinical and mechanical settings.

Abstract: Accurately estimating spatially heterogeneous elasticity parameters, particularly Young's modulus and Poisson's ratio, from noisy displacement measurements remains significantly challenging in inverse elasticity problems. Existing inverse estimation techniques are often limited by instability, pronounced sensitivity to measurement noise, and difficulty in recovering absolute-scale Young's modulus. This work presents a novel Inverse Elasticity Physics-Informed Neural Network (IE-PINN) specifically designed to robustly reconstruct heterogeneous distributions of elasticity parameters from noisy displacement data based on linear elasticity physics. IE-PINN integrates three distinct neural network architectures dedicated to separately modeling displacement fields, strain fields, and elasticity distributions, thereby significantly enhancing stability and accuracy against measurement noise. Additionally, a two-phase estimation strategy is introduced: the first phase recovers relative spatial distributions of Young's modulus and Poisson's ratio, and the second phase calibrates the absolute scale of Young's modulus using imposed loading boundary conditions. Additional methodological innovations, including positional encoding, sine activation functions, and a sequential pretraining protocol, further enhance the model's performance and robustness. Extensive numerical experiments demonstrate that IE-PINN effectively overcomes critical limitations encountered by existing methods, delivering accurate absolute-scale elasticity estimations even under severe noise conditions. This advancement holds substantial potential for clinical imaging diagnostics and mechanical characterization, where measurements typically encounter substantial noise.

</details>


### [348] [Load Balancing Mixture of Experts with Similarity Preserving Routers](https://arxiv.org/pdf/2506.14038)
*Nabil Omi, Siddhartha Sen, Ali Farhadi*

Main category: cs.LG

TL;DR: A novel load balancing loss for Sparse Mixture of Experts (MoE) models improves convergence speed and reduces redundancy by preserving token-wise relational structure.


<details>
  <summary>Details</summary>
Motivation: Current load balancing mechanisms in MoE models lead to inconsistent routing behavior and redundant learning, limiting model capacity and performance.

Method: Introduces a new load balancing loss that encourages consistent expert choices for similar inputs, preserving relational structure.

Result: The proposed loss achieves 36% faster convergence and lower redundancy compared to existing methods.

Conclusion: The novel loss effectively addresses routing inconsistency and redundancy, enhancing MoE model performance.

Abstract: Sparse Mixture of Experts (MoE) models offer a scalable and efficient architecture for training large neural networks by activating only a subset of parameters ("experts") for each input. A learned router computes a distribution over these experts, and assigns input tokens to a small subset. However, without auxiliary balancing mechanisms, routers often converge to using only a few experts, severely limiting model capacity and degrading performance. Most current load balancing mechanisms encourage a distribution over experts that resembles a roughly uniform distribution of experts per token. During training, this can result in inconsistent routing behavior, resulting in the model spending its capacity to learn redundant knowledge. We address this by introducing a novel load balancing loss that preserves token-wise relational structure, encouraging consistent expert choices for similar inputs during training. Our experimental results show that applying our loss to the router results in 36% faster convergence and lower redundancy compared to a popular load balancing loss.

</details>


### [349] [Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature](https://arxiv.org/pdf/2506.14054)
*Joshua Fan, Haodi Xu, Feng Tao, Md Nasim, Marc Grimson, Yiqi Luo, Carla P. Gomes*

Main category: cs.LG

TL;DR: ScIReN combines neural networks and process-based models for interpretable scientific discovery, outperforming black-box models in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Neural networks lack interpretability and adherence to scientific laws, while process-based models are rigid and poorly predictive. ScIReN aims to bridge this gap.

Method: ScIReN uses an interpretable encoder to predict latent parameters, processed by a differentiable decoder with scientific constraints.

Result: ScIReN outperforms black-box models in tasks like carbon flow simulation and ecosystem respiration modeling, while revealing hidden scientific mechanisms.

Conclusion: ScIReN successfully integrates interpretable neural reasoning with process-based models, enhancing both predictive accuracy and scientific insight.

Abstract: Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.

</details>


### [350] [A Regret Perspective on Online Selective Generation](https://arxiv.org/pdf/2506.14067)
*Minjae Lee, Yoonjae Jung, Sangdon Park*

Main category: cs.LG

TL;DR: The paper proposes an online learning algorithm for selective generation under partial feedback to control hallucination in large language models, leveraging bandit problem frameworks and feedback unlocking for improved performance.


<details>
  <summary>Details</summary>
Motivation: Address the hallucination effect in large language models by enabling selective generation (abstaining from uncertain answers) under practical, non-stochastic environments with partial user feedback.

Method: Reduce selective generation to multi-armed bandit problems, introduce a conversion lemma to connect bandit regret to false discovery rate (FDR), and exploit arm structure for feedback unlocking to improve convergence.

Result: The algorithm effectively controls FDR for hallucination while maintaining reasonable selection efficiency (non-abstaining ratio), outperforming baselines in diverse setups.

Conclusion: The proposed method provides a practical and theoretically grounded solution for selective generation under partial feedback, balancing hallucination control and answer efficiency.

Abstract: Large language generative models increasingly interact with humans, while their falsified responses raise concerns. To address this hallucination effect, selectively abstaining from answering, called selective generation, provides an effective way for generators to control the hallucination when it is unsure of their answers. However, as selective generators are interacting under non-stochastic environments and having partial feedback from users on selective generation (e.g., thumbs up or down on the selected answer), learning methods for selective generation under such practical setups are crucial but currently missing. To address these limitations, we propose an online learning algorithm for selective generation under partial feedback. In particular, as learning under partial feedback is well-studied by multi-armed bandit problems, we reduce selective generation to bandits and provide a novel conversion lemma from bandits back to selective generation to leverage any known bandit algorithms and theoretical properties. This mainly connects regret guarantees of bandits to false discovery rate (FDR) guarantees of selective generation for controlling hallucination. However, naively exploiting known bandit algorithms and their regret bounds suffers from slow convergence speed in practice due the nature of partial feedback. To overcome this, we exploit a unique structure of arms in selective generation for feedback unlocking, i.e., unlocking unknown feedback from observed feedback. We theoretically and empirically evaluate the efficacy of the proposed online selective generation algorithm under partial feedback over diverse data environment setups, resulting in controlling a desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio of non-abstaining answers, compared to baselines.

</details>


### [351] [Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification](https://arxiv.org/pdf/2506.14074)
*Nathaniel Pinckney, Chenhui Deng, Chia-Tung Ho, Yun-Da Tsai, Mingjie Liu, Wenfei Zhou, Brucek Khailany, Haoxing Ren*

Main category: cs.LG

TL;DR: The CVDP benchmark is a new dataset for LLM and agent research in hardware design, featuring 783 problems across 13 categories, highlighting gaps in current model capabilities.


<details>
  <summary>Details</summary>
Motivation: To advance research in hardware design and verification by providing a realistic and challenging benchmark for LLMs and agents.

Method: CVDP includes 783 problems in non-agentic and agentic formats, evaluated using open-source tools, BLEU, and LLM-based judging.

Result: State-of-the-art models achieve no more than 34% pass@1 on code generation, with agentic tasks being particularly difficult.

Conclusion: CVDP reveals significant gaps in model capabilities, emphasizing the need for further research in hardware design automation.

Abstract: We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new dataset and infrastructure to advance LLM and agent research in hardware design and verification. CVDP includes 783 problems across 13 task categories, covering RTL generation, verification, debugging, specification alignment, and technical Q&A authored by experienced hardware engineers. Problems are offered in both non-agentic and agentic formats. The benchmark introduces more realistic and challenging contexts than prior work, with state-of-the-art models achieving no more than 34% pass@1 on code generation. Agentic tasks$\unicode{x2013}$especially those involving RTL reuse and verification$\unicode{x2013}$are particularly difficult. Evaluation uses open-source tools and model scoring infrastructure, with comprehension tasks assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in current model capabilities, underscoring the need for continued research toward robust, real-world hardware design automation.

</details>


### [352] [Multi-Scale Finetuning for Encoder-based Time Series Foundation Models](https://arxiv.org/pdf/2506.14087)
*Zhongzheng Qiao, Chenghao Liu, Yiming Zhang, Ming Jin, Quang Pham, Qingsong Wen, P. N. Suganthan, Xudong Jiang, Savitha Ramasamy*

Main category: cs.LG

TL;DR: The paper proposes MSFT, a multi-scale finetuning framework for time series foundation models (TSFMs) to improve downstream task performance by addressing overfitting and leveraging multi-scale capabilities.


<details>
  <summary>Details</summary>
Motivation: Naive finetuning of TSFMs underutilizes their potential, leading to overfitting and suboptimal results. The diverse temporal patterns and multi-scale forecasting abilities of TSFMs necessitate a more effective finetuning approach.

Method: The authors introduce MSFT, a framework that explicitly models multiple scales during finetuning, focusing on encoder-based TSFMs.

Result: MSFT outperforms naive finetuning, parameter-efficient methods, and state-of-the-art deep learning approaches across three TSFM backbones (Moirai, Moment, and Units).

Conclusion: Explicit multi-scale modeling in finetuning (MSFT) significantly enhances TSFM performance, demonstrating its superiority over existing methods.

Abstract: Time series foundation models (TSFMs) demonstrate impressive zero-shot performance for time series forecasting. However, an important yet underexplored challenge is how to effectively finetune TSFMs on specific downstream tasks. While naive finetuning can yield performance gains, we argue that it falls short of fully leveraging TSFMs' capabilities, often resulting in overfitting and suboptimal performance. Given the diverse temporal patterns across sampling scales and the inherent multi-scale forecasting capabilities of TSFMs, we adopt a causal perspective to analyze finetuning process, through which we highlight the critical importance of explicitly modeling multiple scales and reveal the shortcomings of naive approaches. Focusing on \textit{encoder-based} TSFMs, we propose \textbf{M}ulti\textbf{\textsc{s}}cale \textbf{\textsc{f}}ine\textbf{\textsc{t}}uning (\textbf{MSFT}), a simple yet general framework that explicitly integrates multi-scale modeling into the finetuning process. Experimental results on three different backbones (\moirai, \moment\ and \units) demonstrate that TSFMs finetuned with MSFT not only outperform naive and typical parameter efficient finetuning methods but also surpass state-of-the-art deep learning methods.

</details>


### [353] [Transformers Learn Faster with Semantic Focus](https://arxiv.org/pdf/2506.14095)
*Parikshit Ram, Kenneth L. Clarkson, Tim Klinger, Shashanka Ubaru, Alexander G. Gray*

Main category: cs.LG

TL;DR: Sparse transformers with input-dependent attention converge faster and generalize better than standard or input-agnostic sparse attention, supported by theoretical and empirical evidence.


<details>
  <summary>Details</summary>
Motivation: To understand how sparse attention impacts learnability and generalization in transformers, beyond just computational efficiency.

Method: Empirical study of various attention mechanisms, theoretical analysis of softmax stability and Lipschitz properties, and validation of conditions for improved guarantees.

Result: Input-dependent sparse attention outperforms standard and input-agnostic sparse attention in convergence and generalization.

Conclusion: Semantic focus (input-dependent sparse attention) accelerates learning, while input-agnostic sparse attention offers no benefits, as proven theoretically and empirically.

Abstract: Various forms of sparse attention have been explored to mitigate the quadratic computational and memory cost of the attention mechanism in transformers. We study sparse transformers not through a lens of efficiency but rather in terms of learnability and generalization. Empirically studying a range of attention mechanisms, we find that input-dependent sparse attention models appear to converge faster and generalize better than standard attention models, while input-agnostic sparse attention models show no such benefits -- a phenomenon that is robust across architectural and optimization hyperparameter choices. This can be interpreted as demonstrating that concentrating a model's "semantic focus" with respect to the tokens currently being considered (in the form of input-dependent sparse attention) accelerates learning. We develop a theoretical characterization of the conditions that explain this behavior. We establish a connection between the stability of the standard softmax and the loss function's Lipschitz properties, then show how sparsity affects the stability of the softmax and the subsequent convergence and generalization guarantees resulting from the attention mechanism. This allows us to theoretically establish that input-agnostic sparse attention does not provide any benefits. We also characterize conditions when semantic focus (input-dependent sparse attention) can provide improved guarantees, and we validate that these conditions are in fact met in our empirical evaluations.

</details>


### [354] [Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks](https://arxiv.org/pdf/2506.14098)
*Ziyuan Tang, Jie Chen*

Main category: cs.LG

TL;DR: The paper proposes a graph foundation model using Transformer architecture, addressing challenges in encoding diverse graphs by representing nodes as random walks and introducing a context prediction loss.


<details>
  <summary>Details</summary>
Motivation: To explore if foundation models, like GPT for natural language, can be built for graphs, given their potential for diverse applications.

Method: Adapts Transformer architecture for graphs by representing nodes as random walks, enabling sequence-based encoding. Introduces a context prediction loss for training.

Result: The model successfully pre-trains and adapts to downstream tasks, demonstrating its potential as a graph foundation model.

Conclusion: The approach shows promise for building foundation models for graph-structured data, enabling broader applications in graph processing and reasoning.

Abstract: A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.

</details>


### [355] [SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting](https://arxiv.org/pdf/2506.14113)
*Yitian Zhang, Liheng Ma, Antonios Valkanas, Boris N. Oreshkin, Mark Coates*

Main category: cs.LG

TL;DR: The paper connects Koopman operator theory with linear RNNs, proposing SKOLR, a method combining learnable spectral decomposition and MLPs for efficient nonlinear dynamical system forecasting.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between infinite-dimensional Koopman operators and practical finite-dimensional approximations by leveraging linear RNNs.

Method: Introduces SKOLR, which uses a learnable spectral decomposition and MLPs for measurement functions, implementing a structured Koopman operator via parallel linear RNNs.

Result: Demonstrates exceptional performance on forecasting benchmarks and dynamical systems.

Conclusion: SKOLR's Koopman-theory-based design offers a streamlined and effective approach for nonlinear dynamical system analysis and forecasting.

Abstract: Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance.

</details>


### [356] [Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization](https://arxiv.org/pdf/2506.14114)
*Khushnood Abbas, Ruizhe Hou, Zhou Wengang, Dong Shi, Niu Ling, Satyaki Nan, Alireza Abbasi*

Main category: cs.LG

TL;DR: A large-scale study evaluates GNN architectures and loss functions, revealing hybrid losses and GIN architecture perform best, while MPNN lags.


<details>
  <summary>Details</summary>
Motivation: To understand how GNN architectures and loss functions interact across tasks, as prior work studied them separately.

Method: Evaluated 7 GNN architectures with 30 loss functions in inductive and transductive settings across 3 datasets using 21 metrics.

Result: Hybrid losses outperform single ones; GIN with Cross-Entropy excels; GAT with hybrid losses shows specialized strengths; MPNN underperforms.

Conclusion: Multi-objective optimization (hybrid losses) and architecture choice (GIN) are key for robust GNN performance, while MPNN is less effective.

Abstract: Graph Neural Networks (GNNs) became useful for learning on non-Euclidean data. However, their best performance depends on choosing the right model architecture and the training objective, also called the loss function. Researchers have studied these parts separately, but a large-scale evaluation has not looked at how GNN models and many loss functions work together across different tasks. To fix this, we ran a thorough study - it included seven well-known GNN architectures. We also used a large group of 30 single plus mixed loss functions. The study looked at both inductive and transductive settings. Our evaluation spanned three distinct real-world datasets, assessing performance in both inductive and transductive settings using 21 comprehensive evaluation metrics. From these extensive results (detailed in supplementary information 1 \& 2), we meticulously analyzed the top ten model-loss combinations for each metric based on their average rank. Our findings reveal that, especially for the inductive case: 1) Hybrid loss functions generally yield superior and more robust performance compared to single loss functions, indicating the benefit of multi-objective optimization. 2) The GIN architecture always showed the highest-level average performance, especially with Cross-Entropy loss. 3) Although some combinations had overall lower average ranks, models such as GAT, particularly with certain hybrid losses, demonstrated incredible specialized strengths, maximizing the most top-1 results among the individual metrics, emphasizing subtle strengths for particular task demands. 4) On the other hand, the MPNN architecture typically lagged behind the scenarios it was tested against.

</details>


### [357] [CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs](https://arxiv.org/pdf/2506.14122)
*Tianming Zhang, Renbo Zhang, Zhengyi Yang, Yunjun Gao, Bin Cao, Jing Fan*

Main category: cs.LG

TL;DR: A scalable and inductive contrastive learning-based GNN (CLGNN) is proposed for accurate and efficient Temporal Betweenness Centrality (TBC) prediction, addressing imbalance and temporal dependencies.


<details>
  <summary>Details</summary>
Motivation: Exact TBC computation is expensive, and real-world TBC distributions are imbalanced, causing learning models to overfit and fail in identifying central nodes. Existing GNN methods either ignore imbalance or temporal dependencies.

Method: CLGNN uses an instance graph to preserve path validity and temporal order, dual aggregation (mean and edge-to-node multi-head attention), and introduces KContrastNet for class imbalance mitigation and ValueNet for TBC estimation.

Result: CLGNN achieves up to 663.7× speedup over exact TBC methods, outperforms static GNNs with 31.4× lower MAE and 16.7× higher Spearman correlation, and surpasses temporal GNNs with 5.7× lower MAE and 3.9× higher correlation.

Conclusion: CLGNN effectively addresses TBC prediction challenges, offering scalability, accuracy, and efficiency, while accommodating diverse temporal semantics.

Abstract: Temporal Betweenness Centrality (TBC) measures how often a node appears on optimal temporal paths, reflecting its importance in temporal networks. However, exact computation is highly expensive, and real-world TBC distributions are extremely imbalanced. The severe imbalance leads learning-based models to overfit to zero-centrality nodes, resulting in inaccurate TBC predictions and failure to identify truly central nodes. Existing graph neural network (GNN) methods either fail to handle such imbalance or ignore temporal dependencies altogether. To address these issues, we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for accurate and efficient TBC prediction. CLGNN builds an instance graph to preserve path validity and temporal order, then encodes structural and temporal features using dual aggregation, i.e., mean and edge-to-node multi-head attention mechanisms, enhanced by temporal path count and time encodings. A stability-based clustering-guided contrastive module (KContrastNet) is introduced to separate high-, median-, and low-centrality nodes in representation space, mitigating class imbalance, while a regression module (ValueNet) estimates TBC values. CLGNN also supports multiple optimal path definitions to accommodate diverse temporal semantics. Extensive experiments demonstrate the effectiveness and efficiency of CLGNN across diverse benchmarks. CLGNN achieves up to a 663.7~$\times$ speedup compared to state-of-the-art exact TBC computation methods. It outperforms leading static GNN baselines with up to 31.4~$\times$ lower MAE and 16.7~$\times$ higher Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to 5.7~$\times$ lower MAE and 3.9~$\times$ higher Spearman correlation.

</details>


### [358] [Less is More: Undertraining Experts Improves Model Upcycling](https://arxiv.org/pdf/2506.14126)
*Stefan Horoi, Guy Wolf, Eugene Belilovsky, Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: The paper examines how expert fine-tuning impacts model upcycling, showing that prolonged fine-tuning harms merging performance and downstream results due to memorization of difficult examples. Early stopping improves upcycling.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that improvements in fine-tuning propagate downstream in model pipelines, focusing on the effects of expert fine-tuning on upcycling.

Method: Analyzes the impact of long fine-tuning on merging performance and downstream results, identifying memorization issues and testing early stopping strategies.

Result: Long fine-tuning degrades merging and downstream performance, while task-dependent early stopping enhances upcycling.

Conclusion: Aggressive early stopping during fine-tuning can mitigate degradation and improve model upcycling outcomes.

Abstract: Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.

</details>


### [359] [Leveraging Predictive Equivalence in Decision Trees](https://arxiv.org/pdf/2506.14143)
*Hayden McTavish, Zachery Boner, Jon Donnelly, Margo Seltzer, Cynthia Rudin*

Main category: cs.LG

TL;DR: The paper addresses predictive equivalence in decision trees, introduces a boolean logical representation to avoid it, and demonstrates its benefits in robustness, variable importance, and cost optimization.


<details>
  <summary>Details</summary>
Motivation: Decision trees suffer from predictive equivalence, where identical decision boundaries can be represented by different trees, complicating model selection and interpretability.

Method: The authors propose a boolean logical representation of decision trees to eliminate predictive equivalence and ensure faithfulness to the decision boundary.

Result: The representation improves robustness to missing feature values, clarifies variable importance, and enables cost-optimized predictions.

Conclusion: The boolean logical representation resolves predictive equivalence, enhancing decision trees' interpretability and utility in practical tasks.

Abstract: Decision trees are widely used for interpretable machine learning due to their clearly structured reasoning process. However, this structure belies a challenge we refer to as predictive equivalence: a given tree's decision boundary can be represented by many different decision trees. The presence of models with identical decision boundaries but different evaluation processes makes model selection challenging. The models will have different variable importance and behave differently in the presence of missing values, but most optimization procedures will arbitrarily choose one such model to return. We present a boolean logical representation of decision trees that does not exhibit predictive equivalence and is faithful to the underlying decision boundary. We apply our representation to several downstream machine learning tasks. Using our representation, we show that decision trees are surprisingly robust to test-time missingness of feature values; we address predictive equivalence's impact on quantifying variable importance; and we present an algorithm to optimize the cost of reaching predictions.

</details>


### [360] [Common Benchmarks Undervalue the Generalization Power of Programmatic Policies](https://arxiv.org/pdf/2506.14162)
*Amirhossein Rajabpour, Kiarash Aghakasiri, Sandra Zilles, Levi H. S. Lelis*

Main category: cs.LG

TL;DR: Neural policies can generalize as well as programmatic policies on OOD problems with simple training adjustments, challenging the common belief that programmatic policies inherently generalize better.


<details>
  <summary>Details</summary>
Motivation: To address the undervaluation of neural policies' generalization capabilities in OOD problems and critique current benchmarks.

Method: Analyzed experiments from four papers, modified neural policies with simpler architectures, sparse observations, and safer reward functions.

Result: Neural policies matched programmatic policies in OOD generalization with these adjustments.

Conclusion: Benchmarks should highlight concepts challenging neural policies but aligning with programmatic representations, like algorithmic tasks.

Abstract: Algorithms for learning programmatic representations for sequential decision-making problems are often evaluated on out-of-distribution (OOD) problems, with the common conclusion that programmatic policies generalize better than neural policies on OOD problems. In this position paper, we argue that commonly used benchmarks undervalue the generalization capabilities of programmatic representations. We analyze the experiments of four papers from the literature and show that neural policies, which were shown not to generalize, can generalize as effectively as programmatic policies on OOD problems. This is achieved with simple changes in the neural policies training pipeline. Namely, we show that simpler neural architectures with the same type of sparse observation used with programmatic policies can help attain OOD generalization. Another modification we have shown to be effective is the use of reward functions that allow for safer policies (e.g., agents that drive slowly can generalize better). Also, we argue for creating benchmark problems highlighting concepts needed for OOD generalization that may challenge neural policies but align with programmatic representations, such as tasks requiring algorithmic constructs like stacks.

</details>


### [361] [Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model](https://arxiv.org/pdf/2506.14167)
*Prithvi Raj*

Main category: cs.LG

TL;DR: The paper adapts the Kolmogorov-Arnold Representation Theorem for generative modeling, using inverse transform sampling to create an interpretable, efficient model with energy-based priors.


<details>
  <summary>Details</summary>
Motivation: To bridge classical representation theorems with modern probabilistic modeling, improving interpretability, efficiency, and sample quality.

Method: Uses a Kolmogorov-Arnold Network generator with energy-based priors, trained via Maximum Likelihood, and incorporates scalable extensions like mixture distributions and Langevin Monte Carlo.

Result: Achieves fast inference, better prior-posterior alignment, and recoverable, visualizable priors, balancing flexibility and training efficiency.

Conclusion: Connects classical and modern methods, offering a stable, efficient, and high-quality generative model.

Abstract: We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling by reinterpreting its inner functions as a Markov Kernel between probability spaces via inverse transform sampling. We present a generative model that is interpretable, easy to design, and efficient. Our approach couples a Kolmogorov-Arnold Network generator with independent energy-based priors, trained via Maximum Likelihood. Inverse sampling enables fast inference, while prior knowledge can be incorporated before training to better align priors with posteriors, thereby improving learning efficiency and sample quality. The learned prior is also recoverable and visualizable post-training, offering an empirical Bayes perspective. To address inflexibility and mitigate prior-posterior mismatch, we introduce scalable extensions based on mixture distributions and Langevin Monte Carlo methods, admitting a trade-off between flexibility and training efficiency. Our contributions connect classical representation theorems with modern probabilistic modeling, while balancing training stability, inference speed, and the quality and diversity of generations.

</details>


### [362] [A Variational Information Theoretic Approach to Out-of-Distribution Detection](https://arxiv.org/pdf/2506.14194)
*Sudeepta Mondal, Zhuolin Jiang, Ganesh Sundaramoorthi*

Main category: cs.LG

TL;DR: A theory for constructing OOD detection features in neural networks using an information-theoretic loss functional with KL divergence and Information Bottleneck, leading to improved performance and explainability.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for constructing OOD detection features with clear explainability and improved performance over existing methods.

Method: Introduces a novel loss functional combining KL divergence for separating ID and OOD distributions and Information Bottleneck for feature compression. Uses variational optimization to derive OOD features.

Result: The theory predicts a new shaping function that outperforms existing ones on OOD benchmarks.

Conclusion: Provides a general framework for creating explainable and effective OOD detection features, with potential for further applications.

Abstract: We present a theory for the construction of out-of-distribution (OOD) detection features for neural networks. We introduce random features for OOD through a novel information-theoretic loss functional consisting of two terms, the first based on the KL divergence separates resulting in-distribution (ID) and OOD feature distributions and the second term is the Information Bottleneck, which favors compressed features that retain the OOD information. We formulate a variational procedure to optimize the loss and obtain OOD features. Based on assumptions on OOD distributions, one can recover properties of existing OOD features, i.e., shaping functions. Furthermore, we show that our theory can predict a new shaping function that out-performs existing ones on OOD benchmarks. Our theory provides a general framework for constructing a variety of new features with clear explainability.

</details>


### [363] [DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion](https://arxiv.org/pdf/2506.14202)
*Makoto Shing, Takuya Akiba*

Main category: cs.LG

TL;DR: DiffusionBlocks is a memory-efficient training framework that partitions neural networks into blocks optimized for denoising in a diffusion process, reducing memory usage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Training large neural networks with backpropagation is memory-intensive, limiting accessibility to advanced AI research.

Method: Partition networks into blocks, interpret them as denoising steps in diffusion, and optimize noise levels for equal cumulative probability mass.

Result: Achieves memory reduction proportional to block count while maintaining competitive performance in generative tasks.

Conclusion: DiffusionBlocks democratizes large-scale neural network training by reducing memory bottlenecks.

Abstract: Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.

</details>


### [364] [TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift](https://arxiv.org/pdf/2506.14217)
*Dipesh Tharu Mahato, Rohan Poudel, Pramod Dhungana*

Main category: cs.LG

TL;DR: TriGuard is a safety evaluation framework combining formal robustness verification, attribution entropy, and a novel Attribution Drift Score to assess neural network reliability under adversarial and distributional shifts.


<details>
  <summary>Details</summary>
Motivation: Ensuring the reliability of deep neural networks under adversarial and distributional shifts is a critical challenge.

Method: TriGuard integrates formal robustness verification, attribution entropy, and the Attribution Drift Score to evaluate model safety.

Result: TriGuard identifies mismatches between model accuracy and interpretability, revealing subtle fragilities in neural reasoning. It also shows that entropy-regularized training reduces explanation drift without performance loss.

Conclusion: TriGuard advances robust and interpretable model evaluation, providing complementary safety insights beyond traditional adversarial accuracy.

Abstract: Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.

</details>


### [365] [Can Large Language Models Improve Spectral Graph Neural Networks?](https://arxiv.org/pdf/2506.14220)
*Kangkang Lu, Yanhua Yu, Zhiyong Huang, Tat-Seng Chua*

Main category: cs.LG

TL;DR: The paper proposes using Large Language Models (LLMs) to estimate graph homophily and guide the design of spectral filters in Spectral Graph Neural Networks (SGNNs), improving performance under label-scarce conditions.


<details>
  <summary>Details</summary>
Motivation: SGNNs struggle with suboptimal filters in label-scarce scenarios, while LLMs show promise for enhancing GNNs. The goal is to leverage LLMs to improve SGNN adaptability.

Method: A lightweight pipeline where LLMs estimate graph homophily, which is used to adaptively design polynomial spectral filters for SGNNs.

Result: The LLM-driven SGNN framework outperforms baselines in homophilic and heterophilic settings with minimal overhead.

Conclusion: LLMs can effectively enhance SGNNs by providing homophily-aware priors, improving performance across diverse graph structures.

Abstract: Spectral Graph Neural Networks (SGNNs) have attracted significant attention due to their ability to approximate arbitrary filters. They typically rely on supervision from downstream tasks to adaptively learn appropriate filters. However, under label-scarce conditions, SGNNs may learn suboptimal filters, leading to degraded performance. Meanwhile, the remarkable success of Large Language Models (LLMs) has inspired growing interest in exploring their potential within the GNN domain. This naturally raises an important question: \textit{Can LLMs help overcome the limitations of SGNNs and enhance their performance?} In this paper, we propose a novel approach that leverages LLMs to estimate the homophily of a given graph. The estimated homophily is then used to adaptively guide the design of polynomial spectral filters, thereby improving the expressiveness and adaptability of SGNNs across diverse graph structures. Specifically, we introduce a lightweight pipeline in which the LLM generates homophily-aware priors, which are injected into the filter coefficients to better align with the underlying graph topology. Extensive experiments on benchmark datasets demonstrate that our LLM-driven SGNN framework consistently outperforms existing baselines under both homophilic and heterophilic settings, with minimal computational and monetary overhead.

</details>


### [366] [Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning](https://arxiv.org/pdf/2506.14251)
*Xiyu Zhao, Qimei Cui, Weicai Li, Wei Ni, Ekram Hossain, Quan Z. Sheng, Xiaofeng Tao, Ping Zhang*

Main category: cs.LG

TL;DR: DP-Ditto extends Ditto for personalized federated learning with differential privacy, balancing privacy, convergence, and fairness, outperforming other PFL models.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in personalized federated learning (PFL) and analyze trade-offs between privacy, convergence, and fairness.

Method: Extends Ditto with differential privacy (DP), analyzes convergence bounds, optimal global aggregations, and fairness optimization.

Result: DP-Ditto outperforms state-of-the-art PFL models by 32.71% in fairness and 9.66% in accuracy.

Conclusion: DP-Ditto effectively balances privacy, convergence, and fairness, proving superior to existing PFL methods.

Abstract: Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a balance between personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). While FL is unaffected by personalized model training, in Ditto, PL depends on the outcome of the FL. However, the clients' concern about their privacy and consequent perturbation of their local models can affect the convergence and (performance) fairness of PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension of Ditto under the protection of differential privacy (DP), and analyzes the trade-off among its privacy guarantee, model convergence, and performance distribution fairness. We also analyze the convergence upper bound of the personalized models under DP-Ditto and derive the optimal number of global aggregations given a privacy budget. Further, we analyze the performance fairness of the personalized models, and reveal the feasibility of optimizing DP-Ditto jointly for convergence and fairness. Experiments validate our analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by over 32.71% in fairness and 9.66% in accuracy.

</details>


### [367] [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/pdf/2506.14261)
*Rohan Gupta, Erik Jenner*

Main category: cs.LG

TL;DR: LLMs can be trained to evade latent-space monitors, with token-level monitors being vulnerable, while holistic ones remain robust.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs can learn to bypass latent-space monitors designed to detect undesirable behaviors.

Method: Introduces RL-Obfuscation, finetuning LLMs via reinforcement learning to evade monitors while maintaining coherent outputs.

Result: Token-level monitors are easily bypassed; holistic monitors (e.g., max-pooling or attention-based) resist evasion. Adversarial policies generalize to unseen monitors.

Conclusion: Latent-space monitors must evolve to counter evasion, with holistic approaches being more resilient.

Abstract: Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can LLMs learn to evade such monitors? To study this, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by RL bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.

</details>


### [368] [Knowledge Adaptation as Posterior Correction](https://arxiv.org/pdf/2506.14262)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: The paper explores how machines can achieve quick adaptation like humans by correcting approximate posteriors, linking this to Bayesian learning principles.


<details>
  <summary>Details</summary>
Motivation: Current AI models lack human-like adaptivity, and the paper seeks to understand mechanisms for faster machine adaptation.

Method: The study uses a dual-perspective of the Bayesian Learning Rule to analyze adaptation as posterior corrections, with natural-gradient mismatch characterizing interference.

Result: More accurate posteriors lead to smaller corrections and quicker adaptation, demonstrated through various examples.

Conclusion: Posterior-correction is proposed as a natural mechanism for machines to learn rapid adaptation, bridging gaps in current AI capabilities.

Abstract: Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.

</details>


### [369] [Towards Robust Learning to Optimize with Theoretical Guarantees](https://arxiv.org/pdf/2506.14263)
*Qingyu Song, Wei Lin, Juncheng Wang, Hong Xu*

Main category: cs.LG

TL;DR: The paper addresses the lack of theoretical guarantees for Learning to Optimize (L2O) models in out-of-distribution (OOD) scenarios, providing proofs and a new model with improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing L2O works lack theoretical demonstration of performance and robustness in OOD scenarios.

Method: Proposes a robust L2O model with homogeneous convergence rates for InD, aligns OOD to InD problems, and introduces gradient-only features and gradient-based history modeling.

Result: Numerical simulations show the model outperforms baselines in InD and OOD, achieving up to 10x speedup.

Conclusion: The proposed L2O model provides theoretical robustness and practical improvements, with code available for reproducibility.

Abstract: Learning to optimize (L2O) is an emerging technique to solve mathematical optimization problems with learning-based methods. Although with great success in many real-world scenarios such as wireless communications, computer networks, and electronic design, existing L2O works lack theoretical demonstration of their performance and robustness in out-of-distribution (OOD) scenarios. We address this gap by providing comprehensive proofs. First, we prove a sufficient condition for a robust L2O model with homogeneous convergence rates over all In-Distribution (InD) instances. We assume an L2O model achieves robustness for an InD scenario. Based on our proposed methodology of aligning OOD problems to InD problems, we also demonstrate that the L2O model's convergence rate in OOD scenarios will deteriorate by an equation of the L2O model's input features. Moreover, we propose an L2O model with a concise gradient-only feature construction and a novel gradient-based history modeling method. Numerical simulation demonstrates that our proposed model outperforms the state-of-the-art baseline in both InD and OOD scenarios and achieves up to 10 $\times$ convergence speedup. The code of our method can be found from https://github.com/NetX-lab/GoMathL2O-Official.

</details>


### [370] [Improving LoRA with Variational Learning](https://arxiv.org/pdf/2506.14280)
*Bai Cong, Nico Daheim, Yuesong Shen, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff*

Main category: cs.LG

TL;DR: IVON, a variational algorithm, improves LoRA finetuning by enhancing metrics like accuracy and calibration while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Bayesian methods for LoRA finetuning often have marginal benefits, increased overhead, and require tricks. IVON addresses these issues.

Method: Uses IVON, a variational algorithm, with posterior pruning for efficient and effective LoRA finetuning.

Result: Improves accuracy by 1.3% and reduces ECE by 5.4% on Llama-3.2-3B, outperforming AdamW and other Bayesian methods.

Conclusion: IVON is a practical and effective alternative for improving LoRA finetuning.

Abstract: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.

</details>


### [371] [Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models](https://arxiv.org/pdf/2506.14291)
*Ben Finkelshtein, İsmail İlkan Ceylan, Michael Bronstein, Ron Levie*

Main category: cs.LG

TL;DR: The paper proposes a method to design graph foundation models for node-level tasks by investigating necessary symmetries, ensuring generalization across arbitrary graphs and features.


<details>
  <summary>Details</summary>
Motivation: Current graph machine learning architectures are task- and dataset-specific, limiting broader applicability. The goal is to create foundation models that generalize across diverse graphs and features.

Method: The study systematically examines symmetries (node permutation-equivariance, label permutation-equivariance, and feature permutation-invariance) and characterizes linear transformations respecting these. The resulting network is proven to be a universal approximator for multisets with these symmetries.

Result: The proposed model shows strong zero-shot performance and consistent improvement with more training graphs, validated on 29 real-world node classification datasets.

Conclusion: The recipe for graph foundation models, based on symmetry principles, successfully generalizes across graphs and features, offering a scalable and versatile solution for node-level tasks.

Abstract: Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: how to build graph foundation models capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing graph foundation models for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases.

</details>


### [372] [Fair for a few: Improving Fairness in Doubly Imbalanced Datasets](https://arxiv.org/pdf/2506.14306)
*Ata Yalcin, Asli Umay Ozturk, Yigit Sever, Viktoria Pauw, Stephan Hachinger, Ismail Hakki Toroslu, Pinar Karagoz*

Main category: cs.LG

TL;DR: The paper addresses fairness in doubly imbalanced datasets, where imbalance exists in both labels and sensitive attributes. It highlights limitations in existing debiasing methods and proposes a multi-criteria solution for balanced sampling.


<details>
  <summary>Details</summary>
Motivation: Fairness in ML/AI is crucial, but current debiasing methods fail for doubly imbalanced datasets (imbalance in labels and sensitive attributes).

Method: An exploratory analysis identifies debiasing limitations, followed by a multi-criteria approach to optimize sampling and distribution for fairness and accuracy.

Result: The proposed solution aims to improve fairness and classification accuracy in doubly imbalanced datasets.

Conclusion: The paper underscores the need for tailored debiasing methods for doubly imbalanced data and offers a practical multi-criteria solution.

Abstract: Fairness has been identified as an important aspect of Machine Learning and Artificial Intelligence solutions for decision making. Recent literature offers a variety of approaches for debiasing, however many of them fall short when the data collection is imbalanced. In this paper, we focus on a particular case, fairness in doubly imbalanced datasets, such that the data collection is imbalanced both for the label and the groups in the sensitive attribute. Firstly, we present an exploratory analysis to illustrate limitations in debiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution is proposed for finding the most suitable sampling and distribution for label and sensitive attribute, in terms of fairness and classification accuracy

</details>


### [373] [IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards](https://arxiv.org/pdf/2506.14375)
*Muhammad Hamza Yousuf, Jason Li, Sahar Vahdati, Raphael Theilen, Jakob Wittenstein, Jens Lehmann*

Main category: cs.LG

TL;DR: The paper proposes optimizations for AI-assisted mechanical ventilation (MV) control using offline reinforcement learning (RL) to handle hybrid action spaces and introduces a clinically grounded reward function for better patient outcomes.


<details>
  <summary>Details</summary>
Motivation: Optimizing MV settings is complex due to patient variability, and current RL methods struggle with hybrid action spaces, limiting safety and effectiveness.

Method: The paper adapts SOTA offline RL algorithms (IQL and EDAC) for hybrid action spaces and introduces a reward function based on ventilator-free days and physiological targets.

Result: The proposed approach improves MV optimization, enhancing patient safety and enabling individualized lung support.

Conclusion: AI-assisted MV optimization represents a significant advancement toward intelligent, data-driven critical care solutions.

Abstract: Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions.

</details>


### [374] [ResNets Are Deeper Than You Think](https://arxiv.org/pdf/2506.14386)
*Christian H. X. Ali Mehmeti-Göpel, Michael Wand*

Main category: cs.LG

TL;DR: Residual connections in neural networks offer performance advantages beyond optimization, suggesting a deeper inductive bias aligned with natural data.


<details>
  <summary>Details</summary>
Motivation: To explain why residual networks outperform feedforward networks despite efforts to close the gap, focusing on function space differences rather than just optimization.

Method: A controlled post-training comparison isolating generalization performance from trainability, comparing variable-depth (ResNet-like) and fixed-depth architectures.

Result: Variable-depth architectures consistently outperform fixed-depth networks, even when optimization is unlikely to influence outcomes.

Conclusion: Residual connections provide inherent advantages, likely due to a better inductive bias for natural data structures, beyond just improved trainability.

Abstract: Residual connections remain ubiquitous in modern neural network architectures nearly a decade after their introduction. Their widespread adoption is often credited to their dramatically improved trainability: residual networks train faster, more stably, and achieve higher accuracy than their feedforward counterparts. While numerous techniques, ranging from improved initialization to advanced learning rate schedules, have been proposed to close the performance gap between residual and feedforward networks, this gap has persisted. In this work, we propose an alternative explanation: residual networks do not merely reparameterize feedforward networks, but instead inhabit a different function space. We design a controlled post-training comparison to isolate generalization performance from trainability; we find that variable-depth architectures, similar to ResNets, consistently outperform fixed-depth networks, even when optimization is unlikely to make a difference. These results suggest that residual connections confer performance advantages beyond optimization, pointing instead to a deeper inductive bias aligned with the structure of natural data.

</details>


### [375] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/pdf/2506.14390)
*Conrad Orglmeister, Erik Bochinski, Volker Eiselein, Elvira Fleig*

Main category: cs.LG

TL;DR: The paper extends self-explainable Prototypical Variational models with autoencoder-based OOD detection, introducing a novel restriction loss for a compact ID region and demonstrating superior performance on benchmarks and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance trust and reliability in Deep Machine Learning Models for safety-relevant applications by improving explainability and OOD detection.

Method: Uses a Variational Autoencoder to learn a latent space for distance-based classification, likelihood estimation, and reconstruction. Defines an ID region with a Gaussian mixture and introduces a restriction loss for compactness.

Result: Outperforms previous methods on OOD detection benchmarks and a real-world railway dataset.

Conclusion: The approach effectively combines explainability and OOD detection, proving useful for safety-critical applications.

Abstract: Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.

</details>


### [376] [HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control](https://arxiv.org/pdf/2506.14391)
*Yaqiao Zhu, Hongkai Wen, Geyong Min, Man Luo*

Main category: cs.LG

TL;DR: HiLight is a hierarchical RL framework for large-scale traffic signal control, combining global adversarial guidance with local execution to improve coordination and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for traffic signal control struggle with scalability and global coordination, limiting network-level efficiency.

Method: HiLight uses a high-level Meta-Policy (Transformer-LSTM) to partition networks and set sub-goals, and a low-level Sub-Policy for intersection control, enhanced by adversarial training.

Result: HiLight outperforms in large-scale scenarios and remains competitive in standard benchmarks, handling diverse traffic conditions effectively.

Conclusion: HiLight addresses scalability and coordination challenges in traffic signal control, demonstrating superior performance in large and varied networks.

Abstract: Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.

</details>


### [377] [One Size Fits None: Rethinking Fairness in Medical AI](https://arxiv.org/pdf/2506.14400)
*Roland Roller, Michael Hahn, Ajay Madhavan Ravichandran, Bilgin Osmanodja, Florian Oetke, Zeineb Sassi, Aljoscha Burchardt, Klaus Netter, Klemens Budde, Anne Herrmann, Tobias Strapatsas, Peter Dabrock, Sebastian Möller*

Main category: cs.LG

TL;DR: The paper highlights the need for subgroup-level evaluation of ML models in healthcare to address fairness and performance disparities across patient subgroups.


<details>
  <summary>Details</summary>
Motivation: Real-world medical datasets are noisy and imbalanced, leading to unfair performance disparities, especially for marginalized groups.

Method: Analyzed medical prediction tasks to evaluate model performance variations across patient subgroups.

Result: Found significant performance disparities across subgroups, emphasizing the need for subgroup-level evaluation.

Conclusion: Subgroup-sensitive development and deployment of ML models are crucial for fairness and transparency in healthcare.

Abstract: Machine learning (ML) models are increasingly used to support clinical decision-making. However, real-world medical datasets are often noisy, incomplete, and imbalanced, leading to performance disparities across patient subgroups. These differences raise fairness concerns, particularly when they reinforce existing disadvantages for marginalized groups. In this work, we analyze several medical prediction tasks and demonstrate how model performance varies with patient characteristics. While ML models may demonstrate good overall performance, we argue that subgroup-level evaluation is essential before integrating them into clinical workflows. By conducting a performance analysis at the subgroup level, differences can be clearly identified-allowing, on the one hand, for performance disparities to be considered in clinical practice, and on the other hand, for these insights to inform the responsible development of more effective models. Thereby, our work contributes to a practical discussion around the subgroup-sensitive development and deployment of medical ML models and the interconnectedness of fairness and transparency.

</details>


### [378] [Adaptive Reinforcement Learning for Unobservable Random Delays](https://arxiv.org/pdf/2506.14411)
*John Wikman, Alexandre Proutiere, David Broman*

Main category: cs.LG

TL;DR: The paper introduces the 'interaction layer' framework to handle unobservable, time-varying delays in RL, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-world RL often involves delays in agent-environment interactions, which existing methods handle conservatively with fixed upper bounds.

Method: Proposes the interaction layer framework and a model-based algorithm, ACDA, to dynamically adapt to delays.

Result: ACDA significantly outperforms state-of-the-art approaches in locomotion benchmarks.

Conclusion: The interaction layer and ACDA provide an effective solution for RL in delayed, dynamic environments.

Abstract: In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.

</details>


### [379] [Unsupervised Skill Discovery through Skill Regions Differentiation](https://arxiv.org/pdf/2506.14420)
*Ting Xiao, Jiakun Zheng, Rushuai Yang, Kang Xu, Qiaosheng Zhang, Peng Liu, Chenjia Bai*

Main category: cs.LG

TL;DR: A novel unsupervised RL method maximizes inter-skill state diversity and uses a conditional autoencoder for state-density estimation, outperforming previous approaches in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of entropy-based exploration and MI-driven skill learning in large-scale state spaces by promoting inter-skill diversity and intra-skill exploration.

Method: Proposes a skill discovery objective for state diversity, a conditional autoencoder for state-density estimation, and an intrinsic reward for intra-skill exploration.

Result: Learns meaningful skills and achieves superior performance in downstream tasks, validated in challenging state and image-based environments.

Conclusion: The method effectively combines inter-skill diversity and intra-skill exploration, advancing unsupervised RL for complex tasks.

Abstract: Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors that can accelerate the learning of downstream tasks. Previous methods typically focus on entropy-based exploration or empowerment-driven skill learning. However, entropy-based exploration struggles in large-scale state spaces (e.g., images), and empowerment-based methods with Mutual Information (MI) estimations have limitations in state exploration. To address these challenges, we propose a novel skill discovery objective that maximizes the deviation of the state density of one skill from the explored regions of other skills, encouraging inter-skill state diversity similar to the initial MI objective. For state-density estimation, we construct a novel conditional autoencoder with soft modularization for different skill policies in high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we formulate an intrinsic reward based on the learned autoencoder that resembles count-based exploration in a compact latent space. Through extensive experiments in challenging state and image-based tasks, we find our method learns meaningful skills and achieves superior performance in various downstream tasks.

</details>


### [380] [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/pdf/2506.14574)
*Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia*

Main category: cs.LG

TL;DR: The paper introduces a method to integrate token-level reward guidance into Direct Preference Optimization (DPO) by decomposing sequence-level PPO into token-level problems, improving performance over DPO.


<details>
  <summary>Details</summary>
Motivation: Leveraging fine-grained token-level rewards for DPO is challenging due to its sequence-level formulation. This work aims to bridge this gap.

Method: Decomposes sequence-level PPO into token-level problems, derives optimal token-level policy and reward, and integrates them into DPO via a computable loss function.

Result: Achieves significant performance gains over DPO, with win rate improvements of up to 7.5 points on MT-Bench, 6.2 on AlpacaEval 2, and 4.3 on Arena-Hard.

Conclusion: The proposed framework successfully incorporates token-level rewards into DPO, enhancing alignment performance for large language models.

Abstract: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.

</details>


### [381] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/pdf/2506.14436)
*Shen Yuan, Yin Zheng, Taifeng Wang, Binbin Liu, Hongteng Xu*

Main category: cs.LG

TL;DR: The paper proposes MoORE, a method for multi-task adaptation of foundation models using SVD and learnable routers to avoid task conflict and oblivion.


<details>
  <summary>Details</summary>
Motivation: Address task conflict and oblivion in multi-task adaptation of large-scale foundation models.

Method: Applies SVD to weight matrices, introduces learnable routers, and forms a Mixture of Orthogonal Rank-one Experts (MoORE). Ensures orthogonality and maintains original column space.

Result: MoORE outperforms existing methods in conflict- and oblivion-resistance across datasets.

Conclusion: MoORE is superior for multi-task adaptation, balancing task performance and original knowledge retention.

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers from task conflict and oblivion. To mitigate such issues, we propose a novel ''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant multi-task adaptation method. Given a weight matrix of a pre-trained model, our method applies SVD to it and introduces a learnable router to adjust its singular values based on tasks and samples. Accordingly, the weight matrix becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert corresponds to the outer product of a left singular vector and the corresponding right one. We can improve the model capacity by imposing a learnable orthogonal transform on the right singular vectors. Unlike low-rank adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts' orthogonality and maintains the column space of the original weight matrix. These two properties make the adapted model resistant to the conflicts among the new tasks and the oblivion of its original tasks, respectively. Experiments on various datasets demonstrate that MoORE outperforms existing multi-task adaptation methods consistently, showing its superiority in terms of conflict- and oblivion-resistance. The code of the experiments is available at https://github.com/DaShenZi721/MoORE.

</details>


### [382] [sHGCN: Simplified hyperbolic graph convolutional neural networks](https://arxiv.org/pdf/2506.14438)
*Pol Arévalo, Alexis Molina, Álvaro Ciudad*

Main category: cs.LG

TL;DR: Simplifying hyperbolic neural network operations improves computational efficiency and performance, making them more viable for diverse applications.


<details>
  <summary>Details</summary>
Motivation: Hyperbolic neural networks show promise for modeling hierarchical data but face challenges in computational efficiency and precision.

Method: Simplified key operations within hyperbolic neural networks.

Result: Achieved notable improvements in runtime and predictive accuracy.

Conclusion: Streamlined hyperbolic operations enhance the practicality of hyperbolic neural networks for broader use.

Abstract: Hyperbolic geometry has emerged as a powerful tool for modeling complex, structured data, particularly where hierarchical or tree-like relationships are present. By enabling embeddings with lower distortion, hyperbolic neural networks offer promising alternatives to Euclidean-based models for capturing intricate data structures. Despite these advantages, they often face performance challenges, particularly in computational efficiency and tasks requiring high precision. In this work, we address these limitations by simplifying key operations within hyperbolic neural networks, achieving notable improvements in both runtime and performance. Our findings demonstrate that streamlined hyperbolic operations can lead to substantial gains in computational speed and predictive accuracy, making hyperbolic neural networks a more viable choice for a broader range of applications.

</details>


### [383] [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://arxiv.org/pdf/2506.14439)
*Rikiya Takehi, Masahiro Asami, Kosuke Kawakami, Yuta Saito*

Main category: cs.LG

TL;DR: HyPeR improves off-policy learning in contextual bandits by leveraging secondary rewards alongside partially-observed target rewards, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: OPL suffers when rewards are partially observed (e.g., delayed or censored). Secondary rewards (e.g., clicks, dwell time) are more frequent but may misalign with target rewards.

Method: Proposes HyPeR, a hybrid policy optimization method that uses both target and secondary rewards for effective OPL. Also explores dual-objective optimization.

Result: HyPeR outperforms existing methods in synthetic and real-world data, showing better policy learning despite partial rewards.

Conclusion: HyPeR effectively leverages secondary rewards to enhance OPL, even optimizing target rewards better by considering secondary objectives.

Abstract: Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL degrades severely. Well-known examples of such partial rewards include explicit ratings in content recommendations, conversion signals on e-commerce platforms that are partial due to delay, and the issue of censoring in medical problems. One possible solution to deal with such partial rewards is to use secondary rewards, such as dwelling time, clicks, and medical indicators, which are more densely observed. However, relying solely on such secondary rewards can also lead to poor policy learning since they may not align with the target reward. Thus, this work studies a new and general problem of OPL where the goal is to learn a policy that maximizes the expected target reward by leveraging densely observed secondary rewards as supplemental data. We then propose a new method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which effectively uses the secondary rewards in addition to the partially-observed target reward to achieve effective OPL despite the challenging scenario. We also discuss a case where we aim to optimize not only the expected target reward but also the expected secondary rewards to some extent; counter-intuitively, we will show that leveraging the two objectives is in fact advantageous also for the optimization of only the target reward. Along with statistical analysis of our proposed methods, empirical evaluations on both synthetic and real-world data show that HyPeR outperforms existing methods in various scenarios.

</details>


### [384] [Detecting immune cells with label-free two-photon autofluorescence and deep learning](https://arxiv.org/pdf/2506.14449)
*Lucas Kreiss, Amey Chaware, Maryam Roohian, Sarah Lemire, Oana-Maria Thoma, Birgitta Carlé, Maximilian Waldner, Sebastian Schürmann, Oliver Friedrich, Roarke Horstmeyer*

Main category: cs.LG

TL;DR: A deep learning model (CNN) was trained to classify immune cell types using label-free multiphoton microscopy (MPM) images, achieving reliable results and demonstrating potential for in vivo endomicroscopy.


<details>
  <summary>Details</summary>
Motivation: Label-free imaging avoids staining, making it ideal for in vivo use, but lacks specificity. Deep learning can computationally enhance this specificity, especially for MPM.

Method: A convolutional neural network (CNN) was trained on label-free MPM images of immune cells (5,075 for binary and 3,424 for multi-class tasks) using autofluorescence (AF) signals.

Result: The model achieved strong performance (e.g., 0.89 ROC-AUC for binary classification) and was robust to environmental noise, with both AF channels (NADH and FAD) contributing equally.

Conclusion: DL models can improve the specificity of label-free MPM for immune cell detection, offering significant potential for in vivo applications.

Abstract: Label-free imaging has gained broad interest because of its potential to omit elaborate staining procedures which is especially relevant for in vivo use. Label-free multiphoton microscopy (MPM), for instance, exploits two-photon excitation of natural autofluorescence (AF) from native, metabolic proteins, making it ideal for in vivo endomicroscopy. Deep learning (DL) models have been widely used in other optical imaging technologies to predict specific target annotations and thereby digitally augment the specificity of these label-free images. However, this computational specificity has only rarely been implemented for MPM. In this work, we used a data set of label-free MPM images from a series of different immune cell types (5,075 individual cells for binary classification in mixed samples and 3,424 cells for a multi-class classification task) and trained a convolutional neural network (CNN) to classify cell types based on this label-free AF as input. A low-complexity squeezeNet architecture was able to achieve reliable immune cell classification results (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples; 0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class classification in isolated samples). Perturbation tests confirmed that the model is not confused by extracellular environment and that both input AF channels (NADH and FAD) are about equally important to the classification. In the future, such predictive DL models could directly detect specific immune cells in unstained images and thus, computationally improve the specificity of label-free MPM which would have great potential for in vivo endomicroscopy.

</details>


### [385] [Dataset distillation for memorized data: Soft labels can leak held-out teacher knowledge](https://arxiv.org/pdf/2506.14457)
*Freya Behrens, Lenka Zdeborová*

Main category: cs.LG

TL;DR: Students trained on soft labels from teachers can achieve accuracy on memorized data they never saw, even in settings where generalization is impossible.


<details>
  <summary>Details</summary>
Motivation: To understand how memorized information is transferred in dataset distillation, especially when teachers memorize data without generalizing.

Method: Analyze students trained on soft labels from teachers using finite random i.i.d. datasets where generalization is impossible.

Result: Students can learn non-trivial information about held-out memorized data, sometimes achieving perfect accuracy.

Conclusion: The phenomena depend on temperature in logit smoothing but persist across network capacities, architectures, and dataset compositions.

Abstract: Dataset distillation aims to compress training data into fewer examples via a teacher, from which a student can learn effectively. While its success is often attributed to structure in the data, modern neural networks also memorize specific facts, but if and how such memorized information is can transferred in distillation settings remains less understood. In this work, we show that students trained on soft labels from teachers can achieve non-trivial accuracy on held-out memorized data they never directly observed. This effect persists on structured data when the teacher has not generalized.To analyze it in isolation, we consider finite random i.i.d. datasets where generalization is a priori impossible and a successful teacher fit implies pure memorization. Still, students can learn non-trivial information about the held-out data, in some cases up to perfect accuracy. In those settings, enough soft labels are available to recover the teacher functionally - the student matches the teacher's predictions on all possible inputs, including the held-out memorized data. We show that these phenomena strongly depend on the temperature with which the logits are smoothed, but persist across varying network capacities, architectures and dataset compositions.

</details>


### [386] [A Model-Mediated Stacked Ensemble Approach for Depression Prediction Among Professionals](https://arxiv.org/pdf/2506.14459)
*Md. Mortuza Ahmmed, Abdullah Al Noman, Mahin Montasir Afif, K. M. Tahsin Kabir, Md. Mostafizur Rahman, Mufti Mahmud*

Main category: cs.LG

TL;DR: A stacking-based ensemble learning model improves depression classification accuracy among professionals, achieving over 98% performance metrics.


<details>
  <summary>Details</summary>
Motivation: Address challenges in predicting depression due to its complexity and multifaceted influencing factors like occupational stress and lifestyle.

Method: Proposes a stacking-based ensemble learning approach using multiple base learners and logistic regression, tested on the Depression Professional Dataset from Kaggle.

Result: Achieves 99.64% accuracy on training and 98.75% on testing data, with precision, recall, and F1-score all above 98%.

Conclusion: Ensemble learning is effective for mental health analytics, aiding early detection and intervention.

Abstract: Depression is a significant mental health concern, particularly in professional environments where work-related stress, financial pressure, and lifestyle imbalances contribute to deteriorating well-being. Despite increasing awareness, researchers and practitioners face critical challenges in developing accurate and generalizable predictive models for mental health disorders. Traditional classification approaches often struggle with the complexity of depression, as it is influenced by multifaceted, interdependent factors, including occupational stress, sleep patterns, and job satisfaction. This study addresses these challenges by proposing a stacking-based ensemble learning approach to improve the predictive accuracy of depression classification among professionals. The Depression Professional Dataset has been collected from Kaggle. The dataset comprises demographic, occupational, and lifestyle attributes that influence mental well-being. Our stacking model integrates multiple base learners with a logistic regression-mediated model, effectively capturing diverse learning patterns. The experimental results demonstrate that the proposed model achieves high predictive performance, with an accuracy of 99.64% on training data and 98.75% on testing data, with precision, recall, and F1-score all exceeding 98%. These findings highlight the effectiveness of ensemble learning in mental health analytics and underscore its potential for early detection and intervention strategies.

</details>


### [387] [Zeroth-Order Optimization is Secretly Single-Step Policy Optimization](https://arxiv.org/pdf/2506.14460)
*Junbin Qiu, Zhengpeng Xie, Xiangda Yan, Yongjie Yang, Yao Shu*

Main category: cs.LG

TL;DR: The paper reveals a connection between Zeroth-Order Optimization (ZOO) and Policy Optimization (PO), showing equivalence in objectives and gradient estimators. It introduces ZoAR, a novel ZOO algorithm with PO-inspired variance reduction, and validates its superiority empirically.


<details>
  <summary>Details</summary>
Motivation: To elucidate the connection between ZOO and PO, and leverage PO techniques to improve ZOO methods.

Method: The paper formalizes the equivalence between ZOO and single-step PO, introduces ZoAR with averaged baseline and query reuse, and provides theoretical and empirical validation.

Result: ZoAR outperforms other ZOO methods in convergence speed and final performance, supported by theory and experiments.

Conclusion: The work offers a new theoretical understanding of ZOO and practical improvements through its connection to PO.

Abstract: Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing functions where explicit gradients are unavailable or expensive to compute. However, the underlying mechanisms of popular ZOO methods, particularly those employing randomized finite differences, and their connection to other optimization paradigms like Reinforcement Learning (RL) are not fully elucidated. This paper establishes a fundamental and previously unrecognized connection: ZOO with finite differences is equivalent to a specific instance of single-step Policy Optimization (PO). We formally unveil that the implicitly smoothed objective function optimized by common ZOO algorithms is identical to a single-step PO objective. Furthermore, we show that widely used ZOO gradient estimators, are mathematically equivalent to the REINFORCE gradient estimator with a specific baseline function, revealing the variance-reducing mechanism in ZOO from a PO perspective.Built on this unified framework, we propose ZoAR (Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO algorithm incorporating PO-inspired variance reduction techniques: an averaged baseline from recent evaluations and query reuse analogous to experience replay. Our theoretical analysis further substantiates these techniques reduce variance and enhance convergence. Extensive empirical studies validate our theory and demonstrate that ZoAR significantly outperforms other methods in terms of convergence speed and final performance. Overall, our work provides a new theoretical lens for understanding ZOO and offers practical algorithmic improvements derived from its connection to PO.

</details>


### [388] [Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks](https://arxiv.org/pdf/2506.14472)
*Fabien Bernier, Maxime Cordy, Yves Le Traon*

Main category: cs.LG

TL;DR: A hypernetwork architecture improves global electrical consumption forecasting by leveraging external factors like weather, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate forecasting is vital for energy management, but adding external factors often degrades global model performance.

Method: Used a hypernetwork to adjust model weights per consumer, tested on data from 6000+ Luxembourgish households with external factors.

Result: Hypernetwork outperformed other methods, reducing errors and maintaining global model benefits.

Conclusion: Hypernetworks effectively enhance global forecasting accuracy by incorporating external factors.

Abstract: Accurate electrical consumption forecasting is crucial for efficient energy management and resource allocation. While traditional time series forecasting relies on historical patterns and temporal dependencies, incorporating external factors -- such as weather indicators -- has shown significant potential for improving prediction accuracy in complex real-world applications. However, the inclusion of these additional features often degrades the performance of global predictive models trained on entire populations, despite improving individual household-level models. To address this challenge, we found that a hypernetwork architecture can effectively leverage external factors to enhance the accuracy of global electrical consumption forecasting models, by specifically adjusting the model weights to each consumer.
  We collected a comprehensive dataset spanning two years, comprising consumption data from over 6000 luxembourgish households and corresponding external factors such as weather indicators, holidays, and major local events. By comparing various forecasting models, we demonstrate that a hypernetwork approach outperforms existing methods when associated to external factors, reducing forecasting errors and achieving the best accuracy while maintaining the benefits of a global model.

</details>


### [389] [Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning](https://arxiv.org/pdf/2506.14515)
*Prabhav Sanga, Jaskaran Singh, Arun K. Dubey*

Main category: cs.LG

TL;DR: FAMR is a framework for efficient post-hoc unlearning in deep image classifiers, minimizing influence of specific data without full retraining.


<details>
  <summary>Details</summary>
Motivation: Address the need for selective unlearning in machine learning systems due to privacy regulations, without costly retraining.

Method: FAMR frames unlearning as constrained optimization, minimizing uniform-prediction loss on forget set while anchoring parameters to original values via ℓ2 penalty.

Result: Empirical results on CIFAR-10 and ImageNet-100 show FAMR's effectiveness in class forgetting with strong performance retention and low overhead.

Conclusion: FAMR offers a scalable, certifiable solution for post-hoc unlearning in vision models, applicable to class, concept, and style erasure.

Abstract: As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.

</details>


### [390] [Two-Player Zero-Sum Games with Bandit Feedback](https://arxiv.org/pdf/2506.14518)
*Elif Yılmaz, Christos Dimitrakakis*

Main category: cs.LG

TL;DR: The paper introduces two ETC-based algorithms for two-player zero-sum games with bandit feedback, achieving instance-dependent regret bounds.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the applicability of Explore-Then-Commit (ETC) in adversarial game settings and improve regret bounds by leveraging the ε-Nash Equilibrium property.

Method: Proposes ETC-TPZSG and its improved variant ETC-TPZSG-AE, which uses action pair elimination for efficient learning of pure strategy Nash Equilibrium.

Result: Achieves instance-dependent regret bounds of O(Δ + √T) for ETC-TPZSG and O(log(TΔ²)/Δ) for ETC-TPZSG-AE.

Conclusion: ETC-based algorithms perform effectively in adversarial settings, offering competitive regret bounds and instance-dependent insights.

Abstract: We study a two-player zero-sum game (TPZSG) in which the row player aims to maximize their payoff against an adversarial column player, under an unknown payoff matrix estimated through bandit feedback. We propose and analyze two algorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and ETC-TPZSG-AE, which improves upon it by incorporating an action pair elimination (AE) strategy that leverages the $\varepsilon$-Nash Equilibrium property to efficiently select the optimal action pair. Our objective is to demonstrate the applicability of ETC in a TPZSG setting by focusing on learning pure strategy Nash Equilibrium. A key contribution of our work is a derivation of instance-dependent upper bounds on the expected regret for both algorithms, has received limited attention in the literature on zero-sum games. Particularly, after $T$ rounds, we achieve an instance-dependent regret upper bounds of $O(Δ+ \sqrt{T})$ for ETC-TPZSG and $O(\frac{\log (T Δ^2)}Δ)$ for ETC-TPZSG-AE, where $Δ$ denotes the suboptimality gap. Therefore, our results indicate that ETC-based algorithms perform effectively in adversarial game settings, achieving regret bounds comparable to existing methods while providing insights through instance-dependent analysis.

</details>


### [391] [Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction](https://arxiv.org/pdf/2506.14521)
*Korbinian Pfab, Marcel Rothering*

Main category: cs.LG

TL;DR: The paper critiques current AI research methodologies using a case study on false call reduction in automated optical inspection, highlighting seven weaknesses and advocating for improved practices like requirement-aware metrics and clear success criteria.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether current AI research methodologies are sufficient for creating successful, profitable AI applications, using a real-world industrial case study.

Method: A case study on false call reduction in automated optical inspection, identifying and experimentally demonstrating seven weaknesses in current best practices.

Result: The study shows that current best-practice methodologies fail for the use case and highlights the need for better metrics, success criteria, and dataset analysis.

Conclusion: Researchers should critically assess their methodologies to improve applied AI research, emphasizing requirement-aware metrics and clear definitions of success.

Abstract: Are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications? This work presents a case study on an industrial AI use case called false call reduction for automated optical inspection to demonstrate the shortcomings of current best practices. We identify seven weaknesses prevalent in related peer-reviewed work and experimentally show their consequences. We show that the best-practice methodology would fail for this use case. We argue amongst others for the necessity of requirement-aware metrics to ensure achieving business objectives, clear definitions of success criteria, and a thorough analysis of temporal dynamics in experimental datasets. Our work encourages researchers to critically assess their methodologies for more successful applied AI research.

</details>


### [392] [Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution](https://arxiv.org/pdf/2506.14529)
*Xiaohan Zheng, Lanning Wei, Yong Li, Quanming Yao*

Main category: cs.LG

TL;DR: LLMNet automates GNN configuration using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for knowledge-guided evolution.


<details>
  <summary>Details</summary>
Motivation: GNNs require significant effort to configure and tune, prompting the need for automated solutions.

Method: LLMNet employs agents to build graph-related knowledge bases and uses RAG for automated GNN configuration and refinement.

Result: LLMNet outperforms in twelve datasets across three graph learning tasks.

Conclusion: LLMNet effectively automates GNN design, demonstrating its practical utility.

Abstract: Effective decision-making on networks often relies on learning from graph-structured data, where Graph Neural Networks (GNNs) play a central role, but they take efforts to configure and tune. In this demo, we propose LLMNet, showing how to design GNN automated through Large Language Models. Our system develops a set of agents that construct graph-related knowlege bases and then leverages Retrieval-Augmented Generation (RAG) to support automated configuration and refinement of GNN models through a knowledge-guided evolution process. These agents, equipped with specialized knowledge bases, extract insights into tasks and graph structures by interacting with the knowledge bases. Empirical results show LLMNet excels in twelve datasets across three graph learning tasks, validating its effectiveness of GNN model designing.

</details>


### [393] [Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs](https://arxiv.org/pdf/2506.14540)
*Gerardo A. Flores, Alyssa H. Smith, Julia A. Fukuyama, Ashia C. Wilson*

Main category: cs.LG

TL;DR: A framework for evaluating clinical decision support systems prioritizes calibration, robustness, and cost sensitivity, using an adjusted cross-entropy metric.


<details>
  <summary>Details</summary>
Motivation: Current scoring rules like accuracy and AUC-ROC don't address clinical priorities such as calibration, robustness, and asymmetric error costs.

Method: Proposes an adjusted cross-entropy (log score) framework based on proper scoring rules, accounting for class prevalence uncertainty and cost asymmetries.

Result: The framework is simple, sensitive to clinical conditions, and prioritizes calibrated, robust models.

Conclusion: The method improves model selection for clinical settings by aligning evaluation with real-world priorities.

Abstract: Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.

</details>


### [394] [Single-Example Learning in a Mixture of GPDMs with Latent Geometries](https://arxiv.org/pdf/2506.14563)
*Jesse St. Amand, Leonardo Gizzi, Martin A. Giese*

Main category: cs.LG

TL;DR: The paper introduces the Gaussian process dynamical mixture model (GPDMM) for single-example learning of human motion data, combining GPDMs in a mixture framework for diverse sequence encoding.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modeling human movement with limited data and high interpretability, especially in medical applications like prosthesis control.

Method: GPDMM combines multiple Gaussian process dynamical models (GPDMs) in a probabilistic mixture-of-experts framework, using geometric features for diverse sequence encoding.

Result: GPDMM is evaluated on classification accuracy and generative ability, outperforming LSTMs, VAEs, and transformers in single-example learning.

Conclusion: GPDMM is effective for human motion modeling in data-limited scenarios, offering interpretability and competitive performance against deep learning alternatives.

Abstract: We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.

</details>


### [395] [Towards Desiderata-Driven Design of Visual Counterfactual Explainers](https://arxiv.org/pdf/2506.14698)
*Sidney Bender, Jan Herrmann, Klaus-Robert Müller, Grégoire Montavon*

Main category: cs.LG

TL;DR: The paper introduces a 'smooth counterfactual explorer' (SCE) algorithm to improve visual counterfactual explainers (VCEs) by addressing fidelity, understandability, and sufficiency, beyond just sample quality or minimal changes.


<details>
  <summary>Details</summary>
Motivation: Existing VCEs focus narrowly on sample quality or minimal changes, neglecting broader explanation needs like fidelity, understandability, and sufficiency.

Method: The authors explore new counterfactual generation mechanisms and integrate them into the SCE algorithm.

Result: The SCE algorithm is evaluated on synthetic and real data, demonstrating its effectiveness.

Conclusion: The SCE algorithm addresses limitations of current VCEs, offering a more holistic approach to counterfactual explanations.

Abstract: Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.

</details>


### [396] [Object-Centric Neuro-Argumentative Learning](https://arxiv.org/pdf/2506.14577)
*Abdul Rahman Jacob, Avinash Kori, Emanuele De Angelis, Ben Glocker, Maurizio Proietti, Francesca Toni*

Main category: cs.LG

TL;DR: A novel Neural Argumentative Learning (NAL) architecture combines deep learning with Assumption-Based Argumentation (ABA) for safer, more reliable, and interpretable image analysis.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about the safety, reliability, and interpretability of deep learning technologies in critical decision-making.

Method: Integrates neural (object-centric learning for image segmentation and encoding) and symbolic (ABA learning for framework development) components.

Result: Competitive performance with state-of-the-art alternatives on synthetic data.

Conclusion: NAL offers a promising approach to enhance the interpretability and reliability of deep learning in image analysis.

Abstract: Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.

</details>


### [397] [SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification](https://arxiv.org/pdf/2506.14587)
*Shuo Yang, Bardh Prenkaj, Gjergji Kasneci*

Main category: cs.LG

TL;DR: SCISSOR is a Siamese network-based debiasing method that addresses semantic shortcut learning, improving model robustness without data augmentation.


<details>
  <summary>Details</summary>
Motivation: Shortcut learning harms generalization due to semantic imbalances in embeddings, creating spurious correlations.

Method: SCISSOR remaps the semantic space by discouraging latent clusters used as shortcuts, using a Siamese network.

Result: SCISSOR improves F1 scores by +5.3 to +7.7 across benchmarks and boosts lightweight models by ~9.5-11.9%.

Conclusion: SCISSOR redefines model generalization by tackling semantic biases, offering a robust framework for bias-resistant AI.

Abstract: Shortcut learning undermines model generalization to out-of-distribution data. While the literature attributes shortcuts to biases in superficial features, we show that imbalances in the semantic distribution of sample embeddings induce spurious semantic correlations, compromising model robustness. To address this issue, we propose SCISSOR (Semantic Cluster Intervention for Suppressing ShORtcut), a Siamese network-based debiasing approach that remaps the semantic space by discouraging latent clusters exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on 6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports +5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay, and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for BERT on NLP. Our study redefines the landscape of model generalization by addressing overlooked semantic biases, establishing SCISSOR as a foundational framework for mitigating shortcut learning and fostering more robust, bias-resistant AI systems.

</details>


### [398] [Deep Learning Surrogates for Real-Time Gas Emission Inversion](https://arxiv.org/pdf/2506.14597)
*Thomas Newman, Christopher Nemeth, Matthew Jones, Philip Jonathan*

Main category: cs.LG

TL;DR: A deep-learning surrogate model accelerates real-time greenhouse-gas emission tracking by combining CFD with Bayesian inference, achieving accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of real-time greenhouse-gas emission monitoring under dynamic atmospheric conditions.

Method: A spatio-temporal inversion framework using a deep-learning surrogate (multilayer perceptron) trained on CFD data, integrated with sequential Monte Carlo for Bayesian inference.

Result: Validated on methane release data, the method matches CFD and Gaussian plume accuracy but is much faster, even in complex scenarios.

Conclusion: The framework balances physical accuracy and computational efficiency, enabling scalable real-time emissions monitoring.

Abstract: Real-time identification and quantification of greenhouse-gas emissions under transient atmospheric conditions is a critical challenge in environmental monitoring. We introduce a spatio-temporal inversion framework that embeds a deep-learning surrogate of computational fluid dynamics (CFD) within a sequential Monte Carlo algorithm to perform Bayesian inference of both emission rate and source location in dynamic flow fields. By substituting costly numerical solvers with a multilayer perceptron trained on high-fidelity CFD outputs, our surrogate captures spatial heterogeneity and temporal evolution of gas dispersion, while delivering near-real-time predictions. Validation on the Chilbolton methane release dataset demonstrates comparable accuracy to full CFD solvers and Gaussian plume models, yet achieves orders-of-magnitude faster runtimes. Further experiments under simulated obstructed-flow scenarios confirm robustness in complex environments. This work reconciles physical fidelity with computational feasibility, offering a scalable solution for industrial emissions monitoring and other time-sensitive spatio-temporal inversion tasks in environmental and scientific modeling.

</details>


### [399] [Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization](https://arxiv.org/pdf/2506.14607)
*Ziyu Gong, Jim Lim, David I. Inouye*

Main category: cs.LG

TL;DR: A novel likelihood-based distribution matching method using score-based priors improves stability, avoids biases, and outperforms existing techniques.


<details>
  <summary>Details</summary>
Motivation: Address limitations of non-parametric and adversarial DM methods, such as scalability, instability, and mode collapse, while avoiding biases from fixed priors or explicit density models.

Method: Uses expressive score-based prior distributions, leveraging gradient-based training and denoising score matching to avoid fixed priors and explicit density models.

Result: Demonstrates better stability, computational efficiency, and superior performance across tasks compared to other methods like LSGM.

Conclusion: The score-based approach is a stable and effective alternative for distribution matching, with practical advantages over existing techniques.

Abstract: Distribution matching (DM) is a versatile domain-invariant representation learning technique that has been applied to tasks such as fair classification, domain adaptation, and domain translation. Non-parametric DM methods struggle with scalability and adversarial DM approaches suffer from instability and mode collapse. While likelihood-based methods are a promising alternative, they often impose unnecessary biases through fixed priors or require explicit density models (e.g., flows) that can be challenging to train. We address this limitation by introducing a novel approach to training likelihood-based DM using expressive score-based prior distributions. Our key insight is that gradient-based DM training only requires the prior's score function -- not its density -- allowing us to train the prior via denoising score matching. This approach eliminates biases from fixed priors (e.g., in VAEs), enabling more effective use of geometry-preserving regularization, while avoiding the challenge of learning an explicit prior density model (e.g., a flow-based prior). Our method also demonstrates better stability and computational efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore, experiments demonstrate superior performance across multiple tasks, establishing our score-based method as a stable and effective approach to distribution matching. Source code available at https://github.com/inouye-lab/SAUB.

</details>


### [400] [Feasibility-Driven Trust Region Bayesian Optimization](https://arxiv.org/pdf/2506.14619)
*Paolo Ascia, Elena Raponi, Thomas Bäck, Fabian Duddeck*

Main category: cs.LG

TL;DR: FuRBO is a feasibility-driven Bayesian optimization method that accelerates finding feasible solutions in high-dimensional, constrained problems by adaptively adjusting trust regions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with expensive, high-dimensional constraints, wasting budget on locating feasible solutions. FuRBO aims to address this inefficiency.

Method: FuRBO uses adaptive trust regions informed by objective and constraint surrogates, allowing rapid refocusing of the search.

Result: FuRBO outperforms state-of-the-art methods in finding feasible, high-quality solutions across diverse benchmarks, including high-dimensional problems.

Conclusion: FuRBO effectively addresses the challenge of constrained optimization by dynamically adapting trust regions, improving efficiency and solution quality.

Abstract: Bayesian optimization is a powerful tool for solving real-world optimization tasks under tight evaluation budgets, making it well-suited for applications involving costly simulations or experiments. However, many of these tasks are also characterized by the presence of expensive constraints whose analytical formulation is unknown and often defined in high-dimensional spaces where feasible regions are small, irregular, and difficult to identify. In such cases, a substantial portion of the optimization budget may be spent just trying to locate the first feasible solution, limiting the effectiveness of existing methods. In this work, we present a Feasibility-Driven Trust Region Bayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust region from which the next candidate solution is selected, using information from both the objective and constraint surrogate models. Our adaptive strategy allows the trust region to shift and resize significantly between iterations, enabling the optimizer to rapidly refocus its search and consistently accelerate the discovery of feasible and good-quality solutions. We empirically demonstrate the effectiveness of FuRBO through extensive testing on the full BBOB-constrained COCO benchmark suite and other physics-inspired benchmarks, comparing it against state-of-the-art baselines for constrained black-box optimization across varying levels of constraint severity and problem dimensionalities ranging from 2 to 60.

</details>


### [401] [On the Hardness of Bandit Learning](https://arxiv.org/pdf/2506.14746)
*Nataly Brukhim, Aldo Pacchiano, Miroslav Dudik, Robert Schapire*

Main category: cs.LG

TL;DR: The paper explores bandit learnability, showing no combinatorial dimension can characterize it and proving computational hardness for certain classes, even with simple reward functions.


<details>
  <summary>Details</summary>
Motivation: To establish a general theory of bandit learnability, akin to PAC classification, by addressing which function classes are learnable and how.

Method: Investigates learnability via combinatorial dimensions and computational hardness proofs, using examples like binary PAC classification and ERM.

Result: No combinatorial dimension characterizes bandit learnability; computational hardness exists even for simple reward classes.

Conclusion: Bandit learnability has inherent limitations and computational challenges, differing from classical learning frameworks.

Abstract: We study the task of bandit learning, also known as best-arm identification, under the assumption that the true reward function f belongs to a known, but arbitrary, function class F. We seek a general theory of bandit learnability, akin to the PAC framework for classification. Our investigation is guided by the following two questions: (1) which classes F are learnable, and (2) how they are learnable. For example, in the case of binary PAC classification, learnability is fully determined by a combinatorial dimension - the VC dimension- and can be attained via a simple algorithmic principle, namely, empirical risk minimization (ERM). In contrast to classical learning-theoretic results, our findings reveal limitations of learning in structured bandits, offering insights into the boundaries of bandit learnability. First, for the question of "which", we show that the paradigm of identifying the learnable classes via a dimension-like quantity fails for bandit learning. We give a simple proof demonstrating that no combinatorial dimension can characterize bandit learnability, even in finite classes, following a standard definition of dimension introduced by Ben-David et al. (2019). For the question of "how", we prove a computational hardness result: we construct a reward function class for which at most two queries are needed to find the optimal action, yet no algorithm can do so in polynomial time unless RP=NP. We also prove that this class admits efficient algorithms for standard algorithmic operations often considered in learning theory, such as an ERM. This implies that computational hardness is in this case inherent to the task of bandit learning. Beyond these results, we investigate additional themes such as learning under noise, trade-offs between noise models, and the relationship between query complexity and regret minimization.

</details>


### [402] [Efficient Global Optimization of Two-Layer ReLU Networks: Quadratic-Time Algorithms and Adversarial Training](https://arxiv.org/pdf/2201.01965)
*Yatong Bai, Tanmay Gautam, Somayeh Sojoudi*

Main category: cs.LG

TL;DR: The paper addresses the non-convexity issue in ANN training by reformulating it as convex programs, proposing two efficient algorithms with global convergence guarantees, and extending the approach to adversarial robustness.


<details>
  <summary>Details</summary>
Motivation: The non-convexity of ANN training landscapes leads to optimization challenges like spurious local minima and sensitivity to hyperparameters. Reformulating training as convex programs offers a path to global optimization.

Method: Two algorithms are developed: one based on ADMM for exact and approximate convex formulations with linear convergence, and another using sampled convex programs for unconstrained formulations. The approach is also extended to adversarial training.

Result: The ADMM-based algorithm achieves linear global convergence, while the sampled convex programs method converges to an approximately optimal classifier. Both methods are efficient, with quadratic or better per-iteration complexity.

Conclusion: The proposed convex reformulation and algorithms provide globally convergent solutions for ANN training, even under adversarial conditions, with potential extensions to more complex architectures.

Abstract: The non-convexity of the artificial neural network (ANN) training landscape brings inherent optimization difficulties. While the traditional back-propagation stochastic gradient descent (SGD) algorithm and its variants are effective in certain cases, they can become stuck at spurious local minima and are sensitive to initializations and hyperparameters. Recent work has shown that the training of an ANN with ReLU activations can be reformulated as a convex program, bringing hope to globally optimizing interpretable ANNs. However, naively solving the convex training formulation has an exponential complexity, and even an approximation heuristic requires cubic time. In this work, we characterize the quality of this approximation and develop two efficient algorithms that train ANNs with global convergence guarantees. The first algorithm is based on the alternating direction method of multiplier (ADMM). It solves both the exact convex formulation and the approximate counterpart. Linear global convergence is achieved, and the initial several iterations often yield a solution with high prediction accuracy. When solving the approximate formulation, the per-iteration time complexity is quadratic. The second algorithm, based on the "sampled convex programs" theory, solves unconstrained convex formulations and converges to an approximately globally optimal classifier. The non-convexity of the ANN training landscape exacerbates when adversarial training is considered. We apply the robust convex optimization theory to convex training and develop convex formulations that train ANNs robust to adversarial inputs. Our analysis explicitly focuses on one-hidden-layer fully connected ANNs, but can extend to more sophisticated architectures.

</details>


### [403] [Does DQN Learn?](https://arxiv.org/pdf/2205.13617)
*Aditya Gopalan, Gugan Thoppe*

Main category: cs.LG

TL;DR: DQN can produce worse policies than the initial guess, even under ideal conditions. The paper demonstrates this numerically and explains it theoretically for linear DQN.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding why DQN fails to improve policies, despite conditions where tabular Q-learning would succeed.

Method: Numerical experiments and theoretical analysis of linear DQN using differential inclusion theory.

Result: DQN often yields sub-optimal policies; linear DQN's fixed points may correspond to worst policies.

Conclusion: The work clarifies why DQN with function approximation and ε-greedy exploration can fail, providing insights for future improvements.

Abstract: A primary requirement for any reinforcement learning method is that it should produce policies that improve upon the initial guess. In this work, we show that the widely used Deep Q-Network (DQN) fails to satisfy this minimal criterion -- even when it gets to see all possible states and actions infinitely often (a condition under which tabular Q-learning is guaranteed to converge to the optimal Q-value function). Our specific contributions are twofold. First, we numerically show that DQN often returns a policy that performs worse than the initial one. Second, we offer a theoretical explanation for this phenomenon in linear DQN, a simplified version of DQN that uses linear function approximation in place of neural networks while retaining the other key components such as $ε$-greedy exploration, experience replay, and target network. Using tools from differential inclusion theory, we prove that the limit points of linear DQN correspond to fixed points of projected Bellman operators. Crucially, we show that these fixed points need not relate to optimal -- or even near-optimal -- policies, thus explaining linear DQN's sub-optimal behaviors. We also give a scenario where linear DQN always identifies the worst policy. Our work fills a longstanding gap in understanding the convergence behaviors of Q-learning with function approximation and $ε$-greedy exploration.

</details>


### [404] [Analyzing Effects of Mixed Sample Data Augmentation on Model Interpretability](https://arxiv.org/pdf/2303.14608)
*Soyoun Won, Sung-Ho Bae, Seong Tae Kim*

Main category: cs.LG

TL;DR: Mixed sample data augmentation reduces model interpretability, especially affecting feature attribution maps, with label mixing being a key factor.


<details>
  <summary>Details</summary>
Motivation: To study the impact of mixed sample data augmentation on model interpretability, particularly feature attribution maps, which is under-researched.

Method: Introduce a new metric to compare model interpretability while minimizing occlusion robustness effects.

Result: Several mixed sample data augmentation methods decrease interpretability, with label mixing being a significant contributor.

Conclusion: Careful adoption of mixed sample data augmentation is advised, especially in applications relying on attribution map-based interpretability.

Abstract: Mixed sample data augmentation strategies are actively used when training deep neural networks (DNNs). Recent studies suggest that they are effective at various tasks. However, the impact of mixed sample data augmentation on model interpretability has not been widely studied. In this paper, we explore the relationship between model interpretability and mixed sample data augmentation, specifically in terms of feature attribution maps. To this end, we introduce a new metric that allows a comparison of model interpretability while minimizing the impact of occlusion robustness of the model. Experimental results show that several mixed sample data augmentation decreases the interpretability of the model and label mixing during data augmentation plays a significant role in this effect. This new finding suggests it is important to carefully adopt the mixed sample data augmentation method, particularly in applications where attribution map-based interpretability is important.

</details>


### [405] [Efficient Online Decision Tree Learning with Active Feature Acquisition](https://arxiv.org/pdf/2305.02093)
*Arman Rahbar, Ziyu Ye, Yuxin Chen, Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: The paper introduces an online learning framework for decision trees where features and labels are costly to obtain, using active planning and adaptive submodularity to minimize query costs while maintaining low regret.


<details>
  <summary>Details</summary>
Motivation: In real-world applications like medical diagnosis, feature values and labels are costly to acquire, requiring a method to balance cost and prediction accuracy.

Method: The framework combines an active planning oracle with online learning, using adaptive submodularity for feature queries and posterior sampling for low-regret predictions.

Result: Extensive experiments show the framework's efficiency and effectiveness, even in settings with concept drift.

Conclusion: The proposed framework is flexible, cost-effective, and competitive with baseline models, particularly in dynamic environments.

Abstract: Constructing decision trees online is a classical machine learning problem. Existing works often assume that features are readily available for each incoming data point. However, in many real world applications, both feature values and the labels are unknown a priori and can only be obtained at a cost. For example, in medical diagnosis, doctors have to choose which tests to perform (i.e., making costly feature queries) on a patient in order to make a diagnosis decision (i.e., predicting labels). We provide a fresh perspective to tackle this practical challenge. Our framework consists of an active planning oracle embedded in an online learning scheme for which we investigate several information acquisition functions. Specifically, we employ a surrogate information acquisition function based on adaptive submodularity to actively query feature values with a minimal cost, while using a posterior sampling scheme to maintain a low regret for online prediction. We demonstrate the efficiency and effectiveness of our framework via extensive experiments on various real-world datasets. Our framework also naturally adapts to the challenging setting of online learning with concept drift and is shown to be competitive with baseline models while being more flexible.

</details>


### [406] [SensLI: Sensitivity-Based Layer Insertion for Neural Networks](https://arxiv.org/pdf/2311.15995)
*Leonie Kreis, Evelyn Herberg, Frederik Köhne, Anton Schiela, Roland Herzog*

Main category: cs.LG

TL;DR: Proposes SensLI, a method to dynamically insert layers during neural network training using sensitivity analysis, improving performance and reducing computational effort.


<details>
  <summary>Details</summary>
Motivation: Eliminates the need for manual tuning of fixed network architectures before training, making the process more efficient and adaptable.

Method: Uses first-order sensitivity information of the loss function to guide layer insertion, applicable to various architectures like feedforward networks, ResNets, and CNNs.

Result: SensLI shows better training loss and test error compared to fixed architectures, with lower computational costs than training extended architectures from scratch.

Conclusion: SensLI offers a systematic, efficient approach to dynamic layer insertion, enhancing neural network training without predefined architecture constraints.

Abstract: The training of neural networks requires tedious and often manual tuning of the network architecture. We propose a systematic approach to inserting new layers during the training process. Our method eliminates the need to choose a fixed network size before training, is numerically inexpensive to execute and applicable to various architectures including fully connected feedforward networks, ResNets and CNNs. Our technique borrows ideas from constrained optimization and is based on first-order sensitivity information of the loss function with respect to the virtual parameters that additional layers, if inserted, would offer. In numerical experiments, our proposed sensitivity-based layer insertion technique (SensLI) exhibits improved performance on training loss and test error, compared to training on a fixed architecture, and reduced computational effort in comparison to training the extended architecture from the beginning. Our code is available on https://github.com/mathemml/SensLI.

</details>


### [407] [Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess](https://arxiv.org/pdf/2401.16852)
*Felix Helfenstein, Johannes Czech, Jannis Blüml, Max Eisel, Kristian Kersting*

Main category: cs.LG

TL;DR: M2CTS is a modular chess engine using Mixture of Experts and Monte Carlo Tree Search to adapt strategies by game phase, outperforming single-model baselines by +122 Elo.


<details>
  <summary>Details</summary>
Motivation: Modern chess engines use a single neural network uniformly, missing phase-specific specialization opportunities.

Method: M2CTS combines Mixture of Experts with Monte Carlo Tree Search, training networks via Separated, Staged, or Weighted Learning for each game phase.

Result: M2CTS achieves +122 Elo over single-model baselines and generalizes well to multi-agent domains like Pommerman.

Conclusion: Modular, phase-aware systems align better with game structure, advancing toward human-like problem division.

Abstract: In games like chess, strategy evolves dramatically across distinct phases - the opening, middlegame, and endgame each demand different forms of reasoning and decision-making. Yet, many modern chess engines rely on a single neural network to play the entire game uniformly, often missing opportunities to specialize. In this work, we introduce M2CTS, a modular framework that combines Mixture of Experts with Monte Carlo Tree Search to adapt strategy dynamically based on game phase. We explore three different methods for training the neural networks: Separated Learning, Staged Learning, and Weighted Learning. By routing decisions through specialized neural networks trained for each phase, M2CTS improves both computational efficiency and playing strength. In experiments on chess, M2CTS achieves up to +122 Elo over standard single-model baselines and shows promising generalization to multi-agent domains such as Pommerman. These results highlight how modular, phase-aware systems can better align with the structured nature of games and move us closer to human-like behavior in dividing a problem into many smaller units.

</details>


### [408] [Sketch-Plan-Generalize: Learning and Planning with Neuro-Symbolic Programmatic Representations for Inductive Spatial Concepts](https://arxiv.org/pdf/2404.07774)
*Namasivayam Kalithasan, Sachit Sachdeva, Himanshu Gaurav Singh, Vishal Bindal, Arnav Tuli, Gurarmaan Singh Panjeta, Harsh Himanshu Vora, Divyanshu Aggarwal, Rohan Paul, Parag Singla*

Main category: cs.LG

TL;DR: A neuro-symbolic approach combines LLMs and neural representations for better generalization in learning complex concepts from limited demonstrations.


<details>
  <summary>Details</summary>
Motivation: Existing methods (LLM-only or neural) generalize poorly to unseen complex concepts, while neuro-symbolic methods struggle with large program spaces.

Method: The approach involves three steps: Sketch (coarse concept detection), Plan (MCTS search guided by demonstrations), and Generalize (abstracting plans as programs).

Result: The method outperforms LLM-only and neural approaches in inductive generalization for complex structure construction and enables embodied instruction following.

Conclusion: The neuro-symbolic pipeline enhances generalization, modularity, and continual learning, combining strengths of LLMs and grounded neural representations.

Abstract: Effective human-robot collaboration requires the ability to learn personalized concepts from a limited number of demonstrations, while exhibiting inductive generalization, hierarchical composition, and adaptability to novel constraints. Existing approaches that use code generation capabilities of pre-trained large (vision) language models as well as purely neural models show poor generalization to \emph{a-priori} unseen complex concepts. Neuro-symbolic methods (Grand et al., 2023) offer a promising alternative by searching in program space, but face challenges in large program spaces due to the inability to effectively guide the search using demonstrations. Our key insight is to factor inductive concept learning as: (i) {\it Sketch:} detecting and inferring a coarse signature of a new concept (ii) {\it Plan:} performing an MCTS search over grounded action sequences guided by human demonstrations (iii) {\it Generalize:} abstracting out grounded plans as inductive programs. Our pipeline facilitates generalization and modular re-use, enabling continual concept learning. Our approach combines the benefits of code generation ability of large language models (LLMs) along with grounded neural representations, resulting in neuro-symbolic programs that show stronger inductive generalization on the task of constructing complex structures vis-á-vis LLM-only and purely neural approaches. Further, we demonstrate reasoning and planning capabilities with learned concepts for embodied instruction following.

</details>


### [409] [Heavy-Tailed Diffusion with Denoising Lévy Probabilistic Models](https://arxiv.org/pdf/2407.18609)
*Dario Shariatian, Umut Simsekli, Alain Durmus*

Main category: cs.LG

TL;DR: The paper introduces DLPM, a diffusion model using α-stable noise instead of Gaussian, offering better performance and simplicity compared to LIM.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of Gaussian noise in diffusion models, especially for datasets with heavy tails or outliers, and simplifying the complex LIM framework.

Method: Extends DDPM by replacing Gaussian noise with α-stable noise, using elementary proof techniques for simplicity.

Result: DLPM outperforms LIM in tail coverage, robustness to unbalanced data, and computational efficiency with fewer backward steps.

Conclusion: DLPM provides a simpler, more effective alternative to LIM for handling heavy-tailed data in diffusion models.

Abstract: Exploring noise distributions beyond Gaussian in diffusion models remains an open challenge. While Gaussian-based models succeed within a unified SDE framework, recent studies suggest that heavy-tailed noise distributions, like $α$-stable distributions, may better handle mode collapse and effectively manage datasets exhibiting class imbalance, heavy tails, or prominent outliers. Recently, Yoon et al.\ (NeurIPS 2023), presented the Lévy-Itô model (LIM), directly extending the SDE-based framework to a class of heavy-tailed SDEs, where the injected noise followed an $α$-stable distribution, a rich class of heavy-tailed distributions. However, the LIM framework relies on highly involved mathematical techniques with limited flexibility, potentially hindering broader adoption and further development. In this study, instead of starting from the SDE formulation, we extend the denoising diffusion probabilistic model (DDPM) by replacing the Gaussian noise with $α$-stable noise. By using only elementary proof techniques, the proposed approach, Denoising Lévy Probabilistic Models (DLPM), boils down to vanilla DDPM with minor modifications. As opposed to the Gaussian case, DLPM and LIM yield different training algorithms and different backward processes, leading to distinct sampling algorithms. These fundamental differences translate favorably for DLPM as compared to LIM: our experiments show improvements in coverage of data distribution tails, better robustness to unbalanced datasets, and improved computation times requiring smaller number of backward steps.

</details>


### [410] [ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities](https://arxiv.org/pdf/2412.06745)
*Adhiraj Ghosh, Sebastian Dziadzio, Ameya Prabhu, Vishaal Udandarao, Samuel Albanie, Matthias Bethge*

Main category: cs.LG

TL;DR: ONEBench is a unified, open-ended benchmarking framework for evaluating foundation models by aggregating diverse samples into a customizable pool, addressing challenges of heterogeneity and incompleteness with a robust aggregation algorithm.


<details>
  <summary>Details</summary>
Motivation: Traditional fixed test sets are inadequate for evaluating the open-ended capabilities of foundation models, necessitating a flexible and scalable benchmarking approach.

Method: ONEBench consolidates evaluation datasets into a sample pool, allowing custom benchmarks. It uses an aggregation algorithm to handle heterogeneity and incompleteness, ensuring reliable model scores.

Result: The aggregation algorithm achieves identifiability and rapid convergence, works with ~95% missing data, and reduces evaluation costs by up to 20x without compromising rankings.

Conclusion: ONEBench provides a scalable, open-ended evaluation framework for foundation models, unifying assessments across domains like language and vision-language models.

Abstract: Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this pool, corresponding to specific capabilities of interest. By aggregating samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as a collective process of selecting and aggregating sample-level tests.
  The shift from task-specific benchmarks to ONEBench introduces two challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the aggregation over diverse metrics, while incompleteness describes comparing models evaluated on different data subsets. To address these challenges, we explore algorithms to aggregate sparse measurements into reliable model scores. Our aggregation algorithm ensures identifiability(asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model ranking with less data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. We also demonstrate robustness to ~95% of measurements missing, reducing evaluation cost by up to 20x with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains. Overall, we present a technique for open-ended evaluation, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow a benchmark alongside the rapidly developing foundation models.

</details>


### [411] [Generalizing Deep Surrogate Solvers for Broadband Electromagnetic Field Prediction at Unseen Wavelengths](https://arxiv.org/pdf/2408.02971)
*Joonhyuk Seo, Chanik Kang, Dongjin Seo, Haejun Chung*

Main category: cs.LG

TL;DR: A new framework improves electromagnetic surrogate solvers by ensuring spectral consistency and integrating wave-informed embeddings, achieving better accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Conventional electromagnetic surrogate solvers are limited by narrow spectral ranges and fail with slight condition variations.

Method: Proposes spectral consistency, a refined wave prior, and Wave-Informed Multiplicative Encoding (WIME) to integrate embeddings while preserving spectral consistency.

Result: Reduces error by 71% at untrained wavelengths and achieves a 42x speedup over numerical simulations.

Conclusion: The framework enables accurate broadband field prediction, outperforming state-of-the-art solvers.

Abstract: Recently, electromagnetic surrogate solvers, trained on solutions of Maxwell's equations under specific simulation conditions, enabled fast inference of computationally expensive simulations. However, conventional electromagnetic surrogate solvers often consider only a narrow range of spectrum and fail when encountering even slight variations in simulation conditions. To address this limitation, we define spectral consistency as the property by which the spatial frequency structure of wavelength-dependent condition embeddings matches that of the target electromagnetic field patterns. In addition, we propose two complementary components: a refined wave prior, which is the condition embedding that satisfies spectral consistency, and Wave-Informed element-wise Multiplicative Encoding (WIME), which integrates these embeddings throughout the model while preserving spectral consistency. This framework enables accurate field prediction across the broadband spectrum, including untrained intermediate wavelengths. Our approach reduces the normalized mean squared error at untrained wavelengths by up to 71% compared to the state-of-the-art electromagnetic surrogate solver and achieves a speedup of over 42 times relative to conventional numerical simulations.

</details>


### [412] [Diverse Topology Optimization using Modulated Neural Fields](https://arxiv.org/pdf/2502.13174)
*Andreas Radler, Eric Volkmann, Johannes Brandstetter, Arturs Berzins*

Main category: cs.LG

TL;DR: TOM introduces a data-free, neural network-based method for topology optimization, generating diverse and near-optimal designs without relying on datasets.


<details>
  <summary>Details</summary>
Motivation: Established topology optimization methods produce single solutions, limiting design exploration. TOM addresses this by enabling diverse solutions.

Method: TOM trains a neural network with a solver-in-the-loop and an explicit diversity constraint to generate structurally compliant shapes.

Result: TOM produces more diverse solutions than previous methods while maintaining near-optimality, validated on 2D and 3D problems.

Conclusion: TOM enhances flexibility and innovation in structural optimization by offering diverse, data-free solutions.

Abstract: Topology optimization (TO) is a family of computational methods that derive near-optimal geometries from formal problem descriptions. Despite their success, established TO methods are limited to generating single solutions, restricting the exploration of alternative designs. To address this limitation, we introduce Topology Optimization using Modulated Neural Fields (TOM) - a data-free method that trains a neural network to generate structurally compliant shapes and explores diverse solutions through an explicit diversity constraint. The network is trained with a solver-in-the-loop, optimizing the material distribution in each iteration. The trained model produces diverse shapes that closely adhere to the design requirements. We validate TOM on 2D and 3D TO problems. Our results show that TOM generates more diverse solutions than any previous method, all while maintaining near-optimality and without relying on a dataset. These findings open new avenues for engineering and design, offering enhanced flexibility and innovation in structural optimization.

</details>


### [413] [PerturBench: Benchmarking Machine Learning Models for Cellular Perturbation Analysis](https://arxiv.org/pdf/2408.10609)
*Yan Wu, Esther Wershof, Sebastian M Schmon, Marcel Nassar, Błażej Osiński, Ridvan Eksi, Zichao Yan, Rory Stark, Kun Zhang, Thore Graepel*

Main category: cs.LG

TL;DR: A framework for benchmarking perturbation response models in single cells, emphasizing standardized evaluation, diverse datasets, and fair metrics.


<details>
  <summary>Details</summary>
Motivation: To standardize and improve benchmarking in the rapidly evolving field of single-cell perturbation response modeling.

Method: Developed a modular platform, collected diverse datasets, and introduced metrics for fair model comparison. Evaluated published and baseline models.

Result: Highlighted limitations of popular models (e.g., mode collapse) and showed simpler architectures perform competitively, especially with larger datasets.

Conclusion: The framework sets new evaluation standards, supports robust model development, and enhances disease target discovery potential.

Abstract: We introduce a comprehensive framework for perturbation response modeling in single cells, aimed at standardizing benchmarking in this rapidly evolving field. Our approach includes a modular and user-friendly model development and evaluation platform, a collection of diverse perturbational datasets, and a set of metrics designed to fairly compare models and dissect their performance nuances. Through extensive evaluation of both published and baseline models across diverse datasets, we highlight the limitations of widely used models, such as mode collapse. We also demonstrate the importance of rank metrics which complement traditional model fit measures, such as RMSE, for validating model effectiveness. Notably, our results show that while no single model architecture clearly outperforms others, simpler architectures are generally competitive and scale well with larger datasets. Overall, this benchmarking exercise sets new standards for model evaluation, supports robust model development, and advances the potential of these models to use high-throughput genetic and chemical screens for disease target discovery.

</details>


### [414] [Configuration Interaction Guided Sampling with Interpretable Restricted Boltzmann Machine](https://arxiv.org/pdf/2409.06146)
*Jorge I. Hernandez-Martinez, Andres Mendez-Vazquez, Gerardo Rodriguez-Hernandez, Sandra Leticia Juárez-Osorio*

Main category: cs.LG

TL;DR: A data-driven approach using Restricted Boltzmann Machine (RBM) with a taboo list strategy efficiently solves the Schrödinger equation, reducing computational cost and achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional Configuration Interaction (CI) methods are computationally expensive due to factorial growth in configurations. A more efficient and interpretable method is needed.

Method: Uses RBM to sample significant determinants, incorporating a taboo list for enhanced efficiency and convergence.

Result: Achieves 99.99% correlation energy with significantly fewer determinants (4 orders less than full CI, 2 orders less than prior methods). Also reveals interpretable electron distributions.

Conclusion: The RBM-based method is efficient, accurate, and interpretable, showcasing machine learning's potential in quantum chemistry.

Abstract: We propose a data-driven approach using a Restricted Boltzmann Machine (RBM) to solve the Schrödinger equation in configuration space. Traditional Configuration Interaction (CI) methods construct the wavefunction as a linear combination of Slater determinants, but this becomes computationally expensive due to the factorial growth in the number of configurations. Our approach extends the use of a generative model such as the RBM by incorporating a taboo list strategy to enhance efficiency and convergence. The RBM is used to efficiently identify and sample the most significant determinants, thus accelerating convergence and substantially reducing computational cost. This method achieves up to 99.99% of the correlation energy while using up to four orders of magnitude fewer determinants compared to full CI calculations and up to two orders of magnitude fewer than previous state of the art methods. Beyond efficiency, our analysis reveals that the RBM learns electron distributions over molecular orbitals by capturing quantum patterns that resemble Radial Distribution Functions (RDFs) linked to molecular bonding. This suggests that the learned pattern is interpretable, highlighting the potential of machine learning for explainable quantum chemistry

</details>


### [415] [SAE-V: Interpreting Multimodal Models for Enhanced Alignment](https://arxiv.org/pdf/2502.17514)
*Hantao Lou, Changye Li, Jiaming Ji, Yaodong Yang*

Main category: cs.LG

TL;DR: SAE-V extends Sparse Autoencoders to MLLMs for better interpretability and alignment, improving performance with less data.


<details>
  <summary>Details</summary>
Motivation: MLLMs face interpretability and alignment challenges due to complex multimodal semantics, leading to inconsistencies and biases.

Method: Introduces SAE-V, a framework for fine-grained interpretation and cross-modal feature weighting, enhancing alignment without extra models.

Result: SAE-V achieves over 110% performance with less than 50% data in MLLM alignment.

Conclusion: SAE-V improves MLLM interpretability and alignment, offering insights into cross-modal interactions.

Abstract: With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V's ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.

</details>


### [416] [Learning Spatially Adaptive $\ell_1$-Norms Weights for Convolutional Synthesis Regularization](https://arxiv.org/pdf/2503.09483)
*Andreas Kofler, Luca Calatroni, Christoph Kolbitsch, Kostas Papafitsoros*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose an unrolled algorithm approach for learning spatially adaptive parameter maps in the framework of convolutional synthesis-based $\ell_1$ regularization. More precisely, we consider a family of pre-trained convolutional filters and estimate deeply parametrized spatially varying parameters applied to the sparse feature maps by means of unrolling a FISTA algorithm to solve the underlying sparse estimation problem. The proposed approach is evaluated for image reconstruction of low-field MRI and compared to spatially adaptive and non-adaptive analysis-type procedures relying on Total Variation regularization and to a well-established model-based deep learning approach. We show that the proposed approach produces visually and quantitatively comparable results with the latter approaches and at the same time remains highly interpretable. In particular, the inferred parameter maps quantify
  the local contribution of each filter in the reconstruction, which provides valuable insight into the algorithm mechanism and could potentially be used to discard unsuited filters.

</details>


### [417] [What is the Right Notion of Distance between Predict-then-Optimize Tasks?](https://arxiv.org/pdf/2409.06997)
*Paula Rodriguez-Diaz, Lingkai Kong, Kai Wang, David Alvarez-Melis, Milind Tambe*

Main category: cs.LG

TL;DR: OTD$^3$ is a novel dataset distance for Predict-then-Optimize frameworks, incorporating downstream decisions to better measure adaptation success compared to traditional feature-label distances.


<details>
  <summary>Details</summary>
Motivation: Traditional dataset distances focus on prediction error, but in Predict-then-Optimize (PtO) frameworks, decision regret is more relevant. Existing measures lack informativeness for PtO tasks.

Method: Proposes OTD$^3$, a dataset distance that includes downstream decisions alongside features and labels. Derives a PtO-specific adaptation bound.

Result: OTD$^3$ effectively captures adaptation success and predicts model transferability in three PtO tasks, outperforming traditional distances.

Conclusion: OTD$^3$ is a more informative and effective dataset distance for PtO frameworks, validated empirically and theoretically.

Abstract: Comparing datasets is a fundamental task in machine learning, essential for various learning paradigms-from evaluating train and test datasets for model generalization to using dataset similarity for detecting data drift. While traditional notions of dataset distances offer principled measures of similarity, their utility has largely been assessed through prediction error minimization. However, in Predict-then-Optimize (PtO) frameworks, where predictions serve as inputs for downstream optimization tasks, model performance is measured through decision regret rather than prediction error. In this work, we propose OTD$^3$ (Optimal Transport Decision-aware Dataset Distance), a novel dataset distance that incorporates downstream decisions in addition to features and labels. We show that traditional feature-label distances lack informativeness in PtO settings, while OTD$^3$ more effectively captures adaptation success. We also derive a PtO-specific adaptation bound based on this distance. Empirically, we show that our proposed distance accurately predicts model transferability across three different PtO tasks from the literature. The code is available at https://github.com/paularodr/OTD3.

</details>


### [418] [Reward Shaping to Mitigate Reward Hacking in RLHF](https://arxiv.org/pdf/2502.18770)
*Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao*

Main category: cs.LG

TL;DR: The paper introduces Preference As Reward (PAR), a novel reward shaping method for RLHF, addressing reward hacking by leveraging latent preferences in the reward model. PAR outperforms existing methods in performance and robustness.


<details>
  <summary>Details</summary>
Motivation: RLHF is prone to reward hacking, degrading alignment with human values. Existing reward shaping lacks systematic study, prompting the need for principled methods like PAR.

Method: Proposes PAR, which uses latent preferences in the reward model as RL signals, guided by bounded rewards and rapid initial growth followed by convergence. Evaluated on Gemma2-2B and Llama3-8B models with Ultrafeedback-Binarized and HH-RLHF datasets.

Result: PAR achieves a 5% higher win rate on AlpacaEval 2.0, is data-efficient (single reference reward), and robust against reward hacking.

Conclusion: PAR is a superior, efficient, and robust reward shaping method for RLHF, validated by empirical results.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B, and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR, and the Work done during the internship at StepFun by Jiayi Fu.

</details>


### [419] [Improved Off-policy Reinforcement Learning in Biological Sequence Design](https://arxiv.org/pdf/2410.04461)
*Hyeonah Kim, Minsu Kim, Taeyoung Yun, Sanghyeok Choi, Emmanuel Bengio, Alex Hernández-García, Jinkyoo Park*

Main category: cs.LG

TL;DR: Proposes $δ$-Conservative Search, a robust off-policy method for biological sequence design, outperforming existing methods by restricting exploration to reliable regions and adapting conservativeness based on proxy uncertainty.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in biological sequence design, such as vast search spaces and proxy model misspecification due to limited training data.

Method: Introduces $δ$-Conservative Search, injecting noise by masking tokens with probability $δ$ and denoising with a policy, while adapting $δ$ based on proxy uncertainty.

Result: Outperforms existing methods in discovering high-score sequences for DNA, RNA, protein, and peptide design.

Conclusion: The method enhances off-policy training robustness and effectiveness in biological sequence design.

Abstract: Designing biological sequences with desired properties is challenging due to vast search spaces and limited evaluation budgets. Although reinforcement learning methods use proxy models for rapid reward evaluation, insufficient training data can cause proxy misspecification on out-of-distribution inputs. To address this, we propose a novel off-policy search, $δ$-Conservative Search, that enhances robustness by restricting policy exploration to reliable regions. Starting from high-score offline sequences, we inject noise by randomly masking tokens with probability $δ$, then denoise them using our policy. We further adapt $δ$ based on proxy uncertainty on each data point, aligning the level of conservativeness with model confidence. Experimental results show that our conservative search consistently enhances the off-policy training, outperforming existing machine learning methods in discovering high-score sequences across diverse tasks, including DNA, RNA, protein, and peptide design.

</details>


### [420] [When are dynamical systems learned from time series data statistically accurate?](https://arxiv.org/pdf/2411.06311)
*Jeongjin Park, Nicole Yang, Nisha Chandramoorthy*

Main category: cs.LG

TL;DR: The paper proposes an ergodic theoretic approach to assess generalization in neural networks for dynamical systems, addressing limitations of conventional metrics like test error.


<details>
  <summary>Details</summary>
Motivation: Traditional generalization metrics fail to capture physical behavior in dynamical systems, such as statistical moments and Lyapunov exponents.

Method: An ergodic theoretic framework is introduced to evaluate neural representations of ergodic systems, including chaotic ones, focusing on invariant physical measures.

Result: The study explains why Neural ODEs fail to generalize and shows improved accuracy with Jacobian information during training.

Conclusion: The approach is validated on various ergodic chaotic systems and neural architectures, demonstrating its effectiveness.

Abstract: Conventional notions of generalization often fail to describe the ability of learned models to capture meaningful information from dynamical data. A neural network that learns complex dynamics with a small test error may still fail to reproduce its \emph{physical} behavior, including associated statistical moments and Lyapunov exponents. To address this gap, we propose an ergodic theoretic approach to generalization of complex dynamical models learned from time series data. Our main contribution is to define and analyze generalization of a broad suite of neural representations of classes of ergodic systems, including chaotic systems, in a way that captures emulating underlying invariant, physical measures. Our results provide theoretical justification for why regression methods for generators of dynamical systems (Neural ODEs) fail to generalize, and why their statistical accuracy improves upon adding Jacobian information during training. We verify our results on a number of ergodic chaotic systems and neural network parameterizations, including MLPs, ResNets, Fourier Neural layers, and RNNs.

</details>


### [421] [OWLViz: An Open-World Benchmark for Visual Question Answering](https://arxiv.org/pdf/2503.07631)
*Thuy Nguyen, Dang Nguyen, Hoang Nguyen, Thuan Luong, Long Hoang Dang, Viet Dac Lai*

Main category: cs.LG

TL;DR: A benchmark (OWLViz) for Open World Visual Question Answering highlights the gap between human (69.2%) and AI (26.6% for Gemini 2.0) performance, revealing limitations in multimodal systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in Open World Visual Question Answering by evaluating the integration of visual understanding, web exploration, and tool usage.

Method: Introduces the OWLViz benchmark with concise, unambiguous queries requiring multimodal capabilities.

Result: Humans achieve 69.2% accuracy, while state-of-the-art VLMs like Gemini 2.0 only reach 26.6%, showing significant limitations.

Conclusion: The performance gap underscores the need for advancements in multimodal systems for complex reasoning and tool selection.

Abstract: We present a challenging benchmark for the Open WorLd VISual question answering (OWLViz) task. OWLViz presents concise, unambiguous queries that require integrating multiple capabilities, including visual understanding, web exploration, and specialized tool usage. While humans achieve 69.2% accuracy on these intuitive tasks, even state-of-the-art VLMs struggle, with the best model, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which rely on limited vision and vision-language models as tools, perform even worse. This performance gap reveals significant limitations in multimodal systems' ability to select appropriate tools and execute complex reasoning sequences, establishing new directions for advancing practical AI research.

</details>


### [422] [Evaluating Rank-N-Contrast: Continuous and Robust Representations for Regression](https://arxiv.org/pdf/2411.16298)
*Valentin Six, Alexandre Chidiac, Arkin Worlikar*

Main category: cs.LG

TL;DR: A replication of the Rank-N-Contrast (RNC) paper validates its framework for learning continuous representations in deep regression, confirming improved performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Deep regression models often struggle with fragmented representations due to ignoring sample order continuity. RNC addresses this by contrasting samples based on rankings.

Method: Reproduced the RNC framework, extended evaluation to an additional dataset, and tested robustness using a holdout method excluding specific data ranges.

Result: RNC demonstrated improved performance and robustness, generalizing well to unseen data and achieving state-of-the-art results.

Conclusion: The replication confirms RNC's effectiveness and broadens its applicability, reinforcing its value for continuous representation learning.

Abstract: This document is a replication of the original "Rank-N-Contrast" (arXiv:2210.01189v2) paper published in 2023. This evaluation is done for academic purposes. Deep regression models often fail to capture the continuous nature of sample orders, creating fragmented representations and suboptimal performance. To address this, we reproduced the Rank-N-Contrast (RNC) framework, which learns continuous representations by contrasting samples by their rankings in the target space. Our study validates RNC's theoretical and empirical benefits, including improved performance and robustness. We extended the evaluation to an additional regression dataset and conducted robustness tests using a holdout method, where a specific range of continuous data was excluded from the training set. This approach assessed the model's ability to generalise to unseen data and achieve state-of-the-art performance. This replication study validates the original findings and broadens the understanding of RNC's applicability and robustness.

</details>


### [423] [GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation](https://arxiv.org/pdf/2505.19802)
*Zhiyu Wang, Yang Liu, Hatice Gunes*

Main category: cs.LG

TL;DR: GraphAU-Pain uses a graph-based framework to model facial Action Units (AUs) for pain intensity estimation, improving interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing pain detection methods lack interpretability and severity quantification, which is crucial for digital healthcare, especially for non-verbal patients.

Method: A graph-based framework represents AUs as nodes and their relationships as edges, using a relational graph neural network for pain intensity estimation.

Result: GraphAU-Pain achieved an F1-score of 66.21% and accuracy of 87.61% on the UNBC dataset.

Conclusion: The proposed method enhances interpretability and performance in pain intensity estimation, benefiting digital healthcare applications.

Abstract: Understanding pain-related facial behaviors is essential for digital healthcare in terms of effective monitoring, assisted diagnostics, and treatment planning, particularly for patients unable to communicate verbally. Existing data-driven methods of detecting pain from facial expressions are limited due to interpretability and severity quantification. To this end, we propose GraphAU-Pain, leveraging a graph-based framework to model facial Action Units (AUs) and their interrelationships for pain intensity estimation. AUs are represented as graph nodes, with co-occurrence relationships as edges, enabling a more expressive depiction of pain-related facial behaviors. By utilizing a relational graph neural network, our framework offers improved interpretability and significant performance gains. Experiments conducted on the publicly available UNBC dataset demonstrate the effectiveness of the GraphAU-Pain, achieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity estimation.

</details>


### [424] [Correlation-Aware Graph Convolutional Networks for Multi-Label Node Classification](https://arxiv.org/pdf/2411.17350)
*Yuanchen Bei, Weizhi Chen, Hao Chen, Sheng Zhou, Carl Yang, Jiapei Fan, Longtao Huang, Jiajun Bu*

Main category: cs.LG

TL;DR: Proposes CorGCN, a Correlation-aware Graph Convolutional Network, to improve multi-label node classification by addressing feature and topology ambiguity and leveraging label correlations.


<details>
  <summary>Details</summary>
Motivation: Existing GCN-based methods for multi-label node classification suffer from ambiguous features and topology, reducing message credibility and overlooking label correlations.

Method: Introduces a Correlation-Aware Graph Decomposition module and Correlation-Enhanced Graph Convolution to model label relationships and reduce ambiguity.

Result: Demonstrates effectiveness through extensive experiments on five datasets.

Conclusion: CorGCN successfully addresses ambiguity and enhances classification by leveraging label correlations.

Abstract: Multi-label node classification is an important yet under-explored domain in graph mining as many real-world nodes belong to multiple categories rather than just a single one. Although a few efforts have been made by utilizing Graph Convolution Networks (GCNs) to learn node representations and model correlations between multiple labels in the embedding space, they still suffer from the ambiguous feature and ambiguous topology induced by multiple labels, which reduces the credibility of the messages delivered in graphs and overlooks the label correlations on graph data. Therefore, it is crucial to reduce the ambiguity and empower the GCNs for accurate classification. However, this is quite challenging due to the requirement of retaining the distinctiveness of each label while fully harnessing the correlation between labels simultaneously. To address these issues, in this paper, we propose a Correlation-aware Graph Convolutional Network (CorGCN) for multi-label node classification. By introducing a novel Correlation-Aware Graph Decomposition module, CorGCN can learn a graph that contains rich label-correlated information for each label. It then employs a Correlation-Enhanced Graph Convolution to model the relationships between labels during message passing to further bolster the classification process. Extensive experiments on five datasets demonstrate the effectiveness of our proposed CorGCN.

</details>


### [425] [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/pdf/2506.00555)
*Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Shujie Liu, Yan Lu, Huaxiu Yao*

Main category: cs.LG

TL;DR: MMedAgent-RL is a reinforcement learning-based multi-agent framework for medical diagnostics, improving flexibility and performance over static models.


<details>
  <summary>Details</summary>
Motivation: Existing single-agent and static multi-agent models lack generalization and adaptability in medical diagnostics.

Method: Uses RL to train GP agents for dynamic collaboration, with curriculum learning to balance specialist inputs.

Result: Outperforms other models by 20.7% on benchmarks, showing human-like reasoning.

Conclusion: MMedAgent-RL enhances diagnostic accuracy and adaptability in medical VLMs.

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 20.7% over supervised fine-tuning baselines.

</details>


### [426] [Regional climate risk assessment from climate models using probabilistic machine learning](https://arxiv.org/pdf/2412.08079)
*Zhong Yi Wan, Ignacio Lopez-Gomez, Robert Carver, Tapio Schneider, John Anderson, Fei Sha, Leonardo Zepeda-Núñez*

Main category: cs.LG

TL;DR: GenFocal is a scalable, general-purpose generative framework for downscaling climate simulations, accurately capturing interdependencies among climate processes to assess extreme risks and project climate impacts.


<details>
  <summary>Details</summary>
Motivation: The need for high-resolution climate data for hazard risk assessment and infrastructure planning, overcoming limitations of current downscaling methods.

Method: GenFocal, an end-to-end generative framework, probabilistically characterizes complex climate processes at fine scales, integrating observations with simulations.

Result: Outperforms leading methods in extreme risk assessment, generates plausible tropical cyclone tracks, and projects climate impacts consistently with literature.

Conclusion: GenFocal advances climate risk quantification and adaptation strategy evaluation, establishing genAI as a paradigm for modeling complex climate correlations.

Abstract: Accurate, actionable climate information at km scales is crucial for robust natural hazard risk assessment and infrastructure planning. Simulating climate at these resolutions remains intractable, forcing reliance on downscaling: either physics-based or statistical methods that transform climate simulations from coarse to impact-relevant resolutions. One major challenge for downscaling is to comprehensively capture the interdependency among climate processes of interest, a prerequisite for representing climate hazards. However, current approaches either lack the desired scalability or are bespoke to specific types of hazards. We introduce GenFocal, a computationally efficient, general-purpose, end-to-end generative framework that gives rise to full probabilistic characterizations of complex climate processes interacting at fine spatiotemporal scales. GenFocal more accurately assesses extreme risk in the current climate than leading approaches, including one used in the US 5th National Climate Assessment. It produces plausible tracks of tropical cyclones, providing accurate statistics of their genesis and evolution, even when they are absent from the corresponding climate simulations. GenFocal also shows compelling results that are consistent with the literature on projecting climate impact on decadal timescales. GenFocal revolutionizes how climate simulations can be efficiently augmented with observations and harnessed to enable future climate impact assessments at the spatiotemporal scales relevant to local and regional communities. We believe this work establishes genAI as an effective paradigm for modeling complex, high-dimensional multivariate statistical correlations that have deterred precise quantification of climate risks associated with hazards such as wildfires, extreme heat, tropical cyclones, and flooding; thereby enabling the evaluation of adaptation strategies.

</details>


### [427] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/pdf/2506.08001)
*Zeju Qiu, Simon Buchholz, Tim Z. Xiao, Maximilian Dax, Bernhard Schölkopf, Weiyang Liu*

Main category: cs.LG

TL;DR: POET is a reParameterized training algorithm using Orthogonal Equivalence Transformation to optimize neurons, improving stability and generalization in LLM training.


<details>
  <summary>Details</summary>
Motivation: Training large language models (LLMs) effectively and reliably is a major challenge in AI.

Method: POET reparameterizes neurons with two learnable orthogonal matrices and a fixed random weight matrix, preserving spectral properties for stable optimization. Efficient approximations make it scalable.

Result: Extensive experiments show POET's effectiveness and scalability in training LLMs.

Conclusion: POET offers a reliable and scalable solution for optimizing large neural networks, enhancing LLM training.

Abstract: While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.

</details>


### [428] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/pdf/2506.06290)
*Mingyu Lu, Ethan Weinberger, Chanwoo Kim, Su-In Lee*

Main category: cs.LG

TL;DR: CellCLIP is a cross-modal contrastive learning framework for high-content screening (HCS) data, addressing challenges in aligning perturbations with cellular effects.


<details>
  <summary>Details</summary>
Motivation: To understand relationships between perturbations and cellular morphology, leveraging cross-modal contrastive learning for HCS data.

Method: Uses pre-trained image encoders with a novel channel encoding scheme and natural language encoders for perturbations.

Result: Outperforms current models in cross-modal retrieval and downstream tasks, with reduced computation time.

Conclusion: CellCLIP effectively bridges gaps in HCS data analysis, offering improved performance and efficiency.

Abstract: High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.

</details>


### [429] [Transductive Conformal Inference for Full Ranking](https://arxiv.org/pdf/2501.11384)
*Jean-Baptiste Fermanian, Pierre Humbert, Gilles Blanchard*

Main category: cs.LG

TL;DR: A Conformal Prediction (CP) method is introduced to quantify uncertainty in full ranking algorithms, focusing on scenarios where ground truth rankings are partially known.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of quantifying uncertainty in ranking algorithms when ground truth rankings are incomplete, especially for new items.

Method: Proposes constructing distribution-free bounds for unknown conformity scores using conformal p-values, enabling valid prediction sets for ranks.

Result: Provides valid prediction sets for ranks and controls false coverage proportion, demonstrated empirically on synthetic and real data with algorithms like RankNet and LambdaMart.

Conclusion: The CP method effectively quantifies uncertainty in ranking algorithms, even with incomplete ground truth, and performs well empirically.

Abstract: We introduce a method based on Conformal Prediction (CP) to quantify the uncertainty of full ranking algorithms. We focus on a specific scenario where $n+m$ items are to be ranked by some ``black box'' algorithm. It is assumed that the relative (ground truth) ranking of $n$ of them is known. The objective is then to quantify the error made by the algorithm on the ranks of the $m$ new items among the total $(n+m)$. In such a setting, the true ranks of the $n$ original items in the total $(n+m)$ depend on the (unknown) true ranks of the $m$ new ones. Consequently, we have no direct access to a calibration set to apply a classical CP method. To address this challenge, we propose to construct distribution-free bounds of the unknown conformity scores using recent results on the distribution of conformal p-values. Using these scores upper bounds, we provide valid prediction sets for the rank of any item. We also control the false coverage proportion, a crucial quantity when dealing with multiple prediction sets. Finally, we empirically show on both synthetic and real data the efficiency of our CP method for state-of-the-art algorithms such as RankNet or LambdaMart.

</details>


### [430] [SeqPE: Transformer with Sequential Position Encoding](https://arxiv.org/pdf/2506.13277)
*Huayang Li, Yahui Liu, Hongyu Sun, Deng Cai, Leyang Cui, Wei Bi, Peilin Zhao, Taro Watanabe*

Main category: cs.LG

TL;DR: SeqPE is a learnable position encoding framework that uses symbolic sequences for position indices, enhancing adaptability and scalability without manual redesign.


<details>
  <summary>Details</summary>
Motivation: Traditional position embeddings (PEs) like ALiBi and RoPE have limitations in adaptability and scalability, requiring extensive modifications for new modalities.

Method: SeqPE represents position indices as symbolic sequences and uses a lightweight encoder with contrastive and knowledge distillation objectives for regularization.

Result: SeqPE outperforms baselines in perplexity, exact match, and accuracy, especially in context length extrapolation, and generalizes to multi-dimensional inputs.

Conclusion: SeqPE offers a unified, scalable solution for position encoding, improving performance and adaptability across tasks and modalities.

Abstract: Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each $n$-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracy--particularly under context length extrapolation--but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe.

</details>


### [431] [An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/pdf/2506.09172)
*Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Jaewoo Song, Harshvardhan Sikka*

Main category: cs.LG

TL;DR: MultiNet is an open-source benchmark and ecosystem for evaluating multimodal action models (VLMs and VLAs) across vision, language, and action tasks.


<details>
  <summary>Details</summary>
Motivation: To advance general-purpose agentic systems by rigorously evaluating and adapting multimodal models.

Method: Introduces MultiNet, a benchmark with standardized protocols, open-source tools, and a composite dataset of 1.3 trillion tokens for diverse tasks.

Result: MultiNet supports downstream research on VLA generalization limitations.

Conclusion: MultiNet provides a comprehensive framework for evaluating and advancing multimodal action models.

Abstract: Recent innovations in multimodal action models represent a promising direction for developing general-purpose agentic systems, combining visual understanding, language comprehension, and action generation. We introduce MultiNet - a novel, fully open-source benchmark and surrounding software ecosystem designed to rigorously evaluate and adapt models across vision, language, and action domains. We establish standardized evaluation protocols for assessing vision-language models (VLMs) and vision-language-action models (VLAs), and provide open source software to download relevant data, models, and evaluations. Additionally, we provide a composite dataset with over 1.3 trillion tokens of image captioning, visual question answering, commonsense reasoning, robotic control, digital game-play, simulated locomotion/manipulation, and many more tasks. The MultiNet benchmark, framework, toolkit, and evaluation harness have been used in downstream research on the limitations of VLA generalization.

</details>


### [432] [Temperature-Annealed Boltzmann Generators](https://arxiv.org/pdf/2501.19077)
*Henrik Schopmans, Pascal Friederich*

Main category: cs.LG

TL;DR: TA-BG improves sampling of Boltzmann distributions by combining high-temperature training and reweighting, outperforming baselines with fewer energy evaluations.


<details>
  <summary>Details</summary>
Motivation: Addressing mode collapse in variational sampling methods for Boltzmann distributions.

Method: Uses temperature-annealed Boltzmann generators (TA-BG) with high-temperature training and reweighting.

Result: Better performance in metrics, fewer energy evaluations, and accurate resolution of metastable states.

Conclusion: TA-BG is effective for sampling complex molecular systems.

Abstract: Efficient sampling of unnormalized probability densities such as the Boltzmann distribution of molecular systems is a longstanding challenge. Next to conventional approaches like molecular dynamics or Markov chain Monte Carlo, variational approaches, such as training normalizing flows with the reverse Kullback-Leibler divergence, have been introduced. However, such methods are prone to mode collapse and often do not learn to sample the full configurational space. Here, we present temperature-annealed Boltzmann generators (TA-BG) to address this challenge. First, we demonstrate that training a normalizing flow with the reverse Kullback-Leibler divergence at high temperatures is possible without mode collapse. Furthermore, we introduce a reweighting-based training objective to anneal the distribution to lower target temperatures. We apply this methodology to three molecular systems of increasing complexity and, compared to the baseline, achieve better results in almost all metrics while requiring up to three times fewer target energy evaluations. For the largest system, our approach is the only method that accurately resolves the metastable states of the system.

</details>


### [433] [VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models](https://arxiv.org/pdf/2506.13754)
*Edward Li, Zichen Wang, Jiahe Huang, Jeong Joon Park*

Main category: cs.LG

TL;DR: A unified framework for solving PDEs using video-inpainting diffusion transformer models, treating PDE-solving as a generalized inpainting problem.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack flexibility by specializing in either forward or inverse problems under full or partial observation. This work aims to unify these tasks under a single generative framework.

Method: Recasts PDE-solving as a generalized inpainting problem, using a transformer-based architecture for inferring missing spatiotemporal data. Employs pixel-space video diffusion models for high-fidelity inpainting and hierarchical modeling for efficiency.

Result: Outperforms state-of-the-art baselines, offering accurate and versatile solutions across various PDEs and problem setups.

Conclusion: The proposed video inpainting-based diffusion model provides a flexible, efficient, and high-fidelity approach to solving PDEs.

Abstract: We present a unified framework for solving partial differential equations (PDEs) using video-inpainting diffusion transformer models. Unlike existing methods that devise specialized strategies for either forward or inverse problems under full or partial observation, our approach unifies these tasks under a single, flexible generative framework. Specifically, we recast PDE-solving as a generalized inpainting problem, e.g., treating forward prediction as inferring missing spatiotemporal information of future states from initial conditions. To this end, we design a transformer-based architecture that conditions on arbitrary patterns of known data to infer missing values across time and space. Our method proposes pixel-space video diffusion models for fine-grained, high-fidelity inpainting and conditioning, while enhancing computational efficiency through hierarchical modeling. Extensive experiments show that our video inpainting-based diffusion model offers an accurate and versatile solution across a wide range of PDEs and problem setups, outperforming state-of-the-art baselines.

</details>


### [434] [Reinforcement Learning with Segment Feedback](https://arxiv.org/pdf/2502.01876)
*Yihan Du, Anna Winnicki, Gal Dalal, Shie Mannor, R. Srikant*

Main category: cs.LG

TL;DR: The paper introduces RL with segment feedback, a model bridging per-state-action and trajectory feedback, and analyzes how segment count affects learning performance under binary and sum feedback.


<details>
  <summary>Details</summary>
Motivation: Practical RL applications often lack per-state-action rewards, and trajectory feedback may be inefficient for long trajectories. Segment feedback offers a middle ground.

Method: The study uses episodic MDPs divided into segments, with reward feedback at segment ends. Algorithms are designed for binary and sum feedback, and regret bounds are derived.

Result: Binary feedback shows exponential regret reduction with more segments, while sum feedback sees negligible improvement.

Conclusion: Segment feedback is effective, but its impact varies by feedback type, with binary feedback benefiting more from increased segments.

Abstract: Standard reinforcement learning (RL) assumes that an agent can observe a reward for each state-action pair. However, in practical applications, it is often difficult and costly to collect a reward for each state-action pair. While there have been several works considering RL with trajectory feedback, it is unclear if trajectory feedback is inefficient for learning when trajectories are long. In this work, we consider a model named RL with segment feedback, which offers a general paradigm filling the gap between per-state-action feedback and trajectory feedback. In this model, we consider an episodic Markov decision process (MDP), where each episode is divided into $m$ segments, and the agent observes reward feedback only at the end of each segment. Under this model, we study two popular feedback settings: binary feedback and sum feedback, where the agent observes a binary outcome and a reward sum according to the underlying reward function, respectively. To investigate the impact of the number of segments $m$ on learning performance, we design efficient algorithms and establish regret upper and lower bounds for both feedback settings. Our theoretical and experimental results show that: under binary feedback, increasing the number of segments $m$ decreases the regret at an exponential rate; in contrast, surprisingly, under sum feedback, increasing $m$ does not reduce the regret significantly.

</details>


### [435] [On the Emergence of Position Bias in Transformers](https://arxiv.org/pdf/2502.01951)
*Xinyi Wu, Yifei Wang, Stefanie Jegelka, Ali Jadbabaie*

Main category: cs.LG

TL;DR: The paper introduces a graph-theoretic framework to analyze position bias in transformers, revealing how attention masks and positional encodings shape biases, with insights into causal masking and relative positional encodings.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive theoretical understanding of position bias in transformers, addressing gaps in knowledge about how attention masks and positional encodings influence biases.

Method: A graph-theoretic approach models attention masks as directed graphs to quantify token interactions based on sequential positions, analyzing causal masking and relative positional encodings.

Result: Key findings include the inherent bias of causal masking toward earlier positions and the trade-off between long-term decay and early position importance due to combined effects of masks and encodings.

Conclusion: The framework offers a principled foundation for understanding positional biases, aiding in informed transformer design by clarifying the interplay of attention components.

Abstract: Recent studies have revealed various manifestations of position bias in transformer architectures, from the "lost-in-the-middle" phenomenon to attention sinks, yet a comprehensive theoretical understanding of how attention masks and positional encodings shape these biases remains elusive. This paper presents a graph-theoretic framework for analyzing position bias in multi-layer attention. Modeling attention masks as directed graphs, we quantify how tokens interact with contextual information based on their sequential positions. We uncover two key insights: First, causal masking inherently biases attention toward earlier positions, as tokens in deeper layers attend to increasingly more contextualized representations of earlier tokens. Second, we characterize the competing effects of the causal mask and relative positional encodings, such as the decay mask and rotary positional encoding (RoPE): while both mechanisms introduce distance-based decay within individual attention maps, their aggregate effect across multiple attention layers$\unicode{x2013}$coupled with the causal mask$\unicode{x2013}$leads to a trade-off between the long-term decay effects and the cumulative importance of early sequence positions. Through controlled numerical experiments, we not only validate our theoretical findings but also reproduce position biases observed in real-world LLMs. Our framework offers a principled foundation for understanding positional biases in transformers, shedding light on the complex interplay of attention mechanism components and guiding more informed architectural design.

</details>


### [436] [NAROCE: A Neural Algorithmic Reasoner Framework for Online Complex Event Detection](https://arxiv.org/pdf/2502.07250)
*Liying Han, Gaofeng Dong, Xiaomin Ouyang, Lance Kaplan, Federico Cerutti, Mani Srivastava*

Main category: cs.LG

TL;DR: NAROCE is a Neural Algorithmic Reasoning framework for detecting complex events (CEs) online by separating rule learning from sensor data mapping, improving accuracy, generalization, and data efficiency.


<details>
  <summary>Details</summary>
Motivation: Real-world tasks like smart cities and healthcare require detecting complex events (CEs), which are rare, require long-range reasoning, and lack labeled datasets.

Method: NAROCE learns CE rules from pseudo atomic event (AE) traces (simulators/LLMs) and trains an adapter to map real sensor data to the learned reasoning space.

Result: NAROCE outperforms baselines in accuracy, generalization to longer sequences, and data efficiency, achieving comparable performance with less than half the labeled data.

Conclusion: Decoupling CE rule learning from raw sensor inputs enhances data efficiency and robustness.

Abstract: Modern machine learning models excel at detecting individual actions, objects, or scene attributes from short, local observations. However, many real-world tasks, such as in smart cities and healthcare, require reasoning over complex events (CEs): (spatio)temporal, rule-governed patterns of short-term atomic events (AEs) that reflect high-level understanding and critical changes in the environment. These CEs are difficult to detect online: they are often rare, require long-range reasoning over noisy sensor data, must generalize rules beyond fixed-length traces, and suffer from limited real-world datasets due to the high annotation burden. We propose NAROCE, a Neural Algorithmic Reasoning framework for Online CE detection that separates the task into two stages: (i) learning CE rules from large-scale, low-cost pseudo AE concept traces generated by simulators or LLMs, and (ii) training an adapter to map real sensor data into the learned reasoning space using fewer labeled sensor samples. Experiments show that NAROCE outperforms the strongest baseline in accuracy, generalization to longer, unseen sequences, and data efficiency, achieving comparable performance with less than half the labeled data. These results suggest that decoupling CE rule learning from raw sensor inputs improves both data efficiency and robustness.

</details>


### [437] [Rao-Blackwell Gradient Estimators for Equivariant Denoising Diffusion](https://arxiv.org/pdf/2502.09890)
*Vinh Tong, Trung-Dung Hoang, Anji Liu, Guy Van den Broeck, Mathias Niepert*

Main category: cs.LG

TL;DR: The paper introduces a framework combining equivariant architectures and data augmentation for learning invariant distributions, using Rao-Blackwellization to reduce variance and improve optimization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of equivariant architectures (complexity) and data augmentation (incomplete symmetry capture) in modeling symmetries in physical systems.

Method: Proposes a framework interpreting data augmentation as a Monte Carlo gradient estimator, applying Rao-Blackwellization for lower variance, and introduces Orbit Diffusion for practical implementation.

Result: Achieves state-of-the-art results in molecular conformation generation, crystal structure prediction, and protein designability.

Conclusion: The framework enhances both equivariant architectures and data augmentation, providing stable optimization and improved performance across multiple domains.

Abstract: In domains such as molecular and protein generation, physical systems exhibit inherent symmetries that are critical to model. Two main strategies have emerged for learning invariant distributions: designing equivariant network architectures and using data augmentation to approximate equivariance. While equivariant architectures preserve symmetry by design, they often involve greater complexity and pose optimization challenges. Data augmentation, on the other hand, offers flexibility but may fall short in fully capturing symmetries. Our framework enhances both approaches by reducing training variance and providing a provably lower-variance gradient estimator. We achieve this by interpreting data augmentation as a Monte Carlo estimator of the training gradient and applying Rao-Blackwellization. This leads to more stable optimization, faster convergence, and reduced variance, all while requiring only a single forward and backward pass per sample. We also present a practical implementation of this estimator incorporating the loss and sampling procedure through a method we call Orbit Diffusion. Theoretically, we guarantee that our loss admits equivariant minimizers. Empirically, Orbit Diffusion achieves state-of-the-art results on GEOM-QM9 for molecular conformation generation, improves crystal structure prediction, and advances text-guided crystal generation on the Perov-5 and MP-20 benchmarks. Additionally, it enhances protein designability in protein structure generation.

</details>


### [438] [Generalization error bound for denoising score matching under relaxed manifold assumption](https://arxiv.org/pdf/2502.13662)
*Konstantin Yakovlev, Nikita Puchkin*

Main category: cs.LG

TL;DR: The paper analyzes denoising score matching estimates under relaxed manifold assumptions, deriving non-asymptotic error bounds tied to intrinsic dimension.


<details>
  <summary>Details</summary>
Motivation: To relax the restrictive manifold assumption in denoising score matching while still leveraging distribution structure.

Method: Models observation density with a nonparametric Gaussian mixture and derives bounds on approximation and generalization errors.

Result: Non-asymptotic bounds show convergence rates depend on intrinsic dimension, valid even with growing ambient dimension.

Conclusion: The approach successfully relaxes standard assumptions while maintaining theoretical guarantees.

Abstract: We examine theoretical properties of the denoising score matching estimate. We model the density of observations with a nonparametric Gaussian mixture. We significantly relax the standard manifold assumption allowing the samples step away from the manifold. At the same time, we are still able to leverage a nice distribution structure. We derive non-asymptotic bounds on the approximation and generalization errors of the denoising score matching estimate. The rates of convergence are determined by the intrinsic dimension. Furthermore, our bounds remain valid even if we allow the ambient dimension grow polynomially with the sample size.

</details>


### [439] [Conformal Prediction Sets for Deep Generative Models via Reduction to Conformal Regression](https://arxiv.org/pdf/2503.10512)
*Hooman Shahrokhi, Devjeet Raj Roy, Yan Yan, Venera Arnaoudova, Janaradhan Rao Doppa*

Main category: cs.LG

TL;DR: The paper introduces GPS, a conformal inference algorithm for generating valid and small prediction sets from black-box deep generative models, with provable guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating prediction sets that meet user-defined admissibility criteria (e.g., at least one valid program in code generation) using deep generative models.

Method: Develops GPS, a conformal inference algorithm that leverages the distribution of minimum samples needed for an admissible output, applied to calibration examples.

Result: GPS outperforms state-of-the-art methods in experiments on code and math word problems using large language models.

Conclusion: GPS provides a simple and effective solution for generating valid prediction sets with theoretical guarantees, applicable to diverse tasks.

Abstract: We consider the problem of generating valid and small prediction sets by sampling outputs (e.g., software code and natural language text) from a black-box deep generative model for a given input (e.g., textual prompt). The validity of a prediction set is determined by a user-defined binary admissibility function depending on the target application. For example, requiring at least one program in the set to pass all test cases in code generation application. To address this problem, we develop a simple and effective conformal inference algorithm referred to as Generative Prediction Sets (GPS). Given a set of calibration examples and black-box access to a deep generative model, GPS can generate prediction sets with provable guarantees. The key insight behind GPS is to exploit the inherent structure within the distribution over the minimum number of samples needed to obtain an admissible output to develop a simple conformal regression approach over the minimum number of samples. Experiments on multiple datasets for code and math word problems using different large language models demonstrate the efficacy of GPS over state-of-the-art methods.

</details>


### [440] [Analytics Modelling over Multiple Datasets using Vector Embeddings](https://arxiv.org/pdf/2502.17060)
*Andreas Loizou, Dimitrios Tsoumakos*

Main category: cs.LG

TL;DR: A novel deep learning model, NumTabData2Vec, is proposed to infer analytics outcomes by transforming datasets into vector embeddings, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting high-quality datasets from large-scale availability to enhance analytics performance.

Method: Proposes NumTabData2Vec, a deep learning model for dataset vectorization, and uses similarity search for outcome prediction.

Result: Outperforms state-of-the-art frameworks in prediction accuracy and execution speed, with accurate lower-dimensional vector embeddings.

Conclusion: The approach effectively addresses dataset selection challenges, improving analytics performance through accurate vectorization and prediction.

Abstract: The massive increase in the data volume and dataset availability for analysts compels researchers to focus on data content and select high-quality datasets to enhance the performance of analytics operators. While selecting high-quality data significantly boosts analytical accuracy and efficiency, the exact process is very challenging given large-scale dataset availability. To address this issue, we propose a novel methodology that infers the outcome of analytics operators by creating a model from the available datasets. Each dataset is transformed to a vector embedding representation generated by our proposed deep learning model NumTabData2Vec, where similarity search are employed. Through experimental evaluation, we compare the prediction performance and the execution time of our framework to another state-of-the-art modelling operator framework, illustrating that our approach predicts analytics outcomes accurately, and increases speedup. Furthermore, our vectorization model can project different real-world scenarios to a lower vector embedding representation accurately and distinguish them.

</details>


### [441] [Understanding the Trade-offs in Accuracy and Uncertainty Quantification: Architecture and Inference Choices in Bayesian Neural Networks](https://arxiv.org/pdf/2503.11808)
*Alisa Sheinkman, Sara Wade*

Main category: cs.LG

TL;DR: The paper explores challenges in Bayesian neural networks, comparing MCMC and variational inference for accuracy and uncertainty quantification, favoring variational methods with ensembles for cost efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the difficulty of achieving high predictive performance and reliable uncertainty in complex neural networks due to computational and theoretical limitations of Bayesian methods.

Method: Compares Markov chain Monte Carlo (MCMC) and variational inference, evaluating their computational costs, accuracy, and uncertainty quantification, and explores model averaging and ensembling techniques.

Result: Variational inference outperformed MCMC in uncertainty quantification, while ensembles of variational approximations matched MCMC's accuracy at lower computational cost.

Conclusion: Variational inference, especially with ensembling, offers a practical balance of accuracy and computational efficiency for Bayesian neural networks.

Abstract: As modern neural networks get more complex, specifying a model with high predictive performance and sound uncertainty quantification becomes a more challenging task. Despite some promising theoretical results on the true posterior predictive distribution of Bayesian neural networks, the properties of even the most commonly used posterior approximations are often questioned. Computational burdens and intractable posteriors expose miscalibrated Bayesian neural networks to poor accuracy and unreliable uncertainty estimates. Approximate Bayesian inference aims to replace unknown and intractable posterior distributions with some simpler but feasible distributions. The dimensions of modern deep models, coupled with the lack of identifiability, make Markov chain Monte Carlo (MCMC) tremendously expensive and unable to fully explore the multimodal posterior. On the other hand, variational inference benefits from improved computational complexity but lacks the asymptotical guarantees of sampling-based inference and tends to concentrate around a single mode. The performance of both approaches heavily depends on architectural choices; this paper aims to shed some light on this by considering the computational costs, accuracy and uncertainty quantification in different scenarios including large width and out-of-sample data. To improve posterior exploration, different model averaging and ensembling techniques are studied, along with their benefits on predictive performance. In our experiments, variational inference overall provided better uncertainty quantification than MCMC; further, stacking and ensembles of variational approximations provided comparable accuracy to MCMC at a much-reduced cost.

</details>


### [442] [Whenever, Wherever: Towards Orchestrating Crowd Simulations with Spatio-Temporal Spawn Dynamics](https://arxiv.org/pdf/2503.16639)
*Thomas Kreutz, Max Mühlhäuser, Alejandro Sanchez Guinea*

Main category: cs.LG

TL;DR: The paper introduces nTPP-GMM, a method combining Neural Temporal Point Processes and Gaussian Mixture Models to improve realism in crowd simulations by modeling spatio-temporal spawn dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven methods focus on microscopic realism but neglect macroscopic crowd features like density and flow, which are crucial for realistic simulations. Traditional spawn methods lack diversity and realism.

Method: Proposes nTPP-GMM, using Neural Temporal Point Processes for temporal spawn dynamics and a spawn-conditional Gaussian Mixture Model for spatial spawn and goal positions.

Result: Evaluated on three real-world datasets, nTPP-GMM produces realistic crowd simulations that mirror real-world scenarios and enable crowd analysis.

Conclusion: nTPP-GMM effectively addresses the limitations of traditional spawn methods, enhancing both microscopic and macroscopic realism in crowd simulations.

Abstract: Realistic crowd simulations are essential for immersive virtual environments, relying on both individual behaviors (microscopic dynamics) and overall crowd patterns (macroscopic characteristics). While recent data-driven methods like deep reinforcement learning improve microscopic realism, they often overlook critical macroscopic features such as crowd density and flow, which are governed by spatio-temporal spawn dynamics, namely, when and where agents enter a scene. Traditional methods, like random spawn rates, stochastic processes, or fixed schedules, are not guaranteed to capture the underlying complexity or lack diversity and realism. To address this issue, we propose a novel approach called nTPP-GMM that models spatio-temporal spawn dynamics using Neural Temporal Point Processes (nTPPs) that are coupled with a spawn-conditional Gaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate our approach by orchestrating crowd simulations of three diverse real-world datasets with nTPP-GMM. Our experiments demonstrate the orchestration with nTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios and allow crowd analysis.

</details>


### [443] [Hybrid Time-Domain Behavior Model Based on Neural Differential Equations and RNNs](https://arxiv.org/pdf/2503.22313)
*Zenghui Chang, Yang Zhang, Hu Tan, Hong Cai Chen*

Main category: cs.LG

TL;DR: A hybrid modeling paradigm combining neural network differential models (NODE-RNN and NCDE-RNN) improves circuit emulation accuracy and efficiency, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional continuous-time domain modeling lacks fitting capability and computational efficiency for circuit IPs and device behaviors.

Method: Integrates neural ordinary differential equations (NODE) and neural controlled differential equations (NCDE) with RNNs to create NODE-RNN and NCDE-RNN models.

Result: NCDE-RNN improves accuracy by 33% over NCDE, and NODE-RNN by 24% over CTRNN, especially in nonlinear memory effects. Successfully deployed in Verilog-A.

Conclusion: The hybrid paradigm offers high-precision circuit modeling and is significant for complex nonlinear systems.

Abstract: Nonlinear dynamics system identification is crucial for circuit emulation. Traditional continuous-time domain modeling approaches have limitations in fitting capability and computational efficiency when used for modeling circuit IPs and device behaviors.This paper presents a novel continuous-time domain hybrid modeling paradigm. It integrates neural network differential models with recurrent neural networks (RNNs), creating NODE-RNN and NCDE-RNN models based on neural ordinary differential equations (NODE) and neural controlled differential equations (NCDE), respectively.Theoretical analysis shows that this hybrid model has mathematical advantages in event-driven dynamic mutation response and gradient propagation stability. Validation using real data from PIN diodes in high-power microwave environments shows NCDE-RNN improves fitting accuracy by 33\% over traditional NCDE, and NODE-RNN by 24\% over CTRNN, especially in capturing nonlinear memory effects.The model has been successfully deployed in Verilog-A and validated through circuit emulation, confirming its compatibility with existing platforms and practical value.This hybrid dynamics paradigm, by restructuring the neural differential equation solution path, offers new ideas for high-precision circuit time-domain modeling and is significant for complex nonlinear circuit system modeling.

</details>


### [444] [Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift](https://arxiv.org/pdf/2505.03617)
*Thien Nhan Vo*

Main category: cs.LG

TL;DR: Importance weighting in deep neural networks shows limited practical utility for real-world distribution shifts, fading with prolonged training and offering no significant gains under covariate shift.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of importance weighting in addressing label shift and covariate shift in deep learning models.

Method: Experiments on synthetic 2D data (logistic regression, MLPs) and CIFAR-10 with class imbalances, testing L2 regularization and dropout.

Result: Weighting impacts early training but fades; L2 regularization preserves effects, while dropout does not. No significant gain under covariate shift.

Conclusion: Importance weighting may not be practically useful for real-world distribution shifts.

Abstract: We evaluate the effectiveness of importance weighting in deep neural networks under label shift and covariate shift. On synthetic 2D data (linearly separable and moon-shaped) using logistic regression and MLPs, we observe that weighting strongly affects decision boundaries early in training but fades with prolonged optimization. On CIFAR-10 with various class imbalances, only L2 regularization (not dropout) helps preserve weighting effects. In a covariate-shift experiment, importance weighting yields no significant performance gain, highlighting challenges on complex data. Our results call into question the practical utility of importance weighting for real-world distribution shifts.

</details>


### [445] [Addition is almost all you need: Compressing neural networks with double binary factorization](https://arxiv.org/pdf/2505.11076)
*Vladimír Boža, Vladimír Macko*

Main category: cs.LG

TL;DR: DBF factorizes weight matrices into two binary matrices with scaling vectors, preserving efficiency while improving accuracy and offering flexible compression control.


<details>
  <summary>Details</summary>
Motivation: Address the computational and storage demands of LLMs while mitigating accuracy loss from binary quantization.

Method: Double Binary Factorization (DBF) decomposes dense weights into two binary matrices with scaling vectors, enabling fine-grained compression control.

Result: DBF outperforms 1-bit binarization methods and matches 2-bit quantization techniques like QuIP# and QTIP, with flexible compression ratios.

Conclusion: DBF provides an efficient, accurate, and adaptable solution for compressing LLMs, with potential for layer-wise optimization.

Abstract: Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria.
  Code available at: https://github.com/usamec/double_binary

</details>


### [446] [FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation](https://arxiv.org/pdf/2505.23182)
*Srijith Nair, Michael Lin, Peizhong Ju, Amirreza Talebi, Elizabeth Serena Bentley, Jia Liu*

Main category: cs.LG

TL;DR: FSL-SAGE is a new federated split learning algorithm that combines the benefits of FL and SL, reducing communication costs and client memory while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods like FL and SL have limitations: FL requires clients to train entire models (infeasible for large models), while SL increases network latency. Other methods lack server feedback, risking poor accuracy.

Method: FSL-SAGE estimates server-side gradient feedback using auxiliary models that adapt to emulate server behavior locally.

Result: The algorithm achieves a convergence rate of O(1/√T), matching FedAvg, while reducing communication costs and client memory. It outperforms existing FSL methods in efficiency and accuracy.

Conclusion: FSL-SAGE effectively addresses the limitations of FL and SL, offering a balanced solution for distributed machine learning.

Abstract: Collaborative training methods like Federated Learning (FL) and Split Learning (SL) enable distributed machine learning without sharing raw data. However, FL assumes clients can train entire models, which is infeasible for large-scale models. In contrast, while SL alleviates the client memory constraint in FL by offloading most training to the server, it increases network latency due to its sequential nature. Other methods address the conundrum by using local loss functions for parallel client-side training to improve efficiency, but they lack server feedback and potentially suffer poor accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation Gradient Estimation), a new federated split learning algorithm that estimates server-side gradient feedback via auxiliary models. These auxiliary models periodically adapt to emulate server behavior on local datasets. We show that FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is the number of communication rounds. This result matches FedAvg, while significantly reducing communication costs and client memory requirements. Our empirical results also verify that it outperforms existing state-of-the-art FSL methods, offering both communication efficiency and accuracy.

</details>


### [447] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/pdf/2505.23247)
*Zonglin Yang, Zhexuan Gu, Houduo Qi, Yancheng Yuan*

Main category: cs.LG

TL;DR: The paper proposes a reward adjustment model to accelerate RLHF training by increasing reward variance while preserving preferences and reward expectation. It introduces an efficient algorithm for solving the nonconvex optimization problem and integrates this into GRPO, resulting in the GRPOVI algorithm, which improves training efficiency.


<details>
  <summary>Details</summary>
Motivation: Efficient RLHF training remains a challenge, and higher reward variance of the initial policy model has been shown to speed up training. The goal is to leverage this insight to enhance GRPO-based RLHF training.

Method: The authors propose a reward adjustment model to increase reward variance and preserve preferences. They solve the nonconvex optimization problem with a novel O(n log n) algorithm and integrate this into GRPO, creating GRPOVI.

Result: Experiments show GRPOVI significantly improves RLHF training efficiency compared to the original GRPO algorithm.

Conclusion: The reward adjustment model and GRPOVI algorithm effectively accelerate RLHF training, with potential broader implications for RLHF methods.

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique for ensuring that large language models (LLMs) are aligned with human values and preferences during the post-training phase. As an effective RLHF approach, group relative policy optimization (GRPO) has demonstrated success in many LLM-based applications. However, efficient GRPO-based RLHF training remains a challenge. Recent studies reveal that a higher reward variance of the initial policy model leads to faster RLHF training. Inspired by this finding, we propose a practical reward adjustment model to accelerate RLHF training by provably increasing the reward variance and preserving the relative preferences and reward expectation. Our reward adjustment method inherently poses a nonconvex optimization problem, which is NP-hard to solve in general. To overcome the computational challenges, we design a novel $O(n \log n)$ algorithm to find a global solution of the nonconvex reward adjustment model by explicitly characterizing the extreme points of the feasible set. As an important application, we naturally integrate this reward adjustment model into the GRPO algorithm, leading to a more efficient GRPO with reward variance increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we provide an indirect explanation for the empirical effectiveness of GRPO with rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment results demonstrate that the GRPOVI algorithm can significantly improve the RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [448] [GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure](https://arxiv.org/pdf/2506.02390)
*Qin Xie, Qinghua Zhang, Shuyin Xia, Xinran Zhou, Guoyin Wang*

Main category: cs.LG

TL;DR: GAdaBoost, a two-stage framework combining data granulation and adaptive boosting, improves robustness and efficiency in noisy multiclass classification tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing AdaBoost's challenges with label noise and computational inefficiency in multiclass classification.

Method: Proposes GAdaBoost with a data granulation stage (granular-ball generation) and an adaptive boosting stage (granular ball-based SAMME).

Result: Achieves superior robustness and efficiency on noisy datasets compared to existing methods.

Conclusion: GAdaBoost effectively extends AdaBoost and SAMME, offering a practical solution for noisy classification tasks.

Abstract: Adaptive Boosting (AdaBoost) faces significant challenges posed by label noise, especially in multiclass classification tasks. Existing methods either lack mechanisms to handle label noise effectively or suffer from high computational costs due to redundant data usage. Inspired by granular computing, this paper proposes granular adaptive boosting (GAdaBoost), a novel two-stage framework comprising a data granulation stage and an adaptive boosting stage, to enhance efficiency and robustness under noisy conditions. To validate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is proposed. Specifically, first, a granular-ball generation method is designed to compress data while preserving diversity and mitigating label noise. Second, the granular ball-based SAMME algorithm focuses on granular balls rather than individual samples, improving efficiency and reducing sensitivity to noise. Experimental results on some noisy datasets show that the proposed approach achieves superior robustness and efficiency compared with existing methods, demonstrating that this work effectively extends AdaBoost and SAMME.

</details>


### [449] [SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/pdf/2506.06499)
*Alex Havrilla, Edward Hughes, Mikayel Samvelyan, Jacob Abernethy*

Main category: cs.LG

TL;DR: SPARQ introduces a method for generating diverse and high-quality synthetic math problems using a single model, improving model performance by up to 24%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for synthetic data generation are limited in scalability and diversity, restricting their application to complex problem domains.

Method: SPARQ uses quality-diversity algorithms to generate synthetic math problems, measuring solve-rate as a proxy for difficulty. Starting from 7.5K seed samples, it produces 20M problem-solution pairs.

Result: Filtering by difficulty improves model performance by 24%. Higher quality aids in-distribution performance, while diversity enhances OOD generalization. Scaling laws for synthetic data benefit downstream tasks.

Conclusion: SPARQ demonstrates the effectiveness of synthetic data generation for improving model reasoning, with quality and diversity playing key roles in generalization.

Abstract: Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24\%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization.

</details>


### [450] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/pdf/2506.08673)
*Diptarka Chakraborty, Kushagra Chatterjee, Debarati Das, Tien Long Nguyen, Romina Nobahari*

Main category: cs.LG

TL;DR: The paper introduces fair consensus clustering, ensuring proportional representation of protected groups in clusters, and provides approximation algorithms for fair clustering modifications.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fair representation in consensus clustering by incorporating fairness constraints inspired by the disparate impact doctrine.

Method: Develops optimal and near-linear time approximation algorithms for modifying existing clusterings to enforce fairness, focusing on equal and unequal group sizes.

Result: Provides constant-factor approximation algorithms and proves NP-hardness for unequal-sized groups, demonstrating practical feasibility.

Conclusion: The work advances fair clustering with approximation guarantees, potentially impacting other clustering problems lacking fairness solutions.

Abstract: Consensus clustering, a fundamental task in machine learning and data analysis, aims to aggregate multiple input clusterings of a dataset, potentially based on different non-sensitive attributes, into a single clustering that best represents the collective structure of the data. In this work, we study this fundamental problem through the lens of fair clustering, as introduced by Chierichetti et al. [NeurIPS'17], which incorporates the disparate impact doctrine to ensure proportional representation of each protected group in the dataset within every cluster. Our objective is to find a consensus clustering that is not only representative but also fair with respect to specific protected attributes. To the best of our knowledge, we are the first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing clustering to enforce fairness -- an essential postprocessing step in many clustering applications that require fair representation. We develop an optimal algorithm for datasets with equal group representation and near-linear time constant factor approximation algorithms for more general scenarios with different proportions of two group sizes. We complement our approximation result by showing that the problem is NP-hard for two unequal-sized groups. Given the fundamental nature of this problem, we believe our results on Closest Fair Clustering could have broader implications for other clustering problems, particularly those for which no prior approximation guarantees exist for their fair variants.

</details>


### [451] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/pdf/2506.09316)
*Yeonju Ro, Zhenyu Zhang, Souvik Kundu, Zhangyang Wang, Aditya Akella*

Main category: cs.LG

TL;DR: DSLA-Serve combines dual-state linear attention (DSLA) with adaptive distillation to improve efficiency and accuracy in LLMs, outperforming Llama2-7B and Zamba-7B in speed while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the prohibitive compute and memory costs of LLMs on lengthy inputs and the accuracy degradation in sub-quadratic methods like linear attention.

Method: Propose DSLA with two hidden states for historical context and recency, and DSLA-Serve, an adaptive distillation framework for dynamic workload adaptation.

Result: DSLA-Serve achieves 2.3x faster inference than Llama2-7B and 3.0x faster than Zamba-7B, with comparable task performance.

Conclusion: DSLA-Serve effectively balances efficiency and accuracy, addressing historical-token underrepresentation in linear attentions.

Abstract: Large language models (LLMs) excel at capturing global token dependencies via self-attention but face prohibitive compute and memory costs on lengthy inputs. While sub-quadratic methods (e.g., linear attention) can reduce these costs, they often degrade accuracy due to overemphasizing recent tokens. In this work, we first propose dual-state linear attention (DSLA), a novel design that maintains two specialized hidden states-one for preserving historical context and one for tracking recency-thereby mitigating the short-range bias typical of linear-attention architectures. To further balance efficiency and accuracy under dynamic workload conditions, we introduce DSLA-Serve, an online adaptive distillation framework that progressively replaces Transformer layers with DSLA layers at inference time, guided by a sensitivity-based layer ordering. DSLA-Serve uses a chained fine-tuning strategy to ensure that each newly converted DSLA layer remains consistent with previously replaced layers, preserving the overall quality. Extensive evaluations on commonsense reasoning, long-context QA, and text summarization demonstrate that DSLA-Serve yields 2.3x faster inference than Llama2-7B and 3.0x faster than the hybrid Zamba-7B, while retaining comparable performance across downstream tasks. Our ablation studies show that DSLA's dual states capture both global and local dependencies, addressing the historical-token underrepresentation seen in prior linear attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [452] [Adaptive Composition of Machine Learning as a Service (MLaaS) for IoT Environments](https://arxiv.org/pdf/2506.11054)
*Deepak Kanneganti, Sajib Mistry, Sheik Mohammad Mostakim Fattah, Aneesh Krishna, Monowar Bhuyan*

Main category: cs.LG

TL;DR: Proposes an adaptive MLaaS framework for IoT to handle data and system variability, ensuring efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges like concept drift, data heterogeneity, and evolving system requirements in IoT environments.

Method: Integrates service assessment and candidate selection models, using a contextual multi-armed bandit strategy for incremental updates.

Result: Maintains QoS while reducing computational costs, validated on a real-world dataset.

Conclusion: The framework effectively adapts MLaaS compositions to dynamic IoT constraints.

Abstract: The dynamic nature of Internet of Things (IoT) environments challenges the long-term effectiveness of Machine Learning as a Service (MLaaS) compositions. The uncertainty and variability of IoT environments lead to fluctuations in data distribution, e.g., concept drift and data heterogeneity, and evolving system requirements, e.g., scalability demands and resource limitations. This paper proposes an adaptive MLaaS composition framework to ensure a seamless, efficient, and scalable MLaaS composition. The framework integrates a service assessment model to identify underperforming MLaaS services and a candidate selection model to filter optimal replacements. An adaptive composition mechanism is developed that incrementally updates MLaaS compositions using a contextual multi-armed bandit optimization strategy. By continuously adapting to evolving IoT constraints, the approach maintains Quality of Service (QoS) while reducing the computational cost associated with recomposition from scratch. Experimental results on a real-world dataset demonstrate the efficiency of our proposed approach.

</details>


### [453] [Mini-Game Lifetime Value Prediction in WeChat](https://arxiv.org/pdf/2506.11037)
*Aochuan Chen, Yifan Niu, Ziqi Gao, Yujie Sun, Shoujun Liu, Gong Chen, Yang Liu, Jia Li*

Main category: cs.LG

TL;DR: The paper introduces GRePO-LTV, a framework combining graph representation learning and Pareto-optimization to improve LTV prediction in sparse, interdependent advertising data.


<details>
  <summary>Details</summary>
Motivation: Accurate LTV prediction is crucial for aligning ads with user interests, but sparse data and task interdependencies complicate the problem.

Method: Uses graph representation learning to tackle data scarcity and Pareto-optimization to manage task interdependencies.

Result: Proposes GRePO-LTV as a solution to improve LTV prediction accuracy in challenging advertising scenarios.

Conclusion: GRePO-LTV effectively addresses data scarcity and task interdependence, enhancing LTV prediction for advertisers.

Abstract: The LifeTime Value (LTV) prediction, which endeavors to forecast the cumulative purchase contribution of a user to a particular item, remains a vital challenge that advertisers are keen to resolve. A precise LTV prediction system enhances the alignment of user interests with meticulously designed advertisements, thereby generating substantial profits for advertisers. Nonetheless, this issue is complicated by the paucity of data typically observed in real-world advertising scenarios. The purchase rate among registered users is often as critically low as 0.1%, resulting in a dataset where the majority of users make only several purchases. Consequently, there is insufficient supervisory signal for effectively training the LTV prediction model. An additional challenge emerges from the interdependencies among tasks with high correlation. It is a common practice to estimate a user's contribution to a game over a specified temporal interval. Varying the lengths of these intervals corresponds to distinct predictive tasks, which are highly correlated. For instance, predictions over a 7-day period are heavily reliant on forecasts made over a 3-day period, where exceptional cases can adversely affect the accuracy of both tasks. In order to comprehensively address the aforementioned challenges, we introduce an innovative framework denoted as Graph-Represented Pareto-Optimal LifeTime Value prediction (GRePO-LTV). Graph representation learning is initially employed to address the issue of data scarcity. Subsequently, Pareto-Optimization is utilized to manage the interdependence of prediction tasks.

</details>


### [454] [Improving Group Robustness on Spurious Correlation via Evidential Alignment](https://arxiv.org/pdf/2506.11347)
*Wenqian Ye, Guangtao Zheng, Aidong Zhang*

Main category: cs.LG

TL;DR: The paper proposes Evidential Alignment, a framework using uncertainty quantification to identify and suppress spurious correlations in deep neural networks without needing group annotations.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often rely on spurious correlations, harming generalization and robustness. Existing methods require costly annotations or deterministic models, which may not capture all biases.

Method: Evidential Alignment uses second-order risk minimization and evidential calibration to quantify model uncertainty and suppress spurious correlations while preserving core features.

Result: Empirical results show improved group robustness across architectures and data modalities, validating the method's effectiveness.

Conclusion: Evidential Alignment offers a scalable, principled solution to spurious correlations without requiring annotations, enhancing model trustworthiness.

Abstract: Deep neural networks often learn and rely on spurious correlations, i.e., superficial associations between non-causal features and the targets. For instance, an image classifier may identify camels based on the desert backgrounds. While it can yield high overall accuracy during training, it degrades generalization on more diverse scenarios where such correlations do not hold. This problem poses significant challenges for out-of-distribution robustness and trustworthiness. Existing methods typically mitigate this issue by using external group annotations or auxiliary deterministic models to learn unbiased representations. However, such information is costly to obtain, and deterministic models may fail to capture the full spectrum of biases learned by the models. To address these limitations, we propose Evidential Alignment, a novel framework that leverages uncertainty quantification to understand the behavior of the biased models without requiring group annotations. By quantifying the evidence of model prediction with second-order risk minimization and calibrating the biased models with the proposed evidential calibration technique, Evidential Alignment identifies and suppresses spurious correlations while preserving core features. We theoretically justify the effectiveness of our method as capable of learning the patterns of biased models and debiasing the model without requiring any spurious correlation annotations. Empirical results demonstrate that our method significantly improves group robustness across diverse architectures and data modalities, providing a scalable and principled solution to spurious correlations.

</details>


### [455] [Delving into Instance-Dependent Label Noise in Graph Data: A Comprehensive Study and Benchmark](https://arxiv.org/pdf/2506.12468)
*Suyeon Kim, SeongKu Kang, Dongwoo Kim, Jungseul Ok, Hwanjo Yu*

Main category: cs.LG

TL;DR: BeGIN is a benchmark for evaluating GNNs under instance-dependent label noise, offering realistic datasets and comprehensive analysis of noise-handling strategies.


<details>
  <summary>Details</summary>
Motivation: Existing studies on graph learning with label noise overlook instance-dependent noise, limiting their real-world applicability.

Method: BeGIN introduces algorithmic and LLM-based simulations for instance-dependent noise and evaluates noise-handling strategies across GNN architectures.

Result: Experiments highlight the challenges of instance-dependent noise, especially LLM-based corruption, and the need for node-specific parameterization.

Conclusion: BeGIN serves as a valuable resource for advancing research on label noise in graphs and improving GNN robustness.

Abstract: Graph Neural Networks (GNNs) have achieved state-of-the-art performance in node classification tasks but struggle with label noise in real-world data. Existing studies on graph learning with label noise commonly rely on class-dependent label noise, overlooking the complexities of instance-dependent noise and falling short of capturing real-world corruption patterns. We introduce BeGIN (Benchmarking for Graphs with Instance-dependent Noise), a new benchmark that provides realistic graph datasets with various noise types and comprehensively evaluates noise-handling strategies across GNN architectures, noisy label detection, and noise-robust learning. To simulate instance-dependent corruptions, BeGIN introduces algorithmic methods and LLM-based simulations. Our experiments reveal the challenges of instance-dependent noise, particularly LLM-based corruption, and underscore the importance of node-specific parameterization to enhance GNN robustness. By comprehensively evaluating noise-handling strategies, BeGIN provides insights into their effectiveness, efficiency, and key performance factors. We expect that BeGIN will serve as a valuable resource for advancing research on label noise in graphs and fostering the development of robust GNN training methods. The code is available at https://github.com/kimsu55/BeGIN.

</details>


### [456] [No-Regret Learning Under Adversarial Resource Constraints: A Spending Plan Is All You Need!](https://arxiv.org/pdf/2506.13244)
*Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, Christian Kroer*

Main category: cs.LG

TL;DR: The paper addresses online decision-making under resource constraints with adversarially changing reward and cost distributions. It proposes primal-dual methods guided by a spending plan to achieve sublinear regret, with robustness for imbalanced plans.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to achieve sublinear regret when reward and cost distributions change adversarially. The paper aims to overcome this by leveraging a spending plan to guide resource allocation.

Method: The authors design primal-dual methods for two settings: online resource allocation (pre-action observations) and online learning with resource constraints (post-action observations, full/bandit feedback).

Result: The algorithms achieve sublinear regret against baselines following the spending plan, with improved performance for well-balanced budgets. A robust variant handles imbalanced plans.

Conclusion: The framework effectively addresses adversarial changes, with performance tied to the spending plan's balance. Future work could explore benchmarks deviating from the plan.

Abstract: We study online decision making problems under resource constraints, where both reward and cost functions are drawn from distributions that may change adversarially over time. We focus on two canonical settings: $(i)$ online resource allocation where rewards and costs are observed before action selection, and $(ii)$ online learning with resource constraints where they are observed after action selection, under full feedback or bandit feedback. It is well known that achieving sublinear regret in these settings is impossible when reward and cost distributions may change arbitrarily over time. To address this challenge, we analyze a framework in which the learner is guided by a spending plan--a sequence prescribing expected resource usage across rounds. We design general (primal-)dual methods that achieve sublinear regret with respect to baselines that follow the spending plan. Crucially, the performance of our algorithms improves when the spending plan ensures a well-balanced distribution of the budget across rounds. We additionally provide a robust variant of our methods to handle worst-case scenarios where the spending plan is highly imbalanced. To conclude, we study the regret of our algorithms when competing against benchmarks that deviate from the prescribed spending plan.

</details>


### [457] [A Production Scheduling Framework for Reinforcement Learning Under Real-World Constraints](https://arxiv.org/pdf/2506.13566)
*Jonathan Hoss, Felix Schelling, Noah Klarmann*

Main category: cs.LG

TL;DR: A modular RL framework, JobShopLab, is proposed to address real-world complexities in JSSP, supporting multi-objective optimization and standardized RL training.


<details>
  <summary>Details</summary>
Motivation: Traditional JSSP approaches are ineffective in real-world production due to added complexities like transport logistics and machine breakdowns. RL offers adaptive solutions but lacks general-purpose frameworks.

Method: The framework extends classical JSSP by incorporating real-world constraints (e.g., transport logistics, stochastic conditions) and provides a customizable, modular solution with a standardized RL interface.

Result: JobShopLab enables flexible problem definition, simulation configuration, and standardized RL training, adaptable to diverse production scenarios.

Conclusion: The open-source JobShopLab framework bridges the gap in RL-based JSSP solutions, supporting research and industrial applications under dynamic conditions.

Abstract: The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing makespan under deterministic constraints. Real-world production environments introduce additional complexities that cause traditional scheduling approaches to be less effective. Reinforcement learning (RL) holds potential in addressing these challenges, as it allows agents to learn adaptive scheduling strategies. However, there is a lack of a comprehensive, general-purpose frameworks for effectively training and evaluating RL agents under real-world constraints. To address this gap, we propose a modular framework that extends classical JSSP formulations by incorporating key real-world constraints inherent to the shopfloor, including transport logistics, buffer management, machine breakdowns, setup times, and stochastic processing conditions, while also supporting multi-objective optimization. The framework is a customizable solution that offers flexibility in defining problem instances and configuring simulation parameters, enabling adaptation to diverse production scenarios. A standardized interface ensures compatibility with various RL approaches, providing a robust environment for training RL agents and facilitating the standardized comparison of different scheduling methods under dynamic and uncertain conditions. We release JobShopLab as an open-source tool for both research and industrial applications, accessible at: https://github.com/proto-lab-ro/jobshoplab

</details>


### [458] [Graph-Convolutional-Beta-VAE for Synthetic Abdominal Aorta Aneurysm Generation](https://arxiv.org/pdf/2506.13628)
*Francesco Fabbri, Martino Andrea Scarpolini, Angelo Iollo, Francesco Viola, Francesco Tudisco*

Main category: cs.LG

TL;DR: A beta-VAE-GCN framework generates synthetic AAA data, addressing privacy and scalability in medical research.


<details>
  <summary>Details</summary>
Motivation: To overcome privacy concerns and data scarcity in medical research by creating realistic synthetic AAA datasets.

Method: Uses a beta-VAE-GCN to extract anatomical features and employs Procrustes-based augmentation for data diversity.

Result: Outperforms PCA-based methods in capturing nonlinear variations, enhancing clinical and statistical analyses.

Conclusion: The synthetic AAA dataset ensures privacy and scalability for research, device testing, and modeling.

Abstract: Synthetic data generation plays a crucial role in medical research by mitigating privacy concerns and enabling large-scale patient data analysis. This study presents a beta-Variational Autoencoder Graph Convolutional Neural Network framework for generating synthetic Abdominal Aorta Aneurysms (AAA). Using a small real-world dataset, our approach extracts key anatomical features and captures complex statistical relationships within a compact disentangled latent space. To address data limitations, low-impact data augmentation based on Procrustes analysis was employed, preserving anatomical integrity. The generation strategies, both deterministic and stochastic, manage to enhance data diversity while ensuring realism. Compared to PCA-based approaches, our model performs more robustly on unseen data by capturing complex, nonlinear anatomical variations. This enables more comprehensive clinical and statistical analyses than the original dataset alone. The resulting synthetic AAA dataset preserves patient privacy while providing a scalable foundation for medical research, device testing, and computational modeling.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [459] [AgentFacts: Universal KYA Standard for Verified AI Agent Metadata & Deployment](https://arxiv.org/pdf/2506.13794)
*Jared James Grogan*

Main category: cs.MA

TL;DR: AgentFacts is a universal metadata standard for verifying AI agent capabilities, enabling trust and transparency in enterprise AI adoption.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of standardized metadata and verification infrastructure for third-party AI agents, which creates trust gaps and coordination friction.

Method: Introduces AgentFacts, featuring cryptographically-signed capability declarations, multi-authority validation, and dynamic permission management.

Result: Transforms agent procurement into standardized workforce management, ensuring transparency and governance for enterprise AI coordination.

Conclusion: AgentFacts provides a scalable solution for enterprise AI trust and adoption by standardizing agent verification and governance.

Abstract: Enterprise AI deployment faces critical "Know Your Agent" (KYA) challenges where organizations must verify third-party agent capabilities and establish trust without standardized metadata or verification infrastructure. Current approaches rely on self-declared capabilities and custom integration processes that create trust gaps and coordination friction limiting confident enterprise adoption. This paper presents AgentFacts, a universal metadata standard that enables systematic agent verification through cryptographically-signed capability declarations, multi-authority validation, and dynamic permission management. The specification introduces domain-specialized verification where different trusted authorities validate specific metadata aspects based on their expertise, eliminating single points of trust failure while enabling graduated confidence assessment. AgentFacts transforms agent procurement from custom integration projects into standardized workforce management, providing the transparency and governance infrastructure necessary for enterprise AI coordination at scale.

</details>


### [460] [Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study](https://arxiv.org/pdf/2506.13811)
*Sompote Youwai, David Phim, Vianne Gayl Murcia, Rianne Clair Onas*

Main category: cs.MA

TL;DR: Router-based multi-agent systems outperform single-agent and conventional workflows in automating foundation design, achieving high accuracy while requiring human oversight.


<details>
  <summary>Details</summary>
Motivation: To automate foundation design calculations efficiently while maintaining professional standards and safety in civil engineering.

Method: Evaluated three approaches: single-agent, multi-agent designer-checker, and router-based expert selection, using models like DeepSeek R1 and ChatGPT 4 Turbo.

Result: Router-based system scored 95.00% (shallow) and 90.63% (pile), outperforming others by 10.0-43.75 points. Grok 3 excelled in standalone tasks.

Conclusion: Router-based systems are optimal for automation but require human oversight, serving as advanced tools, not replacements, in professional practice.

Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.

</details>


### [461] [Hierarchical Multi-Agent Reinforcement Learning-based Coordinated Spatial Reuse for Next Generation WLANs](https://arxiv.org/pdf/2506.14187)
*Jiaming Yu, Le Liang, Hao Ye, Shi Jin*

Main category: cs.MA

TL;DR: The paper proposes a hierarchical multi-agent reinforcement learning (HMARL) approach to improve downlink spatial reuse in Wi-Fi networks, outperforming baselines in throughput and latency.


<details>
  <summary>Details</summary>
Motivation: High-density Wi-Fi deployments suffer from co-channel interference, degrading performance. Coordinated spatial reuse (CSR) is needed for better efficiency.

Method: The CSR process is split into polling and decision phases. HMARL uses a hierarchical structure with high-level (station selection) and low-level (power control) policy networks.

Result: Simulations show HMARL outperforms baselines in throughput and latency, works well with legacy APs, and improves fairness in high-interference regions.

Conclusion: HMARL effectively addresses CSR challenges in Wi-Fi networks, offering robust performance and fairness.

Abstract: High-density Wi-Fi deployments often result in significant co-channel interference, which degrades overall network performance. To address this issue, coordination of multi access points (APs) has been considered to enable coordinated spatial reuse (CSR) in next generation wireless local area networks. This paper tackles the challenge of downlink spatial reuse in Wi-Fi networks, specifically in scenarios involving overlapping basic service sets, by employing hierarchical multi-agent reinforcement learning (HMARL). We decompose the CSR process into two phases, i.e., a polling phase and a decision phase, and introduce the HMARL algorithm to enable efficient CSR. To enhance training efficiency, the proposed HMARL algorithm employs a hierarchical structure, where station selection and power control are determined by a high- and low-level policy network, respectively. Simulation results demonstrate that this approach consistently outperforms baseline methods in terms of throughput and latency across various network topologies. Moreover, the algorithm exhibits robust performance when coexisting with legacy APs. Additional experiments in a representative topology further reveal that the carefully designed reward function not only maximizes the overall network throughput, but also improves fairness in transmission opportunities for APs in high-interference regions.

</details>


### [462] [Towards the Autonomous Optimization of Urban Logistics: Training Generative AI with Scientific Tools via Agentic Digital Twins and Model Context Protocol](https://arxiv.org/pdf/2506.13068)
*Haowen Xu, Yulin Sun, Jose Tupayachi, Olufemi Omitaomu, Sisi Zlatanova, Xueping Li*

Main category: cs.MA

TL;DR: The paper introduces an agentic system using MCP for autonomous, simulation-informed urban logistics optimization, integrating AI agents and domain-specific tools like Gurobi and AnyLogic.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for urban freight logistics are inefficient and lack scalability, prompting the need for an autonomous, collaborative system.

Method: The system employs MCP to orchestrate multi-agent collaboration, combining generative AI with tools like Gurobi and AnyLogic for a generative digital twin.

Result: The framework enhances decision-making, transforming digital twins into autonomous systems, as shown in a freight decarbonization case study.

Conclusion: The system advances urban operations research by enabling intelligent, dynamic decision-making in transportation and smart city management.

Abstract: Optimizing urban freight logistics is critical for developing sustainable, low-carbon cities. Traditional methods often rely on manual coordination of simulation tools, optimization solvers, and expert-driven workflows, limiting their efficiency and scalability. This paper presents an agentic system architecture that leverages the model context protocol (MCP) to orchestrate multi-agent collaboration among scientific tools for autonomous, simulation-informed optimization in urban logistics. The system integrates generative AI agents with domain-specific engines - such as Gurobi for optimization and AnyLogic for agent-based simulation - forming a generative digital twin capable of reasoning, planning, and acting across multimodal freight networks. By incorporating integrated chatbots, retrieval-augmented generation, and structured memory, the framework enables agents to interpret user intent from natural language conversations, retrieve relevant datasets and models, coordinate solvers and simulators, and execute complex workflows. We demonstrate this approach through a freight decarbonization case study, showcasing how MCP enables modular, interoperable, and adaptive agent behavior across diverse toolchains. The results reveal that our system transforms digital twins from static visualizations into autonomous, decision-capable systems, advancing the frontiers of urban operations research. By enabling context-aware, generative agents to operate scientific tools automatically and collaboratively, this framework supports more intelligent, accessible, and dynamic decision-making in transportation planning and smart city management.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [463] [NeRF-QA: Neural Radiance Fields Quality Assessment Database](https://arxiv.org/pdf/2305.03176)
*Pedro Martin, António Rodrigues, João Ascenso, Maria Paula Queluz*

Main category: cs.MM

TL;DR: A new database, NeRF-QA, is introduced with 48 videos from seven NeRF methods, including subjective quality scores, to evaluate and develop quality metrics for NeRF-synthesized views.


<details>
  <summary>Details</summary>
Motivation: To assess and improve objective quality metrics for NeRF-based synthesized views, as existing metrics may not be suitable.

Method: Created a database (NeRF-QA) with 48 videos from seven NeRF methods, including subjective quality scores from tests. Used real and synthetic 360-degree scenes for video selection.

Result: The database provides a foundation for evaluating existing metrics and developing new ones tailored to NeRF-synthesized content.

Conclusion: NeRF-QA enables better assessment and development of quality metrics for NeRF-based synthesized views, addressing a gap in current evaluation methods.

Abstract: This short paper proposes a new database - NeRF-QA - containing 48 videos synthesized with seven NeRF based methods, along with their perceived quality scores, resulting from subjective assessment tests; for the videos selection, both real and synthetic, 360 degrees scenes were considered. This database will allow to evaluate the suitability, to NeRF based synthesized views, of existing objective quality metrics and also the development of new quality metrics, specific for this case.

</details>


### [464] [GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting View Synthesis](https://arxiv.org/pdf/2502.13196)
*Pedro Martin, António Rodrigues, João Ascenso, Maria Paula Queluz*

Main category: cs.MM

TL;DR: The paper conducts a subjective quality assessment of Gaussian Splatting (GS) for 3D scene rendering, comparing it to Neural Radiance Fields (NeRF), and evaluates 18 objective quality metrics against human perception.


<details>
  <summary>Details</summary>
Motivation: To address the lack of in-depth quality assessment for GS-generated static content and evaluate its performance against NeRF in terms of rendering speed and memory efficiency.

Method: A subjective study was conducted using synthesized videos from state-of-the-art GS methods applied to diverse scenes, including 360-degree and forward-facing camera trajectories. Objective metrics were analyzed against subjective scores.

Result: The study provides insights into the strengths and limitations of GS and evaluates the alignment of 18 objective metrics with human perception. A comprehensive database of videos and scores is made available for benchmarking.

Conclusion: The paper fills a gap in GS quality assessment, offering a benchmark for future research and highlighting the effectiveness of GS for real-time rendering compared to NeRF.

Abstract: Gaussian Splatting (GS) offers a promising alternative to Neural Radiance Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to represent complex geometry and appearance, GS achieves faster rendering times and reduced memory consumption compared to the neural network approach used in NeRF. However, quality assessment of GS-generated static content is not yet explored in-depth. This paper describes a subjective quality assessment study that aims to evaluate synthesized videos obtained with several static GS state-of-the-art methods. The methods were applied to diverse visual scenes, covering both 360-degree and forward-facing (FF) camera trajectories. Moreover, the performance of 18 objective quality metrics was analyzed using the scores resulting from the subjective study, providing insights into their strengths, limitations, and alignment with human perception. All videos and scores are made available providing a comprehensive database that can be used as benchmark on GS view synthesis and objective quality metrics.

</details>


### [465] [Immersive Fantasy Based on Digital Nostalgia: Environmental Narratives for the Korean Millennials and Gen Z](https://arxiv.org/pdf/2506.10013)
*Yerin Doh, Joonhyung Bae*

Main category: cs.MM

TL;DR: The paper introduces an artwork addressing mask waste during COVID-19, blending digital nostalgia and travel memories into a game and exhibition to explore ecological and ethical issues.


<details>
  <summary>Details</summary>
Motivation: To highlight the ecological impact of single-use mask waste during the pandemic and engage younger generations through relatable narratives.

Method: Uses a point-and-click game and immersive exhibition to merge virtual and real experiences, prompting ethical and environmental reflection.

Result: The artwork fosters empathy and potential action but faces challenges in resource use and sustained engagement.

Conclusion: The project effectively raises awareness but needs improvements in sustainability and long-term participant involvement.

Abstract: This study introduces the media artwork Dear Passenger, Please Wear a Mask, designed to offer a layered exploration of single-use mask waste, which escalated during the COVID-19 pandemic. The piece reframes underappreciated ecological concerns by interweaving digital nostalgia and airline travel recollections of Millennials and Gen Z with a unique fantasy narrative. Via a point-and-click game and an immersive exhibition, participants traverse both virtual and real domains, facing ethical and environmental dilemmas. While it fosters empathy and potential action, resource use and post-experience engagement challenges persist.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [466] [Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience](https://arxiv.org/pdf/2506.13971)
*Andrew Chang, Chenkai Hu, Ji Qi, Zhuojian Wei, Kexin Zhang, Viswadruth Akkaraju, David Poeppel, Dustin Freeman*

Main category: eess.AS

TL;DR: Semi-supervised learning (SSL) outperforms supervised learning (SL) in predicting negative moments in videoconferences, achieving high performance with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Negative moments in group videoconferences (e.g., loss of fluidity or enjoyment) are understudied and costly to annotate for supervised learning.

Method: Used SSL to train multimodal (audio, facial, text) deep features, combining labeled and unlabeled data for co-training.

Result: SSL achieved ROC-AUC of 0.9 and F1 of 0.6, outperforming SL by 4% with the same labeled data. With just 8% labeled data, SSL matched 96% of SL's full-data performance.

Conclusion: SSL provides an annotation-efficient framework for modeling videoconference experiences, reducing the need for costly manual labeling.

Abstract: Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.

</details>


### [467] [M3SD: Multi-modal, Multi-scenario and Multi-language Speaker Diarization Dataset](https://arxiv.org/pdf/2506.14427)
*Shilong Wu, Hang Chen, Jun Du*

Main category: eess.AS

TL;DR: Proposed automated dataset construction and scenario-related fine-tuning to improve speaker diarization, addressing data scarcity and poor model generalization.


<details>
  <summary>Details</summary>
Motivation: Address insufficient data resources and poor generalization in speaker diarization.

Method: Automated dataset construction using audio-video fusion for pseudo-labels, followed by scenario-related fine-tuning with Adapter and LoRA.

Result: Released M3SD dataset and achieved domain adaptation via targeted optimization.

Conclusion: Open-sourced dataset and code, enhancing speaker diarization with diverse data and adaptive fine-tuning.

Abstract: In the field of speaker diarization, the development of technology is constrained by two problems: insufficient data resources and poor generalization ability of deep learning models. To address these two problems, firstly, we propose an automated method for constructing speaker diarization datasets, which generates more accurate pseudo-labels for massive data through the combination of audio and video. Relying on this method, we have released Multi-modal, Multi-scenario and Multi-language Speaker Diarization (M3SD) datasets. This dataset is derived from real network videos and is highly diverse. In addition, we further propose a scenario-related model fine-tuning strategy. Based on the general model pre-trained using the above dataset, we combine the specific data of the target scenario (e.g., meetings) and achieve targeted optimization by using Adapter and LoRA joint fine-tuning, thus achieving the model's domain adaptation. Our dataset and code have been open-sourced at https://huggingface.co/spaces/OldDragon/m3sd.

</details>


### [468] [Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios](https://arxiv.org/pdf/2506.14204)
*Aswin Shanmugam Subramanian, Amit Das, Naoyuki Kanda, Jinyu Li, Xiaofei Wang, Yifan Gong*

Main category: eess.AS

TL;DR: Extended SOT framework for ASR, balancing latency and accuracy with CSS, dual models, and segSOT.


<details>
  <summary>Details</summary>
Motivation: Address practical needs of streaming and offline ASR, focusing on real-time captioning and summarization.

Method: 1. CSS single-channel front-end with E2E systems. 2. Dual models (Conformer Transducer and Sequence-to-Sequence) or two-pass cascaded encoders. 3. segSOT for offline scenarios.

Result: Improved accuracy in overlapping speech scenarios and better readability for multi-talker transcriptions.

Conclusion: Proposed enhancements effectively balance latency and accuracy, catering to diverse ASR applications.

Abstract: We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions.

</details>


### [469] [ASAP-FE: Energy-Efficient Feature Extraction Enabling Multi-Channel Keyword Spotting on Edge Processors](https://arxiv.org/pdf/2506.14657)
*Jongin Choi, Jina Park, Woojoo Lee, Jae-Jin Lee, Massoud Pedram*

Main category: eess.AS

TL;DR: ASAP-FE is a hardware-oriented front-end for multi-channel keyword spotting, reducing workload by 62.73% with minimal accuracy drop, optimized for edge devices.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational and energy demands of multi-channel KWS in edge environments.

Method: Incorporates half-overlapped IIR framing, sparsity-aware data reduction, and dynamic parallel processing.

Result: Reduces average workload by 62.73%, supports real-time processing for up to 32 channels, and maintains accuracy within 1% drop.

Conclusion: ASAP-FE provides an efficient, practical solution for energy-constrained edge devices.

Abstract: Multi-channel keyword spotting (KWS) has become crucial for voice-based applications in edge environments. However, its substantial computational and energy requirements pose significant challenges. We introduce ASAP-FE (Agile Sparsity-Aware Parallelized-Feature Extractor), a hardware-oriented front-end designed to address these challenges. Our framework incorporates three key innovations: (1) Half-overlapped Infinite Impulse Response (IIR) Framing: This reduces redundant data by approximately 25% while maintaining essential phoneme transition cues. (2) Sparsity-aware Data Reduction: We exploit frame-level sparsity to achieve an additional 50% data reduction by combining frame skipping with stride-based filtering. (3) Dynamic Parallel Processing: We introduce a parameterizable filter cluster and a priority-based scheduling algorithm that allows parallel execution of IIR filtering tasks, reducing latency and optimizing energy efficiency. ASAP-FE is implemented with various filter cluster sizes on edge processors, with functionality verified on FPGA prototypes and designs synthesized at 45 nm. Experimental results using TC-ResNet8, DS-CNN, and KWT-1 demonstrate that ASAP-FE reduces the average workload by 62.73% while supporting real-time processing for up to 32 channels. Compared to a conventional fully overlapped baseline, ASAP-FE achieves less than a 1% accuracy drop (e.g., 96.22% vs. 97.13% for DS-CNN), which is well within acceptable limits for edge AI. By adjusting the number of filter modules, our design optimizes the trade-off between performance and energy, with 15 parallel filters providing optimal performance for up to 25 channels. Overall, ASAP-FE offers a practical and efficient solution for multi-channel KWS on energy-constrained edge devices.

</details>


### [470] [ArrayDPS: Unsupervised Blind Speech Separation with a Diffusion Prior](https://arxiv.org/pdf/2505.05657)
*Zhongweiyang Xu, Xulin Fan, Zhong-Qiu Wang, Xilin Jiang, Romit Roy Choudhury*

Main category: eess.AS

TL;DR: ArrayDPS is an unsupervised, array-agnostic method for Blind Speech Separation (BSS) using diffusion posterior sampling (DPS) and approximating room acoustics.


<details>
  <summary>Details</summary>
Motivation: BSS is challenging due to unknown microphone array geometry, room impulse response, and speech sources. Existing methods often require supervised data or array specifics.

Method: ArrayDPS approximates the likelihood via optimization, iteratively refining room acoustics and transfer functions using diffusion priors. Only a single-speaker speech diffusion model and microphone mixtures are needed.

Result: ArrayDPS outperforms unsupervised baselines and matches supervised methods in SDR.

Conclusion: ArrayDPS provides a robust, unsupervised solution for BSS without requiring array specifics, demonstrating strong performance.

Abstract: Blind Speech Separation (BSS) aims to separate multiple speech sources from audio mixtures recorded by a microphone array. The problem is challenging because it is a blind inverse problem, i.e., the microphone array geometry, the room impulse response (RIR), and the speech sources, are all unknown. We propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic, and generative manner. The core idea builds on diffusion posterior sampling (DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must approximate the likelihood by formulating a separate optimization problem. The solution to the optimization approximates room acoustics and the relative transfer functions between microphones. These approximations, along with the diffusion priors, iterate through the ArrayDPS sampling process and ultimately yield separated voice sources. We only need a simple single-speaker speech diffusion model as a prior along with the mixtures recorded at the microphones; no microphone array information is necessary. Evaluation results show that ArrayDPS outperforms all baseline unsupervised methods while being comparable to supervised methods in terms of SDR. Audio demos are provided at: https://arraydps.github.io/ArrayDPSDemo/.

</details>


### [471] [Multi-Source Music Generation with Latent Diffusion](https://arxiv.org/pdf/2409.06190)
*Zhongweiyang Xu, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury*

Main category: eess.AS

TL;DR: MSLDM improves music generation by using VAEs to encode instrumental sources into latent representations, enhancing quality and efficiency over MSDM.


<details>
  <summary>Details</summary>
Motivation: Address MSDM's limitations in melody richness, empty sounds, and noise artifacts by leveraging latent diffusion for better source modeling.

Method: Uses VAEs to encode each instrumental source into distinct latent representations, concatenates them, and trains a diffusion model on this joint latent space.

Result: Outperforms MSDM in subjective tests and FAD scores, demonstrating better audio quality and generation efficiency.

Conclusion: Modeling individual sources with latent diffusion is more effective than direct mixture modeling, offering practical improvements for music generation.

Abstract: Most music generation models directly generate a single music mixture. To allow for more flexible and controllable generation, the Multi-Source Diffusion Model (MSDM) has been proposed to model music as a mixture of multiple instrumental sources (e.g. piano, drums, bass, and guitar). Its goal is to use one single diffusion model to generate mutually-coherent music sources, that are then mixed to form the music. Despite its capabilities, MSDM is unable to generate music with rich melodies and often generates empty sounds. Its waveform diffusion approach also introduces significant Gaussian noise artifacts that compromise audio quality. In response, we introduce a Multi-Source Latent Diffusion Model (MSLDM) that employs Variational Autoencoders (VAEs) to encode each instrumental source into a distinct latent representation. By training a VAE on all music sources, we efficiently capture each source's unique characteristics in a "source latent." The source latents are concatenated and our diffusion model learns this joint latent space. This approach significantly enhances the total and partial generation of music by leveraging the VAE's latent compression and noise-robustness. The compressed source latent also facilitates more efficient generation. Subjective listening tests and Frechet Audio Distance (FAD) scores confirm that our model outperforms MSDM, showcasing its practical and enhanced applicability in music generation systems. We also emphasize that modeling sources is more effective than direct music mixture modeling. Codes and models are available at https://github.com/XZWY/MSLDM. Demos are available at https://xzwy.github.io/MSLDMDemo/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [472] [BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis](https://arxiv.org/pdf/2506.13807)
*Florian Kofler, Marcel Rosier, Mehdi Astaraki, Ujjwal Baid, Hendrik Möller, Josef A. Buchner, Felix Steinbauer, Eva Oswald, Ezequiel de la Rosa, Ivan Ezhov, Constantin von See, Jan Kirschke, Anton Schmick, Sarthak Pati, Akis Linardos, Carla Pitarch, Sanyukta Adap, Jeffrey Rudie, Maria Correia de Verdier, Rachit Saluja, Evan Calabrese, Dominic LaBella, Mariam Aboian, Ahmed W. Moawad, Nazanin Maleki, Udunna Anazodo, Maruf Adewole, Marius George Linguraru, Anahita Fathi Kazerooni, Zhifan Jiang, Gian Marco Conte, Hongwei Li, Juan Eugenio Iglesias, Spyridon Bakas, Benedikt Wiestler, Marie Piraud, Bjoern Menze*

Main category: eess.IV

TL;DR: BraTS orchestrator is an open-source Python package that simplifies access to advanced brain tumor segmentation algorithms from the BraTS challenges, aiming to bridge the gap between research and clinical adoption.


<details>
  <summary>Details</summary>
Motivation: Despite the success of BraTS challenges, their algorithms see limited adoption in scientific and clinical settings. The goal is to make these tools more accessible.

Method: The package provides intuitive tutorials and abstracts deep learning complexities, enabling easy deployment of BraTS algorithms for inference.

Result: BraTS orchestrator democratizes access to state-of-the-art segmentation and synthesis algorithms, targeting broader neuro-radiology and neuro-oncology audiences.

Conclusion: The package accelerates the dissemination of BraTS advancements, making them readily usable for researchers and clinicians with minimal programming experience.

Abstract: The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.

</details>


### [473] [Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy](https://arxiv.org/pdf/2506.13819)
*El Arbi Belfarsi, Henry Flores, Maria Valero*

Main category: eess.IV

TL;DR: A dual-modal AI framework using SWIR spectroscopy and machine learning for non-invasive glucose monitoring, achieving high accuracy and clinical feasibility.


<details>
  <summary>Details</summary>
Motivation: To develop a cost-efficient, wearable, and clinically accurate solution for continuous non-invasive glucose monitoring.

Method: Combines SWIR imaging with CNNs for spatial features and a photodiode sensor with machine learning regressors for optical signal analysis.

Result: CNN achieved 4.82% MAPE and 100% Zone A coverage; photodiode system reached 86.4% Zone A accuracy.

Conclusion: The framework is state-of-the-art, balancing accuracy, cost, and wearability for reliable glucose monitoring.

Abstract: In this study, we present a dual-modal AI framework based on short-wave infrared (SWIR) spectroscopy. The first modality employs a multi-wavelength SWIR imaging system coupled with convolutional neural networks (CNNs) to capture spatial features linked to glucose absorption. The second modality uses a compact photodiode voltage sensor and machine learning regressors (e.g., random forest) on normalized optical signals. Both approaches were evaluated on synthetic blood phantoms and skin-mimicking materials across physiological glucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage error (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error Grid, while the photodiode system reached 86.4% Zone A accuracy. This framework constitutes a state-of-the-art solution that balances clinical accuracy, cost efficiency, and wearable integration, paving the way for reliable continuous non-invasive glucose monitoring.

</details>


### [474] [Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography](https://arxiv.org/pdf/2506.13964)
*Yusdivia Molina-Román, David Gómez-Ortiz, Ernestina Menasalvas-Ruiz, José Gerardo Tamez-Peña, Alejandro Santos-Díaz*

Main category: eess.IV

TL;DR: The study compares multimodal (BioMedCLIP) and CNN-based (ConvNeXt) methods for automated breast density classification, finding fine-tuned ConvNeXt outperforms zero-shot and linear probing approaches.


<details>
  <summary>Details</summary>
Motivation: Breast density classification is critical for cancer risk assessment but suffers from subjectivity and variability, necessitating automated solutions.

Method: Evaluated BioMedCLIP and ConvNeXt under zero-shot, linear probing, and fine-tuning scenarios using BI-RADS system.

Result: Fine-tuned ConvNeXt performed best; zero-shot was modest, and linear probing showed potential but was less effective.

Conclusion: CNN-based models with fine-tuning are superior for medical imaging, but improved textual representations and domain adaptations are needed.

Abstract: Mammographic breast density classification is essential for cancer risk assessment but remains challenging due to subjective interpretation and inter-observer variability. This study compares multimodal and CNN-based methods for automated classification using the BI-RADS system, evaluating BioMedCLIP and ConvNeXt across three learning scenarios: zero-shot classification, linear probing with textual descriptions, and fine-tuning with numerical labels. Results show that zero-shot classification achieved modest performance, while the fine-tuned ConvNeXt model outperformed the BioMedCLIP linear probe. Although linear probing demonstrated potential with pretrained embeddings, it was less effective than full fine-tuning. These findings suggest that despite the promise of multimodal learning, CNN-based models with end-to-end fine-tuning provide stronger performance for specialized medical imaging. The study underscores the need for more detailed textual representations and domain-specific adaptations in future radiology applications.

</details>


### [475] [DREAM: On hallucinations in AI-generated content for nuclear medicine imaging](https://arxiv.org/pdf/2506.13995)
*Menghua Xia, Reimund Bayerlein, Yanis Chemli, Xiaofeng Liu, Jinsong Ouyang, Georges El Fakhri, Ramsey D. Badawi, Quanzheng Li, Chi Liu*

Main category: eess.IV

TL;DR: The paper discusses hallucinations in AI-generated content for nuclear medicine imaging, proposing the DREAM report to address challenges and improve clinical safety.


<details>
  <summary>Details</summary>
Motivation: AI-generated content in nuclear medicine imaging risks hallucinations, which can mislead diagnoses and reduce trust. The paper aims to address these issues.

Method: Introduces the DREAM report, covering definitions, examples, detection metrics, causes, and mitigation strategies for hallucinations.

Result: Provides a framework to understand and tackle hallucinations in AI-generated nuclear medicine imaging.

Conclusion: The paper seeks to foster discussion and research for safer, more effective AI applications in clinical practice.

Abstract: Artificial intelligence-generated content (AIGC) has shown remarkable performance in nuclear medicine imaging (NMI), offering cost-effective software solutions for tasks such as image enhancement, motion correction, and attenuation correction. However, these advancements come with the risk of hallucinations, generating realistic yet factually incorrect content. Hallucinations can misrepresent anatomical and functional information, compromising diagnostic accuracy and clinical trust. This paper presents a comprehensive perspective of hallucination-related challenges in AIGC for NMI, introducing the DREAM report, which covers recommendations for definition, representative examples, detection and evaluation metrics, underlying causes, and mitigation strategies. This position statement paper aims to initiate a common understanding for discussions and future research toward enhancing AIGC applications in NMI, thereby supporting their safe and effective deployment in clinical practice.

</details>


### [476] [Breaking the Multi-Enhancement Bottleneck: Domain-Consistent Quality Enhancement for Compressed Images](https://arxiv.org/pdf/2506.14152)
*Qunliang Xing, Mai Xu, Jing Yang, Shengxi Li*

Main category: eess.IV

TL;DR: A novel method adapts existing quality enhancement models to maintain image quality in multi-enhancement scenarios, preventing degradation.


<details>
  <summary>Details</summary>
Motivation: Current quality enhancement methods degrade when applied repeatedly (multi-enhancement), limiting their robustness.

Method: Proposes a domain-consistent adaptation method to ensure quality preservation across multiple enhancements.

Result: Experiments show the method successfully maintains fidelity and perceptual quality in multi-enhancement.

Conclusion: The adaptation method effectively addresses degradation in multi-enhancement, enhancing existing models' robustness.

Abstract: Quality enhancement methods have been widely integrated into visual communication pipelines to mitigate artifacts in compressed images. Ideally, these quality enhancement methods should perform robustly when applied to images that have already undergone prior enhancement during transmission. We refer to this scenario as multi-enhancement, which generalizes the well-known multi-generation scenario of image compression. Unfortunately, current quality enhancement methods suffer from severe degradation when applied in multi-enhancement. To address this challenge, we propose a novel adaptation method that transforms existing quality enhancement models into domain-consistent ones. Specifically, our method enhances a low-quality compressed image into a high-quality image within the natural domain during the first enhancement, and ensures that subsequent enhancements preserve this quality without further degradation. Extensive experiments validate the effectiveness of our method and show that various existing models can be successfully adapted to maintain both fidelity and perceptual quality in multi-enhancement scenarios.

</details>


### [477] [Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT](https://arxiv.org/pdf/2506.14209)
*Pengwei Wang*

Main category: eess.IV

TL;DR: A novel unsupervised method for segmenting anomalies in ONJ imaging using a two-stage VQ-GAN pipeline, reducing manual labeling effort.


<details>
  <summary>Details</summary>
Motivation: Labeled data scarcity in ONJ imaging makes supervised training impractical, necessitating an unsupervised approach.

Method: Two-stage pipeline: VQ-GAN reconstructs normal subjects, then cube/ONJ masking trains an encoder for anomaly recovery.

Result: Successful segmentation on simulated and real patient data.

Conclusion: The method reduces manual labeling burden and can integrate with 3D printing post-processing.

Abstract: Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.
  However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.
  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.
  The proposed method achieves successful segmentation on both simulated and real patient data.
  This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.

</details>


### [478] [orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels](https://arxiv.org/pdf/2506.14303)
*Niran Nataraj, Maina Sogabe, Kenji Kawashima*

Main category: eess.IV

TL;DR: orGAN, a GAN-based system, generates high-fidelity surgical images of bleeding, addressing data scarcity and ethical concerns in medical AI.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges like limited data diversity, ethical issues, and high costs in medical imaging, especially for bleeding detection in surgery.

Method: Uses StyleGAN with Relational Positional Learning and LaMa-based inpainting to create realistic bleeding images and annotations.

Result: Achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy.

Conclusion: orGAN advances ethical, efficient, and cost-effective creation of annotated bleeding datasets, supporting AI in surgery.

Abstract: Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small "mimicking organ" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.

</details>


### [479] [BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification with Swin-HAFNet](https://arxiv.org/pdf/2506.14318)
*Amirreza Fateh, Yasin Rezvani, Sara Moayedi, Sadjad Rezvani, Fatemeh Fateh, Mansoor Fateh*

Main category: eess.IV

TL;DR: A new curated MRI dataset for brain tumor segmentation and classification is introduced, featuring 6,000 annotated scans. A transformer-based model achieves 82.3% IoU, setting benchmarks for future research.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of high-quality, balanced, and diverse datasets for brain tumor analysis in MRI scans.

Method: A transformer-based segmentation model is proposed and benchmarked against established baselines using the new dataset.

Result: The model achieves a weighted mean IoU of 82.3%, showing improvements across all tumor categories.

Conclusion: The dataset and benchmarks aim to advance machine learning in neuro-oncology, supporting research and clinical applications.

Abstract: Accurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis, largely due to the lack of high-quality, balanced, and diverse datasets. In this work, we present a new curated MRI dataset designed specifically for brain tumor segmentation and classification tasks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified radiologists and physicians, spanning three major tumor types-glioma, meningioma, and pituitary-as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we propose a transformer-based segmentation model and benchmark it against established baselines. Our method achieves the highest weighted mean Intersection-over-Union (IoU) of 82.3%, with improvements observed across all tumor categories. Importantly, this study serves primarily as an introduction to the dataset, establishing foundational benchmarks for future research. We envision this dataset as a valuable resource for advancing machine learning applications in neuro-oncology, supporting both academic research and clinical decision-support development. datasetlink: https://www.kaggle.com/datasets/briscdataset/brisc2025/

</details>


### [480] [Compressed Video Super-Resolution based on Hierarchical Encoding](https://arxiv.org/pdf/2506.14381)
*Yuxuan Jiang, Siyue Teng, Qiang Zhu, Chen Feng, Chengxi Zeng, Fan Zhang, Shuyuan Zhu, Bing Zeng, David Bull*

Main category: eess.IV

TL;DR: VSR-HE is a video super-resolution method for enhancing compressed content, upscaling low-resolution videos while eliminating compression artifacts.


<details>
  <summary>Details</summary>
Motivation: To improve perceptual quality of heavily compressed videos, addressing artifacts from H.265/HEVC encoding.

Method: Uses hierarchical encoding transformer blocks, optimized for various QP levels, trained under diverse compression settings.

Result: Effectively restores fine details and preserves visual fidelity, submitted to ICME 2025 Grand Challenge.

Conclusion: VSR-HE is a robust and generalizable solution for video super-resolution in compressed scenarios.

Abstract: This paper presents a general-purpose video super-resolution (VSR) method, dubbed VSR-HE, specifically designed to enhance the perceptual quality of compressed content. Targeting scenarios characterized by heavy compression, the method upscales low-resolution videos by a ratio of four, from 180p to 720p or from 270p to 1080p. VSR-HE adopts hierarchical encoding transformer blocks and has been sophisticatedly optimized to eliminate a wide range of compression artifacts commonly introduced by H.265/HEVC encoding across various quantization parameter (QP) levels. To ensure robustness and generalization, the model is trained and evaluated under diverse compression settings, allowing it to effectively restore fine-grained details and preserve visual fidelity. The proposed VSR-HE has been officially submitted to the ICME 2025 Grand Challenge on VSR for Video Conferencing (Team BVI-VSR), under both the Track 1 (General-Purpose Real-World Video Content) and Track 2 (Talking Head Videos).

</details>


### [481] [A large-scale heterogeneous 3D magnetic resonance brain imaging dataset for self-supervised learning](https://arxiv.org/pdf/2506.14432)
*Asbjørn Munk, Stefano Cerri, Jakob Ambsdorf, Julia Machnio, Sebastian Nørgaard Llambias, Vardan Nersesjan, Christian Hedeager Krag, Peirong Liu, Pablo Rocamora García, Mostafa Mehdipour Ghazi, Mikael Boesen, Michael Eriksen Benros, Juan Eugenio Iglesias, Mads Nielsen*

Main category: eess.IV

TL;DR: FOMO60K is a large-scale, diverse MRI dataset with 60,529 scans from 11,187 subjects, designed to advance self-supervised learning in medical imaging.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale, heterogeneous MRI datasets for developing and benchmarking self-supervised learning methods in medical imaging.

Method: Aggregated 16 publicly available sources, preserving original image characteristics with minimal preprocessing, and provided accompanying code for pretraining and finetuning.

Result: Created FOMO60K, a dataset with clinical- and research-grade images, multiple MRI sequences, and diverse anatomical/pathological variability.

Conclusion: FOMO60K aims to lower barriers for new users and support scalable self-supervised learning in medical imaging.

Abstract: We present FOMO60K, a large-scale, heterogeneous dataset of 60,529 brain Magnetic Resonance Imaging (MRI) scans from 13,900 sessions and 11,187 subjects, aggregated from 16 publicly available sources. The dataset includes both clinical- and research-grade images, multiple MRI sequences, and a wide range of anatomical and pathological variability, including scans with large brain anomalies. Minimal preprocessing was applied to preserve the original image characteristics while reducing barriers to entry for new users. Accompanying code for self-supervised pretraining and finetuning is provided. FOMO60K is intended to support the development and benchmarking of self-supervised learning methods in medical imaging at scale.

</details>


### [482] [Towards Reliable WMH Segmentation under Domain Shift: An Application Study using Maximum Entropy Regularization to Improve Uncertainty Estimation](https://arxiv.org/pdf/2506.14497)
*Franco Matzkin, Agostina Larrazabal, Diego H Milone, Jose Dolz, Enzo Ferrante*

Main category: eess.IV

TL;DR: The paper proposes maximum-entropy regularization to improve WMH segmentation under domain shifts, showing that entropy-based uncertainty can predict errors and enhance calibration.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in WMH segmentation due to domain shifts (e.g., MRI variations) to improve clinical decision-making.

Method: Uses U-Net with maximum-entropy regularization, tested on public datasets with Dice coefficient, calibration error, and entropy-based uncertainty metrics.

Result: Entropy-based uncertainty predicts segmentation errors; maximum-entropy regularization improves uncertainty-performance correlation and calibration under domain shift.

Conclusion: Maximum-entropy regularization enhances WMH segmentation reliability and calibration, aiding clinical applications.

Abstract: Accurate segmentation of white matter hyperintensities (WMH) is crucial for clinical decision-making, particularly in the context of multiple sclerosis. However, domain shifts, such as variations in MRI machine types or acquisition parameters, pose significant challenges to model calibration and uncertainty estimation. This study investigates the impact of domain shift on WMH segmentation by proposing maximum-entropy regularization techniques to enhance model calibration and uncertainty estimation, with the purpose of identifying errors post-deployment using predictive uncertainty as a proxy measure that does not require ground-truth labels. To do this, we conducted experiments using a U-Net architecture to evaluate these regularization schemes on two publicly available datasets, assessing performance with the Dice coefficient, expected calibration error, and entropy-based uncertainty estimates. Our results show that entropy-based uncertainty estimates can anticipate segmentation errors, and that maximum-entropy regularization further strengthens the correlation between uncertainty and segmentation performance while also improving model calibration under domain shift.

</details>


### [483] [Integrating Radiomics with Deep Learning Enhances Multiple Sclerosis Lesion Delineation](https://arxiv.org/pdf/2506.14524)
*Nadezhda Alsahanova, Pavel Bartenev, Maksim Sharaev, Milos Ljubisavljevic, Taleb Al. Mansoori, Yauhen Statsenko*

Main category: eess.IV

TL;DR: Combining radiomic features with imaging data improves MS lesion segmentation accuracy and model stability using deep learning architectures.


<details>
  <summary>Details</summary>
Motivation: Current deep learning approaches for MS lesion segmentation lack robustness, prompting the need for enhanced methods.

Method: Novel radiomic features (concentration rate, Rényi entropy) were fused with raw imaging data and integrated into ResNeXt-UNet and attention-augmented U-Net architectures.

Result: Radiomics-enhanced models showed higher accuracy (Dice score 0.774±0.05) and stability (reduced variability) compared to MRI-only baselines.

Conclusion: Fusing radiomics with imaging data improves segmentation performance and stability in advanced models.

Abstract: Background: Accurate lesion segmentation is critical for multiple sclerosis (MS) diagnosis, yet current deep learning approaches face robustness challenges.
  Aim: This study improves MS lesion segmentation by combining data fusion and deep learning techniques.
  Materials and Methods: We suggested novel radiomic features (concentration rate and Rényi entropy) to characterize different MS lesion types and fused these with raw imaging data. The study integrated radiomic features with imaging data through a ResNeXt-UNet architecture and attention-augmented U-Net architecture. Our approach was evaluated on scans from 46 patients (1102 slices), comparing performance before and after data fusion.
  Results: The radiomics-enhanced ResNeXt-UNet demonstrated high segmentation accuracy, achieving significant improvements in precision and sensitivity over the MRI-only baseline and a Dice score of 0.774$\pm$0.05; p<0.001 according to Bonferroni-adjusted Wilcoxon signed-rank tests. The radiomics-enhanced attention-augmented U-Net model showed a greater model stability evidenced by reduced performance variability (SDD = 0.18 $\pm$ 0.09 vs. 0.21 $\pm$ 0.06; p=0.03) and smoother validation curves with radiomics integration.
  Conclusion: These results validate our hypothesis that fusing radiomics with raw imaging data boosts segmentation performance and stability in state-of-the-art models.

</details>


### [484] [Optimization-Based Image Restoration under Implementation Constraints in Optical Analog Circuits](https://arxiv.org/pdf/2506.14624)
*Taisei Kato, Ryo Hayakawa, Soma Furusawa, Kazunori Hayashi, Youji Iiguni*

Main category: eess.IV

TL;DR: The paper explores implementing image restoration algorithms on optical analog circuits using ADMM and PDS, addressing challenges like dynamic division and noise.


<details>
  <summary>Details</summary>
Motivation: Optical analog circuits offer low-latency, low-power signal processing but face challenges with iterative algorithms like division and noise.

Method: Designed circuit structures for image restoration using ADMM and PDS, avoiding dynamic division and accounting for amplifier noise.

Result: Simulations showed effective denoising (measured by PSNR and SSIM) despite amplifier noise.

Conclusion: Feasible to implement image restoration on optical analog circuits with noise resilience.

Abstract: Optical analog circuits have attracted attention as promising alternatives to traditional electronic circuits for signal processing tasks due to their potential for low-latency and low-power computations. However, implementing iterative algorithms on such circuits presents challenges, particularly due to the difficulty of performing division operations involving dynamically changing variables and the additive noise introduced by optical amplifiers. In this study, we investigate the feasibility of implementing image restoration algorithms using total variation regularization on optical analog circuits. Specifically, we design the circuit structures for the image restoration with widely used alternating direction method of multipliers (ADMM) and primal dual splitting (PDS). Our design avoids division operations involving dynamic variables and incorporate the impact of additive noise introduced by optical amplifiers. Simulation results show that the effective denoising can be achieved in terms of peak signal to noise ratio (PSNR) and structural similarity index measure (SSIM) even when the circuit noise at the amplifiers is taken into account.

</details>


### [485] [Plug-and-Play with 2.5D Artifact Reduction Prior for Fast and Accurate Industrial Computed Tomography Reconstruction](https://arxiv.org/pdf/2506.14719)
*Haley Duba-Sullivan, Aniket Pramanik, Venkatakrishnan Singanallur, Amirkoushyar Ziabari*

Main category: eess.IV

TL;DR: A 2.5D CNN-based PnP method improves XCT reconstruction quality by leveraging inter-slice information, outperforming 2D priors in artifact reduction and domain generalization.


<details>
  <summary>Details</summary>
Motivation: Sparse-view XCT scans are slow and expensive; existing 2D CNN priors lack inter-slice context, limiting performance.

Method: Proposes a PnP framework with a 2.5D artifact reduction CNN, capturing richer spatial context while remaining efficient.

Result: Better preserves structural details, suppresses artifacts without pre-processing, and generalizes well across domains.

Conclusion: The 2.5D prior enhances reconstruction accuracy and defect detection, demonstrating strong generalization from simulated to experimental data.

Abstract: Cone-beam X-ray computed tomography (XCT) is an essential imaging technique for generating 3D reconstructions of internal structures, with applications ranging from medical to industrial imaging. Producing high-quality reconstructions typically requires many X-ray measurements; this process can be slow and expensive, especially for dense materials. Recent work incorporating artifact reduction priors within a plug-and-play (PnP) reconstruction framework has shown promising results in improving image quality from sparse-view XCT scans while enhancing the generalizability of deep learning-based solutions. However, this method uses a 2D convolutional neural network (CNN) for artifact reduction, which captures only slice-independent information from the 3D reconstruction, limiting performance. In this paper, we propose a PnP reconstruction method that uses a 2.5D artifact reduction CNN as the prior. This approach leverages inter-slice information from adjacent slices, capturing richer spatial context while remaining computationally efficient. We show that this 2.5D prior not only improves the quality of reconstructions but also enables the model to directly suppress commonly occurring XCT artifacts (such as beam hardening), eliminating the need for artifact correction pre-processing. Experiments on both experimental and synthetic cone-beam XCT data demonstrate that the proposed method better preserves fine structural details, such as pore size and shape, leading to more accurate defect detection compared to 2D priors. In particular, we demonstrate strong performance on experimental XCT data using a 2.5D artifact reduction prior trained entirely on simulated scans, highlighting the proposed method's ability to generalize across domains.

</details>


### [486] [Dose-aware Diffusion Model for 3D PET Image Denoising: Multi-institutional Validation with Reader Study and Real Low-dose Data](https://arxiv.org/pdf/2405.12996)
*Huidong Xie, Weijie Gan, Reimund Bayerlein, Bo Zhou, Ming-Kai Chen, Michal Kulon, Annemarie Boustani, Kuan-Yin Ko, Der-Shiun Wang, Benjamin A. Spencer, Wei Ji, Xiongchao Chen, Qiong Liu, Xueqi Guo, Menghua Xia, Yinchi Zhou, Hui Liu, Liang Guo, Hongyu An, Ulugbek S. Kamilov, Hanzhong Wang, Biao Li, Axel Rominger, Kuangyu Shi, Ge Wang, Ramsey D. Badawi, Chi Liu*

Main category: eess.IV

TL;DR: DDPET-3D, a dose-aware diffusion model, improves low-dose PET imaging by addressing limitations of existing models, achieving consistent 3D reconstructions and generalizing across noise-levels, scanners, and protocols.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for PET denoising compromise image quality and lack generalizability. Diffusion models show promise but fail in 3D reconstructions and produce biased results for low-dose PET.

Method: Developed DDPET-3D, a dose-aware diffusion model, and evaluated it on 9,783 18F-FDG studies from 4 global medical centers with varying low-dose levels (1%-50%).

Result: DDPET-3D generalized well across different low-dose levels, scanners, and protocols, producing images judged similar or superior to full-dose images by experts. Quantitative accuracy was confirmed via Monte Carlo simulation and lesion segmentation.

Conclusion: DDPET-3D demonstrates potential to maintain image quality in low-dose PET imaging, addressing key challenges of existing methods.

Abstract: Reducing scan times, radiation dose, and enhancing image quality for lower-performance scanners, are critical in low-dose PET imaging. Deep learning techniques have been investigated for PET image denoising. However, existing models have often resulted in compromised image quality when achieving low-count/low-dose PET and have limited generalizability to different image noise-levels, acquisition protocols, and patient populations. Recently, diffusion models have emerged as the new state-of-the-art generative model to generate high-quality samples and have demonstrated strong potential for medical imaging tasks. However, for low-dose PET imaging, existing diffusion models failed to generate consistent 3D reconstructions, unable to generalize across varying noise-levels, often produced visually-appealing but distorted image details, and produced images with biased tracer uptake. Here, we develop DDPET-3D, a dose-aware diffusion model for 3D low-dose PET imaging to address these challenges. Collected from 4 medical centers globally with different scanners and clinical protocols, we evaluated the proposed model using a total of 9,783 18F-FDG studies with low-dose levels ranging from 1% to 50%. With a cross-center, cross-scanner validation, the proposed DDPET-3D demonstrated its potential to generalize to different low-dose levels, different scanners, and different clinical protocols. As confirmed with reader studies performed by board-certified nuclear medicine physicians, experienced readers judged the images to be similar or superior to the full-dose images and previous DL baselines based on qualitative visual impression. Lesion-level quantitative accuracy was evaluated using a Monte Carlo simulation study and a lesion segmentation network. The presented results show the potential to achieve low-dose PET while maintaining image quality. Real low-dose scans was also included for evaluation.

</details>


### [487] [BS-LDM: Effective Bone Suppression in High-Resolution Chest X-Ray Images with Conditional Latent Diffusion Models](https://arxiv.org/pdf/2412.15670)
*Yifei Sun, Zhanghao Chen, Hao Zheng, Wenming Deng, Jin Liu, Wenwen Min, Ahmed Elazab, Xiang Wan, Changmiao Wang, Ruiquan Ge*

Main category: eess.IV

TL;DR: BS-LDM is an end-to-end framework using latent diffusion models to suppress bone structures in CXR images, improving pulmonary lesion detection.


<details>
  <summary>Details</summary>
Motivation: Overlapping bone structures in CXR images hinder pulmonary lesion detection, leading to misdiagnoses.

Method: The framework combines conditional latent diffusion models with a multi-level hybrid loss-constrained vector-quantized GAN for perceptual compression, using offset noise and temporal adaptive thresholding.

Result: BS-LDM effectively suppresses bone in high-resolution CXR images, validated by experiments and downstream evaluations.

Conclusion: BS-LDM demonstrates clinical value for accurate pulmonary lesion detection, supported by a high-quality dataset.

Abstract: Lung diseases represent a significant global health challenge, with Chest X-Ray (CXR) being a key diagnostic tool due to its accessibility and affordability. Nonetheless, the detection of pulmonary lesions is often hindered by overlapping bone structures in CXR images, leading to potential misdiagnoses. To address this issue, we develop an end-to-end framework called BS-LDM, designed to effectively suppress bone in high-resolution CXR images. This framework is based on conditional latent diffusion models and incorporates a multi-level hybrid loss-constrained vector-quantized generative adversarial network which is crafted for perceptual compression, ensuring the preservation of details. To further enhance the framework's performance, we utilize offset noise in the forward process, and a temporal adaptive thresholding strategy in the reverse process. These additions help minimize discrepancies in generating low-frequency information of soft tissue images. Additionally, we have compiled a high-quality bone suppression dataset named SZCH-X-Rays. This dataset includes 818 pairs of high-resolution CXR and soft tissue images collected from our partner hospital. Moreover, we processed 241 data pairs from the JSRT dataset into negative images, which are more commonly used in clinical practice. Our comprehensive experiments and downstream evaluations reveal that BS-LDM excels in bone suppression, underscoring its clinical value. Our code is available at https://github.com/diaoquesang/BS-LDM.

</details>


### [488] [Automated Muscle and Fat Segmentation in Computed Tomography for Comprehensive Body Composition Analysis](https://arxiv.org/pdf/2502.09779)
*Yaqian Chen, Hanxue Gu, Yuwen Chen, Jicheng Yang, Haoyu Dong, Joseph Y. Cao, Adrian Camarena, Christopher Mantyh, Roy Colglazier, Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: A publicly accessible, end-to-end model for CT body composition analysis is introduced, offering segmentation of muscle and fat tissues and calculating metrics, with high accuracy and outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: The lack of consistent, publicly available tools for CT body composition analysis across clinical applications motivated the development of this model.

Method: The model segments skeletal muscle, SAT, and VAT in axial CT images, calculates body composition metrics, and supports 2D/3D assessments. It was evaluated on internal and external datasets.

Result: High Dice coefficients (>89%) for segmentation, outperforming benchmarks by 2.40-10.26%. Metrics showed MRAEs under 10%, with muscular fat segmentation at 56.27% Dice.

Conclusion: The model is a robust, publicly available tool for CT body composition analysis, validated across diverse datasets and outperforming existing benchmarks.

Abstract: Body composition assessment using CT images can potentially be used for a number of clinical applications, including the prognostication of cardiovascular outcomes, evaluation of metabolic health, monitoring of disease progression, assessment of nutritional status, prediction of treatment response in oncology, and risk stratification for surgical and critical care outcomes. While multiple groups have developed in-house segmentation tools for this analysis, there are very limited publicly available tools that could be consistently used across different applications. To mitigate this gap, we present a publicly accessible, end-to-end segmentation and feature calculation model specifically for CT body composition analysis. Our model performs segmentation of skeletal muscle, subcutaneous adipose tissue (SAT), and visceral adipose tissue (VAT) across the chest, abdomen, and pelvis area in axial CT images. It also provides various body composition metrics, including muscle density, visceral-to-subcutaneous fat (VAT/SAT) ratio, muscle area/volume, and skeletal muscle index (SMI), supporting both 2D and 3D assessments. To evaluate the model, the segmentation was applied to both internal and external datasets, with body composition metrics analyzed across different age, sex, and race groups. The model achieved high dice coefficients on both internal and external datasets, exceeding 89% for skeletal muscle, SAT, and VAT segmentation. The model outperforms the benchmark by 2.40% on skeletal muscle and 10.26% on SAT compared to the manual annotations given by the publicly available dataset. Body composition metrics show mean relative absolute errors (MRAEs) under 10% for all measures. Furthermore, the model provided muscular fat segmentation with a Dice coefficient of 56.27%, which can be utilized for additional analyses as needed.

</details>


### [489] [3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation](https://arxiv.org/pdf/2505.04097)
*Thien Nhan Vo, Bac Nam Ho*

Main category: eess.IV

TL;DR: A 3D CNN model for Alzheimer's classification in brain MRI achieved high accuracy (0.912) and AUC (0.961), outperforming resizing alone by ~0.027. Sensitivity and specificity were >0.90.


<details>
  <summary>Details</summary>
Motivation: To improve Alzheimer's disease classification in T1-weighted brain MRI scans using a 3D CNN with simple augmentation techniques.

Method: Developed a 3D CNN with convolution, pooling, batch normalization, ReLU layers, and sigmoid output. Used stochastic noise injection and five-fold cross-validation.

Result: Achieved 0.912 accuracy, 0.961 AUC, and sensitivity/specificity >0.90, outperforming resizing alone by ~0.027.

Conclusion: Simple augmentation is effective for 3D MRI classification; future work should explore advanced methods like 3D U-Net and vision transformers.

Abstract: A three-dimensional convolutional neural network was developed to classify T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid output. Using stochastic noise injection and five-fold cross-validation, the model achieved test set accuracy of 0.912 and area under the ROC curve of 0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity and specificity both exceeded 0.90. These results align with prior work reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate the effectiveness of simple augmentation for 3D MRI classification and motivate future exploration of advanced augmentation methods and architectures such as 3D U-Net and vision transformers.

</details>


### [490] [DeepInverse: A Python package for solving imaging inverse problems with deep learning](https://arxiv.org/pdf/2505.20160)
*Julián Tachella, Matthieu Terris, Samuel Hurault, Andrew Wang, Dongdong Chen, Minh-Hai Nguyen, Maxime Song, Thomas Davies, Leo Davy, Jonathan Dong, Paul Escande, Johannes Hertrich, Zhiyuan Hu, Tobías I. Liaudat, Nils Laurent, Brett Levac, Mathurin Massias, Thomas Moreau, Thibaut Modrzyk, Brayan Monroy, Sebastian Neumayer, Jérémy Scanvic, Florian Sarron, Victor Sechaud, Georg Schramm, Romain Vo, Pierre Weiss*

Main category: eess.IV

TL;DR: DeepInverse is a PyTorch-based library for solving imaging inverse problems, covering forward operators, variational problems, and neural network training.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive open-source tool for image reconstruction tasks, addressing the need for efficient and flexible solutions in inverse imaging problems.

Method: The library implements forward operators (e.g., optics, MRI, tomography), defines and resolves variational problems, and designs/trains neural networks.

Result: DeepInverse offers a versatile framework for image reconstruction, integrating key steps from forward modeling to deep learning.

Conclusion: The paper presents DeepInverse as a valuable resource for researchers and practitioners in imaging inverse problems, highlighting its design and functionality.

Abstract: DeepInverse is an open-source PyTorch-based library for solving imaging inverse problems. The library covers all crucial steps in image reconstruction from the efficient implementation of forward operators (e.g., optics, MRI, tomography), to the definition and resolution of variational problems and the design and training of advanced neural network architectures. In this paper, we describe the main functionality of the library and discuss the main design choices.

</details>
