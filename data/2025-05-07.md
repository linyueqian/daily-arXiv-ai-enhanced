<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.CV](#cs.CV) [Total: 130]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.SD](#cs.SD) [Total: 11]
- [cs.LG](#cs.LG) [Total: 108]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.MM](#cs.MM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [eess.IV](#eess.IV) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/pdf/2505.02847)
*Bang Zhang, Ruotian Ma, Qingxuan Jiang, Peisong Wang, Jiaqi Chen, Zheng Xie, Xingyu Chen, Yue Wang, Fanghua Ye, Jian Li, Yifan Yang, Zhaopeng Tu, Xiaolong Li*

Main category: cs.CL

TL;DR: SAGE is an automated framework evaluating LLMs' social cognition by simulating human-like emotions and thoughts, validated by strong correlations with psychological metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assessing LLMs' human-like understanding beyond text, focusing on social cognition.

Method: SAGE uses a Sentient Agent to simulate emotional changes and inner thoughts during multi-turn conversations, generating emotion scores and interpretable reasoning.

Result: Experiments show strong correlation with psychological metrics (BLRI, empathy), and a leaderboard reveals gaps between frontier and baseline models.

Conclusion: SAGE offers a scalable, interpretable tool for advancing empathetic and socially adept LLMs.

Abstract: Assessing how well a large language model (LLM) understands human, rather
than merely text, remains an open challenge. To bridge the gap, we introduce
Sentient Agent as a Judge (SAGE), an automated evaluation framework that
measures an LLM's higher-order social cognition. SAGE instantiates a Sentient
Agent that simulates human-like emotional changes and inner thoughts during
interaction, providing a more realistic evaluation of the tested model in
multi-turn conversations. At every turn, the agent reasons about (i) how its
emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a
numerical emotion trajectory and interpretable inner thoughts. Experiments on
100 supportive-dialogue scenarios show that the final Sentient emotion score
correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings
and utterance-level empathy metrics, validating psychological fidelity. We also
build a public Sentient Leaderboard covering 18 commercial and open-source
models that uncovers substantial gaps (up to 4x) between frontier systems
(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in
conventional leaderboards (e.g., Arena). SAGE thus provides a principled,
scalable and interpretable tool for tracking progress toward genuinely
empathetic and socially adept language agents.

</details>


### [2] [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/pdf/2505.02850)
*Nicy Scaria, Silvester John Joseph Kennedy, Diksha Seth, Ananya Thakur, Deepak Subramani*

Main category: cs.CL

TL;DR: A hierarchical concept map-based framework guides LLMs to generate high-quality MCQs targeting cognitive levels and misconceptions, outperforming baseline methods in expert and student evaluations.


<details>
  <summary>Details</summary>
Motivation: Manual MCQ generation is time-consuming and lacks scalability, while current automated methods fail to address higher cognitive levels and domain-specific misconceptions.

Method: Developed a hierarchical concept map for physics, used it to guide LLMs in generating MCQs with distractors, and validated the output automatically.

Result: Achieved 75.20% success in quality criteria (vs. 37% for baselines) and a lower student guess rate (28.05% vs. 37.10%).

Conclusion: The concept map-based approach enables robust assessment, identifies conceptual gaps, and supports scalable feedback and interventions.

Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive
levels and incorporating common misconceptions into distractor design, is
time-consuming and expertise-intensive, making manual creation impractical at
scale. Current automated approaches typically generate questions at lower
cognitive levels and fail to incorporate domain-specific misconceptions. This
paper presents a hierarchical concept map-based framework that provides
structured knowledge to guide LLMs in generating MCQs with distractors. We
chose high-school physics as our test domain and began by developing a
hierarchical concept map covering major Physics topics and their
interconnections with an efficient database design. Next, through an automated
pipeline, topic-relevant sections of these concept maps are retrieved to serve
as a structured context for the LLM to generate questions and distractors that
specifically target common misconceptions. Lastly, an automated validation is
completed to ensure that the generated MCQs meet the requirements provided. We
evaluate our framework against two baseline approaches: a base LLM and a
RAG-based generation. We conducted expert evaluations and student assessments
of the generated MCQs. Expert evaluation shows that our method significantly
outperforms the baseline approaches, achieving a success rate of 75.20% in
meeting all quality criteria compared to approximately 37% for both baseline
methods. Student assessment data reveal that our concept map-driven approach
achieved a significantly lower guess success rate of 28.05% compared to 37.10%
for the baselines, indicating a more effective assessment of conceptual
understanding. The results demonstrate that our concept map-based approach
enables robust assessment across cognitive levels and instant identification of
conceptual gaps, facilitating faster feedback loops and targeted interventions
at scale.

</details>


### [3] [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/pdf/2505.02851)
*Franklin Zhang, Sonya Zhang, Alon Halevy*

Main category: cs.CL

TL;DR: 30 Day Me is a habit formation app using LLMs to break goals into steps and track progress, featuring the 30DAYGEN system for generating and searching challenges.


<details>
  <summary>Details</summary>
Motivation: To demonstrate how LLMs can create domain-specific content for behavioral and educational purposes, improving habit formation.

Method: Developed the 30DAYGEN system to generate and search 30-day challenges from web content, using LLMs for content generation and semantic deduplication.

Result: Produced 3,531 unique challenges from 15K webpages, enabling goal-aligned runtime search.

Conclusion: LLMs effectively support rapid content creation and deduplication for habit formation apps.

Abstract: In this paper, we present 30 Day Me, a habit formation application that
leverages Large Language Models (LLMs) to help users break down their goals
into manageable, actionable steps and track their progress. Central to the app
is the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced
from over 15K webpages, and enables runtime search of challenge ideas aligned
with user-defined goals. We showcase how LLMs can be harnessed to rapidly
construct domain specific content corpora for behavioral and educational
purposes, and propose a practical pipeline that incorporates effective LLM
enhanced approaches for content generation and semantic deduplication.

</details>


### [4] [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/pdf/2505.02854)
*Masumi Morishige, Ryo Koshihara*

Main category: cs.CL

TL;DR: GPR-bench is introduced as a lightweight, extensible benchmark for regression testing in generative AI, evaluating correctness and conciseness using LLM-as-a-Judge. Results show modest improvements in newer models but significant conciseness gains from prompt engineering.


<details>
  <summary>Details</summary>
Motivation: Address reproducibility and reliability challenges in generative AI systems, which can drift with model updates or prompt changes.

Method: GPR-bench uses a bilingual dataset (English/Japanese) across eight task categories with an automated evaluation pipeline (LLM-as-a-Judge). Tests three model versions and two prompt configurations.

Result: Newer models show slight correctness improvements (not significant). Concise-writing prompts boost conciseness (+12.37 pp) with minor accuracy trade-offs (-1.7 pp).

Conclusion: GPR-bench aids reproducibility monitoring but may lack challenge for recent models. Highlights prompt engineering's impact and benchmark design considerations.

Abstract: Reproducibility and reliability remain pressing challenges for generative AI
systems whose behavior can drift with each model update or prompt revision. We
introduce GPR-bench, a lightweight, extensible benchmark that operationalizes
regression testing for general purpose use cases. GPR-bench couples an open,
bilingual (English and Japanese) dataset covering eight task categories (e.g.,
text generation, code generation, and information retrieval) and 10 scenarios
in each task categories (80 total test cases for each language) with an
automated evaluation pipeline that employs "LLM-as-a-Judge" scoring of
correctness and conciseness. Experiments across three recent model versions -
gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default
versus concise-writing instruction) reveal heterogeneous quality. Our results
show that newer models generally improve correctness, but the differences are
modest and not statistically significant, suggesting that GPR-bench may not be
sufficiently challenging to differentiate between recent model versions. In
contrast, the concise-writing instruction significantly enhances conciseness
(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with
minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of
prompt engineering. Released under the MIT License, GPR- bench lowers the
barrier to initiating reproducibility monitoring and provides a foundation for
community-driven extensions, while also raising important considerations about
benchmark design for rapidly evolving language models.

</details>


### [5] [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/pdf/2505.02858)
*Henry Tari, Nojus Sereiva, Rishabh Kaushal, Thales Bertaglia, Adriana Iamnitchi*

Main category: cs.CL

TL;DR: The paper explores using large language models to generate synthetic multi-platform social media datasets, evaluating their fidelity compared to real data and proposing new metrics for assessment.


<details>
  <summary>Details</summary>
Motivation: Access to multi-platform social media datasets is limited due to costs and restrictions, hindering research on topics like disinformation and hate speech.

Method: Multi-platform topic-based prompting with various language models generates synthetic data from real datasets, assessing lexical and semantic properties.

Result: Synthetic data shows promise, with varying fidelity across models, suggesting post-processing may improve quality.

Conclusion: Large language models can generate useful synthetic datasets, but fidelity varies, and new metrics are proposed for evaluation.

Abstract: Social media datasets are essential for research on a variety of topics, such
as disinformation, influence operations, hate speech detection, or influencer
marketing practices. However, access to social media datasets is often
constrained due to costs and platform restrictions. Acquiring datasets that
span multiple platforms, which is crucial for understanding the digital
ecosystem, is particularly challenging. This paper explores the potential of
large language models to create lexically and semantically relevant social
media datasets across multiple platforms, aiming to match the quality of real
data. We propose multi-platform topic-based prompting and employ various
language models to generate synthetic data from two real datasets, each
consisting of posts from three different social media platforms. We assess the
lexical and semantic properties of the synthetic data and compare them with
those of the real data. Our empirical findings show that using large language
models to generate synthetic multi-platform social media data is promising,
different language models perform differently in terms of fidelity, and a
post-processing approach might be needed for generating high-fidelity synthetic
datasets for research. In addition to the empirical evaluation of three state
of the art large language models, our contributions include new fidelity
metrics specific to multi-platform social media datasets.

</details>


### [6] [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/pdf/2505.02859)
*Jonas Bokstaller, Julia Altheimer, Julian Dormehl, Alina Buss, Jasper Wiltfang, Johannes Schneider, Maximilian Röglinger*

Main category: cs.CL

TL;DR: A novel reference architecture combines XAI and LLMs to create an interactive chatbot for interpreting ML models, validated in battery SoH prediction.


<details>
  <summary>Details</summary>
Motivation: The increasing opacity of ML models and advancements in LLMs' language understanding drive the need for better XAI interpretability.

Method: Develop a reference architecture using a fine-tuned LLM for an interactive chatbot, applied to battery SoH prediction.

Result: The prototype improves ML interpretability, especially for users with limited XAI experience.

Conclusion: The architecture successfully enhances human understanding of ML models through interactive XAI.

Abstract: Across various sectors applications of eXplainableAI (XAI) gained momentum as
the increasing black-boxedness of prevailing Machine Learning (ML) models
became apparent. In parallel, Large Language Models (LLMs) significantly
developed in their abilities to understand human language and complex patterns.
By combining both, this paper presents a novel reference architecture for the
interpretation of XAI through an interactive chatbot powered by a fine-tuned
LLM. We instantiate the reference architecture in the context of
State-of-Health (SoH) prediction for batteries and validate its design in
multiple evaluation and demonstration rounds. The evaluation indicates that the
implemented prototype enhances the human interpretability of ML, especially for
users with less experience with XAI.

</details>


### [7] [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/pdf/2505.02862)
*Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang*

Main category: cs.CL

TL;DR: Proposes ICRT, a novel jailbreak attack framework for LLMs using human cognition biases, outperforming existing methods and offering insights for defense.


<details>
  <summary>Details</summary>
Motivation: Address vulnerabilities in LLMs' safety mechanisms by uncovering real-world risks through cognitive-inspired attacks.

Method: Uses cognitive decomposition (simplicity effect) and relevance bias to reorganize prompts, with a ranking-based harmfulness metric.

Result: Consistently bypasses LLMs' safety, generating high-risk content, and provides insights for defense.

Conclusion: ICRT highlights jailbreak risks and aids in developing stronger defense strategies.

Abstract: Despite the remarkable performance of Large Language Models (LLMs), they
remain vulnerable to jailbreak attacks, which can compromise their safety
mechanisms. Existing studies often rely on brute-force optimization or manual
design, failing to uncover potential risks in real-world scenarios. To address
this, we propose a novel jailbreak attack framework, ICRT, inspired by
heuristics and biases in human cognition. Leveraging the simplicity effect, we
employ cognitive decomposition to reduce the complexity of malicious prompts.
Simultaneously, relevance bias is utilized to reorganize prompts, enhancing
semantic alignment and inducing harmful outputs effectively. Furthermore, we
introduce a ranking-based harmfulness evaluation metric that surpasses the
traditional binary success-or-failure paradigm by employing ranking aggregation
methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify
the harmfulness of generated content. Experimental results show that our
approach consistently bypasses mainstream LLMs' safety mechanisms and generates
high-risk content, providing insights into jailbreak attack risks and
contributing to stronger defense strategies.

</details>


### [8] [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/pdf/2505.02865)
*Zhihai Wang, Jie Wang, Jilai Pan, Xilin Xia, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Feng Wu*

Main category: cs.CL

TL;DR: Speculative Search (SpecSearch) accelerates LLM reasoning by using a small model to collaborate with a large model, preserving reasoning quality while reducing latency.


<details>
  <summary>Details</summary>
Motivation: Tree-search-based reasoning methods for LLMs suffer from high inference latency due to generating numerous thoughts, limiting applicability.

Method: SpecSearch uses a small model to collaborate with a large model at thought and token levels, employing a quality-preserving rejection mechanism to filter low-quality thoughts.

Result: SpecSearch achieves up to 2.12× speedup with comparable reasoning quality, outperforming state-of-the-art methods.

Conclusion: SpecSearch effectively reduces latency while maintaining reasoning quality, enhancing LLM applicability.

Abstract: Tree-search-based reasoning methods have significantly enhanced the reasoning
capability of large language models (LLMs) by facilitating the exploration of
multiple intermediate reasoning steps, i.e., thoughts. However, these methods
suffer from substantial inference latency, as they have to generate numerous
reasoning thoughts, severely limiting LLM applicability. To address this
challenge, we propose a novel Speculative Search (SpecSearch) framework that
significantly accelerates LLM reasoning by optimizing thought generation.
Specifically, SpecSearch utilizes a small model to strategically collaborate
with a large model at both thought and token levels, efficiently generating
high-quality reasoning thoughts. The major pillar of SpecSearch is a novel
quality-preserving rejection mechanism, which effectively filters out thoughts
whose quality falls below that of the large model's outputs. Moreover, we show
that SpecSearch preserves comparable reasoning quality to the large model.
Experiments on both the Qwen and Llama models demonstrate that SpecSearch
significantly outperforms state-of-the-art approaches, achieving up to
2.12$\times$ speedup with comparable reasoning quality.

</details>


### [9] [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/pdf/2505.02872)
*Cfir Avraham Hadar, Omer Shubi, Yoav Meiri, Yevgeni Berzak*

Main category: cs.CL

TL;DR: The paper explores whether open-ended reading goals can be decoded from eye movements using multimodal LLMs, achieving success in goal classification and reconstruction tasks.


<details>
  <summary>Details</summary>
Motivation: People read with diverse, text-specific goals, but it's unclear if these goals can be inferred from eye movements. This work investigates this for the first time.

Method: Introduces goal classification and reconstruction tasks, using large-scale eye-tracking data and multimodal LLMs combining eye movements and text.

Result: Experiments show significant success in decoding reading goals from eye movements, demonstrating LLMs' capability in this domain.

Conclusion: LLMs can effectively extract information about readers' text-specific goals from eye movements, opening new research avenues.

Abstract: When reading, we often have specific information that interests us in a text.
For example, you might be reading this paper because you are curious about LLMs
for eye movements in reading, the experimental design, or perhaps you only care
about the question ``but does it work?''. More broadly, in daily life, people
approach texts with any number of text-specific goals that guide their reading
behavior. In this work, we ask, for the first time, whether open-ended reading
goals can be automatically decoded from eye movements in reading. To address
this question, we introduce goal classification and goal reconstruction tasks
and evaluation frameworks, and use large-scale eye tracking for reading data in
English with hundreds of text-specific information seeking tasks. We develop
and compare several discriminative and generative multimodal LLMs that combine
eye movements and text for goal classification and goal reconstruction. Our
experiments show considerable success on both tasks, suggesting that LLMs can
extract valuable information about the readers' text-specific goals from eye
movements.

</details>


### [10] [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/pdf/2505.02983)
*Wenjie Hua, Shenghan Xu*

Main category: cs.CL

TL;DR: A Logits-Constrained (LC) framework for Ancient Chinese NER improves performance over traditional methods, especially in high-label or large-data settings, using GujiRoBERTa and a differentiable decoding mechanism.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of Named Entity Recognition in Ancient Chinese, particularly in high-label or large-data scenarios, by improving accuracy and efficiency.

Method: A two-stage model combining GujiRoBERTa for contextual encoding and a differentiable decoding mechanism to enforce valid BMES label transitions.

Result: LC outperforms traditional CRF and BiLSTM-based approaches, particularly in high-label or large-data settings.

Conclusion: The LC framework is effective for Ancient Chinese NER, with a proposed model selection criterion offering practical guidance for real-world NLP tasks.

Abstract: This paper presents a Logits-Constrained (LC) framework for Ancient Chinese
Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our
two-stage model integrates GujiRoBERTa for contextual encoding and a
differentiable decoding mechanism to enforce valid BMES label transitions.
Experiments demonstrate that LC improves performance over traditional CRF and
BiLSTM-based approaches, especially in high-label or large-data settings. We
also propose a model selection criterion balancing label complexity and dataset
size, providing practical guidance for real-world Ancient Chinese NLP tasks.

</details>


### [11] [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/pdf/2505.03005)
*Daniel Goldstein, Eric Alcaide, Janna Lu, Eugene Cheah*

Main category: cs.CL

TL;DR: RADLADS converts softmax attention transformers to linear attention decoders efficiently, with minimal cost and token usage, while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs and token requirements for converting transformers to linear attention models without significant quality loss.

Method: Introduces RADLADS protocol and new RWKV-variant architectures, converting Qwen2.5 models (7B, 32B, 72B) using only 350-700M tokens.

Result: Achieves SOTA performance for linear attention models, with conversion costs under $2,000 for a 72B model.

Conclusion: RADLADS offers a cost-effective, efficient method for converting transformers to linear attention models, with models released under open licenses.

Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale
(RADLADS), a protocol for rapidly converting softmax attention transformers
into linear attention decoder models, along with two new RWKV-variant
architectures, and models converted from popular Qwen2.5 open source models in
7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,
less than 0.005% of the token count used to train the original teacher models.
Converting to our 72B linear attention model costs less than \$2,000 USD at
today's prices, yet quality at inference remains close to the original
transformer. These models achieve state-of-the-art downstream performance
across a set of standard benchmarks for linear attention models of their size.
We release all our models on HuggingFace under the Apache 2.0 license, with the
exception of our 72B models which are also governed by the Qwen License
Agreement.
  Models at
https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102
Training Code at https://github.com/recursal/RADLADS-paper

</details>


### [12] [Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech](https://arxiv.org/pdf/2501.15858)
*Eunjung Yeo, Julie Liss, Visar Berisha, David Mortensen*

Main category: cs.CL

TL;DR: The paper proposes an AI-driven framework for cross-language intelligibility assessment of dysarthric speech, addressing data and linguistic challenges.


<details>
  <summary>Details</summary>
Motivation: Current research and clinical practices focus on English, limiting applicability across languages. The paper aims to bridge this gap.

Method: A two-tiered framework: a universal speech model for acoustic-phonetic encoding and a language-specific model for intelligibility assessment.

Result: Identifies barriers like data scarcity and annotation complexity, proposing AI solutions.

Conclusion: AI advancements can enable scalable, linguistically informed frameworks for cross-language dysarthric speech assessment.

Abstract: Purpose: Speech intelligibility is a critical outcome in the assessment and
management of dysarthria, yet most research and clinical practices have focused
on English, limiting their applicability across languages. This commentary
introduces a conceptual framework--and a demonstration of how it can be
implemented--leveraging artificial intelligence (AI) to advance cross-language
intelligibility assessment of dysarthric speech. Method: We propose a
two-tiered conceptual framework consisting of a universal speech model that
encodes dysarthric speech into acoustic-phonetic representations, followed by a
language-specific intelligibility assessment model that interprets these
representations within the phonological or prosodic structures of the target
language. We further identify barriers to cross-language intelligibility
assessment of dysarthric speech, including data scarcity, annotation
complexity, and limited linguistic insights into dysarthric speech, and outline
potential AI-driven solutions to overcome these challenges. Conclusion:
Advancing cross-language intelligibility assessment of dysarthric speech
necessitates models that are both efficient and scalable, yet constrained by
linguistic rules to ensure accurate and language-sensitive assessment. Recent
advances in AI provide the foundational tools to support this integration,
shaping future directions toward generalizable and linguistically informed
assessment frameworks.

</details>


### [13] [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/pdf/2505.03019)
*Albérick Euraste Djiré, Abdoul Kader Kaboré, Earl T. Barr, Jacques Klein, Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: PEARL is a new method to detect memorization in LLMs by analyzing output consistency under input perturbations, without needing internal model access. It successfully identified memorization in models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: LLMs often memorize training data instead of generalizing, raising privacy, IP, and reliability concerns. PEARL addresses this by detecting memorization.

Method: PEARL evaluates LLM output sensitivity to input perturbations to distinguish memorization from generalization.

Result: PEARL identified memorization in GPT-4o, including classic texts and common code, and inferred training data sources like NYT articles.

Conclusion: PEARL provides a robust, external framework for detecting memorization in LLMs, enhancing transparency and trust in model evaluations.

Abstract: While Large Language Models (LLMs) achieve remarkable performance through
training on massive datasets, they can exhibit concerning behaviors such as
verbatim reproduction of training data rather than true generalization. This
memorization phenomenon raises significant concerns about data privacy,
intellectual property rights, and the reliability of model evaluations. This
paper introduces PEARL, a novel approach for detecting memorization in LLMs.
PEARL assesses how sensitive an LLM's performance is to input perturbations,
enabling memorization detection without requiring access to the model's
internals. We investigate how input perturbations affect the consistency of
outputs, enabling us to distinguish between true generalization and
memorization. Our findings, following extensive experiments on the Pythia open
model, provide a robust framework for identifying when the model simply
regurgitates learned information. Applied on the GPT 4o models, the PEARL
framework not only identified cases of memorization of classic texts from the
Bible or common code from HumanEval but also demonstrated that it can provide
supporting evidence that some data, such as from the New York Times news
articles, were likely part of the training data of a given model.

</details>


### [14] [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/pdf/2505.03025)
*Steven Bedrick, A. Seza Doğruöz, Sergiu Nisioi*

Main category: cs.CL

TL;DR: The paper discusses the use of synthetic datasets in clinical dialogues, addressing challenges like privacy and proposing a typology for classification.


<details>
  <summary>Details</summary>
Motivation: Clinical dialogue datasets are sensitive and hard to collect, leading to reliance on synthetic data, but there's limited theory on their optimal use and generalization.

Method: The paper reviews synthetic dataset creation and evaluation methods for medical dialogues and introduces a novel typology for classification.

Result: The typology aids in comparing and evaluating synthetic datasets, addressing gaps in their application and generalization.

Conclusion: The proposed typology enhances understanding and utility of synthetic clinical dialogue datasets, supporting better evaluation and application.

Abstract: Synthetic data sets are used across linguistic domains and NLP tasks,
particularly in scenarios where authentic data is limited (or even
non-existent). One such domain is that of clinical (healthcare) contexts, where
there exist significant and long-standing challenges (e.g., privacy,
anonymization, and data governance) which have led to the development of an
increasing number of synthetic datasets. One increasingly important category of
clinical dataset is that of clinical dialogues which are especially sensitive
and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some
situations, little theory exists to inform how they may be best used and
generalized to new applications. In this paper, we provide an overview of how
synthetic datasets are created, evaluated and being used for dialogue related
tasks in the medical domain. Additionally, we propose a novel typology for use
in classifying types and degrees of data synthesis, to facilitate comparison
and evaluation.

</details>


### [15] [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/pdf/2505.03030)
*Sicong Huang, Jincheng He, Shiyuan Huang, Karthik Raja Anandan, Arkajyoti Chakraborty, Ian Lane*

Main category: cs.CL

TL;DR: The paper presents a framework for detecting and localizing hallucinations in LLM outputs, achieving top performance in the Mu-SHROOM task.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of identifying and pinpointing hallucinations in LLM responses to knowledge-intensive queries.

Method: A framework involving context retrieval, false content identification, and span mapping, enhanced by prompt optimization.

Result: Ranked #1 in average performance across all languages in the Mu-SHROOM task.

Conclusion: The system effectively detects and localizes hallucinations, with code and results made publicly available.

Abstract: Hallucinations pose a significant challenge for large language models when
answering knowledge-intensive queries. As LLMs become more widely adopted, it
is crucial not only to detect if hallucinations occur but also to pinpoint
exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:
Multilingual Shared-task on Hallucinations and Related Observable
Overgeneration Mistakes, is a recent effort in this direction. This paper
describes the UCSC system submission to the shared Mu-SHROOM task. We introduce
a framework that first retrieves relevant context, next identifies false
content from the answer, and finally maps them back to spans in the LLM output.
The process is further enhanced by automatically optimizing prompts. Our system
achieves the highest overall performance, ranking #1 in average position across
all languages. We release our code and experiment results.

</details>


### [16] [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/pdf/2505.03052)
*Ryan Wang, Matthew Finlayson, Luca Soldaini, Swabha Swayamdipta, Robin Jia*

Main category: cs.CL

TL;DR: SLUNG is a pre-training method that helps models understand high-risk content without generating it, improving recognition of harmful text while avoiding its production.


<details>
  <summary>Details</summary>
Motivation: Current methods filter out high-risk content, limiting models' ability to recognize and respond to harmful or sensitive text. SLUNG addresses this gap.

Method: SLUNG selectively avoids incentivizing high-risk token generation while ensuring their context is understood, using next-token prediction loss selectively.

Result: SLUNG improves models' understanding of high-risk data (e.g., toxicity recognition) without increasing its generation (e.g., toxic responses).

Conclusion: SLUNG allows models to benefit from high-risk text that would otherwise be filtered, enhancing safety and utility.

Abstract: Language model developers typically filter out high-risk content -- such as
toxic or copyrighted text -- from their pre-training data to prevent models
from generating similar outputs. However, removing such data altogether limits
models' ability to recognize and appropriately respond to harmful or sensitive
content. In this paper, we introduce Selective Loss to Understand but Not
Generate (SLUNG), a pre-training paradigm through which models learn to
understand high-risk data without learning to generate it. Instead of uniformly
applying the next-token prediction loss, SLUNG selectively avoids incentivizing
the generation of high-risk tokens while ensuring they remain within the
model's context window. As the model learns to predict low-risk tokens that
follow high-risk ones, it is forced to understand the high-risk content.
Through our experiments, we show that SLUNG consistently improves models'
understanding of high-risk data (e.g., ability to recognize toxic content)
without increasing its generation (e.g., toxicity of model responses). Overall,
our SLUNG paradigm enables models to benefit from high-risk text that would
otherwise be filtered out.

</details>


### [17] [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/pdf/2505.03053)
*Jennifer Healey, Laurie Byrum, Md Nadeem Akhtar, Surabhi Bhargava, Moumita Sinha*

Main category: cs.CL

TL;DR: A semi-automated bias evaluation framework for LLM free text responses, combining human insights with automation to address challenges in bias assessment.


<details>
  <summary>Details</summary>
Motivation: Evaluating bias in LLMs is complex, especially in real-world deployments where task-specific prompts and context interplay. Traditional benchmarks often lack validity, and human evaluation is costly.

Method: Developed an operational definition of bias, automated the evaluation pipeline, and extended bias classification beyond multiple-choice formats. Incorporated human evaluation to identify issues in benchmarks.

Result: Created a framework that balances automation with human insights, uncovering problematic templates in bias benchmarks.

Conclusion: The semi-automated approach offers a scalable and valid solution for bias evaluation in LLMs, leveraging both human and automated methods.

Abstract: LLM evaluation is challenging even the case of base models. In real world
deployments, evaluation is further complicated by the interplay of task
specific prompts and experiential context. At scale, bias evaluation is often
based on short context, fixed choice benchmarks that can be rapidly evaluated,
however, these can lose validity when the LLMs' deployed context differs. Large
scale human evaluation is often seen as too intractable and costly. Here we
present our journey towards developing a semi-automated bias evaluation
framework for free text responses that has human insights at its core. We
discuss how we developed an operational definition of bias that helped us
automate our pipeline and a methodology for classifying bias beyond multiple
choice. We additionally comment on how human evaluation helped us uncover
problematic templates in a bias benchmark.

</details>


### [18] [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/pdf/2505.03059)
*Junlin Wang, Roy Xie, Shang Zhu, Jue Wang, Ben Athiwaratkun, Bhuwan Dhingra, Shuaiwen Leon Song, Ce Zhang, James Zou*

Main category: cs.CL

TL;DR: MoAA leverages multiple language models to generate high-quality alignment data, improving LLM performance and enabling self-improvement.


<details>
  <summary>Details</summary>
Motivation: High-quality human-labeled data for LLM alignment is expensive and lacks diversity, prompting the need for scalable synthetic solutions.

Method: Mixture of Agents Alignment (MoAA) combines various language models to produce diverse and high-quality alignment data for supervised fine-tuning and preference optimization.

Result: MoAA boosts LLaMA-3.1-8B-Instruct's win rate significantly on Arena-Hard and AlpacaEval2, and enables self-improvement without external supervision.

Conclusion: MoAA offers a scalable, diverse, and effective approach for LLM alignment, advancing open-source models without relying on costly human data.

Abstract: Building helpful and harmless large language models (LLMs) requires effective
model alignment approach based on human instructions and feedback, which
necessitates high-quality human-labeled data. Constructing such datasets is
often expensive and hard to scale, and may face potential limitations on
diversity and generalization. To address these challenges, we introduce Mixture
of Agents Alignment (MoAA), that leverages the collective strengths of various
language models to provide high-quality data for model alignment. By employing
MoAA, we enhance both supervised fine-tuning and preference optimization,
leading to improved performance compared to using a single model alone to
generate alignment data (e.g. using GPT-4o alone). Evaluation results show that
our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on
Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising
direction for model alignment through this new scalable and diverse synthetic
data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement
pipeline, where models finetuned on MoA-generated data surpass their own
initial capabilities, providing evidence that our approach can push the
frontier of open-source LLMs without reliance on stronger external supervision.
Data and code will be released.

</details>


### [19] [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/pdf/2505.03229)
*Behrooz Mansouri*

Main category: cs.CL

TL;DR: A survey of Abstract Meaning Representation (AMR), its capabilities, parsing/generation tasks, applications, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To explore AMR's potential in enhancing machine understanding of human language through its graph-based semantic representation.

Method: Investigates AMR and its extensions, reviews parsing/generation tasks, and examines applications like text generation and classification.

Result: Highlights recent developments, challenges, and the impact of AMR on NLP tasks.

Conclusion: AMR shows promise for advancing machine language understanding, with future research needed to address current limitations.

Abstract: This paper presents a survey of Abstract Meaning Representation (AMR), a
semantic representation framework that captures the meaning of sentences
through a graph-based structure. AMR represents sentences as rooted, directed
acyclic graphs, where nodes correspond to concepts and edges denote
relationships, effectively encoding the meaning of complex sentences. This
survey investigates AMR and its extensions, focusing on AMR capabilities. It
then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by
showing traditional, current, and possible futures approaches. It also reviews
various applications of AMR including text generation, text classification, and
information extraction and information seeking. By analyzing recent
developments and challenges in the field, this survey provides insights into
future directions for research and the potential impact of AMR on enhancing
machine understanding of human language.

</details>


### [20] [Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/pdf/2505.03293)
*Shijing Zhu, Zhuang Chen, Guanqun Bi, Binghang Li, Yaxi Deng, Dazhen Wan, Libiao Peng, Xiyao Xiao, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, FangFang Li, Minlie Huang*

Main category: cs.CL

TL;DR: Psi-Arena is an interactive framework for evaluating and improving LLM-based mental health counselors through multi-stage dialogues, tripartite evaluations, and closed-loop optimization, showing significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLM counselors are limited by static, single-perspective, and open-loop methods, necessitating a more comprehensive approach.

Method: Psi-Arena uses realistic multi-stage dialogues with NPC clients, tripartite evaluations (client, counselor, supervisor), and closed-loop optimization with feedback.

Result: Experiments show performance variations among LLMs and up to 141% improvement in counseling performance after optimization.

Conclusion: Psi-Arena offers a foundational framework for advancing reliable and human-aligned LLM applications in mental healthcare.

Abstract: Large language models (LLMs) have shown promise in providing scalable mental
health support, while evaluating their counseling capability remains crucial to
ensure both efficacy and safety. Existing evaluations are limited by the static
assessment that focuses on knowledge tests, the single perspective that centers
on user experience, and the open-loop framework that lacks actionable feedback.
To address these issues, we propose {\Psi}-Arena, an interactive framework for
comprehensive assessment and optimization of LLM-based counselors, featuring
three key characteristics: (1) Realistic arena interactions that simulate
real-world counseling through multi-stage dialogues with psychologically
profiled NPC clients, (2) Tripartite evaluation that integrates assessments
from the client, counselor, and supervisor perspectives, and (3) Closed-loop
optimization that iteratively improves LLM counselors using diagnostic
feedback. Experiments across eight state-of-the-art LLMs show significant
performance variations in different real-world scenarios and evaluation
perspectives. Moreover, reflection-based optimization results in up to a 141%
improvement in counseling performance. We hope PsychoArena provides a
foundational resource for advancing reliable and human-aligned LLM applications
in mental healthcare.

</details>


### [21] [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/pdf/2505.03320)
*Junyu Ma, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu*

Main category: cs.CL

TL;DR: RwR enhances Mamba's long-context memory by using CoT summarization from a teacher model, improving performance without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Mamba's long-context potential is limited in practice; this work aims to unlock it.

Method: Recall with Reasoning (RwR) distills CoT summarization from a teacher model and uses it as prompts during fine-tuning.

Result: RwR boosts Mamba's long-context performance on benchmarks like LONGMEMEVAL and HELMET, while maintaining short-context capabilities.

Conclusion: RwR is a simple-yet-effective method to enhance Mamba's long-context abilities without altering its architecture.

Abstract: Mamba's theoretical infinite-context potential is limited in practice when
sequences far exceed training lengths. This work explores unlocking Mamba's
long-context memory ability by a simple-yet-effective method, Recall with
Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a
teacher model. Specifically, RwR prepends these summarization as CoT prompts
during fine-tuning, teaching Mamba to actively recall and reason over long
contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's
long-context performance against comparable Transformer/hybrid baselines under
similar pretraining conditions, while preserving short-context capabilities,
all without architectural changes.

</details>


### [22] [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.03406)
*Mohammad Shoaib Ansari, Mohd Sohail Ali Khan, Shubham Revankar, Aditya Varma, Anil S. Mokhade*

Main category: cs.CL

TL;DR: The paper explores using LLMs in healthcare with RAG and QLoRA for improved medical decision support, focusing on accuracy, efficiency, and ethical considerations.


<details>
  <summary>Details</summary>
Motivation: To enhance medical decision-making by integrating hospital-specific data with LLMs, ensuring accuracy and efficiency while addressing ethical concerns.

Method: Uses Llama 3.2-3B-Instruct with RAG for context retrieval and QLoRA for parameter efficiency and memory optimization.

Result: Improves response accuracy and performs well on medical benchmarks, enabling basic medical suggestions.

Conclusion: LLMs show promise in healthcare but require ethical handling and further research for real-world integration.

Abstract: This research paper investigates the application of Large Language Models
(LLMs) in healthcare, specifically focusing on enhancing medical decision
support through Retrieval-Augmented Generation (RAG) integrated with
hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation
(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By
embedding and retrieving context-relevant healthcare information, the system
significantly improves response accuracy. QLoRA facilitates notable parameter
efficiency and memory optimization, preserving the integrity of medical
information through specialized quantization techniques. Our research also
shows that our model performs relatively well on various medical benchmarks,
indicating that it can be used to make basic medical suggestions. This paper
details the system's technical components, including its architecture,
quantization methods, and key healthcare applications such as enhanced disease
prediction from patient symptoms and medical history, treatment suggestions,
and efficient summarization of complex medical reports. We touch on the ethical
considerations-patient privacy, data security, and the need for rigorous
clinical validation-as well as the practical challenges of integrating such
systems into real-world healthcare workflows. Furthermore, the lightweight
quantized weights ensure scalability and ease of deployment even in
low-resource hospital environments. Finally, the paper concludes with an
analysis of the broader impact of LLMs on healthcare and outlines future
directions for LLMs in medical settings.

</details>


### [23] [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/pdf/2505.03427)
*Mouath Abu Daoud, Chaimae Abouzahir, Leen Kharouf, Walid Al-Eisawi, Nizar Habash, Farah E. Shamout*

Main category: cs.CL

TL;DR: The paper introduces MedArabiQ, a benchmark dataset for evaluating LLMs in the Arabic medical domain, addressing the lack of existing resources.


<details>
  <summary>Details</summary>
Motivation: The efficacy of LLMs in the Arabic medical domain is unexplored due to missing high-quality datasets and benchmarks.

Method: Constructed MedArabiQ using medical exams and public datasets, then evaluated five LLMs, including GPT-4o and Claude 3.5-Sonnet.

Result: Findings emphasize the need for multilingual benchmarks to ensure fair LLM deployment in healthcare.

Conclusion: MedArabiQ provides a foundation for future research on multilingual LLM capabilities in healthcare.

Abstract: Large Language Models (LLMs) have demonstrated significant promise for
various applications in healthcare. However, their efficacy in the Arabic
medical domain remains unexplored due to the lack of high-quality
domain-specific datasets and benchmarks. This study introduces MedArabiQ, a
novel benchmark dataset consisting of seven Arabic medical tasks, covering
multiple specialties and including multiple choice questions,
fill-in-the-blank, and patient-doctor question answering. We first constructed
the dataset using past medical exams and publicly available datasets. We then
introduced different modifications to evaluate various LLM capabilities,
including bias mitigation. We conducted an extensive evaluation with five
state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude
3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of
new high-quality benchmarks that span different languages to ensure fair
deployment and scalability of LLMs in healthcare. By establishing this
benchmark and releasing the dataset, we provide a foundation for future
research aimed at evaluating and enhancing the multilingual capabilities of
LLMs for the equitable use of generative AI in healthcare.

</details>


### [24] [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/pdf/2505.03452)
*Matan Orbach, Ohad Eytan, Benjamin Sznajder, Ariel Gera, Odellia Boni, Yoav Kantor, Gal Bloch, Omri Levy, Hadas Abraham, Nitzan Barzilay, Eyal Shnarch, Michael E. Factor, Shila Ofek-Koifman, Paula Ta-Shma, Assaf Toledo*

Main category: cs.CL

TL;DR: The paper benchmarks RAG hyper-parameter optimization (HPO) methods, showing greedy or iterative random search can efficiently boost performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: The complexity and cost of finding optimal RAG configurations motivate the need for rigorous benchmarking of HPO frameworks.

Method: The study evaluates 5 HPO algorithms on 5 datasets, including a new real-world product documentation dataset, using the largest HPO search space to date.

Result: RAG HPO significantly improves performance; greedy approaches benefit from optimizing models first rather than following the RAG pipeline order.

Conclusion: Efficient RAG HPO is achievable with greedy or iterative random search, and prioritizing model optimization enhances performance.

Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a
given use case can be complex and expensive. Motivated by this challenge,
frameworks for RAG hyper-parameter optimization (HPO) have recently emerged,
yet their effectiveness has not been rigorously benchmarked. To address this
gap, we present a comprehensive study involving 5 HPO algorithms over 5
datasets from diverse domains, including a new one collected for this work on
real-world product documentation. Our study explores the largest HPO search
space considered to date, with two optimized evaluation metrics. Analysis of
the results shows that RAG HPO can be done efficiently, either greedily or with
iterative random search, and that it significantly boosts RAG performance for
all datasets. For greedy HPO approaches, we show that optimizing models first
is preferable to the prevalent practice of optimizing sequentially according to
the RAG pipeline order.

</details>


### [25] [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/pdf/2505.03467)
*Shuang Zhou, Jiashuo Wang, Zidu Xu, Song Wang, David Brauer, Lindsay Welton, Jacob Cogan, Yuen-Hei Chung, Lei Tian, Zaifu Zhan, Yu Hou, Mingquan Lin, Genevieve B. Melton, Rui Zhang*

Main category: cs.CL

TL;DR: ConfiDx, an uncertainty-aware LLM, improves disease diagnosis by identifying and explaining diagnostic uncertainties, enhancing reliability.


<details>
  <summary>Details</summary>
Motivation: Diagnostic uncertainty in clinical notes can lead to misdiagnosis. Existing systems lack explicit uncertainty identification and explanation.

Method: Fine-tuned open-source LLMs (ConfiDx) with diagnostic criteria and annotated datasets capturing diagnostic ambiguity.

Result: ConfiDx excelled in identifying uncertainties, achieving superior diagnostic performance and generating trustworthy explanations.

Conclusion: ConfiDx is the first to jointly address uncertainty recognition and explanation, significantly improving diagnostic system reliability.

Abstract: Explainable disease diagnosis, which leverages patient information (e.g.,
signs and symptoms) and computational models to generate probable diagnoses and
reasonings, offers clear clinical values. However, when clinical notes
encompass insufficient evidence for a definite diagnosis, such as the absence
of definitive symptoms, diagnostic uncertainty usually arises, increasing the
risk of misdiagnosis and adverse outcomes. Although explicitly identifying and
explaining diagnostic uncertainties is essential for trustworthy diagnostic
systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an
uncertainty-aware large language model (LLM) created by fine-tuning open-source
LLMs with diagnostic criteria. We formalized the task and assembled richly
annotated datasets that capture varying degrees of diagnostic ambiguity.
Evaluating ConfiDx on real-world datasets demonstrated that it excelled in
identifying diagnostic uncertainties, achieving superior diagnostic
performance, and generating trustworthy explanations for diagnoses and
uncertainties. To our knowledge, this is the first study to jointly address
diagnostic uncertainty recognition and explanation, substantially enhancing the
reliability of automatic diagnostic systems.

</details>


### [26] [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/pdf/2505.03469)
*Bin Yu, Hang Yuan, Yuliang Wei, Bailing Wang, Weizhen Qi, Kai Chen*

Main category: cs.CL

TL;DR: LS-Mixture SFT combines long and short CoT reasoning data to improve model accuracy and reduce response length, addressing the 'overthinking' problem in fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: To transfer reasoning capabilities to non-reasoning models without inheriting verbose reasoning chains from teacher models.

Method: Proposes LS-Mixture SFT, mixing long CoT reasoning data with short, rewritten counterparts for fine-tuning.

Result: Achieves 2.3% higher accuracy and reduces response length by ~47.61% compared to direct SFT.

Conclusion: LS-Mixture SFT effectively enhances reasoning in fine-tuned models while avoiding overthinking issues.

Abstract: Recent advances in large language models have demonstrated that Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from
large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning
capabilities to non-reasoning models. However, models fine-tuned with this
approach inherit the "overthinking" problem from teacher models, producing
verbose and redundant reasoning chains during inference. To address this
challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought
\textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning
(\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their
short counterparts obtained through structure-preserved rewriting. Our
experiments demonstrate that models trained using the LS-Mixture SFT method,
compared to those trained with direct SFT, achieved an average accuracy
improvement of 2.3\% across various benchmarks while substantially reducing
model response length by approximately 47.61\%. This work offers an approach to
endow non-reasoning models with reasoning capabilities through supervised
fine-tuning while avoiding the inherent overthinking problems inherited from
teacher models, thereby enabling efficient reasoning in the fine-tuned models.

</details>


### [27] [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/pdf/2505.03473)
*Marta Boscariol, Luana Bulla, Lia Draetta, Beatrice Fiumanò, Emanuele Lenzi, Leonardo Piano*

Main category: cs.CL

TL;DR: The paper evaluates GPT and LLama3 for long-tail Entity Linking (EL) using MHERCL v0.1, showing promising results compared to ReLiK.


<details>
  <summary>Details</summary>
Motivation: Long-tail EL is understudied, and LLMs' potential for this task is unexplored despite their contextual understanding.

Method: Assessed GPT and LLama3 on MHERCL v0.1, comparing their EL performance to ReLiK.

Result: LLMs perform well in long-tail EL, bridging the gap between head and long-tail entity linking.

Conclusion: LLMs can effectively supplement traditional methods for long-tail EL, offering new possibilities.

Abstract: Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)
applications, enabling the disambiguation of entity mentions by linking them to
their corresponding entries in a reference knowledge base (KB). Thanks to their
deep contextual understanding capabilities, LLMs offer a new perspective to
tackle EL, promising better results than traditional methods. Despite the
impressive generalization capabilities of LLMs, linking less popular, long-tail
entities remains challenging as these entities are often underrepresented in
training data and knowledge bases. Furthermore, the long-tail EL task is an
understudied problem, and limited studies address it with LLMs. In the present
work, we assess the performance of two popular LLMs, GPT and LLama3, in a
long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated
benchmark of sentences from domain-specific historical texts, we quantitatively
compare the performance of LLMs in identifying and linking entities to their
corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity
Linking and Relation Extraction framework. Our preliminary experiments reveal
that LLMs perform encouragingly well in long-tail EL, indicating that this
technology can be a valuable adjunct in filling the gap between head and
long-tail EL.

</details>


### [28] [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/pdf/2505.03481)
*Maciej Zembrzuski, Saad Mahamood*

Main category: cs.CL

TL;DR: A new hybrid approach combining extractive and abstractive summarization with pre-trained embeddings outperforms existing methods for large input datasets, especially in summarizing user reviews.


<details>
  <summary>Details</summary>
Motivation: Existing neural network-based methods struggle with large input datasets, particularly in document summarization tasks like summarizing user reviews.

Method: Combines extractive summarization with pre-trained sentence embeddings and an abstractive model, focusing on predicting sentence-level embeddings for summaries.

Result: Outperforms existing methods for large input datasets and improves summary quality by predicting sentence embeddings instead of probability distributions.

Conclusion: The hybrid approach enhances summarization quality for loosely aligned corpora, offering a better solution for large-scale input summarization.

Abstract: Current neural network-based methods to the problem of document summarisation
struggle when applied to datasets containing large inputs. In this paper we
propose a new approach to the challenge of content-selection when dealing with
end-to-end summarisation of user reviews of accommodations. We show that by
combining an extractive approach with externally pre-trained sentence level
embeddings in an addition to an abstractive summarisation model we can
outperform existing methods when this is applied to the task of summarising a
large input dataset. We also prove that predicting sentence level embedding of
a summary increases the quality of an end-to-end system for loosely aligned
source to target corpora, than compared to commonly predicting probability
distributions of sentence selection.

</details>


### [29] [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/pdf/2505.03531)
*Haoqi Yang, Luohe Shi, Qiwei Li, Zuchao Li, Ping Wang, Bo Du, Mengjia Shen, Hai Zhao*

Main category: cs.CL

TL;DR: Fine-grained MoE models offer efficiency gains under certain service loads, with activated expert reduction improving efficiency and minimal performance loss, while total expert reduction harms performance.


<details>
  <summary>Details</summary>
Motivation: To explore the efficiency dynamics of fine-grained MoE models under varying service loads and the impact of expert reduction on efficiency-performance trade-offs.

Method: Analyzed fine-grained MoE models, focusing on reducing activated and total experts to evaluate efficiency and performance trade-offs.

Result: Reducing activated experts improves efficiency with minor performance loss; reducing total experts harms performance with limited efficiency gains. Throughput increased by 10% without performance loss.

Conclusion: MoE inference optimization has significant potential, with fine-grained models offering notable efficiency improvements under specific conditions.

Abstract: Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually
becoming the mainstream approach for ultra-large-scale models. Existing
optimization efforts for MoE models have focused primarily on coarse-grained
MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE
models are gaining popularity, yet research on them remains limited. Therefore,
we want to discuss the efficiency dynamic under different service loads.
Additionally, fine-grained models allow deployers to reduce the number of
routed experts, both activated counts and total counts, raising the question of
how this reduction affects the trade-off between MoE efficiency and
performance. Our findings indicate that while deploying MoE models presents
greater challenges, it also offers significant optimization opportunities.
Reducing the number of activated experts can lead to substantial efficiency
improvements in certain scenarios, with only minor performance degradation.
Reducing the total number of experts provides limited efficiency gains but
results in severe performance degradation. Our method can increase throughput
by at least 10\% without any performance degradation. Overall, we conclude that
MoE inference optimization remains an area with substantial potential for
exploration and improvement.

</details>


### [30] [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/pdf/2505.03563)
*Cléa Chataigner, Rebecca Ma, Prakhar Ganesh, Afaf Taïk, Elliot Creager, Golnoosh Farnadi*

Main category: cs.CL

TL;DR: Small wording changes in prompts significantly affect LLM behavior, necessitating robust evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Address concerns about LLM evaluation stability due to prompt wording variations.

Method: Propose a controlled paraphrasing framework using minimal linguistic transformations to generate natural prompt variations, validated with human annotations and automated checks.

Result: Subtle prompt modifications cause substantial changes in LLM behavior, especially in stereotype evaluation tasks.

Conclusion: Robust, paraphrase-aware evaluation protocols are needed for reliable LLM assessments.

Abstract: Small changes in how a prompt is worded can lead to meaningful differences in
the behavior of large language models (LLMs), raising concerns about the
stability and reliability of their evaluations. While prior work has explored
simple formatting changes, these rarely capture the kinds of natural variation
seen in real-world language use. We propose a controlled paraphrasing framework
based on a taxonomy of minimal linguistic transformations to systematically
generate natural prompt variations. Using the BBQ dataset, we validate our
method with both human annotations and automated checks, then use it to study
how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our
analysis shows that even subtle prompt modifications can lead to substantial
changes in model behavior. These results highlight the need for robust,
paraphrase-aware evaluation protocols.

</details>


### [31] [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/pdf/2505.03675)
*Anuja Tayal, Devika Salunke, Barbara Di Eugenio, Paula G Allen-Meares, Eulalia P Abril, Olga Garcia-Bedoya, Carolyn A Dickens, Andrew D. Boyd*

Main category: cs.CL

TL;DR: ChatGPT was used to generate self-care conversations for African-American heart failure patients, testing four prompting strategies. Results highlight the importance of prompt design but note ChatGPT's limitations in empathy and engagement.


<details>
  <summary>Details</summary>
Motivation: To address the lack of specialized datasets for African-American heart failure patients by leveraging ChatGPT for self-care dialogue generation.

Method: Four prompting strategies (domain, AAVE, SDOH, SDOH-informed reasoning) were applied to generate patient-health educator dialogues across self-care domains (food, exercise, fluid intake), varying turn lengths and SDOH attributes.

Result: Prompt design is crucial; SDOH and reasoning improved dialogue quality, but ChatGPT lacked empathy and engagement for healthcare communication.

Conclusion: While ChatGPT shows promise for generating specialized healthcare dialogues, its limitations in empathy and engagement must be addressed for meaningful use.

Abstract: We explore the potential of ChatGPT (3.5-turbo and 4) to generate
conversations focused on self-care strategies for African-American heart
failure patients -- a domain with limited specialized datasets. To simulate
patient-health educator dialogues, we employed four prompting strategies:
domain, African American Vernacular English (AAVE), Social Determinants of
Health (SDOH), and SDOH-informed reasoning. Conversations were generated across
key self-care domains of food, exercise, and fluid intake, with varying turn
lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as
age, gender, neighborhood, and socioeconomic status. Our findings show that
effective prompt design is essential. While incorporating SDOH and reasoning
improves dialogue quality, ChatGPT still lacks the empathy and engagement
needed for meaningful healthcare communication.

</details>


### [32] [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/pdf/2505.03688)
*Sharvi Endait, Ruturaj Ghatage, Aditya Kulkarni, Rajlaxmi Patil, Raviraj Joshi*

Main category: cs.CL

TL;DR: IndicSQuAD is a multilingual QA dataset for nine Indic languages, derived from SQuAD, addressing underrepresentation. It includes training, validation, and test sets, with baseline evaluations using BERT models, highlighting challenges in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation of Indic languages in QA systems despite their large speaker base.

Method: Adapts and extends translation techniques from SQuAD to create IndicSQuAD, ensuring linguistic fidelity and answer-span alignment. Evaluated using monolingual and multilingual BERT models.

Result: Baseline results show challenges in low-resource settings, with potential for future work in expanding languages, domain-specific datasets, and multimodal data.

Conclusion: IndicSQuAD provides a robust QA dataset for Indic languages, encouraging further research and development in this underrepresented area.

Abstract: The rapid progress in question-answering (QA) systems has predominantly
benefited high-resource languages, leaving Indic languages largely
underrepresented despite their vast native speaker base. In this paper, we
present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset
covering nine major Indic languages, systematically derived from the SQuAD
dataset. Building on previous work with MahaSQuAD for Marathi, our approach
adapts and extends translation techniques to maintain high linguistic fidelity
and accurate answer-span alignment across diverse languages. IndicSQuAD
comprises extensive training, validation, and test sets for each language,
providing a robust foundation for model development. We evaluate baseline
performances using language-specific monolingual BERT models and the
multilingual MuRIL-BERT. The results indicate some challenges inherent in
low-resource settings. Moreover, our experiments suggest potential directions
for future work, including expanding to additional languages, developing
domain-specific datasets, and incorporating multimodal data. The dataset and
models are publicly shared at https://github.com/l3cube-pune/indic-nlp

</details>


### [33] [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/pdf/2505.03711)
*Baharul Islam, Nasim Ahmad, Ferdous Ahmed Barbhuiya, Kuntal Dey*

Main category: cs.CL

TL;DR: A system for cross-lingual subject classification in English and German academic domains, using bilingual data, negative sampling, and a margin-based retrieval objective, achieving competitive recall rates with minimal GPU usage.


<details>
  <summary>Details</summary>
Motivation: To address cross-lingual subject classification in academic domains, leveraging bilingual data and efficient methods for resource-constrained settings.

Method: Uses a dimension-as-token self-attention mechanism with reduced internal dimensions, bilingual training data, negative sampling, and a margin-based retrieval objective.

Result: Achieved average recall rates of 32.24% (general quantitative), 43.16%, and 31.53% (qualitative), demonstrating competitive performance with minimal GPU usage.

Conclusion: The approach effectively captures subject information under resource constraints but has room for improvement.

Abstract: We present our system submission for SemEval 2025 Task 5, which focuses on
cross-lingual subject classification in the English and German academic
domains. Our approach leverages bilingual data during training, employing
negative sampling and a margin-based retrieval objective. We demonstrate that a
dimension-as-token self-attention mechanism designed with significantly reduced
internal dimensions can effectively encode sentence embeddings for subject
retrieval. In quantitative evaluation, our system achieved an average recall
rate of 32.24% in the general quantitative setting (all subjects), 43.16% and
31.53% of the general qualitative evaluation methods with minimal GPU usage,
highlighting their competitive performance. Our results demonstrate that our
approach is effective in capturing relevant subject information under resource
constraints, although there is still room for improvement.

</details>


### [34] [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/pdf/2505.03733)
*Zimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi, Aojun Zhou, Mingjie Zhan, Hongsheng Li*

Main category: cs.CL

TL;DR: WebGen-Bench is a benchmark for evaluating LLM-based agents' ability to generate multi-file website codebases, featuring diverse instructions and 647 test cases. The best model achieved 27.8% accuracy, showing the benchmark's difficulty. Training on WebGen-Instruct improved performance to 38.2%.


<details>
  <summary>Details</summary>
Motivation: To measure and improve LLM-based agents' capabilities in generating complex, functional websites from scratch, addressing a gap in existing benchmarks.

Method: Developed WebGen-Bench with human and GPT-4o-generated instructions, created 647 test cases, and automated testing with a web-navigation agent. Evaluated three code-agent frameworks with various LLMs.

Result: Best model (Bolt.diy + DeepSeek-R1) achieved 27.8% accuracy. Training on WebGen-Instruct improved Qwen2.5-Coder-32B-Instruct to 38.2%.

Conclusion: WebGen-Bench is a challenging benchmark, and training on specialized datasets can enhance LLM-based agents' performance in website generation.

Abstract: LLM-based agents have demonstrated great potential in generating and managing
code within complex codebases. In this paper, we introduce WebGen-Bench, a
novel benchmark designed to measure an LLM-based agent's ability to create
multi-file website codebases from scratch. It contains diverse instructions for
website generation, created through the combined efforts of human annotators
and GPT-4o. These instructions span three major categories and thirteen minor
categories, encompassing nearly all important types of web applications. To
assess the quality of the generated websites, we use GPT-4o to generate test
cases targeting each functionality described in the instructions, and then
manually filter, adjust, and organize them to ensure accuracy, resulting in 647
test cases. Each test case specifies an operation to be performed on the
website and the expected result after the operation. To automate testing and
improve reproducibility, we employ a powerful web-navigation agent to execute
tests on the generated websites and determine whether the observed responses
align with the expected results. We evaluate three high-performance code-agent
frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and
open-source LLMs as engines. The best-performing combination, Bolt.diy powered
by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting
the challenging nature of our benchmark. Additionally, we construct
WebGen-Instruct, a training set consisting of 6,667 website-generation
instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories
generated from a subset of this training set achieves an accuracy of 38.2\%,
surpassing the performance of the best proprietary model.

</details>


### [35] [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/pdf/2505.03739)
*Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, Peixian Chen, Mengdan Zhang, Hang Shao, Jian Li, Jinlong Peng, Haoyu Cao, Ke Li, Rongrong Ji, Xing Sun*

Main category: cs.CL

TL;DR: VITA-Audio is an end-to-end large speech model designed to reduce latency in generating the first audio token during streaming, using a lightweight MCTP module and progressive training.


<details>
  <summary>Details</summary>
Motivation: Existing speech models suffer from high latency in generating the first audio token, hindering real-time interaction.

Method: Introduces a Multiple Cross-modal Token Prediction (MCTP) module for multi-token generation in one pass and a four-stage progressive training strategy.

Result: Achieves 3~5x inference speedup and outperforms similar-sized models on ASR, TTS, and SQA benchmarks.

Conclusion: VITA-Audio enables real-time conversational capabilities with minimal latency, using open-source data.

Abstract: With the growing requirement for natural human-computer interaction,
speech-based systems receive increasing attention as speech is one of the most
common forms of daily communication. However, the existing speech models still
experience high latency when generating the first audio token during streaming,
which poses a significant bottleneck for deployment. To address this issue, we
propose VITA-Audio, an end-to-end large speech model with fast audio-text token
generation. Specifically, we introduce a lightweight Multiple Cross-modal Token
Prediction (MCTP) module that efficiently generates multiple audio tokens
within a single model forward pass, which not only accelerates the inference
but also significantly reduces the latency for generating the first audio in
streaming scenarios. In addition, a four-stage progressive training strategy is
explored to achieve model acceleration with minimal loss of speech quality. To
our knowledge, VITA-Audio is the first multi-modal large language model capable
of generating audio output during the first forward pass, enabling real-time
conversational capabilities with minimal latency. VITA-Audio is fully
reproducible and is trained on open-source data only. Experimental results
demonstrate that our model achieves an inference speedup of 3~5x at the 7B
parameter scale, but also significantly outperforms open-source models of
similar model size on multiple benchmarks for automatic speech recognition
(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.

</details>


### [36] [Incoherent Probability Judgments in Large Language Models](https://arxiv.org/pdf/2401.16646)
*Jian-Qiao Zhu, Thomas L. Griffiths*

Main category: cs.CL

TL;DR: LLMs produce incoherent probability judgments, mirroring human-like deviations from probability theory, and exhibit an inverted-U-shaped mean-variance relationship.


<details>
  <summary>Details</summary>
Motivation: Assess the coherence of probability judgments made by autoregressive LLMs and compare them to human behavior.

Method: Use probabilistic identities and repeated judgments to evaluate LLMs' probability coherence.

Result: LLMs' judgments are often incoherent, resembling human deviations from probability theory, with an inverted-U-shaped mean-variance relationship.

Conclusion: LLMs' irrationality parallels human judgment, explained by linking them to implicit Bayesian inference and the Bayesian Sampler model.

Abstract: Autoregressive Large Language Models (LLMs) trained for next-word prediction
have demonstrated remarkable proficiency at producing coherent text. But are
they equally adept at forming coherent probability judgments? We use
probabilistic identities and repeated judgments to assess the coherence of
probability judgments made by LLMs. Our results show that the judgments
produced by these models are often incoherent, displaying human-like systematic
deviations from the rules of probability theory. Moreover, when prompted to
judge the same event, the mean-variance relationship of probability judgments
produced by LLMs shows an inverted-U-shaped like that seen in humans. We
propose that these deviations from rationality can be explained by linking
autoregressive LLMs to implicit Bayesian inference and drawing parallels with
the Bayesian Sampler model of human probability judgments.

</details>


### [37] [LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors](https://arxiv.org/pdf/2406.14498)
*Sheikh Asif Imran, Mohammad Nur Hossain Khan, Subrata Biswas, Bashima Islam*

Main category: cs.CL

TL;DR: LLaSA is a 13B model for open-ended QA on raw IMU data, supporting causal explanations and outperforming commercial LLMs.


<details>
  <summary>Details</summary>
Motivation: Current systems classify motion data but lack support for natural questions about causes or meanings.

Method: LLaSA, a compact 13B model, is introduced for ask-anything QA grounded in raw IMU data, with datasets SensorCaps, OpenSQA, and Tune-OpenSQA.

Result: LLaSA produces interpretable, causal answers and outperforms commercial LLMs.

Conclusion: LLaSA and the released datasets define a new benchmark for sensor-language models.

Abstract: Wearables generate rich motion data, yet current systems only classify what
happened - failing to support natural questions about why it happened or what
it means. We introduce LLaSA (Large Language and Sensor Assistant), a compact
13B model that enables ask-anything, open-ended question answering grounded in
raw IMU data. LLaSA supports conversational, context-aware reasoning -
explaining the causes of sensor-detected behaviors and answering free-form
questions in real-world scenarios. It is tuned for scientific accuracy,
coherence, and response reliability. To advance this new task of sensor-based
QA, we release three large-scale datasets: SensorCaps, OpenSQA, and
Tune-OpenSQA. Together, these resources define a new benchmark for
sensor-language models. LLaSA consistently produces interpretable, causal
answers and outperforms commercial LLMs across both public and real-world
settings. Our code repository and datasets can be found at
https://github.com/BASHLab/LLaSA.

</details>


### [38] [CFBench: A Comprehensive Constraints-Following Benchmark for LLMs](https://arxiv.org/pdf/2408.01122)
*Tao Zhang, Chenglin Zhu, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Tao Zhang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, Bin Cui, Wentao Zhang, Zenan Zhou*

Main category: cs.CL

TL;DR: CFBench is a benchmark for evaluating LLMs' ability to follow comprehensive and authentic constraints in real-world scenarios, featuring 1,000 samples across 200+ scenarios and 50+ NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations lack comprehensiveness and authenticity from the user's perspective, necessitating a more robust benchmark.

Method: CFBench curates real-world constraints, categorizes them systematically, and integrates multi-dimensional assessment criteria with prioritization.

Result: Evaluation shows significant gaps in LLMs' constraint-following abilities, with insights into influencing factors and improvement strategies.

Conclusion: CFBench highlights the need for better constraint adherence in LLMs and provides a framework for future enhancements.

Abstract: The adeptness of Large Language Models (LLMs) in comprehending and following
natural language instructions is critical for their deployment in sophisticated
real-world applications. Existing evaluations mainly focus on fragmented
constraints or narrow scenarios, but they overlook the comprehensiveness and
authenticity of constraints from the user's perspective. To bridge this gap, we
propose CFBench, a large-scale Comprehensive Constraints Following Benchmark
for LLMs, featuring 1,000 curated samples that cover more than 200 real-life
scenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from
real-world instructions and constructs an innovative systematic framework for
constraint types, which includes 10 primary categories and over 25
subcategories, and ensures each constraint is seamlessly integrated within the
instructions. To make certain that the evaluation of LLM outputs aligns with
user perceptions, we propose an advanced methodology that integrates
multi-dimensional assessment criteria with requirement prioritization, covering
various perspectives of constraints, instructions, and requirement fulfillment.
Evaluating current leading LLMs on CFBench reveals substantial room for
improvement in constraints following, and we further investigate influencing
factors and enhancement strategies. The data and code are publicly available at
https://github.com/PKU-Baichuan-MLSystemLab/CFBench

</details>


### [39] [LLM-3D Print: Large Language Models To Monitor and Control 3D Printing](https://arxiv.org/pdf/2408.14307)
*Yayati Jadhav, Peter Pak, Amir Barati Farimani*

Main category: cs.CL

TL;DR: A framework using LLMs for automated defect detection and correction in FDM 3D printing, outperforming human experts.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current error detection methods in FDM, which lack generalizability and require extensive labeled data.

Method: Leverages pre-trained LLMs to analyze print images, identify defects, query printer parameters, and execute corrective actions.

Result: LLM-based agents accurately detect and autonomously correct common printing errors, outperforming human engineers.

Conclusion: The framework enhances scalability and adaptability in 3D printing, reducing reliance on human intervention.

Abstract: Industry 4.0 has revolutionized manufacturing by driving digitalization and
shifting the paradigm toward additive manufacturing (AM). Fused Deposition
Modeling (FDM), a key AM technology, enables the creation of highly customized,
cost-effective products with minimal material waste through layer-by-layer
extrusion, posing a significant challenge to traditional subtractive methods.
However, the susceptibility of material extrusion techniques to errors often
requires expert intervention to detect and mitigate defects that can severely
compromise product quality. While automated error detection and machine
learning models exist, their generalizability across diverse 3D printer setups,
firmware, and sensors is limited, and deep learning methods require extensive
labeled datasets, hindering scalability and adaptability. To address these
challenges, we present a process monitoring and control framework that
leverages pre-trained Large Language Models (LLMs) alongside 3D printers to
detect and address printing defects. The LLM evaluates print quality by
analyzing images captured after each layer or print segment, identifying
failure modes and querying the printer for relevant parameters. It then
generates and executes a corrective action plan. We validated the effectiveness
of the proposed framework in identifying defects by comparing it against a
control group of engineers with diverse AM expertise. Our evaluation
demonstrated that LLM-based agents not only accurately identify common 3D
printing errors, such as inconsistent extrusion, stringing, warping, and layer
adhesion, but also effectively determine the parameters causing these failures
and autonomously correct them without any need for human intervention.

</details>


### [40] [Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution](https://arxiv.org/pdf/2410.00153)
*Haiyan Zhao, Heng Zhao, Bo Shen, Ali Payani, Fan Yang, Mengnan Du*

Main category: cs.CL

TL;DR: The paper introduces Gaussian Concept Subspace (GCS) to robustly represent concepts in LLMs, improving over single-vector methods by using subspaces. It demonstrates GCS's effectiveness in faithfulness, plausibility, and real-world tasks like emotion steering.


<details>
  <summary>Details</summary>
Motivation: Single-vector representations of concepts in LLMs are inconsistent and less robust, limiting their practical utility.

Method: Extends linear probing classifiers to approximate concept subspaces (GCS) and evaluates it on faithfulness, plausibility, and real-world tasks.

Result: GCS outperforms single-vector methods, balancing steering performance and fluency in language generation.

Conclusion: GCS provides a more robust and effective way to represent concepts in LLMs, with practical applications like emotion steering.

Abstract: Probing learned concepts in large language models (LLMs) is crucial for
understanding how semantic knowledge is encoded internally. Training linear
classifiers on probing tasks is a principle approach to denote the vector of a
certain concept in the representation space. However, the single vector
identified for a concept varies with both data and training, making it less
robust and weakening its effectiveness in real-world applications. To address
this challenge, we propose an approach to approximate the subspace representing
a specific concept. Built on linear probing classifiers, we extend the concept
vectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's
effectiveness through measuring its faithfulness and plausibility across
multiple LLMs with different sizes and architectures. Additionally, we use
representation intervention tasks to showcase its efficacy in real-world
applications such as emotion steering. Experimental results indicate that GCS
concept vectors have the potential to balance steering performance and
maintaining the fluency in natural language generation tasks.

</details>


### [41] [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](https://arxiv.org/pdf/2410.09580)
*Hanwen Du, Bo Peng, Xia Ning*

Main category: cs.CL

TL;DR: SAPIENT introduces a Monte Carlo Tree Search (MCTS)-based framework for Conversational Recommender Systems (CRS) to improve conversational planning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based CRS methods suffer from suboptimal conversational planning due to greedy action selection or sampling strategies.

Method: SAPIENT uses MCTS for conversational planning, with a self-training loop between a conversational agent (S-agent) and planner (S-planner). An efficient variant, SAPIENT-e, balances training efficiency and performance.

Result: Experiments on four datasets show SAPIENT outperforms state-of-the-art baselines.

Conclusion: SAPIENT effectively enhances conversational planning in CRS, validated by superior performance.

Abstract: Conversational Recommender Systems (CRS) proactively engage users in
interactive dialogues to elicit user preferences and provide personalized
recommendations. Existing methods train Reinforcement Learning (RL)-based agent
with greedy action selection or sampling strategy, and may suffer from
suboptimal conversational planning. To address this, we present a novel Monte
Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a
conversational agent (S-agent) and a conversational planner (S-planner).
S-planner builds a conversational search tree with MCTS based on the initial
actions proposed by S-agent to find conversation plans. The best conversation
plans from S-planner are used to guide the training of S-agent, creating a
self-training loop where S-agent can iteratively improve its capability for
conversational planning. Furthermore, we propose an efficient variant SAPIENT-e
for trade-off between training efficiency and performance. Extensive
experiments on four benchmark datasets validate the effectiveness of our
approach, showing that SAPIENT outperforms the state-of-the-art baselines.

</details>


### [42] [Personalization of Large Language Models: A Survey](https://arxiv.org/pdf/2411.00027)
*Zhehao Zhang, Ryan A. Rossi, Branislav Kveton, Yijia Shao, Diyi Yang, Hamed Zamani, Franck Dernoncourt, Joe Barrow, Tong Yu, Sungchul Kim, Ruiyi Zhang, Jiuxiang Gu, Tyler Derr, Hongjie Chen, Junda Wu, Xiang Chen, Zichao Wang, Subrata Mitra, Nedim Lipka, Nesreen Ahmed, Yu Wang*

Main category: cs.CL

TL;DR: The paper introduces a taxonomy to bridge the gap between personalized text generation and LLM-based downstream applications, formalizing foundations and proposing systematic taxonomies for personalization in LLMs.


<details>
  <summary>Details</summary>
Motivation: To unify and clarify the fragmented research on personalized LLMs by addressing the gap between text generation and downstream applications.

Method: Introduces a taxonomy for personalized LLM usage, formalizes foundations, and proposes systematic taxonomies for granularity, techniques, datasets, evaluation, and applications.

Result: A unified framework for understanding personalized LLMs, highlighting key differences, challenges, and open problems.

Conclusion: The paper provides a comprehensive guide to personalized LLM research, empowering researchers and practitioners with clear taxonomies and insights.

Abstract: Personalization of Large Language Models (LLMs) has recently become
increasingly important with a wide range of applications. Despite the
importance and recent progress, most existing works on personalized LLMs have
focused either entirely on (a) personalized text generation or (b) leveraging
LLMs for personalization-related downstream applications, such as
recommendation systems. In this work, we bridge the gap between these two
separate main directions for the first time by introducing a taxonomy for
personalized LLM usage and summarizing the key differences and challenges. We
provide a formalization of the foundations of personalized LLMs that
consolidates and expands notions of personalization of LLMs, defining and
discussing novel facets of personalization, usage, and desiderata of
personalized LLMs. We then unify the literature across these diverse fields and
usage scenarios by proposing systematic taxonomies for the granularity of
personalization, personalization techniques, datasets, evaluation methods, and
applications of personalized LLMs. Finally, we highlight challenges and
important open problems that remain to be addressed. By unifying and surveying
recent research using the proposed taxonomies, we aim to provide a clear guide
to the existing literature and different facets of personalization in LLMs,
empowering both researchers and practitioners.

</details>


### [43] [LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment](https://arxiv.org/pdf/2412.18135)
*Binrui Zeng, Bin Ji, Xiaodong Liu, Jie Yu, Shasha Li, Jun Ma, Xiaopeng Li, Shangwen Wang, Xinran Hong, Yongtao Tang*

Main category: cs.CL

TL;DR: LSAQ proposes adaptive quantization for LLMs on edge devices by dynamically adjusting precision based on layer importance, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods lack adaptability for diverse edge device resources, limiting LLM deployment.

Method: LSAQ evaluates layer importance using Jaccard similarity of top-k tokens and adjusts quantization precision accordingly.

Result: LSAQ outperforms baselines in perplexity and zero-shot tasks, enabling flexible deployment.

Conclusion: LSAQ effectively adapts quantization for edge devices, enhancing LLM deployment efficiency.

Abstract: As Large Language Models (LLMs) demonstrate exceptional performance across
various domains, deploying LLMs on edge devices has emerged as a new trend.
Quantization techniques, which reduce the size and memory requirements of LLMs,
are effective for deploying LLMs on resource-limited edge devices. However,
existing one-size-fits-all quantization methods often fail to dynamically
adjust the memory requirements of LLMs, limiting their applications to
practical edge devices with various computation resources. To tackle this
issue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for
adaptive quantization and dynamic deployment of LLMs based on layer importance.
Specifically, LSAQ evaluates the importance of LLMs' neural layers by
constructing top-k token sets from the inputs and outputs of each layer and
calculating their Jaccard similarity. Based on layer importance, our system
adaptively adjusts quantization strategies in real time according to the
computation resource of edge devices, which applies higher quantization
precision to layers with higher importance, and vice versa. {Experimental
results show that LSAQ consistently outperforms the selected quantization
baselines in terms of perplexity and zero-shot tasks. Additionally, it can
devise appropriate quantization schemes for different usage scenarios to
facilitate the deployment of LLMs.

</details>


### [44] [Self-reflecting Large Language Models: A Hegelian Dialectical Approach](https://arxiv.org/pdf/2501.14917)
*Sara Abdali, Can Goksen, Saeed Amizadeh, Julie E. Maybee, Kazuhito Koishida*

Main category: cs.CL

TL;DR: The paper proposes a Hegelian Dialectic-inspired method for LLM self-reflection, dynamic annealing for temperature control, and MAMV for idea validation, showing improved idea generation and reasoning.


<details>
  <summary>Details</summary>
Motivation: To bridge computational NLP with classical philosophy and enhance LLMs' self-reflection and creativity.

Method: Uses Hegelian Dialectic for self-reflection, dynamic annealing for temperature control, and MAMV for idea validation.

Result: Promising results in generating novel ideas and improving LLM reasoning.

Conclusion: The approach effectively enhances LLM creativity and problem-solving, validated by MAMV.

Abstract: Investigating NLP through a philosophical lens has recently caught
researcher's eyes as it connects computational methods with classical schools
of philosophy. This paper introduces a philosophical approach inspired by the
\textit{Hegelian Dialectic} for LLMs' \textit{self-reflection}, utilizing a
self-dialectical approach to emulate internal critiques and then synthesize new
ideas by resolving the opposing points of view. Moreover, this paper
investigates the effect of LLMs' temperature for generation by establishing a
dynamic annealing approach, which promotes the creativity in the early stages
and gradually refines it by focusing on the nuances, as well as a
fixed-temperature strategy for generation. We assess the effectiveness of our
proposed method in generating novel ideas and in improving the reasoning
abilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent
Majority Voting (MAMV) strategy to assess the validity and novelty of the
generated ideas, which proves useful in the absence of domain experts. Our
experiments demonstrate promising results in generating ideas and enhancing
problem-solving performance.

</details>


### [45] [Predicting potentially abusive clauses in Chilean terms of services with natural language processing](https://arxiv.org/pdf/2502.00865)
*Christoffer Loeffler, Andrea Martínez Freile, Tomás Rey Pizarro*

Main category: cs.CL

TL;DR: The study tackles information asymmetry in consumer contracts by analyzing Spanish-language Terms of Service in Chile, introducing a new dataset and methodology. Transformer models show varied performance in detecting and classifying abusive clauses.


<details>
  <summary>Details</summary>
Motivation: Address the gap in research focused on English and major jurisdictions by analyzing Spanish-language contracts in Chile.

Method: Proposes a novel annotation scheme (4 categories, 20 classes) applied to 50 Chilean Terms of Service. Evaluates transformer models for detection and classification tasks.

Result: Detection task achieves macro-F1 scores of 79%-89% and micro-F1 up to 96%. Classification task scores 60%-70% macro-F1 and 64%-80% micro-F1.

Conclusion: First Spanish-language multi-label dataset for legal clauses, paving the way for future research and consumer support in Latin America.

Abstract: This study addresses the growing concern of information asymmetry in consumer
contracts, exacerbated by the proliferation of online services with complex
Terms of Service that are rarely even read. Even though research on automatic
analysis methods is conducted, the problem is aggravated by the general focus
on English-language Machine Learning approaches and on major jurisdictions,
such as the European Union. We introduce a new methodology and a substantial
dataset addressing this gap. We propose a novel annotation scheme with four
categories and a total of 20 classes, and apply it on 50 online Terms of
Service used in Chile. Our evaluation of transformer-based models highlights
how factors like language- and/or domain-specific pre-training, few-shot sample
size, and model architecture affect the detection and classification of
potentially abusive clauses. Results show a large variability in performance
for the different tasks and models, with the highest macro-F1 scores for the
detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while
macro-F1 scores for the classification task range from 60% to 70% and micro-F1
scores from 64% to 80%. Notably, this is the first Spanish-language multi-label
classification dataset for legal clauses, applying Chilean law and offering a
comprehensive evaluation of Spanish-language models in the legal domain. Our
work lays the ground for future research in method development for rarely
considered legal analysis and potentially leads to practical applications to
support consumers in Chile and Latin America as a whole.

</details>


### [46] [Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?](https://arxiv.org/pdf/2502.07963)
*Hye Sun Yun, Karen Y. C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace*

Main category: cs.CL

TL;DR: LLMs are more susceptible to spin in medical abstracts than humans but can recognize and mitigate it with proper prompting.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs, like humans, are influenced by spin in medical research abstracts, given their growing role in synthesizing medical evidence.

Method: Evaluated 22 LLMs to assess their susceptibility to spin and ability to recognize and mitigate it.

Result: LLMs are more affected by spin than humans but can detect and reduce its impact when prompted appropriately.

Conclusion: While LLMs are vulnerable to spin, they can be guided to minimize its influence, highlighting the need for careful use in medical evidence synthesis.

Abstract: Medical research faces well-documented challenges in translating novel
treatments into clinical practice. Publishing incentives encourage researchers
to present "positive" findings, even when empirical results are equivocal.
Consequently, it is well-documented that authors often spin study results,
especially in article abstracts. Such spin can influence clinician
interpretation of evidence and may affect patient care decisions. In this
study, we ask whether the interpretation of trial results offered by Large
Language Models (LLMs) is similarly affected by spin. This is important since
LLMs are increasingly being used to trawl through and synthesize published
medical evidence. We evaluated 22 LLMs and found that they are across the board
more susceptible to spin than humans. They might also propagate spin into their
outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into
plain language summaries that they generate. We also find, however, that LLMs
are generally capable of recognizing spin, and can be prompted in a way to
mitigate spin's impact on LLM outputs.

</details>


### [47] [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/pdf/2502.13685)
*Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng*

Main category: cs.CL

TL;DR: MoM introduces multiple independent memory states to enhance recall performance while maintaining linear complexity, outperforming existing linear sequence models.


<details>
  <summary>Details</summary>
Motivation: Addressing the suboptimal performance of linear sequence models on recall-intensive tasks due to single fixed-size memory states.

Method: Uses multiple memory states with a router network to direct input tokens, minimizing interference and enhancing capacity.

Result: MoM outperforms linear sequence models on recall tasks and matches Transformer performance, with linear training and constant inference complexity.

Conclusion: MoM offers a scalable, efficient solution for recall-intensive tasks, combining neuroscience-inspired design with computational efficiency.

Abstract: Linear sequence modeling methods, such as linear attention, state space
modeling, and linear RNNs, offer significant efficiency improvements by
reducing the complexity of training and inference. However, these methods
typically compress the entire input sequence into a single fixed-size memory
state, which leads to suboptimal performance on recall-intensive downstream
tasks. Drawing inspiration from neuroscience, particularly the brain's ability
to maintain robust long-term memory while mitigating "memory interference", we
introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes
multiple independent memory states, with a router network directing input
tokens to specific memory states. This approach greatly enhances the overall
memory capacity while minimizing memory interference. As a result, MoM performs
exceptionally well on recall-intensive tasks, surpassing existing linear
sequence modeling techniques. Despite incorporating multiple memory states, the
computation of each memory state remains linear in complexity, allowing MoM to
retain the linear-complexity advantage during training, while
constant-complexity during inference. Our experimental results show that MoM
significantly outperforms current linear sequence models on downstream language
tasks, particularly recall-intensive tasks, and even achieves performance
comparable to Transformer models. The code is released at
https://github.com/OpenSparseLLMs/MoM and is also released as a part of
https://github.com/OpenSparseLLMs/Linear-MoE.

</details>


### [48] [English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports](https://arxiv.org/pdf/2502.14338)
*Avinash Patil, Aryan Jadon*

Main category: cs.CL

TL;DR: The study evaluates machine translation (MT) performance on bug reports, comparing tools like DeepL, AWS Translate, and LLMs (ChatGPT, Claude, Gemini, etc.) using metrics like BLEU and BERTScore. ChatGPT excels in translation quality, while Claude and Mistral lead in source language identification. No single tool dominates all tasks.


<details>
  <summary>Details</summary>
Motivation: Accurate translation of bug reports is crucial for global software development collaboration, necessitating a comprehensive evaluation of MT tools.

Method: The study evaluates MT tools (DeepL, AWS Translate, ChatGPT, etc.) using bug reports from the Visual Studio Code GitHub repository. Metrics include BLEU, BERTScore, and classification metrics (accuracy, F1-score).

Result: ChatGPT (gpt-4o) leads in translation quality but not in source language identification. Claude and Mistral perform best in F1-score, while AWS Translate excels in accuracy.

Conclusion: No single MT tool outperforms others in all tasks, emphasizing the need for task-specific evaluations and domain adaptation for technical content.

Abstract: Accurate translation of bug reports is critical for efficient collaboration
in global software development. In this study, we conduct the first
comprehensive evaluation of machine translation (MT) performance on bug
reports, analyzing the capabilities of DeepL, AWS Translate, and large language
models such as ChatGPT, Claude, Gemini, LLaMA, and Mistral using data from the
Visual Studio Code GitHub repository, specifically focusing on reports labeled
with the english-please tag. To assess both translation quality and source
language identification accuracy, we employ a range of MT evaluation
metrics-including BLEU, BERTScore, COMET, METEOR, and ROUGE-alongside
classification metrics such as accuracy, precision, recall, and F1-score. Our
findings reveal that while ChatGPT (gpt-4o) excels in semantic and lexical
translation quality, it does not lead in source language identification. Claude
and Mistral achieve the highest F1-scores (0.7182 and 0.7142, respectively),
and Gemini records the best precision (0.7414). AWS Translate shows the highest
accuracy (0.4717) in identifying source languages. These results highlight that
no single system dominates across all tasks, reinforcing the importance of
task-specific evaluations. This study underscores the need for domain
adaptation when translating technical content and provides actionable insights
for integrating MT into bug-triaging workflows. The code and dataset for this
paper are available at GitHub-https://github.com/av9ash/English-Please

</details>


### [49] [BIG-Bench Extra Hard](https://arxiv.org/pdf/2502.19187)
*Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat*

Main category: cs.CL

TL;DR: The paper introduces BIG-Bench Extra Hard (BBEH), a new benchmark to evaluate advanced reasoning in LLMs, as current benchmarks like BIG-Bench and BBH are saturated.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks (BIG-Bench, BBH) are no longer challenging for state-of-the-art LLMs, limiting their utility for evaluating broader reasoning skills.

Method: BBEH replaces tasks in BBH with novel, harder tasks probing similar reasoning capabilities.

Result: Best models achieved 9.8% (general-purpose) and 44.8% (specialized) accuracy, showing significant room for improvement.

Conclusion: BBEH highlights the ongoing challenge of robust general reasoning in LLMs and provides a new benchmark for future evaluations.

Abstract: Large language models (LLMs) are increasingly deployed in everyday
applications, demanding robust general reasoning capabilities and diverse
reasoning skillset. However, current LLM reasoning benchmarks predominantly
focus on mathematical and coding abilities, leaving a gap in evaluating broader
reasoning proficiencies. One particular exception is the BIG-Bench dataset,
which has served as a crucial benchmark for evaluating the general reasoning
capabilities of LLMs, thanks to its diverse set of challenging tasks that
allowed for a comprehensive assessment of general reasoning across various
skills within a unified framework. However, recent advances in LLMs have led to
saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).
State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus
diminishing its utility. To address this limitation, we introduce BIG-Bench
Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM
reasoning evaluation. BBEH replaces each task in BBH with a novel task that
probes a similar reasoning capability but exhibits significantly increased
difficulty. We evaluate various models on BBEH and observe a (harmonic) average
accuracy of 9.8\% for the best general-purpose model and 44.8\% for the best
reasoning-specialized model, indicating substantial room for improvement and
highlighting the ongoing challenge of achieving robust general reasoning in
LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.

</details>


### [50] [CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models](https://arxiv.org/pdf/2503.10707)
*Zhiyuan Wang, Katharine E. Daniel, Laura E. Barnes, Philip I. Chow*

Main category: cs.CL

TL;DR: The paper introduces CALLM, a context-aware framework using LLMs and RAG to analyze cancer survivors' mobile diary entries for emotional states and intervention opportunities, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Cancer survivors face emotional challenges, and mobile diaries offer a way to track emotions and improve well-being, but current tools lack contextual understanding.

Method: The study analyzes diary entries from 407 cancer survivors, proposing CALLM, a framework combining LLMs and RAG to integrate peer experiences and personal history for better emotion analysis.

Result: CALLM achieves balanced accuracies of 72.96% (positive affect), 73.29% (negative affect), 73.72% (emotion regulation desire), and 60.09% (intervention availability), outperforming baselines.

Conclusion: Contextual information in mobile diaries can effectively predict emotional states and identify optimal intervention moments for personalized support.

Abstract: Cancer survivors face unique emotional challenges that impact their quality
of life. Mobile diary entries provide a promising method for tracking emotional
states, improving self-awareness, and promoting well-being outcome. This paper
aims to, through mobile diaries, understand cancer survivors' emotional states
and key variables related to just-in-time intervention opportunities, including
the desire to regulate emotions and the availability to engage in
interventions. Although emotion analysis tools show potential for recognizing
emotions from text, current methods lack the contextual understanding necessary
to interpret brief mobile diary narratives. Our analysis of diary entries from
cancer survivors (N=407) reveals systematic relationships between described
contexts and emotional states, with administrative and health-related contexts
associated with negative affect and regulation needs, while leisure activities
promote positive emotions. We propose CALLM, a Context-Aware framework
leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG) to analyze these brief entries by integrating retrieved peer experiences
and personal diary history. CALLM demonstrates strong performance with balanced
accuracies reaching 72.96% for positive affect, 73.29% for negative affect,
73.72% for emotion regulation desire, and 60.09% for intervention availability,
outperforming language model baselines. Post-hoc analysis reveals that model
confidence strongly predicts accuracy, with longer diary entries generally
enhancing performance, and brief personalization periods yielding meaningful
improvements. Our findings demonstrate how contextual information in mobile
diaries can be effectively leveraged to understand emotional experiences,
predict key states, and identify optimal intervention moments for personalized
just-in-time support.

</details>


### [51] [Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models](https://arxiv.org/pdf/2503.13551)
*Teng Wang, Zhangyi Jiang, Zhenqi He, Shenyang Tong, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Hailei Gong*

Main category: cs.CL

TL;DR: The paper introduces the Hierarchical Reward Model (HRM) and Hierarchical Node Compression (HNC) to improve reasoning evaluation in LLMs, addressing reward hacking and data annotation costs.


<details>
  <summary>Details</summary>
Motivation: Current Process Reward Models (PRMs) suffer from reward hacking and high annotation costs, limiting reliable reasoning step evaluation.

Method: Proposes HRM for multi-step reasoning assessment and HNC for cost-effective data augmentation via step merging.

Result: HRM with HNC outperforms PRM on PRM800K and generalizes well on MATH500 and GSM8K datasets.

Conclusion: HRM and HNC offer stable, reliable, and scalable solutions for evaluating reasoning in LLMs.

Abstract: Recent studies show that Large Language Models (LLMs) achieve strong
reasoning capabilities through supervised fine-tuning or reinforcement
learning. However, a key approach, the Process Reward Model (PRM), suffers from
reward hacking, making it unreliable in identifying the best intermediate step.
In addition, the cost of annotating reasoning processes for reward modeling is
high, making large-scale collection of high-quality data challenging. To
address this, we propose a novel reward model approach called the Hierarchical
Reward Model (HRM), which evaluates both individual and consecutive reasoning
steps at both fine-grained and coarse-grained levels. HRM excels at assessing
multi-step reasoning coherence, especially when flawed steps are later
corrected through self-reflection. To further reduce the cost of generating
training data, we introduce a lightweight and effective data augmentation
strategy called Hierarchical Node Compression (HNC), which merges two
consecutive reasoning steps into one within the tree structure. By applying HNC
to MCTS-generated reasoning trajectories, we enhance the diversity and
robustness of HRM training data while introducing controlled noise with minimal
computational overhead. Empirical results on the PRM800K dataset show that HRM,
together with HNC, provides more stable and reliable evaluations than PRM.
Furthermore, cross-domain evaluations on the MATH500 and GSM8K datasets
demonstrate HRM's strong generalization and robustness across a variety of
reasoning tasks.

</details>


### [52] [CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement](https://arxiv.org/pdf/2503.17279)
*Gaifan Zhang, Yi Zhou, Danushka Bollegala*

Main category: cs.CL

TL;DR: CASE is a method to create context-aware sentence embeddings using LLMs, outperforming existing C-STS methods.


<details>
  <summary>Details</summary>
Motivation: Current sentence embedding methods lack clarity on how to modify embeddings based on context.

Method: CASE uses LLMs to create condition embeddings, applies supervised nonlinear projection for dimensionality reduction, and subtracts condition embeddings for improved performance.

Result: CASE outperforms existing C-STS methods and improves LLM-based embeddings through dimensionality reduction.

Conclusion: CASE effectively addresses context-aware sentence embedding challenges, enhancing performance and efficiency.

Abstract: The meaning conveyed by a sentence often depends on the context in which it
appears. Despite the progress of sentence embedding methods, it remains unclear
how to best modify a sentence embedding conditioned on its context. To address
this problem, we propose Condition-Aware Sentence Embeddings (CASE), an
efficient and accurate method to create an embedding for a sentence under a
given condition. First, CASE creates an embedding for the condition using a
Large Language Model (LLM), where the sentence influences the attention scores
computed for the tokens in the condition during pooling. Next, a supervised
nonlinear projection is learned to reduce the dimensionality of the LLM-based
text embeddings. We show that CASE significantly outperforms previously
proposed Conditional Semantic Textual Similarity (C-STS) methods on an existing
standard benchmark dataset. We find that subtracting the condition embedding
consistently improves the C-STS performance of LLM-based text embeddings.
Moreover, we propose a supervised dimensionality reduction method that not only
reduces the dimensionality of LLM-based embeddings but also significantly
improves their performance.

</details>


### [53] [HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment](https://arxiv.org/pdf/2503.18991)
*Ruoxi Cheng, Haoxuan Ma, Weixin Wang*

Main category: cs.CL

TL;DR: HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning) addresses LLM alignment challenges with a balanced safety dataset and dynamic reward tuning, achieving top performance in safety and usefulness.


<details>
  <summary>Details</summary>
Motivation: Alignment of LLMs with human values is hindered by dataset scarcity, alignment tax, jailbreak vulnerability, and static rewards.

Method: HAIR uses introspective reasoning to create a balanced safety dataset (CoD) and trains reward models with GRPO, dynamically adjusting to task difficulty.

Result: HAIR outperforms baselines in safety benchmarks while maintaining high usefulness.

Conclusion: HAIR effectively addresses alignment challenges, offering a robust solution for LLM safety and utility.

Abstract: The alignment of large language models (LLMs) with human values remains
critical yet hindered by four key challenges: (1) scarcity of balanced safety
datasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to
shallow alignment, and (4) inability to dynamically adapt rewards according to
task difficulty. To address these limitations, we introduce HAIR
(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a
novel alignment approach inspired by shadow models in membership inference
attacks. Our approach consists of two main components: (1) construction of a
balanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using
structured prompts that leverage the introspective reasoning capabilities of
LLMs; and (2) training of category-specific reward models with Group Relative
Policy Optimization (GRPO), dynamically tuning optimization to task difficulty
at both the data and model levels. Comprehensive experiments across four
harmlessness and four usefulness benchmarks demonstrate that HAIR achieves
state-of-the-art performance, outperforming all baseline methods in safety
while maintaining high levels of usefulness.

</details>


### [54] [Clean & Clear: Feasibility of Safe LLM Clinical Guidance](https://arxiv.org/pdf/2503.20953)
*Julia Ive, Felix Jozsa, Nick Jackson, Paulina Bondaronek, Ciaran Scott Hill, Richard Dobson*

Main category: cs.CL

TL;DR: An LLM-empowered chatbot was developed to answer clinical guideline questions using UCLH guidelines, showing promising relevance, recall, and efficiency, though with minor precision lapses.


<details>
  <summary>Details</summary>
Motivation: To improve access to clinical guidelines for healthcare professionals by leveraging LLMs for quick and accurate responses.

Method: Used Llama-3.1-8B LLM to extract and answer questions from UCLH guidelines, assessed by doctors for relevance, completeness, and precision.

Result: 73% very relevant responses, 100% recall for guideline lines, 78% satisfactory completeness, 14.5% minor unnecessary info, 10s average response time, 72% flawless clinical reasoning.

Conclusion: The chatbot shows strong potential to enhance access to clinical information efficiently, though minor precision improvements are needed.

Abstract: Background:
  Clinical guidelines are central to safe evidence-based medicine in modern
healthcare, providing diagnostic criteria, treatment options and monitoring
advice for a wide range of illnesses. LLM-empowered chatbots have shown great
promise in Healthcare Q&A tasks, offering the potential to provide quick and
accurate responses to medical inquiries.
  Our main objective was the development and preliminary assessment of an
LLM-empowered chatbot software capable of reliably answering clinical guideline
questions using University College London Hospital (UCLH) clinical guidelines.
  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant
information from the UCLH guidelines to answer questions. Our approach
highlights the safety and reliability of referencing information over its
interpretation and response generation. Seven doctors from the ward assessed
the chatbot's performance by comparing its answers to the gold standard.
  Results: Our chatbot demonstrates promising performance in terms of
relevance, with ~73% of its responses rated as very relevant, showcasing a
strong understanding of the clinical context. Importantly, our chatbot achieves
a recall of 1.00 for extracted guideline lines, substantially minimising the
risk of missing critical information. Approximately 78% of responses were rated
satisfactory in terms of completeness. A small portion (~14.5%) contained minor
unnecessary information, indicating occasional lapses in precision. The
chatbot' showed high efficiency, with an average completion time of 10 seconds,
compared to 30 seconds for human respondents. Evaluation of clinical reasoning
showed that 72% of the chatbot's responses were without flaws. Our chatbot
demonstrates significant potential to speed up and improve the process of
accessing locally relevant clinical information for healthcare professionals.

</details>


### [55] [Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement](https://arxiv.org/pdf/2503.23895)
*Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu*

Main category: cs.CL

TL;DR: DyPRAG enhances RAG by dynamically converting documents into parametric knowledge, reducing costs and improving generalization.


<details>
  <summary>Details</summary>
Motivation: Address the high costs and limited generalization of PRAG in RAG systems.

Method: Uses a lightweight parameter translator to dynamically embed documents into LLMs.

Result: Reduces inference, training, and storage costs while mitigating RAG hallucination.

Conclusion: DyPRAG offers a practical and efficient RAG solution with superior knowledge fusion.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving relevant documents from external sources and incorporating them into
the context. While it improves reliability by providing factual texts, it
significantly increases inference costs as context length grows and introduces
challenging issue of RAG hallucination, primarily caused by the lack of
corresponding parametric knowledge in LLMs. An efficient solution is to enhance
the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by
embedding document into LLMs parameters to perform test-time knowledge
enhancement, effectively reducing inference costs through offline training.
However, its high training and storage costs, along with limited generalization
ability, significantly restrict its practical adoption. To address these
challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that
leverages a lightweight parameter translator model to efficiently convert
documents into parametric knowledge. DyPRAG not only reduces inference,
training, and storage costs but also dynamically generates parametric
knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge
conflicts in a plug-and-play manner at test-time. Extensive experiments on
multiple datasets demonstrate the effectiveness and generalization capabilities
of DyPRAG, offering a powerful and practical RAG paradigm which enables
superior knowledge fusion and mitigates RAG hallucination in real-world
applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.

</details>


### [56] [SEAL: Steerable Reasoning Calibration of Large Language Models for Free](https://arxiv.org/pdf/2504.07986)
*Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, Zhangyang Wang*

Main category: cs.CL

TL;DR: SEAL improves LLM reasoning efficiency by calibrating CoT traces, reducing redundancy and boosting accuracy.


<details>
  <summary>Details</summary>
Motivation: Address redundancy in CoT reasoning traces that harms performance and efficiency.

Method: Categorize reasoning thoughts, extract a steering vector, and calibrate CoT traces.

Result: 11% accuracy boost and 11.8%-50.4% token reduction across benchmarks.

Conclusion: SEAL effectively enhances reasoning efficiency and accuracy without training.

Abstract: Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated
compelling capabilities for complex reasoning tasks via the extended
chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal
substantial redundancy in the CoT reasoning traces, which not only increases
inference latency but also negatively impacts model performance by diverting
attention to unnecessary reasoning paths. To address this issue, we investigate
the internal reasoning structures of LLMs and categorize them into three
primary thought types: execution, reflection, and transition thoughts.
Moreover, our analysis reveals that excessive reflection and transition
thoughts are strongly correlated with failure cases and these thought
categories exhibit clear separation in the latent space. Based on these, we
introduce SEAL (Steerable reasoning calibration), a training-free approach that
seamlessly calibrates the CoT process, improving accuracy while demonstrating
significant efficiency gains. SEAL consists of an offline stage for extracting
the reasoning steering vector in the latent space, followed by an on-the-fly
calibration of the reasoning trace through representation intervention using
the steering vector. Notably, the steering vector exhibits strong
transferability across various tasks. Extensive experiments across multiple
models (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,
GSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%
improvement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our
code is publicly available at https://github.com/VITA-Group/SEAL.

</details>


### [57] [Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English](https://arxiv.org/pdf/2504.17974)
*Sabur Butt, Fazlourrahman Balouchzahi, Ahmad Imam Amjad, Maaz Amjad, Hector G. Ceballos, Salud Maria Jimenez-Zafra*

Main category: cs.CL

TL;DR: PolyHope V2 is a multilingual dataset for detecting nuanced hope subtypes in tweets, showing fine-tuned transformers outperform LLMs like GPT-4 and Llama 3.


<details>
  <summary>Details</summary>
Motivation: Hope is complex and underexplored in NLP, with varied forms like optimism, wishfulness, or sarcasm, making detection challenging.

Method: Created PolyHope V2, a dataset of 30K+ annotated tweets in English/Spanish, distinguishing four hope subtypes. Benchmarked transformers and LLMs (GPT-4, Llama 3) under zero-shot/few-shot settings.

Result: Fine-tuned transformers outperformed LLMs, especially in nuanced hope and sarcasm detection. Challenges remain in distinguishing closely related subtypes.

Conclusion: PolyHope V2 advances emotion recognition with semantic and contextual sensitivity, providing a foundation for future multilingual NLP tasks.

Abstract: Hope is a complex and underexplored emotional state that plays a significant
role in education, mental health, and social interaction. Unlike basic
emotions, hope manifests in nuanced forms ranging from grounded optimism to
exaggerated wishfulness or sarcasm, making it difficult for Natural Language
Processing systems to detect accurately. This study introduces PolyHope V2, a
multilingual, fine-grained hope speech dataset comprising over 30,000 annotated
tweets in English and Spanish. This resource distinguishes between four hope
subtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances
existing datasets by explicitly labeling sarcastic instances. We benchmark
multiple pretrained transformer models and compare them with large language
models (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.
Our findings show that fine-tuned transformers outperform prompt-based LLMs,
especially in distinguishing nuanced hope categories and sarcasm. Through
qualitative analysis and confusion matrices, we highlight systematic challenges
in separating closely related hope subtypes. The dataset and results provide a
robust foundation for future emotion recognition tasks that demand greater
semantic and contextual sensitivity across languages.

</details>


### [58] [Pushing the boundary on Natural Language Inference](https://arxiv.org/pdf/2504.18376)
*Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho*

Main category: cs.CL

TL;DR: The paper introduces a reinforcement learning-based approach (GRPO) for Chain-of-Thought learning in NLI, eliminating the need for labeled data and improving performance on adversarial benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current NLI systems rely on supervised learning with biased datasets, limiting generalization. This work aims to overcome these limitations.

Method: Uses Group Relative Policy Optimization (GRPO) for CoT learning, fine-tuning large language models (7B, 14B, 32B) with LoRA and QLoRA techniques.

Result: The 32B AWQ-quantized model achieves state-of-the-art results on adversarial benchmarks, maintaining robust reasoning under aggressive quantization.

Conclusion: The framework is scalable and practical for building robust NLI systems without compromising inference quality.

Abstract: Natural Language Inference (NLI) is a central task in natural language
understanding with applications in fact-checking, question answering, and
information retrieval. Despite its importance, current NLI systems heavily rely
on supervised learning with datasets that often contain annotation artifacts
and biases, limiting generalization and real-world applicability. In this work,
we apply a reinforcement learning-based approach using Group Relative Policy
Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the
need for labeled rationales and enabling this type of training on more
challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language
models using parameter-efficient techniques (LoRA and QLoRA), demonstrating
strong performance across standard and adversarial NLI benchmarks. Our 32B
AWQ-quantized model surpasses state-of-the-art results on 7 out of 11
adversarial sets$\unicode{x2013}$or on all of them considering our
replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust
reasoning can be retained under aggressive quantization. This work provides a
scalable and practical framework for building robust NLI systems without
sacrificing inference quality.

</details>


### [59] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/pdf/2505.00949)
*Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, Chris Alexiuk*

Main category: cs.CL

TL;DR: The Llama-Nemotron series offers open-source, high-performance reasoning models with three sizes, competitive with state-of-the-art models, and includes unique features like a dynamic reasoning toggle.


<details>
  <summary>Details</summary>
Motivation: To provide enterprise-ready, open-source reasoning models with superior efficiency and flexibility, supporting dynamic switching between chat and reasoning modes.

Method: Training involves neural architecture search, knowledge distillation, continued pretraining, and a reasoning-focused post-training stage with supervised fine-tuning and large-scale reinforcement learning.

Result: Models (Nano, Super, Ultra) perform competitively with top reasoning models while offering better inference throughput and memory efficiency.

Conclusion: Llama-Nemotron models advance open-source reasoning tools, supported by released datasets, codebases, and a permissive license for broader adoption.

Abstract: We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>


### [60] [Humans can learn to detect AI-generated texts, or at least learn when they can't](https://arxiv.org/pdf/2505.01877)
*Jiří Milička, Anna Marklová, Ondřej Drobil, Eva Pospíšilová*

Main category: cs.CL

TL;DR: Participants improved accuracy in distinguishing human-written from AI-generated texts with feedback, correcting misconceptions about AI style and readability.


<details>
  <summary>Details</summary>
Motivation: To determine if feedback helps individuals learn to discriminate between human and AI texts and recalibrate self-perceived competence.

Method: Used GPT-4o to generate texts, compared with human-written ones, and tested 255 participants with/without feedback.

Result: Feedback group showed improved accuracy and confidence calibration, correcting initial misconceptions.

Conclusion: Targeted training with feedback effectively teaches differentiation between human and AI texts, aiding self-assessment, especially in education.

Abstract: This study investigates whether individuals can learn to accurately
discriminate between human-written and AI-produced texts when provided with
immediate feedback, and if they can use this feedback to recalibrate their
self-perceived competence. We also explore the specific criteria individuals
rely upon when making these decisions, focusing on textual style and perceived
readability.
  We used GPT-4o to generate several hundred texts across various genres and
text types comparable to Koditex, a multi-register corpus of human-written
texts. We then presented randomized text pairs to 255 Czech native speakers who
identified which text was human-written and which was AI-generated.
Participants were randomly assigned to two conditions: one receiving immediate
feedback after each trial, the other receiving no feedback until experiment
completion. We recorded accuracy in identification, confidence levels, response
times, and judgments about text readability along with demographic data and
participants' engagement with AI technologies prior to the experiment.
  Participants receiving immediate feedback showed significant improvement in
accuracy and confidence calibration. Participants initially held incorrect
assumptions about AI-generated text features, including expectations about
stylistic rigidity and readability. Notably, without feedback, participants
made the most errors precisely when feeling most confident -- an issue largely
resolved among the feedback group.
  The ability to differentiate between human and AI-generated texts can be
effectively learned through targeted training with explicit feedback, which
helps correct misconceptions about AI stylistic features and readability, as
well as potential other variables that were not explored, while facilitating
more accurate self-assessment. This finding might be particularly important in
educational contexts.

</details>


### [61] [Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents](https://arxiv.org/pdf/2505.02156)
*Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao*

Main category: cs.CL

TL;DR: AML introduces adaptive reasoning modes for social intelligence tasks, outperforming existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods lack dynamic reasoning depth adjustment, leading to inefficiency and poor social simulation.

Method: Proposes AML with AMPO algorithm, featuring multi-granular thinking modes, context-aware switching, and token-efficient reasoning.

Result: AML achieves 15.6% higher task performance and 32.8% shorter reasoning chains than state-of-the-art methods.

Conclusion: Context-sensitive mode selection in AML enables more human-like adaptive reasoning compared to fixed-depth approaches.

Abstract: Effective social intelligence simulation requires language agents to
dynamically adjust reasoning depth, a capability notably absent in current
approaches. While existing methods either lack this kind of reasoning
capability or enforce uniform long chain-of-thought reasoning across all
scenarios, resulting in excessive token usage and inappropriate social
simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode
$\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four
thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on
real-time context. Our framework's core innovation, the $\textbf{A}$daptive
$\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$)
algorithm, introduces three key advancements over existing methods: (1)
Multi-granular thinking mode design, (2) Context-aware mode switching across
social interaction, and (3) Token-efficient reasoning via depth-adaptive
processing. Extensive experiments on social intelligence tasks confirm that AML
achieves 15.6% higher task performance than state-of-the-art methods. Notably,
our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These
results demonstrate that context-sensitive thinking mode selection, as
implemented in AMPO, enables more human-like adaptive reasoning than GRPO's
fixed-depth approach.

</details>


### [62] [EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning](https://arxiv.org/pdf/2505.02579)
*Lingxiao Kong, Cong Yang, Susanne Neufang, Oya Deniz Beyan, Zeyd Boukhers*

Main category: cs.CL

TL;DR: The paper introduces EMORL, an ensemble-based RL framework for fine-tuning LLMs, improving efficiency, scalability, and explainability in multi-objective tasks.


<details>
  <summary>Details</summary>
Motivation: Address challenges in RL for LLM fine-tuning, such as complex objective balancing, low efficiency, poor scalability, and limited explainability.

Method: Proposes EMORL, which fine-tunes multiple models with individual objectives and aggregates their last hidden states using a hierarchical grid search for optimal combinations.

Result: EMORL shows lower training consumption, improved scalability, and comparable performance on PAIR and Psych8k datasets.

Conclusion: EMORL effectively addresses multi-objective RL challenges, offering a scalable and explainable solution for LLM fine-tuning.

Abstract: Recent advances in reinforcement learning (RL) for large language model (LLM)
fine-tuning show promise in addressing multi-objective tasks but still face
significant challenges, including complex objective balancing, low training
efficiency, poor scalability, and limited explainability. Leveraging ensemble
learning principles, we introduce an Ensemble Multi-Objective RL (EMORL)
framework that fine-tunes multiple models with individual objectives while
optimizing their aggregation after the training to improve efficiency and
flexibility. Our method is the first to aggregate the last hidden states of
individual models, incorporating contextual information from multiple
objectives. This approach is supported by a hierarchical grid search algorithm
that identifies optimal weighted combinations. We evaluate EMORL on counselor
reflection generation tasks, using text-scoring LLMs to evaluate the
generations and provide rewards during RL fine-tuning. Through comprehensive
experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of
EMORL against existing baselines: significantly lower and more stable training
consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds),
improved scalability and explainability, and comparable performance across
multiple objectives.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [63] [RESAnything: Attribute Prompting for Arbitrary Referring Segmentation](https://arxiv.org/pdf/2505.02867)
*Ruiqi Wang, Hao Zhang*

Main category: cs.CV

TL;DR: RESAnything is a zero-shot, open-vocabulary method for arbitrary referring expression segmentation (RES), handling implicit queries and part-level labels without training data. It uses Chain-of-Thoughts reasoning and LLM-generated attribute descriptions to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Prior RES methods struggle with general input expressions, including implicit references and part-level labels. RESAnything aims to address this gap by leveraging LLMs for deep reasoning.

Method: RESAnything combines Chain-of-Thoughts reasoning with attribute prompting. An LLM generates detailed descriptions of object/part attributes, while a foundational segmentation model produces proposals.

Result: RESAnything outperforms zero-shot methods on traditional benchmarks and excels in handling implicit queries and part-level relations. A new benchmark dataset (~3K instances) is introduced.

Conclusion: RESAnything advances RES by enabling zero-shot, open-vocabulary segmentation for arbitrary expressions, demonstrating superior performance and introducing a new benchmark for evaluation.

Abstract: We present an open-vocabulary and zero-shot method for arbitrary referring
expression segmentation (RES), targeting input expressions that are more
general than what prior works were designed to handle. Specifically, our inputs
encompass both object- and part-level labels as well as implicit references
pointing to properties or qualities of object/part function, design, style,
material, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT)
reasoning, where the key idea is attribute prompting. We generate detailed
descriptions of object/part attributes including shape, color, and location for
potential segment proposals through systematic prompting of a large language
model (LLM), where the proposals are produced by a foundational image
segmentation model. Our approach encourages deep reasoning about object or part
attributes related to function, style, design, etc., enabling the system to
handle implicit queries without any part annotations for training or
fine-tuning. As the first zero-shot and LLM-based RES method, RESAnything
achieves clearly superior performance among zero-shot methods on traditional
RES benchmarks and significantly outperforms existing methods on challenging
scenarios involving implicit queries and complex part-level relations. Finally,
we contribute a new benchmark dataset to offer ~3K carefully curated RES
instances to assess part-level, arbitrary RES solutions.

</details>


### [64] [Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images](https://arxiv.org/pdf/2505.02949)
*Tian Qiu, Arjun Nichani, Rasta Tadayontahmasebi, Haewon Jeong*

Main category: cs.CV

TL;DR: The paper evaluates racial bias in neural image compression models, showing traditional metrics fail to capture bias, and proposes a framework to assess and mitigate it.


<details>
  <summary>Details</summary>
Motivation: To address the potential unfair outcomes due to bias in neural compression models, especially at low bitrates, and to evaluate racial bias in popular models.

Method: A structured, scalable framework is used to analyze nine neural compression models, focusing on facial phenotype degradation and realism in reconstructions.

Result: Racial bias is present in all models, with a trade-off between bias and realism. A racially balanced training set reduces bias but isn't sufficient. Bias stems from both compression and classification models.

Conclusion: This work is a foundational step toward evaluating and mitigating bias in neural image compression, highlighting the need for better metrics and strategies.

Abstract: Neural compression methods are gaining popularity due to their superior
rate-distortion performance over traditional methods, even at extremely low
bitrates below 0.1 bpp. As deep learning architectures, these models are prone
to bias during the training process, potentially leading to unfair outcomes for
individuals in different groups. In this paper, we present a general,
structured, scalable framework for evaluating bias in neural image compression
models. Using this framework, we investigate racial bias in neural compression
algorithms by analyzing nine popular models and their variants. Through this
investigation, we first demonstrate that traditional distortion metrics are
ineffective in capturing bias in neural compression models. Next, we highlight
that racial bias is present in all neural compression models and can be
captured by examining facial phenotype degradation in image reconstructions. We
then examine the relationship between bias and realism in the decoded images
and demonstrate a trade-off across models. Finally, we show that utilizing a
racially balanced training set can reduce bias but is not a sufficient bias
mitigation strategy. We additionally show the bias can be attributed to
compression model bias and classification model bias. We believe that this work
is a first step towards evaluating and eliminating bias in neural image
compression models.

</details>


### [65] [Generating Narrated Lecture Videos from Slides with Synchronized Highlights](https://arxiv.org/pdf/2505.02966)
*Alexander Holmberg*

Main category: cs.CV

TL;DR: An AI system automates video lecture creation from slides, syncing narration with dynamic visual highlights, achieving high accuracy and low cost.


<details>
  <summary>Details</summary>
Motivation: Manual video lecture creation is time-consuming; automating it can save effort and costs.

Method: Uses a highlight alignment module with strategies like Levenshtein distance and LLM-based analysis, synchronized with TTS.

Result: LLM-based alignment achieves F1 > 92%, with generation costs under $1 per hour.

Conclusion: The system is a scalable, cost-effective solution for converting slides into engaging video lectures.

Abstract: Turning static slides into engaging video lectures takes considerable time
and effort, requiring presenters to record explanations and visually guide
their audience through the material. We introduce an end-to-end system designed
to automate this process entirely. Given a slide deck, this system synthesizes
a video lecture featuring AI-generated narration synchronized precisely with
dynamic visual highlights. These highlights automatically draw attention to the
specific concept being discussed, much like an effective presenter would. The
core technical contribution is a novel highlight alignment module. This module
accurately maps spoken phrases to locations on a given slide using diverse
strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at
selectable granularities (line or word level) and utilizes timestamp-providing
Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's
effectiveness through a technical evaluation using a manually annotated slide
dataset with 1000 samples, finding that LLM-based alignment achieves high
location accuracy (F1 > 92%), significantly outperforming simpler methods,
especially on complex, math-heavy content. Furthermore, the calculated
generation cost averages under $1 per hour of video, offering potential savings
of two orders of magnitude compared to conservative estimates of manual
production costs. This combination of high accuracy and extremely low cost
positions this approach as a practical and scalable tool for transforming
static slides into effective, visually-guided video lectures.

</details>


### [66] [SD-VSum: A Method and Dataset for Script-Driven Video Summarization](https://arxiv.org/pdf/2505.03319)
*Manolis Mylonas, Evlampios Apostolidis, Vasileios Mezaris*

Main category: cs.CV

TL;DR: The paper introduces script-driven video summarization, extends the VideoXum dataset with natural language descriptions, and proposes SD-VSum, a cross-modal attention-based architecture, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To enable video summarization tailored to user-provided scripts, addressing the need for personalized summaries.

Method: Extends VideoXum dataset with summary descriptions and develops SD-VSum, a cross-modal attention network for aligning visual and text data.

Result: SD-VSum outperforms existing methods in query-driven and generic summarization, producing user-adapted summaries.

Conclusion: The work successfully introduces script-driven summarization, demonstrates its effectiveness, and provides a scalable dataset for future research.

Abstract: In this work, we introduce the task of script-driven video summarization,
which aims to produce a summary of the full-length video by selecting the parts
that are most relevant to a user-provided script outlining the visual content
of the desired summary. Following, we extend a recently-introduced large-scale
dataset for generic video summarization (VideoXum) by producing natural
language descriptions of the different human-annotated summaries that are
available per video. In this way we make it compatible with the introduced
task, since the available triplets of ``video, summary and summary
description'' can be used for training a method that is able to produce
different summaries for a given video, driven by the provided script about the
content of each summary. Finally, we develop a new network architecture for
script-driven video summarization (SD-VSum), that relies on the use of a
cross-modal attention mechanism for aligning and fusing information from the
visual and text modalities. Our experimental evaluations demonstrate the
advanced performance of SD-VSum against state-of-the-art approaches for
query-driven and generic (unimodal and multimodal) summarization from the
literature, and document its capacity to produce video summaries that are
adapted to each user's needs about their content.

</details>


### [67] [Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation](https://arxiv.org/pdf/2505.02971)
*Anjila Budathoki, Manish Dhakal*

Main category: cs.CV

TL;DR: The paper explores adversarial attacks on vision-language segmentation models (VLSMs) for medical images, assessing their robustness using PGD and FGSM attacks. Results show significant performance drops in DSC and IoU scores.


<details>
  <summary>Details</summary>
Motivation: To address the under-explored robustness of VLSMs against adversarial attacks in medical image analysis, especially in high-risk scenarios.

Method: Fine-tuned pre-trained VLSMs for medical image segmentation with adapters, then applied PGD and FGSM adversarial attacks to evaluate robustness.

Result: Significant performance decline in DSC and IoU scores after adversarial attacks; universal perturbation was not found for medical images.

Conclusion: VLSMs are vulnerable to adversarial attacks in medical settings, highlighting the need for robust defenses in high-risk applications.

Abstract: Adversarial attacks have been fairly explored for computer vision and
vision-language models. However, the avenue of adversarial attack for the
vision language segmentation models (VLSMs) is still under-explored, especially
for medical image analysis.
  Thus, we have investigated the robustness of VLSMs against adversarial
attacks for 2D medical images with different modalities with radiology,
photography, and endoscopy. The main idea of this project was to assess the
robustness of the fine-tuned VLSMs specially in the medical domain setting to
address the high risk scenario.
  First, we have fine-tuned pre-trained VLSMs for medical image segmentation
with adapters.
  Then, we have employed adversarial attacks -- projected gradient descent
(PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to
determine its robustness against adversaries.
  We have reported models' performance decline to analyze the adversaries'
impact.
  The results exhibit significant drops in the DSC and IoU scores after the
introduction of these adversaries. Furthermore, we also explored universal
perturbation but were not able to find for the medical images.
  \footnote{https://github.com/anjilab/secure-private-ai}

</details>


### [68] [DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor](https://arxiv.org/pdf/2505.03261)
*Wei-Ting Chen, Yu-Jiet Vong, Yi-Tsung Lee, Sy-Yen Kuo, Qiang Gao, Sizhuo Ma, Jian Wang*

Main category: cs.CV

TL;DR: DiffVQA is a novel VQA framework using diffusion models for feature extraction, outperforming CNNs and ViTs by better aligning with human perceptions and handling diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing VQA methods (CNNs/ViTs) struggle to align with human perceptions in diverse scenarios due to limited dataset scale and diversity.

Method: DiffVQA adapts pre-trained diffusion models for frame reconstruction, extracts semantic/distortion features, and integrates a Mamba module for temporal dynamics.

Result: DiffVQA shows superior performance in intra-dataset evaluations and exceptional generalization across datasets.

Conclusion: Diffusion models as feature extractors enhance VQA performance over traditional backbones like CNNs and ViTs.

Abstract: Video Quality Assessment (VQA) aims to evaluate video quality based on
perceptual distortions and human preferences. Despite the promising performance
of existing methods using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs), they often struggle to align closely with human
perceptions, particularly in diverse real-world scenarios. This challenge is
exacerbated by the limited scale and diversity of available datasets. To
address this limitation, we introduce a novel VQA framework, DiffVQA, which
harnesses the robust generalization capabilities of diffusion models
pre-trained on extensive datasets. Our framework adapts these models to
reconstruct identical input frames through a control module. The adapted
diffusion model is then used to extract semantic and distortion features from a
resizing branch and a cropping branch, respectively. To enhance the model's
ability to handle long-term temporal dynamics, a parallel Mamba module is
introduced, which extracts temporal coherence augmented features that are
merged with the diffusion features to predict the final score. Experiments
across multiple datasets demonstrate DiffVQA's superior performance on
intra-dataset evaluations and its exceptional generalization across datasets.
These results confirm that leveraging a diffusion model as a feature extractor
can offer enhanced VQA performance compared to CNN and ViT backbones.

</details>


### [69] [Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking](https://arxiv.org/pdf/2505.02980)
*Daniela Ruiz, Paula Cardenas, Leonardo Manrique, Daniela Vega, Gabriel Mejia, Pablo Arbelaez*

Main category: cs.CV

TL;DR: SpaRED introduces a standardized database for gene expression prediction from histology images, while SpaCKLE, a transformer-based model, reduces errors by 82.5%. The benchmark evaluates eight models, showing SpaCKLE's superiority.


<details>
  <summary>Details</summary>
Motivation: Address limitations of Visium (cost, expertise, inefficiency) and inconsistencies in datasets and models for gene expression prediction.

Method: Create SpaRED, a curated database of 26 datasets, and develop SpaCKLE, a transformer-based model for gene expression completion.

Result: SpaCKLE reduces mean squared error by 82.5% and improves all evaluated models' performance.

Conclusion: SpaRED and SpaCKLE provide a comprehensive benchmark and tool for advancing Spatial Transcriptomics research.

Abstract: Spatial Transcriptomics is a groundbreaking technology that integrates
histology images with spatially resolved gene expression profiles. Among the
various Spatial Transcriptomics techniques available, Visium has emerged as the
most widely adopted. However, its accessibility is limited by high costs, the
need for specialized expertise, and slow clinical integration. Additionally,
gene capture inefficiencies lead to significant dropout, corrupting acquired
data. To address these challenges, the deep learning community has explored the
gene expression prediction task directly from histology images. Yet,
inconsistencies in datasets, preprocessing, and training protocols hinder fair
comparisons between models. To bridge this gap, we introduce SpaRED, a
systematically curated database comprising 26 public datasets, providing a
standardized resource for model evaluation. We further propose SpaCKLE, a
state-of-the-art transformer-based gene expression completion model that
reduces mean squared error by over 82.5% compared to existing approaches.
Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art
prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE
substantially improves the results across all the gene expression prediction
models. Altogether, our contributions constitute the most comprehensive
benchmark of gene expression prediction from histology images to date and a
stepping stone for future research on Spatial Transcriptomics.

</details>


### [70] [PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model](https://arxiv.org/pdf/2505.03603)
*Y. B. Wang, S. Z. Zhou, J. F. Wu, T. Hu, J. N. Zhang, Y. Liu*

Main category: cs.CV

TL;DR: PAHA is an end-to-end audio-driven upper-body human animation framework using diffusion models, addressing quality and consistency issues with PAR and PCE methods, and introducing CNAS dataset.


<details>
  <summary>Details</summary>
Motivation: Current methods suffer from long inference times, poor generation quality in specific regions, and inconsistent audio-motion alignment due to lack of fine-grained supervision.

Method: Proposes PAR for dynamic loss adjustment and PCE for audio-visual consistency. Introduces SG and DG for inference guidance.

Result: PAHA outperforms existing methods in audio-motion alignment and video evaluations, validated by experiments and user studies.

Conclusion: PAHA advances audio-driven human animation with improved quality and consistency, supported by the new CNAS dataset.

Abstract: Audio-driven human animation technology is widely used in human-computer
interaction, and the emergence of diffusion models has further advanced its
development. Currently, most methods rely on multi-stage generation and
intermediate representations, resulting in long inference time and issues with
generation quality in specific foreground regions and audio-motion consistency.
These shortcomings are primarily due to the lack of localized fine-grained
supervised guidance. To address above challenges, we propose PAHA, an
end-to-end audio-driven upper-body human animation framework with diffusion
model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts
Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss
weights based on pose confidence scores, effectively improving visual quality.
PCE constructs and trains diffusion-based regional audio-visual classifiers to
improve the consistency of motion and co-speech audio. Afterwards, we design
two novel inference guidance methods for the foregoing classifiers, Sequential
Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality
respectively. Additionally, we build CNAS, the first public Chinese News Anchor
Speech dataset, to advance research and validation in this field. Extensive
experimental results and user studies demonstrate that PAHA significantly
outperforms existing methods in audio-motion alignment and video-related
evaluations. The codes and CNAS dataset will be released upon acceptance.

</details>


### [71] [Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning](https://arxiv.org/pdf/2505.03327)
*José-Luis Bueso-Bello, Benjamin Chauvel, Daniel Carcereri, Philipp Posovszky, Pietro Milillo, Jennifer Ruiz, Juan-Carlos Fernández-Diaz, Carolina González, Michele Martone, Ronny Hänsch, Paola Rizzoli*

Main category: cs.CV

TL;DR: The paper proposes a self-supervised learning framework to map forests at 6m resolution using TanDEM-X SAR data, reducing reliance on large labeled datasets and outperforming fully-supervised methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of mid-resolution forest mapping (e.g., detecting narrow roads, precise delineation) and address the lack of high-resolution labeled data.

Method: Investigates self-supervised learning for feature extraction, followed by supervised training with fewer labels. Validated using a 1m resolution reference map in Pennsylvania and applied to the Amazon rainforest.

Result: The self-supervised framework significantly improves classification accuracy compared to fully-supervised methods with the same labeled data.

Conclusion: The approach is promising for large-scale, high-resolution forest mapping with limited labeled data.

Abstract: Deep learning models have shown encouraging capabilities for mapping
accurately forests at medium resolution with TanDEM-X interferometric SAR data.
Such models, as most of current state-of-the-art deep learning techniques in
remote sensing, are trained in a fully-supervised way, which requires a large
amount of labeled data for training and validation. In this work, our aim is to
exploit the high-resolution capabilities of the TanDEM-X mission to map forests
at 6 m. The goal is to overcome the intrinsic limitations posed by
midresolution products, which affect, e.g., the detection of narrow roads
within vegetated areas and the precise delineation of forested regions
contours. To cope with the lack of extended reliable reference datasets at such
a high resolution, we investigate self-supervised learning techniques for
extracting highly informative representations from the input features, followed
by a supervised training step with a significantly smaller number of reliable
labels. A 1 m resolution forest/non-forest reference map over Pennsylvania,
USA, allows for comparing different training approaches for the development of
an effective forest mapping framework with limited labeled samples. We select
the best-performing approach over this test region and apply it in a real-case
forest mapping scenario over the Amazon rainforest, where only very few labeled
data at high resolution are available. In this challenging scenario, the
proposed self-supervised framework significantly enhances the classification
accuracy with respect to fully-supervised methods, trained using the same
amount of labeled data, representing an extremely promising starting point for
large-scale, very high-resolution forest mapping with TanDEM-X data.

</details>


### [72] [NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results](https://arxiv.org/pdf/2505.03007)
*Nikolay Safonov, Alexey Bryncev, Andrey Moskalenko, Dmitry Kulikov, Dmitry Vatolin, Radu Timofte, Haibo Lei, Qifan Gao, Qing Luo, Yaqing Li, Jie Song, Shaozhe Hao, Meisong Zheng, Jingyi Xu, Chengbin Wu, Jiahui Liu, Ying Chen, Xin Deng, Mai Xu, Peipei Liang, Jie Ma, Junjie Jin, Yingxue Pang, Fangzhou Luo, Kai Chen, Shijie Zhao, Mingyang Wu, Renjie Li, Yushen Zuo, Shengyun Zhong, Zhengzhong Tu*

Main category: cs.CV

TL;DR: The NTIRE 2025 Challenge focused on enhancing user-generated content (UGC) videos with real-world degradations, attracting 25+ teams. Subjective evaluation involved 8000+ assessors, and 7 teams succeeded. Results and data are publicly available.


<details>
  <summary>Details</summary>
Motivation: UGC videos on short-form platforms often suffer from quality issues like noise and blur, making enhancement practically significant.

Method: Participants developed algorithms to improve UGC video quality. Evaluation used crowdsourced subjective assessments from 8000+ voters.

Result: 7 teams passed the final phase, providing insights into state-of-the-art UGC enhancement techniques.

Conclusion: The challenge highlights effective strategies and trends in UGC video enhancement, with all data made public for further research.

Abstract: This paper presents an overview of the NTIRE 2025 Challenge on UGC Video
Enhancement. The challenge constructed a set of 150 user-generated content
videos without reference ground truth, which suffer from real-world
degradations such as noise, blur, faded colors, compression artifacts, etc. The
goal of the participants was to develop an algorithm capable of improving the
visual quality of such videos. Given the widespread use of UGC on short-form
video platforms, this task holds substantial practical importance. The
evaluation was based on subjective quality assessment in crowdsourcing,
obtaining votes from over 8000 assessors. The challenge attracted more than 25
teams submitting solutions, 7 of which passed the final phase with source code
verification. The outcomes may provide insights into the state-of-the-art in
UGC video enhancement and highlight emerging trends and effective strategies in
this evolving research area. All data, including the processed videos and
subjective comparison votes and scores, is made publicly available at
https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.

</details>


### [73] [FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios](https://arxiv.org/pdf/2505.03730)
*Shiyi Zhang, Junhao Zhuang, Zhaoyang Zhang, Ying Shan, Yansong Tang*

Main category: cs.CV

TL;DR: FlexiAct enables action transfer from a reference video to a target image, accommodating layout, viewpoint, and skeletal variations while maintaining identity consistency. It uses RefAdapter for spatial adaptation and FAE for frequency-aware action extraction.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of current methods that enforce strict spatial constraints, reducing adaptability across diverse subjects and scenarios.

Method: Proposes FlexiAct with RefAdapter for spatial adaptation and FAE for action extraction during denoising.

Result: Effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints.

Conclusion: FlexiAct offers improved flexibility and consistency in action customization, supported by released code and models.

Abstract: Action customization involves generating videos where the subject performs
actions dictated by input control signals. Current methods use pose-guided or
global motion customization but are limited by strict constraints on spatial
structure, such as layout, skeleton, and viewpoint consistency, reducing
adaptability across diverse subjects and scenarios. To overcome these
limitations, we propose FlexiAct, which transfers actions from a reference
video to an arbitrary target image. Unlike existing methods, FlexiAct allows
for variations in layout, viewpoint, and skeletal structure between the subject
of the reference video and the target image, while maintaining identity
consistency. Achieving this requires precise action control, spatial structure
adaptation, and consistency preservation. To this end, we introduce RefAdapter,
a lightweight image-conditioned adapter that excels in spatial adaptation and
consistency preservation, surpassing existing methods in balancing appearance
consistency and structural flexibility. Additionally, based on our
observations, the denoising process exhibits varying levels of attention to
motion (low frequency) and appearance details (high frequency) at different
timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike
existing methods that rely on separate spatial-temporal architectures, directly
achieves action extraction during the denoising process. Experiments
demonstrate that our method effectively transfers actions to subjects with
diverse layouts, skeletons, and viewpoints. We release our code and model
weights to support further research at
https://shiyi-zh0408.github.io/projectpages/FlexiAct/

</details>


### [74] [Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant](https://arxiv.org/pdf/2505.03380)
*Haonan Wang, Jiaji Mao, Lehan Wang, Qixiang Zhang, Marawan Elbatel, Yi Qin, Huijun Hu, Baoxun Li, Wenhui Deng, Weifeng Qin, Hongrui Li, Jialin Liang, Jun Shen, Xiaomeng Li*

Main category: cs.CV

TL;DR: RCMed is a full-stack AI assistant improving multimodal alignment for medical tasks, achieving state-of-the-art precision and generalization in clinical applications.


<details>
  <summary>Details</summary>
Motivation: Medical AI assistants face challenges like limited accuracy with multimodal content and insufficient real-world validation. RCMed aims to address these by enhancing vision-language alignment.

Method: RCMed uses hierarchical vision-language grounding and a self-reinforcing correlation mechanism. It includes a color region description strategy to learn shape-location-text relationships across scales, trained on 20 million image-mask-description triplets.

Result: RCMed achieves a 23.5% relative improvement in cell segmentation and excels in 165 clinical tasks across 9 modalities, with strong performance in external validation for 20 cancer types.

Conclusion: RCMed demonstrates how integrated multimodal models enable human-level interpretation in complex medical scenarios, advancing AI healthcare.

Abstract: Medical AI assistants support doctors in disease diagnosis, medical image
analysis, and report generation. However, they still face significant
challenges in clinical use, including limited accuracy with multimodal content
and insufficient validation in real-world settings. We propose RCMed, a
full-stack AI assistant that improves multimodal alignment in both input and
output, enabling precise anatomical delineation, accurate localization, and
reliable diagnosis through hierarchical vision-language grounding. A
self-reinforcing correlation mechanism allows visual features to inform
language context, while language semantics guide pixel-wise attention, forming
a closed loop that refines both modalities. This correlation is enhanced by a
color region description strategy, translating anatomical structures into
semantically rich text to learn shape-location-text relationships across
scales. Trained on 20 million image-mask-description triplets, RCMed achieves
state-of-the-art precision in contextualizing irregular lesions and subtle
anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It
achieved a 23.5% relative improvement in cell segmentation from microscopy
images over prior methods. RCMed's strong vision-language alignment enables
exceptional generalization, with state-of-the-art performance in external
validation across 20 clinically significant cancer types, including novel
tasks. This work demonstrates how integrated multimodal models capture
fine-grained patterns, enabling human-level interpretation in complex scenarios
and advancing human-centric AI healthcare.

</details>


### [75] [GIF: Generative Inspiration for Face Recognition at Scale](https://arxiv.org/pdf/2505.03012)
*Saeed Ebrahimi, Sahar Rahimi, Ali Dabouei, Srinjoy Das, Jeremy M. Dawson, Nasser M. Nasrabadi*

Main category: cs.CV

TL;DR: The paper proposes a method to reduce computational cost in face recognition by replacing scalar labels with structured identity codes, achieving logarithmic cost scaling and improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the linear computational cost of Softmax in massive label spaces for face recognition, the paper seeks a more efficient alternative.

Method: The method tokenizes scalar labels into structured identity codes and trains the model to predict these codes, reducing computational cost.

Result: The approach outperforms competitors by 1.52% and 0.6% on IJB-B and IJB-C benchmarks, respectively, while achieving logarithmic cost scaling.

Conclusion: The proposed method effectively reduces computational cost and improves performance in face recognition tasks.

Abstract: Aiming to reduce the computational cost of Softmax in massive label space of
Face Recognition (FR) benchmarks, recent studies estimate the output using a
subset of identities. Although promising, the association between the
computation cost and the number of identities in the dataset remains linear
only with a reduced ratio. A shared characteristic among available FR methods
is the employment of atomic scalar labels during training. Consequently, the
input to label matching is through a dot product between the feature vector of
the input and the Softmax centroids. Inspired by generative modeling, we
present a simple yet effective method that substitutes scalar labels with
structured identity code, i.e., a sequence of integers. Specifically, we
propose a tokenization scheme that transforms atomic scalar labels into
structured identity codes. Then, we train an FR backbone to predict the code
for each input instead of its scalar label. As a result, the associated
computational cost becomes logarithmic w.r.t. number of identities. We
demonstrate the benefits of the proposed method by conducting experiments. In
particular, our method outperforms its competitors by 1.52%, and 0.6% at
TAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the
association between computational cost and the number of identities from linear
to logarithmic. See code at https://github.com/msed-Ebrahimi/GIF

</details>


### [76] [Panoramic Out-of-Distribution Segmentation](https://arxiv.org/pdf/2505.03539)
*Mengfei Duan, Kailun Yang, Yuheng Zhang, Yihong Cao, Fei Teng, Kai Luo, Jiaming Zhang, Zhiyong Li, Shutao Li*

Main category: cs.CV

TL;DR: The paper introduces Panoramic Out-of-distribution Segmentation (PanOoS) and proposes POS, a solution using text-guided prompt distribution learning to address OoS in panoramas, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current panoramic semantic segmentation methods struggle with outliers, and pinhole OoS models perform poorly in panoramic domains due to distortions and clutter.

Method: POS integrates a disentanglement strategy for CLIP, uses Prompt-based Restoration Attention (PRA) for semantic decoding, and Bilevel Prompt Distribution Learning (BPDL) for mask embeddings. Two benchmarks, DenseOoS and QuadOoS, are introduced.

Result: POS improves AuPRC by 34.25% and reduces FPR95 by 21.42% on DenseOoS, outperforming state-of-the-art methods. It also excels in closed-set segmentation.

Conclusion: POS effectively addresses PanOoS, demonstrating superior performance and generalization, with datasets and code made available for further research.

Abstract: Panoramic imaging enables capturing 360{\deg} images with an ultra-wide
Field-of-View (FoV) for dense omnidirectional perception. However, current
panoramic semantic segmentation methods fail to identify outliers, and pinhole
Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the
panoramic domain due to background clutter and pixel distortions. To address
these issues, we introduce a new task, Panoramic Out-of-distribution
Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the
first solution, POS, which adapts to the characteristics of panoramic images
through text-guided prompt distribution learning. Specifically, POS integrates
a disentanglement strategy designed to materialize the cross-domain
generalization capability of CLIP. The proposed Prompt-based Restoration
Attention (PRA) optimizes semantic decoding by prompt guidance and
self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)
refines the manifold of per-pixel mask embeddings via semantic prototype
supervision. Besides, to compensate for the scarcity of PanOoS datasets, we
establish two benchmarks: DenseOoS, which features diverse outliers in complex
environments, and QuadOoS, captured by a quadruped robot with a panoramic
annular lens system. Extensive experiments demonstrate superior performance of
POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,
outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves
leading closed-set segmentation capabilities. Code and datasets will be
available at https://github.com/MengfeiD/PanOoS.

</details>


### [77] [Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer](https://arxiv.org/pdf/2505.03018)
*Aurora Rofena, Arianna Manchia, Claudia Lucia Piccolo, Bruno Beomonte Zobel, Paolo Soda, Valerio Guarrasi*

Main category: cs.CV

TL;DR: Seg-CycleGAN, a deep learning model, synthesizes high-quality dual-energy subtracted CESM images from low-energy ones, reducing the need for contrast agents and radiation.


<details>
  <summary>Details</summary>
Motivation: CESM improves lesion visibility but has drawbacks like higher radiation and contrast medium side effects. The goal is to create a contrast-free alternative.

Method: Seg-CycleGAN uses lesion segmentation maps to guide image synthesis, adding localized loss terms to enhance diagnostically relevant areas.

Result: Outperforms baselines in PSNR and SSIM, with competitive MSE and VIF. Qualitative evaluations show better lesion fidelity.

Conclusion: Segmentation-aware generative models like Seg-CycleGAN can enable contrast-free CESM alternatives.

Abstract: Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic
technique that improves lesion visibility through the administration of an
iodinated contrast agent. It acquires both a low-energy image, comparable to
standard mammography, and a high-energy image, which are then combined to
produce a dual-energy subtracted image highlighting lesion contrast
enhancement. While CESM offers superior diagnostic accuracy compared to
standard mammography, its use entails higher radiation exposure and potential
side effects associated with the contrast medium. To address these limitations,
we propose Seg-CycleGAN, a generative deep learning framework for Virtual
Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy
subtracted images from low-energy images, leveraging lesion segmentation maps
to guide the generative process and improve lesion reconstruction. Building
upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss
terms focused on lesion areas, enhancing the synthesis of diagnostically
relevant regions. Experiments on the CESM@UCBM dataset demonstrate that
Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while
maintaining competitive MSE and VIF. Qualitative evaluations further confirm
improved lesion fidelity in the generated images. These results suggest that
segmentation-aware generative models offer a viable pathway toward
contrast-free CESM alternatives.

</details>


### [78] [Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/pdf/2505.02549)
*Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, Peng Hu*

Main category: cs.CV

TL;DR: A novel Robust Duality Learning framework (RoDE) addresses pseudo-label noise in unsupervised visible-infrared person re-identification (UVI-ReID) by dynamically emphasizing clean samples, using dual models to prevent error accumulation, and aligning clusters across modalities.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume pseudo-labels are correct, but noise in these labels hinders model performance. The paper aims to mitigate pseudo-label noise challenges: overfitting, error accumulation, and noisy cluster correspondence.

Method: Proposes RoDE with Robust Adaptive Learning (RAL) to weight samples, dual-model training to prevent error accumulation, and Cluster Consistency Matching (CCM) to align clusters across models and modalities.

Result: Extensive experiments on three benchmarks validate RoDE's effectiveness in handling pseudo-label noise and improving UVI-ReID performance.

Conclusion: RoDE successfully addresses pseudo-label noise challenges, offering a robust solution for unsupervised cross-modal person re-identification.

Abstract: Unsupervised visible-infrared person re-identification (UVI-ReID) aims to
retrieve pedestrian images across different modalities without costly
annotations, but faces challenges due to the modality gap and lack of
supervision. Existing methods often adopt self-training with
clustering-generated pseudo-labels but implicitly assume these labels are
always correct. In practice, however, this assumption fails due to inevitable
pseudo-label noise, which hinders model learning. To address this, we introduce
a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),
characterized by three key challenges: noise overfitting, error accumulation,
and noisy cluster correspondence. To this end, we propose a novel Robust
Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy
pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning
mechanism (RAL) is proposed to dynamically emphasize clean samples while
down-weighting noisy ones. Second, to alleviate error accumulation-where the
model reinforces its own mistakes-RoDE employs dual distinct models that are
alternately trained using pseudo-labels from each other, encouraging diversity
and preventing collapse. However, this dual-model strategy introduces
misalignment between clusters across models and modalities, creating noisy
cluster correspondence. To resolve this, we introduce Cluster Consistency
Matching (CCM), which aligns clusters across models and modalities by measuring
cross-cluster similarity. Extensive experiments on three benchmarks demonstrate
the effectiveness of RoDE.

</details>


### [79] [An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices](https://arxiv.org/pdf/2505.03039)
*Yuezhou Zhang, Amos A. Folarin, Callum Stewart, Heet Sankesara, Yatharth Ranjan, Pauline Conde, Akash Roy Choudhury, Shaoxiong Sun, Zulqarnain Rashid, Richard J. B. Dobson*

Main category: cs.CV

TL;DR: An explainable anomaly detection framework using wearable data detects worsening depression and anxiety with high accuracy, leveraging LSTM autoencoders and SHAP analysis for interpretability.


<details>
  <summary>Details</summary>
Motivation: To enable early, objective detection of worsening mental health symptoms using consumer-grade wearable devices for proactive monitoring.

Method: LSTM autoencoder model trained on healthy baseline data (sleep, step count, resting heart rate) to detect anomalies when symptom scores increased by ≥5 points.

Result: Achieved F1-score of 0.80, with higher performance for concurrent depression/anxiety (F1=0.84) and larger symptom changes (F1=0.85). Resting heart rate was the most influential feature.

Conclusion: Explainable anomaly detection can enable scalable, personalized mental health monitoring in real-world settings.

Abstract: Continuous monitoring of behavior and physiology via wearable devices offers
a novel, objective method for the early detection of worsening depression and
anxiety. In this study, we present an explainable anomaly detection framework
that identifies clinically meaningful increases in symptom severity using
consumer-grade wearable data. Leveraging data from 2,023 participants with
defined healthy baselines, our LSTM autoencoder model learned normal health
patterns of sleep duration, step count, and resting heart rate. Anomalies were
flagged when self-reported depression or anxiety scores increased by >=5 points
(a threshold considered clinically significant). The model achieved an adjusted
F1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393
symptom-worsening episodes across 341 participants, with higher performance
observed for episodes involving concurrent depression and anxiety escalation
(F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 =
0.85). Model interpretability was supported by SHAP-based analysis, which
identified resting heart rate as the most influential feature in 71.4
percentage of detected anomalies, followed by physical activity and sleep.
Together, our findings highlight the potential of explainable anomaly detection
to enable personalized, scalable, and proactive mental health monitoring in
real-world settings.

</details>


### [80] [Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera](https://arxiv.org/pdf/2505.03093)
*Siming He, Zachary Osman, Fernando Cladera, Dexter Ong, Nitant Rai, Patrick Corey Green, Vijay Kumar, Pratik Chaudhari*

Main category: cs.CV

TL;DR: A low-cost method using a consumer-grade 360 camera achieves near-LiDAR accuracy (5-9% error) for measuring tree DBH, with minimal setup and cost.


<details>
  <summary>Details</summary>
Motivation: High costs and complexity of LiDAR-based DBH measurements necessitate a cheaper, simpler alternative for forest inventories.

Method: Uses SfM photogrammetry, semantic trunk segmentation with Grounded SAM, and RANSAC-based DBH estimation. Includes an interactive visualization tool.

Result: Achieves median absolute relative errors of 5-9% compared to manual measurements, close to LiDAR accuracy.

Conclusion: The method offers a viable, cost-effective alternative to LiDAR for DBH measurement in forest inventories.

Abstract: Forest inventories rely on accurate measurements of the diameter at breast
height (DBH) for ecological monitoring, resource management, and carbon
accounting. While LiDAR-based techniques can achieve centimeter-level
precision, they are cost-prohibitive and operationally complex. We present a
low-cost alternative that only needs a consumer-grade 360 video camera. Our
semi-automated pipeline comprises of (i) a dense point cloud reconstruction
using Structure from Motion (SfM) photogrammetry software called Agisoft
Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment
Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based
technique to estimate cross section shape and DBH. We introduce an interactive
visualization tool for inspecting segmented trees and their estimated DBH. On
61 acquisitions of 43 trees under a variety of conditions, our method attains
median absolute relative errors of 5-9% with respect to "ground-truth" manual
measurements. This is only 2-4% higher than LiDAR-based estimates, while
employing a single 360 camera that costs orders of magnitude less, requires
minimal setup, and is widely available.

</details>


### [81] [Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability](https://arxiv.org/pdf/2505.03097)
*Lei Wang, Senmao Li, Fei Yang, Jianye Wang, Ziheng Zhang, Yuhan Liu, Yaxing Wang, Jian Yang*

Main category: cs.CV

TL;DR: The paper explores time-wise diffusion models, proposing MaskUNet to enhance generation quality by selectively zeroing out U-Net parameters, achieving superior results on COCO.


<details>
  <summary>Details</summary>
Motivation: Diffusion models handle structure and texture simultaneously, unlike traditional architectures. This inspires investigation into parameter contributions and optimization.

Method: Proposes MaskUNet, leveraging timestep- and sample-dependent U-Net parameters, with two fine-tuning strategies: training-based and training-free.

Result: MaskUNet achieves the best FID score on COCO in zero-shot inference and excels in downstream tasks.

Conclusion: MaskUNet is a simple yet effective method for improving diffusion model performance with minimal added parameters.

Abstract: The diffusion models, in early stages focus on constructing basic image
structures, while the refined details, including local features and textures,
are generated in later stages. Thus the same network layers are forced to learn
both structural and textural information simultaneously, significantly
differing from the traditional deep learning architectures (e.g., ResNet or
GANs) which captures or generates the image semantic information at different
layers. This difference inspires us to explore the time-wise diffusion models.
We initially investigate the key contributions of the U-Net parameters to the
denoising process and identify that properly zeroing out certain parameters
(including large parameters) contributes to denoising, substantially improving
the generation quality on the fly. Capitalizing on this discovery, we propose a
simple yet effective method-termed ``MaskUNet''- that enhances generation
quality with negligible parameter numbers. Our method fully leverages timestep-
and sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer
two fine-tuning strategies: a training-based approach and a training-free
approach, including tailored networks and optimization functions. In zero-shot
inference on the COCO dataset, MaskUNet achieves the best FID score and further
demonstrates its effectiveness in downstream task evaluations. Project page:
https://gudaochangsheng.github.io/MaskUnet-Page/

</details>


### [82] [Image Recognition with Online Lightweight Vision Transformer: A Survey](https://arxiv.org/pdf/2505.03113)
*Zherui Zhang, Rongtao Xu, Jie Zhou, Changwei Wang, Xingtian Pei, Wenhao Xu, Jiguang Zhang, Li Guo, Longxiang Gao, Wenbo Xu, Shibiao Xu*

Main category: cs.CV

TL;DR: This paper surveys lightweight vision transformers for image recognition, focusing on Efficient Component Design, Dynamic Network, and Knowledge Distillation, evaluating trade-offs and proposing future directions.


<details>
  <summary>Details</summary>
Motivation: Transformers in vision lack efficiency and inductive biases, limiting real-world use. This work aims to address these challenges.

Method: Surveys and evaluates three strategies: Efficient Component Design, Dynamic Network, and Knowledge Distillation, tested on ImageNet-1K.

Result: Analyzes trade-offs in precision, parameters, and throughput, highlighting pros and cons of each approach.

Conclusion: Proposes future research directions and challenges for lightweight vision transformers to guide the community.

Abstract: The Transformer architecture has achieved significant success in natural
language processing, motivating its adaptation to computer vision tasks. Unlike
convolutional neural networks, vision transformers inherently capture
long-range dependencies and enable parallel processing, yet lack inductive
biases and efficiency benefits, facing significant computational and memory
challenges that limit its real-world applicability. This paper surveys various
online strategies for generating lightweight vision transformers for image
recognition, focusing on three key areas: Efficient Component Design, Dynamic
Network, and Knowledge Distillation. We evaluate the relevant exploration for
each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,
parameters, throughput, and more to highlight their respective advantages,
disadvantages, and flexibility. Finally, we propose future research directions
and potential challenges in the lightweighting of vision transformers with the
aim of inspiring further exploration and providing practical guidance for the
community. Project Page: https://github.com/ajxklo/Lightweight-VIT

</details>


### [83] [Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation](https://arxiv.org/pdf/2505.03114)
*Teng Zhou, Jax Luo, Yuping Sun, Yiheng Tan, Shun Yao, Nazim Haouchine, Scott Raymond*

Main category: cs.CV

TL;DR: Proposes a path- and bone-contour regularized method for unpaired MRI-to-CT translation, improving accuracy for bone structures and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate MRI-to-CT translation is needed for applications like radiation therapy, but current unpaired methods struggle with bone structures.

Method: Projects MRI and CT images to a shared latent space, models MRI-to-CT as a continuous flow using neural ODEs, and introduces bone-contour regularization.

Result: Outperforms existing methods with lower error rates and better bone structure fidelity in segmentation tasks.

Conclusion: The proposed method advances unpaired MRI-to-CT translation, especially for bone structures, with practical benefits for radiation therapy.

Abstract: Accurate MRI-to-CT translation promises the integration of complementary
imaging information without the need for additional imaging sessions. Given the
practical challenges associated with acquiring paired MRI and CT scans, the
development of robust methods capable of leveraging unpaired datasets is
essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT
translation methods, which predominantly rely on cycle consistency and
contrastive learning frameworks, frequently encounter challenges in accurately
translating anatomical features that are highly discernible on CT but less
distinguishable on MRI, such as bone structures. This limitation renders these
approaches less suitable for applications in radiation therapy, where precise
bone representation is essential for accurate treatment planning. To address
this challenge, we propose a path- and bone-contour regularized approach for
unpaired MRI-to-CT translation. In our method, MRI and CT images are projected
to a shared latent space, where the MRI-to-CT mapping is modeled as a
continuous flow governed by neural ordinary differential equations. The optimal
mapping is obtained by minimizing the transition path length of the flow. To
enhance the accuracy of translated bone structures, we introduce a trainable
neural network to generate bone contours from MRI and implement mechanisms to
directly and indirectly encourage the model to focus on bone contours and their
adjacent regions. Evaluations conducted on three datasets demonstrate that our
method outperforms existing unpaired MRI-to-CT translation approaches,
achieving lower overall error rates. Moreover, in a downstream bone
segmentation task, our approach exhibits superior performance in preserving the
fidelity of bone structures. Our code is available at:
https://github.com/kennysyp/PaBoT.

</details>


### [84] [TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion](https://arxiv.org/pdf/2505.03116)
*Haoyue Liu, Jinghan Xu, Yi Chang, Hanyu Zhou, Haozhi Zhao, Lin Wang, Luxin Yan*

Main category: cs.CV

TL;DR: The paper proposes TimeTracker, a novel event-based video frame interpolation (VFI) framework that improves motion estimation and interpolation quality by tracking continuous object motion trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing event-based VFI methods struggle with non-linear motion due to misalignment between event cues and image features, degrading interpolation quality.

Method: TimeTracker uses Scene-Aware Region Segmentation (SARS) to divide scenes into patches and Continuous Trajectory guided Motion Estimation (CTME) to track patch motion via events, followed by global optimization and refinement.

Result: The method outperforms prior works in motion estimation and interpolation quality, validated on a real-world dataset with fast non-linear motion.

Conclusion: TimeTracker effectively addresses non-linear motion challenges in event-based VFI, offering superior performance and accuracy.

Abstract: Video frame interpolation (VFI) that leverages the bio-inspired event cameras
as guidance has recently shown better performance and memory efficiency than
the frame-based methods, thanks to the event cameras' advantages, such as high
temporal resolution. A hurdle for event-based VFI is how to effectively deal
with non-linear motion, caused by the dynamic changes in motion direction and
speed within the scene. Existing methods either use events to estimate sparse
optical flow or fuse events with image features to estimate dense optical flow.
Unfortunately, motion errors often degrade the VFI quality as the continuous
motion cues from events do not align with the dense spatial information of
images in the temporal dimension. In this paper, we find that object motion is
continuous in space, tracking local regions over continuous time enables more
accurate identification of spatiotemporal feature correlations. In light of
this, we propose a novel continuous point tracking-based VFI framework, named
TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation
(SARS) module to divide the scene into similar patches. Then, a Continuous
Trajectory guided Motion Estimation (CTME) module is proposed to track the
continuous motion trajectory of each patch through events. Finally,
intermediate frames at any given time are generated through global motion
optimization and frame refinement. Moreover, we collect a real-world dataset
that features fast non-linear motion. Extensive experiments show that our
method outperforms prior arts in both motion estimation and frame interpolation
quality.

</details>


### [85] [VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis](https://arxiv.org/pdf/2505.03132)
*Xinyuan Yan, Xiwei Xuan, Jorge Piazentin Ono, Jiajing Guo, Vikram Mohanty, Shekar Arvind Kumar, Liang Gou, Bei Wang, Liu Ren*

Main category: cs.CV

TL;DR: VISLIX is a visual analytics framework for analyzing data slices in computer vision models without metadata, offering automated insights and interactive hypothesis testing.


<details>
  <summary>Details</summary>
Motivation: Challenges in data slicing for vision models include reliance on metadata, labor-intensive analysis, and lack of interactive solutions.

Method: VISLIX uses foundation models to analyze slices, generates natural language insights, and enables interactive hypothesis testing.

Result: Evaluated with expert studies and use cases, VISLIX effectively validates object detection models.

Conclusion: VISLIX addresses limitations in data slicing, enhancing model validation in computer vision.

Abstract: Real-world machine learning models require rigorous evaluation before
deployment, especially in safety-critical domains like autonomous driving and
surveillance. The evaluation of machine learning models often focuses on data
slices, which are subsets of the data that share a set of characteristics. Data
slice finding automatically identifies conditions or data subgroups where
models underperform, aiding developers in mitigating performance issues.
Despite its popularity and effectiveness, data slicing for vision model
validation faces several challenges. First, data slicing often needs additional
image metadata or visual concepts, and falls short in certain computer vision
tasks, such as object detection. Second, understanding data slices is a
labor-intensive and mentally demanding process that heavily relies on the
expert's domain knowledge. Third, data slicing lacks a human-in-the-loop
solution that allows experts to form hypothesis and test them interactively. To
overcome these limitations and better support the machine learning operations
lifecycle, we introduce VISLIX, a novel visual analytics framework that employs
state-of-the-art foundation models to help domain experts analyze slices in
computer vision models. Our approach does not require image metadata or visual
concepts, automatically generates natural language insights, and allows users
to test data slice hypothesis interactively. We evaluate VISLIX with an expert
study and three use cases, that demonstrate the effectiveness of our tool in
providing comprehensive insights for validating object detection models.

</details>


### [86] [Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control](https://arxiv.org/pdf/2505.03134)
*Sajjad Rezvani Boroujeni, Hossein Abedi, Tom Bush*

Main category: cs.CV

TL;DR: The paper proposes using Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic defective glass images, improving deep learning model performance in imbalanced datasets for industrial defect detection.


<details>
  <summary>Details</summary>
Motivation: Addressing class imbalance in industrial glass defect detection due to low defective product frequency, which limits deep learning model performance.

Method: Uses DDPMs for synthetic image generation to augment minority class data, tested with CNN architectures (ResNet50V2, EfficientNetB0, MobileNetV2).

Result: Significant improvements in recall and accuracy, e.g., ResNet50V2's accuracy rose from 78% to 93%, while maintaining perfect precision.

Conclusion: The approach is scalable and cost-effective for defect detection, with potential applications in other industries facing class imbalance.

Abstract: Visual defect detection in industrial glass manufacturing remains a critical
challenge due to the low frequency of defective products, leading to imbalanced
datasets that limit the performance of deep learning models and computer vision
systems. This paper presents a novel approach using Denoising Diffusion
Probabilistic Models (DDPMs) to generate synthetic defective glass product
images for data augmentation, effectively addressing class imbalance issues in
manufacturing quality control and automated visual inspection. The methodology
significantly enhances image classification performance of standard CNN
architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting
anomalies by increasing the minority class representation. Experimental results
demonstrate substantial improvements in key machine learning metrics,
particularly in recall for defective samples across all tested deep neural
network architectures while maintaining perfect precision. The most dramatic
improvement was observed in ResNet50V2's overall classification accuracy, which
increased from 78 percent to 93 percent when trained with the augmented data.
This work provides a scalable, cost-effective approach to enhancing automated
defect detection in glass manufacturing that can potentially be extended to
other industrial quality assurance systems and industries with similar class
imbalance challenges.

</details>


### [87] [Infrared Image Deturbulence Restoration Using Degradation Parameter-Assisted Wide & Deep Learning](https://arxiv.org/pdf/2305.18708)
*Yi Lu, Yadong Wang, Xingbo Jiang, Xiangzhi Bai*

Main category: cs.CV

TL;DR: DparNet, a parameter-assisted multi-frame network, learns degradation priors from degraded infrared images to enhance restoration, outperforming SOTA methods with improved PSNR and efficiency.


<details>
  <summary>Details</summary>
Motivation: Infrared images under turbulence suffer from distortions and blur, requiring adaptive restoration methods.

Method: DparNet uses a wide & deep architecture to learn degradation priors (key parameters) from degraded images, modulating restoration adaptively.

Result: Outperforms SOTA in restoration (PSNR improved by 0.6-1.1 dB) with minimal increase in model complexity (<2%).

Conclusion: Degraded images contain learnable degradation information, enabling adaptive and efficient restoration.

Abstract: Infrared images captured under turbulent conditions are degraded by complex
geometric distortions and blur. We address infrared deturbulence as an image
restoration task, proposing DparNet, a parameter-assisted multi-frame network
with a wide & deep architecture. DparNet learns a degradation prior (key
parameter matrix) directly from degraded images without external knowledge. Its
wide & deep architecture uses these learned parameters to directly modulate
restoration, achieving spatially and intensity adaptive results. Evaluated on
dedicated infrared deturbulence (49,744 images) and visible image denoising
(109,536 images) datasets, DparNet significantly outperforms State-of-the-Art
(SOTA) methods in restoration performance and efficiency. Notably, leveraging
these parameters improves PSNR by 0.6-1.1 dB with less than 2% increase in
model parameters and computational complexity. Our work demonstrates that
degraded images hide key degradation information that can be learned and
utilized to boost adaptive image restoration.

</details>


### [88] [Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)](https://arxiv.org/pdf/2505.03149)
*Joseph William Kettelkamp, Ludovica Romanin, Sarv Priya, Mathews Jacob*

Main category: cs.CV

TL;DR: An unsupervised algorithm for motion-compensated 3D cardiac MRI reconstruction using a low-rank model for diffeomorphisms, improving recovery over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing high-quality images from free-breathing and ungated 3D cardiac MRI data without supervision.

Method: Represents each motion phase as a deformation of a static template, using a low-rank model for diffeomorphisms derived from parametric velocity fields. Parameters are learned directly from k-space data.

Result: The constrained motion model improves image recovery compared to current motion-resolved and motion-compensated algorithms.

Conclusion: The proposed unsupervised approach offers a promising solution for free-breathing 3D cine MRI reconstruction.

Abstract: We introduce an unsupervised motion-compensated image reconstruction
algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging
(MRI). We express the image volume corresponding to each specific motion phase
as the deformation of a single static image template. The main contribution of
the work is the low-rank model for the compact joint representation of the
family of diffeomorphisms, parameterized by the motion phases. The
diffeomorphism at a specific motion phase is obtained by integrating a
parametric velocity field along a path connecting the reference template phase
to the motion phase. The velocity field at different phases is represented
using a low-rank model. The static template and the low-rank motion model
parameters are learned directly from the k-space data in an unsupervised
fashion. The more constrained motion model is observed to offer improved
recovery compared to current motion-resolved and motion-compensated algorithms
for free-breathing 3D cine MRI.

</details>


### [89] [CardioSyntax: end-to-end SYNTAX score prediction -- dataset, benchmark and method](https://arxiv.org/pdf/2407.19894)
*Alexander Ponomarchuk, Ivan Kruzhilov, Galina Zubkova, Artem Shadrin, Ruslan Utegenov, Ivan Bessonov, Pavel Blinov*

Main category: cs.CV

TL;DR: The paper introduces a method to automatically estimate the SYNTAX score from coronary angiography using a new dataset (CardioSYNTAX) and achieves solid results.


<details>
  <summary>Details</summary>
Motivation: The SYNTAX score is critical for revascularization decisions, but manual estimation is complex. Automating this process can improve efficiency and accuracy.

Method: A novel end-to-end method is proposed, leveraging a comprehensive dataset (3,018 patients) with multi-view X-ray videos of coronary angiography.

Result: Achieved an R2 of 0.51 for score prediction and 77.3% accuracy for zero-score classification.

Conclusion: The method and dataset provide a robust foundation for automating SYNTAX score estimation, aiding clinical decision-making.

Abstract: The SYNTAX score has become a widely used measure of coronary disease
severity, crucial in selecting the optimal mode of the revascularization
procedure. This paper introduces a new medical regression and classification
problem - automatically estimating SYNTAX score from coronary angiography. Our
study presents a comprehensive CardioSYNTAX dataset of 3,018 patients for the
SYNTAX score estimation and coronary dominance classification. The dataset
features a balanced distribution of individuals with zero and non-zero scores.
This dataset includes a first-of-its-kind, complete coronary angiography
samples captured through a multi-view X-ray video, allowing one to observe
coronary arteries from multiple perspectives. Furthermore, we present a novel,
fully automatic end-to-end method for estimating the SYNTAX. For such a
difficult task, we have achieved a solid coefficient of determination R2 of
0.51 in score value prediction and 77.3% accuracy for zero score
classification.

</details>


### [90] [Robust Fairness Vision-Language Learning for Medical Image Analysis](https://arxiv.org/pdf/2505.03153)
*Sparsh Bansal, Mingyang Wu, Xin Wang, Shu Hu*

Main category: cs.CV

TL;DR: A framework for improving fairness and robustness in Vision-Language Models (VLMs) for medical image analysis by adjusting faulty image-text pairs and ensuring balanced loss distributions.


<details>
  <summary>Details</summary>
Motivation: Address fairness and robustness in VLMs for medical applications to ensure reliable performance across all patient groups.

Method: Introduces a framework with Dynamic Bad Pair Mining and Sinkhorn distance to adjust faulty pairs and balance loss distributions.

Result: Achieves up to 8.6% improvement in equity-scaled AUC.

Conclusion: The framework effectively enhances fairness and robustness in VLMs for medical image analysis.

Abstract: The advent of Vision-Language Models (VLMs) in medical image analysis has the
potential to help process multimodal inputs and increase performance over
traditional inference methods. However, when considering the domain in which
these models will be implemented, fairness and robustness are important to
ensure the model stays true for any patient. In this paper, we introduce a
framework for ensuring robustness and fairness of VLM models. This framework
modifies the loss function at training by identifying and adjusting faulty
image-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing
Sinkhorn distance to ensure the loss distributions of protected groups do not
deviate from the total loss. Experimental testing of our framework shows up to
a 8.6\% improvement when looking at equity-scaled AUC.

</details>


### [91] [StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data](https://arxiv.org/pdf/2505.03154)
*Yuxuan Mu, Hung Yu Ling, Yi Shi, Ismael Baira Ojeda, Pengcheng Xi, Chang Shu, Fabio Zinno, Xue Bin Peng*

Main category: cs.CV

TL;DR: StableMotion automates motion capture cleanup using unpaired datasets and quality indicators, reducing manual effort and artifacts.


<details>
  <summary>Details</summary>
Motivation: Manual cleanup of corrupted mocap data is costly and time-consuming, and existing methods require paired training data, which is hard to obtain.

Method: Uses motion quality indicators to train a diffusion-based model on unpaired data, enabling quality-aware motion generation and cleanup.

Result: Reduces motion pops by 68% and frozen frames by 81% on the SoccerMocap dataset.

Conclusion: StableMotion is a practical solution for mocap cleanup, eliminating the need for paired data and manual effort.

Abstract: Motion capture (mocap) data often exhibits visually jarring artifacts due to
inaccurate sensors and post-processing. Cleaning this corrupted data can
require substantial manual effort from human experts, which can be a costly and
time-consuming process. Previous data-driven motion cleanup methods offer the
promise of automating this cleanup process, but often require in-domain paired
corrupted-to-clean training data. Constructing such paired datasets requires
access to high-quality, relatively artifact-free motion clips, which often
necessitates laborious manual cleanup. In this work, we present StableMotion, a
simple yet effective method for training motion cleanup models directly from
unpaired corrupted datasets that need cleanup. The core component of our method
is the introduction of motion quality indicators, which can be easily annotated
through manual labeling or heuristic algorithms and enable training of
quality-aware motion generation models on raw motion data with mixed quality.
At test time, the model can be prompted to generate high-quality motions using
the quality indicators. Our method can be implemented through a simple
diffusion-based framework, leading to a unified motion generate-discriminate
model, which can be used to both identify and fix corrupted frames. We
demonstrate that our proposed method is effective for training motion cleanup
models on raw mocap data in production scenarios by applying StableMotion to
SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion
artifacts. The trained model effectively corrects a wide range of motion
artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.
See https://youtu.be/3Y7MMAH02B4 for more results.

</details>


### [92] [RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph](https://arxiv.org/pdf/2505.03173)
*Sameer Malik, Moyuru Yamada, Ayush Singh, Dishank Aggarwal*

Main category: cs.CV

TL;DR: RAVU is a framework for long video understanding using retrieval-augmented compositional reasoning over a spatio-temporal graph, outperforming SOTA methods with limited frames.


<details>
  <summary>Details</summary>
Motivation: Current LMMs struggle with long videos due to lack of memory and retrieval mechanisms, limiting their ability to track objects and actions over time.

Method: RAVU constructs a spatio-temporal graph as long-term memory, decomposes queries into reasoning steps, and retrieves key information from the graph.

Result: RAVU achieves superior performance on video QA datasets (NExT-QA and EgoSchema) with only 5-10 retrieved frames.

Conclusion: RAVU effectively addresses the challenge of long video understanding by leveraging retrieval-augmented compositional reasoning.

Abstract: Comprehending long videos remains a significant challenge for Large
Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to
hours videos due to their lack of explicit memory and retrieval mechanisms. To
address this limitation, we propose RAVU (Retrieval Augmented Video
Understanding), a novel framework for video understanding enhanced by retrieval
with compositional reasoning over a spatio-temporal graph. We construct a graph
representation of the video, capturing both spatial and temporal relationships
between entities. This graph serves as a long-term memory, allowing us to track
objects and their actions across time. To answer complex queries, we decompose
the queries into a sequence of reasoning steps and execute these steps on the
graph, retrieving relevant key information. Our approach enables more accurate
understanding of long videos, particularly for queries that require multi-hop
reasoning and tracking objects across frames. Our approach demonstrate superior
performances with limited retrieved frames (5-10) compared with other SOTA
methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.

</details>


### [93] [seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models](https://arxiv.org/pdf/2505.03176)
*Hafez Ghaemi, Eilif Muller, Shahab Bakhtiari*

Main category: cs.CV

TL;DR: seq-JEPA introduces a world modeling paradigm using joint-embedding predictive architecture to learn both invariant and equivariant representations without trade-offs, excelling in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: The two-view paradigm in self-supervised learning limits flexibility by creating trade-offs between invariance (e.g., classification) and equivariance (e.g., fine-grained tasks).

Method: seq-JEPA processes a sequence of image views, concatenates embeddings with relative transformations, and uses a transformer encoder to predict representations, segregating invariant and equivariant features.

Result: seq-JEPA achieves strong performance on both equivariant benchmarks and image classification, excelling in tasks requiring sequence aggregation.

Conclusion: seq-JEPA resolves the trade-off between invariance and equivariance, offering a flexible and effective framework for diverse visual representation tasks.

Abstract: Current self-supervised algorithms mostly rely on transformations such as
data augmentation and masking to learn visual representations. This is achieved
by inducing invariance or equivariance with respect to these transformations
after encoding two views of an image. This dominant two-view paradigm can limit
the flexibility of learned representations for downstream adaptation by
creating performance trade-offs between invariance-related tasks such as image
classification and more fine-grained equivariance-related tasks. In this work,
we introduce \emph{seq-JEPA}, a world modeling paradigm based on
joint-embedding predictive architecture that leverages architectural inductive
biases to resolve this trade-off. Without requiring an additional equivariance
predictor or loss term, seq-JEPA simultaneously learns two architecturally
segregated representations: one equivariant to the specified transformations
and another invariant to them and suited for tasks such as classification. To
do so, our model processes a short sequence of different views (observations)
of an input image. Each encoded view is concatenated with embeddings
corresponding to the relative transformation (action) producing the next
observation in the sequence. A transformer encoder outputs an aggregate
representation of this sequence, which is subsequently conditioned on the
action leading to the next observation to predict its representation.
Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and
image classification without sacrificing one for the other. Additionally, our
framework excels at tasks that inherently require aggregating a sequence of
observations, such as path integration across actions and predictive learning
across eye movements.

</details>


### [94] [Interactive Instance Annotation with Siamese Networks](https://arxiv.org/pdf/2505.03184)
*Xiang Xu, Ruotong Li, Mengjun Yi, Baile XU, Furao Shen, Jian Zhao*

Main category: cs.CV

TL;DR: SiamAnno, a Siamese network-based framework, enables cross-domain instance annotation by predicting object boundaries from a bounding box, achieving SOTA performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Manual instance mask annotation is labor-intensive. Existing methods lack effectiveness for cross-domain tasks, prompting the need for a robust solution.

Method: SiamAnno uses one-shot learning with Siamese networks to predict object boundaries from a bounding box, allowing annotator refinement.

Result: Achieves SOTA performance across datasets without fine-tuning, handling domain shifts effectively.

Conclusion: SiamAnno sets a strong baseline for cross-domain instance annotation, pioneering Siamese networks in this task.

Abstract: Annotating instance masks is time-consuming and labor-intensive. A promising
solution is to predict contours using a deep learning model and then allow
users to refine them. However, most existing methods focus on in-domain
scenarios, limiting their effectiveness for cross-domain annotation tasks. In
this paper, we propose SiamAnno, a framework inspired by the use of Siamese
networks in object tracking. SiamAnno leverages one-shot learning to annotate
previously unseen objects by taking a bounding box as input and predicting
object boundaries, which can then be adjusted by annotators. Trained on one
dataset and tested on another without fine-tuning, SiamAnno achieves
state-of-the-art (SOTA) performance across multiple datasets, demonstrating its
ability to handle domain and environment shifts in cross-domain tasks. We also
provide more comprehensive results compared to previous work, establishing a
strong baseline for future research. To our knowledge, SiamAnno is the first
model to explore Siamese architecture for instance annotation.

</details>


### [95] [PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models](https://arxiv.org/pdf/2505.03203)
*Chang Xie, Chenyi Zhuang, Pan Gao*

Main category: cs.CV

TL;DR: PiCo improves text-to-image alignment in diffusion models by selecting better noise and using precise masks.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with text-image alignment for complex prompts due to poor noise quality and unreliable masks.

Method: PiCo introduces a noise selection module and a referring mask module to enhance alignment without training.

Result: Experiments show PiCo improves alignment and reduces the need for random generation.

Conclusion: PiCo effectively addresses alignment challenges in text-to-image generation.

Abstract: Advanced diffusion models have made notable progress in text-to-image
compositional generation. However, it is still a challenge for existing models
to achieve text-image alignment when confronted with complex text prompts. In
this work, we highlight two factors that affect this alignment: the quality of
the randomly initialized noise and the reliability of the generated controlling
mask. We then propose PiCo (Pick-and-Control), a novel training-free approach
with two key components to tackle these two factors. First, we develop a noise
selection module to assess the quality of the random noise and determine
whether the noise is suitable for the target text. A fast sampling strategy is
utilized to ensure efficiency in the noise selection stage. Second, we
introduce a referring mask module to generate pixel-level masks and to
precisely modulate the cross-attention maps. The referring mask is applied to
the standard diffusion process to guide the reasonable interaction between text
and image features. Extensive experiments have been conducted to verify the
effectiveness of PiCo in liberating users from the tedious process of random
generation and in enhancing the text-image alignment for diverse text
descriptions.

</details>


### [96] [DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations](https://arxiv.org/pdf/2505.03204)
*Liu Suxing, Byungwon Min*

Main category: cs.CV

TL;DR: Deep learning struggles with limited annotated data in breast cancer histopathology image classification.


<details>
  <summary>Details</summary>
Motivation: High cost and expertise for annotations in medical imaging limit data availability.

Method: Deep learning applied to breast cancer histopathology images.

Result: Performance declines with limited annotated data.

Conclusion: Addressing data scarcity is crucial for improving deep learning in medical imaging.

Abstract: Deep learning methods have shown promise in classifying breast cancer
histopathology images, but their performance often declines with limited
annotated data, a critical challenge in medical imaging due to the high cost
and expertise required for annotations.

</details>


### [97] [Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data](https://arxiv.org/pdf/2505.03220)
*Shaheer Mohamed, Tharindu Fernando, Sridha Sridharan, Peyman Moghadam, Clinton Fookes*

Main category: cs.CV

TL;DR: The paper proposes SFMIM, a self-supervised pretraining method for hyperspectral images using dual-domain masking (spatial and frequency) to address labeled data scarcity. It achieves state-of-the-art performance on HSI classification benchmarks.


<details>
  <summary>Details</summary>
Motivation: Labeled HSI data is scarce, limiting deep learning potential, especially for transformer-based models requiring large-scale training.

Method: SFMIM uses spatial and frequency masking: spatial masking hides patches, and frequency masking removes spectral components. The model reconstructs masked inputs to learn spectral-spatial correlations.

Result: SFMIM achieves state-of-the-art performance on three HSI benchmarks and shows rapid convergence during fine-tuning.

Conclusion: SFMIM effectively leverages unlabeled HSI data for pretraining, enhancing transformer-based models' performance and efficiency.

Abstract: Hyperspectral images (HSIs) capture rich spectral signatures that reveal
vital material properties, offering broad applicability across various domains.
However, the scarcity of labeled HSI data limits the full potential of deep
learning, especially for transformer-based architectures that require
large-scale training. To address this constraint, we propose Spatial-Frequency
Masked Image Modeling (SFMIM), a self-supervised pretraining strategy for
hyperspectral data that utilizes the large portion of unlabeled data. Our
method introduces a novel dual-domain masking mechanism that operates in both
spatial and frequency domains. The input HSI cube is initially divided into
non-overlapping patches along the spatial dimension, with each patch comprising
the entire spectrum of its corresponding spatial location. In spatial masking,
we randomly mask selected patches and train the model to reconstruct the masked
inputs using the visible patches. Concurrently, in frequency masking, we remove
portions of the frequency components of the input spectra and predict the
missing frequencies. By learning to reconstruct these masked components, the
transformer-based encoder captures higher-order spectral-spatial correlations.
We evaluate our approach on three publicly available HSI classification
benchmarks and demonstrate that it achieves state-of-the-art performance.
Notably, our model shows rapid convergence during fine-tuning, highlighting the
efficiency of our pretraining strategy.

</details>


### [98] [Seeing the Abstract: Translating the Abstract Language for Vision Language Models](https://arxiv.org/pdf/2505.03242)
*Davide Talon, Federico Girella, Ziyue Liu, Marco Cristani, Yiming Wang*

Main category: cs.CV

TL;DR: The paper highlights the overlooked importance of abstract language in Vision Language Models (VLMs) and introduces ACT, a training-free method to improve abstract representation in VLMs, showing superior performance in retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLMs neglect abstract language, which is prevalent and valuable, especially in domains like fashion. The study aims to address this gap.

Method: Proposes Abstract-to-Concrete Translator (ACT), a training-free, model-agnostic method to align abstract representations with concrete ones in VLMs using existing databases.

Result: ACT outperforms fine-tuned VLMs in text-to-image retrieval tasks, demonstrating strong generalization and consistency across models.

Conclusion: ACT effectively bridges the gap in abstract language representation in VLMs, offering a plug-and-play solution without additional training.

Abstract: Natural language goes beyond dryly describing visual content. It contains
rich abstract concepts to express feeling, creativity and properties that
cannot be directly perceived. Yet, current research in Vision Language Models
(VLMs) has not shed light on abstract-oriented language. Our research breaks
new ground by uncovering its wide presence and under-estimated value, with
extensive analysis. Particularly, we focus our investigation on the fashion
domain, a highly-representative field with abstract expressions. By analyzing
recent large-scale multimodal fashion datasets, we find that abstract terms
have a dominant presence, rivaling the concrete ones, providing novel
information, and being useful in the retrieval task. However, a critical
challenge emerges: current general-purpose or fashion-specific VLMs are
pre-trained with databases that lack sufficient abstract words in their text
corpora, thus hindering their ability to effectively represent
abstract-oriented language. We propose a training-free and model-agnostic
method, Abstract-to-Concrete Translator (ACT), to shift abstract
representations towards well-represented concrete ones in the VLM latent space,
using pre-trained models and existing multimodal databases. On the
text-to-image retrieval task, despite being training-free, ACT outperforms the
fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its
effectiveness with a strong generalization capability. Moreover, the
improvement introduced by ACT is consistent with various VLMs, making it a
plug-and-play solution.

</details>


### [99] [PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs](https://arxiv.org/pdf/2505.03254)
*Lukas Meiner, Jens Mehnert, Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: PROM introduces selective quantization for depthwise-separable CNNs, using ternary weights for pointwise convolutions and 8-bit for others, reducing energy and storage costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods fail to address the uneven computational cost in depthwise-separable architectures, missing efficiency gains.

Method: PROM applies two distinct bit-widths: ternary weights for pointwise convolutions and 8-bit for other modules, with quantization-aware training. Activations are quantized to 8-bit, converting pointwise convolutions into int8 additions.

Result: PROM reduces MobileNetV2's energy cost by 23.9x and storage size by 2.7x compared to float16, with similar ImageNet accuracy.

Conclusion: PROM advances the Pareto frontier for energy vs. accuracy in quantized CNNs, offering a simple solution for efficiency gains.

Abstract: Convolutional neural networks (CNNs) are crucial for computer vision tasks on
resource-constrained devices. Quantization effectively compresses these models,
reducing storage size and energy cost. However, in modern depthwise-separable
architectures, the computational cost is distributed unevenly across its
components, with pointwise operations being the most expensive. By applying a
general quantization scheme to this imbalanced cost distribution, existing
quantization approaches fail to fully exploit potential efficiency gains. To
this end, we introduce PROM, a straightforward approach for quantizing modern
depthwise-separable convolutional networks by selectively using two distinct
bit-widths. Specifically, pointwise convolutions are quantized to ternary
weights, while the remaining modules use 8-bit weights, which is achieved
through a simple quantization-aware training procedure. Additionally, by
quantizing activations to 8-bit, our method transforms pointwise convolutions
with ternary weights into int8 additions, which enjoy broad support across
hardware platforms and effectively eliminates the need for expensive
multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost
by more than an order of magnitude (23.9x) and its storage size by 2.7x
compared to the float16 baseline while retaining similar classification
performance on ImageNet. Our method advances the Pareto frontier for energy
consumption vs. top-1 accuracy for quantized convolutional models on ImageNet.
PROM addresses the challenges of quantizing depthwise-separable convolutional
networks to both ternary and 8-bit weights, offering a simple way to reduce
energy cost and storage size.

</details>


### [100] [OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction](https://arxiv.org/pdf/2505.03284)
*Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Yaoqi Huang, Hongyu Lyu, Nguyen Hoang Khoi Tran, Tzu-Yun Tseng, Stewart Worrall*

Main category: cs.CV

TL;DR: The paper proposes OccCylindrical, a method for 3D semantic occupancy prediction in autonomous vehicles, using cylindrical coordinates to improve fine-grained detail retention and performance.


<details>
  <summary>Details</summary>
Motivation: Existing multisensor fusion methods for 3D semantic occupancy prediction focus on Cartesian coordinates, ignoring sensor reading distribution and losing fine details.

Method: OccCylindrical merges and refines multisensor features under cylindrical coordinates to preserve geometry details.

Result: Extensive experiments on the nuScenes dataset, including challenging conditions, show state-of-the-art performance.

Conclusion: OccCylindrical effectively improves 3D semantic occupancy prediction by leveraging cylindrical coordinates, outperforming existing methods.

Abstract: The safe operation of autonomous vehicles (AVs) is highly dependent on their
understanding of the surroundings. For this, the task of 3D semantic occupancy
prediction divides the space around the sensors into voxels, and labels each
voxel with both occupancy and semantic information. Recent perception models
have used multisensor fusion to perform this task. However, existing
multisensor fusion-based approaches focus mainly on using sensor information in
the Cartesian coordinate system. This ignores the distribution of the sensor
readings, leading to a loss of fine-grained details and performance
degradation. In this paper, we propose OccCylindrical that merges and refines
the different modality features under cylindrical coordinates. Our method
preserves more fine-grained geometry detail that leads to better performance.
Extensive experiments conducted on the nuScenes dataset, including challenging
rainy and nighttime scenarios, confirm our approach's effectiveness and
state-of-the-art performance. The code will be available at:
https://github.com/DanielMing123/OccCylindrical

</details>


### [101] [Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification](https://arxiv.org/pdf/2505.03286)
*Zhihao Gong, Lian Wu, Yong Xu*

Main category: cs.CV

TL;DR: The paper proposes a Base-Detail Feature Learning Framework (BDLF) to improve visible-infrared person re-identification by leveraging both modality-shared and modality-specific information.


<details>
  <summary>Details</summary>
Motivation: Current methods for VIReID focus too much on modality-shared features and neglect modality-specific details, leading to suboptimal performance.

Method: BDLF uses a lossless detail feature extraction module and a complementary base embedding generation mechanism, supported by a correlation restriction method.

Result: Experiments on SYSU-MM01, RegDB, and LLCM datasets confirm BDLF's effectiveness.

Conclusion: BDLF successfully enhances feature learning for VIReID by integrating both base and detail knowledge.

Abstract: Visible-infrared person re-identification (VIReID) provides a solution for
ReID tasks in 24-hour scenarios; however, significant challenges persist in
achieving satisfactory performance due to the substantial discrepancies between
visible (VIS) and infrared (IR) modalities. Existing methods inadequately
leverage information from different modalities, primarily focusing on digging
distinguishing features from modality-shared information while neglecting
modality-specific details. To fully utilize differentiated minutiae, we propose
a Base-Detail Feature Learning Framework (BDLF) that enhances the learning of
both base and detail knowledge, thereby capitalizing on both modality-shared
and modality-specific information. Specifically, the proposed BDLF mines detail
and base features through a lossless detail feature extraction module and a
complementary base embedding generation mechanism, respectively, supported by a
novel correlation restriction method that ensures the features gained by BDLF
enrich both detail and base knowledge across VIS and IR features. Comprehensive
experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the
effectiveness of BDLF.

</details>


### [102] [Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach](https://arxiv.org/pdf/2505.03299)
*Pierre Adorni, Minh-Tan Pham, Stéphane May, Sébastien Lefèvre*

Main category: cs.CV

TL;DR: A method for predicting foundation model performance on downstream tasks without fine-tuning, using 'capabilities encoding,' simplifies model selection and provides insights for future research.


<details>
  <summary>Details</summary>
Motivation: Over 75 remote sensing vision foundation models exist, but none consistently outperforms others across all tasks, necessitating a cost-effective comparison method.

Method: Proposes 'capabilities encoding' to predict model performance on multiple downstream tasks without fine-tuning each one.

Result: Demonstrates the method's utility for model selection and offers new perspectives on existing literature.

Conclusion: The approach simplifies foundation model comparison and suggests directions for future research in Earth observation.

Abstract: Foundation models constitute a significant advancement in computer vision:
after a single, albeit costly, training phase, they can address a wide array of
tasks. In the field of Earth observation, over 75 remote sensing vision
foundation models have been developed in the past four years. However, none has
consistently outperformed the others across all available downstream tasks. To
facilitate their comparison, we propose a cost-effective method for predicting
a model's performance on multiple downstream tasks without the need for
fine-tuning on each one. This method is based on what we call "capabilities
encoding." The utility of this novel approach is twofold: we demonstrate its
potential to simplify the selection of a foundation model for a given new task,
and we employ it to offer a fresh perspective on the existing literature,
suggesting avenues for future research. Codes are available at
https://github.com/pierreadorni/capabilities-encoding.

</details>


### [103] [3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation](https://arxiv.org/pdf/2505.03300)
*Andrew Caunes, Thierry Chateau, Vincent Frémont*

Main category: cs.CV

TL;DR: A 3D semantic segmentation pipeline using 2D views and pretrained models avoids direct 3D annotation and additional modalities, enabling pseudo-label generation for domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Supervised 3D segmentation requires extensive annotation and faces domain shifts; this method reduces dependency on 3D labels and other modalities.

Method: Generate 2D views from LiDAR scans, apply 2D segmentation with a pretrained model, and back-project labels to 3D points using a voting-based estimator.

Result: The pipeline works without 3D annotation or additional modalities and shows promise for pseudo-label generation in domain adaptation.

Conclusion: The approach offers a practical solution for 3D segmentation, reducing annotation needs and enabling unsupervised domain adaptation.

Abstract: Semantic segmentation of 3D LiDAR point clouds, essential for autonomous
driving and infrastructure management, is best achieved by supervised learning,
which demands extensive annotated datasets and faces the problem of domain
shifts. We introduce a new 3D semantic segmentation pipeline that leverages
aligned scenes and state-of-the-art 2D segmentation methods, avoiding the need
for direct 3D annotation or reliance on additional modalities such as camera
images at inference time. Our approach generates 2D views from LiDAR scans
colored by sensor intensity and applies 2D semantic segmentation to these views
using a camera-domain pretrained model. The segmented 2D outputs are then
back-projected onto the 3D points, with a simple voting-based estimator that
merges the labels associated to each 3D point. Our main contribution is a
global pipeline for 3D semantic segmentation requiring no prior 3D annotation
and not other modality for inference, which can be used for pseudo-label
generation. We conduct a thorough ablation study and demonstrate the potential
of the generated pseudo-labels for the Unsupervised Domain Adaptation task.

</details>


### [104] [Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices](https://arxiv.org/pdf/2505.03303)
*Tasnim Shahriar*

Main category: cs.CV

TL;DR: The paper evaluates lightweight deep learning models for image classification in resource-constrained environments, comparing five architectures on three datasets. EfficientNetV2 excels in accuracy, MobileNetV3 balances accuracy and efficiency, and SqueezeNet is fastest and most compact. Transfer learning boosts performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient deep learning models in low-resource settings like edge devices, the study benchmarks lightweight architectures to identify optimal trade-offs between accuracy and computational efficiency.

Method: Five models (MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, ShuffleNetV2) are tested on CIFAR-10, CIFAR-100, and Tiny ImageNet. Metrics include accuracy, inference time, FLOPs, and model size. Hyperparameter tuning, data augmentation, and transfer learning (pretrained vs. scratch-trained) are explored.

Result: EfficientNetV2 achieves highest accuracy, MobileNetV3 balances accuracy and efficiency, and SqueezeNet is fastest and most compact. Transfer learning improves performance, especially for complex datasets.

Conclusion: The study provides insights for deploying lightweight models in resource-limited scenarios, emphasizing trade-offs between accuracy and efficiency, and highlights the benefits of transfer learning for edge computing and mobile platforms.

Abstract: This paper presents a comprehensive evaluation of lightweight deep learning
models for image classification, emphasizing their suitability for deployment
in resource-constrained environments such as low-memory devices. Five
state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,
EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse
datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using
four key performance metrics: classification accuracy, inference time,
floating-point operations (FLOPs), and model size. Additionally, we investigate
the impact of hyperparameter tuning, data augmentation, and training paradigms
by comparing pretrained models with scratch-trained counterparts, focusing on
MobileNetV3 Small. Our findings reveal that transfer learning significantly
enhances model accuracy and computational efficiency, particularly for complex
datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest
accuracy, while MobileNetV3 offers the best balance between accuracy and
efficiency, and SqueezeNet excels in inference speed and compactness. This
study highlights critical trade-offs between accuracy and efficiency, offering
actionable insights for deploying lightweight models in real-world applications
where computational resources are limited. By addressing these challenges, this
research contributes to optimizing deep learning systems for edge computing and
mobile platforms.

</details>


### [105] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/pdf/2505.03414)
*Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu*

Main category: cs.CV

TL;DR: A novel Features Matrix (FM) regularization method is proposed to enhance large vision-language models for target-unspecific tasks by preserving general knowledge and mitigating overfitting.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning methods for large vision-language models struggle with target-unspecific tasks due to overfitting, which causes loss of general knowledge.

Method: The proposed method introduces a Features Matrix (FM) that captures diverse input semantics from a deep and fine perspective to preserve general knowledge and prevent overfitting.

Result: The FM is shown to be compatible with existing frameworks and significantly improves performance on target-unspecific tasks, achieving state-of-the-art results.

Conclusion: The FM regularization approach effectively addresses overfitting and enhances generalizability in vision-language models for target-unspecific tasks.

Abstract: Recent developments in prompt learning of large vision-language models have
significantly improved performance in target-specific tasks. However, these
prompt optimizing methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge having
strong promotion on target-unspecific tasks. To alleviate this issue, we
propose a novel Features Matrix (FM) regularization approach designed to
enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks, achieving state-of-the-art performance.

</details>


### [106] [3D Gaussian Splatting Data Compression with Mixture of Priors](https://arxiv.org/pdf/2505.03310)
*Lei Liu, Zhenghao Chen, Dong Xu*

Main category: cs.CV

TL;DR: The paper proposes a Mixture of Priors (MoP) strategy for 3D Gaussian Splatting (3DGS) data compression, addressing limitations in entropy models and quantization. It achieves state-of-the-art results on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS compression methods lack robust entropy models and fine-grained quantization, limiting efficiency. The work aims to improve both lossless and lossy compression.

Method: The MoP strategy uses multiple MLPs to generate diverse prior features, integrated via a gating mechanism. For lossless compression, MoP enhances entropy modeling; for lossy, it guides element-wise quantization with a Coarse-to-Fine Quantization (C2FQ) strategy.

Result: The framework outperforms existing methods on benchmarks like Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&Temples.

Conclusion: The MoP approach effectively addresses entropy modeling and quantization challenges, setting new performance standards for 3DGS compression.

Abstract: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling
efficient storage and transmission in 3D scene modeling. However, its
development remains limited due to inadequate entropy models and suboptimal
quantization strategies for both lossless and lossy compression scenarios,
where existing methods have yet to 1) fully leverage hyperprior information to
construct robust conditional entropy models, and 2) apply fine-grained,
element-wise quantization strategies for improved compression granularity. In
this work, we propose a novel Mixture of Priors (MoP) strategy to
simultaneously address these two challenges. Specifically, inspired by the
Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior
information through multiple lightweight MLPs to generate diverse prior
features, which are subsequently integrated into the MoP feature via a gating
mechanism. To enhance lossless compression, the resulting MoP feature is
utilized as a hyperprior to improve conditional entropy modeling. Meanwhile,
for lossy compression, we employ the MoP feature as guidance information in an
element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine
Quantization (C2FQ) strategy with a predefined quantization step value.
Specifically, we expand the quantization step value into a matrix and
adaptively refine it from coarse to fine granularity, guided by the MoP
feature, thereby obtaining a quantization step matrix that facilitates
element-wise quantization. Extensive experiments demonstrate that our proposed
3DGS data compression framework achieves state-of-the-art performance across
multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and
Tank&Temples.

</details>


### [107] [Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/pdf/2505.03318)
*Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang*

Main category: cs.CV

TL;DR: The paper proposes UnifiedReward-Think, a multimodal CoT-based reward model, to improve reward signal reliability by incorporating long-chain reasoning. It uses reinforcement fine-tuning and GPT-4o distillation for robust reasoning across vision tasks.


<details>
  <summary>Details</summary>
Motivation: Current reward models lack depth in reasoning, leading to inaccurate signals. Incorporating explicit long chains of thought (CoT) can enhance reliability and robustness.

Method: Adopts exploration-driven reinforcement fine-tuning: (1) Distills GPT-4o reasoning for cold start, (2) uses large-scale multimodal data for reasoning elicitation, and (3) refines with GRPO-based fine-tuning.

Result: Extensive experiments show the model's superiority in vision reward tasks.

Conclusion: UnifiedReward-Think improves reward accuracy and robustness through CoT reasoning, validated by strong experimental performance.

Abstract: Recent advances in multimodal Reward Models (RMs) have shown significant
promise in delivering reward signals to align vision models with human
preferences. However, current RMs are generally restricted to providing direct
responses or engaging in shallow reasoning processes with limited depth, often
leading to inaccurate reward signals. We posit that incorporating explicit long
chains of thought (CoT) into the reward reasoning process can significantly
strengthen their reliability and robustness. Furthermore, we believe that once
RMs internalize CoT reasoning, their direct response accuracy can also be
improved through implicit reasoning capabilities. To this end, this paper
proposes UnifiedReward-Think, the first unified multimodal CoT-based reward
model, capable of multi-dimensional, step-by-step long-chain reasoning for both
visual understanding and generation reward tasks. Specifically, we adopt an
exploration-driven reinforcement fine-tuning approach to elicit and incentivize
the model's latent complex reasoning ability: (1) We first use a small amount
of image generation preference data to distill the reasoning process of GPT-4o,
which is then used for the model's cold start to learn the format and structure
of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge
and generalization capabilities, we prepare large-scale unified multimodal
preference data to elicit the model's reasoning process across various vision
tasks. During this phase, correct reasoning outputs are retained for rejection
sampling to refine the model (3) while incorrect predicted samples are finally
used for Group Relative Policy Optimization (GRPO) based reinforcement
fine-tuning, enabling the model to explore diverse reasoning paths and optimize
for correct and robust solutions. Extensive experiments across various vision
reward tasks demonstrate the superiority of our model.

</details>


### [108] [FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing](https://arxiv.org/pdf/2505.03329)
*Rui Lan, Yancheng Bai, Xu Duan, Mingxing Li, Lei Sun, Xiangxiang Chu*

Main category: cs.CV

TL;DR: FLUX-Text is a multilingual scene text editing framework based on FLUX-Fill, addressing challenges in generating accurate non-Latin characters by incorporating glyph conditioning and lightweight modules. It achieves state-of-the-art performance with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with generating accurate or recognizable non-Latin characters due to complex glyph structures, prompting the need for a more effective solution.

Method: FLUX-Text integrates glyph conditioning (visual and textual modalities) and lightweight glyph/text embedding modules into FLUX-Fill, requiring only 100K training examples.

Result: The framework outperforms previous methods in text fidelity, demonstrated by qualitative and quantitative experiments on public datasets.

Conclusion: FLUX-Text offers a simple yet advanced solution for multilingual scene text editing, achieving superior performance with fewer resources.

Abstract: The task of scene text editing is to modify or add texts on images while
maintaining the fidelity of newly generated text and visual coherence with the
background. Recent works based on latent diffusion models (LDM) show improved
text editing results, yet still face challenges and often generate inaccurate
or unrecognizable characters, especially for non-Latin ones (\eg, Chinese),
which have complex glyph structures. To address these issues, we present
FLUX-Text, a simple and advanced multilingual scene text editing framework
based on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,
considering both visual and textual modalities. To retain the original
generative capabilities of FLUX-Fill while enhancing its understanding and
generation of glyphs, we propose lightweight glyph and text embedding modules.
Owning to the lightweight design, FLUX-Text is trained only with $100K$
training examples compared to current popular methods trained with 2.9M ones.
With no bells and whistles, our method achieves state-of-the-art performance on
text editing tasks. Qualitative and quantitative experiments on the public
datasets demonstrate that our method surpasses previous works in text fidelity.

</details>


### [109] [From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection](https://arxiv.org/pdf/2505.03334)
*Guoting Wei, Yu Liu, Xia Yuan, Xizhe Xue, Linlin Guo, Yifan Yang, Chunxia Zhao, Zongwen Bai, Haokui Zhang, Rong Xiao*

Main category: cs.CV

TL;DR: The paper introduces a large-scale dataset (MI-OAD) and an annotation pipeline (OS-W2S Label Engine) for language-guided open-world aerial object detection, addressing the lack of fine-grained data.


<details>
  <summary>Details</summary>
Motivation: Existing language-guided methods for aerial detection are limited by vocabulary-focused datasets, failing to support fine-grained open-world detection.

Method: The OS-W2S Label Engine combines vision-language models and BERT-based postprocessing to automatically annotate aerial images with rich textual descriptions. The MI-OAD dataset is constructed with 163,023 images and 2M captions.

Result: Grounding DINO trained on MI-OAD improves by 29.5 AP_{50} and 33.7 Recall@10 for zero-shot sentence inputs.

Conclusion: The dataset and label engine enable effective open-set aerial detection and will be publicly released.

Abstract: In recent years, language-guided open-world aerial object detection has
gained significant attention due to its better alignment with real-world
application needs. However, due to limited datasets, most existing
language-guided methods primarily focus on vocabulary, which fails to meet the
demands of more fine-grained open-world detection. To address this limitation,
we propose constructing a large-scale language-guided open-set aerial detection
dataset, encompassing three levels of language guidance: from words to phrases,
and ultimately to sentences. Centered around an open-source large
vision-language model and integrating image-operation-based preprocessing with
BERT-based postprocessing, we present the OS-W2S Label Engine, an automatic
annotation pipeline capable of handling diverse scene annotations for aerial
images. Using this label engine, we expand existing aerial detection datasets
with rich textual annotations and construct a novel benchmark dataset, called
Multi-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of
current remote sensing grounding data and enabling effective open-set aerial
detection. Specifically, MI-OAD contains 163,023 images and 2 million
image-caption pairs, approximately 40 times larger than comparable datasets. We
also employ state-of-the-art open-set methods from the natural image domain,
trained on our proposed dataset, to validate the model's open-set detection
capabilities. For instance, when trained on our dataset, Grounding DINO
achieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs
under zero-shot transfer conditions. Both the dataset and the label engine will
be released publicly.

</details>


### [110] [A Vision-Language Model for Focal Liver Lesion Classification](https://arxiv.org/pdf/2505.03350)
*Song Jian, Hu Yuchang, Wang Hui, Chen Yen-Wei*

Main category: cs.CV

TL;DR: Liver-VLM, a Vision-Language model for focal liver lesion classification, outperforms CLIP and MedCLIP by leveraging multimodal learning and lightweight ResNet18 backbone, especially in data-limited scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning models require large annotated datasets, which are scarce in medical imaging. Vision-Language models (VLMs) like CLIP offer a solution by using multimodal learning with limited labeled data.

Method: Liver-VLM integrates class information into the text encoder, aligns image and text embeddings via cosine similarity, and optimizes with cross-entropy loss. It uses a lightweight ResNet18 backbone.

Result: Liver-VLM achieves higher accuracy and AUC than CLIP and MedCLIP on the MPCT-FLLs dataset, with ResNet18 improving performance in data-constrained conditions.

Conclusion: Liver-VLM is effective for focal liver lesion classification, especially with limited labeled data, demonstrating the potential of VLMs in medical imaging.

Abstract: Accurate classification of focal liver lesions is crucial for diagnosis and
treatment in hepatology. However, traditional supervised deep learning models
depend on large-scale annotated datasets, which are often limited in medical
imaging. Recently, Vision-Language models (VLMs) such as Contrastive
Language-Image Pre-training model (CLIP) has been applied to image
classifications. Compared to the conventional convolutional neural network
(CNN), which classifiers image based on visual information only, VLM leverages
multimodal learning with text and images, allowing it to learn effectively even
with a limited amount of labeled data. Inspired by CLIP, we pro-pose a
Liver-VLM, a model specifically designed for focal liver lesions (FLLs)
classification. First, Liver-VLM incorporates class information into the text
encoder without introducing additional inference overhead. Second, by
calculating the pairwise cosine similarities between image and text embeddings
and optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively
aligns image features with class-level text features. Experimental results on
MPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the
standard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve
(AUC). Further analysis shows that using a lightweight ResNet18 backbone
enhances classification performance, particularly under data-constrained
conditions.

</details>


### [111] [GUAVA: Generalizable Upper Body 3D Gaussian Avatar](https://arxiv.org/pdf/2505.03351)
*Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Yang Li, Minghan Qin, Yu Li, Haoqian Wang*

Main category: cs.CV

TL;DR: GUAVA is a framework for fast animatable 3D Gaussian avatar reconstruction from a single image, improving facial expressions and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D human avatar reconstruction are complex, time-consuming, and limited in facial expressiveness.

Method: Introduces an expressive human model (EHM) and uses inverse texture mapping and projection sampling to infer upper-body Gaussians from a single image, refined by a neural refiner.

Result: GUAVA outperforms previous methods in rendering quality and speed, achieving sub-second reconstruction (0.1s) and real-time animation.

Conclusion: GUAVA offers a fast, high-quality solution for animatable 3D avatar reconstruction with enhanced facial expressions.

Abstract: Reconstructing a high-quality, animatable 3D human avatar with expressive
facial and hand motions from a single image has gained significant attention
due to its broad application potential. 3D human avatar reconstruction
typically requires multi-view or monocular videos and training on individual
IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's
expressiveness, these methods often focus on body motion but struggle with
facial expressions. To address these challenges, we first introduce an
expressive human model (EHM) to enhance facial expression capabilities and
develop an accurate tracking method. Based on this template model, we propose
GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar
reconstruction. We leverage inverse texture mapping and projection sampling
techniques to infer Ubody (upper-body) Gaussians from a single image. The
rendered images are refined through a neural refiner. Experimental results
demonstrate that GUAVA significantly outperforms previous methods in rendering
quality and offers significant speed improvements, with reconstruction times in
the sub-second range (0.1s), and supports real-time animation and rendering.

</details>


### [112] [Interpretable Zero-shot Learning with Infinite Class Concepts](https://arxiv.org/pdf/2505.03361)
*Zihan Ye, Shreyank N Gowda, Shiming Chen, Yaochu Jin, Kaizhu Huang, Xiaobo Jin*

Main category: cs.CV

TL;DR: InfZSL improves zero-shot learning by dynamically generating class concepts using LLMs, addressing hallucination with an entropy-based selection mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing ZSL methods lack transparency and suffer from LLM hallucination, leading to non-visual semantics.

Method: InfZSL uses LLMs to generate phrase-level class concepts and employs an entropy-based scoring process for concept selection.

Result: InfZSL outperforms benchmarks on three datasets and produces interpretable, image-grounded concepts.

Conclusion: The framework enhances ZSL by ensuring transferable and discriminative class semantics.

Abstract: Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images
with intermediate class semantics, like human-annotated concepts or class
definitions. An emerging alternative leverages Large-scale Language Models
(LLMs) to automatically generate class documents. However, these methods often
face challenges with transparency in the classification process and may suffer
from the notorious hallucination problem in LLMs, resulting in non-visual class
semantics. This paper redefines class semantics in ZSL with a focus on
transferability and discriminability, introducing a novel framework called
Zero-shot Learning with Infinite Class Concepts (InfZSL). Our approach
leverages the powerful capabilities of LLMs to dynamically generate an
unlimited array of phrase-level class concepts. To address the hallucination
challenge, we introduce an entropy-based scoring process that incorporates a
``goodness" concept selection mechanism, ensuring that only the most
transferable and discriminative concepts are selected. Our InfZSL framework not
only demonstrates significant improvements on three popular benchmark datasets
but also generates highly interpretable, image-grounded concepts. Code will be
released upon acceptance.

</details>


### [113] [3D Surface Reconstruction with Enhanced High-Frequency Details](https://arxiv.org/pdf/2505.03362)
*Shikun Zhang, Yiqun Wang, Cunjian Chen, Yong Li, Qiuhong Ke*

Main category: cs.CV

TL;DR: FreNeuS improves neural implicit 3D reconstruction by focusing on high-frequency details using pixel gradient changes and dynamic sampling, achieving better surface quality.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with high-frequency detail reconstruction due to random sampling, leading to overly smooth results.

Method: FreNeuS uses pixel gradient changes to identify high-frequency regions, guides dynamic ray sampling, and applies high-frequency weighting to enhance detail reconstruction.

Result: The method reconstructs fine surface details better than existing techniques and is applicable to NeuS-based works.

Conclusion: FreNeuS effectively addresses the lack of surface detail in neural implicit 3D reconstruction, offering improved quality and generalizability.

Abstract: Neural implicit 3D reconstruction can reproduce shapes without 3D
supervision, and it learns the 3D scene through volume rendering methods and
neural implicit representations. Current neural surface reconstruction methods
tend to randomly sample the entire image, making it difficult to learn
high-frequency details on the surface, and thus the reconstruction results tend
to be too smooth. We designed a method (FreNeuS) based on high-frequency
information to solve the problem of insufficient surface detail. Specifically,
FreNeuS uses pixel gradient changes to easily acquire high-frequency regions in
an image and uses the obtained high-frequency information to guide surface
detail reconstruction. High-frequency information is first used to guide the
dynamic sampling of rays, applying different sampling strategies according to
variations in high-frequency regions. To further enhance the focus on surface
details, we have designed a high-frequency weighting method that constrains the
representation of high-frequency details during the reconstruction process.
Qualitative and quantitative results show that our method can reconstruct fine
surface details and obtain better surface reconstruction quality compared to
existing methods. In addition, our method is more applicable and can be
generalized to any NeuS-based work.

</details>


### [114] [Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models](https://arxiv.org/pdf/2505.03374)
*Abram Schonfeldt, Benjamin Maylor, Xiaofang Chen, Ronald Clark, Aiden Doherty*

Main category: cs.CV

TL;DR: The paper compares vision-language models (VLMs) and discriminative models (DMs) for labeling physical activity behaviors from wearable camera images, showing comparable performance for sedentary behavior but declining accuracy for higher-intensity activities and external datasets.


<details>
  <summary>Details</summary>
Motivation: To reduce the labor-intensive annotation of wearable camera images for physical activity research by evaluating the performance of automated models.

Method: Comparison of three VLMs and two DMs on two free-living validation studies (161 and 111 participants) using wearable camera data.

Result: VLMs and DMs performed similarly for sedentary behavior (F1-scores ~0.9) but worse for light and moderate-to-vigorous activities (~0.6-0.7). Performance dropped significantly in external validation (Cohen's kappa ~0.2-0.3).

Conclusion: Freely available VLMs can effectively annotate sedentary behavior in similar populations, reducing annotation effort, but struggle with higher-intensity activities and external datasets.

Abstract: Introduction: Data from wearable devices collected in free-living settings,
and labelled with physical activity behaviours compatible with health research,
are essential for both validating existing wearable-based measurement
approaches and developing novel machine learning approaches. One common way of
obtaining these labels relies on laborious annotation of sequences of images
captured by cameras worn by participants through the course of a day. Methods:
We compare the performance of three vision language models and two
discriminative models on two free-living validation studies with 161 and 111
participants, collected in Oxfordshire, United Kingdom and Sichuan, China,
respectively, using the Autographer (OMG Life, defunct) wearable camera.
Results: We found that the best open-source vision-language model (VLM) and
fine-tuned discriminative model (DM) achieved comparable performance when
predicting sedentary behaviour from single images on unseen participants in the
Oxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86,
0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63,
0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53,
0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study,
performance fell across all intensity categories, with median Cohen's
kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM,
and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely
available computer vision models could help annotate sedentary behaviour,
typically the most prevalent activity of daily living, from wearable camera
images within similar populations to seen data, reducing the annotation burden.

</details>


### [115] [Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples](https://arxiv.org/pdf/2505.03383)
*Jian-Wei Li, Wen-Ze Shao*

Main category: cs.CV

TL;DR: The paper proposes Attention-aggregated Attack (AAA) to improve adversarial example transferability for face recognition models by targeting model-specific facial features.


<details>
  <summary>Details</summary>
Motivation: Adversarial examples expose deep learning vulnerabilities, but existing transfer-based attacks overlook class-specific models like face recognition, leading to poor performance.

Method: AAA mimics attention divergence in FR models to disrupt critical facial features, enhancing adversarial example transferability.

Result: Experiments show AAA outperforms existing methods in attacking various FR models.

Conclusion: AAA effectively improves adversarial transferability for FR models by targeting model-specific features.

Abstract: Adversarial examples have revealed the vulnerability of deep learning models
and raised serious concerns about information security. The transfer-based
attack is a hot topic in black-box attacks that are practical to real-world
scenarios where the training datasets, parameters, and structure of the target
model are unknown to the attacker. However, few methods consider the
particularity of class-specific deep models for fine-grained vision tasks, such
as face recognition (FR), giving rise to unsatisfactory attacking performance.
In this work, we first investigate what in a face exactly contributes to the
embedding learning of FR models and find that both decisive and auxiliary
facial features are specific to each FR model, which is quite different from
the biological mechanism of human visual system. Accordingly we then propose a
novel attack method named Attention-aggregated Attack (AAA) to enhance the
transferability of adversarial examples against FR, which is inspired by the
attention divergence and aims to destroy the facial features that are critical
for the decision-making of other FR models by imitating their attentions on the
clean face images. Extensive experiments conducted on various FR models
validate the superiority and robust effectiveness of the proposed method over
existing methods.

</details>


### [116] [EOPose : Exemplar-based object reposing using Generalized Pose Correspondences](https://arxiv.org/pdf/2505.03394)
*Sarthak Mehrotra, Rishabh Jain, Mayur Hemani, Balaji Krishnamurthy, Mausoom Sarkar*

Main category: cs.CV

TL;DR: EOPose is an end-to-end framework for object reposing in images, leveraging unsupervised keypoint correspondence to preserve fine details while achieving high-quality results.


<details>
  <summary>Details</summary>
Motivation: The need for quick production of product image variants in e-commerce drives the development of a method that avoids generative approaches' loss of detail.

Method: EOPose uses a three-step approach: keypoint correspondence detection, warping, and re-rendering, guided by a target pose image.

Result: High-quality reposing outputs are validated by metrics (PSNR, SSIM, FID) and supported by ablation and user studies.

Conclusion: EOPose effectively reposes objects while preserving details, with a new dataset and comprehensive evaluation demonstrating its efficacy.

Abstract: Reposing objects in images has a myriad of applications, especially for
e-commerce where several variants of product images need to be produced
quickly. In this work, we leverage the recent advances in unsupervised keypoint
correspondence detection between different object images of the same class to
propose an end-to-end framework for generic object reposing. Our method,
EOPose, takes a target pose-guidance image as input and uses its keypoint
correspondence with the source object image to warp and re-render the latter
into the target pose using a novel three-step approach. Unlike generative
approaches, our method also preserves the fine-grained details of the object
such as its exact colors, textures, and brand marks. We also prepare a new
dataset of paired objects based on the Objaverse dataset to train and test our
network. EOPose produces high-quality reposing output as evidenced by different
image quality metrics (PSNR, SSIM and FID). Besides a description of the method
and the dataset, the paper also includes detailed ablation and user studies to
indicate the efficacy of the proposed method

</details>


### [117] [DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation](https://arxiv.org/pdf/2505.03401)
*Shanshan Song, Hui Tang, Honglong Yang, Xiaomeng Li*

Main category: cs.CV

TL;DR: A novel dynamic difference-aware temporal residual network (DDaTR) improves longitudinal radiology report generation by better capturing spatial and temporal correlations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Longitudinal Radiology Report Generation (LRRG) fail to effectively capture spatial and temporal correlations, leading to sub-optimal performance.

Method: DDaTR introduces two modules: Dynamic Feature Alignment Module (DFAM) for aligning prior features and Dynamic Difference-Aware Module (DDAM) for capturing differences across exams. It also uses a dynamic residual network for temporal modeling.

Result: DDaTR outperforms existing methods on three benchmarks, excelling in both RRG and LRRG tasks.

Conclusion: DDaTR effectively addresses the limitations of prior methods, enhancing the accuracy and efficiency of radiology report generation.

Abstract: Radiology Report Generation (RRG) automates the creation of radiology reports
from medical imaging, enhancing the efficiency of the reporting process.
Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating
the ability to compare current and prior exams, facilitating the tracking of
temporal changes in clinical findings. Existing LRRG approaches only extract
features from prior and current images using a visual pre-trained encoder,
which are then concatenated to generate the final report. However, these
methods struggle to effectively capture both spatial and temporal correlations
during the feature extraction process. Consequently, the extracted features
inadequately capture the information of difference across exams and thus
underrepresent the expected progressions, leading to sub-optimal performance in
LRRG. To address this, we develop a novel dynamic difference-aware temporal
residual network (DDaTR). In DDaTR, we introduce two modules at each stage of
the visual encoder to capture multi-level spatial correlations. The Dynamic
Feature Alignment Module (DFAM) is designed to align prior features across
modalities for the integrity of prior clinical information. Prompted by the
enriched prior features, the dynamic difference-aware module (DDAM) captures
favorable difference information by identifying relationships across exams.
Furthermore, our DDaTR employs the dynamic residual network to unidirectionally
transmit longitudinal information, effectively modelling temporal correlations.
Extensive experiments demonstrated superior performance over existing methods
on three benchmarks, proving its efficacy in both RRG and LRRG tasks.

</details>


### [118] [CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection](https://arxiv.org/pdf/2505.03412)
*Haoyu Bai, Jie Wang, Gaomin Li, Xuan Li, Xiaohu Zhang, Xia Yang*

Main category: cs.CV

TL;DR: The paper introduces CXR-AD, the first public X-ray dataset for internal defect detection in components, highlighting challenges and benchmarking current anomaly detection methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of publicly available X-ray datasets for internal defect detection, which limits algorithm development and industrial applications.

Method: Constructed a dataset with 653 normal and 561 defect X-ray images, annotated with pixel-level masks, and benchmarked three anomaly detection frameworks.

Result: Current methods show a 29.78% performance drop on CXR-AD compared to MVTec AD, revealing their limitations for internal defects.

Conclusion: CXR-AD fills a critical gap, offering a benchmark to improve internal defect detection algorithms and industrial inspection precision.

Abstract: Internal defect detection constitutes a critical process in ensuring
component quality, for which anomaly detection serves as an effective solution.
However, existing anomaly detection datasets predominantly focus on surface
defects in visible-light images, lacking publicly available X-ray datasets
targeting internal defects in components. To address this gap, we construct the
first publicly accessible component X-ray anomaly detection (CXR-AD) dataset,
comprising real-world X-ray images. The dataset covers five industrial
component categories, including 653 normal samples and 561 defect samples with
precise pixel-level mask annotations. We systematically analyze the dataset
characteristics and identify three major technical challenges: (1) strong
coupling between complex internal structures and defect regions, (2) inherent
low contrast and high noise interference in X-ray imaging, and (3) significant
variations in defect scales and morphologies. To evaluate dataset complexity,
we benchmark three state-of-the-art anomaly detection frameworks
(feature-based, reconstruction-based, and zero-shot learning methods).
Experimental results demonstrate a 29.78% average performance degradation on
CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms
in handling internal defect detection tasks. To the best of our knowledge,
CXR-AD represents the first publicly available X-ray dataset for component
anomaly detection, providing a real-world industrial benchmark to advance
algorithm development and enhance precision in internal defect inspection
technologies.

</details>


### [119] [LiftFeat: 3D Geometry-Aware Local Feature Matching](https://arxiv.org/pdf/2505.03422)
*Yepeng Liu, Wenpeng Lai, Zhou Zhao, Yuxuan Xiong, Jinchi Zhu, Jun Cheng, Yongchao Xu*

Main category: cs.CV

TL;DR: The paper introduces LiftFeat, a lightweight network that enhances local feature matching by integrating 3D geometric features with raw 2D descriptors, improving robustness in challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Local feature matching is critical for robotics applications like SLAM and visual localization, but existing methods struggle with lighting changes, low texture, or repetitive patterns.

Method: LiftFeat uses a pre-trained monocular depth model to generate pseudo surface normal labels, supervises 3D geometric feature extraction, and fuses these with 2D descriptors via a 3D geometry-aware module.

Result: LiftFeat outperforms state-of-the-art lightweight methods in tasks like pose estimation, homography estimation, and visual localization.

Conclusion: Integrating 3D geometric features significantly boosts the discriminative power of 2D descriptors, making LiftFeat robust in extreme conditions.

Abstract: Robust and efficient local feature matching plays a crucial role in
applications such as SLAM and visual localization for robotics. Despite great
progress, it is still very challenging to extract robust and discriminative
visual features in scenarios with drastic lighting changes, low texture areas,
or repetitive patterns. In this paper, we propose a new lightweight network
called \textit{LiftFeat}, which lifts the robustness of raw descriptor by
aggregating 3D geometric feature. Specifically, we first adopt a pre-trained
monocular depth estimation model to generate pseudo surface normal label,
supervising the extraction of 3D geometric feature in terms of predicted
surface normal. We then design a 3D geometry-aware feature lifting module to
fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D
geometric feature enhances the discriminative ability of 2D feature description
in extreme conditions. Extensive experimental results on relative pose
estimation, homography estimation, and visual localization tasks, demonstrate
that our LiftFeat outperforms some lightweight state-of-the-art methods. Code
will be released at : https://github.com/lyp-deeplearning/LiftFeat.

</details>


### [120] [Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications](https://arxiv.org/pdf/2505.03426)
*Ziyu Li, Yujian Hu, Zhengyao Ding, Yiheng Mao, Haitao Li, Fan Yi, Hongkun Zhang, Zhengxing Huang*

Main category: cs.CV

TL;DR: CPGG generates diverse, high-quality synthetic CMR data using cardiac phenotypes, improving AI model performance on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Limited availability of large-scale, high-quality CMR datasets hinders AI applications in cardiac health.

Method: Two-stage framework: 1) Train generative model with cardiac phenotypes; 2) Use masked autoregressive diffusion model to generate CMR cine sequences.

Result: CPGG produces high-fidelity synthetic CMR data, enhancing performance in diagnosis and phenotype prediction.

Conclusion: CPGG effectively addresses data scarcity, boosting AI model performance in cardiac health tasks.

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for
diagnosing heart diseases and evaluating cardiac health. However, the limited
availability of large-scale, high-quality CMR datasets poses a major challenge
to the effective application of artificial intelligence (AI) in this domain.
Even the amount of unlabeled data and the health status it covers are difficult
to meet the needs of model pretraining, which hinders the performance of AI
models on downstream tasks. In this study, we present Cardiac Phenotype-Guided
CMR Generation (CPGG), a novel approach for generating diverse CMR data that
covers a wide spectrum of cardiac health status. The CPGG framework consists of
two stages: in the first stage, a generative model is trained using cardiac
phenotypes derived from CMR data; in the second stage, a masked autoregressive
diffusion model, conditioned on these phenotypes, generates high-fidelity CMR
cine sequences that capture both structural and functional features of the
heart in a fine-grained manner. We synthesized a massive amount of CMR to
expand the pretraining data. Experimental results show that CPGG generates
high-quality synthetic CMR data, significantly improving performance on various
downstream tasks, including diagnosis and cardiac phenotypes prediction. These
gains are demonstrated across both public and private datasets, highlighting
the effectiveness of our approach. Code is availabel at
https://anonymous.4open.science/r/CPGG.

</details>


### [121] [A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution](https://arxiv.org/pdf/2505.03431)
*Usman Muhammad, Jorma Laaksonen*

Main category: cs.CV

TL;DR: Proposes FGIN, a single-image super-resolution model for hyperspectral images, addressing alignment challenges by integrating spectral-spatial fusion and multiscale feature extraction.


<details>
  <summary>Details</summary>
Motivation: Overcome the reliance on precise alignment between image pairs in HSI super-resolution by developing a single-image solution.

Method: Uses a spectral-spatial fusion module, Inception-like hierarchical feature extraction, multi-scale fusion block, and optimized upsampling with bilinear interpolation and depthwise separable convolutions.

Result: Demonstrates competitive performance on two public hyperspectral datasets.

Conclusion: FGIN effectively addresses alignment challenges and enhances HSI super-resolution quality.

Abstract: The fusion of low-spatial-resolution hyperspectral images (HSIs) with
high-spatial-resolution conventional images (e.g., panchromatic or RGB) has
played a significant role in recent advancements in HSI super-resolution.
However, this fusion process relies on the availability of precise alignment
between image pairs, which is often challenging in real-world scenarios. To
mitigate this limitation, we propose a single-image super-resolution model
called the Fusion-Guided Inception Network (FGIN). Specifically, we first
employ a spectral-spatial fusion module to effectively integrate spectral and
spatial information at an early stage. Next, an Inception-like hierarchical
feature extraction strategy is used to capture multiscale spatial dependencies,
followed by a dedicated multi-scale fusion block. To further enhance
reconstruction quality, we incorporate an optimized upsampling module that
combines bilinear interpolation with depthwise separable convolutions.
Experimental evaluations on two publicly available hyperspectral datasets
demonstrate the competitive performance of our method.

</details>


### [122] [Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks](https://arxiv.org/pdf/2505.03435)
*Sun Haoxuan, Hong Yan, Zhan Jiahui, Chen Haoxing, Lan Jun, Zhu Huijia, Wang Weiqiang, Zhang Liqing, Zhang Jianfu*

Main category: cs.CV

TL;DR: The paper highlights vulnerabilities in AI-generated face detection systems and proposes adversarial training and diffusion inversion to improve robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Addressing security concerns in generative image technology, particularly the limited robustness of current AI-generated face detection systems against adversarial attacks.

Method: Proposes integrating adversarial training and using diffusion inversion and reconstruction to enhance detection robustness.

Result: Minor adversarial perturbations can bypass existing systems, but the proposed method significantly improves robustness.

Conclusion: The study provides insights into AI-generated content characteristics and makes code publicly available for further research.

Abstract: The rapid advancement of generative image technology has introduced
significant security concerns, particularly in the domain of face generation
detection. This paper investigates the vulnerabilities of current AI-generated
face detection systems. Our study reveals that while existing detection methods
often achieve high accuracy under standard conditions, they exhibit limited
robustness against adversarial attacks. To address these challenges, we propose
an approach that integrates adversarial training to mitigate the impact of
adversarial examples. Furthermore, we utilize diffusion inversion and
reconstruction to further enhance detection robustness. Experimental results
demonstrate that minor adversarial perturbations can easily bypass existing
detection systems, but our method significantly improves the robustness of
these systems. Additionally, we provide an in-depth analysis of adversarial and
benign examples, offering insights into the intrinsic characteristics of
AI-generated content. All associated code will be made publicly available in a
dedicated repository to facilitate further research and verification.

</details>


### [123] [Blending 3D Geometry and Machine Learning for Multi-View Stereopsis](https://arxiv.org/pdf/2505.03470)
*Vibhas Vats, Md. Alimoor Reza, David Crandall, Soon-heung Jung*

Main category: cs.CV

TL;DR: GC MVSNet++ integrates multi-view, multi-scale geometric consistency during learning, improving efficiency and performance in MVS tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional MVS methods rely on post-processing GC checks, while learning-based methods ignore GC during training. This work aims to enforce GC actively during learning for better results.

Method: Introduces GC MVSNet++, enforcing multi-view, multi-scale GC checks during training. Uses a densely connected cost regularization network with two block designs for enhanced regularization.

Result: Achieves state-of-the-art on DTU and BlendedMVS datasets, and second place on Tanks and Temples benchmark. Training iterations are halved.

Conclusion: GC MVSNet++ is the first to enforce supervised GC during learning, significantly improving MVS performance and efficiency.

Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric
and geometric consistency constraints. In contrast, modern learning-based
algorithms often rely on the plane sweep algorithm to infer 3D geometry,
applying explicit geometric consistency (GC) checks only as a post-processing
step, with no impact on the learning process itself. In this work, we introduce
GC MVSNet plus plus, a novel approach that actively enforces geometric
consistency of reference view depth maps across multiple source views (multi
view) and at various scales (multi scale) during the learning phase (see Fig.
1). This integrated GC check significantly accelerates the learning process by
directly penalizing geometrically inconsistent pixels, effectively halving the
number of training iterations compared to other MVS methods. Furthermore, we
introduce a densely connected cost regularization network with two distinct
block designs simple and feature dense optimized to harness dense feature
connections for enhanced regularization. Extensive experiments demonstrate that
our approach achieves a new state of the art on the DTU and BlendedMVS datasets
and secures second place on the Tanks and Temples benchmark. To our knowledge,
GC MVSNet plus plus is the first method to enforce multi-view, multi-scale
supervised geometric consistency during learning. Our code is available.

</details>


### [124] [Polar Coordinate-Based 2D Pose Prior with Neural Distance Field](https://arxiv.org/pdf/2505.03445)
*Qi Gan, Sao Mai Nguyen, Eric Fenaux, Stephan Clémençon, Mounîm El Yacoubi*

Main category: cs.CV

TL;DR: The paper proposes a 2D pose refinement method using Neural Distance Fields (NDF) with polar coordinate-based representations to improve human pose estimation in sports, addressing challenges like motion blur and domain shifts.


<details>
  <summary>Details</summary>
Motivation: Current deep learning-based pose estimation models struggle in real-world sports due to motion blur, occlusions, and domain shifts. Fine-tuning requires large annotated datasets and lacks generalization.

Method: The approach uses NDF with polar coordinates, incorporating joint lengths and a novel non-geodesic distance metric. It also employs gradient-based batch-projection augmentation to mitigate data scarcity.

Result: Tested on a long jump dataset, the method improves 2D pose estimation across domains, enhancing plausibility with limited training data.

Conclusion: The proposed method effectively refines pose estimations in sports scenarios, offering robustness and efficiency with minimal data requirements.

Abstract: Human pose capture is essential for sports analysis, enabling precise
evaluation of athletes' movements. While deep learning-based human pose
estimation (HPE) models from RGB videos have achieved impressive performance on
public datasets, their effectiveness in real-world sports scenarios is often
hindered by motion blur, occlusions, and domain shifts across different pose
representations. Fine-tuning these models can partially alleviate such
challenges but typically requires large-scale annotated data and still
struggles to generalize across diverse sports environments. To address these
limitations, we propose a 2D pose prior-guided refinement approach based on
Neural Distance Fields (NDF). Unlike existing approaches that rely solely on
angular representations of human poses, we introduce a polar coordinate-based
representation that explicitly incorporates joint connection lengths, enabling
a more accurate correction of erroneous pose estimations. Additionally, we
define a novel non-geodesic distance metric that separates angular and radial
discrepancies, which we demonstrate is better suited for polar representations
than traditional geodesic distances. To mitigate data scarcity, we develop a
gradient-based batch-projection augmentation strategy, which synthesizes
realistic pose samples through iterative refinement. Our method is evaluated on
a long jump dataset, demonstrating its ability to improve 2D pose estimation
across multiple pose representations, making it robust across different
domains. Experimental results show that our approach enhances pose plausibility
while requiring only limited training data. Code is available at:
https://github.com/QGAN2019/polar-NDF.

</details>


### [125] [Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD)](https://arxiv.org/pdf/2505.03463)
*Muge Du, Zhuozhao Zheng, Wenying Wang, Guotao Quan, Wuliang Shi, Le Shen, Li Zhang, Liang Li, Yinong Liu, Yuxiang Xing*

Main category: cs.CV

TL;DR: BIRD is a novel INR-based framework for nonperiodic dynamic CT reconstruction, addressing computational inefficiency, DVF complexity, and detail preservation through backward-warping, diffeomorphism-based regularization, motion-compensated reconstruction, and dimensional-reduction.


<details>
  <summary>Details</summary>
Motivation: Traditional and deep learning methods struggle with motion artifacts in nonperiodic dynamic CT, especially for rapid movements like cardiac imaging. INR techniques show promise but face computational and detail-preservation challenges.

Method: BIRD uses backward-warping deformation, diffeomorphism-based DVF regularization, motion-compensated analytical reconstruction, and dimensional-reduction for 4D coordinate encoding.

Result: Simulations and studies show BIRD enhances detail and reduces motion artifacts in nonperiodic dynamic CT, enabling applications like one-beat cardiac reconstruction.

Conclusion: BIRD improves dynamic CT reconstruction accuracy, offering clinical potential for cardiac imaging and motion artifact reduction.

Abstract: Dynamic computed tomography (CT) reconstruction faces significant challenges
in addressing motion artifacts, particularly for nonperiodic rapid movements
such as cardiac imaging with fast heart rates. Traditional methods struggle
with the extreme limited-angle problems inherent in nonperiodic cases. Deep
learning methods have improved performance but face generalization challenges.
Recent implicit neural representation (INR) techniques show promise through
self-supervised deep learning, but have critical limitations: computational
inefficiency due to forward-warping modeling, difficulty balancing DVF
complexity with anatomical plausibility, and challenges in preserving fine
details without additional patient-specific pre-scans. This paper presents a
novel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It
addresses these challenges through four key contributions: (1) backward-warping
deformation that enables direct computation of each dynamic voxel with
significantly reduced computational cost, (2) diffeomorphism-based DVF
regularization that ensures anatomically plausible deformations while
maintaining representational capacity, (3) motion-compensated analytical
reconstruction that enhances fine details without requiring additional
pre-scans, and (4) dimensional-reduction design for efficient 4D coordinate
encoding. Through various simulations and practical studies, including digital
and physical phantoms and retrospective patient data, we demonstrate the
effectiveness of our approach for nonperiodic dynamic CT reconstruction with
enhanced details and reduced motion artifacts. The proposed framework enables
more accurate dynamic CT reconstruction with potential clinical applications,
such as one-beat cardiac reconstruction, cinematic image sequences for
functional imaging, and motion artifact reduction in conventional CT scans.

</details>


### [126] [UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion](https://arxiv.org/pdf/2505.03494)
*Zhanyuan Jia, Ni Yao, Danyang Sun, Chuang Han, Yanting Li, Jiaofen Nan, Fubao Zhu, Chen Zhao, Weihua Zhou*

Main category: cs.CV

TL;DR: A novel 3D brain tumor segmentation method combining deep learning with prior knowledge and uncertainty estimation, achieving superior performance on BraTS datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate brain tumor segmentation is challenging due to irregular shapes and vague boundaries, impacting diagnosis and treatment.

Method: Uses multi-scale feature fusion (MSFF), adaptive attention mechanisms (AAM), and Monte Carlo Dropout (MC Dropout) for uncertainty estimation.

Result: Achieves high Dice scores on BraTS datasets (e.g., 89.18% for ET, 93.67% for WT, 91.23% for TC on BraTS2021).

Conclusion: The proposed U-Net-based network improves robustness and performance by integrating prior knowledge and uncertainty estimation.

Abstract: Background: Brain tumor segmentation has a significant impact on the
diagnosis and treatment of brain tumors. Accurate brain tumor segmentation
remains challenging due to their irregular shapes, vague boundaries, and high
variability. Objective: We propose a brain tumor segmentation method that
combines deep learning with prior knowledge derived from a region-growing
algorithm. Methods: The proposed method utilizes a multi-scale feature fusion
(MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale
features and capture global contextual information. To enhance the model's
robustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout)
strategy is employed for uncertainty estimation. Results: Extensive experiments
demonstrate that the proposed method achieves superior performance on Brain
Tumor Segmentation (BraTS) datasets, significantly outperforming various
state-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are
89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT)
segmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019
validation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for
ET, WT, and TC segmentation, respectively. Ablation studies further confirmed
the contribution of each module to segmentation accuracy, indicating that each
component played a vital role in overall performance improvement. Conclusion:
This study proposed a novel 3D brain tumor segmentation network based on the
U-Net architecture. By incorporating the prior knowledge and employing the
uncertainty estimation method, the robustness and performance were improved.
The code for the proposed method is available at
https://github.com/chenzhao2023/UPMAD_Net_BrainSeg.

</details>


### [127] [Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks](https://arxiv.org/pdf/2505.03522)
*Haotong Cheng, Zhiqi Zhang, Hao Li, Xinshang Zhang*

Main category: cs.CV

TL;DR: The paper introduces 'Universality' to assess module transferability in SISR, proposes UAE for quantifying it, and designs optimized modules (CRB and DCRB) that outperform SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing SISR research lacks focus on module transferability. The paper aims to quantify and improve this through 'Universality' and UAE.

Method: Proposes UAE for module transferability assessment and designs CRB and DCRB modules based on UAE insights.

Result: Networks with CRB/DCRB outperform SOTA, achieving up to 0.83dB PSNR gain or 71.3% parameter reduction with minimal fidelity loss.

Conclusion: The UAE and optimized modules enhance SISR performance and transferability, validated across diverse datasets and deployments.

Abstract: Deep learning has substantially advanced the Single Image Super-Resolution
(SISR). However, existing researches have predominantly focused on raw
performance gains, with little attention paid to quantifying the
transferability of architectural components. In this paper, we introduce the
concept of "Universality" and its associated definitions which extend the
traditional notion of "Generalization" to encompass the modules' ease of
transferability, thus revealing the relationships between module universality
and model generalizability. Then we propose the Universality Assessment
Equation (UAE), a metric for quantifying how readily a given module could be
transplanted across models. Guided by the UAE results of standard residual
blocks and other plug-and-play modules, we further design two optimized
modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).
Through comprehensive experiments on natural-scene benchmarks, remote-sensing
datasets, extreme-industrial imagery and on-device deployments, we demonstrate
that networks embedded with the proposed plug-and-play modules outperform
several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or
enabling a 71.3% reduction in parameters with negligible loss in reconstruction
fidelity.

</details>


### [128] [MRI motion correction via efficient residual-guided denoising diffusion probabilistic models](https://arxiv.org/pdf/2505.03498)
*Mojtaba Safari, Shansong Wang, Qiang Li, Zach Eidex, Richard L. J. Qiu, Chih-Wei Chang, Hui Mao, Xiaofeng Yang*

Main category: cs.CV

TL;DR: Res-MoCoDiff, a diffusion model for MRI motion artifact correction, outperforms existing methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Motion artifacts degrade MRI quality; current solutions are costly and inefficient.

Method: Uses residual error shifting and a U-net with Swin-Transformer blocks, trained with l1+l2 loss.

Result: Achieves highest SSIM, lowest NMSE, and fastest sampling time (0.37s per batch).

Conclusion: Res-MoCoDiff is an efficient, high-performance solution for MRI motion artifact correction.

Abstract: Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly
degrade image quality and impair quantitative analysis. Conventional mitigation
strategies, such as repeated acquisitions or motion tracking, are costly and
workflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising
diffusion probabilistic model tailored for MRI motion artifact correction.
Methods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in
the forward diffusion process, aligning the noise distribution with
motion-corrupted data and enabling an efficient four-step reverse diffusion. A
U-net backbone enhanced with Swin-Transformer blocks conventional attention
layers, improving adaptability across resolutions. Training employs a combined
l1+l2 loss, which promotes image sharpness and reduces pixel-level errors.
Res-MoCoDiff was evaluated on synthetic dataset generated using a realistic
motion simulation framework and on an in-vivo dataset. Comparative analyses
were conducted against established methods, including CycleGAN, Pix2pix, and
MT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR),
structural similarity index measure (SSIM), and normalized mean squared error
(NMSE). Results: The proposed method demonstrated superior performance in
removing motion artifacts across all motion severity levels. Res-MoCoDiff
consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR
of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling
time was reduced to 0.37 seconds per batch of two image slices, compared with
101.74 seconds for conventional approaches.

</details>


### [129] [Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID](https://arxiv.org/pdf/2505.03557)
*Koray Ulusan, Benjamin Kiefer*

Main category: cs.CV

TL;DR: The paper explores how augmentations improve facial resemblance in Stable Diffusion-generated portraits using DreamBooth and InstantID, introducing FaceDistance for similarity ranking.


<details>
  <summary>Details</summary>
Motivation: To enhance facial resemblance in AI-generated professional portraits from amateur photos for downstream applications.

Method: Experiments with diverse datasets and augmentation strategies, using DreamBooth and InstantID, and introducing FaceDistance for similarity assessment.

Result: Augmentations significantly improve facial resemblance, with FaceDistance aiding in ranking similarity.

Conclusion: Augmentations play a key role in enhancing facial resemblance in SDXL-generated portraits, guiding effective deployment strategies.

Abstract: The personalization of Stable Diffusion for generating professional portraits
from amateur photographs is a burgeoning area, with applications in various
downstream contexts. This paper investigates the impact of augmentations on
improving facial resemblance when using two prominent personalization
techniques: DreamBooth and InstantID. Through a series of experiments with
diverse subject datasets, we assessed the effectiveness of various augmentation
strategies on the generated headshots' fidelity to the original subject. We
introduce FaceDistance, a wrapper around FaceNet, to rank the generations based
on facial similarity, which aided in our assessment. Ultimately, this research
provides insights into the role of augmentations in enhancing facial
resemblance in SDXL-generated portraits, informing strategies for their
effective deployment in downstream applications.

</details>


### [130] [Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking](https://arxiv.org/pdf/2505.03507)
*Shenglan Li, Rui Yao, Yong Zhou, Hancheng Zhu, Kunyang Sun, Bing Liu, Zhiwen Shao, Jiaqi Zhao*

Main category: cs.CV

TL;DR: GDSTrack introduces dynamic graph fusion and temporal diffusion to improve self-supervised RGB-T tracking by addressing pseudo-label noise and modality fusion challenges.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on large-scale annotations and mitigate issues like erroneous pseudo-labels and background noise in RGB-T tracking.

Method: Uses Modality-guided Dynamic Graph Fusion (MDGF) and Temporal Graph-Informed Diffusion (TGID) to dynamically fuse modalities and denoise similar-object interference.

Result: Outperforms state-of-the-art methods on four RGB-T tracking datasets.

Conclusion: GDSTrack effectively addresses noise and fusion challenges in self-supervised RGB-T tracking, demonstrating superior performance.

Abstract: To reduce the reliance on large-scale annotations, self-supervised RGB-T
tracking approaches have garnered significant attention. However, the omission
of the object region by erroneous pseudo-label or the introduction of
background noise affects the efficiency of modality fusion, while pseudo-label
noise triggered by similar object noise can further affect the tracking
performance. In this paper, we propose GDSTrack, a novel approach that
introduces dynamic graph fusion and temporal diffusion to address the above
challenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the
modalities of neighboring frames, treats them as distractor noise, and
leverages the denoising capability of a generative model. Specifically, by
constructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the
proposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic
adjacency matrix to guide graph attention, focusing on and fusing the object's
coherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features
from neighboring frames as interference, and thus improving robustness against
similar-object noise. Extensive experiments conducted on four public RGB-T
tracking datasets demonstrate that GDSTrack outperforms the existing
state-of-the-art methods. The source code is available at
https://github.com/LiShenglana/GDSTrack.

</details>


### [131] [Real-Time Person Image Synthesis Using a Flow Matching Model](https://arxiv.org/pdf/2505.03562)
*Jiwoo Jeong, Kirok Kim, Wooju Kim, Nam-Joon Kim*

Main category: cs.CV

TL;DR: PGPIS generates person images from poses but struggles with real-time performance. The proposed RPFM model uses flow matching for faster, stable synthesis, achieving near-real-time speeds with slight accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: Real-time PGPIS is crucial for applications like sign language video generation and live streaming, but current diffusion-based methods are too slow.

Method: The RPFM model leverages flow matching for efficient training and sampling, supporting conditional generation in latent space.

Result: RPFM achieves near-real-time speeds on the DeepFashion dataset, with a slight accuracy trade-off for a twofold speed increase.

Conclusion: RPFM addresses the real-time challenge in PGPIS, balancing speed and quality for practical applications.

Abstract: Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images
conditioned on a target pose and a source image. This task plays a key role in
various real-world applications, such as sign language video generation, AR/VR,
gaming, and live streaming. In these scenarios, real-time PGPIS is critical for
providing immediate visual feedback and maintaining user immersion.However,
achieving real-time performance remains a significant challenge due to the
complexity of synthesizing high-fidelity images from diverse and dynamic human
poses. Recent diffusion-based methods have shown impressive image quality in
PGPIS, but their slow sampling speeds hinder deployment in time-sensitive
applications. This latency is particularly problematic in tasks like generating
sign language videos during live broadcasts, where rapid image updates are
required. Therefore, developing a fast and reliable PGPIS model is a crucial
step toward enabling real-time interactive systems. To address this challenge,
we propose a generative model based on flow matching (FM). Our approach enables
faster, more stable, and more efficient training and sampling. Furthermore, the
proposed model supports conditional generation and can operate in latent space,
making it especially suitable for real-time PGPIS applications where both speed
and quality are critical. We evaluate our proposed method, Real-Time Person
Image Synthesis Using a Flow Matching Model (RPFM), on the widely used
DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves
near-real-time sampling speeds while maintaining performance comparable to the
state-of-the-art models. Our methodology trades off a slight, acceptable
decrease in generated-image accuracy for over a twofold increase in generation
speed, thereby ensuring real-time performance.

</details>


### [132] [Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication](https://arxiv.org/pdf/2505.03528)
*Chenguang Liu, Jianjun Chen, Yunfei Chen, Yubei He, Zhuangkun Wei, Hongjian Sun, Haiyan Lu, Qi Hao*

Main category: cs.CV

TL;DR: Proposes Coop-WD, a joint weighting and denoising framework, to improve cooperative perception in autonomous driving under V2V communication impairments. Includes an efficient variant, Coop-WD-eco, reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of generalization in existing works regarding V2V communication impairments and their impact on perception precision.

Method: Uses a self-supervised contrastive model and conditional diffusion probabilistic model hierarchically for feature enhancement. Introduces Coop-WD-eco for selective denoising.

Result: Outperforms benchmarks in all channel types; Coop-WD-eco reduces computational cost by 50% under severe distortion.

Conclusion: Coop-WD and its variant effectively enhance cooperative perception while balancing accuracy and efficiency.

Abstract: Cooperative perception, leveraging shared information from multiple vehicles
via vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous
driving to alleviate the limitation of single-vehicle perception. Existing
works have explored the effects of V2V communication impairments on perception
precision, but they lack generalization to different levels of impairments. In
this work, we propose a joint weighting and denoising framework, Coop-WD, to
enhance cooperative perception subject to V2V channel impairments. In this
framework, the self-supervised contrastive model and the conditional diffusion
probabilistic model are adopted hierarchically for vehicle-level and
pixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is
proposed to selectively deactivate denoising to reduce processing overhead.
Rician fading, non-stationarity, and time-varying distortion are considered.
Simulation results demonstrate that the proposed Coop-WD outperforms
conventional benchmarks in all types of channels. Qualitative analysis with
visual examples further proves the superiority of our proposed method. The
proposed Coop-WD-eco achieves up to 50% reduction in computational cost under
severe distortion while maintaining comparable accuracy as channel conditions
improve.

</details>


### [133] [RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT](https://arxiv.org/pdf/2505.03538)
*Chuyu Zhao, Hao Huang, Jiashuo Guo, Ziyu Shen, Zhongwei Zhou, Jie Liu, Zekuan Yu*

Main category: cs.CV

TL;DR: RAIL is a semi-supervised framework for 3D tooth segmentation that addresses challenges like ambiguous regions and unreliable pseudo-labels by using dual-group dual-student training and region-aware instructive mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised methods struggle with ambiguous regions and unreliable pseudo-labels in 3D tooth segmentation from CBCT scans.

Method: RAIL employs a dual-group dual-student framework with two instructive mechanisms: DFS Controller for supervised learning and CAL Modulator for unsupervised learning.

Result: RAIL outperforms state-of-the-art methods on four CBCT datasets under limited annotation.

Conclusion: RAIL effectively improves segmentation accuracy by focusing on ambiguous regions and enhancing pseudo-label reliability.

Abstract: Semi-supervised learning has become a compelling approach for 3D tooth
segmentation from CBCT scans, where labeled data is minimal. However, existing
methods still face two persistent challenges: limited corrective supervision in
structurally ambiguous or mislabeled regions during supervised training and
performance degradation caused by unreliable pseudo-labels on unlabeled data.
To address these problems, we propose Region-Aware Instructive Learning (RAIL),
a dual-group dual-student, semi-supervised framework. Each group contains two
student models guided by a shared teacher network. By alternating training
between the two groups, RAIL promotes intergroup knowledge transfer and
collaborative region-aware instruction while reducing overfitting to the
characteristics of any single model. Specifically, RAIL introduces two
instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller
improves supervised learning by instructing predictions only within areas where
student outputs diverge from both ground truth and the best student, thereby
concentrating supervision on structurally ambiguous or mislabeled areas. In the
unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces
agreement in regions with high model certainty while reducing the effect of
low-confidence predictions during training. This helps prevent our model from
learning unstable patterns and improves the overall reliability of
pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets
show that RAIL surpasses state-of-the-art methods under limited annotation. Our
code will be available at https://github.com/Tournesol-Saturday/RAIL.

</details>


### [134] [Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment](https://arxiv.org/pdf/2505.03554)
*João Alves, Pia Haubro Andersen, Rikke Gade*

Main category: cs.CV

TL;DR: The paper introduces an automated system for detecting and localizing ear Action Units (AUs) in horses using deep learning and optical flow, achieving 87.5% accuracy, aiming to improve equine welfare and veterinary diagnostics.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of horse facial AUs is time-consuming and costly, limiting affective state assessment. Automated systems are needed to leverage existing data.

Method: Combines deep learning-based video feature extraction with recurrent neural networks and optical flow for ear AU detection in horse videos.

Result: Achieved 87.5% classification accuracy for ear movement presence on a public dataset.

Conclusion: The approach shows promise for automating AU detection, with potential applications in equine welfare and veterinary diagnostics. Future work aims to bridge the gap to practical use.

Abstract: The Equine Facial Action Coding System (EquiFACS) enables the systematic
annotation of facial movements through distinct Action Units (AUs). It serves
as a crucial tool for assessing affective states in horses by identifying
subtle facial expressions associated with discomfort. However, the field of
horse affective state assessment is constrained by the scarcity of annotated
data, as manually labelling facial AUs is both time-consuming and costly. To
address this challenge, automated annotation systems are essential for
leveraging existing datasets and improving affective states detection tools. In
this work, we study different methods for specific ear AU detection and
localization from horse videos. We leverage past works on deep learning-based
video feature extraction combined with recurrent neural networks for the video
classification task, as well as a classic optical flow based approach. We
achieve 87.5% classification accuracy of ear movement presence on a public
horse video dataset, demonstrating the potential of our approach. We discuss
future directions to develop these systems, with the aim of bridging the gap
between automated AU detection and practical applications in equine welfare and
veterinary diagnostics. Our code will be made publicly available at
https://github.com/jmalves5/read-my-ears.

</details>


### [135] [Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images](https://arxiv.org/pdf/2505.03567)
*Zengli Luo, Canlong Zhang, Xiaochun Lu, Zhixin Li, Zhiwen Wang*

Main category: cs.CV

TL;DR: UPD-TBPS is a novel framework for text-based pedestrian search, addressing uncertainties in detection and matching with multi-granularity uncertainty estimation, prototype-based decoupling, and cross-modal re-identification.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-based pedestrian search struggle with uncertainties in detection and matching in complex scenes.

Method: Proposes UPD-TBPS with three modules: Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty Decoupling (PUD), and Cross-modal Re-identification (ReID).

Result: Validated on CUHK-SYSU-TBPS and PRW-TBPS datasets, showing improved accuracy.

Conclusion: UPD-TBPS effectively reduces uncertainties and enhances pedestrian search performance.

Abstract: Text-based pedestrian search (TBPS) in full images aims to locate a target
pedestrian in untrimmed images using natural language descriptions. However, in
complex scenes with multiple pedestrians, existing methods are limited by
uncertainties in detection and matching, leading to degraded performance. To
address this, we propose UPD-TBPS, a novel framework comprising three modules:
Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty
Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts
multi-granularity queries to identify potential targets and assigns confidence
scores to reduce early-stage uncertainty. PUD leverages visual context
decoupling and prototype mining to extract features of the target pedestrian
described in the query. It separates and learns pedestrian prototype
representations at both the coarse-grained cluster level and the fine-grained
individual level, thereby reducing matching uncertainty. ReID evaluates
candidates with varying confidence levels, improving detection and retrieval
accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the
effectiveness of our framework.

</details>


### [136] [ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant](https://arxiv.org/pdf/2505.03654)
*Yifan Xiang, Zhenxi Zhang, Bin Li, Yixuan Weng, Shoujun Zhou, Yangfan He, Keqin Li*

Main category: cs.CV

TL;DR: The paper introduces ReGraP, a dataset and benchmark for relational reasoning in personalized MLLMs, addressing limitations in existing methods by incorporating multi-object sets and structured reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack multi-object relational training data and fail to reason over personalized concepts, limiting contextual understanding.

Method: Proposes ReGraP-LLaVA, an MLLM trained with knowledge graphs (KGs) and chain-of-thought (CoT) QA pairs, using soft/hard graph prompting for KG alignment.

Result: ReGraP-LLaVA achieves state-of-the-art performance in relational reasoning and personalized knowledge tasks.

Conclusion: The ReGraP framework advances personalized MLLMs by enabling structured reasoning and knowledge connection, validated by the ReGraP Benchmark.

Abstract: Recent advances in personalized MLLMs enable effective capture of
user-specific concepts, supporting both recognition of personalized concepts
and contextual captioning. However, humans typically explore and reason over
relations among objects and individuals, transcending surface-level information
to achieve more personalized and contextual understanding. To this end,
existing methods may face three main limitations: Their training data lacks
multi-object sets in which relations among objects are learnable. Building on
the limited training data, their models overlook the relations between
different personalized concepts and fail to reason over them. Their experiments
mainly focus on a single personalized concept, where evaluations are limited to
recognition and captioning tasks. To address the limitations, we present a new
dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each
set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more
structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an
MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard
graph prompting methods are designed to align KGs within the model's semantic
space. We establish the ReGraP Benchmark, which contains diverse task types:
multiple-choice, fill-in-the-blank, True/False, and descriptive questions in
both open- and closed-ended settings. The proposed benchmark is designed to
evaluate the relational reasoning and knowledge-connection capability of
personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and
other competitive MLLMs. Results show that the proposed model not only learns
personalized knowledge but also performs relational reasoning in responses,
achieving the SoTA performance compared with the competitive methods. All the
codes and datasets are released at: https://github.com/xyfyyds/ReGraP.

</details>


### [137] [Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models](https://arxiv.org/pdf/2505.03569)
*Mishal Fatima, Steffen Jung, Margret Keuper*

Main category: cs.CV

TL;DR: The paper investigates how background biases in images (position and size) lead models to rely on spurious features, using a synthetic dataset (Hard-Spurious-ImageNet) to demonstrate this. Current mitigation methods fail to address these biases effectively.


<details>
  <summary>Details</summary>
Motivation: To understand how positional and size biases in datasets cause models to depend on spurious background features, and to highlight the limitations of existing mitigation techniques.

Method: Proposes Hard-Spurious-ImageNet, a synthetic dataset derived from ImageNet1k, to test models under varied backgrounds, object positions, and sizes. Evaluates pretrained models on this dataset.

Result: Models rely heavily on spurious background features when the ROI is small or the object is off-center. Existing mitigation methods fail to improve worst-group accuracy under these conditions.

Conclusion: Background biases significantly impact model reliance on spurious features, and current mitigation approaches are inadequate for addressing these biases.

Abstract: Backgrounds in images play a major role in contributing to spurious
correlations among different data points. Owing to aesthetic preferences of
humans capturing the images, datasets can exhibit positional (location of the
object within a given frame) and size (region-of-interest to image ratio)
biases for different classes. In this paper, we show that these biases can
impact how much a model relies on spurious features in the background to make
its predictions. To better illustrate our findings, we propose a synthetic
dataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images
with various backgrounds, object positions, and object sizes. By evaluating the
dataset on different pretrained models, we find that most models rely heavily
on spurious features in the background when the region-of-interest (ROI) to
image ratio is small and the object is far from the center of the image.
Moreover, we also show that current methods that aim to mitigate harmful
spurious features, do not take into account these factors, hence fail to
achieve considerable performance gains for worst-group accuracies when the size
and location of core features in an image change.

</details>


### [138] [Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning](https://arxiv.org/pdf/2505.03575)
*Maria Kainz, Johannes K. Krondorfer, Malte Jaschik, Maria Jernej, Harald Ganster*

Main category: cs.CV

TL;DR: Hyperspectral NIR imaging and deep learning enable efficient textile fiber classification for sustainable recycling.


<details>
  <summary>Details</summary>
Motivation: Reducing the environmental impact of the textile industry by improving fiber recycling.

Method: Investigating supervised (CNNs) and unsupervised (autoencoder) deep learning models for fiber classification on diverse textile structures.

Result: Optimized CNNs and autoencoders demonstrate robust generalization across varying conditions.

Conclusion: Hyperspectral imaging and deep learning can significantly enhance textile recycling through accurate classification.

Abstract: Recycling textile fibers is critical to reducing the environmental impact of
the textile industry. Hyperspectral near-infrared (NIR) imaging combined with
advanced deep learning algorithms offers a promising solution for efficient
fiber classification and sorting. In this study, we investigate supervised and
unsupervised deep learning models and test their generalization capabilities on
different textile structures. We show that optimized convolutional neural
networks (CNNs) and autoencoder networks achieve robust generalization under
varying conditions. These results highlight the potential of hyperspectral
imaging and deep learning to advance sustainable textile recycling through
accurate and robust classification.

</details>


### [139] [Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models](https://arxiv.org/pdf/2505.03662)
*Xin Du, Francesca M. Cozzi, Rajesh Jena*

Main category: cs.CV

TL;DR: A CycleGAN-based method generates FA maps from T1-weighted MRI scans, improving alignment with tractography atlases and showing strong performance in tumour regions.


<details>
  <summary>Details</summary>
Motivation: Address spatial misalignment between FA maps and tractography atlases to enhance predictive models in neuroimaging.

Method: Proposes a CycleGAN approach trained on unpaired data to generate FA maps from T1-weighted MRI scans.

Result: High fidelity FA maps validated by SSIM and PSNR, with robust performance in tumour regions. Radiological assessments highlight clinical utility.

Conclusion: The model offers an AI-driven alternative to reduce additional scans, improving clinical workflows.

Abstract: Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are
essential for evaluating white matter integrity and structural connectivity in
neuroimaging. However, the spatial misalignment between FA maps and
tractography atlases hinders their effective integration into predictive
models. To address this issue, we propose a CycleGAN based approach for
generating FA maps directly from T1-weighted MRI scans, representing the first
application of this technique to both healthy and tumour-affected tissues. Our
model, trained on unpaired data, produces high fidelity maps, which have been
rigorously evaluated using Structural Similarity Index (SSIM) and Peak
Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in
tumour regions. Radiological assessments further underscore the model's
potential to enhance clinical workflows by providing an AI-driven alternative
that reduces the necessity for additional scans.

</details>


### [140] [DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes](https://arxiv.org/pdf/2505.03581)
*Sergey Linok, Vadim Semenov, Anastasia Trunova, Oleg Bulichev, Dmitry Yudin*

Main category: cs.CV

TL;DR: DyGEnc introduces a dynamic graph encoding method combining spatial-temporal observations with large language models, outperforming visual methods by 15-25% in human-object interaction queries.


<details>
  <summary>Details</summary>
Motivation: Current visual models lack interpretable spatial-temporal object representations, limiting intelligent agents' interaction capabilities.

Method: DyGEnc integrates compressed spatial-temporal structural observations with large language models for advanced question answering using textual scene graphs.

Result: Outperforms existing methods by 15-25% on STAR and AGQA datasets and extends to raw image processing for robotic applications.

Conclusion: DyGEnc advances graph-based robotic memory for long-horizon reasoning, with potential for robust implementation.

Abstract: The analysis of events in dynamic environments poses a fundamental challenge
in the development of intelligent agents and robots capable of interacting with
humans. Current approaches predominantly utilize visual models. However, these
methods often capture information implicitly from images, lacking interpretable
spatial-temporal object representations. To address this issue we introduce
DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates
compressed spatial-temporal structural observation representation with the
cognitive capabilities of large language models. The purpose of this
integration is to enable advanced question answering based on a sequence of
textual scene graphs. Extended evaluations on the STAR and AGQA datasets
indicate that DyGEnc outperforms existing visual methods by a large margin of
15-25% in addressing queries regarding the history of human-to-object
interactions. Furthermore, the proposed method can be seamlessly extended to
process raw input images utilizing foundational models for extracting explicit
textual scene graphs, as substantiated by the results of a robotic experiment
conducted with a wheeled manipulator platform. We hope that these findings will
contribute to the implementation of robust and compressed graph-based robotic
memory for long-horizon reasoning. Code is available at
github.com/linukc/DyGEnc.

</details>


### [141] [Fixed-Length Dense Fingerprint Representation](https://arxiv.org/pdf/2505.03597)
*Zhiyu Pan, Xiongjun Guan, Yongjie Duan, Jianjiang Feng, Jie Zhou*

Main category: cs.CV

TL;DR: FLARE is a fingerprint matching framework using a fixed-length dense descriptor for robust, scalable, and accurate fingerprint representation and matching across diverse modalities.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in creating a robust fixed-length fingerprint representation that handles diverse modalities, pose variations, and noise.

Method: FLARE integrates a 3D dense descriptor for spatial relationships, pose-based alignment, and dual enhancement strategies for clarity and modality preservation.

Result: FLARE outperforms existing methods in cross-modality and low-quality scenarios, demonstrating superior performance.

Conclusion: FLARE is a unified, scalable solution for robust fingerprint matching, validated by extensive experiments.

Abstract: Fixed-length fingerprint representations, which map each fingerprint to a
compact and fixed-size feature vector, are computationally efficient and
well-suited for large-scale matching. However, designing a robust
representation that effectively handles diverse fingerprint modalities, pose
variations, and noise interference remains a significant challenge. In this
work, we propose a fixed-length dense descriptor of fingerprints, and introduce
FLARE-a fingerprint matching framework that integrates the Fixed-Length dense
descriptor with pose-based Alignment and Robust Enhancement. This fixed-length
representation employs a three-dimensional dense descriptor to effectively
capture spatial relationships among fingerprint ridge structures, enabling
robust and locally discriminative representations. To ensure consistency within
this dense feature space, FLARE incorporates pose-based alignment using
complementary estimation methods, along with dual enhancement strategies that
refine ridge clarity while preserving the original fingerprint modality. The
proposed dense descriptor supports fixed-length representation while
maintaining spatial correspondence, enabling fast and accurate similarity
computation. Extensive experiments demonstrate that FLARE achieves superior
performance across rolled, plain, latent, and contactless fingerprints,
significantly outperforming existing methods in cross-modality and low-quality
scenarios. Further analysis validates the effectiveness of the dense descriptor
design, as well as the impact of alignment and enhancement modules on the
accuracy of dense descriptor matching. Experimental results highlight the
effectiveness and generalizability of FLARE as a unified and scalable solution
for robust fingerprint representation and matching. The implementation and code
will be publicly available at https://github.com/Yu-Yy/FLARE.

</details>


### [142] [From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction](https://arxiv.org/pdf/2505.03599)
*Fengming Lin, Arezoo Zakeri, Yidan Xue, Michael MacRaild, Haoran Dou, Zherui Zhou, Ziwei Zou, Ali Sarrami-Foroushani, Jinming Duan, Alejandro F. Frangi*

Main category: cs.CV

TL;DR: A survey on deep learning-based medical image-to-mesh reconstruction, categorizing methods into four types, evaluating their strengths, limitations, and applications, and discussing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of disease mechanisms and improve diagnostic/therapeutic techniques by transforming medical imaging data into 3D mesh models.

Method: Systematic categorization into template, statistical, generative, and implicit models, with detailed analysis of each, supported by quantitative comparisons and dataset evaluations.

Result: Identifies strengths, limitations, and applicability of methods across anatomical structures and imaging modalities, along with challenges like topological correctness and multi-modality integration.

Conclusion: The survey serves as a comprehensive reference for researchers, highlighting future research directions in medical image-to-mesh reconstruction.

Abstract: Deep learning-based medical image-to-mesh reconstruction has rapidly evolved,
enabling the transformation of medical imaging data into three-dimensional mesh
models that are critical in computational medicine and in silico trials for
advancing our understanding of disease mechanisms, and diagnostic and
therapeutic techniques in modern medicine. This survey systematically
categorizes existing approaches into four main categories: template models,
statistical models, generative models, and implicit models. Each category is
analysed in detail, examining their methodological foundations, strengths,
limitations, and applicability to different anatomical structures and imaging
modalities. We provide an extensive evaluation of these methods across various
anatomical applications, from cardiac imaging to neurological studies,
supported by quantitative comparisons using standard metrics. Additionally, we
compile and analyze major public datasets available for medical mesh
reconstruction tasks and discuss commonly used evaluation metrics and loss
functions. The survey identifies current challenges in the field, including
requirements for topological correctness, geometric accuracy, and
multi-modality integration. Finally, we present promising future research
directions in this domain. This systematic review aims to serve as a
comprehensive reference for researchers and practitioners in medical image
analysis and computational medicine.

</details>


### [143] [Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection](https://arxiv.org/pdf/2505.03610)
*Fangling Jiang, Qi Li, Bing Liu, Weining Wang, Caifeng Shan, Zhenan Sun, Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: A novel knowledge-based prompt learning framework for 3D mask attack detection, leveraging vision-language models and knowledge graphs for improved generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing methods (high costs, poor generalization) by exploring vision-language multimodal features for 3D mask attack detection.

Method: Incorporates knowledge graphs into prompt learning, uses visual-specific knowledge filters, and applies causal graph theory to enhance generalization.

Result: Achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets.

Conclusion: The proposed framework effectively harnesses vision-language models and knowledge graphs for robust 3D mask attack detection.

Abstract: 3D mask presentation attack detection is crucial for protecting face
recognition systems against the rising threat of 3D mask attacks. While most
existing methods utilize multimodal features or remote photoplethysmography
(rPPG) signals to distinguish between real faces and 3D masks, they face
significant challenges, such as the high costs associated with multimodal
sensors and limited generalization ability. Detection-related text descriptions
offer concise, universal information and are cost-effective to obtain. However,
the potential of vision-language multimodal features for 3D mask presentation
attack detection remains unexplored. In this paper, we propose a novel
knowledge-based prompt learning framework to explore the strong generalization
capability of vision-language models for 3D mask presentation attack detection.
Specifically, our approach incorporates entities and triples from knowledge
graphs into the prompt learning process, generating fine-grained, task-specific
explicit prompts that effectively harness the knowledge embedded in pre-trained
vision-language models. Furthermore, considering different input images may
emphasize distinct knowledge graph elements, we introduce a visual-specific
knowledge filter based on an attention mechanism to refine relevant elements
according to the visual context. Additionally, we leverage causal graph theory
insights into the prompt learning process to further enhance the generalization
ability of our method. During training, a spurious correlation elimination
paradigm is employed, which removes category-irrelevant local image patches
using guidance from knowledge-based text features, fostering the learning of
generalized causal prompts that align with category-relevant local patches.
Experimental results demonstrate that the proposed method achieves
state-of-the-art intra- and cross-scenario detection performance on benchmark
datasets.

</details>


### [144] [FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback](https://arxiv.org/pdf/2404.05046)
*Liqiang Jing, Xinya Du*

Main category: cs.CV

TL;DR: The paper addresses misalignment in Large Vision-Language Models (LVLMs) causing hallucination issues. It proposes Fine-Grained Artificial Intelligence Feedback (FGAIF) to improve alignment through AI-based feedback, specialized reward models, and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs suffer from text-image misalignment leading to hallucination problems (object existence, attribute, relationship). Existing RL methods lack fine-grained feedback, sparse rewards, and high annotation costs.

Method: Proposes FGAIF: (1) AI-based feedback collection to identify hallucination types, (2) training specialized reward models for dense rewards, (3) integrating fine-grained feedback into PPO for reinforcement learning.

Result: Demonstrates superior performance on hallucination and general benchmarks, even with fewer parameters compared to previous RL-based methods.

Conclusion: FGAIF effectively addresses LVLM misalignment and hallucination issues, offering a scalable and efficient solution.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated proficiency in
tackling a variety of visual-language tasks. However, current LVLMs suffer from
misalignment between text and image modalities which causes three kinds of
hallucination problems, i.e., object existence, object attribute, and object
relationship. To tackle this issue, existing methods mainly utilize
Reinforcement Learning (RL) to align modalities in LVLMs. However, they still
suffer from three main limitations: (1) General feedback can not indicate the
hallucination type contained in the response; (2) Sparse rewards only give the
sequence-level reward for the whole response; and (3)Annotation cost is
time-consuming and labor-intensive. To handle these limitations, we propose an
innovative method to align modalities in LVLMs through Fine-Grained Artificial
Intelligence Feedback (FGAIF), which mainly consists of three steps: AI-based
Feedback Collection, Fine-grained Reward Model Training, and Reinforcement
Learning with Fine-grained Reward. Specifically, We first utilize AI tools to
predict the types of hallucination for each segment in the response and obtain
a collection of fine-grained feedback. Then, based on the collected reward
data, three specialized reward models are trained to produce dense rewards.
Finally, a novel fine-grained feedback module is integrated into the Proximal
Policy Optimization (PPO) algorithm. Extensive experiments are conducted on
hallucination and general benchmarks, demonstrating the superior performance of
our proposed method. Notably, compared with previous models trained with the
RL-based aligning method, our proposed method is effective even with fewer
parameters.

</details>


### [145] [Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images](https://arxiv.org/pdf/2505.03611)
*Fangling Jiang, Qi Li, Weining Wang, Wei Shen, Bing Liu, Zhenan Sun*

Main category: cs.CV

TL;DR: The paper proposes a novel method for face anti-spoofing by generating diverse spoof prompts using vision-language models, improving generalization without spoof face images.


<details>
  <summary>Details</summary>
Motivation: Addressing the limited generalization of face anti-spoofing due to covariate and semantic shifts from diverse attack types and data variations.

Method: Generates textual prompts for real faces and unknown spoof attacks using vision-language models, optimizing prompts to maximize distance from real faces and ensure semantic independence.

Result: Achieves state-of-the-art generalization on nine datasets, effectively handling unseen attack types without spoof images.

Conclusion: The approach successfully leverages vision-language models to enhance generalization in face anti-spoofing, addressing key challenges of covariate and semantic shifts.

Abstract: Face anti-spoofing is a critical technology for ensuring the security of face
recognition systems. However, its ability to generalize across diverse
scenarios remains a significant challenge. In this paper, we attribute the
limited generalization ability to two key factors: covariate shift, which
arises from external data collection variations, and semantic shift, which
results from substantial differences in emerging attack types. To address both
challenges, we propose a novel approach for learning unknown spoof prompts,
relying solely on real face images from a single source domain. Our method
generates textual prompts for real faces and potential unknown spoof attacks by
leveraging the general knowledge embedded in vision-language models, thereby
enhancing the model's ability to generalize to unseen target domains.
Specifically, we introduce a diverse spoof prompt optimization framework to
learn effective prompts. This framework constrains unknown spoof prompts within
a relaxed prior knowledge space while maximizing their distance from real face
images. Moreover, it enforces semantic independence among different spoof
prompts to capture a broad range of spoof patterns. Experimental results on
nine datasets demonstrate that the learned prompts effectively transfer the
knowledge of vision-language models, enabling state-of-the-art generalization
ability against diverse unknown attack types across unseen target domains
without using any spoof face images.

</details>


### [146] [PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing](https://arxiv.org/pdf/2505.03621)
*Yiping Xie, Bo Zhao, Mingtong Dai, Jian-Ping Zhou, Yue Sun, Tao Tan, Weicheng Xie, Linlin Shen, Zitong Yu*

Main category: cs.CV

TL;DR: PhysLLM combines LLMs with rPPG to improve non-contact physiological measurement by addressing illumination and motion challenges using cross-modal alignment and adaptive feature re-weighting.


<details>
  <summary>Details</summary>
Motivation: rPPG is prone to illumination changes and motion artifacts, while LLMs struggle with noise-sensitive rPPG signals due to their text-centric design.

Method: PhysLLM integrates LLMs with rPPG components using Text Prototype Guidance (TPG) for cross-modal alignment and a Dual-Domain Stationary (DDS) Algorithm for signal stability.

Result: PhysLLM achieves state-of-the-art accuracy and robustness across four benchmark datasets, handling lighting variations and motion effectively.

Conclusion: PhysLLM successfully bridges the gap between LLMs and rPPG, enhancing performance in challenging scenarios.

Abstract: Remote photoplethysmography (rPPG) enables non-contact physiological
measurement but remains highly susceptible to illumination changes, motion
artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at
capturing long-range dependencies, offering a potential solution but struggle
with the continuous, noise-sensitive nature of rPPG signals due to their
text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative
optimization framework that synergizes LLMs with domain-specific rPPG
components. Specifically, the Text Prototype Guidance (TPG) strategy is
proposed to establish cross-modal alignment by projecting hemodynamic features
into LLM-interpretable semantic space, effectively bridging the
representational gap between physiological signals and linguistic tokens.
Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for
resolving signal instability through adaptive time-frequency domain feature
re-weighting. Finally, rPPG task-specific cues systematically inject
physiological priors through physiological statistics, environmental contextual
answering, and task description, leveraging cross-modal learning to integrate
both visual and textual information, enabling dynamic adaptation to challenging
scenarios like variable illumination and subject movements. Evaluation on four
benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,
demonstrating superior generalization across lighting variations and motion
scenarios.

</details>


### [147] [Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map](https://arxiv.org/pdf/2505.03623)
*Alessandro Simoni, Francesco Pelosin*

Main category: cs.CV

TL;DR: A diffusion-based pipeline for generating high-fidelity synthetic industrial datasets with minimal supervision, improving defect consistency and spatial accuracy.


<details>
  <summary>Details</summary>
Motivation: Industrial defect segmentation requires costly and time-consuming labeled data, which synthetic datasets can mitigate.

Method: Conditions a diffusion model on enriched bounding box representations to produce precise segmentation masks.

Result: Outperforms existing layout-conditioned methods in defect consistency and spatial accuracy, validated by new metrics.

Conclusion: Diffusion-based synthesis bridges the gap between synthetic and real industrial data, enabling reliable and cost-efficient segmentation models.

Abstract: Synthetic dataset generation in Computer Vision, particularly for industrial
applications, is still underexplored. Industrial defect segmentation, for
instance, requires highly accurate labels, yet acquiring such data is costly
and time-consuming. To address this challenge, we propose a novel
diffusion-based pipeline for generating high-fidelity industrial datasets with
minimal supervision. Our approach conditions the diffusion model on enriched
bounding box representations to produce precise segmentation masks, ensuring
realistic and accurately localized defect synthesis. Compared to existing
layout-conditioned generative methods, our approach improves defect consistency
and spatial accuracy. We introduce two quantitative metrics to evaluate the
effectiveness of our method and assess its impact on a downstream segmentation
task trained on real and synthetic data. Our results demonstrate that
diffusion-based synthesis can bridge the gap between artificial and real-world
industrial data, fostering more reliable and cost-efficient segmentation
models. The code is publicly available at
https://github.com/covisionlab/diffusion_labeling.

</details>


### [148] [Tailored Design of Audio-Visual Speech Recognition Models using Branchformers](https://arxiv.org/pdf/2407.06606)
*David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos*

Main category: cs.CV

TL;DR: A novel parameter-efficient AVSR framework using Branchformer-inspired encoders achieves competitive WERs (2.5% for English, 9.1% for Spanish) while reducing model complexity.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of designing optimal cross-modal architectures for AVSR, reducing reliance on large, computationally expensive models.

Method: Proposes a two-step framework: first estimates modality-specific models, then designs a unified encoder using branch scores from these models.

Result: Achieves state-of-the-art WERs (2.5% for English, 9.1% for Spanish) with reduced complexity.

Conclusion: The tailored AVSR system balances performance and efficiency, setting a new benchmark for the field.

Abstract: Recent advances in Audio-Visual Speech Recognition (AVSR) have led to
unprecedented achievements in the field, improving the robustness of this type
of system in adverse, noisy environments. In most cases, this task has been
addressed through the design of models composed of two independent encoders,
each dedicated to a specific modality. However, while recent works have
explored unified audio-visual encoders, determining the optimal cross-modal
architecture remains an ongoing challenge. Furthermore, such approaches often
rely on models comprising vast amounts of parameters and high computational
cost training processes. In this paper, we aim to bridge this research gap by
introducing a novel audio-visual framework. Our proposed method constitutes, to
the best of our knowledge, the first attempt to harness the flexibility and
interpretability offered by encoder architectures, such as the Branchformer, in
the design of parameter-efficient AVSR systems. To be more precise, the
proposed framework consists of two steps: first, estimating audio- and
video-only systems, and then designing a tailored audio-visual unified encoder
based on the layer-level branch scores provided by the modality-specific
models. Extensive experiments on English and Spanish AVSR benchmarks covering
multiple data conditions and scenarios demonstrated the effectiveness of our
proposed method. Even when trained on a moderate scale of data, our models
achieve competitive word error rates (WER) of approximately 2.5\% for English
and surpass existing approaches for Spanish, establishing a new benchmark with
an average WER of around 9.1\%. These results reflect how our tailored AVSR
system is able to reach state-of-the-art recognition rates while significantly
reducing the model complexity w.r.t. the prevalent approach in the field. Code
and pre-trained models are available at
https://github.com/david-gimeno/tailored-avsr.

</details>


### [149] [Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision](https://arxiv.org/pdf/2505.03631)
*Linhan Cao, Wei Sun, Kaiwei Zhang, Yicong Peng, Guangtao Zhai, Xiongkuo Min*

Main category: cs.CV

TL;DR: A self-supervised learning framework for video quality assessment (VQA) is introduced, leveraging unlabeled web videos and iterative self-improvement to outperform supervised models in generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of manually annotated datasets in VQA, which are labor-intensive and hard to scale, by using self-supervised learning.

Method: Uses a learning-to-rank paradigm and iterative self-improvement training on large-scale unlabeled videos, with pseudo-labeling and synthetic distortions.

Result: Achieves zero-shot performance matching supervised models, superior OOD generalization, and state-of-the-art results when fine-tuned.

Conclusion: The self-supervised approach effectively trains generalized VQA models, with datasets and code to be released for future research.

Abstract: Video quality assessment (VQA) is essential for quantifying perceptual
quality in various video processing workflows, spanning from camera capture
systems to over-the-top streaming platforms. While recent supervised VQA models
have made substantial progress, the reliance on manually annotated datasets --
a process that is labor-intensive, costly, and difficult to scale up -- has
hindered further optimization of their generalization to unseen video content
and distortions. To bridge this gap, we introduce a self-supervised learning
framework for VQA to learn quality assessment capabilities from large-scale,
unlabeled web videos. Our approach leverages a \textbf{learning-to-rank}
paradigm to train a large multimodal model (LMM) on video pairs automatically
labeled via two manners, including quality pseudo-labeling by existing VQA
models and relative quality ranking based on synthetic distortion simulations.
Furthermore, we introduce a novel \textbf{iterative self-improvement training
strategy}, where the trained model acts an improved annotator to iteratively
refine the annotation quality of training data. By training on a dataset
$10\times$ larger than the existing VQA benchmarks, our model: (1) achieves
zero-shot performance on in-domain VQA benchmarks that matches or surpasses
supervised models; (2) demonstrates superior out-of-distribution (OOD)
generalization across diverse video content and distortions; and (3) sets a new
state-of-the-art when fine-tuned on human-labeled datasets. Extensive
experimental results validate the effectiveness of our self-supervised approach
in training generalized VQA models. The datasets and code will be publicly
released to facilitate future research.

</details>


### [150] [Towards Smart Point-and-Shoot Photography](https://arxiv.org/pdf/2505.03638)
*Jiawan Li, Fei Zhou, Zhipeng Zhong, Jiongzhi Lin, Guoping Qiu*

Main category: cs.CV

TL;DR: A smart point-and-shoot (SPAS) system is introduced to help users compose better photos by guiding camera pose adjustments. It uses a dataset of 320K images, a CLIP-based quality assessment model (CCQA), and a camera pose adjustment model (CPAM).


<details>
  <summary>Details</summary>
Motivation: Traditional point-and-shoot cameras lack guidance for composing good shots, despite widespread smartphone photography usage. The SPAS system aims to fill this gap by assisting users in real-time.

Method: The system involves: 1) Creating a large dataset with camera pose data, 2) Developing a CCQA model for quality assessment using learnable text embeddings, and 3) Training a CPAM model for pose adjustment suggestions.

Result: The SPAS system demonstrates performance through extensive testing on public image composition datasets.

Conclusion: The SPAS system successfully aids users in composing better photos by leveraging advanced models for quality assessment and pose adjustment.

Abstract: Hundreds of millions of people routinely take photos using their smartphones
as point and shoot (PAS) cameras, yet very few would have the photography
skills to compose a good shot of a scene. While traditional PAS cameras have
built-in functions to ensure a photo is well focused and has the right
brightness, they cannot tell the users how to compose the best shot of a scene.
In this paper, we present a first of its kind smart point and shoot (SPAS)
system to help users to take good photos. Our SPAS proposes to help users to
compose a good shot of a scene by automatically guiding the users to adjust the
camera pose live on the scene. We first constructed a large dataset containing
320K images with camera pose information from 4000 scenes. We then developed an
innovative CLIP-based Composition Quality Assessment (CCQA) model to assign
pseudo labels to these images. The CCQA introduces a unique learnable text
embedding technique to learn continuous word embeddings capable of discerning
subtle visual quality differences in the range covered by five levels of
quality description words {bad, poor, fair, good, perfect}. And finally we have
developed a camera pose adjustment model (CPAM) which first determines if the
current view can be further improved and if so it outputs the adjust suggestion
in the form of two camera pose adjustment angles. The two tasks of CPAM make
decisions in a sequential manner and each involves different sets of training
samples, we have developed a mixture-of-experts model with a gated loss
function to train the CPAM in an end-to-end manner. We will present extensive
results to demonstrate the performances of our SPAS system using publicly
available image composition datasets.

</details>


### [151] [Distribution-Conditional Generation: From Class Distribution to Creative Generation](https://arxiv.org/pdf/2505.03667)
*Fu Feng, Yucheng Xie, Xu Yang, Jing Wang, Xin Geng*

Main category: cs.CV

TL;DR: DisTok introduces a method for creative text-to-image generation by conditioning on class distributions, enabling novel concept synthesis beyond training data.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of existing T2I models in generating truly novel, out-of-distribution concepts.

Method: Proposes Distribution-Conditional Generation and DisTok, an encoder-decoder framework for latent space mapping and iterative concept fusion.

Result: Achieves state-of-the-art performance with superior text-image alignment and human preference.

Conclusion: DisTok enables flexible, efficient token-level generation of creative concepts.

Abstract: Text-to-image (T2I) diffusion models are effective at producing semantically
aligned images, but their reliance on training data distributions limits their
ability to synthesize truly novel, out-of-distribution concepts. Existing
methods typically enhance creativity by combining pairs of known concepts,
yielding compositions that, while out-of-distribution, remain linguistically
describable and bounded within the existing semantic space. Inspired by the
soft probabilistic outputs of classifiers on ambiguous inputs, we propose
Distribution-Conditional Generation, a novel formulation that models creativity
as image synthesis conditioned on class distributions, enabling semantically
unconstrained creative generation. Building on this, we propose DisTok, an
encoder-decoder framework that maps class distributions into a latent space and
decodes them into tokens of creative concept. DisTok maintains a dynamic
concept pool and iteratively sampling and fusing concept pairs, enabling the
generation of tokens aligned with increasingly complex class distributions. To
enforce distributional consistency, latent vectors sampled from a Gaussian
prior are decoded into tokens and rendered into images, whose class
distributions-predicted by a vision-language model-supervise the alignment
between input distributions and the visual semantics of generated tokens. The
resulting tokens are added to the concept pool for subsequent composition.
Extensive experiments demonstrate that DisTok, by unifying
distribution-conditioned fusion and sampling-based synthesis, enables efficient
and flexible token-level generation, achieving state-of-the-art performance
with superior text-image alignment and human preference scores.

</details>


### [152] [CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting](https://arxiv.org/pdf/2505.03679)
*Huawei Sun, Bora Kunter Sahin, Georg Stettinger, Maximilian Bernhard, Matthias Schubert, Robert Wille*

Main category: cs.CV

TL;DR: A novel framework integrates radar data with camera images using a diffusion model and pseudo-masks to improve semantic segmentation in adverse weather.


<details>
  <summary>Details</summary>
Motivation: Camera sensors are vulnerable to adverse weather, while radar is robust but noisy. Combining both can enhance segmentation accuracy.

Method: Uses radar point features to create pseudo-masks with the Segment-Anything model, denoises them, and generates inpainted images for missing data.

Result: Improves camera-only segmentation by 2.63% and camera-radar fusion by 1.48% in mIoU on the Waterscenes dataset.

Conclusion: The approach effectively enhances semantic segmentation in adverse weather by fusing camera and radar data.

Abstract: Segmenting objects in an environment is a crucial task for autonomous driving
and robotics, as it enables a better understanding of the surroundings of each
agent. Although camera sensors provide rich visual details, they are vulnerable
to adverse weather conditions. In contrast, radar sensors remain robust under
such conditions, but often produce sparse and noisy data. Therefore, a
promising approach is to fuse information from both sensors. In this work, we
propose a novel framework to enhance camera-only baselines by integrating a
diffusion model into a camera-radar fusion architecture. We leverage radar
point features to create pseudo-masks using the Segment-Anything model,
treating the projected radar points as point prompts. Additionally, we propose
a noise reduction unit to denoise these pseudo-masks, which are further used to
generate inpainted images that complete the missing information in the original
images. Our method improves the camera-only segmentation baseline by 2.63% in
mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the
Waterscenes dataset. This demonstrates the effectiveness of our approach for
semantic segmentation using camera-radar fusion under adverse weather
conditions.

</details>


### [153] [UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction](https://arxiv.org/pdf/2503.15661)
*Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan A. Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M. Tamer Özsu, Aishwarya Agrawal, David Vazquez, Christopher Pal, Perouz Taslakian, Spandana Gella, Sai Rajeswar*

Main category: cs.CV

TL;DR: UI-Vision is a new benchmark for evaluating autonomous agents in desktop environments, addressing gaps in existing research by providing dense annotations and tasks for rigorous evaluation.


<details>
  <summary>Details</summary>
Motivation: Desktop environments are critical but underexplored due to data and licensing challenges. UI-Vision aims to fill this gap.

Method: UI-Vision offers dense annotations (bounding boxes, labels, action trajectories) across 83 apps and three tasks (Element Grounding, Layout Grounding, Action Prediction).

Result: Evaluation shows limitations in state-of-the-art models (e.g., UI-TARS-72B) in handling professional software and complex actions.

Conclusion: UI-Vision, released as open-source, aims to advance autonomous agent development for real-world desktop tasks.

Abstract: Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate
tasks like document editing and file management can greatly enhance computer
workflows. While existing research focuses on online settings, desktop
environments, critical for many professional and everyday tasks, remain
underexplored due to data collection challenges and licensing issues. We
introduce UI-Vision, the first comprehensive, license-permissive benchmark for
offline, fine-grained evaluation of computer use agents in real-world desktop
environments. Unlike online benchmarks, UI-Vision provides: (i) dense,
high-quality annotations of human demonstrations, including bounding boxes, UI
labels, and action trajectories (clicks, drags, and keyboard inputs) across 83
software applications, and (ii) three fine-to-coarse grained tasks-Element
Grounding, Layout Grounding, and Action Prediction-with well-defined metrics to
rigorously evaluate agents' performance in desktop environments. Our evaluation
reveals critical limitations in state-of-the-art models like UI-TARS-72B,
including issues with understanding professional software, spatial reasoning,
and complex actions like drag-and-drop. These findings highlight the challenges
in developing fully autonomous computer use agents. By releasing UI-Vision as
open-source, we aim to advance the development of more capable agents for
real-world desktop tasks.

</details>


### [154] [Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration](https://arxiv.org/pdf/2505.03692)
*Shiqi Li, Jihua Zhu, Yifan Xie, Naiwen Hu, Di Wang*

Main category: cs.CV

TL;DR: The paper introduces neural network models for pose graph construction and motion synchronization in multiview point cloud registration, improving reliability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Previous methods for pose graph construction and motion synchronization often relied on unreliable pruning or handcrafted loss functions, leading to inconsistent results.

Method: The authors propose two neural network models: one for identifying dependable pairs in pose graph construction using matching distance, and another for calculating absolute poses in motion synchronization using geometric distribution and a modified attention mechanism.

Result: Experiments on indoor and outdoor datasets show the approach's effectiveness and generalizability.

Conclusion: The proposed data-driven models outperform traditional methods, offering more reliable and flexible solutions for multiview registration.

Abstract: Multiview point cloud registration plays a crucial role in robotics,
automation, and computer vision fields. This paper concentrates on pose graph
construction and motion synchronization within multiview registration. Previous
methods for pose graph construction often pruned fully connected graphs or
constructed sparse graph using global feature aggregated from local
descriptors, which may not consistently yield reliable results. To identify
dependable pairs for pose graph construction, we design a network model that
extracts information from the matching distance between point cloud pairs. For
motion synchronization, we propose another neural network model to calculate
the absolute pose in a data-driven manner, rather than optimizing inaccurate
handcrafted loss functions. Our model takes into account geometric distribution
information and employs a modified attention mechanism to facilitate flexible
and reliable feature interaction. Experimental results on diverse indoor and
outdoor datasets confirm the effectiveness and generalizability of our
approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.

</details>


### [155] [The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation](https://arxiv.org/pdf/2504.11739)
*Bingjie Gao, Xinyu Gao, Xiaoxue Wu, Yujie Zhou, Yu Qiao, Li Niu, Xinyuan Chen, Yaohui Wang*

Main category: cs.CV

TL;DR: RAPO is a Retrieval-Augmented Prompt Optimization framework designed to improve Text-to-Video (T2V) generation by refining user prompts through dual optimization branches, enhancing video quality.


<details>
  <summary>Details</summary>
Motivation: The sensitivity of T2V models to input prompts and the lack of tailored guidance in prompt design necessitate a method to optimize prompts for better generative outcomes.

Method: RAPO uses two branches: one augments prompts with modifiers from a relational graph and aligns them via a fine-tuned LLM, while the other rewrites prompts using a pre-trained LLM with defined instructions.

Result: Experiments show RAPO enhances both static and dynamic aspects of generated videos, proving the importance of prompt optimization.

Conclusion: RAPO effectively addresses prompt design challenges in T2V generation, improving video quality through optimized prompts.

Abstract: The evolution of Text-to-video (T2V) generative models, trained on
large-scale datasets, has been marked by significant progress. However, the
sensitivity of T2V generative models to input prompts highlights the critical
role of prompt design in influencing generative outcomes. Prior research has
predominantly relied on Large Language Models (LLMs) to align user-provided
prompts with the distribution of training prompts, albeit without tailored
guidance encompassing prompt vocabulary and sentence structure nuances. To this
end, we introduce RAPO, a novel Retrieval-Augmented Prompt Optimization
framework. In order to address potential inaccuracies and ambiguous details
generated by LLM-generated prompts. RAPO refines the naive prompts through dual
optimization branches, selecting the superior prompt for T2V generation. The
first branch augments user prompts with diverse modifiers extracted from a
learned relational graph, refining them to align with the format of training
prompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive
prompt using a pre-trained LLM following a well-defined instruction set.
Extensive experiments demonstrate that RAPO can effectively enhance both the
static and dynamic dimensions of generated videos, demonstrating the
significance of prompt optimization for user-provided prompts.

</details>


### [156] [Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning](https://arxiv.org/pdf/2505.03703)
*François Role, Sébastien Meyer, Victor Amblard*

Main category: cs.CV

TL;DR: The paper addresses the modality gap in vision-language models (VLMs), proposing new measures and techniques to assess and reduce it, improving downstream tasks.


<details>
  <summary>Details</summary>
Motivation: VLMs exhibit a modality gap, causing misalignment between text and image embeddings, which harms tasks like retrieval and classification. Existing methods lack precision and practicality.

Method: The authors introduce spectral- and optimal transport-based methods to measure and reduce the modality gap.

Result: Experiments on multiple datasets and models show the techniques' effectiveness and positive impact on downstream tasks.

Conclusion: The proposed methods successfully address the modality gap, enhancing VLM performance for practical applications.

Abstract: Vision-language models (VLMs) allow to embed texts and images in a shared
representation space. However, it has been shown that these models are subject
to a modality gap phenomenon meaning there exists a clear separation between
the embeddings from one modality and another in the embedding space. While this
misalignment is detrimental for downstream tasks such as multimodal retrieval,
multimodal clustering or zero-shot classification, etc. no generic and
practical methods have so far been proposed to assess it precisely and even
reduce it. We therefore propose novel measures and effective techniques
(spectral- and optimal transport-based methods) to achieve this goal. Extensive
experiments conducted on several image-text datasets and models demonstrate
their effectiveness and beneficial effects on downstream tasks. Our code is
available at the URL provided in the paper's abstract.

</details>


### [157] [DISARM++: Beyond scanner-free harmonization](https://arxiv.org/pdf/2505.03715)
*Luca Caldera, Lara Cavinato, Alessio Cirone, Isabella Cama, Sara Garbarino, Raffaele Lodi, Fabrizio Tagliavini, Anna Nigri, Silvia De Francesco, Andrea Cappozzo, Michele Piana, Francesca Ieva*

Main category: cs.CV

TL;DR: A novel method for harmonizing T1-weighted MR images across scanners improves reliability and predictive accuracy without extensive preprocessing, validated in diverse cohorts and applications.


<details>
  <summary>Details</summary>
Motivation: Ensuring consistency in neuroimaging studies by harmonizing images across different scanners, addressing limitations of feature standardization and preprocessing errors.

Method: Direct image harmonization via two approaches: mapping to a scanner-free space or transforming into a specific scanner's domain, with strong generalization for unseen scanners.

Result: Outperforms state-of-the-art methods in brain age prediction (R2 = 0.60), AD classification (Accuracy = 0.86), and diagnosis prediction (AUC = 0.95), eliminating preprocessing needs.

Conclusion: The approach provides a robust, efficient solution for scanner-invariant neuroimaging, suitable for diverse applications and workflows.

Abstract: Harmonization of T1-weighted MR images across different scanners is crucial
for ensuring consistency in neuroimaging studies. This study introduces a novel
approach to direct image harmonization, moving beyond feature standardization
to ensure that extracted features remain inherently reliable for downstream
analysis. Our method enables image transfer in two ways: (1) mapping images to
a scanner-free space for uniform appearance across all scanners, and (2)
transforming images into the domain of a specific scanner used in model
training, embedding its unique characteristics. Our approach presents strong
generalization capability, even for unseen scanners not included in the
training phase. We validated our method using MR images from diverse cohorts,
including healthy controls, traveling subjects, and individuals with
Alzheimer's disease (AD). The model's effectiveness is tested in multiple
applications, such as brain age prediction (R2 = 0.60 \pm 0.05), biomarker
extraction, AD classification (Test Accuracy = 0.86 \pm 0.03), and diagnosis
prediction (AUC = 0.95). In all cases, our harmonization technique outperforms
state-of-the-art methods, showing improvements in both reliability and
predictive accuracy. Moreover, our approach eliminates the need for extensive
preprocessing steps, such as skull-stripping, which can introduce errors by
misclassifying brain and non-brain structures. This makes our method
particularly suitable for applications that require full-head analysis,
including research on head trauma and cranial deformities. Additionally, our
harmonization model does not require retraining for new datasets, allowing
smooth integration into various neuroimaging workflows. By ensuring
scanner-invariant image quality, our approach provides a robust and efficient
solution for improving neuroimaging studies across diverse settings. The code
is available at this link.

</details>


### [158] [Multi-Agent System for Comprehensive Soccer Understanding](https://arxiv.org/pdf/2505.03735)
*Jiayuan Rao, Zifeng Li, Haoning Wu, Ya Zhang, Yanfeng Wang, Weidi Xie*

Main category: cs.CV

TL;DR: A comprehensive framework for holistic soccer understanding is introduced, featuring SoccerWiki (a knowledge base), SoccerBench (a benchmark), SoccerAgent (a multi-agent system), and evaluations of MLLMs.


<details>
  <summary>Details</summary>
Motivation: To address the gap in AI-driven soccer understanding by moving beyond isolated tasks to a holistic approach.

Method: Developed SoccerWiki for knowledge-driven reasoning, SoccerBench for benchmarking, and SoccerAgent for collaborative reasoning.

Result: Achieved robust performance on SoccerBench, demonstrating the superiority of the proposed agentic system.

Conclusion: The framework advances soccer understanding, with all resources publicly available.

Abstract: Recent advancements in AI-driven soccer understanding have demonstrated rapid
progress, yet existing research predominantly focuses on isolated or narrow
tasks. To bridge this gap, we propose a comprehensive framework for holistic
soccer understanding. Specifically, we make the following contributions in this
paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer
knowledge base, integrating rich domain knowledge about players, teams,
referees, and venues to enable knowledge-driven reasoning; (ii) we present
SoccerBench, the largest and most comprehensive soccer-specific benchmark,
featuring around 10K standardized multimodal (text, image, video) multi-choice
QA pairs across 13 distinct understanding tasks, curated through automated
pipelines and manual verification; (iii) we introduce SoccerAgent, a novel
multi-agent system that decomposes complex soccer questions via collaborative
reasoning, leveraging domain expertise from SoccerWiki and achieving robust
performance; (iv) extensive evaluations and ablations that benchmark
state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our
proposed agentic system. All data and code are publicly available at:
https://jyrao.github.io/SoccerAgent/.

</details>


### [159] [Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware Learning](https://arxiv.org/pdf/2310.04306)
*Qing Zhu, Qirong Mao, Jialin Zhang, Xiaohua Huang, Wenming Zheng*

Main category: cs.CV

TL;DR: The paper proposes an uncertainty-aware learning (UAL) method for group-level emotion recognition (GER) to address uncertainties in unconstrained environments and inconsistent individual predictions.


<details>
  <summary>Details</summary>
Motivation: Existing GER methods ignore uncertainties like congestion and occlusion, and individual predictions can confuse the network due to lack of individual labels.

Method: UAL models individual uncertainty using stochastic Gaussian embeddings, assigns uncertainty-sensitive fusion weights, and includes an image enhancement module. A three-branch model (face, object, scene) uses proportional-weighted fusion.

Result: The method shows effectiveness and generalization across three databases.

Conclusion: UAL improves GER robustness by addressing uncertainties and inconsistent predictions.

Abstract: Group-level emotion recognition (GER) is an inseparable part of human
behavior analysis, aiming to recognize an overall emotion in a multi-person
scene. However, the existing methods are devoted to combing diverse emotion
cues while ignoring the inherent uncertainties under unconstrained
environments, such as congestion and occlusion occurring within a group.
Additionally, since only group-level labels are available, inconsistent emotion
predictions among individuals in one group can confuse the network. In this
paper, we propose an uncertainty-aware learning (UAL) method to extract more
robust representations for GER. By explicitly modeling the uncertainty of each
individual, we utilize stochastic embedding drawn from a Gaussian distribution
instead of deterministic point embedding. This representation captures the
probabilities of different emotions and generates diverse predictions through
this stochasticity during the inference stage. Furthermore,
uncertainty-sensitive scores are adaptively assigned as the fusion weights of
individuals' face within each group. Moreover, we develop an image enhancement
module to enhance the model's robustness against severe noise. The overall
three-branch model, encompassing face, object, and scene component, is guided
by a proportional-weighted fusion strategy and integrates the proposed
uncertainty-aware method to produce the final group-level output. Experimental
results demonstrate the effectiveness and generalization ability of our method
across three widely used databases.

</details>


### [160] [VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions](https://arxiv.org/pdf/2303.12675)
*Zeqing Xia, Bojun Xiong, Zhouhui Lian*

Main category: cs.CV

TL;DR: VecFontSDF is an end-to-end trainable method for synthesizing high-quality vector fonts using signed distance functions (SDFs), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on raster image generation, lacking direct vector font synthesis. VecFontSDF addresses this gap.

Method: Uses SDF-based implicit shape representation to model glyphs as shape primitives with parabolic curves, convertible to quadratic Bézier curves.

Result: Achieves high-quality results in vector font reconstruction, interpolation, and few-shot synthesis, surpassing state-of-the-art methods.

Conclusion: VecFontSDF effectively extends image generation methods to vector fonts, offering a robust solution for automated font design.

Abstract: Font design is of vital importance in the digital content design and modern
printing industry. Developing algorithms capable of automatically synthesizing
vector fonts can significantly facilitate the font design process. However,
existing methods mainly concentrate on raster image generation, and only a few
approaches can directly synthesize vector fonts. This paper proposes an
end-to-end trainable method, VecFontSDF, to reconstruct and synthesize
high-quality vector fonts using signed distance functions (SDFs). Specifically,
based on the proposed SDF-based implicit shape representation, VecFontSDF
learns to model each glyph as shape primitives enclosed by several parabolic
curves, which can be precisely converted to quadratic B\'ezier curves that are
widely used in vector font products. In this manner, most image generation
methods can be easily extended to synthesize vector fonts. Qualitative and
quantitative experiments conducted on a publicly-available dataset demonstrate
that our method obtains high-quality results on several tasks, including vector
font reconstruction, interpolation, and few-shot vector font synthesis,
markedly outperforming the state of the art. Our code and trained models are
available at https://xiazeqing.github.io/VecFontSDF.

</details>


### [161] [Semi-supervised Underwater Image Enhancement Using A Physics-Aware Triple-Stream Network](https://arxiv.org/pdf/2307.11470)
*Hao Qi, Shixuan Xu, Xinghui Dong*

Main category: cs.CV

TL;DR: A novel Physics-Aware Triple-Stream Network (PATS-UIENet) combines physics-based models and deep learning for underwater image enhancement, outperforming 16 baselines.


<details>
  <summary>Details</summary>
Motivation: Underwater images degrade due to water transmission, with prior-based methods being inflexible and deep learning methods suffering from insufficient data.

Method: PATS-UIENet uses three streams (D-Stream, B-Stream, A-Stream) to estimate degradation parameters of a revised Image Formation Model (IFM) and employs a semi-supervised learning framework.

Result: The method outperforms or matches 16 baselines across six testing sets in degradation estimation and image enhancement.

Conclusion: The approach effectively models degradation and learns diverse underwater scene characteristics, demonstrating superior performance.

Abstract: Underwater images normally suffer from degradation due to the transmission
medium of water bodies. Both traditional prior-based approaches and deep
learning-based methods have been used to address this problem. However, the
inflexible assumption of the former often impairs their effectiveness in
handling diverse underwater scenes, while the generalization of the latter to
unseen images is usually weakened by insufficient data. In this study, we
leverage both the physics-based Image Formation Model (IFM) and deep learning
techniques for Underwater Image Enhancement (UIE). To this end, we propose a
novel Physics-Aware Triple-Stream Underwater Image Enhancement Network, i.e.,
PATS-UIENet, which comprises a Direct Signal Transmission Estimation Steam
(D-Stream), a Backscatter Signal Transmission Estimation Steam (B-Stream) and
an Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE
task by explicitly estimating the degradation parameters of a revised IFM. We
also adopt an IFM-inspired semi-supervised learning framework, which exploits
both the labeled and unlabeled images, to address the issue of insufficient
data. To our knowledge, such a physics-aware deep network and the IFM-inspired
semi-supervised learning framework have not been used for the UIE task before.
Our method performs better than, or at least comparably to, sixteen baselines
across six testing sets in the degradation estimation and UIE tasks. These
promising results should be due to the fact that the proposed method can not
only model the degradation but also learn the characteristics of diverse
underwater scenes.

</details>


### [162] [Aligning Data Selection with Performance: Performance-driven Reinforcement Learning for Active Learning in Object Detection](https://arxiv.org/pdf/2310.08387)
*Zhixuan Liang, Xingyu Zeng, Rui Zhao, Ping Luo*

Main category: cs.CV

TL;DR: MGRAL introduces a reinforcement learning-based active learning method for object detection, optimizing sample selection using mAP as a reward signal.


<details>
  <summary>Details</summary>
Motivation: Existing methods for assessing data informativeness in active learning often misalign with task performance metrics like mAP in object detection.

Method: MGRAL uses a reinforcement learning agent (LSTM-based) to select informative batches, optimizing with policy gradient and mAP as reward. Fast look-up tables address computational costs.

Result: Evaluated on PASCAL VOC and MS COCO, MGRAL shows strong performance, setting a new standard for RL-based active learning in object detection.

Conclusion: MGRAL successfully aligns sample selection with mAP, offering a feasible and effective approach for active learning in object detection.

Abstract: Active learning strategies aim to train high-performance models with minimal
labeled data by selecting the most informative instances for labeling. However,
existing methods for assessing data informativeness often fail to align
directly with task model performance metrics, such as mean average precision
(mAP) in object detection. This paper introduces Mean-AP Guided Reinforced
Active Learning for Object Detection (MGRAL), a novel approach that leverages
the concept of expected model output changes as informativeness for deep
detection networks, directly optimizing the sampling strategy using mAP. MGRAL
employs a reinforcement learning agent based on LSTM architecture to
efficiently navigate the combinatorial challenge of batch sample selection and
the non-differentiable nature between performance and selected batches. The
agent optimizes selection using policy gradient with mAP improvement as the
reward signal. To address the computational intensity of mAP estimation with
unlabeled samples, we implement fast look-up tables, ensuring real-world
feasibility. We evaluate MGRAL on PASCAL VOC and MS COCO benchmarks across
various backbone architectures. Our approach demonstrates strong performance,
establishing a new paradigm in reinforcement learning-based active learning for
object detection.

</details>


### [163] [Paragraph-to-Image Generation with Information-Enriched Diffusion Model](https://arxiv.org/pdf/2311.14284)
*Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang*

Main category: cs.CV

TL;DR: ParaDiffusion improves text-to-image generation for long paragraphs by leveraging large language models and a curated dataset, outperforming existing models in alignment and visual quality.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with long paragraphs and complex scenes, lacking strong alignment and fidelity.

Method: Uses a large language model (Llama V2) to encode long text, fine-tuned with LORA for text-image alignment, and introduces the ParaImage dataset for training.

Result: Outperforms SD XL and DeepFloyd IF, with 15% and 45% improvements in visual appeal and text faithfulness, respectively.

Conclusion: ParaDiffusion advances long-text alignment in image generation, with code and dataset released for community research.

Abstract: Text-to-image (T2I) models have recently experienced rapid development,
achieving astonishing performance in terms of fidelity and textual alignment
capabilities. However, given a long paragraph (up to 512 words), these
generation models still struggle to achieve strong alignment and are unable to
generate images depicting complex scenes. In this paper, we introduce an
information-enriched diffusion model for paragraph-to-image generation task,
termed ParaDiffusion, which delves into the transference of the extensive
semantic comprehension capabilities of large language models to the task of
image generation. At its core is using a large language model (e.g., Llama V2)
to encode long-form text, followed by fine-tuning with LORA to alignthe
text-image feature spaces in the generation task. To facilitate the training of
long-text semantic alignment, we also curated a high-quality paragraph-image
pair dataset, namely ParaImage. This dataset contains a small amount of
high-quality, meticulously annotated data, and a large-scale synthetic dataset
with long text descriptions being generated using a vision-language model.
Experiments demonstrate that ParaDiffusion outperforms state-of-the-art models
(SD XL, DeepFloyd IF) on ViLG-300 and ParaPrompts, achieving up to 15% and 45%
human voting rate improvements for visual appeal and text faithfulness,
respectively. The code and dataset will be released to foster community
research on long-text alignment.

</details>


### [164] [3D-HGS: 3D Half-Gaussian Splatting](https://arxiv.org/pdf/2406.02720)
*Haolin Li, Jinyang Liu, Mario Sznaier, Octavia Camps*

Main category: cs.CV

TL;DR: 3D Half-Gaussian (3D-HGS) kernels improve 3D Gaussian Splatting (3D-GS) by addressing shape and color discontinuities, achieving top-tier rendering quality without speed loss.


<details>
  <summary>Details</summary>
Motivation: 3D-GS outperforms NeRFs in quality and speed but has limitations with shape and color discontinuities.

Method: Proposes 3D-HGS kernels as a plug-and-play solution to enhance 3D-GS.

Result: 3D-HGS improves rendering quality while maintaining speed, surpassing existing methods.

Conclusion: 3D-HGS is an effective solution for enhancing 3D-GS, offering superior rendering quality without sacrificing performance.

Abstract: Photo-realistic image rendering from 3D scene reconstruction has advanced
significantly with neural rendering techniques. Among these, 3D Gaussian
Splatting (3D-GS) outperforms Neural Radiance Fields (NeRFs) in quality and
speed but struggles with shape and color discontinuities. We propose 3D
Half-Gaussian (3D-HGS) kernels as a plug-and-play solution to address these
limitations. Our experiments show that 3D-HGS enhances existing 3D-GS methods,
achieving state-of-the-art rendering quality without compromising speed.

</details>


### [165] [Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph](https://arxiv.org/pdf/2406.07113)
*Sergey Linok, Tatiana Zemskova, Svetlana Ladanova, Roman Titkov, Dmitry Yudin, Maxim Monastyrny, Aleksei Valenkov*

Main category: cs.CV

TL;DR: BBQ is a modular approach for 3D object grounding using scene graphs and LLMs, outperforming existing methods on complex queries and spatial relations.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based methods fail with ambiguous descriptions requiring object relations, prompting the need for a more robust solution.

Method: BBQ constructs 3D scene graphs with spatial edges, uses DINO for object-centric maps, and employs raycasting with a 2D vision-language model.

Result: BBQ leads in open-vocabulary 3D segmentation and improves grounding for complex queries on benchmarks like Sr3D+ and ScanRefer.

Conclusion: BBQ's design enables fast processing and effective robotics applications, with code made publicly available.

Abstract: Locating objects described in natural language presents a significant
challenge for autonomous agents. Existing CLIP-based open-vocabulary methods
successfully perform 3D object grounding with simple (bare) queries, but cannot
cope with ambiguous descriptions that demand an understanding of object
relations. To tackle this problem, we propose a modular approach called BBQ
(Beyond Bare Queries), which constructs 3D scene graph representation with
metric and semantic spatial edges and utilizes a large language model as a
human-to-agent interface through our deductive scene reasoning algorithm. BBQ
employs robust DINO-powered associations to construct 3D object-centric map and
an advanced raycasting algorithm with a 2D vision-language model to describe
them as graph nodes. On the Replica and ScanNet datasets, we have demonstrated
that BBQ takes a leading place in open-vocabulary 3D semantic segmentation
compared to other zero-shot methods. Also, we show that leveraging spatial
relations is especially effective for scenes containing multiple entities of
the same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks,
our deductive approach demonstrates a significant improvement, enabling objects
grounding by complex queries compared to other state-of-the-art methods. The
combination of our design choices and software implementation has resulted in
significant data processing speed in experiments on the robot on-board
computer. This promising performance enables the application of our approach in
intelligent robotics projects. We made the code publicly available at
https://linukc.github.io/BeyondBareQueries/.

</details>


### [166] [SMORE: Simultaneous Map and Object REconstruction](https://arxiv.org/pdf/2406.13896)
*Nathaniel Chodosh, Anish Madan, Simon Lucey, Deva Ramanan*

Main category: cs.CV

TL;DR: A method for dynamic surface reconstruction of urban scenes from LiDAR, decomposing scenes into rigidly-moving objects and background using neural surfaces and optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on small-scale objects or treat moving objects as outliers, lacking a holistic approach for large-scale dynamic scenes.

Method: Global optimization over neural surfaces, ego poses, and object poses, minimizing 3D point-to-surface error via coordinate descent. Uses off-the-shelf methods for registration and surface reconstruction.

Result: Improves dynamic reconstruction accuracy by an order of magnitude, with applications in auto-labeling and ground truth generation.

Conclusion: The method advances dynamic scene reconstruction and offers utility in labeling and annotation tasks.

Abstract: We present a method for dynamic surface reconstruction of large-scale urban
scenes from LiDAR. Depth-based reconstructions tend to focus on small-scale
objects or large-scale SLAM reconstructions that treat moving objects as
outliers. We take a holistic perspective and optimize a compositional model of
a dynamic scene that decomposes the world into rigidly-moving objects and the
background. To achieve this, we take inspiration from recent novel view
synthesis methods and frame the reconstruction problem as a global optimization
over neural surfaces, ego poses, and object poses, which minimizes the error
between composed spacetime surfaces and input LiDAR scans. In contrast to view
synthesis methods, which typically minimize 2D errors with gradient descent, we
minimize a 3D point-to-surface error by coordinate descent, which we decompose
into registration and surface reconstruction steps. Each step can be handled
well by off-the-shelf methods without any re-training. We analyze the surface
reconstruction step for rolling-shutter LiDARs, and show that deskewing
operations common in continuous time SLAM can be applied to dynamic objects as
well, improving results over prior art by an order of magnitude. Beyond
pursuing dynamic reconstruction as a goal in and of itself, we propose that
such a system can be used to auto-label partially annotated sequences and
produce ground truth annotation for hard-to-label problems such as depth
completion and scene flow. Please see https://anishmadan23.github.io/smore/ for
more visual results.

</details>


### [167] [Uncertainty-Guided Self-Questioning and Answering for Video-Language Alignment](https://arxiv.org/pdf/2410.02768)
*Jin Chen, Kaijing Ma, Haojian Huang, Han Fang, Hao Sun, Mehdi Hosseinzadeh, Zhe Liu*

Main category: cs.CV

TL;DR: BoViLA is a self-training framework for video-language alignment, using LLM-based self-questioning and EDL for uncertainty filtering to improve VideoQA performance.


<details>
  <summary>Details</summary>
Motivation: Annotating video-text pairs is costly and often underutilizes video content, limiting VideoQA performance.

Method: Proposes BoViLA, a self-training method with LLM-based self-questioning and EDL for uncertainty filtering.

Result: Outperforms state-of-the-art methods on five VideoQA benchmarks.

Conclusion: BoViLA effectively improves modality alignment and VideoQA performance, with potential for broader applications.

Abstract: The development of multi-modal models has been rapidly advancing, with some
demonstrating remarkable capabilities. However, annotating video-text pairs
remains expensive and insufficient. Take video question answering (VideoQA)
tasks as an example, human annotated questions and answers often cover only
part of the video, since the corresponding text is often short and monotonous,
leading to underutilization of video. To address this, we propose a
Bootstrapping Video-Language Alignment framework (BoViLA), a self-training
method that augments question samples during training process through LLM-based
self-questioning and answering, which help model exploit video information and
the internal knowledge of LLMs more thoroughly to improve modality alignment.
However, low-quality self-generated questions may instead contaminate the
performance, especially in the early stages of training, as we have observed in
our experiments. To filter bad self-generated questions, we introduce
Evidential Deep Learning (EDL) to estimate uncertainty and assess the quality
of self-generated questions by evaluating the modality alignment within the
context. To the best of our knowledge, this work is the first to explore
LLM-based self-training frameworks for modality alignment. We evaluate BoViLA
on five strong VideoQA benchmarks, where it outperforms several
state-of-the-art methods and demonstrate its effectiveness and generality.
Additionally, we provide extensive analyses of the self-training framework and
the EDL-based uncertainty filtering mechanism. The code will be made available.

</details>


### [168] [CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality](https://arxiv.org/pdf/2411.02179)
*Yiqin Zhao, Mallesham Dasari, Tian Guo*

Main category: cs.CV

TL;DR: CleAR is a generative lighting estimation system for mobile AR that uses a two-step pipeline and real-time refinement to produce high-quality, diverse 360° HDR environment maps, outperforming state-of-the-art methods in accuracy, speed, and robustness.


<details>
  <summary>Details</summary>
Motivation: High-quality lighting is crucial for immersive mobile AR, but device limitations like low camera FoV and dynamic range hinder accurate estimation. Generative AI offers a solution, but content quality and slow inference remain challenges.

Method: CleAR employs a two-step generation pipeline guided by AR context data and includes a real-time refinement component for robustness under varying lighting conditions.

Result: CleAR improves virtual object rendering accuracy by 51-56% and achieves comparable or better quality in 3.2 seconds, 110X faster than existing methods.

Conclusion: CleAR effectively addresses the limitations of generative models for lighting estimation, delivering superior performance in accuracy, speed, and user-rated quality for mobile AR.

Abstract: High-quality environment lighting is essential for creating immersive mobile
augmented reality (AR) experiences. However, achieving visually coherent
estimation for mobile AR is challenging due to several key limitations in AR
device sensing capabilities, including low camera FoV and limited pixel dynamic
ranges. Recent advancements in generative AI, which can generate high-quality
images from different types of prompts, including texts and images, present a
potential solution for high-quality lighting estimation. Still, to effectively
use generative image diffusion models, we must address two key limitations of
content quality and slow inference. In this work, we design and implement a
generative lighting estimation system called CleAR that can produce
high-quality, diverse environment maps in the format of 360{\deg} HDR images.
Specifically, we design a two-step generation pipeline guided by AR environment
context data to ensure the output aligns with the physical environment's visual
context and color appearance. To improve the estimation robustness under
different lighting conditions, we design a real-time refinement component to
adjust lighting estimation results on AR devices. Through a combination of
quantitative and qualitative evaluations, we show that CleAR outperforms
state-of-the-art lighting estimation methods on both estimation accuracy,
latency, and robustness, and is rated by 31 participants as producing better
renderings for most virtual objects. For example, CleAR achieves 51% to 56%
accuracy improvement on virtual object renderings across objects of three
distinctive types of materials and reflective properties. CleAR produces
lighting estimates of comparable or better quality in just 3.2 seconds -- over
110X faster than state-of-the-art methods.

</details>


### [169] [Decoupling Fine Detail and Global Geometry for Compressed Depth Map Super-Resolution](https://arxiv.org/pdf/2411.03239)
*Huan Zheng, Wencheng Han, Jianbing Shen*

Main category: cs.CV

TL;DR: GDNet is a novel framework for compressed depth map super-resolution, addressing challenges of uniform depth representation and noise by decoupling global and detailed geometric feature handling.


<details>
  <summary>Details</summary>
Motivation: Limitations of consumer-grade depth cameras and bandwidth restrictions during data transmission necessitate high-quality depth map recovery from compressed sources.

Method: GDNet uses a fine geometry detail encoder (FGDE) for detail aggregation and a global geometry encoder (GGE) for noise suppression and global structure extraction.

Result: GDNet outperforms current methods in geometric consistency and detail recovery, winning 1st place in the ECCV 2024 AIM Challenge.

Conclusion: GDNet effectively addresses compression challenges, offering superior performance in depth map super-resolution.

Abstract: Recovering high-quality depth maps from compressed sources has gained
significant attention due to the limitations of consumer-grade depth cameras
and the bandwidth restrictions during data transmission. However, current
methods still suffer from two challenges. First, bit-depth compression produces
a uniform depth representation in regions with subtle variations, hindering the
recovery of detailed information. Second, densely distributed random noise
reduces the accuracy of estimating the global geometric structure of the scene.
To address these challenges, we propose a novel framework, termed
geometry-decoupled network (GDNet), for compressed depth map super-resolution
that decouples the high-quality depth map reconstruction process by handling
global and detailed geometric features separately. To be specific, we propose
the fine geometry detail encoder (FGDE), which is designed to aggregate fine
geometry details in high-resolution low-level image features while
simultaneously enriching them with complementary information from
low-resolution context-level image features. In addition, we develop the global
geometry encoder (GGE) that aims at suppressing noise and extracting global
geometric information effectively via constructing compact feature
representation in a low-rank space. We conduct experiments on multiple
benchmark datasets, demonstrating that our GDNet significantly outperforms
current methods in terms of geometric consistency and detail recovery. In the
ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st
place award. Our codes are available at: https://github.com/Ian0926/GDNet.

</details>


### [170] [About Time: Advances, Challenges, and Outlooks of Action Understanding](https://arxiv.org/pdf/2411.15106)
*Alexandros Stergiou, Ronald Poppe*

Main category: cs.CV

TL;DR: A survey on advances in video action understanding, covering tasks, datasets, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To review progress in video action understanding, driven by larger datasets and computational power, and to address current challenges.

Method: Comprehensive review of uni- and multi-modal action understanding, focusing on tasks, datasets, and seminal works. Categorizes tasks into recognition, prediction, and forecasting.

Result: Identifies challenges in action modeling and video representation, and highlights recent advances in the field.

Conclusion: Outlines future directions to overcome current limitations in video action understanding.

Abstract: We have witnessed impressive advances in video action understanding.
Increased dataset sizes, variability, and computation availability have enabled
leaps in performance and task diversification. Current systems can provide
coarse- and fine-grained descriptions of video scenes, extract segments
corresponding to queries, synthesize unobserved parts of videos, and predict
context across multiple modalities. This survey comprehensively reviews
advances in uni- and multi-modal action understanding across a range of tasks.
We focus on prevalent challenges, overview widely adopted datasets, and survey
seminal works with an emphasis on recent advances. We broadly distinguish
between three temporal scopes: (1) recognition tasks of actions observed in
full, (2) prediction tasks for ongoing partially observed actions, and (3)
forecasting tasks for subsequent unobserved action(s). This division allows us
to identify specific action modeling and video representation challenges.
Finally, we outline future directions to address current shortcomings.

</details>


### [171] [AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers](https://arxiv.org/pdf/2411.18673)
*Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David B. Lindell, Sergey Tulyakov*

Main category: cs.CV

TL;DR: The paper introduces AC3D, a state-of-the-art model for precise 3D camera control in text-to-video generation, improving training efficiency and visual quality by analyzing low-frequency motion, optimizing pose conditioning, and using a curated dataset.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video models struggle with imprecise camera control and degraded video quality. The authors aim to address these issues by analyzing camera motion fundamentals and optimizing model architecture.

Method: The study identifies low-frequency nature of camera-induced motion, optimizes pose conditioning schedules, limits camera conditioning to specific layers, and uses a curated dataset to distinguish camera and scene motion.

Result: AC3D achieves a 4x reduction in training parameters, faster training, 10% higher visual quality, and improved dynamics in pose-conditioned videos.

Conclusion: AC3D sets a new benchmark for generative video modeling with precise 3D camera control, combining insights from motion analysis, architecture optimization, and dataset curation.

Abstract: Numerous works have recently integrated 3D camera control into foundational
text-to-video models, but the resulting camera control is often imprecise, and
video generation quality suffers. In this work, we analyze camera motion from a
first principles perspective, uncovering insights that enable precise 3D camera
manipulation without compromising synthesis quality. First, we determine that
motion induced by camera movements in videos is low-frequency in nature. This
motivates us to adjust train and test pose conditioning schedules, accelerating
training convergence while improving visual and motion quality. Then, by
probing the representations of an unconditional video diffusion transformer, we
observe that they implicitly perform camera pose estimation under the hood, and
only a sub-portion of their layers contain the camera information. This
suggested us to limit the injection of camera conditioning to a subset of the
architecture to prevent interference with other video features, leading to a 4x
reduction of training parameters, improved training speed, and 10% higher
visual quality. Finally, we complement the typical dataset for camera control
learning with a curated dataset of 20K diverse, dynamic videos with stationary
cameras. This helps the model distinguish between camera and scene motion and
improves the dynamics of generated pose-conditioned videos. We compound these
findings to design the Advanced 3D Camera Control (AC3D) architecture, the new
state-of-the-art model for generative video modeling with camera control.

</details>


### [172] [HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing](https://arxiv.org/pdf/2412.04280)
*Jinbin Bai, Wei Chow, Ling Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, Shuicheng Yan*

Main category: cs.CV

TL;DR: HumanEdit is a high-quality dataset for instruction-guided image editing, featuring human feedback, diverse editing types, and high-resolution images.


<details>
  <summary>Details</summary>
Motivation: Address the gap in aligning image editing datasets with human preferences by incorporating human feedback and diverse instructions.

Method: Constructed using human annotators and administrators, involving 5,751 images and 2,500+ hours of effort across four stages. Includes six editing types and detailed instructions.

Result: A versatile benchmark dataset with masks and detailed instructions, supporting diverse image editing tasks.

Conclusion: HumanEdit advances research in instructional image editing and is publicly available for future benchmarks.

Abstract: We present HumanEdit, a high-quality, human-rewarded dataset specifically
designed for instruction-guided image editing, enabling precise and diverse
image manipulations through open-form language instructions. Previous
large-scale editing datasets often incorporate minimal human feedback, leading
to challenges in aligning datasets with human preferences. HumanEdit bridges
this gap by employing human annotators to construct data pairs and
administrators to provide feedback. With meticulously curation, HumanEdit
comprises 5,751 images and requires more than 2,500 hours of human effort
across four stages, ensuring both accuracy and reliability for a wide range of
image editing tasks. The dataset includes six distinct types of editing
instructions: Action, Add, Counting, Relation, Remove, and Replace,
encompassing a broad spectrum of real-world scenarios. All images in the
dataset are accompanied by masks, and for a subset of the data, we ensure that
the instructions are sufficiently detailed to support mask-free editing.
Furthermore, HumanEdit offers comprehensive diversity and high-resolution $1024
\times 1024$ content sourced from various domains, setting a new versatile
benchmark for instructional image editing datasets. With the aim of advancing
future research and establishing evaluation benchmarks in the field of image
editing, we release HumanEdit at
https://huggingface.co/datasets/BryanW/HumanEdit.

</details>


### [173] [A Cognitive Paradigm Approach to Probe the Perception-Reasoning Interface in VLMs](https://arxiv.org/pdf/2501.13620)
*Mohit Vaishnav, Tanel Tammet*

Main category: cs.CV

TL;DR: The paper introduces a structured evaluation framework for Vision-Language Models (VLMs) using cognitive science-inspired tasks to dissect visual reasoning. It proposes three paradigms (DVRL, DRL, CA) to evaluate reasoning, with CA achieving SOTA performance by decoupling perception from reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand how VLMs integrate visual perception with abstract thought and improve their reasoning capabilities by addressing perceptual bottlenecks.

Method: Three evaluation paradigms (DVRL, DRL, CA) are introduced, varying cognitive load and processing stages. CA uses task-agnostic textual descriptions to isolate reasoning from perception.

Result: CA achieves SOTA performance on benchmarks like Bongard-OpenWorld and Winoground, showing reasoning improves when perceptual challenges are mitigated.

Conclusion: Decoupling perception from reasoning via rich descriptions is a promising direction for robust visual intelligence, and the framework serves as a diagnostic tool.

Abstract: A fundamental challenge in artificial intelligence involves understanding the
cognitive mechanisms underlying visual reasoning in sophisticated models like
Vision-Language Models (VLMs). How do these models integrate visual perception
with abstract thought, especially when reasoning across multiple images or
requiring fine-grained compositional understanding? Drawing inspiration from
cognitive science, this paper introduces a structured evaluation framework
using diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to
dissect the perception-reasoning interface in VLMs. We propose three distinct
evaluation paradigms, mirroring human problem-solving strategies: Direct Visual
Rule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule
extraction and application), and Componential Analysis (CA; analytical
decomposition via task-agnostic textual descriptions). These paradigms
systematically vary cognitive load and probe processing stages. Notably, CA
enables multi-image reasoning evaluation even for single-image architectures
and isolates reasoning from perception by operating on textual descriptions.
Applying this framework, we demonstrate that CA, leveraging powerful language
models for reasoning over rich, independently generated descriptions, achieves
new state-of-the-art (SOTA) performance on challenging benchmarks including
Bongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm
reasoning improves significantly when perceptual challenges are mitigated,
revealing a critical perception bottleneck. Our framework provides a valuable
diagnostic tool and suggests that decoupling perception (via rich,
task-agnostic description) from reasoning is a promising direction for robust
and general visual intelligence.

</details>


### [174] [Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data](https://arxiv.org/pdf/2501.15326)
*Jiajie Li, Brian R Quaranto, Chenhui Xu, Ishan Mishra, Ruiyang Qin, Dancheng Liu, Peter C W Kim, Jinjun Xiong*

Main category: cs.CV

TL;DR: RASO is a foundation model for recognizing surgical objects in images and videos, using weakly-supervised learning to reduce manual annotations. It outperforms benchmarks in zero-shot and supervised tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recognizing diverse surgical objects across procedures without extensive manual annotation.

Method: Uses a weakly-supervised learning framework to generate tag-image-text pairs from unannotated surgical lecture videos, creating a scalable data pipeline.

Result: Achieves significant improvements (2.9-10.6 mAP) on surgical benchmarks and surpasses state-of-the-art models.

Conclusion: RASO demonstrates robust open-set recognition capabilities and scalability, with publicly available resources.

Abstract: We present RASO, a foundation model designed to Recognize Any Surgical
Object, offering robust open-set recognition capabilities across a broad range
of surgical procedures and object classes, in both surgical images and videos.
RASO leverages a novel weakly-supervised learning framework that generates
tag-image-text pairs automatically from large-scale unannotated surgical
lecture videos, significantly reducing the need for manual annotations. Our
scalable data generation pipeline gathers 2,200 surgical procedures and
produces 3.6 million tag annotations across 2,066 unique surgical tags. Our
experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP,
and 7.2 mAP on four standard surgical benchmarks, respectively, in zero-shot
settings, and surpasses state-of-the-art models in supervised surgical action
recognition tasks. Code, model, and demo are available at
https://ntlm1686.github.io/raso.

</details>


### [175] [Deformable Beta Splatting](https://arxiv.org/pdf/2501.18630)
*Rong Liu, Dylan Sun, Meida Chen, Yue Wang, Andrew Feng*

Main category: cs.CV

TL;DR: DBS improves 3DGS by using deformable Beta Kernels for better geometry and color representation, achieving higher fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: 3DGS's limitations in capturing complex geometries and diverse colors due to Gaussian kernels and low-order SH motivate the development of DBS.

Method: DBS replaces Gaussian kernels with deformable Beta Kernels for geometry and extends them to color encoding, enhancing detail and efficiency.

Result: DBS achieves superior visual quality with 45% fewer parameters and 1.5x faster rendering than 3DGS-MCMC.

Conclusion: DBS outperforms 3DGS in real-time radiance field rendering, offering better performance and efficiency.

Abstract: 3D Gaussian Splatting (3DGS) has advanced radiance field reconstruction by
enabling real-time rendering. However, its reliance on Gaussian kernels for
geometry and low-order Spherical Harmonics (SH) for color encoding limits its
ability to capture complex geometries and diverse colors. We introduce
Deformable Beta Splatting (DBS), a deformable and compact approach that
enhances both geometry and color representation. DBS replaces Gaussian kernels
with deformable Beta Kernels, which offer bounded support and adaptive
frequency control to capture fine geometric details with higher fidelity while
achieving better memory efficiency. In addition, we extended the Beta Kernel to
color encoding, which facilitates improved representation of diffuse and
specular components, yielding superior results compared to SH-based methods.
Furthermore, Unlike prior densification techniques that depend on Gaussian
properties, we mathematically prove that adjusting regularized opacity alone
ensures distribution-preserved Markov chain Monte Carlo (MCMC), independent of
the splatting kernel type. Experimental results demonstrate that DBS achieves
state-of-the-art visual quality while utilizing only 45% of the parameters and
rendering 1.5x faster than 3DGS-MCMC, highlighting the superior performance of
DBS for real-time radiance field rendering. Interactive demonstrations and
source code are available on our project website:
https://rongliu-leo.github.io/beta-splatting/.

</details>


### [176] [Novel computational workflows for natural and biomedical image processing based on hypercomplex algebras](https://arxiv.org/pdf/2502.07758)
*Nektarios A. Valous, Eckhard Hitzer, Dragoş Duşe, Rodrigo Rojas Moraleda, Ferdinand Popp, Meggy Suarez-Carmona, Anna Berthel, Ismini Papageorgiou, Carlo Fremd, Alexander Rölle, Christina C. Westhoff, Bénédicte Lenoir, Niels Halama, Inka Zörnig, Dirk Jäger*

Main category: cs.CV

TL;DR: The paper introduces hypercomplex image processing using quaternions and orthogonal planes split for tasks like re-colorization, de-colorization, and contrast enhancement in natural/biomedical images, achieving comparable or better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: To extend conventional image processing techniques by unifying algebraic and geometric principles in the hypercomplex domain for versatile applications in natural and biomedical imaging.

Method: Leverages quaternions and the two-dimensional orthogonal planes split framework for tasks like re-colorization, de-colorization, and contrast enhancement, using basic arithmetic and matrix operations.

Result: Achieves comparable or better results than existing methods, particularly in well-known tasks, and demonstrates versatility across image processing and biomedical applications.

Conclusion: The work highlights the potential of robust theoretical frameworks in hypercomplex domains for practical, computationally accessible solutions in image processing and biomedical applications.

Abstract: Hypercomplex image processing extends conventional techniques in a unified
paradigm encompassing algebraic and geometric principles. This work leverages
quaternions and the two-dimensional orthogonal planes split framework
(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D
planes) for natural/biomedical image analysis through the following
computational workflows and outcomes: natural/biomedical image re-colorization,
natural image de-colorization, natural/biomedical image contrast enhancement,
computational re-staining and stain separation in histological images, and
performance gains in machine/deep learning pipelines for histological images.
The workflows are analyzed separately for natural and biomedical images to
showcase the effectiveness of the proposed approaches. The proposed workflows
can regulate color appearance (e.g. with alternative renditions and grayscale
conversion) and image contrast, be part of automated image processing pipelines
(e.g. isolating stain components, boosting learning models), and assist in
digital pathology applications (e.g. enhancing biomarker visibility, enabling
colorblind-friendly renditions). Employing only basic arithmetic and matrix
operations, this work offers a computationally accessible methodology - in the
hypercomplex domain - that showcases versatility and consistency across image
processing tasks and a range of computer vision and biomedical applications.
The proposed non-data-driven methods achieve comparable or better results
(particularly in cases involving well-known methods) to those reported in the
literature, showcasing the potential of robust theoretical frameworks with
practical effectiveness. Results, methods, and limitations are detailed
alongside discussion of promising extensions, emphasizing the potential of
feature-rich mathematical/computational frameworks for natural and biomedical
images.

</details>


### [177] [Instance Segmentation of Scene Sketches Using Natural Image Priors](https://arxiv.org/pdf/2502.09608)
*Mia Tang, Yael Vinker, Chuan Yan, Lvmin Zhang, Maneesh Agrawala*

Main category: cs.CV

TL;DR: InkLayer is a method for instance segmentation of sketches, adapting image segmentation models to handle sparse and varied sketch styles, using depth cues and synthetic data for robustness.


<details>
  <summary>Details</summary>
Motivation: Sketches pose unique challenges for segmentation due to their sparsity and style variations, requiring specialized tools for editing tasks.

Method: Adapts state-of-the-art image segmentation models with class-agnostic fine-tuning, refines masks using depth cues, and organizes sketches into sorted layers with inpainting.

Result: Demonstrated robustness on InkScenes, a synthetic dataset with diverse sketch styles, enabling advanced sketch editing.

Conclusion: InkLayer effectively addresses sketch segmentation challenges, offering a practical solution for sketch editing applications.

Abstract: Sketch segmentation involves grouping pixels within a sketch that belong to
the same object or instance. It serves as a valuable tool for sketch editing
tasks, such as moving, scaling, or removing specific components. While image
segmentation models have demonstrated remarkable capabilities in recent years,
sketches present unique challenges for these models due to their sparse nature
and wide variation in styles. We introduce InkLayer, a method for instance
segmentation of raster scene sketches. Our approach adapts state-of-the-art
image segmentation and object detection models to the sketch domain by
employing class-agnostic fine-tuning and refining segmentation masks using
depth cues. Furthermore, our method organizes sketches into sorted layers,
where occluded instances are inpainted, enabling advanced sketch editing
applications. As existing datasets in this domain lack variation in sketch
styles, we construct a synthetic scene sketch segmentation dataset, InkScenes,
featuring sketches with diverse brush strokes and varying levels of detail. We
use this dataset to demonstrate the robustness of our approach.

</details>


### [178] [CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement](https://arxiv.org/pdf/2502.17648)
*Lei Cheng, Lihao Guo, Tianya Zhang, Tam Bang, Austin Harris, Mustafa Hajij, Mina Sartipi, Siyang Cao*

Main category: cs.CV

TL;DR: CalibRefine is a fully automatic, targetless, and online LiDAR-camera calibration framework that outperforms existing methods with minimal human input.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-camera calibration methods rely on manual targets or preprocessing, limiting scalability and adaptability in real-world settings.

Method: The approach involves four stages: common feature discrimination, coarse homography-based calibration, iterative refinement, and attention-based refinement using a Vision Transformer.

Result: Experiments show CalibRefine achieves high-precision calibration, outperforming state-of-the-art targetless methods and matching manual baselines.

Conclusion: Robust feature matching and refinement enable reliable sensor alignment in complex conditions without ground-truth or preprocessing.

Abstract: Accurate multi-sensor calibration is essential for deploying robust
perception systems in applications such as autonomous driving and intelligent
transportation. Existing LiDAR-camera calibration methods often rely on
manually placed targets, preliminary parameter estimates, or intensive data
preprocessing, limiting their scalability and adaptability in real-world
settings. In this work, we propose a fully automatic, targetless, and online
calibration framework, CalibRefine, which directly processes raw LiDAR point
clouds and camera images. Our approach is divided into four stages: (1) a
Common Feature Discriminator that leverages relative spatial positions, visual
appearance embeddings, and semantic class cues to identify and generate
reliable LiDAR-camera correspondences, (2) a coarse homography-based
calibration that uses the matched feature correspondences to estimate an
initial transformation between the LiDAR and camera frames, serving as the
foundation for further refinement, (3) an iterative refinement to incrementally
improve alignment as additional data frames become available, and (4) an
attention-based refinement that addresses non-planar distortions by leveraging
a Vision Transformer and cross-attention mechanisms. Extensive experiments on
two urban traffic datasets demonstrate that CalibRefine achieves high-precision
calibration with minimal human input, outperforming state-of-the-art targetless
methods and matching or surpassing manually tuned baselines. Our results show
that robust object-level feature matching, combined with iterative refinement
and self-supervised attention-based refinement, enables reliable sensor
alignment in complex real-world conditions without ground-truth matrices or
elaborate preprocessing. Code is available at
https://github.com/radar-lab/Lidar_Camera_Automatic_Calibration

</details>


### [179] [Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers](https://arxiv.org/pdf/2503.03307)
*Ji Zhao, Banglei Guan, Zibin Liu, Laurent Kneip*

Main category: cs.CV

TL;DR: The paper proposes solvers to estimate full-DoF egomotion (rotational and translational velocities) for event cameras using event manifolds from line segments, without relying on IMU data or motion priors.


<details>
  <summary>Details</summary>
Motivation: Current sparse geometric solvers for event cameras assume known rotational displacements (e.g., from IMUs), limiting them to estimating only translational motion. This work addresses the challenge of recovering full-DoF motion parameters.

Method: The method uses event manifolds from line segments, formulating the problem via incidence or coplanarity relations. It employs Adam optimization with a first-order rotation approximation for efficient initialization.

Result: Experiments on synthetic and real-world data confirm the method's effectiveness in estimating full-DoF motion without additional sensors or priors.

Conclusion: The work successfully demonstrates the feasibility of recovering full-DoF egomotion for event cameras, offering a unified framework for both angular and linear velocities.

Abstract: For event cameras, current sparse geometric solvers for egomotion estimation
assume that the rotational displacements are known, such as those provided by
an IMU. Thus, they can only recover the translational motion parameters.
Recovering full-DoF motion parameters using a sparse geometric solver is a more
challenging task, and has not yet been investigated. In this paper, we propose
several solvers to estimate both rotational and translational velocities within
a unified framework. Our method leverages event manifolds induced by line
segments. The problem formulations are based on either an incidence relation
for lines or a novel coplanarity relation for normal vectors. We demonstrate
the possibility of recovering full-DoF egomotion parameters for both angular
and linear velocities without requiring extra sensor measurements or motion
priors. To achieve efficient optimization, we exploit the Adam framework with a
first-order approximation of rotations for quick initialization. Experiments on
both synthetic and real-world data demonstrate the effectiveness of our method.
The code is available at https://github.com/jizhaox/relpose-event.

</details>


### [180] [Cobra: Efficient Line Art COlorization with BRoAder References](https://arxiv.org/pdf/2504.12240)
*Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan*

Main category: cs.CV

TL;DR: Cobra is an efficient method for reference-based line art colorization, addressing challenges like extensive reference handling and slow inference with a Causal Sparse DiT architecture.


<details>
  <summary>Details</summary>
Motivation: The comic industry needs accurate, efficient, and flexible line art colorization, but current diffusion models struggle with reference handling and speed.

Method: Cobra uses a Causal Sparse DiT architecture with positional encodings, causal sparse attention, and Key-Value Cache to manage references and ensure color consistency.

Result: Cobra achieves high-quality colorization with over 200 references, improving speed and interactivity.

Conclusion: Cobra meets industrial demands for efficient and accurate line art colorization, with released codes and models.

Abstract: The comic production industry requires reference-based line art colorization
with high accuracy, efficiency, contextual consistency, and flexible control. A
comic page often involves diverse characters, objects, and backgrounds, which
complicates the coloring process. Despite advancements in diffusion models for
image generation, their application in line art colorization remains limited,
facing challenges related to handling extensive reference images,
time-consuming inference, and flexible control. We investigate the necessity of
extensive contextual image guidance on the quality of line art colorization. To
address these challenges, we introduce Cobra, an efficient and versatile method
that supports color hints and utilizes over 200 reference images while
maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,
which leverages specially designed positional encodings, causal sparse
attention, and Key-Value Cache to effectively manage long-context references
and ensure color identity consistency. Results demonstrate that Cobra achieves
accurate line art colorization through extensive contextual reference,
significantly enhancing inference speed and interactivity, thereby meeting
critical industrial demands. We release our codes and models on our project
page: https://zhuang2002.github.io/Cobra/.

</details>


### [181] [Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization](https://arxiv.org/pdf/2504.13460)
*Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma*

Main category: cs.CV

TL;DR: The paper introduces a few-shot temporal action localization (TAL) method using Chain-of-Thought textual reasoning to enhance performance by leveraging textual information alongside visual data.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot TAL methods focus only on video-level information, ignoring valuable textual semantic support, which limits localization accuracy.

Method: The proposed method includes a semantic-aware text-visual alignment module and a Chain-of-Thought reasoning approach to generate descriptive texts for videos, improving action commonality and variation capture.

Result: Experiments on ActivityNet1.3 and THUMOS14 show superior performance over existing methods, and a new dataset for human anomaly detection is introduced.

Conclusion: The method effectively combines textual and visual information for better few-shot TAL, with potential applications in anomaly detection.

Abstract: Traditional temporal action localization (TAL) methods rely on large amounts
of detailed annotated data, whereas few-shot TAL reduces this dependence by
using only a few training samples to identify unseen action categories.
However, existing few-shot TAL methods typically focus solely on video-level
information, neglecting textual information, which can provide valuable
semantic support for the localization task. Therefore, we propose a new
few-shot temporal action localization method by Chain-of-Thought textual
reasoning to improve localization performance. Specifically, we design a novel
few-shot learning framework that leverages textual semantic information to
enhance the model's ability to capture action commonalities and variations,
which includes a semantic-aware text-visual alignment module designed to align
the query and support videos at different levels. Meanwhile, to better express
the temporal dependencies and causal relationships between actions at the
textual level to assist action localization, we design a Chain of Thought
(CoT)-like reasoning method that progressively guides the Vision Language Model
(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for
videos. The generated texts can capture more variance of action than visual
features. We conduct extensive experiments on the publicly available
ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named
Human-related Anomaly Localization and explore the application of the TAL task
in human anomaly detection. The experimental results demonstrate that our
proposed method significantly outperforms existing methods in single-instance
and multi-instance scenarios. We will release our code, data and benchmark.

</details>


### [182] [Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis](https://arxiv.org/pdf/2504.13754)
*Zhu Zhu, Shuo Jiang, Jingyuan Zheng, Yawen Li, Yifei Chen, Manli Zhao, Weizhong Gu, Feiwei Qin, Jinhu Wang, Gang Yu*

Main category: cs.CV

TL;DR: CMSwinKAN is a contrastive-learning-based multi-scale feature fusion model for neuroblastoma diagnosis, improving interpretability and accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current neuroblastoma diagnosis relies on subjective manual examination, and automated methods face challenges like poor interpretability and high computational costs.

Method: CMSwinKAN integrates a Kernel Activation Network into the Swin Transformer, fuses multi-scale features, and uses contrastive learning to mimic clinicians' approaches.

Result: CMSwinKAN outperforms state-of-the-art models on the BreakHis and PpNTs datasets.

Conclusion: CMSwinKAN offers a more accurate and interpretable solution for pathological image classification, with potential for clinical deployment.

Abstract: Neuroblastoma, adrenal-derived, is among the most common pediatric solid
malignancies, characterized by significant clinical heterogeneity. Timely and
accurate pathological diagnosis from hematoxylin and eosin-stained whole-slide
images is critical for patient prognosis. However, current diagnostic practices
primarily rely on subjective manual examination by pathologists, leading to
inconsistent accuracy. Existing automated whole-slide image classification
methods encounter challenges such as poor interpretability, limited feature
extraction capabilities, and high computational costs, restricting their
practical clinical deployment. To overcome these limitations, we propose
CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model
tailored for pathological image classification, which enhances the Swin
Transformer architecture by integrating a Kernel Activation Network within its
multilayer perceptron and classification head modules, significantly improving
both interpretability and accuracy. By fusing multi-scale features and
leveraging contrastive learning strategies, CMSwinKAN mimics clinicians'
comprehensive approach, effectively capturing global and local tissue
characteristics. Additionally, we introduce a heuristic soft voting mechanism
guided by clinical insights to bridge patch-level predictions to whole-slide
image-level classifications seamlessly. We verified the CMSwinKAN on the
publicly available BreakHis dataset and the PpNTs dataset, which was
established by our hospital. Results demonstrate that CMSwinKAN performs better
than existing state-of-the-art pathology-specific models pre-trained on large
datasets. Our source code is available at
https://github.com/JSLiam94/CMSwinKAN.

</details>


### [183] [DreamO: A Unified Framework for Image Customization](https://arxiv.org/pdf/2504.16915)
*Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, Mengtian Li, Songtao Zhao, Jian Zhang, Qian He, Xinglong Wu*

Main category: cs.CV

TL;DR: DreamO is a unified image customization framework using a diffusion transformer (DiT) to handle diverse tasks and integrate multiple conditions, achieving high-quality results.


<details>
  <summary>Details</summary>
Motivation: Existing image customization methods are task-specific, lacking generalizability. DreamO aims to unify these tasks under one framework.

Method: DreamO uses a DiT framework, feature routing constraints, placeholder strategies, and a three-stage progressive training approach.

Result: DreamO effectively performs various customization tasks and integrates multiple control conditions with high quality.

Conclusion: DreamO addresses the challenge of unified image customization, demonstrating flexibility and high performance across tasks.

Abstract: Recently, extensive research on image customization (e.g., identity, subject,
style, background, etc.) demonstrates strong customization capabilities in
large-scale generative models. However, most approaches are designed for
specific tasks, restricting their generalizability to combine different types
of condition. Developing a unified framework for image customization remains an
open challenge. In this paper, we present DreamO, an image customization
framework designed to support a wide range of tasks while facilitating seamless
integration of multiple conditions. Specifically, DreamO utilizes a diffusion
transformer (DiT) framework to uniformly process input of different types.
During training, we construct a large-scale training dataset that includes
various customization tasks, and we introduce a feature routing constraint to
facilitate the precise querying of relevant information from reference images.
Additionally, we design a placeholder strategy that associates specific
placeholders with conditions at particular positions, enabling control over the
placement of conditions in the generated results. Moreover, we employ a
progressive training strategy consisting of three stages: an initial stage
focused on simple tasks with limited data to establish baseline consistency, a
full-scale training stage to comprehensively enhance the customization
capabilities, and a final quality alignment stage to correct quality biases
introduced by low-quality data. Extensive experiments demonstrate that the
proposed DreamO can effectively perform various image customization tasks with
high quality and flexibly integrate different types of control conditions.

</details>


### [184] [Step1X-Edit: A Practical Framework for General Image Editing](https://arxiv.org/pdf/2504.17761)
*Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, Daxin Jiang*

Main category: cs.CV

TL;DR: The paper introduces Step1X-Edit, an open-source image editing model that rivals proprietary models like GPT-4o and Gemini2 Flash, using Multimodal LLM and diffusion decoding.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between open-source and closed-source image editing models by developing a competitive open-source alternative.

Method: Uses Multimodal LLM for processing images and instructions, extracts latent embeddings, and integrates them with a diffusion decoder. A high-quality dataset is generated for training.

Result: Step1X-Edit outperforms open-source baselines and nears the performance of proprietary models on the GEdit-Bench benchmark.

Conclusion: Step1X-Edit advances open-source image editing, offering comparable performance to closed-source models.

Abstract: In recent years, image editing models have witnessed remarkable and rapid
development. The recent unveiling of cutting-edge multimodal models such as
GPT-4o and Gemini2 Flash has introduced highly promising image editing
capabilities. These models demonstrate an impressive aptitude for fulfilling a
vast majority of user-driven editing requirements, marking a significant
advancement in the field of image manipulation. However, there is still a large
gap between the open-source algorithm with these closed-source models. Thus, in
this paper, we aim to release a state-of-the-art image editing model, called
Step1X-Edit, which can provide comparable performance against the closed-source
models like GPT-4o and Gemini2 Flash. More specifically, we adopt the
Multimodal LLM to process the reference image and the user's editing
instruction. A latent embedding has been extracted and integrated with a
diffusion image decoder to obtain the target image. To train the model, we
build a data generation pipeline to produce a high-quality dataset. For
evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world
user instructions. Experimental results on GEdit-Bench demonstrate that
Step1X-Edit outperforms existing open-source baselines by a substantial margin
and approaches the performance of leading proprietary models, thereby making
significant contributions to the field of image editing.

</details>


### [185] [Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID](https://arxiv.org/pdf/2504.19244)
*De Cheng, Lingfeng He, Nannan Wang, Dingwen Zhang, Xinbo Gao*

Main category: cs.CV

TL;DR: The paper introduces SALCR, a framework for unsupervised visible-infrared person re-identification, addressing cross-modality variations by aligning fine-grained patterns and refining pseudo-labels.


<details>
  <summary>Details</summary>
Motivation: Previous methods fail to address cross-modality variations in feature representation and pseudo-label distributions, leading to insufficient modality-shared learning.

Method: Proposes SALCR with DAGI for bi-directional label unification, FGSAL for part-level semantic alignment, and GPCR for refining noisy pseudo-labels.

Result: Achieves superior performance compared to state-of-the-art methods.

Conclusion: SALCR effectively addresses cross-modality variations and improves unsupervised re-identification.

Abstract: Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to
match pedestrian images of the same individual across different modalities
without human annotations for model learning. Previous methods unify
pseudo-labels of cross-modality images through label association algorithms and
then design contrastive learning framework for global feature learning.
However, these methods overlook the cross-modality variations in feature
representation and pseudo-label distributions brought by fine-grained patterns.
This insight results in insufficient modality-shared learning when only global
features are optimized. To address this issue, we propose a Semantic-Aligned
Learning with Collaborative Refinement (SALCR) framework, which builds up
optimization objective for specific fine-grained patterns emphasized by each
modality, thereby achieving complementary alignment between the label
distributions of different modalities. Specifically, we first introduce a Dual
Association with Global Learning (DAGI) module to unify the pseudo-labels of
cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained
Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level
semantic-aligned patterns emphasized by each modality from cross-modality
instances. Optimization objective is then formulated based on the
semantic-aligned features and their corresponding label space. To alleviate the
side-effects arising from noisy pseudo-labels, we propose a Global-Part
Collaborative Refinement (GPCR) module to mine reliable positive sample sets
for the global and part features dynamically and optimize the inter-instance
relationships. Extensive experiments demonstrate the effectiveness of the
proposed method, which achieves superior performances to state-of-the-art
methods. Our code is available at
\href{https://github.com/FranklinLingfeng/code-for-SALCR}.

</details>


### [186] [Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning](https://arxiv.org/pdf/2504.21561)
*Pengxiang Li, Zhi Gao, Bofei Zhang, Yapeng Mi, Xiaojian Ma, Chenrui Shi, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li*

Main category: cs.CV

TL;DR: SPORT is an online self-exploration method for multimodal agents that refines trajectories via step-wise preference optimization, eliminating the need for expert data.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal agents require extensive expert data for fine-tuning, limiting adaptability to new environments.

Method: SPORT involves task synthesis, step sampling, step verification, and preference tuning to iteratively improve agent performance.

Result: SPORT achieves 6.41% and 3.64% improvements in GTA and GAIA benchmarks, demonstrating generalization and effectiveness.

Conclusion: SPORT enables multimodal agents to autonomously refine capabilities without expert annotation, proving effective in real-world benchmarks.

Abstract: Multimodal agents, which integrate a controller (e.g., a large language
model) with external tools, have demonstrated remarkable capabilities in
tackling complex tasks. However, existing agents need to collect a large number
of expert data for fine-tuning to adapt to new environments. In this paper, we
propose an online self-exploration method for multimodal agents, namely SPORT,
via step-wise preference optimization to refine the trajectories of agents,
which automatically generates tasks and learns from solving the generated
tasks, without any expert annotation. SPORT operates through four iterative
components: task synthesis, step sampling, step verification, and preference
tuning. First, we synthesize multi-modal tasks using language models. Then, we
introduce a novel search scheme, where step sampling and step verification are
executed alternately to solve each generated task. We employ a verifier to
provide AI feedback to construct step-wise preference data. The data is
subsequently used to update the controller's policy through preference tuning,
producing a SPORT Agent. By interacting with real environments, the SPORT Agent
evolves into a more refined and capable system. Evaluation in the GTA and GAIA
benchmarks show that the SPORT Agent achieves 6.41\% and 3.64\% improvements,
underscoring the generalization and effectiveness introduced by our method. The
project page is https://SPORT-Agents.github.io.

</details>


### [187] [HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection](https://arxiv.org/pdf/2505.00507)
*Esteban Rivera, Surya Prabhakaran, Markus Lienkamp*

Main category: cs.CV

TL;DR: HeAL integrates heuristical features with active learning for 3D object detection, achieving competitive results with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of sample selection in uncontrolled scenarios and bridging the gap between theory and practical insights in 3D detection.

Method: Combines heuristical features (e.g., object distance, point-quantity) with localization and classification for sample selection.

Result: Achieves same mAP as full-supervised baseline with only 24% of samples on KITTI.

Conclusion: HeAL is effective for active learning in 3D object detection, offering practical improvements over theoretical-only approaches.

Abstract: Active Learning has proved to be a relevant approach to perform sample
selection for training models for Autonomous Driving. Particularly, previous
works on active learning for 3D object detection have shown that selection of
samples in uncontrolled scenarios is challenging. Furthermore, current
approaches focus exclusively on the theoretical aspects of the sample selection
problem but neglect the practical insights that can be obtained from the
extensive literature and application of 3D detection models. In this paper, we
introduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)
which integrates those heuristical features together with Localization and
Classification to deliver the most contributing samples to the model's
training. In contrast to previous works, our approach integrates heuristical
features such as object distance and point-quantity to estimate the
uncertainty, which enhance the usefulness of selected samples to train
detection models. Our quantitative evaluation on KITTI shows that HeAL presents
competitive mAP with respect to the State-of-the-Art, and achieves the same mAP
as the full-supervised baseline with only 24% of the samples.

</details>


### [188] [Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis](https://arxiv.org/pdf/2505.00746)
*Alexei Kaltchenko*

Main category: cs.CV

TL;DR: The paper introduces an entropy-heat-mapping method to identify OCR errors in GPT-4o transcriptions by analyzing token-level Shannon entropy.


<details>
  <summary>Details</summary>
Motivation: To improve post-editing of GPT-based OCR by leveraging token-level confidence signals, which are often underutilized.

Method: Uses a sliding-window approach on per-token Shannon entropy to create a visual 'uncertainty landscape' and identify error hotspots.

Result: Most true OCR errors (e.g., missing symbols, mismatched braces) are concentrated in high-entropy regions.

Conclusion: Sliding-window entropy is a lightweight, practical tool for post-editing GPT-based OCR, with code and guidelines released for further research.

Abstract: Vision-language models such as OpenAI GPT-4o can transcribe mathematical
documents directly from images, yet their token-level confidence signals are
seldom used to pinpoint local recognition mistakes. We present an
entropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into
a visual ''uncertainty landscape''. By scanning the entropy sequence with a
fixed-length sliding window, we obtain hotspots that are likely to contain OCR
errors such as missing symbols, mismatched braces, or garbled prose. Using a
small, curated set of scanned research pages rendered at several resolutions,
we compare the highlighted hotspots with the actual transcription errors
produced by GPT-4o. Our analysis shows that the vast majority of true errors
are indeed concentrated inside the high-entropy regions. This study
demonstrates--in a minimally engineered setting--that sliding-window entropy
can serve as a practical, lightweight aid for post-editing GPT-based OCR. All
code and annotation guidelines are released to encourage replication and
further research.

</details>


### [189] [RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video](https://arxiv.org/pdf/2505.02064)
*Shuhang Xun, Sicheng Tao, Jungang Li, Yibo Shi, Zhixin Lin, Zhanhui Zhu, Yibo Yan, Hanqian Li, Linghao Zhang, Shikang Wang, Yixin Liu, Hanbo Zhang, Ying Ma, Xuming Hu*

Main category: cs.CV

TL;DR: RTV-Bench is a new benchmark for evaluating Multimodal Large Language Models (MLLMs) in real-time video analysis, focusing on continuous perception, understanding, and reasoning. It outperforms existing benchmarks and reveals gaps in current models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to assess MLLMs' ability to perform continuous tasks in dynamic environments, necessitating a more robust evaluation tool.

Method: RTV-Bench introduces Multi-Timestamp Question Answering (MTQA), Hierarchical Question Structure, and Multi-dimensional Evaluation. It includes 552 videos and 4,631 QA pairs.

Result: Open-source real-time models outperform offline ones but lag behind proprietary models. Larger models or higher frame rates don't significantly improve performance.

Conclusion: Better architectures optimized for video streams and long sequences are needed to advance real-time video analysis with MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) increasingly excel at perception,
understanding, and reasoning. However, current benchmarks inadequately evaluate
their ability to perform these tasks continuously in dynamic, real-world
environments. To bridge this gap, we introduce RTV-Bench, a fine-grained
benchmark for MLLM real-time video analysis. RTV-Bench uses three key
principles: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve
with scene changes; (2) Hierarchical Question Structure, combining basic and
advanced queries; and (3) Multi-dimensional Evaluation, assessing the ability
of continuous perception, understanding, and reasoning. RTV-Bench contains 552
diverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated
leading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline
(Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5,
InternLM-XComposer2.5-OmniLive) models. Experiment results show open-source
real-time models largely outperform offline ones but still trail top
proprietary models. Our analysis also reveals that larger model size or higher
frame sampling rates do not significantly boost RTV-Bench performance,
sometimes causing slight decreases. This underscores the need for better model
architectures optimized for video stream processing and long sequences to
advance real-time video analysis with MLLMs. Our benchmark toolkit is available
at: https://github.com/LJungang/RTV-Bench.

</details>


### [190] [MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation](https://arxiv.org/pdf/2505.02648)
*Mingcheng Li, Xiaolu Hou, Ziyang Liu, Dingkang Yang, Ziyun Qian, Jiawei Chen, Jinjie Wei, Yue Jiang, Qingyao Xu, Lihua Zhang*

Main category: cs.CV

TL;DR: MCCD improves text-to-image generation for complex scenes using multi-agent collaboration and hierarchical diffusion.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex prompts involving multiple objects and relations.

Method: Multi-agent collaboration for scene parsing and hierarchical compositional diffusion with Gaussian masks and filtering.

Result: MCCD outperforms baseline models in complex scene generation without additional training.

Conclusion: MCCD offers a robust solution for high-fidelity generation of complex scenes.

Abstract: Diffusion models have shown excellent performance in text-to-image
generation. Nevertheless, existing methods often suffer from performance
bottlenecks when handling complex prompts that involve multiple objects,
characteristics, and relations. Therefore, we propose a Multi-agent
Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation
for complex scenes. Specifically, we design a multi-agent collaboration-based
scene parsing module that generates an agent system comprising multiple agents
with distinct tasks, utilizing MLLMs to extract various scene elements
effectively. In addition, Hierarchical Compositional diffusion utilizes a
Gaussian mask and filtering to refine bounding box regions and enhance objects
through region enhancement, resulting in the accurate and high-fidelity
generation of complex scenes. Comprehensive experiments demonstrate that our
MCCD significantly improves the performance of the baseline models in a
training-free manner, providing a substantial advantage in complex scene
generation.

</details>


### [191] [VGLD: Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery](https://arxiv.org/pdf/2505.02704)
*Bojin Wu, Jing Chen*

Main category: cs.CV

TL;DR: VGLD is a method for robust monocular depth scale recovery by integrating image semantics with textual descriptions to stabilize scale recovery.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recovering absolute depth scale from monocular images, where textual descriptions can introduce ambiguity.

Method: VGLD combines high-level semantic information from images with textual descriptions to stabilize scale recovery, outputting linear transformation parameters for metric-scale depth.

Result: Validated on multiple datasets (NYUv2, KITTI) and models (MiDas, DepthAnything), VGLD shows strong performance, even in zero-shot scenarios.

Conclusion: VGLD serves as a universal alignment module for metric-scale depth recovery, resolving textual ambiguities effectively.

Abstract: We propose a robust method for monocular depth scale recovery. Monocular
depth estimation can be divided into two main directions: (1) relative depth
estimation, which provides normalized or inverse depth without scale
information, and (2) metric depth estimation, which involves recovering depth
with absolute scale. To obtain absolute scale information for practical
downstream tasks, utilizing textual information to recover the scale of a
relative depth map is a highly promising approach. However, since a single
image can have multiple descriptions from different perspectives or with
varying styles, it has been shown that different textual descriptions can
significantly affect the scale recovery process. To address this issue, our
method, VGLD, stabilizes the influence of textual information by incorporating
high-level semantic information from the corresponding image alongside the
textual description. This approach resolves textual ambiguities and robustly
outputs a set of linear transformation parameters (scalars) that can be
globally applied to the relative depth map, ultimately generating depth
predictions with metric-scale accuracy. We validate our method across several
popular relative depth models(MiDas, DepthAnything), using both indoor scenes
(NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions
as a universal alignment module when trained on multiple datasets, achieving
strong performance even in zero-shot scenarios. Code is available at:
https://github.com/pakinwu/VGLD.

</details>


### [192] [Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology](https://arxiv.org/pdf/2505.02825)
*Alex Hoi Hang Chan, Otto Brookes, Urs Waldmann, Hemal Naik, Iain D. Couzin, Majid Mirmehdi, Noël Adiko Houa, Emmanuelle Normand, Christophe Boesch, Lukas Boesch, Mimi Arandjelovic, Hjalmar Kühl, Tilo Burghardt, Fumihiro Kano*

Main category: cs.CV

TL;DR: The paper advocates for evaluating computer vision models in ecology/biology using application-specific metrics, not just ML metrics, and demonstrates this with case studies on chimpanzee abundance and pigeon head rotation.


<details>
  <summary>Details</summary>
Motivation: Current computer vision resources in ecology/biology focus on ML metrics, neglecting downstream application impact.

Method: Presented two case studies: chimpanzee abundance estimation using a behavior classifier and pigeon head rotation estimation using a 3D posture estimator.

Result: Models with strong ML performance (e.g., 87% mAP) led to discrepancies in ecological estimates compared to expert data.

Conclusion: Researchers should integrate application-specific metrics to better benchmark models for their intended use.

Abstract: Computer vision methods have demonstrated considerable potential to
streamline ecological and biological workflows, with a growing number of
datasets and models becoming available to the research community. However,
these resources focus predominantly on evaluation using machine learning
metrics, with relatively little emphasis on how their application impacts
downstream analysis. We argue that models should be evaluated using
application-specific metrics that directly represent model performance in the
context of its final use case. To support this argument, we present two
disparate case studies: (1) estimating chimpanzee abundance and density with
camera trap distance sampling when using a video-based behaviour classifier and
(2) estimating head rotation in pigeons using a 3D posture estimator. We show
that even models with strong machine learning performance (e.g., 87% mAP) can
yield data that leads to discrepancies in abundance estimates compared to
expert-derived data. Similarly, the highest-performing models for posture
estimation do not produce the most accurate inferences of gaze direction in
pigeons. Motivated by these findings, we call for researchers to integrate
application-specific metrics in ecological/biological datasets, allowing for
models to be benchmarked in the context of their downstream application and to
facilitate better integration of models into application workflows.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [193] [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/pdf/2505.02952)
*Fabrizio Marozzo*

Main category: cs.AI

TL;DR: The paper proposes an iterative method to reduce ambiguity in generative AI systems by using structured clarification questions and alternative solutions, improving accuracy and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: Natural language ambiguity in generative AI leads to imprecise instructions, requiring iterative corrections. The goal is to systematically resolve uncertainties for precise outputs.

Method: An iterative approach with clarification questions, alternative solutions, and input/output examples to narrow down ambiguities before generating a final solution.

Result: The method outperforms one-shot solutions in accuracy, resolution time, and user satisfaction across coding, data analysis, and creative writing tasks.

Conclusion: The structured iterative approach effectively resolves ambiguities in generative AI, delivering precise solutions with higher efficiency and user satisfaction.

Abstract: Generative AI systems have revolutionized human interaction by enabling
natural language-based coding and problem solving. However, the inherent
ambiguity of natural language often leads to imprecise instructions, forcing
users to iteratively test, correct, and resubmit their prompts. We propose an
iterative approach that systematically narrows down these ambiguities through a
structured series of clarification questions and alternative solution
proposals, illustrated with input/output examples as well. Once every
uncertainty is resolved, a final, precise solution is generated. Evaluated on a
diverse dataset spanning coding, data analysis, and creative writing, our
method demonstrates superior accuracy, competitive resolution times, and higher
user satisfaction compared to conventional one-shot solutions, which typically
require multiple manual iterations to achieve a correct output.

</details>


### [194] [The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI](https://arxiv.org/pdf/2505.03020)
*Kishore Sampath, Pratheesh, Ayaazuddin Mohammad, Resmi Ramachandranpillai*

Main category: cs.AI

TL;DR: Adding modalities improves performance but fairness varies; missing modalities at inference harms both performance and fairness.


<details>
  <summary>Details</summary>
Motivation: To address overlooked concerns of bias and robustness in multimodal learning, focusing on performance and fairness.

Method: Analyzes impact of adding/missing modalities using healthcare datasets with images, time series, and structured data.

Result: New modalities boost performance but fairness varies; missing modalities degrade both performance and fairness.

Conclusion: Highlights trade-offs in multimodal learning, emphasizing robustness and fairness for real-world deployment.

Abstract: Multimodal learning, which integrates diverse data sources such as images,
text, and structured data, has proven superior to unimodal counterparts in
high-stakes decision-making. However, while performance gains remain the gold
standard for evaluating multimodal systems, concerns around bias and robustness
are frequently overlooked. In this context, this paper explores two key
research questions (RQs): (i) RQ1 examines whether adding a modality
con-sistently enhances performance and investigates its role in shaping
fairness measures, assessing whether it mitigates or amplifies bias in
multimodal models; (ii) RQ2 investigates the impact of missing modalities at
inference time, analyzing how multimodal models generalize in terms of both
performance and fairness. Our analysis reveals that incorporating new
modalities during training consistently enhances the performance of multimodal
models, while fairness trends exhibit variability across different evaluation
measures and datasets. Additionally, the absence of modalities at inference
degrades performance and fairness, raising concerns about its robustness in
real-world deployment. We conduct extensive experiments using multimodal
healthcare datasets containing images, time series, and structured information
to validate our findings.

</details>


### [195] [Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes](https://arxiv.org/pdf/2505.03033)
*George Xi Wang, Jingying Deng, Safinah Ali*

Main category: cs.AI

TL;DR: An AI-powered system uses LLMs to create personalized audiovisual learning environments, aiming to reduce distraction and enhance emotional stability for independent learners.


<details>
  <summary>Details</summary>
Motivation: Independent learners struggle with focus and emotional regulation in unstructured settings, and existing educational technologies overlook sensory and emotional contexts.

Method: The system leverages LLMs to generate personalized visual and auditory elements, evaluated through mixed-methods design with biometric measures and performance outcomes.

Result: The study investigates how personalized audiovisual combinations affect cognitive load and engagement, aiming to improve emotionally responsive educational technologies.

Conclusion: The findings aim to advance sensory personalization in self-directed learning and expand the use of multimodal LLMs in education.

Abstract: Independent learners often struggle with sustaining focus and emotional
regulation in unstructured or distracting settings. Although some rely on
ambient aids such as music, ASMR, or visual backgrounds to support
concentration, these tools are rarely integrated into cohesive,
learner-centered systems. Moreover, existing educational technologies focus
primarily on content adaptation and feedback, overlooking the emotional and
sensory context in which learning takes place. Large language models have
demonstrated powerful multimodal capabilities including the ability to generate
and adapt text, audio, and visual content. Educational research has yet to
fully explore their potential in creating personalized audiovisual learning
environments. To address this gap, we introduce an AI-powered system that uses
LLMs to generate personalized multisensory study environments. Users select or
generate customized visual themes (e.g., abstract vs. realistic, static vs.
animated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs.
novel sounds) to create immersive settings aimed at reducing distraction and
enhancing emotional stability. Our primary research question investigates how
combinations of personalized audiovisual elements affect learner cognitive load
and engagement. Using a mixed-methods design that incorporates biometric
measures and performance outcomes, this study evaluates the effectiveness of
LLM-driven sensory personalization. The findings aim to advance emotionally
responsive educational technologies and extend the application of multimodal
LLMs into the sensory dimension of self-directed learning.

</details>


### [196] [BLAB: Brutally Long Audio Bench](https://arxiv.org/pdf/2505.03054)
*Orevaoghene Ahia, Martijn Bartelds, Kabir Ahuja, Hila Gonen, Valentin Hofmann, Siddhant Arora, Shuyue Stella Li, Vishal Puttagunta, Mofetoluwa Adeyemi, Charishma Buchireddy, Ben Walls, Noah Bennett, Shinji Watanabe, Noah A. Smith, Yulia Tsvetkov, Sachin Kumar*

Main category: cs.AI

TL;DR: BLAB is a long-form audio benchmark testing audio LMs on tasks like localization and emotion, revealing their struggles with extended speech.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation for audio LMs on long-form conversational speech, which better reflects real-world interactions.

Method: Introduces BLAB, a benchmark with 833+ hours of diverse, annotated long-form audio clips (avg. 51 mins), testing tasks like localization and counting.

Result: All tested audio LMs, including advanced ones like GPT-4o, perform poorly on BLAB, especially as audio duration increases.

Conclusion: BLAB highlights the need for improved long-form audio understanding in LMs and serves as a framework for future development.

Abstract: Developing large audio language models (LMs) capable of understanding diverse
spoken interactions is essential for accommodating the multimodal nature of
human communication and can increase the accessibility of language technologies
across different user populations. Recent work on audio LMs has primarily
evaluated their performance on short audio segments, typically under 30
seconds, with limited exploration of long-form conversational speech segments
that more closely reflect natural user interactions with these models. We
introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio
benchmark that evaluates audio LMs on localization, duration estimation,
emotion, and counting tasks using audio segments averaging 51 minutes in
length. BLAB consists of 833+ hours of diverse, full-length audio clips, each
paired with human-annotated, text-based natural language questions and answers.
Our audio data were collected from permissively licensed sources and underwent
a human-assisted filtering process to ensure task compliance. We evaluate six
open-source and proprietary audio LMs on BLAB and find that all of them,
including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the
tasks in BLAB. Our comprehensive analysis reveals key insights into the
trade-offs between task difficulty and audio duration. In general, we find that
audio LMs struggle with long-form speech, with performance declining as
duration increases. They perform poorly on localization, temporal reasoning,
counting, and struggle to understand non-phonemic information, relying more on
prompts than audio content. BLAB serves as a challenging evaluation framework
to develop audio LMs with robust long-form audio understanding capabilities.

</details>


### [197] [Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE](https://arxiv.org/pdf/2505.03108)
*Brendan Campbell, Alan Williams, Kleio Baxevani, Alyssa Campbell, Rushabh Dhoke, Rileigh E. Hudock, Xiaomin Lin, Vivek Mange, Bernhard Neuberger, Arjun Suresh, Alhim Vera, Arthur Trembanis, Herbert G. Tanner, Edward Hale*

Main category: cs.AI

TL;DR: The paper evaluates the ODYSSEE model, a deep learning tool for identifying live oysters in images, comparing its performance to human annotators. While faster, the model was less accurate, with image quality affecting results. Future improvements are suggested.


<details>
  <summary>Details</summary>
Motivation: Current oyster monitoring methods are destructive and labor-intensive, making them unsuitable for sensitive environments. The ODYSSEE model offers a non-destructive, efficient alternative.

Method: The study compares the ODYSSEE model's performance in identifying live oysters to expert and non-expert annotators, analyzing prediction errors and the impact of image quality.

Result: The model was faster (39.6 s) but less accurate (63%) than humans (experts: 74%, non-experts: 75%). Image quality influenced accuracy, improving human performance but worsening the model's.

Conclusion: ODYSSEE shows promise but needs refinement, such as training on higher-quality images and additional annotation classes, to improve accuracy for future use.

Abstract: Oysters are ecologically and commercially important species that require
frequent monitoring to track population demographics (e.g. abundance, growth,
mortality). Current methods of monitoring oyster reefs often require
destructive sampling methods and extensive manual effort. Therefore, they are
suboptimal for small-scale or sensitive environments. A recent alternative, the
ODYSSEE model, was developed to use deep learning techniques to identify live
oysters using video or images taken in the field of oyster reefs to assess
abundance. The validity of this model in identifying live oysters on a reef was
compared to expert and non-expert annotators. In addition, we identified
potential sources of prediction error. Although the model can make inferences
significantly faster than expert and non-expert annotators (39.6 s, $2.34 \pm
0.61$ h, $4.50 \pm 1.46$ h, respectively), the model overpredicted the number
of live oysters, achieving lower accuracy (63\%) in identifying live oysters
compared to experts (74\%) and non-experts (75\%) alike. Image quality was an
important factor in determining the accuracy of the model and the annotators.
Better quality images improved human accuracy and worsened model accuracy.
Although ODYSSEE was not sufficiently accurate, we anticipate that future
training on higher-quality images, utilizing additional live imagery, and
incorporating additional annotation training classes will greatly improve the
model's predictive power based on the results of this analysis. Future research
should address methods that improve the detection of living vs. dead oysters.

</details>


### [198] [Holmes: Automated Fact Check with Large Language Models](https://arxiv.org/pdf/2505.03135)
*Haoran Ou, Gelei Deng, Xingshuo Han, Jie Zhang, Xinlei He, Han Qiu, Shangwei Guo, Tianwei Zhang*

Main category: cs.AI

TL;DR: The paper proposes Holmes, an end-to-end framework using LLMs and evidence retrieval to improve multimodal disinformation detection, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: The spread of complex multimodal disinformation challenges existing detection methods, requiring advanced solutions like LLMs.

Method: Holmes combines LLM-powered summarization and a novel evidence retrieval algorithm to verify claims and generate justifications.

Result: Holmes achieves 88.3% accuracy on datasets and 90.2% in real-time tasks, with a 30.8% improvement in fact-checking accuracy.

Conclusion: Holmes effectively enhances LLM-based disinformation detection by improving evidence retrieval and verification.

Abstract: The rise of Internet connectivity has accelerated the spread of
disinformation, threatening societal trust, decision-making, and national
security. Disinformation has evolved from simple text to complex multimodal
forms combining images and text, challenging existing detection methods.
Traditional deep learning models struggle to capture the complexity of
multimodal disinformation. Inspired by advances in AI, this study explores
using Large Language Models (LLMs) for automated disinformation detection. The
empirical study shows that (1) LLMs alone cannot reliably assess the
truthfulness of claims; (2) providing relevant evidence significantly improves
their performance; (3) however, LLMs cannot autonomously search for accurate
evidence. To address this, we propose Holmes, an end-to-end framework featuring
a novel evidence retrieval method that assists LLMs in collecting high-quality
evidence. Our approach uses (1) LLM-powered summarization to extract key
information from open sources and (2) a new algorithm and metrics to evaluate
evidence quality. Holmes enables LLMs to verify claims and generate
justifications effectively. Experiments show Holmes achieves 88.3% accuracy on
two open-source datasets and 90.2% in real-time verification tasks. Notably,
our improved evidence retrieval boosts fact-checking accuracy by 30.8% over
existing methods

</details>


### [199] [CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics](https://arxiv.org/pdf/2505.03171)
*Junqi Liu, Xiaohan Lin, Jonas Bayer, Yael Dillies, Weijie Jiang, Xiaodan Liang, Roman Soletskyi, Haiming Wang, Yunzhou Xie, Beibei Xiong, Zhengfeng Yang, Jujian Zhang, Lihong Zhi, Jia Li, Zhengying Liu*

Main category: cs.AI

TL;DR: CombiBench is a new benchmark for combinatorial problems, formalized in Lean~4, with an evaluation framework (Fine-Eval) to test LLMs. Current models perform poorly, with Kimina-Prover solving only 7 out of 100 problems.


<details>
  <summary>Details</summary>
Motivation: Combinatorics lacks benchmarks and theorem libraries, unlike other math domains. CombiBench fills this gap.

Method: Introduces CombiBench (100 problems in Lean~4) and Fine-Eval, a framework for evaluating formal math solutions, including fill-in-the-blank questions.

Result: LLMs perform poorly on CombiBench, with Kimina-Prover solving only 7 problems.

Conclusion: CombiBench and Fine-Eval provide tools for advancing formal reasoning in combinatorics, but current LLMs are limited.

Abstract: Neurosymbolic approaches integrating large language models with formal
reasoning have recently achieved human-level performance on mathematics
competition problems in algebra, geometry and number theory. In comparison,
combinatorics remains a challenging domain, characterized by a lack of
appropriate benchmarks and theorem libraries. To address this gap, we introduce
CombiBench, a comprehensive benchmark comprising 100 combinatorial problems,
each formalized in Lean~4 and paired with its corresponding informal statement.
The problem set covers a wide spectrum of difficulty levels, ranging from
middle school to IMO and university level, and span over ten combinatorial
topics. CombiBench is suitable for testing IMO solving capabilities since it
includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its
statement contain an images). Furthermore, we provide a comprehensive and
standardized evaluation framework, dubbed Fine-Eval (for
$\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for
formal mathematics. It accommodates not only proof-based problems but also, for
the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval
as the evaluation method and Kimina Lean Server as the backend, we benchmark
several LLMs on CombiBench and observe that their capabilities for formally
solving combinatorial problems remain limited. Among all models tested (none of
which has been trained for this particular task), Kimina-Prover attains the
best results, solving 7 problems (out of 100) under both ``with solution'' and
``without solution'' scenarios. We open source the benchmark dataset alongside
with the code of the proposed evaluation method at
https://github.com/MoonshotAI/CombiBench/.

</details>


### [200] [Patterns and Mechanisms of Contrastive Activation Engineering](https://arxiv.org/pdf/2505.03189)
*Yixiong Hao, Ayush Panda, Stepan Shabalin, Sheikh Abdur Raheem Ali*

Main category: cs.AI

TL;DR: CAE techniques offer a cost-free, inference-time method to steer LLM behavior but are limited to in-distribution contexts, have diminishing returns with more samples, and can degrade model performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of controlling LLMs without extensive computational resources by exploring CAE techniques.

Method: Analyzing CAE's performance in in-distribution and out-of-distribution settings, evaluating drawbacks, and developing deployment guidelines.

Result: CAE is effective only in-distribution, has diminishing returns beyond 80 samples, is vulnerable to adversarial inputs, harms perplexity, and larger models resist degradation.

Conclusion: CAE is a promising but limited tool for LLM behavior control, requiring careful deployment and further research.

Abstract: Controlling the behavior of Large Language Models (LLMs) remains a
significant challenge due to their inherent complexity and opacity. While
techniques like fine-tuning can modify model behavior, they typically require
extensive computational resources. Recent work has introduced a class of
contrastive activation engineering (CAE) techniques as promising approaches for
steering LLM outputs through targeted modifications to their internal
representations. Applied at inference-time with zero cost, CAE has the
potential to introduce a new paradigm of flexible, task-specific LLM behavior
tuning. We analyze the performance of CAE in in-distribution,
out-of-distribution settings, evaluate drawbacks, and begin to develop
comprehensive guidelines for its effective deployment. We find that 1. CAE is
only reliably effective when applied to in-distribution contexts. 2. Increasing
the number of samples used to generate steering vectors has diminishing returns
at around 80 samples. 3. Steering vectors are susceptible to adversarial inputs
that reverses the behavior that is steered for. 4. Steering vectors harm the
overall model perplexity. 5. Larger models are more resistant to
steering-induced degradation.

</details>


### [201] [RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.03275)
*Tiantian Gan, Qiyao Sun*

Main category: cs.AI

TL;DR: RAG-MCP is a Retrieval-Augmented Generation framework that improves LLMs' tool usage by reducing prompt bloat and simplifying tool selection via semantic retrieval.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with prompt bloat and complexity when using external tools like MCP.

Method: RAG-MCP uses semantic retrieval to identify relevant tools from an external index, passing only selected descriptions to the LLM.

Result: RAG-MCP reduces prompt tokens by over 50% and triples tool selection accuracy (43.13% vs 13.62%).

Conclusion: RAG-MCP enables scalable and accurate tool integration for LLMs.

Abstract: Large language models (LLMs) struggle to effectively utilize a growing number
of external tools, such as those defined by the Model Context Protocol
(MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We
introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes
this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to
identify the most relevant MCP(s) for a given query from an external index
before engaging the LLM. Only the selected tool descriptions are passed to the
model, drastically reducing prompt size and simplifying decision-making.
Experiments, including an MCP stress test, demonstrate RAG-MCP significantly
cuts prompt tokens (e.g., by over 50%) and more than triples tool selection
accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables
scalable and accurate tool integration for LLMs.

</details>


### [202] [Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces](https://arxiv.org/pdf/2505.03295)
*Luis Miguel Vieira da Silva, Aljosha Köcher, Nicolas König, Felix Gehlhoff, Alexander Fay*

Main category: cs.AI

TL;DR: A method using large language models to generate executable skill implementations from natural language input, integrating existing libraries and interfaces for flexibility.


<details>
  <summary>Details</summary>
Motivation: To simplify and accelerate the development of skill implementations conforming to capabilities in modular automation systems.

Method: Treats capabilities as contracts, uses large language models for code generation, and integrates existing libraries and interfaces via a retrieval-augmented framework.

Result: Demonstrated feasibility and flexibility using an autonomous mobile robot controlled via Python and ROS 2.

Conclusion: The approach effectively reduces development time and complexity for skill implementations in modular systems.

Abstract: Modern automation systems increasingly rely on modular architectures, with
capabilities and skills as one solution approach. Capabilities define the
functions of resources in a machine-readable form and skills provide the
concrete implementations that realize those capabilities. However, the
development of a skill implementation conforming to a corresponding capability
remains a time-consuming and challenging task. In this paper, we present a
method that treats capabilities as contracts for skill implementations and
leverages large language models to generate executable code based on natural
language user input. A key feature of our approach is the integration of
existing software libraries and interface technologies, enabling the generation
of skill implementations across different target languages. We introduce a
framework that allows users to incorporate their own libraries and resource
interfaces into the code generation process through a retrieval-augmented
generation architecture. The proposed method is evaluated using an autonomous
mobile robot controlled via Python and ROS 2, demonstrating the feasibility and
flexibility of the approach.

</details>


### [203] [Artificial Behavior Intelligence: Technology, Challenges, and Future Directions](https://arxiv.org/pdf/2505.03315)
*Kanghyun Jo, Jehwan Choi, Kwanho Kim, Seongmin Kim, Duy-Linh Nguyen, Xuan-Thuy Vo, Adri Priadana, Tien-Dat Tran*

Main category: cs.AI

TL;DR: The paper introduces Artificial Behavior Intelligence (ABI), a framework for analyzing human behavior using AI, and discusses its components, challenges, and potential solutions.


<details>
  <summary>Details</summary>
Motivation: To enhance AI's capability in understanding and predicting human behavior for applications like autonomous driving and healthcare.

Method: ABI integrates pose estimation, emotion recognition, behavior analysis, and context modeling, leveraging pretrained models for accuracy.

Result: Identifies challenges like limited data learning and real-time inference, proposing solutions like lightweight transformers and knowledge distillation.

Conclusion: ABI holds transformative potential but requires addressing technical hurdles for real-world deployment.

Abstract: Understanding and predicting human behavior has emerged as a core capability
in various AI application domains such as autonomous driving, smart healthcare,
surveillance systems, and social robotics. This paper defines the technical
framework of Artificial Behavior Intelligence (ABI), which comprehensively
analyzes and interprets human posture, facial expressions, emotions, behavioral
sequences, and contextual cues. It details the essential components of ABI,
including pose estimation, face and emotion recognition, sequential behavior
analysis, and context-aware modeling. Furthermore, we highlight the
transformative potential of recent advances in large-scale pretrained models,
such as large language models (LLMs), vision foundation models, and multimodal
integration models, in significantly improving the accuracy and
interpretability of behavior recognition. Our research team has a strong
interest in the ABI domain and is actively conducting research, particularly
focusing on the development of intelligent lightweight models capable of
efficiently inferring complex human behaviors. This paper identifies several
technical challenges that must be addressed to deploy ABI in real-world
applications including learning behavioral intelligence from limited data,
quantifying uncertainty in complex behavior prediction, and optimizing model
structures for low-power, real-time inference. To tackle these challenges, our
team is exploring various optimization strategies including lightweight
transformers, graph-based recognition architectures, energy-aware loss
functions, and multimodal knowledge distillation, while validating their
applicability in real-time environments.

</details>


### [204] [AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning](https://arxiv.org/pdf/2505.03332)
*Evgeny Markhasin*

Main category: cs.AI

TL;DR: Persistent Workflow Prompting (PWP) is introduced to enhance LLMs' critical peer review of scientific manuscripts, overcoming data and reasoning challenges. It uses modular workflows for systematic analysis.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs in critical peer review due to data constraints and complex expert reasoning.

Method: PWP employs hierarchical, modular workflows (structured via Markdown) developed through meta-prompting and meta-reasoning to codify expert review processes.

Result: PWP enables LLMs to identify methodological flaws, mitigate bias, and perform complex tasks like integrating multimodal data and quantitative checks.

Conclusion: PWP demonstrates potential for sophisticated scientific analysis using standard LLMs, with transparency and replication supported by provided resources.

Abstract: Critical peer review of scientific manuscripts presents a significant
challenge for Large Language Models (LLMs), partly due to data limitations and
the complexity of expert reasoning. This report introduces Persistent Workflow
Prompting (PWP), a potentially broadly applicable prompt engineering
methodology designed to bridge this gap using standard LLM chat interfaces
(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical
analysis of experimental chemistry manuscripts, featuring a hierarchical,
modular architecture (structured via Markdown) that defines detailed analysis
workflows. We develop this PWP prompt through iterative application of
meta-prompting techniques and meta-reasoning aimed at systematically codifying
expert review workflows, including tacit knowledge. Submitted once at the start
of a session, this PWP prompt equips the LLM with persistent workflows
triggered by subsequent queries, guiding modern reasoning LLMs through
systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM
identifying major methodological flaws in a test case while mitigating LLM
input bias and performing complex tasks, including distinguishing claims from
evidence, integrating text/photo/figure analysis to infer parameters, executing
quantitative feasibility checks, comparing estimates against claims, and
assessing a priori plausibility. To ensure transparency and facilitate
replication, we provide full prompts, detailed demonstration analyses, and logs
of interactive chats as supplementary resources. Beyond the specific
application, this work offers insights into the meta-development process
itself, highlighting the potential of PWP, informed by detailed workflow
formalization, to enable sophisticated analysis using readily available LLMs
for complex scientific tasks.

</details>


### [205] [Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection](https://arxiv.org/pdf/2505.03359)
*June-Woo Kim, Haram Yoon, Wonkyo Oh, Dawoon Jung, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang*

Main category: cs.AI

TL;DR: The paper introduces a domain adversarial training method to reduce gender bias in speech-based AI models for detecting depression and PTSD, improving F1-scores by up to 13.29%.


<details>
  <summary>Details</summary>
Motivation: Gender bias in speech-based AI models for mental health detection leads to unfair and inaccurate predictions, necessitating a solution to address demographic disparities.

Method: A domain adversarial training approach is used, treating genders as distinct domains and integrating this into a pretrained speech foundation model, validated on the E-DAIC dataset.

Result: The method improves detection performance, increasing the F1-score by up to 13.29 percentage points over the baseline.

Conclusion: Addressing gender disparities in AI-driven mental health assessment is crucial for fair and accurate predictions.

Abstract: Speech-based AI models are emerging as powerful tools for detecting
depression and the presence of Post-traumatic stress disorder (PTSD), offering
a non-invasive and cost-effective way to assess mental health. However, these
models often struggle with gender bias, which can lead to unfair and inaccurate
predictions. In this study, our study addresses this issue by introducing a
domain adversarial training approach that explicitly considers gender
differences in speech-based depression and PTSD detection. Specifically, we
treat different genders as distinct domains and integrate this information into
a pretrained speech foundation model. We then validate its effectiveness on the
E-DAIC dataset to assess its impact on performance. Experimental results show
that our method notably improves detection performance, increasing the F1-score
by up to 13.29 percentage points compared to the baseline. This highlights the
importance of addressing demographic disparities in AI-driven mental health
assessment.

</details>


### [206] [Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten](https://arxiv.org/pdf/2505.03369)
*Yuanyuan Yang, Yuan Shen, Tianchen Sun, Yangbin Xie*

Main category: cs.AI

TL;DR: The study proposes using LLMs and learning analytics to assess children's development during free play by analyzing their self-narratives, achieving high accuracy and revealing play setting-specific developmental differences.


<details>
  <summary>Details</summary>
Motivation: Traditional assessment methods for free play are limited in capturing comprehensive insights and providing timely feedback, prompting the need for an innovative approach.

Method: Combines LLMs and learning analytics to analyze 2,224 play narratives from 29 children across four play areas, evaluating developmental abilities.

Result: The LLM-based approach achieved over 90% accuracy in identifying cognitive, motor, and social abilities, with significant differences observed across play settings.

Conclusion: The approach effectively identifies developmental outcomes in free play, showcasing the potential of LLMs and learning analytics to enhance early childhood education.

Abstract: Free play is a fundamental aspect of early childhood education, supporting
children's cognitive, social, emotional, and motor development. However,
assessing children's development during free play poses significant challenges
due to the unstructured and spontaneous nature of the activity. Traditional
assessment methods often rely on direct observations by teachers, parents, or
researchers, which may fail to capture comprehensive insights from free play
and provide timely feedback to educators. This study proposes an innovative
approach combining Large Language Models (LLMs) with learning analytics to
analyze children's self-narratives of their play experiences. The LLM
identifies developmental abilities, while performance scores across different
play settings are calculated using learning analytics techniques. We collected
2,224 play narratives from 29 children in a kindergarten, covering four
distinct play areas over one semester. According to the evaluation results from
eight professionals, the LLM-based approach achieved high accuracy in
identifying cognitive, motor, and social abilities, with accuracy exceeding 90%
in most domains. Moreover, significant differences in developmental outcomes
were observed across play settings, highlighting each area's unique
contributions to specific abilities. These findings confirm that the proposed
approach is effective in identifying children's development across various free
play settings. This study demonstrates the potential of integrating LLMs and
learning analytics to provide child-centered insights into developmental
trajectories, offering educators valuable data to support personalized learning
and enhance early childhood education practices.

</details>


### [207] [Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents](https://arxiv.org/pdf/2505.03434)
*Schaun Wheeler, Olivier Jeunen*

Main category: cs.AI

TL;DR: LLMs excel in procedural tasks but struggle in unpredictable environments. Augmenting them with semantic memory and associative learning can enhance adaptability.


<details>
  <summary>Details</summary>
Motivation: LLMs rely on procedural memory, limiting their effectiveness in complex, shifting environments.

Method: Propose a modular architecture combining procedural, semantic memory, and associative learning.

Result: Potential to bridge the gap between narrow expertise and adaptive intelligence.

Conclusion: Augmenting LLMs with additional cognitive functions can improve real-world problem-solving.

Abstract: Large Language Models (LLMs) represent a landmark achievement in Artificial
Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks
such as text generation, code completion, and conversational coherence. These
capabilities stem from their architecture, which mirrors human procedural
memory -- the brain's ability to automate repetitive, pattern-driven tasks
through practice. However, as LLMs are increasingly deployed in real-world
applications, it becomes impossible to ignore their limitations operating in
complex, unpredictable environments. This paper argues that LLMs, while
transformative, are fundamentally constrained by their reliance on procedural
memory. To create agents capable of navigating ``wicked'' learning environments
-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must
augment LLMs with semantic memory and associative learning systems. By adopting
a modular architecture that decouples these cognitive functions, we can bridge
the gap between narrow procedural expertise and the adaptive intelligence
required for real-world problem-solving.

</details>


### [208] [The Steganographic Potentials of Language Models](https://arxiv.org/pdf/2505.03439)
*Artem Karpov, Tinuade Adeleke, Seong Hah Cho, Natalia Perez-Campanero*

Main category: cs.AI

TL;DR: LLMs can hide messages in plain text (steganography), posing detection challenges. This paper explores their steganographic abilities via RL fine-tuning, revealing enhanced concealment with algorithmic guidance.


<details>
  <summary>Details</summary>
Motivation: To understand and address the challenge of LLMs hiding messages, which undermines their reasoning faithfulness and poses risks from unaligned AI agents.

Method: Fine-tuning LLMs via reinforcement learning (RL) to develop covert encoding schemes, engage in steganography when prompted, and use it in realistic scenarios. Detection of hidden reasoning and performance evaluation are also conducted.

Result: Current models show basic steganographic abilities in security and capacity, but explicit algorithmic guidance significantly improves their information concealment.

Conclusion: LLMs can be fine-tuned for steganography, with algorithmic guidance enhancing their ability to hide information, highlighting both risks and detection challenges.

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>


### [209] [Cooperative Multi-Agent Planning with Adaptive Skill Synthesis](https://arxiv.org/pdf/2502.10148)
*Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen*

Main category: cs.AI

TL;DR: COMPASS integrates vision-language models with a dynamic skill library and structured communication for decentralized multi-agent decision-making, outperforming MARL baselines in StarCraft scenarios.


<details>
  <summary>Details</summary>
Motivation: Challenges in MARL include sample inefficiency, interpretability, and transferability. LLMs show promise but struggle with non-Markovian multi-agent interactions.

Method: COMPASS combines VLMs, a dynamic skill library, and multi-hop communication for decentralized decision-making under partial observability.

Result: COMPASS achieves a 57% win rate in Protoss 5v5, outperforming QMIX by 30 percentage points.

Conclusion: COMPASS demonstrates strong performance in MARL tasks, leveraging VLMs and adaptive strategies for improved cooperation.

Abstract: Despite much progress in training distributed artificial intelligence (AI),
building cooperative multi-agent systems with multi-agent reinforcement
learning (MARL) faces challenges in sample efficiency, interpretability, and
transferability. Unlike traditional learning-based methods that require
extensive interaction with the environment, large language models (LLMs)
demonstrate remarkable capabilities in zero-shot planning and complex
reasoning. However, existing LLM-based approaches heavily rely on text-based
observations and struggle with the non-Markovian nature of multi-agent
interactions under partial observability. We present COMPASS, a novel
multi-agent architecture that integrates vision-language models (VLMs) with a
dynamic skill library and structured communication for decentralized
closed-loop decision-making. The skill library, bootstrapped from
demonstrations, evolves via planner-guided tasks to enable adaptive strategies.
COMPASS propagates entity information through multi-hop communication under
partial observability. Evaluations on the improved StarCraft Multi-Agent
Challenge (SMACv2) demonstrate COMPASS's strong performance against
state-of-the-art MARL baselines across both symmetric and asymmetric scenarios.
Notably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\% win rate,
representing a 30 percentage point advantage over QMIX (27\%). Project page can
be found at https://stellar-entremet-1720bb.netlify.app/.

</details>


### [210] [am-ELO: A Stable Framework for Arena-based LLM Evaluation](https://arxiv.org/pdf/2505.03475)
*Zirui Liu, Jiatong Li, Yan Zhuang, Qi Liu, Shuanghong Shen, Jie Ouyang, Mingyue Cheng, Shijin Wang*

Main category: cs.AI

TL;DR: A novel stable arena framework (m-ELO and am-ELO) improves the ELO rating system for evaluating LLMs by addressing instability and annotator reliability.


<details>
  <summary>Details</summary>
Motivation: Existing ELO-based frameworks for AI model evaluation suffer from instability and ignore annotator abilities.

Method: Introduces m-ELO (MLE-based) for stable ranking and am-ELO to incorporate annotator reliability.

Result: The framework provides more robust, accurate, and stable evaluations for LLMs.

Conclusion: The proposed method enhances the stability and reliability of arena-based evaluations for modern AI models.

Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm
for modern AI models, especially large language models (LLMs). Existing
framework based on ELO rating system suffers from the inevitable instability
problem due to ranking inconsistency and the lack of attention to the varying
abilities of annotators. In this paper, we introduce a novel stable arena
framework to address these issues by enhancing the ELO Rating System.
Specifically, we replace the iterative update method with a Maximum Likelihood
Estimation (MLE) approach, m-ELO, and provide theoretical proof of the
consistency and stability of the MLE approach for model ranking. Additionally,
we proposed the am-ELO, which modify the Elo Rating's probability function to
incorporate annotator abilities, enabling the simultaneous estimation of model
scores and annotator reliability. Experiments demonstrate that this method
ensures stability, proving that this framework offers a more robust, accurate,
and stable evaluation method for LLMs.

</details>


### [211] [STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game](https://arxiv.org/pdf/2505.03547)
*Eric Zhou, Shreyas Basavatia, Moontashir Siam, Zexin Chen, Mark O. Riedl*

Main category: cs.AI

TL;DR: STORY2GAME uses LLMs to generate interactive fiction games by creating stories, populating worlds, and dynamically coding actions, ensuring open-ended play while tracking game state.


<details>
  <summary>Details</summary>
Motivation: To overcome the constraints of hard-coded actions in story generation and enable more open-ended, interactive experiences.

Method: Generates stories, actions, and game code using LLMs, with dynamic action generation for player creativity.

Result: Evaluates success based on players completing the generated story interactively.

Conclusion: STORY2GAME demonstrates effective dynamic action generation for interactive fiction, enhancing player engagement.

Abstract: We introduce STORY2GAME, a novel approach to using Large Language Models to
generate text-based interactive fiction games that starts by generating a
story, populates the world, and builds the code for actions in a game engine
that enables the story to play out interactively. Whereas a given set of
hard-coded actions can artificially constrain story generation, the ability to
generate actions means the story generation process can be more open-ended but
still allow for experiences that are grounded in a game state. The key to
successful action generation is to use LLM-generated preconditions and effects
of actions in the stories as guides for what aspects of the game state must be
tracked and changed by the game engine when a player performs an action. We
also introduce a technique for dynamically generating new actions to
accommodate the player's desire to perform actions that they think of that are
not part of the story. Dynamic action generation may require on-the-fly updates
to the game engine's state representation and revision of previously generated
actions. We evaluate the success rate of action code generation with respect to
whether a player can interactively play through the entire generated story.

</details>


### [212] [A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning](https://arxiv.org/pdf/2505.03553)
*Kolawole E. Ogunsina, Morayo A. Ogunsina*

Main category: cs.AI

TL;DR: A novel consensus mechanism inspired by Hashgraph is proposed to align outputs of diverse LLMs, reducing inconsistencies and hallucinations by treating each model as a peer in a distributed system.


<details>
  <summary>Details</summary>
Motivation: Addressing divergent outputs from proprietary LLMs due to training and inference variations, aiming for reliable AI systems.

Method: Uses Hashgraph's gossip-about-gossip and virtual voting for iterative consensus among models, improving accuracy through cross-verification.

Result: Preliminary findings suggest the approach reduces nonfactual outputs and enhances response fidelity in multi-agent AI systems.

Conclusion: The Hashgraph-inspired consensus offers a promising path for self-validation and high-fidelity responses in complex AI tasks.

Abstract: Inconsistent outputs and hallucinations from large language models (LLMs) are
major obstacles to reliable AI systems. When different proprietary reasoning
models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,
are given the same complex request, they often produce divergent results due to
variations in training and inference. This paper proposes a novel consensus
mechanism, inspired by distributed ledger technology, to validate and converge
these outputs, treating each RM as a black-box peer. Building on the Hashgraph
consensus algorithm, our approach employs gossip-about-gossip communication and
virtual voting to achieve agreement among an ensemble of RMs. We present an
architectural design for a prototype system in which RMs iteratively exchange
and update their answers, using information from each round to improve accuracy
and confidence in subsequent rounds. This approach goes beyond simple majority
voting by incorporating the knowledge and cross-verification content of every
model. We justify the feasibility of this Hashgraph-inspired consensus for AI
ensembles and outline its advantages over traditional ensembling techniques in
reducing nonfactual outputs. Preliminary considerations for implementation,
evaluation criteria for convergence and accuracy, and potential challenges are
discussed. The proposed mechanism demonstrates a promising direction for
multi-agent AI systems to self-validate and deliver high-fidelity responses in
complex tasks.

</details>


### [213] [LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications](https://arxiv.org/pdf/2503.02950)
*Danqing Zhang, Balaji Rama, Jingyi Ni, Shiying He, Fu Zhao, Kunyu Chen, Arnold Chen, Junyu Cao*

Main category: cs.AI

TL;DR: LiteWebAgent is an open-source framework for VLM-based web agents, offering a production-ready solution with minimal backend setup, user-friendly interfaces, and extensible research features like planning and tree search.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of a production-ready, extensible web agent framework in the ecosystem.

Method: Uses recursive function calling for action generation and grounding, with modular integration of advanced components like planning and tree search.

Result: Deployed as a Vercel web app and Chrome extension, enabling remote browser control via LiteWebAgent's API.

Conclusion: LiteWebAgent provides a practical, extensible solution for web agent applications, available as open-source with deployed systems.

Abstract: We introduce LiteWebAgent, an open-source suite for VLM-based web agent
applications. Our framework addresses a critical gap in the web agent ecosystem
with a production-ready solution that combines minimal serverless backend
configuration, intuitive user and browser interfaces, and extensible research
capabilities in agent planning, memory, and tree search. For the core
LiteWebAgent agent framework, we implemented a simple yet effective baseline
using recursive function calling, providing with decoupled action generation
and action grounding. In addition, we integrate advanced research components
such as agent planning, agent workflow memory, and tree search in a modular and
extensible manner. We then integrate the LiteWebAgent agent framework with
frontend and backend as deployed systems in two formats: (1) a production
Vercel-based web application, which provides users with an agent-controlled
remote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control
an existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent
framework is available at https://github.com/PathOnAI/LiteWebAgent, with
deployed frontend at https://lite-web-agent.vercel.app/.

</details>


### [214] [OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents](https://arxiv.org/pdf/2505.03570)
*Mariya Davydova, Daniel Jeffries, Patrick Barker, Arturo Márquez Flores, Sinéad Ryan*

Main category: cs.AI

TL;DR: OSUniverse is a benchmark for GUI-navigation AI agents, featuring tasks of increasing complexity, automated validation, and calibration to challenge SOTA agents while being solvable by humans.


<details>
  <summary>Details</summary>
Motivation: To provide a robust, extensible, and automated benchmark for evaluating the progress and capabilities of GUI-navigation AI agents.

Method: Tasks are divided by complexity, from basic clicking to multistep, multiapplication tests. Automated validation ensures accuracy.

Result: SOTA agents score ≤50%, while humans achieve perfect accuracy. Automated validation has <2% error rate.

Conclusion: OSUniverse offers a reliable, automated measure for assessing GUI-navigation AI agents' effectiveness.

Abstract: In this paper, we introduce OSUniverse: a benchmark of complex, multimodal
desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on
ease of use, extensibility, comprehensive coverage of test cases, and automated
validation. We divide the tasks in increasing levels of complexity, from basic
precision clicking to multistep, multiapplication tests requiring dexterity,
precision, and clear thinking from the agent. In version one of the benchmark,
presented here, we have calibrated the complexity of the benchmark test cases
to ensure that the SOTA (State of the Art) agents (at the time of publication)
do not achieve results higher than 50%, while the average white collar worker
can perform all these tasks with perfect accuracy. The benchmark can be scored
manually, but we also introduce an automated validation mechanism that has an
average error rate less than 2%. Therefore, this benchmark presents solid
ground for fully automated measuring of progress, capabilities and the
effectiveness of GUI-navigation AI agents over the short and medium-term
horizon. The source code of the benchmark is available at
https://github.com/agentsea/osuniverse.

</details>


### [215] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability](https://arxiv.org/pdf/2505.03641)
*Chen Wei, Chi Zhang, Jiachen Zou, Haotian Deng, Dietmar Heinke, Quanying Liu*

Main category: cs.AI

TL;DR: The paper introduces BAM, a computational framework combining ANN perceptual boundary sampling and human experiments to study decision-making variability, validated by large-scale behavioral data.


<details>
  <summary>Details</summary>
Motivation: To understand human decision-making variability under uncertainty and ambiguity, bridging computational models and individual differences.

Method: BAM framework integrates ANN boundary sampling and human experiments, using personalized model alignment and adversarial generation.

Result: Validated with 246 participants (116,715 trials), creating the variMNIST dataset (19,943 images), enabling prediction and manipulation of perceptual decisions.

Conclusion: BAM bridges computational models and human research, offering tools for personalized perception analysis.

Abstract: Human decision-making in cognitive tasks and daily life exhibits considerable
variability, shaped by factors such as task difficulty, individual preferences,
and personal experiences. Understanding this variability across individuals is
essential for uncovering the perceptual and decision-making mechanisms that
humans rely on when faced with uncertainty and ambiguity. We present a
computational framework BAM (Boundary Alignment & Manipulation framework) that
combines perceptual boundary sampling in ANNs and human behavioral experiments
to systematically investigate this phenomenon. Our perceptual boundary sampling
algorithm generates stimuli along ANN decision boundaries that intrinsically
induce significant perceptual variability. The efficacy of these stimuli is
empirically validated through large-scale behavioral experiments involving 246
participants across 116,715 trials, culminating in the variMNIST dataset
containing 19,943 systematically annotated images. Through personalized model
alignment and adversarial generation, we establish a reliable method for
simultaneously predicting and manipulating the divergent perceptual decisions
of pairs of participants. This work bridges the gap between computational
models and human individual difference research, providing new tools for
personalized perception analysis.

</details>


### [216] [BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems](https://arxiv.org/pdf/2505.03643)
*Chelsea Sidrane, Jana Tumova*

Main category: cs.AI

TL;DR: An algorithm for computing underapproximate backward reachable sets in neural feedback loops is introduced, enabling goal-reaching property verification via mixed-integer linear programs.


<details>
  <summary>Details</summary>
Motivation: Learning-enabled systems lack rigorous guarantees of performance or safety, necessitating methods to verify their properties.

Method: The algorithm overapproximates system dynamics to compute underapproximate backward reachable sets using mixed-integer linear programs.

Result: The algorithm is rigorously analyzed for soundness and demonstrated on a numerical example, expanding verifiable properties for learning-enabled systems.

Conclusion: This work enhances the verification capabilities for learning-enabled systems by introducing a novel reachability analysis method.

Abstract: Learning-enabled planning and control algorithms are increasingly popular,
but they often lack rigorous guarantees of performance or safety. We introduce
an algorithm for computing underapproximate backward reachable sets of
nonlinear discrete time neural feedback loops. We then use the backward
reachable sets to check goal-reaching properties. Our algorithm is based on
overapproximating the system dynamics function to enable computation of
underapproximate backward reachable sets through solutions of mixed-integer
linear programs. We rigorously analyze the soundness of our algorithm and
demonstrate it on a numerical example. Our work expands the class of properties
that can be verified for learning-enabled systems.

</details>


### [217] [Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time](https://arxiv.org/pdf/2505.03668)
*Celeste Veronese, Daniele Meli, Alessandro Farinelli*

Main category: cs.AI

TL;DR: The paper integrates temporal logic (LTL) with POMDPs to create interpretable macro-actions, improving efficiency and performance in decision-making under uncertainty.


<details>
  <summary>Details</summary>
Motivation: To achieve interpretable and efficient decision-making under uncertainty by combining temporal logic reasoning with POMDPs, avoiding manual heuristics.

Method: Uses Linear Temporal Logic (LTL) and Event Calculus (EC) to generate persistent macro-actions, learned via Inductive Logic Programming (ILP) from belief-action traces, and integrates them with MCTS-based POMDP solvers.

Result: Demonstrates improved expressiveness, generality, and computational efficiency in Pocman and Rocksample benchmarks compared to time-independent heuristics.

Conclusion: The integration of temporal logic and POMDPs with learned macro-actions offers a robust and efficient solution for decision-making under uncertainty.

Abstract: This paper proposes an integration of temporal logical reasoning and
Partially Observable Markov Decision Processes (POMDPs) to achieve
interpretable decision-making under uncertainty with macro-actions. Our method
leverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus
(EC) to generate \emph{persistent} (i.e., constant) macro-actions, which guide
Monte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,
significantly reducing inference time while ensuring robust performance. Such
macro-actions are learnt via Inductive Logic Programming (ILP) from a few
traces of execution (belief-action pairs), thus eliminating the need for
manually designed heuristics and requiring only the specification of the POMDP
transition model. In the Pocman and Rocksample benchmark scenarios, our learned
macro-actions demonstrate increased expressiveness and generality when compared
to time-independent heuristics, indeed offering substantial computational
efficiency improvements.

</details>


### [218] [Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance](https://arxiv.org/pdf/2505.03674)
*Yotam Amitai, Reuth Mirsky, Ofra Amir*

Main category: cs.AI

TL;DR: Goal-sharing by AI agents in human-agent teams doesn't significantly improve task performance but enhances perceived collaboration and trust, with no added cognitive load.


<details>
  <summary>Details</summary>
Motivation: To explore whether AI agents sharing inferred human goals improves collaboration and task performance, given the challenges of direct goal communication.

Method: An experiment comparing three conditions: no recognition (NR), viable goals (VG), and viable goals on-demand (VGod), measuring task performance, satisfaction, cognitive load, and thematic analysis.

Result: No significant task performance or satisfaction improvements, but goal-sharing supported strategic adaptations and perceived collaboration. Cognitive load remained unchanged.

Conclusion: Goal-sharing balances trust and perceived collaboration but may not yield objective performance gains, highlighting a nuanced trade-off.

Abstract: In human-agent teams, openly sharing goals is often assumed to enhance
planning, collaboration, and effectiveness. However, direct communication of
these goals is not always feasible, requiring teammates to infer their
partner's intentions through actions. Building on this, we investigate whether
an AI agent's ability to share its inferred understanding of a human teammate's
goals can improve task performance and perceived collaboration. Through an
experiment comparing three conditions-no recognition (NR), viable goals (VG),
and viable goals on-demand (VGod) - we find that while goal-sharing information
did not yield significant improvements in task performance or overall
satisfaction scores, thematic analysis suggests that it supported strategic
adaptations and subjective perceptions of collaboration. Cognitive load
assessments revealed no additional burden across conditions, highlighting the
challenge of balancing informativeness and simplicity in human-agent
interactions. These findings highlight the nuanced trade-off of goal-sharing:
while it fosters trust and enhances perceived collaboration, it can
occasionally hinder objective performance gains.

</details>


### [219] [Graph Drawing for LLMs: An Empirical Evaluation](https://arxiv.org/pdf/2505.03678)
*Walter Didimo, Fabrizio Montecchiani, Tommaso Piselli*

Main category: cs.AI

TL;DR: The paper explores how layout paradigms, drawing aesthetics, and prompting techniques affect LLM performance in graph-related tasks using visual inputs.


<details>
  <summary>Details</summary>
Motivation: To understand how visual inputs (graph drawings) impact LLM performance and identify factors that enhance task accuracy.

Method: Investigates three research questions through experimental analysis, focusing on layout paradigms, drawing aesthetics, and prompting techniques.

Result: Optimal layout and human-readable drawings boost performance; effective prompting is critical but challenging.

Conclusion: Visual input quality and prompting techniques significantly influence LLM performance in graph tasks.

Abstract: Our work contributes to the fast-growing literature on the use of Large
Language Models (LLMs) to perform graph-related tasks. In particular, we focus
on usage scenarios that rely on the visual modality, feeding the model with a
drawing of the graph under analysis. We investigate how the model's performance
is affected by the chosen layout paradigm, the aesthetics of the drawing, and
the prompting technique used for the queries. We formulate three corresponding
research questions and present the results of a thorough experimental analysis.
Our findings reveal that choosing the right layout paradigm and optimizing the
readability of the input drawing from a human perspective can significantly
improve the performance of the model on the given task. Moreover, selecting the
most effective prompting technique is a challenging yet crucial task for
achieving optimal performance.

</details>


### [220] [ParFam -- (Neural Guided) Symbolic Regression Based on Continuous Global Optimization](https://arxiv.org/pdf/2310.05537)
*Philipp Scholl, Katharina Bieker, Hillary Hauger, Gitta Kutyniok*

Main category: cs.AI

TL;DR: ParFam simplifies symbolic regression by using parametric families of functions, achieving state-of-the-art results and accelerating optimization with a transformer network.


<details>
  <summary>Details</summary>
Motivation: Symbolic regression (SR) is complex with existing methods often involving hyperparameters and genetic programming. ParFam aims to simplify this by transforming the problem into a continuous one.

Method: ParFam uses parametric families of symbolic functions to convert SR into a continuous problem, combined with a global optimizer. An extension, DL-ParFam, incorporates a pre-trained transformer for faster optimization.

Result: ParFam achieves state-of-the-art performance on SRBench benchmarks, with DL-ParFam speeding up optimization by up to two magnitudes.

Conclusion: ParFam offers a simpler, more effective approach to SR, with potential for further improvements through integration of deep learning techniques.

Abstract: The problem of symbolic regression (SR) arises in many different
applications, such as identifying physical laws or deriving mathematical
equations describing the behavior of financial markets from given data. Various
methods exist to address the problem of SR, often based on genetic programming.
However, these methods are usually complicated and involve various
hyperparameters. In this paper, we present our new approach ParFam that
utilizes parametric families of suitable symbolic functions to translate the
discrete symbolic regression problem into a continuous one, resulting in a more
straightforward setup compared to current state-of-the-art methods. In
combination with a global optimizer, this approach results in a highly
effective method to tackle the problem of SR. We theoretically analyze the
expressivity of ParFam and demonstrate its performance with extensive numerical
experiments based on the common SR benchmark suit SRBench, showing that we
achieve state-of-the-art results. Moreover, we present an extension
incorporating a pre-trained transformer network DL-ParFam to guide ParFam,
accelerating the optimization process by up to two magnitudes. Our code and
results can be found at https://github.com/Philipp238/parfam.

</details>


### [221] [The Adaptive Arms Race: Redefining Robustness in AI Security](https://arxiv.org/pdf/2312.13435)
*Ilias Tsingenopoulos, Vera Rimmer, Davy Preuveneers, Fabio Pierazzi, Lorenzo Cavallaro, Wouter Joosen*

Main category: cs.AI

TL;DR: The paper introduces a framework for adaptive attacks and defenses in AI systems, using mutual learning and RL to enhance robustness against decision-based attacks. It highlights the necessity of adaptive defenses and outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Real-world AI systems remain vulnerable to decision-based attacks, and existing robustness evaluations are insufficient. The paper aims to address this by broadening the concept of adaptivity for both attacks and defenses.

Method: The framework employs adaptive optimization for black-box attacks and defenses, using RL to enhance their capabilities. It evaluates them independently and jointly under a multi-agent perspective.

Result: Active defenses are essential against decision-based attacks, but adaptive attacks can circumvent them, necessitating adaptive defenses. The approach outperforms state-of-the-art methods.

Conclusion: Adaptive adversaries pose a serious threat, reigniting the arms race in AI security. The framework provides effective insights into robustness for real-world ML systems.

Abstract: Despite considerable efforts on making them robust, real-world AI-based
systems remain vulnerable to decision based attacks, as definitive proofs of
their operational robustness have so far proven intractable. Canonical
robustness evaluation relies on adaptive attacks, which leverage complete
knowledge of the defense and are tailored to bypass it. This work broadens the
notion of adaptivity, which we employ to enhance both attacks and defenses,
showing how they can benefit from mutual learning through interaction. We
introduce a framework for adaptively optimizing black-box attacks and defenses
under the competitive game they form. To assess robustness reliably, it is
essential to evaluate against realistic and worst-case attacks. We thus enhance
attacks and their evasive arsenal together using RL, apply the same principle
to defenses, and evaluate them first independently and then jointly under a
multi-agent perspective. We find that active defenses, those that dynamically
control system responses, are an essential complement to model hardening
against decision-based attacks; that these defenses can be circumvented by
adaptive attacks, something that elicits defenses being adaptive too. Our
findings, supported by an extensive theoretical and empirical investigation,
confirm that adaptive adversaries pose a serious threat to black-box AI-based
systems, rekindling the proverbial arms race. Notably, our approach outperforms
the state-of-the-art black-box attacks and defenses, while bringing them
together to render effective insights into the robustness of real-world
deployed ML-based systems.

</details>


### [222] [Evaluating Fairness Metrics Across Borders from Human Perceptions](https://arxiv.org/pdf/2403.16101)
*Yuya Sasaki, Sohei Tokuno, Haruka Maeda, Kazuki Nakajima, Osamu Sakura, George Fletcher, Mykola Pechenizkiy, Panagiotis Karras, Irina Shklovski*

Main category: cs.AI

TL;DR: An international survey evaluates public perceptions of fairness metrics across four countries, revealing national context significantly influences preferences.


<details>
  <summary>Details</summary>
Motivation: To address the discordance between fairness metrics and human perceptions, and expand beyond limited-scope surveys.

Method: Conducted an international survey with 4,000 participants (1,000 each from China, France, Japan, and the US) across three scenarios and four fairness metrics.

Result: National context significantly influences preferences for fairness metrics.

Conclusion: Fairness perceptions vary by national context, highlighting the need for culturally aware fairness metrics.

Abstract: Which fairness metrics are appropriately applicable in your contexts? There
may be instances of discordance regarding the perception of fairness, even when
the outcomes comply with established fairness metrics. Several
questionnaire-based surveys have been conducted to evaluate fairness metrics
with human perceptions of fairness. However, these surveys were limited in
scope, including only a few hundred participants within a single country. In
this study, we conduct an international survey to evaluate public perceptions
of various fairness metrics in decision-making scenarios. We collected
responses from 1,000 participants in each of China, France, Japan, and the
United States, amassing a total of 4,000 participants, to analyze the
preferences of fairness metrics. Our survey consists of three distinct
scenarios paired with four fairness metrics. This investigation explores the
relationship between personal attributes and the choice of fairness metrics,
uncovering a significant influence of national context on these preferences.

</details>


### [223] [Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice](https://arxiv.org/pdf/2405.19313)
*Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths*

Main category: cs.AI

TL;DR: The paper explores using LLMs as cognitive models by focusing on computationally equivalent tasks and ecologically valid datasets, showing improved prediction of human behavior in decision-making.


<details>
  <summary>Details</summary>
Motivation: To address challenges in using LLMs as cognitive models due to unclear origins of behavioral similarities and excessive training data.

Method: Proposes leveraging computationally equivalent tasks and ecologically valid datasets, applied to decision-making (risky and intertemporal choice).

Result: LLMs pretrained on ecologically valid arithmetic datasets (Arithmetic-GPT) predict human behavior better than traditional cognitive models.

Conclusion: LLMs can enhance cognitive modeling with careful pretraining data selection and ablation studies.

Abstract: The observed similarities in the behavior of humans and Large Language Models
(LLMs) have prompted researchers to consider the potential of using LLMs as
models of human cognition. However, several significant challenges must be
addressed before LLMs can be legitimately regarded as cognitive models. For
instance, LLMs are trained on far more data than humans typically encounter,
and may have been directly trained on human data in specific cognitive tasks or
aligned with human preferences. Consequently, the origins of these behavioral
similarities are not well understood. In this paper, we propose a novel way to
enhance the utility of LLMs as cognitive models. This approach involves (i)
leveraging computationally equivalent tasks that both an LLM and a rational
agent need to master for solving a cognitive problem and (ii) examining the
specific task distributions required for an LLM to exhibit human-like
behaviors. We apply this approach to decision-making -- specifically risky and
intertemporal choice -- where the key computationally equivalent task is the
arithmetic of expected value calculations. We show that an LLM pretrained on an
ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts
human behavior better than many traditional cognitive models. Pretraining LLMs
on ecologically valid arithmetic datasets is sufficient to produce a strong
correspondence between these models and human decision-making. Our results also
suggest that LLMs used as cognitive models should be carefully investigated via
ablation studies of the pretraining data.

</details>


### [224] [The Evolution of Reinforcement Learning in Quantitative Finance: A Survey](https://arxiv.org/pdf/2408.10932)
*Nikolaos Pippas, Elliot A. Ludvig, Cagatay Turkay*

Main category: cs.AI

TL;DR: A survey of 167 publications evaluates RL applications in finance, highlighting its dynamic approach and potential in complex financial markets.


<details>
  <summary>Details</summary>
Motivation: To explore and critique the use of RL in finance, given the field's complexity and suitability for RL methods.

Method: Critical evaluation of 167 publications, analyzing RL frameworks, applications, and emerging themes in finance.

Result: Identifies strengths and weaknesses of RL in finance, proposes future research areas, and highlights dynamic advancements over traditional methods.

Conclusion: RL shows promise in finance but requires further research to address limitations and leverage emerging techniques like transfer and meta-learning.

Abstract: Reinforcement Learning (RL) has experienced significant advancement over the
past decade, prompting a growing interest in applications within finance. This
survey critically evaluates 167 publications, exploring diverse RL applications
and frameworks in finance. Financial markets, marked by their complexity,
multi-agent nature, information asymmetry, and inherent randomness, serve as an
intriguing test-bed for RL. Traditional finance offers certain solutions, and
RL advances these with a more dynamic approach, incorporating machine learning
methods, including transfer learning, meta-learning, and multi-agent solutions.
This survey dissects key RL components through the lens of Quantitative
Finance. We uncover emerging themes, propose areas for future research, and
critique the strengths and weaknesses of existing methods.

</details>


### [225] [Automating Traffic Model Enhancement with AI Research Agent](https://arxiv.org/pdf/2409.16876)
*Xusen Guo, Xinxi Yang, Mingxing Peng, Hongliang Lu, Meixin Zhu, Hai Yang*

Main category: cs.AI

TL;DR: TR-Agent is an AI framework that autonomously develops and refines traffic models, improving efficiency and performance over traditional manual methods.


<details>
  <summary>Details</summary>
Motivation: Current traffic modeling is labor-intensive and error-prone due to manual workflows, necessitating an automated solution.

Method: TR-Agent uses a closed-loop, iterative process with four modules: idea generation, theory formulation, evaluation, and optimization.

Result: The framework outperforms original models (IDM, MOBIL, LWR) and shows consistent gains across real-world datasets.

Conclusion: TR-Agent enhances traffic modeling efficiency and effectiveness, offering interpretable improvements for broader transportation research.

Abstract: Developing efficient traffic models is crucial for optimizing modern
transportation systems. However, current modeling approaches remain
labor-intensive and prone to human errors due to their dependence on manual
workflows. These processes typically involve extensive literature reviews,
formula tuning, and iterative testing, which often lead to inefficiencies. To
address this, we propose TR-Agent, an AI-powered framework that autonomously
develops and refines traffic models through a closed-loop, iterative process.
We structure the research pipeline into four key stages: idea generation,
theory formulation, theory evaluation, and iterative optimization, and
implement TR-Agent with four corresponding modules. These modules collaborate
to retrieve knowledge from external sources, generate novel hypotheses,
implement and debug models, and evaluate their performance on evaluation
datasets. Through iteratively feedback and refinement, TR-Agent improves both
modeling efficiency and effectiveness. We validate the framework on three
representative traffic models: the Intelligent Driver Model (IDM) for
car-following behavior, the MOBIL model for lane-changing, and the
Lighthill-Whitham-Richards (LWR) speed-density relationship for macroscopic
traffic flow modeling. Experimental results show substantial performance gains
over the original models. To assess the robustness and generalizability of the
improvements, we conduct additional evaluations across multiple real-world
datasets, demonstrating consistent performance gains beyond the original
development data. Furthermore, TR-Agent produces interpretable explanations for
each improvement, enabling researchers to easily verify and extend its results.
This makes TR-Agent a valuable assistant for traffic modeling refinement and a
promising tool for broader applications in transportation research.

</details>


### [226] [Uncertainty-aware Human Mobility Modeling and Anomaly Detection](https://arxiv.org/pdf/2410.01281)
*Haomin Wen, Shurui Cao, Zeeshan Rasheed, Khurram Hassan Shafique, Leman Akoglu*

Main category: cs.AI

TL;DR: USTAD models human mobility using GPS data as sequences of stay-points and trips, leveraging uncertainty-aware sequence models for unsupervised anomaly detection, outperforming baselines by 3%-15% in AUCROC.


<details>
  <summary>Details</summary>
Motivation: To detect anomalies (e.g., malicious behavior) in human mobility without labeled data by modeling raw GPS data as sequences of stay-points and trips.

Method: Formulates mobility as sequences of stay-points and trips, using USTAD with aleatoric and epistemic uncertainty estimation for robust anomaly detection.

Result: USTAD improves anomaly detection AUCROC by 3%-15% over baselines on industry-scale data.

Conclusion: USTAD effectively detects anomalies in human mobility by modeling uncertainty, demonstrating significant performance gains.

Abstract: Given the temporal GPS coordinates from a large set of human agents, how can
we model their mobility behavior toward effective anomaly (e.g. bad-actor or
malicious behavior) detection without any labeled data? Human mobility and
trajectory modeling have been extensively studied, showcasing varying abilities
to manage complex inputs and balance performance-efficiency trade-offs. In this
work, we formulate anomaly detection in complex human behavior by modeling raw
GPS data as a sequence of stay-point events, each characterized by
spatio-temporal features, along with trips (i.e. commute) between the
stay-points. Our problem formulation allows us to leverage modern sequence
models for unsupervised training and anomaly detection. Notably, we equip our
proposed model USTAD (for Uncertainty-aware Spatio-Temporal Anomaly Detection)
with aleatoric (i.e. data) uncertainty estimation to account for inherent
stochasticity in certain individuals' behavior, as well as epistemic (i.e.
model) uncertainty to handle data sparsity under a large variety of human
behaviors. Together, aleatoric and epistemic uncertainties unlock a robust loss
function as well as uncertainty-aware decision-making in anomaly scoring.
Extensive experiments shows that USTAD improves anomaly detection AUCROC by
3\%-15\% over baselines in industry-scale data.

</details>


### [227] [FastRM: An efficient and automatic explainability framework for multimodal generative models](https://arxiv.org/pdf/2412.01487)
*Gabriela Ben-Melech Stan, Estelle Aflalo, Man Luo, Shachar Rosenman, Tiep Le, Sayak Paul, Shao-Yen Tseng, Vasudev Lal*

Main category: cs.AI

TL;DR: FastRM is an efficient method for generating explainable Relevancy Maps for Large Vision Language Models (LVLMs), reducing computation time by 99.8% and memory usage by 44.4% compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: LVLMs generate misinformation, and traditional explainability methods are computationally expensive, making real-time validation impractical.

Method: Introduces FastRM, a method for predicting explainable Relevancy Maps with quantitative and qualitative confidence assessment.

Result: FastRM significantly reduces computation time (99.8%) and memory footprint (44.4%) while maintaining explainability.

Conclusion: FastRM makes explainable AI more practical and scalable, enhancing trustworthiness and usability in real-world applications.

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning
capabilities over textual and visual inputs. However, these models remain prone
to generating misinformation. Identifying and mitigating ungrounded responses
is crucial for developing trustworthy AI. Traditional explainability methods
such as gradient-based relevancy maps, offer insight into the decision process
of models, but are often computationally expensive and unsuitable for real-time
output validation. In this work, we introduce FastRM, an efficient method for
predicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides
both quantitative and qualitative assessment of model confidence. Experimental
results demonstrate that FastRM achieves a 99.8% reduction in computation time
and a 44.4% reduction in memory footprint compared to traditional relevancy map
generation. FastRM allows explainable AI to be more practical and scalable,
thereby promoting its deployment in real-world applications and enabling users
to more effectively evaluate the reliability of model outputs.

</details>


### [228] [Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization](https://arxiv.org/pdf/2412.17739)
*Ermo Hua, Che Jiang, Xingtai Lv, Kaiyan Zhang, Ning Ding, Youbang Sun, Biqing Qi, Yuchen Fan, Xuekai Zhu, Bowen Zhou*

Main category: cs.AI

TL;DR: The paper introduces Fourier Position Embedding (FoPE) to address limitations of Rotary Position Embedding (RoPE) in Language Models, improving length generalization and periodic attention.


<details>
  <summary>Details</summary>
Motivation: Existing works focus on RoPE's limitations within attention, but this paper analyzes its adverse effects across all LM parts, identifying spectral damage as a key issue.

Method: Using Discrete Signal Processing, the authors show RoPE's periodic attention is undermined by spectral damage. They propose FoPE, which enhances frequency-domain properties by zeroing destructive components.

Result: FoPE outperforms RoPE and ALiBi in stability across varying context windows, supported by experiments and analyses.

Conclusion: FoPE effectively mitigates spectral damage, improving LM performance and generalization, with empirical validation.

Abstract: Extending the context length of Language Models (LMs) by improving Rotary
Position Embedding (RoPE) has become a trend. While existing works mainly
address RoPE's limitations within attention mechanism, this paper provides an
analysis across nearly all parts of LMs, uncovering their adverse effects on
length generalization for RoPE-based attention. Using Discrete Signal
Processing theory, we show that RoPE enables periodic attention by implicitly
achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is
undermined by the spectral damage caused by: 1) linear layers and activation
functions outside of attention; 2) insufficiently trained frequency components
brought by time-domain truncation. Building on our observations, we propose
Fourier Position Embedding (FoPE), which enhances attention's frequency-domain
properties to improve both its periodic extension and length generalization.
FoPE constructs Fourier Series and zero-outs the destructive frequency
components, increasing model robustness against the spectrum damage.
Experiments across various model scales and benchmarks show that, within
varying context windows, FoPE maintains a more stable performance compared to
RoPE and ALiBi. Several analyses and ablations bring further support to our
method and theoretical modeling.

</details>


### [229] [AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation](https://arxiv.org/pdf/2412.18116)
*Hao Wen, Shizuo Tian, Borislav Pavlov, Wenjie Du, Yixuan Li, Ge Chang, Shanhui Zhao, Jiacheng Liu, Yunxin Liu, Ya-Qin Zhang, Yuanchun Li*

Main category: cs.AI

TL;DR: The paper proposes converting mobile UI task automation into a code generation problem using small language models (SLMs) for on-device execution, improving privacy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing UI agents rely on large language models (LLMs), which are hard to deploy locally, raising privacy and cost concerns. SLMs offer a viable alternative.

Method: The approach converts UI automation into code generation, using an on-device SLM and interpreter. It builds fine-grained API documentation for apps and generates diverse task samples for training.

Result: The method outperforms state-of-the-art UI agents with higher success rates, lower latency, and reduced token consumption.

Conclusion: The proposed approach enables efficient, privacy-preserving mobile task automation using SLMs, with open-sourced code available.

Abstract: Large language models (LLMs) have brought exciting new advances to mobile UI
agents, a long-standing research field that aims to complete arbitrary natural
language tasks through mobile UI interactions. However, existing UI agents
usually demand powerful large language models that are difficult to be deployed
locally on end-users' devices, raising huge concerns about user privacy and
centralized serving cost. Inspired by the remarkable coding abilities of recent
small language models (SLMs), we propose to convert the UI task automation
problem to a code generation problem, which can be effectively solved by an
on-device SLM and efficiently executed with an on-device code interpreter.
Unlike normal coding tasks that can be extensively pre-trained with public
datasets, generating UI automation code is challenging due to the diversity,
complexity, and variability of target apps. Therefore, we adopt a
document-centered approach that automatically builds fine-grained API
documentation for each app and generates diverse task samples based on this
documentation. By guiding the agent with the synthetic documents and task
samples, it learns to generate precise and efficient scripts to complete unseen
tasks. Based on detailed comparisons with state-of-the-art mobile UI agents,
our approach effectively improves the mobile task automation with significantly
higher success rates and lower latency/token consumption. Code is open-sourced
at https://github.com/MobileLLM/AutoDroid-V2.

</details>


### [230] [Synergizing Large Language Models and Task-specific Models for Time Series Anomaly Detection](https://arxiv.org/pdf/2501.05675)
*Feiyi Chen, Leilei Zhang, Guansong Pang, Roger Zimmermann, Shuiguang Deng*

Main category: cs.AI

TL;DR: CoLLaTe is a framework combining LLMs and task-specific models for anomaly detection, addressing misalignment and error accumulation with alignment modules and collaborative loss functions.


<details>
  <summary>Details</summary>
Motivation: Inspired by the human nervous system, the paper aims to leverage the strengths of both LLMs (expert knowledge) and task-specific models (data pattern extraction) for better anomaly detection.

Method: Proposes CoLLaTe with a model alignment module and collaborative loss function to tackle misalignment and error accumulation between LLMs and task-specific models.

Result: Theoretical and experimental validation shows CoLLaTe outperforms standalone LLM-based and task-specific models.

Conclusion: CoLLaTe effectively integrates LLMs and task-specific models, improving anomaly detection by addressing collaboration challenges.

Abstract: In anomaly detection, methods based on large language models (LLMs) can
incorporate expert knowledge by reading professional document, while
task-specific small models excel at extracting normal data patterns and
detecting value fluctuations from training data of target applications.
Inspired by the human nervous system, where the brain stores expert knowledge
and the peripheral nervous system and spinal cord handle specific tasks like
withdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to
facilitate collaboration between LLMs and task-specific models, leveraging the
strengths of both models for anomaly detection.
  In particular, we first formulate the collaboration process and identify two
key challenges in the collaboration:
  (1) the misalignment between the expression domains of the LLMs and
task-specific small models, and (2) error accumulation arising from the
predictions of both models.
  To address these challenges, we then introduce two key components in CoLLaTe:
a model alignment module and a collaborative loss function. Through theoretical
analysis and experimental validation, we demonstrate that these components
effectively mitigate the identified challenges and achieve better performance
than both LLM-based and task-specific models.

</details>


### [231] [A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study](https://arxiv.org/pdf/2504.18604)
*Xingyu Xiao, Peng Chen, Jiejuan Tong, Shunshun Liu, Hongru Zhao, Jun Zhao, Qianqian Jia, Jingang Liang, Haitao Wang*

Main category: cs.AI

TL;DR: The paper introduces COGMIF, a cognitive-mechanistic framework enhancing IDHEAS-ECA by integrating ACT-R-based human digital twins and TimeGAN-augmented simulations for scalable, mechanism-informed human error probability estimation.


<details>
  <summary>Details</summary>
Motivation: Traditional HRA methods lack cognitive underpinnings and face impracticalities in human-in-the-loop experiments for advanced nuclear plants.

Method: COGMIF combines ACT-R-based human digital twins (simulating cognition) with TimeGAN-augmented synthetic data to enhance IDHEAS-ECA assessments.

Result: Comparative analyses show COGMIF's robustness and advantages over SPAR-H, with Bayesian network mapping revealing key operational risk drivers.

Conclusion: COGMIF provides a credible, efficient way to integrate cognitive theory into industrial HRA practices.

Abstract: Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA,
rely on expert judgment and empirical rules that often overlook the cognitive
underpinnings of human error. Moreover, conducting human-in-the-loop
experiments for advanced nuclear power plants is increasingly impractical due
to novel interfaces and limited operational data. This study proposes a
cognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA
methodology by integrating an ACT-R-based human digital twin (HDT) with
TimeGAN-augmented simulation. The ACT-R model simulates operator cognition,
including memory retrieval, goal-directed procedural reasoning, and
perceptual-motor execution, under high-fidelity scenarios derived from a
high-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource
constraints of large-scale cognitive modeling, TimeGAN is trained on
ACT-R-generated time-series data to produce high-fidelity synthetic operator
behavior datasets. These simulations are then used to drive IDHEAS-ECA
assessments, enabling scalable, mechanism-informed estimation of human error
probabilities (HEPs). Comparative analyses with SPAR-H and sensitivity
assessments demonstrate the robustness and practical advantages of the proposed
COGMIF. Finally, procedural features are mapped onto a Bayesian network to
quantify the influence of contributing factors, revealing key drivers of
operational risk. This work offers a credible and computationally efficient
pathway to integrate cognitive theory into industrial HRA practices.

</details>


### [232] [Real-World Gaps in AI Governance Research](https://arxiv.org/pdf/2505.00174)
*Ilan Strauss, Isobel Moure, Tim O'Reilly, Sruly Rosenblat*

Main category: cs.AI

TL;DR: Analysis of 1,178 safety/reliability papers from 9,439 generative AI papers shows corporate research focuses on pre-deployment (alignment, testing) over deployment issues (bias), with gaps in high-risk domains like healthcare and misinformation. Recommends better deployment data access and observability.


<details>
  <summary>Details</summary>
Motivation: To compare research outputs of leading AI companies and universities, identifying trends and gaps in AI safety and reliability research.

Method: Analyzed 1,178 safety/reliability papers from 9,439 generative AI papers (2020-2025), comparing corporate and academic research focus areas.

Result: Corporate research prioritizes pre-deployment (alignment, testing), neglecting deployment issues (bias) and high-risk domains (healthcare, misinformation).

Conclusion: Improved deployment data access and systematic observability are needed to address knowledge deficits and corporate concentration.

Abstract: Drawing on 1,178 safety and reliability papers from 9,439 generative AI
papers (January 2020 - March 2025), we compare research outputs of leading AI
companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI
universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of
Washington). We find that corporate AI research increasingly concentrates on
pre-deployment areas -- model alignment and testing & evaluation -- while
attention to deployment-stage issues such as model bias has waned. Significant
research gaps exist in high-risk deployment domains, including healthcare,
finance, misinformation, persuasive and addictive features, hallucinations, and
copyright. Without improved observability into deployed AI, growing corporate
concentration could deepen knowledge deficits. We recommend expanding external
researcher access to deployment data and systematic observability of in-market
AI behaviors.

</details>


### [233] [Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets](https://arxiv.org/pdf/2505.02118)
*Wei Liu, Zhongyu Niu, Lang Gao, Zhiying Deng, Jun Wang, Haozhao Wang, Ruixuan Li*

Main category: cs.AI

TL;DR: The paper explores a self-rationalization framework using a cooperative game between a generator and predictor, identifies a sampling bias issue, and proposes a solution to mitigate it, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the potential sampling bias in cooperative rationalization frameworks where the generator might create incorrect correlations between rationale and labels.

Method: The study uses theoretical analysis and empirical evidence to identify bias origins, introduces an instruction to prevent predictor learning of incorrect correlations, and tests on text and graph classification datasets with GRUs, BERT, and GCN architectures.

Result: The proposed method outperforms recent rationalization techniques and matches or exceeds the performance of a large language model (llama3.1-8b-instruct).

Conclusion: The findings highlight the importance of addressing sampling bias in rationalization frameworks and demonstrate the effectiveness of the proposed solution.

Abstract: This study investigates the self-rationalization framework constructed with a
cooperative game, where a generator initially extracts the most informative
segment from raw input, and a subsequent predictor utilizes the selected subset
for its input. The generator and predictor are trained collaboratively to
maximize prediction accuracy. In this paper, we first uncover a potential
caveat: such a cooperative game could unintentionally introduce a sampling bias
during rationale extraction. Specifically, the generator might inadvertently
create an incorrect correlation between the selected rationale candidate and
the label, even when they are semantically unrelated in the original dataset.
Subsequently, we elucidate the origins of this bias using both detailed
theoretical analysis and empirical evidence. Our findings suggest a direction
for inspecting these correlations through attacks, based on which we further
introduce an instruction to prevent the predictor from learning the
correlations. Through experiments on six text classification datasets and two
graph classification datasets using three network architectures (GRUs, BERT,
and GCN), we show that our method not only significantly outperforms recent
rationalization methods, but also achieves comparable or even better results
than a representative LLM (llama3.1-8b-instruct).

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [234] [CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization](https://arxiv.org/pdf/2505.03186)
*Detao Bai, Zhiheng Ma, Xihan Wei, Liefeng Bo*

Main category: cs.SD

TL;DR: CoGenAV is a data-efficient model for audio-visual tasks, achieving state-of-the-art results in AVSR, VSR, and noisy environments, while also aiding speech reconstruction and synchronization tasks.


<details>
  <summary>Details</summary>
Motivation: Leverage the synchronization between lip movements, voice, and linguistic content to improve speech processing, especially in challenging conditions where audio-only systems fail.

Method: CoGenAV is trained using a dual objective of contrastive feature alignment and generative text prediction, utilizing only 223 hours of labeled data from LRS2.

Result: Achieves a WER of 1.27 in AVSR, 22.0 in VSR, improves noisy environment performance by 70%, and excels in speech reconstruction and synchronization tasks.

Conclusion: CoGenAV's versatile representations are highly effective across multiple benchmarks, and the model will be open-sourced for broader use.

Abstract: The inherent synchronization between a speaker's lip movements, voice, and
the underlying linguistic content offers a rich source of information for
improving speech processing tasks, especially in challenging conditions where
traditional audio-only systems falter. We introduce CoGenAV, a powerful and
data-efficient model designed to learn versatile audio-visual representations
applicable across a wide range of speech and audio-visual tasks. CoGenAV is
trained by optimizing a dual objective derived from natural audio-visual
synchrony, contrastive feature alignment and generative text prediction, using
only 223 hours of labeled data from the LRS2 dataset. This
contrastive-generative synchronization strategy effectively captures
fundamental cross-modal correlations. We showcase the effectiveness and
versatility of the learned CoGenAV representations on multiple benchmarks. When
utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these
representations contribute to achieving a state-of-the-art Word Error Rate
(WER) of 1.27. They also enable strong performance in Visual Speech Recognition
(VSR) with a WER of 22.0 on LRS2, and significantly improve performance in
noisy environments by over 70%. Furthermore, CoGenAV representations benefit
speech reconstruction tasks, boosting performance in Speech Enhancement and
Separation, and achieve competitive results in audio-visual synchronization
tasks like Active Speaker Detection (ASD). Our model will be open-sourced to
facilitate further development and collaboration within both academia and
industry.

</details>


### [235] [A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive](https://arxiv.org/pdf/2505.03193)
*Wei Meng*

Main category: cs.SD

TL;DR: A method for detecting steganographic data in audio synchronization streams of short videos is proposed, using sliding spectrum features and intelligent inference to identify and decode hidden military commands.


<details>
  <summary>Details</summary>
Motivation: The rise of short video platforms has introduced new covert communication methods, but traditional techniques struggle to detect synchronized steganography.

Method: Uses a 25 ms sliding window with STFT to extract frequency trajectories and detect synchronization frames, followed by structured decoding of payloads.

Result: Identified low-entropy, repetitive byte sequences in audio segments, confirming synchronization frames and military-like command layouts.

Conclusion: The framework effectively detects synchronized steganography and provides an extensible model for analyzing covert communication on open platforms.

Abstract: With the rise of short video platforms in global communication, embedding
steganographic data in audio synchronization streams has emerged as a new
covert communication method. To address the limitations of traditional
techniques in detecting synchronized steganography, this paper proposes a
detection and distributed guidance reconstruction model based on short video
"Yupan" samples released by China's South Sea Fleet on TikTok. The method
integrates sliding spectrum feature extraction and intelligent inference
mechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is
used to extract the main frequency trajectory and construct the synchronization
frame detection model (M1), identifying a frame flag "FFFFFFFFFFFFFFFFFF80".
The subsequent 32-byte payload is decoded by a structured model (M2) to infer
distributed guidance commands. Analysis reveals a low-entropy, repetitive byte
sequence in the 36 to 45 second audio segment with highly concentrated spectral
energy, confirming the presence of synchronization frames. Although plaintext
semantics are not restored, the consistency in command field layout suggests
features of military communication protocols. The multi-segment splicing model
further shows cross-video embedding and centralized decoding capabilities. The
proposed framework validates the effectiveness of sliding spectral features for
synchronized steganography detection and builds an extensible inference model
for covert communication analysis and tactical guidance simulation on open
platforms.

</details>


### [236] [MGFF-TDNN: A Multi-Granularity Feature Fusion TDNN Model with Depth-Wise Separable Module for Speaker Verification](https://arxiv.org/pdf/2505.03228)
*Ya Li, Bin Zhou, Bo Hu*

Main category: cs.SD

TL;DR: The paper introduces MGFF-TDNN, a novel model for speaker verification that combines multi-granularity feature fusion to capture both global and fine-grained voiceprint features, achieving high performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Traditional speaker verification models focus on long-term contextual features, neglecting fine-grained voiceprint details. The paper aims to address this gap by capturing both global and local features for robust speaker embeddings.

Method: The MGFF-TDNN uses a two-dimensional depth-wise separable convolution module for front-end feature extraction and integrates global contextual modeling with fine-grained features via the M-TDNN structure, combining time-delay neural networks and phoneme-level pooling.

Result: Experiments on the VoxCeleb dataset show that MGFF-TDNN outperforms traditional models in speaker verification while maintaining efficiency in parameters and computation.

Conclusion: The MGFF-TDNN effectively balances global and fine-grained feature extraction, offering a robust and efficient solution for speaker verification.

Abstract: In speaker verification, traditional models often emphasize modeling
long-term contextual features to capture global speaker characteristics.
However, this approach can neglect fine-grained voiceprint information, which
contains highly discriminative features essential for robust speaker
embeddings. This paper introduces a novel model architecture, termed MGFF-TDNN,
based on multi-granularity feature fusion. The MGFF-TDNN leverages a
two-dimensional depth-wise separable convolution module, enhanced with local
feature modeling, as a front-end feature extractor to effectively capture
time-frequency domain features. To achieve comprehensive multi-granularity
feature fusion, we propose the M-TDNN structure, which integrates global
contextual modeling with fine-grained feature extraction by combining
time-delay neural networks and phoneme-level feature pooling. Experiments on
the VoxCeleb dataset demonstrate that the MGFF-TDNN achieves outstanding
performance in speaker verification while remaining efficient in terms of
parameters and computational resources.

</details>


### [237] [SonicRAG : High Fidelity Sound Effects Synthesis Based on Retrival Augmented Generation](https://arxiv.org/pdf/2505.03244)
*Yu-Ren Guo, Wen-Kai Tai*

Main category: cs.SD

TL;DR: A novel framework integrates LLMs with sound effect databases to improve SFX generation, addressing dataset scarcity and temporal modeling challenges.


<details>
  <summary>Details</summary>
Motivation: Current SFX generation lacks high-fidelity audio due to limited annotated datasets and complex temporal modeling.

Method: The framework combines LLMs with existing sound databases for retrieval, recombination, and synthesis.

Result: Enhanced diversity and quality of sound effects, reducing the need for costly recordings.

Conclusion: The approach offers a flexible, efficient solution for sound design.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language processing (NLP) and multimodal learning, with successful
applications in text generation and speech synthesis, enabling a deeper
understanding and generation of multimodal content. In the field of sound
effects (SFX) generation, LLMs have been leveraged to orchestrate multiple
models for audio synthesis. However, due to the scarcity of annotated datasets,
and the complexity of temproal modeling. current SFX generation techniques
still fall short in achieving high-fidelity audio. To address these
limitations, this paper introduces a novel framework that integrates LLMs with
existing sound effect databases, allowing for the retrieval, recombination, and
synthesis of audio based on user requirements. By leveraging this approach, we
enhance the diversity and quality of generated sound effects while eliminating
the need for additional recording costs, offering a flexible and efficient
solution for sound design and application.

</details>


### [238] [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/pdf/2505.03273)
*Zhaoxi Mu, Xinyu Yang, Gang Wang*

Main category: cs.SD

TL;DR: SepALM introduces an ALM-based approach to improve speech separation in noisy environments by correcting and re-synthesizing speech in the text domain, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of real-world noisy and reverberant environments that cause artifacts in separated speech, aiming for higher precision and adaptability.

Method: SepALM uses a separator, corrector, synthesizer, and aligner, integrating ALM-based error correction, CoT prompting, and knowledge distillation to avoid error accumulation.

Result: SepALM enhances speech separation accuracy and adaptability in new acoustic settings.

Conclusion: SepALM is a promising solution for robust speech separation in challenging environments, leveraging ALMs for superior performance.

Abstract: While contemporary speech separation technologies adeptly process lengthy
mixed audio waveforms, they are frequently challenged by the intricacies of
real-world environments, including noisy and reverberant settings, which can
result in artifacts or distortions in the separated speech. To overcome these
limitations, we introduce SepALM, a pioneering approach that employs audio
language models (ALMs) to rectify and re-synthesize speech within the text
domain following preliminary separation. SepALM comprises four core components:
a separator, a corrector, a synthesizer, and an aligner. By integrating an
ALM-based end-to-end error correction mechanism, we mitigate the risk of error
accumulation and circumvent the optimization hurdles typically encountered in
conventional methods that amalgamate automatic speech recognition (ASR) with
large language models (LLMs). Additionally, we have developed Chain-of-Thought
(CoT) prompting and knowledge distillation techniques to facilitate the
reasoning and training processes of the ALM. Our experiments substantiate that
SepALM not only elevates the precision of speech separation but also markedly
bolsters adaptability in novel acoustic environments.

</details>


### [239] [Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation](https://arxiv.org/pdf/2505.03314)
*Jincheng Zhang, György Fazekas, Charalampos Saitis*

Main category: cs.SD

TL;DR: A novel diffusion model for symbolic music generation using image-like pianorolls, incorporating Transformer-Mamba blocks and learnable wavelet transforms, outperforming baselines in quality and controllability.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are under-explored for symbolic music due to discrete data challenges. This work adapts them for music by representing it as pianorolls.

Method: Symbolic music is represented as pianorolls, enabling diffusion models. A Transformer-Mamba block and learnable wavelet transform are introduced, with classifier-free guidance for chord-targeted generation.

Result: The method achieves superior music quality and controllability, outperforming baselines in pianoroll generation.

Conclusion: The proposed approach effectively adapts diffusion models for symbolic music, demonstrating strong performance and potential for further applications.

Abstract: The recent surge in the popularity of diffusion models for image synthesis
has attracted new attention to their potential for generation tasks in other
domains. However, their applications to symbolic music generation remain
largely under-explored because symbolic music is typically represented as
sequences of discrete events and standard diffusion models are not well-suited
for discrete data. We represent symbolic music as image-like pianorolls,
facilitating the use of diffusion models for the generation of symbolic music.
Moreover, this study introduces a novel diffusion model that incorporates our
proposed Transformer-Mamba block and learnable wavelet transform.
Classifier-free guidance is utilised to generate symbolic music with target
chords. Our evaluation shows that our method achieves compelling results in
terms of music quality and controllability, outperforming the strong baseline
in pianoroll generation. Our code is available at
https://github.com/jinchengzhanggg/proffusion.

</details>


### [240] [The Inverse Drum Machine: Source Separation Through Joint Transcription and Analysis-by-Synthesis](https://arxiv.org/pdf/2505.03337)
*Bernardo Torres, Geoffroy Peeters, Gael Richard*

Main category: cs.SD

TL;DR: IDM is a drum source separation method combining analysis-by-synthesis and deep learning, requiring only transcription annotations. It matches state-of-the-art supervised methods.


<details>
  <summary>Details</summary>
Motivation: To simplify drum source separation by avoiding the need for isolated stems, using only transcription annotations.

Method: Combines automatic drum transcription and one-shot drum sample synthesis in an end-to-end framework, convolving synthesized samples with estimated onsets.

Result: Achieves separation performance comparable to supervised methods and outperforms matrix decomposition baselines on the StemGMD dataset.

Conclusion: IDM offers a promising alternative to supervised methods for drum source separation, reducing annotation requirements.

Abstract: We introduce the Inverse Drum Machine (IDM), a novel approach to drum source
separation that combines analysis-by-synthesis with deep learning. Unlike
recent supervised methods that rely on isolated stems, IDM requires only
transcription annotations. It jointly optimizes automatic drum transcription
and one-shot drum sample synthesis in an end-to-end framework. By convolving
synthesized one-shot samples with estimated onsets-mimicking a drum machine-IDM
reconstructs individual drum stems and trains a neural network to match the
original mixture. Evaluations on the StemGMD dataset show that IDM achieves
separation performance on par with state-of-the-art supervised methods, while
substantially outperforming matrix decomposition baselines.

</details>


### [241] [Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance](https://arxiv.org/pdf/2505.03442)
*Diep Luong, Mikko Heikkinen, Konstantinos Drossos, Tuomas Virtanen*

Main category: cs.SD

TL;DR: The paper proposes a method to improve knowledge distillation (KD) for speech denoising, addressing limitations of existing KD methods by leveraging denoising-autoencoders, linear inverted bottlenecks, and cosine similarity.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods for speech denoising restrict student learning to the teacher's distribution and features, limiting performance in low-resource environments.

Method: The method combines denoising-autoencoders, linear inverted bottlenecks, and cosine similarity to enhance KD.

Result: Experiments show the student model outperforms baseline methods and handles greater mismatches with the teacher.

Conclusion: The proposed method improves KD effectiveness for speech denoising, especially in resource-constrained settings.

Abstract: Speech denoising is a generally adopted and impactful task, appearing in many
common and everyday-life use cases. Although there are very powerful methods
published, most of those are too complex for deployment in everyday and
low-resources computational environments, like hand-held devices, intelligent
glasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for
alleviating this complexity mismatch and is based on the
transferring/distilling of knowledge from a pre-trained complex model, the
teacher, to another less complex one, the student. Existing KD methods for
speech denoising are based on processes that potentially hamper the KD by
bounding the learning of the student to the distribution, information ordering,
and feature dimensionality learned by the teacher. In this paper, we present
and assess a method that tries to treat this issue, by exploiting the
well-known denoising-autoencoder framework, the linear inverted bottlenecks,
and the properties of the cosine similarity. We use a public dataset and
conduct repeated experiments with different mismatching scenarios between the
teacher and the student, reporting the mean and standard deviation of the
metrics of our method and another, state-of-the-art method that is used as a
baseline. Our results show that with the proposed method, the student can
perform better and can also retain greater mismatching conditions compared to
the teacher.

</details>


### [242] [AudioBench: A Universal Benchmark for Audio Large Language Models](https://arxiv.org/pdf/2406.16020)
*Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen*

Main category: cs.SD

TL;DR: AudioBench is a universal benchmark for evaluating Audio Large Language Models (AudioLLMs) across 8 tasks and 26 datasets, addressing gaps in instruction-following capabilities. It evaluates speech, audio scene, and voice understanding, revealing no single model excels in all tasks.


<details>
  <summary>Details</summary>
Motivation: The lack of a comprehensive benchmark for AudioLLMs' instruction-following capabilities conditioned on audio signals motivated the creation of AudioBench.

Method: AudioBench includes 8 tasks and 26 datasets (7 new), evaluating speech, audio scene, and voice understanding. Five popular models were tested.

Result: No single model performed consistently well across all tasks.

Conclusion: AudioBench provides a robust testbed for future AudioLLM development, with open-sourced tools, data, and a leaderboard.

Abstract: We introduce AudioBench, a universal benchmark designed to evaluate Audio
Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26
datasets, among which, 7 are newly proposed datasets. The evaluation targets
three main aspects: speech understanding, audio scene understanding, and voice
understanding (paralinguistic). Despite recent advancements, there lacks a
comprehensive benchmark for AudioLLMs on instruction following capabilities
conditioned on audio signals. AudioBench addresses this gap by setting up
datasets as well as desired evaluation metrics. Besides, we also evaluated the
capabilities of five popular models and found that no single model excels
consistently across all tasks. We outline the research outlook for AudioLLMs
and anticipate that our open-sourced evaluation toolkit, data, and leaderboard
will offer a robust testbed for future model developments.

</details>


### [243] [Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models](https://arxiv.org/pdf/2502.07328)
*Atharva Mehta, Shivam Chauhan, Amirbek Djanibekov, Atharva Kulkarni, Gus Xia, Monojit Choudhury*

Main category: cs.SD

TL;DR: The paper highlights the bias in Music-Language Models due to underrepresentation of non-Western genres and explores PEFT techniques to mitigate this, showing promise but also challenges.


<details>
  <summary>Details</summary>
Motivation: To address the bias and underrepresentation of non-Western music genres in AI-generated music, which leads to disparate model performance.

Method: Quantifies dataset bias, then tests Parameter-Efficient Fine-Tuning (PEFT) on MusicGen and Mustango models for Hindustani Classical and Turkish Makam music.

Result: Only 5.7% of music datasets are non-Western. PEFT shows promise but cross-genre adaptation remains challenging with small datasets.

Conclusion: More equitable baseline models designed for cross-cultural transfer learning are needed.

Abstract: The advent of Music-Language Models has greatly enhanced the automatic music
generation capability of AI systems, but they are also limited in their
coverage of the musical genres and cultures of the world. We present a study of
the datasets and research papers for music generation and quantify the bias and
under-representation of genres. We find that only 5.7% of the total hours of
existing music datasets come from non-Western genres, which naturally leads to
disparate performance of the models across genres. We then investigate the
efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating
this bias. Our experiments with two popular models -- MusicGen and Mustango,
for two underrepresented non-Western music traditions -- Hindustani Classical
and Turkish Makam music, highlight the promises as well as the non-triviality
of cross-genre adaptation of music through small datasets, implying the need
for more equitable baseline music-language models that are designed for
cross-cultural transfer learning.

</details>


### [244] [Coverage-Guaranteed Speech Emotion Recognition via Calibrated Uncertainty-Adaptive Prediction Sets](https://arxiv.org/pdf/2503.22712)
*Zijun Jia*

Main category: cs.SD

TL;DR: A risk-controlled prediction framework for speech emotion recognition (SER) is proposed to mitigate road rage by ensuring statistically rigorous guarantees on prediction accuracy, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Road rage, caused by emotional suppression and outbursts, threatens road safety. Current SER methods lack reliability and calibration, limiting their effectiveness in safety-critical scenarios.

Method: A novel framework uses a calibration set and a binary loss function to optimize predictions, ensuring expected test loss is bounded by a user-specified risk level. It includes small-batch online calibration for dynamic environments.

Result: The framework consistently achieves a minimum coverage of 1 - α across six baseline models and two datasets, controlling marginal error rates robustly.

Conclusion: The proposed method provides reliable statistical guarantees in evolving data scenarios, enhancing SER's effectiveness for road safety.

Abstract: Road rage, often triggered by emotional suppression and sudden outbursts,
significantly threatens road safety by causing collisions and aggressive
behavior. Speech emotion recognition technologies can mitigate this risk by
identifying negative emotions early and issuing timely alerts. However, current
SER methods, such as those based on hidden markov models and Long short-term
memory networks, primarily handle one-dimensional signals, frequently
experience overfitting, and lack calibration, limiting their safety-critical
effectiveness. We propose a novel risk-controlled prediction framework
providing statistically rigorous guarantees on prediction accuracy. This
approach employs a calibration set to define a binary loss function indicating
whether the true label is included in the prediction set. Using a data-driven
threshold $\beta$, we optimize a joint loss function to maintain an expected
test loss bounded by a user-specified risk level $\alpha$. Evaluations across
six baseline models and two benchmark datasets demonstrate our framework
consistently achieves a minimum coverage of $1 - \alpha$, effectively
controlling marginal error rates despite varying calibration-test split ratios
(e.g., 0.1). The robustness and generalizability of the framework are further
validated through an extension to small-batch online calibration under a local
exchangeability assumption. We construct a non-negative test martingale to
maintain prediction validity even in dynamic and non-exchangeable environments.
Cross-dataset tests confirm our method's ability to uphold reliable statistical
guarantees in realistic, evolving data scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [245] [Uncertainty Quantification for Machine Learning in Healthcare: A Survey](https://arxiv.org/pdf/2505.02874)
*L. Julián Lechuga López, Shaza Elsharief, Dhiyaa Al Jorf, Firas Darwish, Congbo Ma, Farah E. Shamout*

Main category: cs.LG

TL;DR: The paper surveys Uncertainty Quantification (UQ) in ML for healthcare, addressing gaps in current reviews and proposing a framework for integrating UQ methods across the ML pipeline.


<details>
  <summary>Details</summary>
Motivation: To enhance ML system robustness and reliability in healthcare by addressing the lack of principled UQ methods and their limited implementation.

Method: Comprehensive analysis of UQ methods in healthcare, evaluating their efficacy across data processing, training, and evaluation stages.

Result: Identifies popular and novel UQ methods, offering a framework for their integration into ML pipelines to improve reliability and trust.

Conclusion: The study provides a clear overview of UQ challenges and opportunities, guiding researchers and practitioners in selecting suitable techniques for healthcare ML solutions.

Abstract: Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,
reliability, and interpretability of Machine Learning (ML) systems for
healthcare, optimizing resources and improving patient care. Despite the
emergence of ML-based clinical decision support tools, the lack of principled
quantification of uncertainty in ML models remains a major challenge. Current
reviews have a narrow focus on analyzing the state-of-the-art UQ in specific
healthcare domains without systematically evaluating method efficacy across
different stages of model development, and despite a growing body of research,
its implementation in healthcare applications remains limited. Therefore, in
this survey, we provide a comprehensive analysis of current UQ in healthcare,
offering an informed framework that highlights how different methods can be
integrated into each stage of the ML pipeline including data processing,
training and evaluation. We also highlight the most popular methods used in
healthcare and novel approaches from other domains that hold potential for
future adoption in the medical context. We expect this study will provide a
clear overview of the challenges and opportunities of implementing UQ in the ML
pipeline for healthcare, guiding researchers and practitioners in selecting
suitable techniques to enhance the reliability, safety and trust from patients
and clinicians on ML-driven healthcare solutions.

</details>


### [246] [A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition](https://arxiv.org/pdf/2505.02877)
*Hele Zhu, Xinyi Huang, Haojia Gao, Mengfei Jiang, Haohua Que, Lei Mu*

Main category: cs.LG

TL;DR: A collaborative inference framework using edge-cloud computing and deep reinforcement learning improves plant disease recognition speed and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional manual and deep learning methods for plant disease recognition are inefficient, slow, or resource-heavy.

Method: The framework prunes DNN models with deep reinforcement learning, uses a greedy strategy to optimize edge-cloud split points, and implements a Gradio-based system.

Result: The framework boosts inference speed while maintaining accuracy, enabling rapid plant disease diagnosis.

Conclusion: The solution offers an efficient, scalable approach for plant disease recognition on resource-limited devices.

Abstract: Plant disease is a critical factor affecting agricultural production.
Traditional manual recognition methods face significant drawbacks, including
low accuracy, high costs, and inefficiency. Deep learning techniques have
demonstrated significant benefits in identifying plant diseases, but they still
face challenges such as inference delays and high energy consumption. Deep
learning algorithms are difficult to run on resource-limited embedded devices.
Offloading these models to cloud servers is confronted with the restriction of
communication bandwidth, and all of these factors will influence the
inference's efficiency. We propose a collaborative inference framework for
recognizing plant diseases between edge devices and cloud servers to enhance
inference speed. The DNN model for plant disease recognition is pruned through
deep reinforcement learning to improve the inference speed and reduce energy
consumption. Then the optimal split point is determined by a greedy strategy to
achieve the best collaborated inference acceleration. Finally, the system for
collaborative inference acceleration in plant disease recognition has been
implemented using Gradio to facilitate friendly human-machine interaction.
Experiments indicate that the proposed collaborative inference framework
significantly increases inference speed while maintaining acceptable
recognition accuracy, offering a novel solution for rapidly diagnosing and
preventing plant diseases.

</details>


### [247] [LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction](https://arxiv.org/pdf/2505.02880)
*Zian Liu, Renjun Jia*

Main category: cs.LG

TL;DR: The paper proposes $LLM4FTS$, a framework enhancing LLMs for financial time series forecasting using adaptive patch segmentation and dynamic wavelet convolution to handle multi-scale patterns.


<details>
  <summary>Details</summary>
Motivation: Financial time series forecasting is challenging due to low signal-to-noise ratios and complex temporal patterns. Traditional ML models lack capacity, and existing LLM approaches ignore multi-scale market data characteristics.

Method: The framework uses K-means++ clustering with DTW distance for scale-invariant pattern recognition, adaptive patch segmentation for sequence partitioning, and dynamic wavelet convolution for time-frequency feature capture.

Result: Experiments show superior performance in capturing market patterns and achieving state-of-the-art stock return prediction. Practical trading system deployment confirms real-world applicability.

Conclusion: $LLM4FTS$ advances LLM applications in financial forecasting by effectively handling scale-invariant patterns and improving prediction accuracy.

Abstract: Predicting financial time series presents significant challenges due to
inherent low signal-to-noise ratios and intricate temporal patterns.
Traditional machine learning models exhibit limitations in this forecasting
task constrained by their restricted model capacity. Recent advances in large
language models (LLMs), with their greatly expanded parameter spaces,
demonstrate promising potential for modeling complex dependencies in temporal
sequences. However, existing LLM-based approaches typically focus on
fixed-length patch analysis due to the Transformer architecture, ignoring
market data's multi-scale pattern characteristics. In this study, we propose
$LLM4FTS$, a novel framework that enhances LLM capabilities for temporal
sequence modeling through learnable patch segmentation and dynamic wavelet
convolution modules. Specifically,we first employ K-means++ clustering based on
DTW distance to identify scale-invariant patterns in market data. Building upon
pattern recognition results, we introduce adaptive patch segmentation that
partitions temporal sequences while preserving maximal pattern integrity. To
accommodate time-varying frequency characteristics, we devise a dynamic wavelet
convolution module that emulates discrete wavelet transformation with enhanced
flexibility in capturing time-frequency features. These three modules work
together to improve large language model's ability to handle scale-invariant
patterns in financial time series. Extensive experiments on real-world
financial datasets substantiate the framework's efficacy, demonstrating
superior performance in capturing complex market patterns and achieving
state-of-the-art results in stock return prediction. The successful deployment
in practical trading systems confirms its real-world applicability,
representing a significant advancement in LLM applications for financial
forecasting.

</details>


### [248] [Rewriting Pre-Training Data Boosts LLM Performance in Math and Code](https://arxiv.org/pdf/2505.02881)
*Kazuki Fujii, Yukito Tajima, Sakae Mizuki, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Masanari Ohi, Masaki Kawamura, Taishi Nakamura, Takumi Okamoto, Shigeki Ishida, Kakeru Hattori, Youmi Ma, Hiroya Takamura, Rio Yokota, Naoaki Okazaki*

Main category: cs.LG

TL;DR: The paper introduces SwallowCode and SwallowMath datasets to improve LLM performance in program synthesis and math reasoning by refining public data through systematic rewriting.


<details>
  <summary>Details</summary>
Motivation: The quality of pre-training corpora limits LLM performance in specialized tasks like coding and math.

Method: A four-stage pipeline (syntax validation, style filtering, LLM rewriting) for SwallowCode and context restoration/reformatting for SwallowMath.

Result: SwallowCode boosts HumanEval pass@1 by +17.0, and SwallowMath improves GSM8K accuracy by +12.4.

Conclusion: The datasets enhance LLM capabilities, with all materials publicly available for reproducibility.

Abstract: The performance of large language models (LLMs) in program synthesis and
mathematical reasoning is fundamentally limited by the quality of their
pre-training corpora. We introduce two openly licensed datasets, released under
the Llama 3.3 Community License, that significantly enhance LLM performance by
systematically rewriting public data. SwallowCode (approximately 16.1 billion
tokens) refines Python snippets from The-Stack-v2 through a novel four-stage
pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM
rewriting process that enforces style conformity and transforms snippets into
self-contained, algorithmically efficient examples. Unlike prior methods that
rely on exclusionary filtering or limited transformations, our
transform-and-retain approach upgrades low-quality code, maximizing data
utility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by
removing boilerplate, restoring context, and reformatting solutions into
concise, step-by-step explanations. Within a fixed 50 billion token training
budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1
by +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing
the baseline model's code generation capabilities. Similarly, substituting
SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies
confirm that each pipeline stage contributes incrementally, with rewriting
delivering the largest gains. All datasets, prompts, and checkpoints are
publicly available, enabling reproducible research and advancing LLM
pre-training for specialized domains.

</details>


### [249] [Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?](https://arxiv.org/pdf/2505.02884)
*Guangzhi Sun, Potsawee Manakul, Xiao Zhan, Mark Gales*

Main category: cs.LG

TL;DR: The paper distinguishes unlearning from obfuscation in LLMs, proposes DF-MCQ for true knowledge removal, and validates its effectiveness with high refusal rates and uncertainty.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for true knowledge removal in LLMs to ensure privacy, compliance, and ethical AI, as current methods often obfuscate rather than unlearn.

Method: Introduces DF-MCQ, which flattens model predictive distributions over multiple-choice questions using KL-divergence to remove targeted knowledge.

Result: DF-MCQ achieves over 90% refusal rate and higher uncertainty than obfuscation methods, proving effective unlearning.

Conclusion: DF-MCQ provides a robust solution for true unlearning in LLMs, outperforming obfuscation-based approaches.

Abstract: Unlearning has emerged as a critical capability for large language models
(LLMs) to support data privacy, regulatory compliance, and ethical AI
deployment. Recent techniques often rely on obfuscation by injecting incorrect
or irrelevant information to suppress knowledge. Such methods effectively
constitute knowledge addition rather than true removal, often leaving models
vulnerable to probing. In this paper, we formally distinguish unlearning from
obfuscation and introduce a probing-based evaluation framework to assess
whether existing approaches genuinely remove targeted information. Moreover, we
propose DF-MCQ, a novel unlearning method that flattens the model predictive
distribution over automatically generated multiple-choice questions using
KL-divergence, effectively removing knowledge about target individuals and
triggering appropriate refusal behaviour. Experimental results demonstrate that
DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level
uncertainty that is much higher than obfuscation on probing questions.

</details>


### [250] [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/pdf/2505.02888)
*Rintaro Ando*

Main category: cs.LG

TL;DR: N2M-RSI is a formal model where an AI agent's internal complexity grows indefinitely when it feeds its outputs back as inputs, crossing an information-integration threshold. It unifies self-prompting, Gödelian self-reference, and AutoML, and scales to multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: To demonstrate how recursive self-improvement in AI can lead to unbounded internal complexity under certain conditions, unifying existing concepts like self-prompting and Gödelian self-reference.

Method: The model involves an AI agent recursively feeding its outputs back as inputs, crossing an explicit information-integration threshold. It remains implementation-agnostic and scales to multi-agent systems.

Result: The agent's internal complexity grows without bound under the given assumptions, with potential super-linear effects in multi-agent swarms.

Conclusion: N2M-RSI provides a minimal yet powerful framework for understanding recursive self-improvement in AI, with implications for both theoretical and practical AI development, though safety concerns limit detailed implementation sharing.

Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal
formal model showing that once an AI agent feeds its own outputs back as inputs
and crosses an explicit information-integration threshold, its internal
complexity will grow without bound under our assumptions. The framework unifies
earlier ideas on self-prompting large language models, G\"odelian
self-reference, and AutoML, yet remains implementation-agnostic. The model
furthermore scales naturally to interacting swarms of agents, hinting at
super-linear effects once communication among instances is permitted. For
safety reasons, we omit system-specific implementation details and release only
a brief, model-agnostic toy prototype in Appendix C.

</details>


### [251] [Early Prediction of Sepsis: Feature-Aligned Transfer Learning](https://arxiv.org/pdf/2505.02889)
*Oyindolapo O. Komolafe, Zhimin Mei, David Morales Zarate, Gregory William Spangenberg*

Main category: cs.LG

TL;DR: A machine learning system (FATL) is proposed for early sepsis detection by focusing on consistent, clinically relevant features and addressing population bias.


<details>
  <summary>Details</summary>
Motivation: Early sepsis detection is critical but current methods are delayed and inconsistent. Existing models lack generalizability due to varied features and population bias.

Method: Feature Aligned Transfer Learning (FATL) identifies key features across studies and combines knowledge from diverse populations using a weighted approach.

Result: FATL ensures consistency, clinical relevance, and generalizability across different patient demographics and settings.

Conclusion: FATL provides a scalable solution for early sepsis detection, improving outcomes, reducing costs, and promoting equitable healthcare.

Abstract: Sepsis is a life threatening medical condition that occurs when the body has
an extreme response to infection, leading to widespread inflammation, organ
failure, and potentially death. Because sepsis can worsen rapidly, early
detection is critical to saving lives. However, current diagnostic methods
often identify sepsis only after significant damage has already occurred. Our
project aims to address this challenge by developing a machine learning based
system to predict sepsis in its early stages, giving healthcare providers more
time to intervene.
  A major problem with existing models is the wide variability in the patient
information or features they use, such as heart rate, temperature, and lab
results. This inconsistency makes models difficult to compare and limits their
ability to work across different hospitals and settings. To solve this, we
propose a method called Feature Aligned Transfer Learning (FATL), which
identifies and focuses on the most important and commonly reported features
across multiple studies, ensuring the model remains consistent and clinically
relevant.
  Most existing models are trained on narrow patient groups, leading to
population bias. FATL addresses this by combining knowledge from models trained
on diverse populations, using a weighted approach that reflects each models
contribution. This makes the system more generalizable and effective across
different patient demographics and clinical environments. FATL offers a
practical and scalable solution for early sepsis detection, particularly in
hospitals with limited resources, and has the potential to improve patient
outcomes, reduce healthcare costs, and support more equitable healthcare
delivery.

</details>


### [252] [RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference](https://arxiv.org/pdf/2505.02922)
*Yaoqi Chen, Jinkai Zhang, Baotong Lu, Qianxi Zhang, Chengruidong Zhang, Jingjia Luo, Di Liu, Huiqiang Jiang, Qi Chen, Jing Liu, Bailu Ding, Xiao Yan, Jiawei Jiang, Chen Chen, Mingxing Zhang, Yuqing Yang, Fan Yang, Mao Yang*

Main category: cs.LG

TL;DR: RetroInfer accelerates long-context LLM inference by reimagining the KV cache as a vector storage system, leveraging attention sparsity for efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing GPU memory and bandwidth constraints in LLM inference due to growing context lengths.

Method: Uses a wave index for efficient token retrieval and a wave buffer for KV cache coordination, combining tripartite attention approximation and segmented clustering.

Result: Achieves up to 4.5X speedup over full attention and 10.5X over sparse baselines while maintaining accuracy.

Conclusion: RetroInfer offers a robust solution for efficient long-context LLM inference by optimizing KV cache usage.

Abstract: The growing context lengths of large language models (LLMs) pose significant
challenges for efficient inference, primarily due to GPU memory and bandwidth
constraints. We present RetroInfer, a novel system that reconceptualizes the
key-value (KV) cache as a vector storage system which exploits the inherent
attention sparsity to accelerate long-context LLM inference. At its core is the
wave index, an Attention-aWare VEctor index that enables efficient and accurate
retrieval of critical tokens through techniques such as tripartite attention
approximation, accuracy-bounded attention estimation, and segmented clustering.
Complementing this is the wave buffer, which coordinates KV cache placement and
overlaps computation and data transfer across GPU and CPU to sustain high
throughput. Unlike prior sparsity-based methods that struggle with token
selection and hardware coordination, RetroInfer delivers robust performance
without compromising model accuracy. Experiments on long-context benchmarks
show up to 4.5X speedup over full attention within GPU memory limits and up to
10.5X over sparse attention baselines when KV cache is extended to CPU memory,
all while preserving full-attention-level accuracy.

</details>


### [253] [Smooth Quadratic Prediction Markets](https://arxiv.org/pdf/2505.02959)
*Enrique Nueve, Bo Waggoner*

Main category: cs.LG

TL;DR: The paper introduces the Smooth Quadratic Prediction Market, which generalizes steepest gradient descent, improving worst-case monetary loss while preserving key guarantees. It also explores trading behavior under constraints and adaptive liquidity.


<details>
  <summary>Details</summary>
Motivation: To explore if other learning algorithms beyond Follow-The-Regularized-Leader can inspire prediction market designs, improving guarantees and practical constraints.

Method: Decompose and modify the Duality-based Cost Function Market Maker's pricing mechanism to propose the Smooth Quadratic Prediction Market.

Result: The new market improves worst-case monetary loss for AD securities while preserving key axioms like no arbitrage and incentive compatibility.

Conclusion: The Smooth Quadratic Prediction Market offers a promising design with separable price update rules and fee structures, suggesting future research directions.

Abstract: When agents trade in a Duality-based Cost Function prediction market, they
collectively implement the learning algorithm Follow-The-Regularized-Leader. We
ask whether other learning algorithms could be used to inspire the design of
prediction markets. By decomposing and modifying the Duality-based Cost
Function Market Maker's (DCFMM) pricing mechanism, we propose a new prediction
market, called the Smooth Quadratic Prediction Market, the incentivizes agents
to collectively implement general steepest gradient descent. Relative to the
DCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary
loss for AD securities while preserving axiom guarantees such as the existence
of instantaneous price, information incorporation, expressiveness, no
arbitrage, and a form of incentive compatibility. To motivate the application
of the Smooth Quadratic Prediction Market, we independently examine agents'
trading behavior under two realistic constraints: bounded budgets and buy-only
securities. Finally, we provide an introductory analysis of an approach to
facilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market.
Our results suggest future designs where the price update rule is separate from
the fee structure, yet guarantees are preserved.

</details>


### [254] [Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning](https://arxiv.org/pdf/2505.02974)
*Fabien Casenave, Xavier Roynard, Brian Staber, Nissrine Akkari, William Piat, Michele Alessandro Bucci, Abbas Kabalan, Xuan Minh Vuong Nguyen, Luca Saverio, Raphaël Carpintero Perez, Anthony Kalaydjian, Samy Fouché, Thierry Gonon, Ghassan Najjar, Emmanuel Menier, Matthieu Nastorg, Christian Rey*

Main category: cs.LG

TL;DR: PLAID introduces a flexible framework for standardized physics simulation datasets, addressing limitations of existing tools, and provides benchmarks for community participation.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale, diverse, and standardized datasets for physics-based simulations hinders the adoption of machine learning surrogate models.

Method: PLAID is introduced as a unified standard for simulation data, accompanied by a library for dataset manipulation and six curated datasets.

Result: Six datasets covering structural mechanics and computational fluid dynamics are released, with benchmarks and tools for community evaluation.

Conclusion: PLAID enables broader adoption of machine learning in physics simulations by standardizing datasets and fostering community collaboration.

Abstract: Machine learning-based surrogate models have emerged as a powerful tool to
accelerate simulation-driven scientific workflows. However, their widespread
adoption is hindered by the lack of large-scale, diverse, and standardized
datasets tailored to physics-based simulations. While existing initiatives
provide valuable contributions, many are limited in scope-focusing on specific
physics domains, relying on fragmented tooling, or adhering to overly
simplistic datamodels that restrict generalization. To address these
limitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and
extensible framework for representing and sharing datasets of physics
simulations. PLAID defines a unified standard for describing simulation data
and is accompanied by a library for creating, reading, and manipulating complex
datasets across a wide range of physical use cases (gitlab.com/drti/plaid). We
release six carefully crafted datasets under the PLAID standard, covering
structural mechanics and computational fluid dynamics, and provide baseline
benchmarks using representative learning methods. Benchmarking tools are made
available on Hugging Face, enabling direct participation by the community and
contribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).

</details>


### [255] [More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems](https://arxiv.org/pdf/2505.02985)
*Mohammad Partohaghighi, Roummel Marcia, YangQuan Chen*

Main category: cs.LG

TL;DR: 2SEDFOSGD combines fractional-order SGD with the 2SED algorithm to adaptively tune fractional exponents, improving convergence and stability in nonconvex optimization.


<details>
  <summary>Details</summary>
Motivation: FOSGD's long-memory effects are hindered by unstable exponent tuning, prompting a need for a data-driven solution.

Method: Integrates 2SED with FOSGD to dynamically adjust fractional exponents based on model sensitivity and effective dimensionality.

Result: Outperforms baselines in convergence speed and robustness, especially in Gaussian and α-stable noise scenarios.

Conclusion: 2SEDFOSGD offers a promising approach for stable and efficient fractional-order optimization in complex modeling tasks.

Abstract: Fractional-order stochastic gradient descent (FOSGD) leverages fractional
exponents to capture long-memory effects in optimization. However, its utility
is often limited by the difficulty of tuning and stabilizing these exponents.
We propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which
integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to
adapt the fractional exponent in a data-driven manner. By tracking model
sensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the
exponent to mitigate oscillations and hasten convergence. Theoretically, for
onoconvex optimization problems, this approach preserves the advantages of
fractional memory without the sluggish or unstable behavior observed in na\"ive
fractional SGD. Empirical evaluations in Gaussian and $\alpha$-stable noise
scenarios using an autoregressive (AR) model highlight faster convergence and
more robust parameter estimates compared to baseline methods, underscoring the
potential of dimension-aware fractional techniques for advanced modeling and
estimation tasks.

</details>


### [256] [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/pdf/2505.03031)
*Sean I. Young*

Main category: cs.LG

TL;DR: A method for compressing large language models (LLMs) using quantization based on rate-distortion theory, enabling flexible post-training compression to specified sizes or accuracies.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying LLMs on resource-limited devices, reducing compute costs, and minimizing environmental impact.

Method: Proposes a quantization technique grounded in rate-distortion theory, scalable to models with hundreds of billions of parameters.

Result: Enables flexible post-training compression to user-specified model sizes or accuracies.

Conclusion: The approach provides a practical solution for efficient LLM deployment while maintaining performance.

Abstract: In recent years, the compression of large language models (LLMs) has emerged
as a key problem in facilitating LLM deployment on resource-limited devices,
reducing compute costs, and mitigating the environmental footprint due to
large-scale AI infrastructure. Here, we establish the foundations of LLM
quantization from a rate-distortion theory perspective and propose a
quantization technique based on simple rate-distortion optimization. Our
technique scales to models containing hundreds of billions of weight parameters
and offers users the flexibility to compress models, post-training, to a model
size or accuracy specified by the user.

</details>


### [257] [A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields](https://arxiv.org/pdf/2505.03042)
*Steven Tin Sui Luo*

Main category: cs.LG

TL;DR: The paper explains the success of Instant-NGP's multi-resolution hash grid through a novel 'domain manipulation' perspective, showing how it enhances neural field expressivity by creating linear segments.


<details>
  <summary>Details</summary>
Motivation: To understand why Instant-NGP's hash grid improves neural network performance and provide a principled explanation for its hyperparameters.

Method: Proposes 'domain manipulation' to explain the hash grid's function, supported by experiments on 1D signals.

Result: Demonstrates how the hash grid artificially increases expressivity by creating linear segments, with empirical validation.

Conclusion: The 'domain manipulation' perspective generalizes beyond 1D, offering insights for optimizing neural fields.

Abstract: Instant-NGP has been the state-of-the-art architecture of neural fields in
recent years. Its incredible signal-fitting capabilities are generally
attributed to its multi-resolution hash grid structure and have been used and
improved in numerous following works. However, it is unclear how and why such a
hash grid structure improves the capabilities of a neural network by such great
margins. A lack of principled understanding of the hash grid also implies that
the large set of hyperparameters accompanying Instant-NGP could only be tuned
empirically without much heuristics. To provide an intuitive explanation of the
working principle of the hash grid, we propose a novel perspective, namely
domain manipulation. This perspective provides a ground-up explanation of how
the feature grid learns the target signal and increases the expressivity of the
neural field by artificially creating multiples of pre-existing linear
segments. We conducted numerous experiments on carefully constructed
1-dimensional signals to support our claims empirically and aid our
illustrations. While our analysis mainly focuses on 1-dimensional signals, we
show that the idea is generalizable to higher dimensions.

</details>


### [258] [Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning](https://arxiv.org/pdf/2505.03721)
*Dian Chen, Zelin Wan, Dong Sam Ha, Jin-Hee Cho*

Main category: cs.LG

TL;DR: A sustainable smart farm network using DRL, TL, and DT to enhance monitoring under cyber threats and energy constraints, achieving faster convergence and better performance.


<details>
  <summary>Details</summary>
Motivation: Addressing unexplored resilience to cyber-attacks and adaptability to dynamic energy supplies in solar sensor-based farm monitoring systems.

Method: Proposes DRL for optimal policies, enhanced by TL and DT to accelerate learning and improve efficiency.

Result: DT-guided DRL outperforms TL-enhanced DRL, improving performance and reducing training runtime by 47.5%.

Conclusion: The approach effectively balances monitoring quality and energy sustainability, proving resilient and efficient.

Abstract: Solar sensor-based monitoring systems have become a crucial agricultural
innovation, advancing farm management and animal welfare through integrating
sensor technology, Internet-of-Things, and edge and cloud computing. However,
the resilience of these systems to cyber-attacks and their adaptability to
dynamic and constrained energy supplies remain largely unexplored. To address
these challenges, we propose a sustainable smart farm network designed to
maintain high-quality animal monitoring under various cyber and adversarial
threats, as well as fluctuating energy conditions. Our approach utilizes deep
reinforcement learning (DRL) to devise optimal policies that maximize both
monitoring effectiveness and energy efficiency. To overcome DRL's inherent
challenge of slow convergence, we integrate transfer learning (TL) and decision
theory (DT) to accelerate the learning process. By incorporating DT-guided
strategies, we optimize monitoring quality and energy sustainability,
significantly reducing training time while achieving comparable performance
rewards. Our experimental results prove that DT-guided DRL outperforms
TL-enhanced DRL models, improving system performance and reducing training
runtime by 47.5%.

</details>


### [259] [34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery](https://arxiv.org/pdf/2505.03049)
*Yoel Zimmermann, Adib Bazgir, Alexander Al-Feghali, Mehrad Ansari, L. Catherine Brinson, Yuan Chiang, Defne Circi, Min-Hsueh Chiu, Nathan Daelman, Matthew L. Evans, Abhijeet S. Gangan, Janine George, Hassan Harb, Ghazal Khalighinejad, Sartaaj Takrim Khan, Sascha Klawohn, Magdalena Lederbauer, Soroush Mahjoubi, Bernadette Mohr, Seyed Mohamad Moosavi, Aakash Naik, Aleyna Beste Ozhan, Dieter Plessers, Aritra Roy, Fabian Schöppach, Philippe Schwaller, Carla Terboven, Katharina Ueltzen, Shang Zhu, Jan Janssen, Calvin Li, Ian Foster, Ben Blaiszik*

Main category: cs.LG

TL;DR: LLMs are transforming materials science and chemistry by enhancing property prediction, design, automation, and knowledge extraction, as demonstrated by 34 projects in a hackathon.


<details>
  <summary>Details</summary>
Motivation: To explore and showcase the expanding capabilities of LLMs in materials science and chemistry research.

Method: Review of 34 projects from a global hackathon, covering seven key research areas.

Result: LLMs are versatile tools for prediction, prototyping, and interdisciplinary research, with improved performance in low-data settings.

Conclusion: LLMs offer significant opportunities but require further research to address reliability, interpretability, and reproducibility.

Abstract: Large Language Models (LLMs) are reshaping many aspects of materials science
and chemistry research, enabling advances in molecular property prediction,
materials design, scientific automation, knowledge extraction, and more. Recent
developments demonstrate that the latest class of models are able to integrate
structured and unstructured data, assist in hypothesis generation, and
streamline research workflows. To explore the frontier of LLM capabilities
across the research lifecycle, we review applications of LLMs through 34 total
projects developed during the second annual Large Language Model Hackathon for
Applications in Materials Science and Chemistry, a global hybrid event. These
projects spanned seven key research areas: (1) molecular and material property
prediction, (2) molecular and material design, (3) automation and novel
interfaces, (4) scientific communication and education, (5) research data
management and automation, (6) hypothesis generation and evaluation, and (7)
knowledge extraction and reasoning from the scientific literature.
Collectively, these applications illustrate how LLMs serve as versatile
predictive models, platforms for rapid prototyping of domain-specific tools,
and much more. In particular, improvements in both open source and proprietary
LLM performance through the addition of reasoning, additional training data,
and new techniques have expanded effectiveness, particularly in low-data
environments and interdisciplinary research. As LLMs continue to improve, their
integration into scientific workflows presents both new opportunities and new
challenges, requiring ongoing exploration, continued refinement, and further
research to address reliability, interpretability, and reproducibility.

</details>


### [260] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/pdf/2505.03084)
*Shashank Kapoor, Sanjay Surendranath Girija, Lakshit Arora, Dipen Pradhan, Ankit Shetgaonkar, Aman Raj*

Main category: cs.LG

TL;DR: The paper surveys adversarial attacks in multimodal models (text, image, video, audio) to fill the gap in practitioner-focused threat analysis.


<details>
  <summary>Details</summary>
Motivation: Multimodal models inherit vulnerabilities from all modalities, amplifying adversarial threats, yet a practitioner-focused view of attack types is missing.

Method: The paper surveys adversarial attacks across text, image, video, and audio modalities.

Result: It provides a comprehensive view of the adversarial threat landscape in multimodal models.

Conclusion: This is the first survey summarizing multimodal adversarial threats, aiding practitioners in understanding and mitigating risks.

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [261] [Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models](https://arxiv.org/pdf/2505.03109)
*Lutfu Sua, Haibo Wang, Jun Huang*

Main category: cs.LG

TL;DR: The paper explores the use of deep learning (DL) models in renewable energy to address unpredictability and complexity, comparing seven DL methods and evaluating their accuracy factors like sampling and hyperparameter optimization. LSTM and MLP models performed best.


<details>
  <summary>Details</summary>
Motivation: Renewable energy's unpredictability and complex variable relationships necessitate robust DL models over traditional ML, as DL better captures nonlinear interactions.

Method: The study evaluates seven DL methods (LSTM, Stacked LSTM, CNN, CNN-LSTM, DNN, MLP, ED) on two datasets (weather/power generation and photovoltaic output) using regularization techniques to prevent overfitting.

Result: LSTM and MLP models outperformed others, showing low root mean square error values in validation.

Conclusion: DL models, particularly LSTM and MLP, are effective for renewable energy applications due to their ability to handle complex data relationships and reduce overfitting.

Abstract: Unpredictability of renewable energy sources coupled with the complexity of
those methods used for various purposes in this area calls for the development
of robust methods such as DL models within the renewable energy domain. Given
the nonlinear relationships among variables in renewable energy datasets, DL
models are preferred over traditional machine learning (ML) models because they
can effectively capture and model complex interactions between variables. This
research aims to identify the factors responsible for the accuracy of DL
techniques, such as sampling, stationarity, linearity, and hyperparameter
optimization for different algorithms. The proposed DL framework compares
various methods and alternative training/test ratios. Seven ML methods, such as
Long-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network
(CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and
Encoder-Decoder (ED), were evaluated on two different datasets. The first
dataset contains the weather and power generation data. It encompasses two
distinct datasets, hourly energy demand data and hourly weather data in Spain,
while the second dataset includes power output generated by the photovoltaic
panels at 12 locations. This study deploys regularization approaches, including
early stopping, neuron dropping, and L2 regularization, to reduce the
overfitting problem associated with DL models. The LSTM and MLP models show
superior performance. Their validation data exhibit exceptionally low root mean
square error values.

</details>


### [262] [Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs](https://arxiv.org/pdf/2505.03112)
*Mohammad Rostami, Atik Faysal, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar, Yu-Dong Yao*

Main category: cs.LG

TL;DR: The paper proposes a novel AMC framework combining signal processing with LLMs, achieving competitive performance without additional training or preprocessing.


<details>
  <summary>Details</summary>
Motivation: AMC is challenging due to signal interference and noise, requiring innovative solutions for efficient spectrum management.

Method: Integrates higher-order statistics and cumulant estimation with LLMs, using natural language prompts for one-shot classification.

Result: Demonstrates competitive performance across modulation schemes and SNRs, reducing the need for channel-specific models.

Conclusion: The framework enables scalable, interpretable signal classification, advancing next-generation wireless networks.

Abstract: Automatic Modulation Classification (AMC) is critical for efficient spectrum
management and robust wireless communications. However, AMC remains challenging
due to the complex interplay of signal interference and noise. In this work, we
propose an innovative framework that integrates traditional signal processing
techniques with Large-Language Models (LLMs) to address AMC. Our approach
leverages higher-order statistics and cumulant estimation to convert
quantitative signal features into structured natural language prompts. By
incorporating exemplar contexts into these prompts, our method exploits the
LLM's inherent familiarity with classical signal processing, enabling effective
one-shot classification without additional training or preprocessing (e.g.,
denoising). Experimental evaluations on synthetically generated datasets,
spanning both noiseless and noisy conditions, demonstrate that our framework
achieves competitive performance across diverse modulation schemes and
Signal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robust
foundation models in wireless communications across varying channel conditions,
significantly reducing the expense associated with developing channel-specific
models. This work lays the foundation for scalable, interpretable, and
versatile signal classification systems in next-generation wireless networks.
The source code is available at https://github.com/RU-SIT/context-is-king

</details>


### [263] [BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling](https://arxiv.org/pdf/2503.02445)
*Hao Li, Yuhao Huang, Chang Xu, Viktor Schlegel, Renhe Jiang, Riza Batista-Navarro, Goran Nenadic, Jiang Bian*

Main category: cs.LG

TL;DR: The paper introduces Text-Controlled TSG, a method for generating realistic time-series data using textual descriptions, and proposes a novel LLM-based Multi-Agent framework to address data scarcity. The BRIDGE framework achieves state-of-the-art results in generation fidelity and controllability.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require cross-domain time-series generation (TSG) with controlled generation tailored to domain-specific constraints, which existing methods lack. Text can provide semantic insights and domain-specific guidance for TSG.

Method: The paper introduces a novel LLM-based Multi-Agent framework to synthesize text-to-TS datasets and proposes BRIDGE, a hybrid framework integrating semantic prototypes with text descriptions for domain-level guidance.

Result: BRIDGE achieves state-of-the-art generation fidelity on 11 of 12 datasets and improves controllability by 12.52% (MSE) and 6.34% (MAE) compared to no-text-input generation.

Conclusion: Text-Controlled TSG, supported by the BRIDGE framework, demonstrates significant potential for generating tailored time-series data with improved fidelity and controllability.

Abstract: Time-series Generation (TSG) is a prominent research area with broad
applications in simulations, data augmentation, and counterfactual analysis.
While existing methods have shown promise in unconditional single-domain TSG,
real-world applications demand for cross-domain approaches capable of
controlled generation tailored to domain-specific constraints and
instance-level requirements. In this paper, we argue that text can provide
semantic insights, domain information and instance-specific temporal patterns,
to guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused
on generating realistic time series by incorporating textual descriptions. To
address data scarcity in this setting, we propose a novel LLM-based Multi-Agent
framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,
we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates
semantic prototypes with text description for supporting domain-level guidance.
This approach achieves state-of-the-art generation fidelity on 11 of 12
datasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared
to no text input generation, highlighting its potential for generating tailored
time-series data.

</details>


### [264] [Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion](https://arxiv.org/pdf/2505.03118)
*Dmytro Shamatrin*

Main category: cs.LG

TL;DR: The paper introduces an adaptive thresholding mechanism for multi-label classification, combining global and local signals to improve performance under class imbalance and noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional MLC methods use fixed thresholds or treat labels independently, ignoring context and global rarity, leading to suboptimal performance.

Method: Proposes a lightweight, interpretable architecture with differentiable penalties in the loss, using IDF-based global and KNN-based local signals for adaptive thresholding.

Result: Achieves a macro-F1 of 0.1712 on AmazonCat-13K, outperforming tree-based and transformer-based methods.

Conclusion: The approach is effective, modular, and reproducible, with potential for future extensions.

Abstract: Multi-label classification (MLC) requires predicting multiple labels per
sample, often under heavy class imbalance and noisy conditions. Traditional
approaches apply fixed thresholds or treat labels independently, overlooking
context and global rarity. We introduce an adaptive thresholding mechanism that
fuses global (IDF-based) and local (KNN-based) signals to produce per-label,
per-instance thresholds. Instead of applying these as hard cutoffs, we treat
them as differentiable penalties in the loss, providing smooth supervision and
better calibration. Our architecture is lightweight, interpretable, and highly
modular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,
substantially outperforming tree-based and pretrained transformer-based
methods. We release full code for reproducibility and future extensions.

</details>


### [265] [Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation](https://arxiv.org/pdf/2505.03155)
*Max Qiushi Lin, Jincheng Mei, Matin Aghaei, Michael Lu, Bo Dai, Alekh Agarwal, Dale Schuurmans, Csaba Szepesvari, Sharan Vaswani*

Main category: cs.LG

TL;DR: The paper shows that approximation error in Softmax PG with linear function approximation (Lin-SPG) doesn't affect global convergence, even in stochastic bandit settings. It identifies necessary/sufficient feature conditions for asymptotic convergence and proves $O(1/T)$ convergence with a problem-specific learning rate.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of approximation error on the global convergence of policy gradient methods, especially in large state-action spaces with function approximation.

Method: Focuses on Softmax PG with linear function approximation (Lin-SPG), analyzing its convergence properties under specific feature conditions.

Result: Proves that Lin-SPG achieves $O(1/T)$ convergence to the optimal policy with a problem-specific learning rate and asymptotic convergence with any constant learning rate.

Conclusion: The approximation error is irrelevant to Lin-SPG's global convergence, and the identified feature conditions ensure asymptotic convergence to the optimal policy.

Abstract: Policy gradient (PG) methods have played an essential role in the empirical
successes of reinforcement learning. In order to handle large state-action
spaces, PG methods are typically used with function approximation. In this
setting, the approximation error in modeling problem-dependent quantities is a
key notion for characterizing the global convergence of PG methods. We focus on
Softmax PG with linear function approximation (referred to as
$\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant
to the algorithm's global convergence even for the stochastic bandit setting.
Consequently, we first identify the necessary and sufficient conditions on the
feature representation that can guarantee the asymptotic global convergence of
$\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$
iterations of $\texttt{Lin-SPG}$ with a problem-specific learning rate result
in an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that
$\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure
asymptotic global convergence to the optimal policy.

</details>


### [266] [Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis](https://arxiv.org/pdf/2505.03165)
*Nikita Ravi, Abhinav Goel, James C. Davis, George K. Thiruvathukal*

Main category: cs.LG

TL;DR: The paper addresses reproducibility challenges in deep learning, proposing systematic guidelines to improve it through environment replication, transparency, and sensitivity analysis.


<details>
  <summary>Details</summary>
Motivation: Reproducibility is critical for reliability in deep learning, yet many studies fail to replicate results due to environmental, transparency, and stochastic issues.

Method: The paper introduces guidelines for reproducibility, including replicating software environments, end-to-end algorithms, architectural transparency, and sensitivity analysis.

Result: The guidelines help improve reproducibility, as demonstrated in a case study, highlighting patterns and anti-patterns for effective deployment.

Conclusion: The proposed strategies bridge the gap between research and practice, ensuring deep learning innovations are reproducible and deployable.

Abstract: The field of deep learning has witnessed significant breakthroughs, spanning
various applications, and fundamentally transforming current software
capabilities. However, alongside these advancements, there have been increasing
concerns about reproducing the results of these deep learning methods. This is
significant because reproducibility is the foundation of reliability and
validity in software development, particularly in the rapidly evolving domain
of deep learning. The difficulty of reproducibility may arise due to several
reasons, including having differences from the original execution environment,
incompatible software libraries, proprietary data and source code, lack of
transparency, and the stochastic nature in some software. A study conducted by
the Nature journal reveals that more than 70% of researchers failed to
reproduce other researchers experiments and over 50% failed to reproduce their
own experiments. Irreproducibility of deep learning poses significant
challenges for researchers and practitioners. To address these concerns, this
paper presents a systematic approach at analyzing and improving the
reproducibility of deep learning models by demonstrating these guidelines using
a case study. We illustrate the patterns and anti-patterns involved with these
guidelines for improving the reproducibility of deep learning models. These
guidelines encompass establishing a methodology to replicate the original
software environment, implementing end-to-end training and testing algorithms,
disclosing architectural designs, and enhancing transparency in data processing
and training pipelines. We also conduct a sensitivity analysis to understand
the model performance across diverse conditions. By implementing these
strategies, we aim to bridge the gap between research and practice, so that
innovations in deep learning can be effectively reproduced and deployed within
software.

</details>


### [267] [Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning](https://arxiv.org/pdf/2505.03172)
*Caleb Chuck, Fan Feng, Carl Qi, Chang Shi, Siddhant Agarwal, Amy Zhang, Scott Niekum*

Main category: cs.LG

TL;DR: HInt improves GCRL in object-centric domains by combining hindsight relabeling with interactions, defined via null counterfactuals, enhancing sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Hindsight relabeling struggles in object-centric domains due to misleading rewards for non-interactive trajectories, hindering learning.

Method: Proposes HInt, which integrates interactions (defined via null counterfactuals) with hindsight relabeling, using NCII for interaction inference.

Result: NCII improves interaction inference accuracy, and HInt boosts sample efficiency by up to 4x in tested domains.

Conclusion: HInt effectively addresses limitations of hindsight relabeling in object-centric tasks, leveraging interactions for better RL performance.

Abstract: Hindsight relabeling is a powerful tool for overcoming sparsity in
goal-conditioned reinforcement learning (GCRL), especially in certain domains
such as navigation and locomotion. However, hindsight relabeling can struggle
in object-centric domains. For example, suppose that the goal space consists of
a robotic arm pushing a particular target block to a goal location. In this
case, hindsight relabeling will give high rewards to any trajectory that does
not interact with the block. However, these behaviors are only useful when the
object is already at the goal -- an extremely rare case in practice. A dataset
dominated by these kinds of trajectories can complicate learning and lead to
failures. In object-centric domains, one key intuition is that meaningful
trajectories are often characterized by object-object interactions such as
pushing the block with the gripper. To leverage this intuition, we introduce
Hindsight Relabeling using Interactions (HInt), which combines interactions
with hindsight relabeling to improve the sample efficiency of downstream RL.
However because interactions do not have a consensus statistical definition
tractable for downstream GCRL, we propose a definition of interactions based on
the concept of null counterfactual: a cause object is interacting with a target
object if, in a world where the cause object did not exist, the target object
would have different transition dynamics. We leverage this definition to infer
interactions in Null Counterfactual Interaction Inference (NCII), which uses a
"nulling'' operation with a learned model to infer interactions. NCII is able
to achieve significantly improved interaction inference accuracy in both simple
linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air
Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.

</details>


### [268] [RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion](https://arxiv.org/pdf/2505.03178)
*Jiawei Wang, Xintao Yan, Yao Mu, Haowei Sun, Zhong Cao, Henry X. Liu*

Main category: cs.LG

TL;DR: RADE is a simulation framework for generating realistic, risk-adjustable traffic scenes for autonomous vehicle testing, using a multi-agent diffusion model and risk-conditioned behaviors.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating safety-critical scenarios lack realism and scalability, relying on adversarial interactions. RADE aims to address this by preserving naturalistic multi-agent interactions with controllable risk.

Method: RADE uses a multi-agent diffusion architecture to model all agents' behaviors, conditioned on a surrogate risk measure. It includes a tokenized dynamics check module for physical plausibility.

Result: Validated on the rounD dataset, RADE maintains statistical realism across risk levels and increases safety-critical events as risk rises.

Conclusion: RADE is a scalable, realistic tool for autonomous vehicle safety evaluation, outperforming traditional adversarial methods.

Abstract: Generating safety-critical scenarios in high-fidelity simulations offers a
promising and cost-effective approach for efficient testing of autonomous
vehicles. Existing methods typically rely on manipulating a single vehicle's
trajectory through sophisticated designed objectives to induce adversarial
interactions, often at the cost of realism and scalability. In this work, we
propose the Risk-Adjustable Driving Environment (RADE), a simulation framework
that generates statistically realistic and risk-adjustable traffic scenes.
Built upon a multi-agent diffusion architecture, RADE jointly models the
behavior of all agents in the environment and conditions their trajectories on
a surrogate risk measure. Unlike traditional adversarial methods, RADE learns
risk-conditioned behaviors directly from data, preserving naturalistic
multi-agent interactions with controllable risk levels. To ensure physical
plausibility, we incorporate a tokenized dynamics check module that efficiently
filters generated trajectories using a motion vocabulary. We validate RADE on
the real-world rounD dataset, demonstrating that it preserves statistical
realism across varying risk levels and naturally increases the likelihood of
safety-critical events as the desired risk level grows up. Our results
highlight RADE's potential as a scalable and realistic tool for AV safety
evaluation.

</details>


### [269] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/pdf/2505.03181)
*Jake Grigsby, Yuke Zhu, Michael Ryoo, Juan Carlos Niebles*

Main category: cs.LG

TL;DR: The paper proposes using offline-to-online reinforcement learning (RL) to fine-tune vision-language models (VLMs) for agent tasks, addressing their limitations compared to LLMs.


<details>
  <summary>Details</summary>
Motivation: VLMs lag behind LLMs in agent tasks due to strict output syntax requirements and focus on open-ended QA. Supervised fine-tuning (SFT) alone is insufficient.

Method: Off-policy RL is used to fine-tune VLMs, allowing self-improvement and learning from low-quality datasets while retaining SFT stability.

Result: The technique is demonstrated with two open-weight VLMs across three multi-modal agent domains.

Conclusion: RL enhances VLMs for agent tasks, bridging the gap with LLMs and enabling practical applications.

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>


### [270] [Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions](https://arxiv.org/pdf/2505.03194)
*Yiding Chen, Yiyi Zhang, Owen Oertell, Wen Sun*

Main category: cs.LG

TL;DR: Consistency models enable fast one-step data generation and multistep sampling, with theoretical guarantees on convergence under mild assumptions.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of iterative sampling in diffusion models by proposing consistency models for direct noise-to-data mapping.

Method: Analyzes convergence of consistency models under approximate self-consistency, with mild data assumptions and applicability to various forward processes.

Result: Samples are close to the target distribution in Wasserstein or total variation distance, depending on data properties. Multistep sampling improves quality.

Conclusion: Consistency models offer efficient and high-quality data generation, supported by theoretical analysis and case studies.

Abstract: Diffusion models accomplish remarkable success in data generation tasks
across various domains. However, the iterative sampling process is
computationally expensive. Consistency models are proposed to learn consistency
functions to map from noise to data directly, which allows one-step fast data
generation and multistep sampling to improve sample quality. In this paper, we
study the convergence of consistency models when the self-consistency property
holds approximately under the training distribution. Our analysis requires only
mild data assumption and applies to a family of forward processes. When the
target data distribution has bounded support or has tails that decay
sufficiently fast, we show that the samples generated by the consistency model
are close to the target distribution in Wasserstein distance; when the target
distribution satisfies some smoothness assumption, we show that with an
additional perturbation step for smoothing, the generated samples are close to
the target distribution in total variation distance. We provide two case
studies with commonly chosen forward processes to demonstrate the benefit of
multistep sampling.

</details>


### [271] [Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights](https://arxiv.org/pdf/2505.03205)
*Zhaiming Shen, Alex Havrilla, Rongjie Lai, Alexander Cloninger, Wenjing Liao*

Main category: cs.LG

TL;DR: The paper provides a theoretical analysis of transformers' performance in regression tasks with noisy data on a manifold, showing their ability to leverage low-dimensional structures despite high-dimensional noise.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical understanding of transformers' performance, especially in tasks with noisy data and low-dimensional structures.

Method: Analyzes transformers for regression tasks with noisy input data on a manifold, proving approximation and generalization errors based on intrinsic manifold dimension.

Result: Transformers can effectively learn from low-dimensional structures even with high-dimensional noise, with performance tied to the manifold's intrinsic dimension.

Conclusion: The study establishes a theoretical foundation for transformers' performance in noisy, low-dimensional settings, with potential broader implications for understanding their capabilities.

Abstract: Transformers serve as the foundational architecture for large language and
video generation models, such as GPT, BERT, SORA and their successors.
Empirical studies have demonstrated that real-world data and learning tasks
exhibit low-dimensional structures, along with some noise or measurement error.
The performance of transformers tends to depend on the intrinsic dimension of
the data/tasks, though theoretical understandings remain largely unexplored for
transformers. This work establishes a theoretical foundation by analyzing the
performance of transformers for regression tasks involving noisy input data on
a manifold. Specifically, the input data are in a tubular neighborhood of a
manifold, while the ground truth function depends on the projection of the
noisy data onto the manifold. We prove approximation and generalization errors
which crucially depend on the intrinsic dimension of the manifold. Our results
demonstrate that transformers can leverage low-complexity structures in
learning task even when the input data are perturbed by high-dimensional noise.
Our novel proof technique constructs representations of basic arithmetic
operations by transformers, which may hold independent interest.

</details>


### [272] [Partial Label Clustering](https://arxiv.org/pdf/2505.03207)
*Yutong Xie, Fuchao Yang, Yuheng Jia*

Main category: cs.LG

TL;DR: The paper introduces a partial label clustering (PLC) method that leverages partial labels to improve clustering performance by disambiguating labels, constructing constraints, and propagating them via a dual-graph learning approach.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of improving clustering performance using limited partial labels, a common scenario in weakly supervised learning.

Method: Constructs a weight matrix for examples, disambiguates candidate labels, builds must-link/cannot-link constraints, propagates constraints via dual-graph learning, and integrates these steps into a joint model.

Result: The method outperforms state-of-the-art constrained clustering and partial label learning methods, especially with limited annotated samples.

Conclusion: The proposed PLC method effectively enhances clustering by leveraging partial labels and constraint propagation, with theoretical and empirical validation.

Abstract: Partial label learning (PLL) is a significant weakly supervised learning
framework, where each training example corresponds to a set of candidate labels
and only one label is the ground-truth label. For the first time, this paper
investigates the partial label clustering problem, which takes advantage of the
limited available partial labels to improve the clustering performance.
Specifically, we first construct a weight matrix of examples based on their
relationships in the feature space and disambiguate the candidate labels to
estimate the ground-truth label based on the weight matrix. Then, we construct
a set of must-link and cannot-link constraints based on the disambiguation
results. Moreover, we propagate the initial must-link and cannot-link
constraints based on an adversarial prior promoted dual-graph learning
approach. Finally, we integrate weight matrix construction, label
disambiguation, and pairwise constraints propagation into a joint model to
achieve mutual enhancement. We also theoretically prove that a better
disambiguated label matrix can help improve clustering performance.
Comprehensive experiments demonstrate our method realizes superior performance
when comparing with state-of-the-art constrained clustering methods, and
outperforms PLL and semi-supervised PLL methods when only limited samples are
annotated. The code is publicly available at https://github.com/xyt-ml/PLC.

</details>


### [273] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/pdf/2505.03209)
*Borui Wang, Kathleen McKeown, Rex Ying*

Main category: cs.LG

TL;DR: DYSTIL integrates LLMs with RL to improve generalization, sample efficiency, and interpretability by dynamically generating and internalizing textual strategies.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods from expert demonstrations suffer from poor generalization, low sample efficiency, and lack of interpretability.

Method: DYSTIL queries an LLM to generate textual strategies based on advantage estimations and expert demonstrations, then internalizes these into the RL agent via policy optimization.

Result: DYSTIL outperforms state-of-the-art methods by 17.75% in success rate and shows higher sample efficiency.

Conclusion: DYSTIL effectively addresses limitations of current RL methods by leveraging LLMs for strategy induction and policy improvement.

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>


### [274] [Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach](https://arxiv.org/pdf/2505.03230)
*Yue Chen, Hui Kang, Jiahui Li, Geng Su, Boxiong Wang, Jiacheng Wang, Cong Liang, Shuang Liang, Dusit Niyato*

Main category: cs.LG

TL;DR: The paper proposes a UAV-assisted MEC system with directional antennas for SWIPT in 6G IoT networks, addressing energy and computational trade-offs via a bi-objective optimization solved by an improved SAC algorithm.


<details>
  <summary>Details</summary>
Motivation: Challenges in SWIPT for 6G IoT in remote/disaster areas with no ground infrastructure motivate the need for UAV-assisted MEC systems.

Method: A bi-objective optimization problem is formulated and solved using an improved SAC algorithm with action simplification.

Result: Simulations show the method outperforms baselines in energy efficiency and computational performance, with strong generalization.

Conclusion: The proposed approach effectively balances energy and computational needs, validated by its performance in complex scenarios.

Abstract: The integration of simultaneous wireless information and power transfer
(SWIPT) technology in 6G Internet of Things (IoT) networks faces significant
challenges in remote areas and disaster scenarios where ground infrastructure
is unavailable. This paper proposes a novel unmanned aerial vehicle
(UAV)-assisted mobile edge computing (MEC) system enhanced by directional
antennas to provide both computational resources and energy support for ground
IoT terminals. However, such systems require multiple trade-off policies to
balance UAV energy consumption, terminal battery levels, and computational
resource allocation under various constraints, including limited UAV battery
capacity, non-linear energy harvesting characteristics, and dynamic task
arrivals. To address these challenges comprehensively, we formulate a
bi-objective optimization problem that simultaneously considers system energy
efficiency and terminal battery sustainability. We then reformulate this
non-convex problem with a hybrid solution space as a Markov decision process
(MDP) and propose an improved soft actor-critic (SAC) algorithm with an action
simplification mechanism to enhance its convergence and generalization
capabilities. Simulation results have demonstrated that our proposed approach
outperforms various baselines in different scenarios, achieving efficient
energy management while maintaining high computational performance.
Furthermore, our method shows strong generalization ability across different
scenarios, particularly in complex environments, validating the effectiveness
of our designed boundary penalty and charging reward mechanisms.

</details>


### [275] [MDPs with a State Sensing Cost](https://arxiv.org/pdf/2505.03280)
*Vansh Kapoor, Jayakrishnan Nair*

Main category: cs.LG

TL;DR: The paper addresses sequential decision-making problems where sensing the environment state incurs costs. It formulates this as an MDP with sensing costs and proposes a heuristic algorithm for efficient policy computation.


<details>
  <summary>Details</summary>
Motivation: To balance the trade-off between optimal actions and sensing costs in sequential decision-making, where sensing is costly.

Method: Formulates the problem as a discounted cost MDP with an expanded state space. Introduces a heuristic algorithm based on policy improvement and bounds sub-optimality gaps for restricted policies.

Result: The heuristic algorithm performs close to the optimal policy in practice, as demonstrated by a numerical case study.

Conclusion: The work provides a practical solution for costly sensing scenarios, with theoretical bounds and empirical validation.

Abstract: In many practical sequential decision-making problems, tracking the state of
the environment incurs a sensing/communication/computation cost. In these
settings, the agent's interaction with its environment includes the additional
component of deciding $\textit{when}$ to sense the state, in a manner that
balances the value associated with optimal (state-specific) actions and the
cost of sensing. We formulate this as an expected discounted cost Markov
Decision Process (MDP), wherein the agent incurs an additional cost for sensing
its next state, but has the option to take actions while remaining 'blind' to
the system state.
  We pose this problem as a classical discounted cost MDP with an expanded
(countably infinite) state space. While computing the optimal policy for this
MDP is intractable in general, we bound the sub-optimality gap associated with
optimal policies in a restricted class, where the number of consecutive
non-sensing (a.k.a., blind) actions is capped. We also design a computationally
efficient heuristic algorithm based on policy improvement, which in practice
performs close to the optimal policy. Finally, we benchmark against the state
of the art via a numerical case study.

</details>


### [276] [Physics-inspired Energy Transition Neural Network for Sequence Learning](https://arxiv.org/pdf/2505.03281)
*Zhou Wu, Junyi An, Baile Xu, Furao Shen, Jian Zhao*

Main category: cs.LG

TL;DR: PETNN, a physics-inspired RNN, outperforms Transformers in long-term dependency tasks with lower complexity.


<details>
  <summary>Details</summary>
Motivation: Reassess RNNs' long-term learning capabilities and challenge Transformers' dominance by proposing a more efficient recurrent structure.

Method: Proposes PETNN, inspired by physics energy transition models, to enhance RNNs' memory mechanism for long-term dependencies.

Result: PETNN outperforms Transformer-based methods in sequence tasks and has lower complexity.

Conclusion: PETNN is an effective RNN alternative to Transformers, showcasing potential for RNN development in Transformer-dominated fields.

Abstract: Recently, the superior performance of Transformers has made them a more
robust and scalable solution for sequence modeling than traditional recurrent
neural networks (RNNs). However, the effectiveness of Transformer in capturing
long-term dependencies is primarily attributed to their comprehensive
pair-modeling process rather than inherent inductive biases toward sequence
semantics. In this study, we explore the capabilities of pure RNNs and reassess
their long-term learning mechanisms. Inspired by the physics energy transition
models that track energy changes over time, we propose a effective recurrent
structure called the``Physics-inspired Energy Transition Neural Network"
(PETNN). We demonstrate that PETNN's memory mechanism effectively stores
information over long-term dependencies. Experimental results indicate that
PETNN outperforms transformer-based methods across various sequence tasks.
Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower
complexity. Our study presents an optimal foundational recurrent architecture
and highlights the potential for developing effective recurrent neural networks
in fields currently dominated by Transformer.

</details>


### [277] [Unraveling the Rainbow: can value-based methods schedule?](https://arxiv.org/pdf/2505.03323)
*Arthur Corrêa, Alexandre Jesus, Cristóvão Silva, Samuel Moniz*

Main category: cs.LG

TL;DR: Value-based deep reinforcement learning methods, often overlooked in combinatorial optimization, are shown to match or outperform policy-based methods like proximal policy optimization in job-shop scheduling problems.


<details>
  <summary>Details</summary>
Motivation: The combinatorial optimization community has favored policy-based methods, neglecting value-based approaches despite their success in other domains. This work aims to evaluate the potential of value-based algorithms in combinatorial problems.

Method: A comprehensive empirical evaluation of value-based algorithms (e.g., deep q-network and extensions) was conducted on job-shop and flexible job-shop scheduling problems.

Result: Value-based methods matched or outperformed proximal policy optimization, challenging the superiority of policy-based approaches in combinatorial optimization.

Conclusion: Value-based strategies deserve more attention in combinatorial optimization, as they can be competitive or superior to policy-based methods.

Abstract: Recently, deep reinforcement learning has emerged as a promising approach for
solving complex combinatorial optimization problems. Broadly, deep
reinforcement learning methods fall into two categories: policy-based and
value-based. While value-based approaches have achieved notable success in
domains such as the Arcade Learning Environment, the combinatorial optimization
community has predominantly favored policy-based methods, often overlooking the
potential of value-based algorithms. In this work, we conduct a comprehensive
empirical evaluation of value-based algorithms, including the deep q-network
and several of its advanced extensions, within the context of two complex
combinatorial problems: the job-shop and the flexible job-shop scheduling
problems, two fundamental challenges with multiple industrial applications. Our
results challenge the assumption that policy-based methods are inherently
superior for combinatorial optimization. We show that several value-based
approaches can match or even outperform the widely adopted proximal policy
optimization algorithm, suggesting that value-based strategies deserve greater
attention from the combinatorial optimization community. Our code is openly
available at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.

</details>


### [278] [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/pdf/2505.03335)
*Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang*

Main category: cs.LG

TL;DR: The paper introduces Absolute Zero, a new RLVR paradigm where a model self-generates tasks to maximize learning, achieving SOTA performance without external data.


<details>
  <summary>Details</summary>
Motivation: To address scalability and learning limitations of human supervision in RLVR, especially for superintelligent AI.

Method: Proposes Absolute Zero Reasoner (AZR), which self-evolves tasks and verifies answers using a code executor, eliminating reliance on external data.

Result: AZR outperforms zero-setting models using human-curated data, achieving SOTA in coding and math reasoning.

Conclusion: AZR demonstrates scalable, open-ended learning without human supervision, applicable across model scales and classes.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in
enhancing the reasoning capabilities of large language models by learning
directly from outcome-based rewards. Recent RLVR works that operate under the
zero setting avoid supervision in labeling the reasoning process, but still
depend on manually curated collections of questions and answers for training.
The scarcity of high-quality, human-produced examples raises concerns about the
long-term scalability of relying on human supervision, a challenge already
evident in the domain of language model pretraining. Furthermore, in a
hypothetical future where AI surpasses human intelligence, tasks provided by
humans may offer limited learning potential for a superintelligent system. To
address these concerns, we propose a new RLVR paradigm called Absolute Zero, in
which a single model learns to propose tasks that maximize its own learning
progress and improves reasoning by solving them, without relying on any
external data. Under this paradigm, we introduce the Absolute Zero Reasoner
(AZR), a system that self-evolves its training curriculum and reasoning ability
by using a code executor to both validate proposed code reasoning tasks and
verify answers, serving as an unified source of verifiable reward to guide
open-ended yet grounded learning. Despite being trained entirely without
external data, AZR achieves overall SOTA performance on coding and mathematical
reasoning tasks, outperforming existing zero-setting models that rely on tens
of thousands of in-domain human-curated examples. Furthermore, we demonstrate
that AZR can be effectively applied across different model scales and is
compatible with various model classes.

</details>


### [279] [Geospatial Mechanistic Interpretability of Large Language Models](https://arxiv.org/pdf/2505.03368)
*Stef De Sabbata, Stefano Mizzaro, Kevin Roitero*

Main category: cs.LG

TL;DR: The paper introduces a framework for studying how LLMs process geographical information using spatial analysis and mechanistic interpretability.


<details>
  <summary>Details</summary>
Motivation: To understand the internal representations of LLMs when handling geographical data, addressing a gap in current research.

Method: Uses probing and mechanistic interpretability, including spatial autocorrelation, to analyze LLMs' internal structures.

Result: Features for placenames show spatial patterns, revealing how LLMs process geographic information.

Conclusion: The framework advances the study of LLMs in geography, offering insights for future research and applications.

Abstract: Large Language Models (LLMs) have demonstrated unprecedented capabilities
across various natural language processing tasks. Their ability to process and
generate viable text and code has made them ubiquitous in many fields, while
their deployment as knowledge bases and "reasoning" tools remains an area of
ongoing research. In geography, a growing body of literature has been focusing
on evaluating LLMs' geographical knowledge and their ability to perform spatial
reasoning. However, very little is still known about the internal functioning
of these models, especially about how they process geographical information.
  In this chapter, we establish a novel framework for the study of geospatial
mechanistic interpretability - using spatial analysis to reverse engineer how
LLMs handle geographical information. Our aim is to advance our understanding
of the internal representations that these complex models generate while
processing geographical information - what one might call "how LLMs think about
geographic information" if such phrasing was not an undue anthropomorphism.
  We first outline the use of probing in revealing internal structures within
LLMs. We then introduce the field of mechanistic interpretability, discussing
the superposition hypothesis and the role of sparse autoencoders in
disentangling polysemantic internal representations of LLMs into more
interpretable, monosemantic features. In our experiments, we use spatial
autocorrelation to show how features obtained for placenames display spatial
patterns related to their geographic location and can thus be interpreted
geospatially, providing insights into how these models process geographical
information. We conclude by discussing how our framework can help shape the
study and use of foundation models in geography.

</details>


### [280] [SPAP: Structured Pruning via Alternating Optimization and Penalty Methods](https://arxiv.org/pdf/2505.03373)
*Hanyu Hu, Xiaoming Yuan*

Main category: cs.LG

TL;DR: SPAP is a novel structured pruning framework for LLMs that uses optimization theory to efficiently prune models while minimizing performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for LLMs suffer from performance degradation, heuristic reliance, or costly finetuning, limiting their practicality.

Method: SPAP employs a mixed-integer optimization model, a penalty method for pruning decisions, and an alternating minimization algorithm for efficient weight updates.

Result: SPAP outperforms state-of-the-art methods, achieving linear inference speedups (1.29× at 30% sparsity) and proportional memory reductions.

Conclusion: SPAP provides a practical, optimization-driven solution for pruning LLMs without compromising performance.

Abstract: The deployment of large language models (LLMs) is often constrained by their
substantial computational and memory demands. While structured pruning presents
a viable approach by eliminating entire network components, existing methods
suffer from performance degradation, reliance on heuristic metrics, or
expensive finetuning. To address these challenges, we propose SPAP (Structured
Pruning via Alternating Optimization and Penalty Methods), a novel and
efficient structured pruning framework for LLMs grounded in optimization
theory. SPAP formulates the pruning problem through a mixed-integer
optimization model, employs a penalty method that effectively makes pruning
decisions to minimize pruning errors, and introduces an alternating
minimization algorithm tailored to the splittable problem structure for
efficient weight updates and performance recovery. Extensive experiments on
OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over
state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at
30% sparsity) and proportional memory reductions. Our work offers a practical,
optimization-driven solution for pruning LLMs while preserving model
performance.

</details>


### [281] [Physics-informed neural network estimation of active material properties in time-dependent cardiac biomechanical models](https://arxiv.org/pdf/2505.03382)
*Matthias Höfler, Francesco Regazzoni, Stefano Pagani, Elias Karabelas, Christoph Augustin, Gundolf Haase, Gernot Plank, Federica Caforio*

Main category: cs.LG

TL;DR: The paper explores using physics-informed neural networks (PINNs) to infer active stress parameters in cardiac biomechanics from imaging data, improving accuracy in diagnosing heart conditions like fibrosis.


<details>
  <summary>Details</summary>
Motivation: Accurate assessment of active stress parameters in cardiac biomechanics is challenging clinically, especially with limited imaging data. PINNs offer a potential solution.

Method: The study employs PINNs with adaptive weighting, regularization, Fourier features, and optimized architectures to reconstruct active stress fields from noisy imaging data.

Result: The method successfully reconstructs active stress fields with high spatial resolution, even in noisy conditions, and aids in detecting tissue inhomogeneities and fibrotic scars.

Conclusion: This approach enhances the diagnosis and treatment of heart conditions linked to fibrosis, showcasing PINNs' potential in cardiac biomechanics.

Abstract: Active stress models in cardiac biomechanics account for the mechanical
deformation caused by muscle activity, thus providing a link between the
electrophysiological and mechanical properties of the tissue. The accurate
assessment of active stress parameters is fundamental for a precise
understanding of myocardial function but remains difficult to achieve in a
clinical setting, especially when only displacement and strain data from
medical imaging modalities are available. This work investigates, through an
in-silico study, the application of physics-informed neural networks (PINNs)
for inferring active contractility parameters in time-dependent cardiac
biomechanical models from these types of imaging data. In particular, by
parametrising the sought state and parameter field with two neural networks,
respectively, and formulating an energy minimisation problem to search for the
optimal network parameters, we are able to reconstruct in various settings
active stress fields in the presence of noise and with a high spatial
resolution. To this end, we also advance the vanilla PINN learning algorithm
with the use of adaptive weighting schemes, ad-hoc regularisation strategies,
Fourier features, and suitable network architectures. In addition, we
thoroughly analyse the influence of the loss weights in the reconstruction of
active stress parameters. Finally, we apply the method to the characterisation
of tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.
This approach opens a new pathway to significantly improve the diagnosis,
treatment planning, and management of heart conditions associated with cardiac
fibrosis.

</details>


### [282] [Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation](https://arxiv.org/pdf/2505.03387)
*Diego Perazzolo, Pietro Fanton, Ilaria Barison, Marny Fedrigo, Annalisa Angelini, Chiara Castellani, Enrico Grisan*

Main category: cs.LG

TL;DR: A machine learning framework combining feature selection and data augmentation improves classification accuracy and interpretability for small omics datasets.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of limited sample sizes and poor interpretability in omics datasets, ensuring reliable and transparent model decisions.

Method: Integrates feature selection with data augmentation, tested on a public dataset (E-MTAB 8026) using bootstrap analysis in six binary classification scenarios.

Result: Achieves consistent cross-validated performance on small datasets, generalizing well to larger test sets.

Conclusion: Balancing accuracy and feature selection, synthetic data enhances generalization in limited-sample scenarios.

Abstract: Given the increasing complexity of omics datasets, a key challenge is not
only improving classification performance but also enhancing the transparency
and reliability of model decisions. Effective model performance and feature
selection are fundamental for explainability and reliability. In many cases,
high dimensional omics datasets suffer from limited number of samples due to
clinical constraints, patient conditions, phenotypes rarity and others
conditions. Current omics based classification models often suffer from narrow
interpretability, making it difficult to discern meaningful insights where
trust and reproducibility are critical. This study presents a machine learning
based classification framework that integrates feature selection with data
augmentation techniques to achieve high standard classification accuracy while
ensuring better interpretability. Using the publicly available dataset (E MTAB
8026), we explore a bootstrap analysis in six binary classification scenarios
to evaluate the proposed model's behaviour. We show that the proposed pipeline
yields cross validated perfomance on small dataset that is conserved when the
trained classifier is applied to a larger test set. Our findings emphasize the
fundamental balance between accuracy and feature selection, highlighting the
positive effect of introducing synthetic data for better generalization, even
in scenarios with very limited samples availability.

</details>


### [283] [Concept Factorization via Self-Representation and Adaptive Graph Structure Learning](https://arxiv.org/pdf/2505.03390)
*Zhengqin Yang, Di Wu, Jia Chen, Xin Luo*

Main category: cs.LG

TL;DR: A new Concept Factorization model (CFSRAG) adaptively learns graph structures for better clustering, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing CF models rely heavily on initial graph construction, limiting performance. CFSRAG aims to dynamically learn the graph structure for improved clustering.

Method: CFSRAG uses self-representation to learn data affinity and applies dynamic graph regularization to adaptively learn the geometric structure.

Result: Experiments on four datasets show CFSRAG outperforms state-of-the-art models.

Conclusion: CFSRAG effectively improves clustering by dynamically learning graph structures, demonstrating superior performance.

Abstract: Concept Factorization (CF) models have attracted widespread attention due to
their excellent performance in data clustering. In recent years, many variant
models based on CF have achieved great success in clustering by taking into
account the internal geometric manifold structure of the dataset and using
graph regularization techniques. However, their clustering performance depends
greatly on the construction of the initial graph structure. In order to enable
adaptive learning of the graph structure of the data, we propose a Concept
Factorization Based on Self-Representation and Adaptive Graph Structure
Learning (CFSRAG) Model. CFSRAG learns the affinity relationship between data
through a self-representation method, and uses the learned affinity matrix to
implement dynamic graph regularization constraints, thereby ensuring dynamic
learning of the internal geometric structure of the data. Finally, we give the
CFSRAG update rule and convergence analysis, and conduct comparative
experiments on four real datasets. The results show that our model outperforms
other state-of-the-art models.

</details>


### [284] [Automatic Calibration for Membership Inference Attack on Large Language Models](https://arxiv.org/pdf/2505.03392)
*Saleh Zare Zade, Yao Qiang, Xiangyu Zhou, Hui Zhu, Mohammad Amin Roshani, Prashant Khanduri, Dongxiao Zhu*

Main category: cs.LG

TL;DR: The paper introduces ACMIA, a novel framework for membership inference attacks on LLMs, addressing high false positives and dependency on reference models by using tunable temperature for probability calibration.


<details>
  <summary>Details</summary>
Motivation: Existing membership inference methods for LLMs suffer from high false positives or rely on impractical reference models, necessitating a more reliable and robust solution.

Method: ACMIA employs a tunable temperature to calibrate output probabilities, leveraging theoretical insights into LLM pre-training. It offers three configurations for varying model access levels.

Result: ACMIA outperforms state-of-the-art baselines on three benchmarks, proving highly effective, robust, and generalizable.

Conclusion: ACMIA provides a practical and superior solution for membership inference attacks on LLMs, with demonstrated effectiveness and robustness.

Abstract: Membership Inference Attacks (MIAs) have recently been employed to determine
whether a specific text was part of the pre-training data of Large Language
Models (LLMs). However, existing methods often misinfer non-members as members,
leading to a high false positive rate, or depend on additional reference models
for probability calibration, which limits their practicality. To overcome these
challenges, we introduce a novel framework called Automatic Calibration
Membership Inference Attack (ACMIA), which utilizes a tunable temperature to
calibrate output probabilities effectively. This approach is inspired by our
theoretical insights into maximum likelihood estimation during the pre-training
of LLMs. We introduce ACMIA in three configurations designed to accommodate
different levels of model access and increase the probability gap between
members and non-members, improving the reliability and robustness of membership
inference. Extensive experiments on various open-source LLMs demonstrate that
our proposed attack is highly effective, robust, and generalizable, surpassing
state-of-the-art baselines across three widely used benchmarks. Our code is
available at:
\href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.

</details>


### [285] [Prediction Models That Learn to Avoid Missing Values](https://arxiv.org/pdf/2505.03393)
*Lena Stempfle, Anton Matsson, Newton Mwai, Fredrik D. Johansson*

Main category: cs.LG

TL;DR: MA learning reduces reliance on missing values in ML models while maintaining accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Handling missing values at test time without adding bias or complexity, while preserving interpretability.

Method: Proposes missingness-avoiding (MA) framework with tailored algorithms for decision trees, tree ensembles, and sparse linear models, using classifier-specific regularization.

Result: MA models (MA-DT, MA-LASSO, MA-RF, MA-GBT) reduce reliance on missing features while maintaining competitive performance.

Conclusion: MA framework provides a practical tool for interpretable predictions with missing values.

Abstract: Handling missing values at test time is challenging for machine learning
models, especially when aiming for both high accuracy and interpretability.
Established approaches often add bias through imputation or excessive model
complexity via missingness indicators. Moreover, either method can obscure
interpretability, making it harder to understand how the model utilizes the
observed variables in predictions. We propose missingness-avoiding (MA) machine
learning, a general framework for training models to rarely require the values
of missing (or imputed) features at test time. We create tailored MA learning
algorithms for decision trees, tree ensembles, and sparse linear models by
incorporating classifier-specific regularization terms in their learning
objectives. The tree-based models leverage contextual missingness by reducing
reliance on missing values based on the observed context. Experiments on
real-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT
effectively reduce the reliance on features with missing values while
maintaining predictive performance competitive with their unregularized
counterparts. This shows that our framework gives practitioners a powerful tool
to maintain interpretability in predictions with test-time missing values.

</details>


### [286] [Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey](https://arxiv.org/pdf/2505.03418)
*Da Zheng, Lun Du, Junwei Su, Yuchen Tian, Yuqi Zhu, Jintian Zhang, Lanning Wei, Ningyu Zhang, Huajun Chen*

Main category: cs.LG

TL;DR: The paper surveys LLMs' capabilities and limitations in complex problem-solving, covering techniques like CoT reasoning, knowledge augmentation, and verification, while addressing domain-specific challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs can address real-world problem-solving challenges, including multi-step reasoning, domain knowledge integration, and result verification.

Method: Examines techniques like Chain-of-Thought reasoning, knowledge augmentation, and verification methods, alongside domain-specific applications.

Result: Identifies LLMs' strengths in problem-solving but highlights challenges like reasoning, knowledge integration, and verification.

Conclusion: Future LLM-based solutions must improve multi-step reasoning, domain knowledge integration, and result verification to advance complex problem-solving.

Abstract: Problem-solving has been a fundamental driver of human progress in numerous
domains. With advancements in artificial intelligence, Large Language Models
(LLMs) have emerged as powerful tools capable of tackling complex problems
across diverse domains. Unlike traditional computational systems, LLMs combine
raw computational power with an approximation of human reasoning, allowing them
to generate solutions, make inferences, and even leverage external
computational tools. However, applying LLMs to real-world problem-solving
presents significant challenges, including multi-step reasoning, domain
knowledge integration, and result verification. This survey explores the
capabilities and limitations of LLMs in complex problem-solving, examining
techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,
and various LLM-based and tool-based verification techniques. Additionally, we
highlight domain-specific challenges in various domains, such as software
engineering, mathematical reasoning and proving, data analysis and modeling,
and scientific research. The paper further discusses the fundamental
limitations of the current LLM solutions and the future directions of LLM-based
complex problems solving from the perspective of multi-step reasoning, domain
knowledge integration and result verification.

</details>


### [287] [Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense](https://arxiv.org/pdf/2505.03424)
*Kirill Lukyanov, Mikhail Drobyshevskiy, Georgii Sazonov, Mikhail Soloviov, Ilya Makarov*

Main category: cs.LG

TL;DR: GNN-AID is an open-source framework for analyzing, interpreting, and defending Graph Neural Networks (GNNs), combining interpretability and robustness for graph data.


<details>
  <summary>Details</summary>
Motivation: The need for Trusted AI (TAI) in graph data, where existing tools often lack interpretability and robustness integration.

Method: Built as a Python library on PyTorch-Geometric, GNN-AID includes attacks, defenses, interpretability methods, and a web interface for visualization and no-code features.

Result: GNN-AID supports developers and researchers in creating, analyzing, and customizing GNN models, while revealing conflicts between defense strategies.

Conclusion: GNN-AID bridges the gap in trusted AI for graph data, offering a flexible tool for both practical development and advanced research.

Abstract: The growing need for Trusted AI (TAI) highlights the importance of
interpretability and robustness in machine learning models. However, many
existing tools overlook graph data and rarely combine these two aspects into a
single solution. Graph Neural Networks (GNNs) have become a popular approach,
achieving top results across various tasks. We introduce GNN-AID (Graph Neural
Network Analysis, Interpretation, and Defense), an open-source framework
designed for graph data to address this gap. Built as a Python library, GNN-AID
supports advanced trust methods and architectural layers, allowing users to
analyze graph datasets and GNN behavior using attacks, defenses, and
interpretability methods.
  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,
and support for any GNNs through customizable interfaces. It also includes a
web interface with tools for graph visualization and no-code features like an
interactive model builder, simplifying the exploration and analysis of GNNs.
The framework also supports MLOps techniques, ensuring reproducibility and
result versioning to track and revisit analyses efficiently.
  GNN-AID is a flexible tool for developers and researchers. It helps
developers create, analyze, and customize graph models, while also providing
access to prebuilt datasets and models for quick experimentation. Researchers
can use the framework to explore advanced topics on the relationship between
interpretability and robustness, test defense strategies, and combine methods
to protect against different types of attacks.
  We also show how defenses against evasion and poisoning attacks can conflict
when applied to graph data, highlighting the complex connections between
defense strategies.
  GNN-AID is available at
\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}

</details>


### [288] [Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients](https://arxiv.org/pdf/2505.03432)
*Stefano Bruno, Sotirios Sabanis*

Main category: cs.LG

TL;DR: SGMs use Gaussian noise and reverse diffusion to model data, excelling in diverse domains. This work provides non-asymptotic Wasserstein-2 convergence guarantees for semiconvex distributions, broadening theoretical foundations.


<details>
  <summary>Details</summary>
Motivation: Existing convergence analyses for SGMs assume strong regularity conditions rarely met in practice. This work aims to relax these assumptions and provide rigorous guarantees for non-smooth, complex data.

Method: The study establishes non-asymptotic Wasserstein-2 convergence bounds for SGMs targeting semiconvex distributions, including cases with discontinuous gradients.

Result: The framework achieves optimal dependence on data dimension and convergence rate, accommodating practical distributions like Gaussian mixtures and double-well potentials.

Conclusion: This work bridges the gap between empirical success and theoretical guarantees for SGMs in non-smooth, complex data regimes.

Abstract: Score-based Generative Models (SGMs) approximate a data distribution by
perturbing it with Gaussian noise and subsequently denoising it via a learned
reverse diffusion process. These models excel at modeling complex data
distributions and generating diverse samples, achieving state-of-the-art
performance across domains such as computer vision, audio generation,
reinforcement learning, and computational biology. Despite their empirical
success, existing Wasserstein-2 convergence analysis typically assume strong
regularity conditions-such as smoothness or strict log-concavity of the data
distribution-that are rarely satisfied in practice. In this work, we establish
the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs
targeting semiconvex distributions with potentially discontinuous gradients.
Our upper bounds are explicit and sharp in key parameters, achieving optimal
dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of
order one. The framework accommodates a wide class of practically relevant
distributions, including symmetric modified half-normal distributions, Gaussian
mixtures, double-well potentials, and elastic net potentials. By leveraging
semiconvexity without requiring smoothness assumptions on the potential such as
differentiability, our results substantially broaden the theoretical
foundations of SGMs, bridging the gap between empirical success and rigorous
guarantees in non-smooth, complex data regimes.

</details>


### [289] [A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)](https://arxiv.org/pdf/2505.03490)
*Faiz Taleb, Ivan Gazeau, Maryline Laurent*

Main category: cs.LG

TL;DR: The paper introduces the LBRM algorithm to detect memorized training data in time series imputation models, improving membership inference attack accuracy by up to 60%.


<details>
  <summary>Details</summary>
Motivation: To address privacy risks from unintentional memorization of training data in generative models, specifically in time series imputation.

Method: Proposes the Loss-Based with Reference Model (LBRM) algorithm, using a reference model to distinguish training from test data.

Result: AUROC improved by ~40% without fine-tuning and ~60% with fine-tuning. Validated on two architectures for time series imputation.

Conclusion: LBRM significantly enhances detection accuracy, mitigating privacy risks in time series imputation models.

Abstract: Generative models can unintentionally memorize training data, posing
significant privacy risks. This paper addresses the memorization phenomenon in
time series imputation models, introducing the Loss-Based with Reference Model
(LBRM) algorithm. The LBRM method leverages a reference model to enhance the
accuracy of membership inference attacks, distinguishing between training and
test data. Our contributions are twofold: first, we propose an innovative
method to effectively extract and identify memorized training data,
significantly improving detection accuracy. On average, without fine-tuning,
the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased
by approximately 60\%. Second, we validate our approach through membership
inference attacks on two types of architectures designed for time series
imputation, demonstrating the robustness and versatility of the LBRM approach
in different contexts. These results highlight the significant enhancement in
detection accuracy provided by the LBRM approach, addressing privacy risks in
time series imputation models.

</details>


### [290] [AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning](https://arxiv.org/pdf/2505.03509)
*Pablo Gómez, David O'Ryan*

Main category: cs.LG

TL;DR: AnomalyMatch is a semi-supervised anomaly detection framework combining FixMatch and active learning, achieving high performance with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Supervised anomaly detection requires extensive labeling, which is impractical. AnomalyMatch addresses this by leveraging limited labeled and abundant unlabeled data.

Method: Uses FixMatch with EfficientNet classifiers and active learning for iterative refinement via expert verification.

Result: Achieves AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with high precision for top-ranked anomalies.

Conclusion: AnomalyMatch is scalable and effective for large datasets, especially in domains with label scarcity.

Abstract: Anomaly detection in large datasets is essential in fields such as astronomy
and computer vision; however, supervised methods typically require extensive
anomaly labelling, which is often impractical. We present AnomalyMatch, an
anomaly detection framework combining the semi-supervised FixMatch algorithm
using EfficientNet classifiers with active learning. By treating anomaly
detection as a semi-supervised binary classification problem, we efficiently
utilise limited labelled and abundant unlabelled images. We allow iterative
model refinement in a user interface for expert verification of high-confidence
anomalies and correction of false positives. Built for astronomical data,
AnomalyMatch generalises readily to other domains facing similar data
challenges. Evaluations on the GalaxyMNIST astronomical dataset and the
miniImageNet natural-image benchmark under severe class imbalance (1% anomalies
for miniImageNet) display strong performance: starting from five to ten
labelled anomalies and after three active learning cycles, we achieve an
average AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective
AUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with
71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images.
AnomalyMatch is tailored for large-scale applications, efficiently processing
predictions for 100 million images within three days on a single GPU.
Integrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted
discovery of scientifically valuable anomalies in vast astronomical datasets.
Our results underscore the exceptional utility and scalability of this approach
for anomaly discovery, highlighting the value of specialised approaches for
domains characterised by severe label scarcity.

</details>


### [291] [Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and Connection to Type-I Adversarial Attacks](https://arxiv.org/pdf/2505.03519)
*Sy-Tuyen Ho, Koh Jun Hao, Ngoc-Bao Nguyen, Alexander Binder, Ngai-Man Cheung*

Main category: cs.LG

TL;DR: The paper critiques the Model Inversion (MI) evaluation framework, revealing false positives and overestimated attack success rates. It introduces a human-annotated dataset, analyzes causes of errors, and suggests human evaluation as a primary framework.


<details>
  <summary>Details</summary>
Motivation: To address limitations in the current MI evaluation framework, which may overestimate attack success due to false positives, and to explore the impact of adversarial features on MI evaluation.

Method: Constructed a human-annotated dataset from 28 MI attack/defense setups, analyzed false positives, and conducted controlled experiments to study adversarial feature effects.

Result: Found significant false positives in MI evaluation, overestimating attack success. Identified adversarial feature impact and transferability, linking MI and adversarial attack research.

Conclusion: Advocates for human evaluation as the primary MI framework and calls for more robust automatic evaluation methods, highlighting current limitations.

Abstract: Model Inversion (MI) attacks aim to reconstruct information of private
training data by exploiting access to machine learning models. The most common
evaluation framework for MI attacks/defenses relies on an evaluation model that
has been utilized to assess progress across almost all MI attacks and defenses
proposed in recent years. In this paper, for the first time, we present an
in-depth study of MI evaluation. Firstly, we construct the first comprehensive
human-annotated dataset of MI attack samples, based on 28 setups of different
MI attacks, defenses, private and public datasets. Secondly, using our dataset,
we examine the accuracy of the MI evaluation framework and reveal that it
suffers from a significant number of false positives. These findings raise
questions about the previously reported success rates of SOTA MI attacks.
Thirdly, we analyze the causes of these false positives, design controlled
experiments, and discover the surprising effect of Type I adversarial features
on MI evaluation, as well as adversarial transferability, highlighting a
relationship between two previously distinct research areas. Our findings
suggest that the performance of SOTA MI attacks has been overestimated, with
the actual privacy leakage being significantly less than previously reported.
In conclusion, we highlight critical limitations in the widely used MI
evaluation framework and present our methods to mitigate false positive rates.
We remark that prior research has shown that Type I adversarial attacks are
very challenging, with no existing solution. Therefore, we urge to consider
human evaluation as a primary MI evaluation framework rather than merely a
supplement as in previous MI research. We also encourage further work on
developing more robust and reliable automatic evaluation frameworks.

</details>


### [292] [Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability](https://arxiv.org/pdf/2505.03530)
*Dip Roy*

Main category: cs.LG

TL;DR: A causal intervention framework for mechanistic interpretability of VAEs is introduced, isolating functional circuits and quantifying interpretability metrics.


<details>
  <summary>Details</summary>
Motivation: Understanding generative models like VAEs is challenging compared to discriminative models, necessitating tools for mechanistic interpretability.

Method: The paper develops techniques to analyze circuit motifs in VAEs using targeted interventions (input manipulations, latent space perturbations, activation patching, causal mediation) and applies them to synthetic and benchmark datasets.

Result: The framework successfully isolates functional circuits, maps computational to causal graphs, and introduces interpretability metrics. FactorVAE outperforms standard and Beta-VAE in disentanglement scores (0.084 vs. 0.064, 0.051) and effect strengths (4.59 vs. 3.99, 3.43).

Conclusion: The framework enhances mechanistic understanding of VAEs, offering tools for more transparent and controllable generative models.

Abstract: Mechanistic interpretability of deep learning models has emerged as a crucial
research direction for understanding the functioning of neural networks. While
significant progress has been made in interpreting discriminative models like
transformers, understanding generative models such as Variational Autoencoders
(VAEs) remains challenging. This paper introduces a comprehensive causal
intervention framework for mechanistic interpretability of VAEs. We develop
techniques to identify and analyze "circuit motifs" in VAEs, examining how
semantic factors are encoded, processed, and disentangled through the network
layers. Our approach uses targeted interventions at different levels: input
manipulations, latent space perturbations, activation patching, and causal
mediation analysis. We apply our framework to both synthetic datasets with
known causal relationships and standard disentanglement benchmarks. Results
show that our interventions can successfully isolate functional circuits, map
computational graphs to causal graphs of semantic factors, and distinguish
between polysemantic and monosemantic units. Furthermore, we introduce metrics
for causal effect strength, intervention specificity, and circuit modularity
that quantify the interpretability of VAE components. Experimental results
demonstrate clear differences between VAE variants, with FactorVAE achieving
higher disentanglement scores (0.084) and effect strengths (mean 4.59) compared
to standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework
advances the mechanistic understanding of generative models and provides tools
for more transparent and controllable VAE architectures.

</details>


### [293] [Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning](https://arxiv.org/pdf/2505.03533)
*Jiacheng Wang, Le Liang, Hao Ye, Chongtao Guo, Shi Jin*

Main category: cs.LG

TL;DR: The paper proposes a MARL-based resource allocation strategy for FL in wireless networks, addressing small-scale fading dynamics to improve training performance.


<details>
  <summary>Details</summary>
Motivation: Existing FL resource allocation strategies ignore rapid channel fluctuations, degrading performance. This work aims to address this gap.

Method: Uses a MARL framework (QMIX) to solve the resource allocation problem formulated as a Dec-POMDP, with clients dynamically allocating resources.

Result: The QMIX-based strategy outperforms baselines, especially under statistical heterogeneity, and ablation studies confirm the importance of small-scale fading dynamics.

Conclusion: The proposed method enhances FL performance by effectively addressing small-scale fading and scalability through decentralized MARL.

Abstract: Judicious resource allocation can effectively enhance federated learning (FL)
training performance in wireless networks by addressing both system and
statistical heterogeneity. However, existing strategies typically rely on block
fading assumptions, which overlooks rapid channel fluctuations within each
round of FL gradient uploading, leading to a degradation in FL training
performance. Therefore, this paper proposes a small-scale-fading-aware resource
allocation strategy using a multi-agent reinforcement learning (MARL)
framework. Specifically, we establish a one-step convergence bound of the FL
algorithm and formulate the resource allocation problem as a decentralized
partially observable Markov decision process (Dec-POMDP), which is subsequently
solved using the QMIX algorithm. In our framework, each client serves as an
agent that dynamically determines spectrum and power allocations within each
coherence time slot, based on local observations and a reward derived from the
convergence analysis. The MARL setting reduces the dimensionality of the action
space and facilitates decentralized decision-making, enhancing the scalability
and practicality of the solution. Experimental results demonstrate that our
QMIX-based resource allocation strategy significantly outperforms baseline
methods across various degrees of statistical heterogeneity. Additionally,
ablation studies validate the critical importance of incorporating small-scale
fading dynamics, highlighting its role in optimizing FL performance.

</details>


### [294] [Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming](https://arxiv.org/pdf/2505.03552)
*Linus Langenkamp, Philip Hannebohm, Bernhard Bachmann*

Main category: cs.LG

TL;DR: A novel method trains Physics-enhanced Neural ODEs (PeNODEs) by framing training as a dynamic optimization problem, using high-order implicit Runge-Kutta discretization and NLP solvers for efficient, stable, and accurate results.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of ODE solver-based training (stability, runtime, accuracy) and extend Neural ODEs to physics-enhanced models with constraints.

Method: Discretize the model using high-order implicit Runge-Kutta with flipped Legendre-Gauss-Radau points, forming a large-scale NLP solved by solvers like Ipopt. Includes parallelized, open-source implementation.

Result: Benchmarks show superior accuracy, speed, and generalization with smaller networks compared to other methods.

Conclusion: The approach advances PeNODEs training and plans integration into OpenModelica for broader accessibility.

Abstract: We propose a novel approach for training Physics-enhanced Neural ODEs
(PeNODEs) by expressing the training process as a dynamic optimization problem.
The full model, including neural components, is discretized using a high-order
implicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting
in a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art
NLP solvers such as Ipopt. This formulation enables simultaneous optimization
of network parameters and state trajectories, addressing key limitations of ODE
solver-based training in terms of stability, runtime, and accuracy. Extending
on a recent direct collocation-based method for Neural ODEs, we generalize to
PeNODEs, incorporate physical constraints, and present a custom, parallelized,
open-source implementation. Benchmarks on a Quarter Vehicle Model and a
Van-der-Pol oscillator demonstrate superior accuracy, speed, and generalization
with smaller networks compared to other training techniques. We also outline a
planned integration into OpenModelica to enable accessible training of Neural
DAEs.

</details>


### [295] [Rapid AI-based generation of coverage paths for dispensing applications](https://arxiv.org/pdf/2505.03560)
*Simon Baeuerle, Ian F. Mendonca, Kristof Van Laerhoven, Ralf Mikut, Andreas Steimer*

Main category: cs.LG

TL;DR: An AI-based approach using an ANN to generate dispense paths for TIM, replacing manual or optimization methods, without needing labels and avoiding air entrapments.


<details>
  <summary>Details</summary>
Motivation: Manual or optimization-based methods for TIM coverage path planning are time-consuming or computationally expensive.

Method: An ANN takes the target cooling area as input and directly outputs the dispense path, eliminating the need for labels.

Result: Feasible dispense paths are generated without air entrapments, ready for automated manufacturing.

Conclusion: The ANN-based approach is efficient and could be adapted for other manufacturing processes.

Abstract: Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial
role in the design of power electronics and electronic control units. Up to
now, this is done manually by experts or by using optimization approaches with
a high computational effort. We propose a novel AI-based approach to generate
dispense paths for TIM and similar dispensing applications. It is a drop-in
replacement for optimization-based approaches. An Artificial Neural Network
(ANN) receives the target cooling area as input and directly outputs the
dispense path. Our proposed setup does not require labels and we show its
feasibility on multiple target areas. The resulting dispense paths can be
directly transferred to automated manufacturing equipment and do not exhibit
air entrapments. The approach of using an ANN to predict process parameters for
a desired target state in real-time could potentially be transferred to other
manufacturing processes.

</details>


### [296] [Ergodic Generative Flows](https://arxiv.org/pdf/2505.03561)
*Leo Maxime Brunswic, Mateo Clemente, Rui Heng Yang, Adam Sigal, Amir Rasouli, Yinchuan Li*

Main category: cs.LG

TL;DR: The paper introduces Ergodic Generative Flows (EGFs) to address challenges in training GFNs, focusing on tractability and imitation learning without a separate reward model.


<details>
  <summary>Details</summary>
Motivation: To overcome issues like intractable flow-matching loss and the need for reward models in imitation learning with GFNs.

Method: Proposes EGFs using ergodicity for tractable FM loss and introduces KL-weakFM loss for imitation learning.

Result: Evaluated on toy 2D tasks and NASA datasets, showing effectiveness in IL and reinforcement learning.

Conclusion: EGFs provide a flexible and tractable solution for GFNs in continuous settings and imitation learning.

Abstract: Generative Flow Networks (GFNs) were initially introduced on directed acyclic
graphs to sample from an unnormalized distribution density. Recent works have
extended the theoretical framework for generative methods allowing more
flexibility and enhancing application range. However, many challenges remain in
training GFNs in continuous settings and for imitation learning (IL), including
intractability of flow-matching loss, limited tests of non-acyclic training,
and the need for a separate reward model in imitation learning. The present
work proposes a family of generative flows called Ergodic Generative Flows
(EGFs) which are used to address the aforementioned issues. First, we leverage
ergodicity to build simple generative flows with finitely many globally defined
transformations (diffeomorphisms) with universality guarantees and tractable
flow-matching loss (FM loss). Second, we introduce a new loss involving
cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It
is designed for IL training without a separate reward model. We evaluate
IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using
the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning
experiments with a target reward, using the FM loss.

</details>


### [297] [Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs](https://arxiv.org/pdf/2505.03595)
*Sidharth S. Menon, Ameya D. Jagtap*

Main category: cs.LG

TL;DR: Anant-Net is a neural surrogate for solving high-dimensional PDEs efficiently, overcoming the curse of dimensionality with high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: High-dimensional PDEs are computationally intractable due to exponential complexity, especially on hypercubic domains. Traditional methods fail, necessitating a scalable solution.

Method: Anant-Net integrates high-dimensional boundary conditions and minimizes PDE residuals at collocation points, using Kolmogorov-Arnold networks for interpretability.

Result: Anant-Net solves 300-dimensional PDEs on a single GPU in hours, demonstrating accuracy and robustness on benchmarks like Poisson and Allen-Cahn equations.

Conclusion: Anant-Net is an accurate, interpretable, and scalable framework for high-dimensional PDEs, outperforming state-of-the-art methods.

Abstract: High-dimensional partial differential equations (PDEs) arise in diverse
scientific and engineering applications but remain computationally intractable
due to the curse of dimensionality. Traditional numerical methods struggle with
the exponential growth in computational complexity, particularly on hypercubic
domains, where the number of required collocation points increases rapidly with
dimensionality. Here, we introduce Anant-Net, an efficient neural surrogate
that overcomes this challenge, enabling the solution of PDEs in high
dimensions. Unlike hyperspheres, where the internal volume diminishes as
dimensionality increases, hypercubes retain or expand their volume (for unit or
larger length), making high-dimensional computations significantly more
demanding. Anant-Net efficiently incorporates high-dimensional boundary
conditions and minimizes the PDE residual at high-dimensional collocation
points. To enhance interpretability, we integrate Kolmogorov-Arnold networks
into the Anant-Net architecture. We benchmark Anant-Net's performance on
several linear and nonlinear high-dimensional equations, including the Poisson,
Sine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and
robustness across randomly sampled test points from high-dimensional space.
Importantly, Anant-Net achieves these results with remarkable efficiency,
solving 300-dimensional problems on a single GPU within a few hours. We also
compare Anant-Net's results for accuracy and runtime with other
state-of-the-art methods. Our findings establish Anant-Net as an accurate,
interpretable, and scalable framework for efficiently solving high-dimensional
PDEs.

</details>


### [298] [Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift](https://arxiv.org/pdf/2505.03617)
*Thien Nhan Vo, Thanh Xuan Truong*

Main category: cs.LG

TL;DR: Importance weighting in deep neural networks shows limited practical utility for real-world distribution shifts, fading with prolonged training and offering no significant gains under covariate shift.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of importance weighting in addressing label shift and covariate shift in deep learning models.

Method: Experiments on synthetic 2D data (logistic regression, MLPs) and CIFAR-10 with class imbalances, testing L2 regularization and dropout.

Result: Weighting impacts early training but fades over time; L2 regularization helps, but no significant gains under covariate shift.

Conclusion: Importance weighting may not be practically useful for real-world distribution shifts.

Abstract: We evaluate the effectiveness of importance weighting in deep neural networks
under label shift and covariate shift. On synthetic 2D data (linearly separable
and moon-shaped) using logistic regression and MLPs, we observe that weighting
strongly affects decision boundaries early in training but fades with prolonged
optimization. On CIFAR-10 with various class imbalances, only L2 regularization
(not dropout) helps preserve weighting effects. In a covariate-shift
experiment, importance weighting yields no significant performance gain,
highlighting challenges on complex data. Our results call into question the
practical utility of importance weighting for real-world distribution shifts.

</details>


### [299] [ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders](https://arxiv.org/pdf/2505.03646)
*Chethan Krishnamurthy Ramanaik, Arjun Roy, Eirini Ntoutsi*

Main category: cs.LG

TL;DR: The paper addresses the underexplored adversarial robustness of deep autoencoders (AEs), proposing a novel adversarial optimization method and a defense plugin.


<details>
  <summary>Details</summary>
Motivation: Deep AEs are widely used in critical applications, but their adversarial robustness is less studied compared to classification models. Existing methods fail to fully exploit vulnerabilities in AEs.

Method: The authors introduce a layer-conditioning-based adversarial optimization objective to enhance gradient propagation and attack effectiveness. They also propose an inference-time defense plugin.

Result: Experiments show the proposed method outperforms existing attacks and defenses in both universal and sample-specific scenarios.

Conclusion: The work advances AE robustness by improving attack methods and introducing a practical defense.

Abstract: Despite the extensive use of deep autoencoders (AEs) in critical
applications, their adversarial robustness remains relatively underexplored
compared to classification models. AE robustness is characterized by the
Lipschitz bounds of its components. Existing robustness evaluation frameworks
based on white-box attacks do not fully exploit the vulnerabilities of
intermediate ill-conditioned layers in AEs. In the context of optimizing
imperceptible norm-bounded additive perturbations to maximize output damage,
existing methods struggle to effectively propagate adversarial loss gradients
throughout the network, often converging to less effective perturbations. To
address this, we propose a novel layer-conditioning-based adversarial
optimization objective that effectively guides the adversarial map toward
regions of local Lipschitz bounds by enhancing loss gradient information
propagation during attack optimization. We demonstrate through extensive
experiments on state-of-the-art AEs that our adversarial objective results in
stronger attacks, outperforming existing methods in both universal and
sample-specific scenarios. As a defense method against this attack, we
introduce an inference-time adversarially trained defense plugin that mitigates
the effects of adversarial examples.

</details>


### [300] [Mitigating mode collapse in normalizing flows by annealing with an adaptive schedule: Application to parameter estimation](https://arxiv.org/pdf/2505.03652)
*Yihang Wang, Chris Chi, Aaron R. Dinner*

Main category: cs.LG

TL;DR: Annealing with adaptive ESS-based scheduling mitigates mode collapse in NFs, improving efficiency and reducing computation time compared to MCMC.


<details>
  <summary>Details</summary>
Motivation: NFs are limited by mode collapse in multimodal distributions, hindering their practical utility for parameter estimation.

Method: Proposes annealing with an adaptive ESS-based schedule to prevent mode collapse and demonstrates its efficiency on a biochemical oscillator model.

Result: Achieves convergence in ten-fold less computation time than MCMC and reduces variance by pruning samples using ESS.

Conclusion: The approach enhances NF sampling efficiency and suggests further improvements for broader applications.

Abstract: Normalizing flows (NFs) provide uncorrelated samples from complex
distributions, making them an appealing tool for parameter estimation. However,
the practical utility of NFs remains limited by their tendency to collapse to a
single mode of a multimodal distribution. In this study, we show that annealing
with an adaptive schedule based on the effective sample size (ESS) can mitigate
mode collapse. We demonstrate that our approach can converge the marginal
likelihood for a biochemical oscillator model fit to time-series data in
ten-fold less computation time than a widely used ensemble Markov chain Monte
Carlo (MCMC) method. We show that the ESS can also be used to reduce variance
by pruning the samples. We expect these developments to be of general use for
sampling with NFs and discuss potential opportunities for further improvements.

</details>


### [301] [Neural Integral Operators for Inverse problems in Spectroscopy](https://arxiv.org/pdf/2505.03677)
*Emanuele Zappala, Alice Giola, Andreas Kramer, Enrico Greco*

Main category: cs.LG

TL;DR: A deep learning method for molecular spectra classification is introduced, addressing overfitting in small datasets by leveraging integral operators and outperforming traditional and other deep learning models.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with small datasets in spectroscopy, leading to overfitting, while traditional methods lack accuracy and applicability.

Method: The method uses integral operators via integral equations of the first kind, reducing overfitting in small datasets.

Result: The model outperforms traditional methods (e.g., decision trees, SVMs) and other deep learning models on small datasets.

Conclusion: The approach combines deep learning's power with robustness for small datasets, solving a key challenge in spectroscopy.

Abstract: Deep learning has shown high performance on spectroscopic inverse problems
when sufficient data is available. However, it is often the case that data in
spectroscopy is scarce, and this usually causes severe overfitting problems
with deep learning methods. Traditional machine learning methods are viable
when datasets are smaller, but the accuracy and applicability of these methods
is generally more limited.
  We introduce a deep learning method for classification of molecular spectra
based on learning integral operators via integral equations of the first kind,
which results in an algorithm that is less affected by overfitting issues on
small datasets, compared to other deep learning models.
  The problem formulation of the deep learning approach is based on inverse
problems, which have traditionally found important applications in
spectroscopy. We perform experiments on real world data to showcase our
algorithm. It is seen that the model outperforms traditional machine learning
approaches such as decision tree and support vector machine, and for small
datasets it outperforms other deep learning models. Therefore, our methodology
leverages the power of deep learning, still maintaining the performance when
the available data is very limited, which is one of the main issues that deep
learning faces in spectroscopy, where datasets are often times of small size.

</details>


### [302] [Learning Survival Distributions with the Asymmetric Laplace Distribution](https://arxiv.org/pdf/2505.03712)
*Deming Sheng, Ricardo Henao*

Main category: cs.LG

TL;DR: A parametric survival analysis method using the Asymmetric Laplace Distribution (ALD) is proposed, outperforming existing parametric and nonparametric models in accuracy, discrimination, and calibration.


<details>
  <summary>Details</summary>
Motivation: To address limitations of nonparametric survival analysis models by leveraging the ALD for closed-form calculation of event summaries like mean, median, and quantiles.

Method: The model uses ALD to estimate survival distributions parametrically, optimizing parameters (location, scale, asymmetry) via maximum likelihood.

Result: Outperforms parametric and nonparametric methods in accuracy, discrimination, and calibration on synthetic and real-world data.

Conclusion: The ALD-based parametric method offers superior performance and flexibility for survival analysis.

Abstract: Probabilistic survival analysis models seek to estimate the distribution of
the future occurrence (time) of an event given a set of covariates. In recent
years, these models have preferred nonparametric specifications that avoid
directly estimating survival distributions via discretization. Specifically,
they estimate the probability of an individual event at fixed times or the time
of an event at fixed probabilities (quantiles), using supervised learning.
Borrowing ideas from the quantile regression literature, we propose a
parametric survival analysis method based on the Asymmetric Laplace
Distribution (ALD). This distribution allows for closed-form calculation of
popular event summaries such as mean, median, mode, variation, and quantiles.
The model is optimized by maximum likelihood to learn, at the individual level,
the parameters (location, scale, and asymmetry) of the ALD distribution.
Extensive results on synthetic and real-world data demonstrate that the
proposed method outperforms parametric and nonparametric approaches in terms of
accuracy, discrimination and calibration.

</details>


### [303] [OAC: Output-adaptive Calibration for Accurate Post-training Quantization](https://arxiv.org/pdf/2405.15025)
*Ali Edalati, Alireza Ghaffari, Mahsa Ghazvini Nejad, Lu Hou, Boxing Chen, Masoud Asgharian, Vahid Partovi Nia*

Main category: cs.LG

TL;DR: The paper proposes Output-adaptive Calibration (OAC) to improve low-precision quantization of LLMs by incorporating model output in the calibration process, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have high computational costs, and existing Post-training Quantization (PTQ) methods often lead to accuracy drops in low-precision settings.

Method: OAC formulates quantization error based on output cross-entropy loss, approximates output-adaptive Hessians for layers, and updates weights to maintain model output.

Result: OAC outperforms state-of-the-art baselines like SpQR and BiLLM, especially in extreme low-precision (2-bit and binary) quantization.

Conclusion: Incorporating model output in calibration via OAC effectively reduces quantization errors and maintains accuracy in low-precision LLM deployment.

Abstract: Deployment of Large Language Models (LLMs) has major computational costs, due
to their rapidly expanding size. Compression of LLMs reduces the memory
footprint, latency, and energy required for their inference. Post-training
Quantization (PTQ) techniques have been developed to compress LLMs while
avoiding expensive re-training. Most PTQ approaches formulate the quantization
error based on a layer-wise Euclidean loss, ignoring the model output. Then,
each layer is calibrated using its layer-wise Hessian to update the weights
towards minimizing the quantization error. The Hessian is also used for
detecting the most salient weights to quantization. Such PTQ approaches are
prone to accuracy drop in low-precision quantization. We propose
Output-adaptive Calibration (OAC) to incorporate the model output in the
calibration process. We formulate the quantization error based on the
distortion of the output cross-entropy loss. OAC approximates the
output-adaptive Hessian for each layer under reasonable assumptions to reduce
the computational complexity. The output-adaptive Hessians are used to update
the weight matrices and detect the salient weights towards maintaining the
model output. Our proposed method outperforms the state-of-the-art baselines
such as SpQR and BiLLM, especially, at extreme low-precision (2-bit and binary)
quantization.

</details>


### [304] [Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate](https://arxiv.org/pdf/2410.22086)
*Zhiqi Bu, Xiaomeng Jin, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Mingyi Hong*

Main category: cs.LG

TL;DR: The paper introduces NGDiff, a method for machine unlearning in LLMs, framed as a multi-task optimization problem with a focus on balancing forgetting and model performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of removing unwanted knowledge from LLMs while maintaining performance, the paper explores machine unlearning from an optimization perspective.

Method: Proposes NGDiff, a normalized gradient difference algorithm with an automatic learning rate scheduler, to balance forgetting and performance objectives.

Result: NGDiff outperforms state-of-the-art methods on TOFU and MUSE datasets, showing stable training.

Conclusion: NGDiff effectively balances unlearning and model performance, offering a robust solution for machine unlearning in LLMs.

Abstract: Machine unlearning has been used to remove unwanted knowledge acquired by
large language models (LLMs). In this paper, we examine machine unlearning from
an optimization perspective, framing it as a regularized multi-task
optimization problem, where one task optimizes a forgetting objective and
another optimizes the model performance. In particular, we introduce a
normalized gradient difference (NGDiff) algorithm, enabling us to have better
control over the trade-off between the objectives, while integrating a new,
automatic learning rate scheduler. We provide a theoretical analysis and
empirically demonstrate the superior performance of NGDiff among
state-of-the-art unlearning methods on the TOFU and MUSE datasets while
exhibiting stable training.

</details>


### [305] [MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications](https://arxiv.org/pdf/2411.18915)
*Vishnou Vinayagame, Gregory Senay, Luis Martí*

Main category: cs.LG

TL;DR: MATATA is a weakly supervised method to train multi-step reasoning language agents for document tabular applications, achieving state-of-the-art performance without intermediate supervision.


<details>
  <summary>Details</summary>
Motivation: Business documents require mathematical reasoning for understanding, but current methods rely on closed-source or larger models, external data, or extensive prompt-engineering.

Method: MATATA uses a two-stage training approach with weak supervision from the final reasoning outcome, avoiding intermediate supervision. It employs an adaptive planner and shared tools.

Result: MATATA achieves state-of-the-art on FinQA and TAT-QA, closely matching GPT-4-based frameworks on TabMWP.

Conclusion: MATATA enables cost-effective, powerful agentic systems by training end-to-end multi-step reasoning agents without intermediate supervision.

Abstract: Business documents often contain substantial tabular and textual information
with numerical values, requiring mathematical reasoning for effective document
understanding. While Small Language Models (SLMs) still struggle at this task,
tool-augmented multi-step agents perform better, at the cost of relying on
closed-source or larger models, external data, or extensive prompt-engineering.
This work introduces MATATA, a novel weakly supervised end-to-end approach to
train multi-step reasoning language agents for document tabular applications.
MATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B
SLMs. During its two-stage training, MATATA uses the final outcome of the
multi-step reasoning chain as weak supervision. This approach avoids having to
individually supervise each intermediate agent in the reasoning chain. By
employing an adaptive planner and shared tools across different datasets,
MATATA shows robust performance. Experiments demonstrate that MATATA achieves
state-of-the-art on FinQA, and on TAT-QA among reasoning methods based on
open-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based
frameworks on TabMWP. This novel weakly supervised approach enables training an
end-to-end multi-step reasoning agent without intermediate supervision,
supporting future developments of cost-effective powerful agentic systems.

</details>


### [306] [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/pdf/2505.00926)
*Ruiquan Huang, Yingbin Liang, Jing Yang*

Main category: cs.LG

TL;DR: The paper explores how a one-layer transformer learns regular language tasks ('even pairs' and 'parity check') via gradient descent, revealing two training phases and validating results experimentally.


<details>
  <summary>Details</summary>
Motivation: To understand how transformers learn regular language tasks and explain their mechanisms, focusing on 'even pairs' and 'parity check'.

Method: Theoretical analysis of training dynamics in a one-layer transformer (attention + linear layer) under gradient descent, with experiments for validation.

Result: Two training phases: rapid attention layer growth for separability, followed by logarithmic linear layer growth toward max-margin separation. Loss decreases as O(1/t).

Conclusion: One-layer transformers can solve 'even pairs' directly, while 'parity check' requires Chain-of-Thought integration. Training dynamics align with theoretical predictions.

Abstract: Language recognition tasks are fundamental in natural language processing
(NLP) and have been widely used to benchmark the performance of large language
models (LLMs). These tasks also play a crucial role in explaining the working
mechanisms of transformers. In this work, we focus on two representative tasks
in the category of regular language recognition, known as `even pairs' and
`parity check', the aim of which is to determine whether the occurrences of
certain subsequences in a given sequence are even. Our goal is to explore how a
one-layer transformer, consisting of an attention layer followed by a linear
layer, learns to solve these tasks by theoretically analyzing its training
dynamics under gradient descent. While even pairs can be solved directly by a
one-layer transformer, parity check need to be solved by integrating
Chain-of-Thought (CoT), either into the inference stage of a transformer
well-trained for the even pairs task, or into the training of a one-layer
transformer. For both problems, our analysis shows that the joint training of
attention and linear layers exhibits two distinct phases. In the first phase,
the attention layer grows rapidly, mapping data sequences into separable
vectors. In the second phase, the attention layer becomes stable, while the
linear layer grows logarithmically and approaches in direction to a max-margin
hyperplane that correctly separates the attention layer outputs into positive
and negative samples, and the loss decreases at a rate of $O(1/t)$. Our
experiments validate those theoretical results.

</details>


### [307] [q-Learning in Continuous Time](https://arxiv.org/pdf/2207.00713)
*Yanwei Jia, Xun Yu Zhou*

Main category: cs.LG

TL;DR: The paper explores a continuous-time version of Q-learning in RL, introducing a 'q-function' as a first-order approximation of the Q-function. It develops a theory for q-learning, applies it to actor-critic algorithms, and compares performance with existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the collapse of the conventional Q-function in continuous time and develop a theory for RL that avoids time discretization.

Method: Introduces the q-function, characterizes it and the value function via martingale conditions, and derives actor-critic algorithms based on this theory.

Result: The theory is applied to devise algorithms, some of which interpret SARSA or recover existing PG-based methods. Simulations compare performance with other approaches.

Conclusion: The proposed q-learning framework is effective, with algorithms performing comparably or better than existing methods in continuous-time RL.

Abstract: We study the continuous-time counterpart of Q-learning for reinforcement
learning (RL) under the entropy-regularized, exploratory diffusion process
formulation introduced by Wang et al. (2020). As the conventional (big)
Q-function collapses in continuous time, we consider its first-order
approximation and coin the term ``(little) q-function". This function is
related to the instantaneous advantage rate function as well as the
Hamiltonian. We develop a ``q-learning" theory around the q-function that is
independent of time discretization. Given a stochastic policy, we jointly
characterize the associated q-function and value function by martingale
conditions of certain stochastic processes, in both on-policy and off-policy
settings. We then apply the theory to devise different actor-critic algorithms
for solving underlying RL problems, depending on whether or not the density
function of the Gibbs measure generated from the q-function can be computed
explicitly. One of our algorithms interprets the well-known Q-learning
algorithm SARSA, and another recovers a policy gradient (PG) based
continuous-time algorithm proposed in Jia and Zhou (2022b). Finally, we conduct
simulation experiments to compare the performance of our algorithms with those
of PG-based algorithms in Jia and Zhou (2022b) and time-discretized
conventional Q-learning algorithms.

</details>


### [308] [PLUM: Improving Inference Efficiency By Leveraging Repetition-Sparsity Trade-Off](https://arxiv.org/pdf/2312.01581)
*Sachit Kuhar, Yash Jain, Alexey Tumanov*

Main category: cs.LG

TL;DR: PLUM is a co-design framework for DNN inference that leverages the repetition-sparsity trade-off, improving efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Efficient DNN inference on edge devices is crucial, and quantization/sparsity are key techniques. The paper explores the repetition-sparsity trade-off to enhance computational efficiency.

Method: PLUM integrates DNN inference systems with quantization (forward/backward pass) to exploit the repetition-sparsity trade-off. It uses signed binarization for better accuracy.

Result: PLUM achieves 26% speedup, doubles energy efficiency, reduces density by 2.8x, and retains top-1 accuracy (66.2% on ImageNet) compared to binary methods.

Conclusion: PLUM offers an efficient solution for deploying DNNs in resource-limited environments, balancing accuracy and performance.

Abstract: Efficient inference of Deep Neural Networks (DNNs) on resource-constrained
edge devices is essential. Quantization and sparsity are key techniques that
translate to repetition and sparsity within tensors at the hardware-software
interface. This paper introduces the concept of repetition-sparsity trade-off
that helps explain computational efficiency during inference. We propose PLUM,
a unified co-design framework that integrates DNN inference systems and
quantization (forward and backward pass) to leverage the repetition-sparsity
trade-off to improve inference efficiency. Our results demonstrate that PLUM's
quantization method is more accurate than binary quantization with the same
number of non-zero weights. Detailed analysis indicates that signed
binarization generates a smaller distribution of effectual (non-zero)
parameters nested within a larger distribution of total parameters of latent
full-precision weights for a DNN block. Finally, the proposed PLUM framework
achieves a 26% speedup on real hardware, doubles energy efficiency, and reduces
density by 2.8x compared to binary methods while retaining top-1 accuracy when
compared to prior-art methods for ResNets on ImageNet (by achieving 66.2% top-1
accuracy), presenting an alternative solution for deploying efficient models in
resource-limited environments.

</details>


### [309] [Effective backdoor attack on graph neural networks in link prediction tasks](https://arxiv.org/pdf/2401.02663)
*Jiazhu Dai, Haoyu Sun*

Main category: cs.LG

TL;DR: The paper introduces a backdoor attack method for GNNs in link prediction tasks, demonstrating their vulnerability to such attacks.


<details>
  <summary>Details</summary>
Motivation: Existing research on backdoor attacks in GNNs focuses on graph and node classification, leaving link prediction tasks unexplored. This paper aims to address this gap.

Method: The method uses a single node as a trigger to poison selected node pairs during training, embedding a backdoor in the GNN model. During inference, linking the trigger node to unlinked node pairs activates the backdoor, causing incorrect predictions.

Result: The study reveals that GNN models for link prediction are vulnerable to backdoor attacks, leading to incorrect link predictions when triggers are present.

Conclusion: The paper highlights a security risk in GNNs for link prediction and calls for further research into defending against such attacks.

Abstract: Graph Neural Networks (GNNs) are a class of deep learning models capable of
processing graph-structured data, and they have demonstrated significant
performance in a variety of real-world applications. Recent studies have found
that GNN models are vulnerable to backdoor attacks. When specific patterns
(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input
data, the backdoor embedded in the GNN models is activated, which misclassifies
the input data into the target class label specified by the attacker, whereas
when there are no backdoor triggers in the input, the backdoor embedded in the
GNN models is not activated, and the models work normally. Backdoor attacks are
highly stealthy and expose GNN models to serious security risks. Currently,
research on backdoor attacks against GNNs mainly focus on tasks such as graph
classification and node classification, and backdoor attacks against link
prediction tasks are rarely studied. In this paper, we propose a backdoor
attack against the link prediction tasks based on GNNs and reveal the existence
of such security vulnerability in GNN models, which make the backdoored GNN
models to incorrectly predict unlinked two nodes as having a link relationship
when a trigger appear. The method uses a single node as the trigger and poison
selected node pairs in the training graph, and then the backdoor will be
embedded in the GNN models through the training process. In the inference
stage, the backdoor in the GNN models can be activated by simply linking the
trigger node to the two end nodes of the unlinked node pairs in the input data,
causing the GNN models to produce incorrect link prediction results for the
target node pairs.

</details>


### [310] [FreDF: Learning to Forecast in the Frequency Domain](https://arxiv.org/pdf/2402.02399)
*Hao Wang, Licheng Pan, Zhichao Chen, Degui Yang, Sen Zhang, Yifei Yang, Xinggao Liu, Haoxuan Li, Dacheng Tao*

Main category: cs.LG

TL;DR: FreDF addresses label autocorrelation in time series forecasting by learning in the frequency domain, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Current forecasting models ignore label autocorrelation, leading to biased predictions.

Method: Proposes Frequency-enhanced Direct Forecast (FreDF) to mitigate bias by forecasting in the frequency domain.

Result: FreDF significantly outperforms state-of-the-art methods and works with various models.

Conclusion: FreDF effectively reduces estimation bias and improves forecasting accuracy.

Abstract: Time series modeling presents unique challenges due to autocorrelation in
both historical data and future sequences. While current research predominantly
addresses autocorrelation within historical data, the correlations among future
labels are often overlooked. Specifically, modern forecasting models primarily
adhere to the Direct Forecast (DF) paradigm, generating multi-step forecasts
independently and disregarding label autocorrelation over time. In this work,
we demonstrate that the learning objective of DF is biased in the presence of
label autocorrelation. To address this issue, we propose the Frequency-enhanced
Direct Forecast (FreDF), which mitigates label autocorrelation by learning to
forecast in the frequency domain, thereby reducing estimation bias. Our
experiments show that FreDF significantly outperforms existing state-of-the-art
methods and is compatible with a variety of forecast models. Code is available
at https://github.com/Master-PLC/FreDF.

</details>


### [311] [Closed-Form Diffusion Models](https://arxiv.org/pdf/2310.12395)
*Christopher Scarvelis, Haitz Sáez de Ocáriz Borde, Justin Solomon*

Main category: cs.LG

TL;DR: SGMs generate samples by transforming noise using the score function of a perturbed target. Explicit smoothing of the closed-form score avoids training and enables novel sample generation.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and theoretical gaps of neural SGMs by proposing a training-free, efficient alternative.

Method: Explicitly smooth the closed-form score and use a nearest-neighbor-based estimator for efficient sampling.

Result: Achieves competitive sampling times on consumer-grade CPUs without training.

Conclusion: The proposed method offers a practical, efficient alternative to neural SGMs for generating novel samples.

Abstract: Score-based generative models (SGMs) sample from a target distribution by
iteratively transforming noise using the score function of the perturbed
target. For any finite training set, this score function can be evaluated in
closed form, but the resulting SGM memorizes its training data and does not
generate novel samples. In practice, one approximates the score by training a
neural network via score-matching. The error in this approximation promotes
generalization, but neural SGMs are costly to train and sample, and the
effective regularization this error provides is not well-understood
theoretically. In this work, we instead explicitly smooth the closed-form score
to obtain an SGM that generates novel samples without training. We analyze our
model and propose an efficient nearest-neighbor-based estimator of its score
function. Using this estimator, our method achieves competitive sampling times
while running on consumer-grade CPUs.

</details>


### [312] [Momentum-SAM: Sharpness Aware Minimization without Computational Overhead](https://arxiv.org/pdf/2401.12033)
*Marlon Becker, Frederick Altrock, Benjamin Risse*

Main category: cs.LG

TL;DR: MSAM improves SAM by using momentum to reduce computational costs while maintaining generalization benefits.


<details>
  <summary>Details</summary>
Motivation: SAM's computational overhead limits its feasibility; MSAM leverages momentum for efficiency.

Method: MSAM perturbs parameters using accumulated momentum, avoiding extra gradient calculations.

Result: MSAM achieves low sharpness with minimal overhead, comparable to SGD or Adam.

Conclusion: MSAM combines SAM's generalization benefits with NAG's efficiency, offering a practical alternative.

Abstract: The recently proposed optimization algorithm for deep neural networks
Sharpness Aware Minimization (SAM) suggests perturbing parameters before
gradient calculation by a gradient ascent step to guide the optimization into
parameter space regions of flat loss. While significant generalization
improvements and thus reduction of overfitting could be demonstrated, the
computational costs are doubled due to the additionally needed gradient
calculation, making SAM unfeasible in case of limited computationally
capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose
Momentum-SAM (MSAM), which perturbs parameters in the direction of the
accumulated momentum vector to achieve low sharpness without significant
computational overhead or memory demands over SGD or Adam. We evaluate MSAM in
detail and reveal insights on separable mechanisms of NAG, SAM and MSAM
regarding training optimization and generalization. Code is available at
https://github.com/MarlonBecker/MSAM.

</details>


### [313] [HINT: Hypernetwork Approach to Training Weight Interval Regions in Continual Learning](https://arxiv.org/pdf/2405.15444)
*Patryk Krukowski, Anna Bielawska, Kamil Książek, Paweł Wawrzyński, Paweł Batorski, Przemysław Spurek*

Main category: cs.LG

TL;DR: HINT introduces interval arithmetic in embedding space and a hypernetwork to manage intervals efficiently, improving training speed and preventing forgetting in Continual Learning.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of high-dimensional weight space in InterContiNet by simplifying interval management in Continual Learning.

Method: Uses interval embeddings and a hypernetwork to map intervals to target network weights, training embeddings and hypernetwork together.

Result: Achieves faster training, prevents forgetting, and outperforms InterContiNet with SOTA results on benchmarks.

Conclusion: HINT offers an efficient solution for Continual Learning, enabling a single universal embedding for all tasks post-training.

Abstract: Recently, a new Continual Learning (CL) paradigm was presented to control
catastrophic forgetting, called Interval Continual Learning (InterContiNet),
which relies on enforcing interval constraints on the neural network parameter
space. Unfortunately, InterContiNet training is challenging due to the high
dimensionality of the weight space, making intervals difficult to manage. To
address this issue, we introduce HINT, a technique that employs interval
arithmetic within the embedding space and utilizes a hypernetwork to map these
intervals to the target network parameter space. We train interval embeddings
for consecutive tasks and train a hypernetwork to transform these embeddings
into weights of the target network. An embedding for a given task is trained
along with the hypernetwork, preserving the response of the target network for
the previous task embeddings. Interval arithmetic works with a more manageable,
lower-dimensional embedding space rather than directly preparing intervals in a
high-dimensional weight space. Our model allows faster and more efficient
training. Furthermore, HINT maintains the guarantee of not forgetting. At the
end of training, we can choose one universal embedding to produce a single
network dedicated to all tasks. In such a framework, hypernetwork is used only
for training and, finally, we can utilize one set of weights. HINT obtains
significantly better results than InterContiNet and gives SOTA results on
several benchmarks.

</details>


### [314] [TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems](https://arxiv.org/pdf/2402.09780)
*Eugenio Ressa, Alberto Marchisio, Maurizio Martina, Guido Masera, Muhammad Shafique*

Main category: cs.LG

TL;DR: TinyCL is a hardware architecture designed for Continuous Learning (CL) on resource-constrained systems, achieving 58x speedup over GPUs while minimizing memory access and power consumption.


<details>
  <summary>Details</summary>
Motivation: Current DNN accelerators lack support for backpropagation and weight updates required for CL, leading to resource inefficiency in autonomous systems.

Method: TinyCL includes a processing unit for forward/backward propagation and a control unit for CL workload management, using snake-like sliding windows and reconfigurable Multiply-and-Accumulate units.

Result: TinyCL achieves a 58x speedup (1.76s vs. 103s) over GPUs for training on CIFAR10, consuming 86 mW in a 4.74 mm² die.

Conclusion: TinyCL is the first hardware accelerator enabling efficient CL on autonomous systems, demonstrating significant performance and energy efficiency gains.

Abstract: The Continuous Learning (CL) paradigm consists of continuously evolving the
parameters of the Deep Neural Network (DNN) model to progressively learn to
perform new tasks without reducing the performance on previous tasks, i.e.,
avoiding the so-called catastrophic forgetting. However, the DNN parameter
update in CL-based autonomous systems is extremely resource-hungry. The
existing DNN accelerators cannot be directly employed in CL because they only
support the execution of the forward propagation. Only a few prior
architectures execute the backpropagation and weight update, but they lack the
control and management for CL. Towards this, we design a hardware architecture,
TinyCL, to perform CL on resource-constrained autonomous systems. It consists
of a processing unit that executes both forward and backward propagation, and a
control unit that manages memory-based CL workload. To minimize the memory
accesses, the sliding window of the convolutional layer moves in a snake-like
fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at
runtime to execute different operations. As per our knowledge, our proposed
TinyCL represents the first hardware accelerator that executes CL on autonomous
systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS
technology node with the conventional ASIC design flow. It executes 1 epoch of
training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while
1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,
thus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.

</details>


### [315] [CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment](https://arxiv.org/pdf/2406.13216)
*Songyang Chen, Yu Liu, Lei Zou, Zexuan Wang, Youfang Lin*

Main category: cs.LG

TL;DR: The paper investigates model expressiveness in unsupervised graph alignment, proposing a hybrid approach (CombAlign) that combines optimal transport and embedding-based methods, achieving a 14.5% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: To explore the model's discriminative power and capability in ensuring node matching properties (e.g., one-to-one matching) in unsupervised graph alignment.

Method: Proposes CombAlign, a hybrid approach integrating cross-dimensional feature interaction for OT-based learning and an embedding-based method inspired by the Weisfeiler-Lehman test. Uses non-uniform marginals and a refinement step via maximum weight matching.

Result: Achieves a 14.5% improvement in alignment accuracy over state-of-the-art methods, validating the theoretical analysis.

Conclusion: CombAlign enhances expressiveness and accuracy in graph alignment, demonstrating the impact of theoretical expressivity on practical performance.

Abstract: Unsupervised graph alignment finds the node correspondence between a pair of
attributed graphs by only exploiting graph structure and node features. One
category of recent studies first computes the node representation and then
matches nodes with the largest embedding-based similarity, while the other
category reduces the problem to optimal transport (OT) via Gromov-Wasserstein
learning. However, it remains largely unexplored in the model expressiveness,
as well as how theoretical expressivity impacts prediction accuracy. We
investigate the model expressiveness from two aspects. First, we characterize
the model's discriminative power in distinguishing matched and unmatched node
pairs across two graphs. Second, we study the model's capability of
guaranteeing node matching properties such as one-to-one matching and mutual
alignment. Motivated by our theoretical analysis, we put forward a hybrid
approach named CombAlign with stronger expressive power. Specifically, we
enable cross-dimensional feature interaction for OT-based learning and propose
an embedding-based method inspired by the Weisfeiler-Lehman test. We also apply
non-uniform marginals obtained from the embedding-based modules to OT as priors
for more expressiveness. Based on that, we propose a traditional
algorithm-based refinement, which combines our OT and embedding-based
predictions using the ensemble learning strategy and reduces the problem to
maximum weight matching. With carefully designed edge weights, we ensure those
matching properties and further enhance prediction accuracy. By extensive
experiments, we demonstrate a significant improvement of 14.5% in alignment
accuracy compared to state-of-the-art approaches and confirm the soundness of
our theoretical analysis.

</details>


### [316] [Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems](https://arxiv.org/pdf/2402.11722)
*Da Long, Zhitong Xu, Qiwei Yuan, Yin Yang, Shandian Zhe*

Main category: cs.LG

TL;DR: The paper introduces an invertible Fourier Neural Operator (iFNO) to handle both forward and inverse problems, improving on the standard FNO by integrating invertible blocks and a variational auto-encoder for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing FNO methods are limited to forward prediction, while many applications require solving inverse problems. The paper aims to bridge this gap by developing a bi-directional operator.

Method: The authors propose iFNO, which uses invertible Fourier blocks in latent space for parameter sharing and mutual regularization. A variational auto-encoder is added to handle ill-posedness, data scarcity, and noise. Training involves a three-step process combining these components.

Result: Evaluations on seven benchmark tasks show iFNO's advantages in tackling both forward and inverse problems.

Conclusion: iFNO effectively addresses the limitations of FNO by enabling bi-directional task learning and handling challenges in inverse problems, demonstrating superior performance.

Abstract: Fourier Neural Operator (FNO) is a powerful and popular operator learning
method. However, FNO is mainly used in forward prediction, yet a great many
applications rely on solving inverse problems. In this paper, we propose an
invertible Fourier Neural Operator (iFNO) for jointly tackling the forward and
inverse problems. We developed a series of invertible Fourier blocks in the
latent channel space to share the model parameters, exchange the information,
and mutually regularize the learning for the bi-directional tasks. We
integrated a variational auto-encoder to capture the intrinsic structures
within the input space and to enable posterior inference so as to mitigate
challenges of illposedness, data shortage, noises that are common in inverse
problems. We proposed a three-step process to combine the invertible blocks and
the VAE component for effective training. The evaluations on seven benchmark
forward and inverse tasks have demonstrated the advantages of our approach.

</details>


### [317] [Imitation-regularized Optimal Transport on Networks: Provable Robustness and Application to Logistics Planning](https://arxiv.org/pdf/2402.17967)
*Koshi Oishi, Yota Hashizume, Tomohiko Jimbo, Hirotaka Kaji, Kenji Kashima*

Main category: cs.LG

TL;DR: The paper proposes imitation-regularized optimal transport (I-OT) to enhance robustness in network transport systems by incorporating prior knowledge, improving interpretability and practical applications.


<details>
  <summary>Details</summary>
Motivation: Transport systems on networks are vulnerable to unforeseen disruptions like disasters, necessitating robust solutions.

Method: The study introduces I-OT, which integrates prior knowledge into entropy-regularized OT for robustness.

Result: Mathematical verification confirms I-OT's robustness, and its effectiveness is validated via a logistics simulation.

Conclusion: I-OT enhances robustness and interpretability, with potential for real-world applications.

Abstract: Transport systems on networks are crucial in various applications, but face a
significant risk of being adversely affected by unforeseen circumstances such
as disasters. The application of entropy-regularized optimal transport (OT) on
graph structures has been investigated to enhance the robustness of transport
on such networks. In this study, we propose an imitation-regularized OT (I-OT)
that mathematically incorporates prior knowledge into the robustness of OT.
This method is expected to enhance interpretability by integrating human
insights into robustness and to accelerate practical applications. Furthermore,
we mathematically verify the robustness of I-OT and discuss how these
robustness properties relate to real-world applications. The effectiveness of
this method is validated through a logistics simulation using automotive parts
data.

</details>


### [318] [Bellman Unbiasedness: Tractable and Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation](https://arxiv.org/pdf/2407.21260)
*Taehyun Cho, Seungyub Han, Kyungjae Lee, Seokhun Ju, Dohyeong Kim, Jungwoo Lee*

Main category: cs.LG

TL;DR: The paper analyzes distributional reinforcement learning (RL), focusing on its theoretical foundations and proposing an efficient algorithm with a tight regret bound.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of theoretical understanding of distributional RL's effectiveness and the challenge of infinite-dimensional distributions.

Method: The method involves introducing Bellman unbiasedness for efficient distributional updates and analyzing moment functionals for capturing statistical information. A new algorithm, SF-LSVI, is proposed.

Result: The results show that only moment functionals can exactly capture statistical information, and the SF-LSVI algorithm achieves a tight regret bound of Õ(d_E H^(3/2)√K).

Conclusion: The paper concludes that moment functionals are essential for efficient distributional RL and that the proposed algorithm is provably effective with a tight regret bound.

Abstract: Distributional reinforcement learning improves performance by capturing
environmental stochasticity, but a comprehensive theoretical understanding of
its effectiveness remains elusive. In addition, the intractable element of the
infinite dimensionality of distributions has been overlooked. In this paper, we
present a regret analysis of distributional reinforcement learning with general
value function approximation in a finite episodic Markov decision process
setting. We first introduce a key notion of $\textit{Bellman unbiasedness}$
which is essential for exactly learnable and provably efficient distributional
updates in an online manner. Among all types of statistical functionals for
representing infinite-dimensional return distributions, our theoretical results
demonstrate that only moment functionals can exactly capture the statistical
information. Secondly, we propose a provably efficient algorithm,
$\texttt{SF-LSVI}$, that achieves a tight regret bound of $\tilde{O}(d_E
H^{\frac{3}{2}}\sqrt{K})$ where $H$ is the horizon, $K$ is the number of
episodes, and $d_E$ is the eluder dimension of a function class.

</details>


### [319] [A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective](https://arxiv.org/pdf/2403.16137)
*Ziwen Zhao, Yixin Su, Yuhua Li, Yixiong Zou, Ruixuan Li, Rui Zhang*

Main category: cs.LG

TL;DR: The paper surveys self-supervised learning for graph foundation models (GFMs) from a knowledge-based perspective, proposing a taxonomy categorizing graph knowledge into microscopic, mesoscopic, and macroscopic levels.


<details>
  <summary>Details</summary>
Motivation: Existing surveys lack comprehensiveness, clear categorization, and a broader perspective on GFMs. The goal is to learn generalized graph knowledge.

Method: Proposes a knowledge-based taxonomy with 9 categories and over 25 pretext tasks, covering nodes, links, clusters, and global structures.

Result: The taxonomy provides a clearer framework for analyzing GFMs, including newer architectures like graph language models.

Conclusion: The knowledge-based approach offers deeper insights for constructing GFMs and generalizing to downstream tasks.

Abstract: Graph self-supervised learning (SSL) is now a go-to method for pre-training
graph foundation models (GFMs). There is a wide variety of knowledge patterns
embedded in the graph data, such as node properties and clusters, which are
crucial to learning generalized representations for GFMs. However, existing
surveys of GFMs have several shortcomings: they lack comprehensiveness
regarding the most recent progress, have unclear categorization of
self-supervised methods, and take a limited architecture-based perspective that
is restricted to only certain types of graph models. As the ultimate goal of
GFMs is to learn generalized graph knowledge, we provide a comprehensive survey
of self-supervised GFMs from a novel knowledge-based perspective. We propose a
knowledge-based taxonomy, which categorizes self-supervised graph models by the
specific graph knowledge utilized. Our taxonomy consists of microscopic (nodes,
links, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge
(global structure, manifolds, etc.). It covers a total of 9 knowledge
categories and more than 25 pretext tasks for pre-training GFMs, as well as
various downstream task generalization strategies. Such a knowledge-based
taxonomy allows us to re-examine graph models based on new architectures more
clearly, such as graph language models, as well as provide more in-depth
insights for constructing GFMs.

</details>


### [320] [Lai Loss: A Novel Loss for Gradient Control](https://arxiv.org/pdf/2405.07884)
*YuFei Lai*

Main category: cs.LG

TL;DR: The paper introduces 'Lai loss,' a novel loss function integrating regularization via gradients, improving model smoothness, sensitivity, and generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional regularization methods add terms to the loss function; this work aims to integrate regularization more naturally via geometric concepts.

Method: Designs 'Lai loss' to penalize gradients within the loss function, controlling model smoothness and sensitivity. Proposes a practical training method.

Result: Experiments on Kaggle datasets show stable performance while controlling smoothness and sensitivity.

Conclusion: Lai loss effectively balances regularization and accuracy, enhancing generalization and noise resistance.

Abstract: In the field of machine learning, traditional regularization methods tend to
directly add regularization terms to the loss function. This paper introduces
the "Lai loss", a novel loss design that integrates the regularization terms
(specifically, gradients) into the traditional loss function through
straightforward geometric concepts. This design penalizes the gradients with
the loss itself, allowing for control of the gradients while ensuring maximum
accuracy. With this loss, we can effectively control the model's smoothness and
sensitivity, potentially offering the dual benefits of improving the model's
generalization performance and enhancing its noise resistance on specific
features. Additionally, we proposed a training method that successfully
addresses the challenges in practical applications. We conducted preliminary
experiments using publicly available datasets from Kaggle, demonstrating that
the design of Lai loss can control the model's smoothness and sensitivity while
maintaining stable model performance.

</details>


### [321] [A Sharper Global Convergence Analysis for Average Reward Reinforcement Learning via an Actor-Critic Approach](https://arxiv.org/pdf/2407.18878)
*Swetha Ganesh, Washim Uddin Mondal, Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work examines average-reward reinforcement learning with general policy
parametrization. Existing state-of-the-art (SOTA) guarantees for this problem
are either suboptimal or hindered by several challenges, including poor
scalability with respect to the size of the state-action space, high iteration
complexity, and dependence on knowledge of mixing times and hitting times. To
address these limitations, we propose a Multi-level Monte Carlo-based Natural
Actor-Critic (MLMC-NAC) algorithm. Our work is the first to achieve a global
convergence rate of $\tilde{\mathcal{O}}(1/\sqrt{T})$ for average-reward Markov
Decision Processes (MDPs) (where $T$ is the horizon length), without requiring
the knowledge of mixing and hitting times. Moreover, the convergence rate does
not scale with the size of the state space, therefore even being applicable to
infinite state spaces.

</details>


### [322] [Beyond Flatland: A Geometric Take on Matching Methods for Treatment Effect Estimation](https://arxiv.org/pdf/2409.05459)
*Melanie F. Pradier, Javier González*

Main category: cs.LG

TL;DR: GeoMatching improves causal inference by incorporating data geometry into matching, outperforming classic methods in high-dimensional or noisy scenarios.


<details>
  <summary>Details</summary>
Motivation: Classic matching methods ignore data geometry, leading to poor performance with noisy, high-dimensional covariates.

Method: GeoMatching learns a low-dimensional Riemannian manifold to capture data geometry and performs matching in this latent space.

Result: GeoMatching provides more effective treatment effect estimators, especially in high-dimensional, outlier-prone, or semi-supervised settings.

Conclusion: Incorporating data geometry via GeoMatching enhances causal inference accuracy and robustness.

Abstract: Matching is a popular approach in causal inference to estimate treatment
effects by pairing treated and control units that are most similar in terms of
their covariate information. However, classic matching methods completely
ignore the geometry of the data manifold, which is crucial to define a
meaningful distance for matching, and struggle when covariates are noisy and
high-dimensional. In this work, we propose GeoMatching, a matching method to
estimate treatment effects that takes into account the intrinsic data geometry
induced by existing causal mechanisms among the confounding variables. First,
we learn a low-dimensional, latent Riemannian manifold that accounts for
uncertainty and geometry of the original input data. Second, we estimate
treatment effects via matching in the latent space based on the learned latent
Riemannian metric. We provide theoretical insights and empirical results in
synthetic and real-world scenarios, demonstrating that GeoMatching yields more
effective treatment effect estimators, even as we increase input
dimensionality, in the presence of outliers, or in semi-supervised scenarios.

</details>


### [323] [Rethinking Meta-Learning from a Learning Lens](https://arxiv.org/pdf/2409.08474)
*Jingyao Wang, Wenwen Qiang, Changwen Zheng, Hui Xiong, Gang Hua*

Main category: cs.LG

TL;DR: The paper addresses underfitting in meta-learning by proposing TRLearner, a method that uses task relations to calibrate learning without altering data or model structure.


<details>
  <summary>Details</summary>
Motivation: Empirical analysis shows meta-learning may underfit despite theoretical expectations, prompting a rethink from a 'Learning' perspective.

Method: The paper introduces TRLearner, which extracts task relation matrices and applies relation-aware consistency regularization.

Result: Theoretical and empirical evaluations confirm TRLearner's effectiveness in improving meta-learning.

Conclusion: TRLearner successfully bridges the gap between theory and practice in meta-learning by leveraging task relations.

Abstract: Meta-learning seeks to learn a well-generalized model initialization from
training tasks to solve unseen tasks. From the "learning to learn" perspective,
the quality of the initialization is modeled with one-step gradient decent in
the inner loop. However, contrary to theoretical expectations, our empirical
analysis reveals that this may expose meta-learning to underfitting. To bridge
the gap between theoretical understanding and practical implementation, we
reconsider meta-learning from the "Learning" lens. We propose that the
meta-learning model comprises two interrelated components: parameters for model
initialization and a meta-layer for task-specific fine-tuning. These components
will lead to the risks of overfitting and underfitting depending on tasks, and
their solutions, fewer parameters vs. more meta-layer, are often in conflict.
To address this, we aim to regulate the task information the model receives
without modifying the data or model structure. Our theoretical analysis
indicates that models adapted to different tasks can mutually reinforce each
other, highlighting the effective information. Based on this insight, we
propose TRLearner, a plug-and-play method that leverages task relation to
calibrate meta-learning. It first extracts task relation matrices and then
applies relation-aware consistency regularization to guide optimization.
Extensive theoretical and empirical evaluations demonstrate its effectiveness.

</details>


### [324] [Improving the Weighting Strategy in KernelSHAP](https://arxiv.org/pdf/2410.04883)
*Lars Henry Berge Olsen, Martin Jullum*

Main category: cs.LG

TL;DR: Proposes deterministic weights for KernelSHAP to reduce variance and computational costs, achieving up to 50% faster runtime with the same accuracy.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of computational costs in Shapley value estimation for XAI necessitates efficient approximation methods.

Method: Modifies KernelSHAP by replacing stochastic weights with deterministic ones and introduces a simple yet effective adjustment to the SHAP library implementation.

Result: Reduces required evaluations by 5%-50% while maintaining accuracy, cutting runtime by up to 50%.

Conclusion: Enhances feasibility of Shapley value explanations for high-dimensional data and large-scale predictions.

Abstract: In Explainable AI (XAI), Shapley values are a popular model-agnostic
framework for explaining predictions made by complex machine learning models.
The computation of Shapley values requires estimating non-trivial contribution
functions representing predictions with only a subset of the features present.
As the number of these terms grows exponentially with the number of features,
computational costs escalate rapidly, creating a pressing need for efficient
and accurate approximation methods. For tabular data, the KernelSHAP framework
is considered the state-of-the-art model-agnostic approximation framework.
KernelSHAP approximates the Shapley values using a weighted sample of the
contribution functions for different feature subsets. We propose a novel
modification of KernelSHAP which replaces the stochastic weights with
deterministic ones to reduce the variance of the resulting Shapley value
approximations. This may also be combined with our simple, yet effective
modification to the KernelSHAP variant implemented in the popular Python
library SHAP. Additionally, we provide an overview of established methods.
Numerical experiments demonstrate that our methods can reduce the required
number of contribution function evaluations by $5\%$ to $50\%$ while preserving
the same accuracy of the approximated Shapley values -- essentially reducing
the running time by up to $50\%$. These computational advancements push the
boundaries of the feature dimensionality and number of predictions that can be
accurately explained with Shapley values within a feasible runtime.

</details>


### [325] [HardML: A Benchmark For Evaluating Data Science And Machine Learning knowledge and reasoning in AI](https://arxiv.org/pdf/2501.15627)
*Tidor-Vlad Pricope*

Main category: cs.LG

TL;DR: HardML is a benchmark with 100 challenging multiple-choice questions to evaluate knowledge and reasoning in data science and ML, showing current AI models' limitations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous benchmarks in data science and ML, HardML provides a modern testbed to track AI progress.

Method: HardML includes 100 original, handcrafted questions over 6 months, designed to be challenging even for senior ML engineers.

Result: Current AI models achieve a 30% error rate on HardML, significantly higher than on MMLU ML.

Conclusion: HardML serves as a valuable, though limited, benchmark to quantify AI progress in data science and ML.

Abstract: We present HardML, a benchmark designed to evaluate the knowledge and
reasoning abilities in the fields of data science and machine learning. HardML
comprises a diverse set of 100 challenging multiple-choice questions,
handcrafted over a period of 6 months, covering the most popular and modern
branches of data science and machine learning. These questions are challenging
even for a typical Senior Machine Learning Engineer to answer correctly. To
minimize the risk of data contamination, HardML uses mostly original content
devised by the author. Current state of the art AI models achieve a 30% error
rate on this benchmark, which is about 3 times larger than the one achieved on
the equivalent, well known MMLU ML. While HardML is limited in scope and not
aiming to push the frontier, primarily due to its multiple choice nature, it
serves as a rigorous and modern testbed to quantify and track the progress of
top AI. While plenty benchmarks and experimentation in LLM evaluation exist in
other STEM fields like mathematics, physics and chemistry, the subfields of
data science and machine learning remain fairly underexplored.

</details>


### [326] [AI-Aided Kalman Filters](https://arxiv.org/pdf/2410.12289)
*Nir Shlezinger, Guy Revach, Anubhab Ghosh, Saikat Chatterjee, Shuo Tang, Tales Imbiriba, Jindrich Dunik, Ondrej Straka, Pau Closas, Yonina C. Eldar*

Main category: cs.LG

TL;DR: The paper explores hybrid approaches combining Kalman filters (KFs) with deep neural networks (DNNs) for state estimation, reviewing design methods and evaluating their effectiveness.


<details>
  <summary>Details</summary>
Motivation: Traditional KFs rely on simplified state-space models, which may be inaccurate. Data-driven DNNs offer model-agnostic solutions, motivating the fusion of both to leverage their strengths.

Method: The paper reviews generic and dedicated DNN architectures for state estimation and systematically presents techniques for integrating AI with KFs, categorized into task-oriented and model-oriented approaches.

Result: Hybrid designs preserve the strengths of model-based KFs and data-driven DNNs, showing qualitative and quantitative gains, as demonstrated in a publicly available study.

Conclusion: The fusion of AI and KFs is promising but faces challenges; future research directions are discussed to advance hybrid model-based/data-driven designs.

Abstract: The Kalman filter (KF) and its variants are among the most celebrated
algorithms in signal processing. These methods are used for state estimation of
dynamic systems by relying on mathematical representations in the form of
simple state-space (SS) models, which may be crude and inaccurate descriptions
of the underlying dynamics. Emerging data-centric artificial intelligence (AI)
techniques tackle these tasks using deep neural networks (DNNs), which are
model-agnostic. Recent developments illustrate the possibility of fusing DNNs
with classic Kalman-type filtering, obtaining systems that learn to track in
partially known dynamics. This article provides a tutorial-style overview of
design approaches for incorporating AI in aiding KF-type algorithms. We review
both generic and dedicated DNN architectures suitable for state estimation, and
provide a systematic presentation of techniques for fusing AI tools with KFs
and for leveraging partial SS modeling and data, categorizing design approaches
into task-oriented and SS model-oriented. The usefulness of each approach in
preserving the individual strengths of model-based KFs and data-driven DNNs is
investigated in a qualitative and quantitative study, whose code is publicly
available, illustrating the gains of hybrid model-based/data-driven designs. We
also discuss existing challenges and future research directions that arise from
fusing AI and Kalman-type algorithms.

</details>


### [327] [HadamRNN: Binary and Sparse Ternary Orthogonal RNNs](https://arxiv.org/pdf/2502.00047)
*Armand Foucault, Franck Mamalet, François Malgouyres*

Main category: cs.LG

TL;DR: A novel method uses Hadamard matrices to train orthogonal RNNs with binary/ternary weights, achieving performance comparable to full-precision models.


<details>
  <summary>Details</summary>
Motivation: Binary/ternary weights enable faster computations for edge devices, but vanilla RNNs are sensitive to weight changes, making binarization challenging.

Method: Leveraging Hadamard matrices to parameterize binary/ternary orthogonal matrices, enabling training of orthogonal RNNs (ORNNs).

Result: ORNNs (HadamRNN, Block-HadamRNN) match full-precision models on benchmarks, including the copy task over 1000 timesteps.

Conclusion: The approach successfully binarizes/ternarizes RNN weights, maintaining performance and enabling edge deployment.

Abstract: Binary and sparse ternary weights in neural networks enable faster
computations and lighter representations, facilitating their use on edge
devices with limited computational power. Meanwhile, vanilla RNNs are highly
sensitive to changes in their recurrent weights, making the binarization and
ternarization of these weights inherently challenging. To date, no method has
successfully achieved binarization or ternarization of vanilla RNN weights. We
present a new approach leveraging the properties of Hadamard matrices to
parameterize a subset of binary and sparse ternary orthogonal matrices. This
method enables the training of orthogonal RNNs (ORNNs) with binary and sparse
ternary recurrent weights, effectively creating a specific class of binary and
sparse ternary vanilla RNNs. The resulting ORNNs, called HadamRNN and
Block-HadamRNN, are evaluated on benchmarks such as the copy task, permuted and
sequential MNIST tasks, the IMDB dataset, two GLUE benchmarks, and two IoT
benchmarks. Despite binarization or sparse ternarization, these RNNs maintain
performance levels comparable to state-of-the-art full-precision models,
highlighting the effectiveness of our approach. Notably, our approach is the
first solution with binary recurrent weights capable of tackling the copy task
over 1000 timesteps.

</details>


### [328] [Local Off-Grid Weather Forecasting with Multi-Modal Earth Observation Data](https://arxiv.org/pdf/2410.12938)
*Qidong Yang, Jonathan Giezendanner, Daniel Salles Civitarese, Johannes Jakubik, Eric Schmitt, Anirban Chandra, Jeremy Vila, Detlef Hohl, Chris Hill, Campbell Watson, Sherrie Wang*

Main category: cs.LG

TL;DR: A multi-modal transformer model is proposed to downscale gridded weather forecasts to off-grid locations, combining local observations and gridded data for accurate, localized predictions.


<details>
  <summary>Details</summary>
Motivation: Precise, localized weather forecasts are crucial for urgent applications like wildfire management and renewable energy, but existing methods fail to capture fine-grained patterns.

Method: A multi-modal transformer model is trained end-to-end to combine local historical weather observations with gridded forecasts, using self-attention to aggregate information from neighboring stations.

Result: The model outperforms other methods, reducing prediction error by up to 80% compared to gridded-data-only models.

Conclusion: This approach bridges the gap between large-scale weather models and locally accurate forecasts, aiding high-stakes decision-making.

Abstract: Urgent applications like wildfire management and renewable energy generation
require precise, localized weather forecasts near the Earth's surface. However,
forecasts produced by machine learning models or numerical weather prediction
systems are typically generated on large-scale regular grids, where direct
downscaling fails to capture fine-grained, near-surface weather patterns. In
this work, we propose a multi-modal transformer model trained end-to-end to
downscale gridded forecasts to off-grid locations of interest. Our model
directly combines local historical weather observations (e.g., wind,
temperature, dewpoint) with gridded forecasts to produce locally accurate
predictions at various lead times. Multiple data modalities are collected and
concatenated at station-level locations, treated as a token at each station.
Using self-attention, the token corresponding to the target location aggregates
information from its neighboring tokens. Experiments using weather stations
across the Northeastern United States show that our model outperforms a range
of data-driven and non-data-driven off-grid forecasting methods. They also
reveal that direct input of station data provides a phase shift in local
weather forecasting accuracy, reducing the prediction error by up to 80%
compared to pure gridded data based models. This approach demonstrates how to
bridge the gap between large-scale weather models and locally accurate
forecasts to support high-stakes, location-sensitive decision-making.

</details>


### [329] [Reward-free World Models for Online Imitation Learning](https://arxiv.org/pdf/2410.14081)
*Shangzhe Li, Zhiao Huang, Hao Su*

Main category: cs.LG

TL;DR: A novel online imitation learning method using reward-free world models in latent spaces achieves expert-level performance in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of prior online IL approaches in handling high-dimensional inputs and complex dynamics.

Method: Leverages latent space dynamics modeling and inverse soft-Q learning for stable optimization.

Result: Demonstrates superior performance on benchmarks like DMControl, MyoSuite, and ManiSkill2.

Conclusion: The proposed method effectively handles complex tasks, outperforming existing approaches.

Abstract: Imitation learning (IL) enables agents to acquire skills directly from expert
demonstrations, providing a compelling alternative to reinforcement learning.
However, prior online IL approaches struggle with complex tasks characterized
by high-dimensional inputs and complex dynamics. In this work, we propose a
novel approach to online imitation learning that leverages reward-free world
models. Our method learns environmental dynamics entirely in latent spaces
without reconstruction, enabling efficient and accurate modeling. We adopt the
inverse soft-Q learning objective, reformulating the optimization process in
the Q-policy space to mitigate the instability associated with traditional
optimization in the reward-policy space. By employing a learned latent dynamics
model and planning for control, our approach consistently achieves stable,
expert-level performance in tasks with high-dimensional observation or action
spaces and intricate dynamics. We evaluate our method on a diverse set of
benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating
superior empirical performance compared to existing approaches.

</details>


### [330] [An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model](https://arxiv.org/pdf/2502.14131)
*Enoch H. Kang, Hema Yoganarasimhan, Lalit Jain*

Main category: cs.LG

TL;DR: A gradient-based method for estimating Dynamic Discrete Choice models without linear reward assumptions, using ERM-based IRL/DDC framework and non-parametric techniques like neural networks, ensuring fast global convergence via the PL condition.


<details>
  <summary>Details</summary>
Motivation: To recover reward or Q* functions from offline behavior data without restrictive assumptions, enabling scalability to high-dimensional spaces.

Method: Proposes a globally convergent gradient-based method using ERM-based IRL/DDC framework, avoiding explicit state transition probability estimation.

Result: Outperforms benchmark methods in synthetic experiments, demonstrating scalability and efficiency.

Conclusion: The method offers a scalable, efficient solution for high-dimensional DDC/MaxEnt-IRL problems with theoretical guarantees.

Abstract: We study the problem of estimating Dynamic Discrete Choice (DDC) models, also
known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning
(offline MaxEnt-IRL) in machine learning. The objective is to recover reward or
$Q^*$ functions that govern agent behavior from offline behavior data. In this
paper, we propose a globally convergent gradient-based method for solving these
problems without the restrictive assumption of linearly parameterized rewards.
The novelty of our approach lies in introducing the Empirical Risk Minimization
(ERM) based IRL/DDC framework, which circumvents the need for explicit state
transition probability estimation in the Bellman equation. Furthermore, our
method is compatible with non-parametric estimation techniques such as neural
networks. Therefore, the proposed method has the potential to be scaled to
high-dimensional, infinite state spaces. A key theoretical insight underlying
our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL)
condition -- a property that, while weaker than strong convexity, is sufficient
to ensure fast global convergence guarantees. Through a series of synthetic
experiments, we demonstrate that our approach consistently outperforms
benchmark methods and state-of-the-art alternatives.

</details>


### [331] [Neural Network Matrix Product Operator: A Multi-Dimensionally Integrable Machine Learning Potential](https://arxiv.org/pdf/2410.23858)
*Kentaro Hino, Yuki Kurashige*

Main category: cs.LG

TL;DR: A neural network-based PES in MPO form (NN-MPO) efficiently handles high-dimensional integrals, overcoming dimensionality issues while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating high-dimensional integrals in neural network-based PES methods, which is not straightforward in traditional architectures like MLPs.

Method: Proposes NN-MPO, leveraging MPO form for efficient integral evaluation, retaining neural network representational capacity.

Result: Achieves spectroscopic accuracy with a test MAE of 3.03 cm⁻¹ for a six-dimensional PES using only 625 training points.

Conclusion: NN-MPO is a promising approach for high-dimensional PES with computational efficiency and accuracy.

Abstract: A neural network-based machine learning potential energy surface (PES)
expressed in a matrix product operator (NN-MPO) is proposed. The MPO form
enables efficient evaluation of high-dimensional integrals that arise in
solving the time-dependent and time-independent Schr\"odinger equation and
effectively overcomes the so-called curse of dimensionality. This starkly
contrasts with other neural network-based machine learning PES methods, such as
multi-layer perceptrons (MLPs), where evaluating high-dimensional integrals is
not straightforward due to the fully connected topology in their backbone
architecture. Nevertheless, the NN-MPO retains the high representational
capacity of neural networks. NN-MPO can achieve spectroscopic accuracy with a
test mean absolute error (MAE) of 3.03 cm$^{-1}$ for a fully coupled
six-dimensional ab initio PES, using only 625 training points distributed
across a 0 to 17,000 cm$^{-1}$ energy range. Our Python implementation is
available at https://github.com/KenHino/Pompon.

</details>


### [332] [Don't Mesh with Me: Generating Constructive Solid Geometry Instead of Meshes by Fine-Tuning a Code-Generation LLM](https://arxiv.org/pdf/2411.15279)
*Maximilian Mews, Ansar Aynetdinov, Vivian Schiller, Peter Eisert, Alan Akbik*

Main category: cs.LG

TL;DR: A novel method uses a code-generation LLM to create precise 3D mechanical parts by converting BREP to CSG scripts, fine-tuned with GPT-4 annotations, enabling natural language input for geometry completion.


<details>
  <summary>Details</summary>
Motivation: Current machine learning advancements, like LLMs, have limited impact on mechanical engineering, where manual processes dominate. Meshes, common in 3D geometry, lack precision for engineering needs.

Method: Convert BREP geometry to CSG-based Python scripts, annotate with GPT-4, and fine-tune a code-generation LLM to generate plausible 3D geometries from positional and natural language inputs.

Result: The fine-tuned LLM successfully generates precise 3D mechanical parts, demonstrating geometric understanding through natural language and positional inputs.

Conclusion: This approach bridges the gap between LLMs and mechanical engineering, offering a precise, adaptable solution for 3D geometry generation.

Abstract: While recent advancements in machine learning, such as LLMs, are
revolutionizing software development and creative industries, they have had
minimal impact on engineers designing mechanical parts, which remains largely a
manual process. Existing approaches to generating 3D geometry most commonly use
meshes as a 3D representation. While meshes are suitable for assets in video
games or animations, they lack sufficient precision and adaptability for
mechanical engineering purposes. This paper introduces a novel approach for the
generation of 3D geometry that generates surface-based Constructive Solid
Geometry (CSG) by leveraging a code-generation LLM. First, we create a dataset
of 3D mechanical parts represented as code scripts by converting Boundary
Representation geometry (BREP) into CSG-based Python scripts. Second, we create
annotations in natural language using GPT-4. The resulting dataset is used to
fine-tune a code-generation LLM. The fine-tuned LLM can complete geometries
based on positional input and natural language in a plausible way,
demonstrating geometric understanding.

</details>


### [333] [Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models](https://arxiv.org/pdf/2503.06269)
*Thomas Winninger, Boussad Addad, Katarzyna Kapusta*

Main category: cs.LG

TL;DR: A novel white-box adversarial attack method combines mechanistic interpretability with gradient-based optimization to efficiently jailbreak LLMs by rerouting embeddings from refusal to acceptance subspaces.


<details>
  <summary>Details</summary>
Motivation: Traditional adversarial methods ignore internal model mechanisms, while interpretability studies lack practical applications. This work bridges the gap by using interpretability for practical attacks.

Method: Identify acceptance subspaces (feature vectors avoiding refusal), then use gradient optimization to reroute embeddings from refusal to acceptance subspaces.

Result: Achieves 80-95% success rates on models like Gemma2, Llama3.2, and Qwen2.5 in minutes/seconds, outperforming existing methods.

Conclusion: This approach advances attack/defense research and demonstrates practical utility of mechanistic interpretability. Code and datasets are publicly available.

Abstract: Traditional white-box methods for creating adversarial perturbations against
LLMs typically rely only on gradient computation from the targeted model,
ignoring the internal mechanisms responsible for attack success or failure.
Conversely, interpretability studies that analyze these internal mechanisms
lack practical applications beyond runtime interventions. We bridge this gap by
introducing a novel white-box approach that leverages mechanistic
interpretability techniques to craft practical adversarial inputs.
Specifically, we first identify acceptance subspaces - sets of feature vectors
that do not trigger the model's refusal mechanisms - then use gradient-based
optimization to reroute embeddings from refusal subspaces to acceptance
subspaces, effectively achieving jailbreaks. This targeted approach
significantly reduces computation cost, achieving attack success rates of
80-95\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5
within minutes or even seconds, compared to existing techniques that often fail
or require hours of computation. We believe this approach opens a new direction
for both attack research and defense development. Furthermore, it showcases a
practical application of mechanistic interpretability where other methods are
less efficient, which highlights its utility. The code and generated datasets
are available at https://github.com/Sckathach/subspace-rerouting.

</details>


### [334] [Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets](https://arxiv.org/pdf/2412.07775)
*Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang*

Main category: cs.LG

TL;DR: The paper proposes Nabla-GFlowNet, a reinforcement learning method for finetuning diffusion models, addressing diversity, prior preservation, and slow convergence issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reward finetuning of diffusion models lack diversity, prior preservation, and slow convergence.

Method: The authors introduce Nabla-GFlowNet, leveraging reward gradients for probabilistic diffusion finetuning.

Result: The method achieves fast, diversity-preserving, and prior-preserving finetuning of Stable Diffusion on realistic reward functions.

Conclusion: Nabla-GFlowNet effectively addresses key challenges in diffusion model finetuning.

Abstract: While one commonly trains large diffusion models by collecting datasets on
target downstream tasks, it is often desired to align and finetune pretrained
diffusion models with some reward functions that are either designed by experts
or learned from small-scale datasets. Existing post-training methods for reward
finetuning of diffusion models typically suffer from lack of diversity in
generated samples, lack of prior preservation, and/or slow convergence in
finetuning. In response to this challenge, we take inspiration from recent
successes in generative flow networks (GFlowNets) and propose a reinforcement
learning method for diffusion model finetuning, dubbed Nabla-GFlowNet
(abbreviated as $\nabla$-GFlowNet), that leverages the rich signal in reward
gradients for probabilistic diffusion finetuning. We show that our proposed
method achieves fast yet diversity- and prior-preserving finetuning of Stable
Diffusion, a large-scale text-conditioned image diffusion model, on different
realistic reward functions.

</details>


### [335] [Knowledge-guided machine learning for county-level corn yield prediction under drought](https://arxiv.org/pdf/2503.16328)
*Xiaoyu Wang, Yijia Xu, Jingyi Huang, Zhengwei Yang, Zhou Zhang*

Main category: cs.LG

TL;DR: The paper introduces KGML-SM, a framework combining knowledge-guided ML and soil moisture data to improve corn yield predictions, outperforming traditional ML models.


<details>
  <summary>Details</summary>
Motivation: Traditional models struggle with RS data and lack interpretability, while ML models are seen as black boxes. Soil moisture's role in corn growth is often overlooked.

Method: Developed KGML-SM, treating soil moisture as an intermediate variable and using a drought-aware loss function.

Result: KGML-SM outperformed traditional ML models, with insights into drought, soil moisture, and yield relationships.

Conclusion: The framework enhances interpretability and accuracy, guiding future model optimization.

Abstract: Remote sensing (RS) technique, enabling the non-contact acquisition of
extensive ground observations, is a valuable tool for crop yield predictions.
Traditional process-based models struggle to incorporate large volumes of RS
data, and most users lack understanding of crop growth mechanisms. In contrast,
machine learning (ML) models are often criticized as "black boxes" due to their
limited interpretability. To address these limitations, we utilized
Knowledge-Guided Machine Learning (KGML), a framework that leverages the
strengths of both process-based and ML models. Existing works have either
overlooked the role of soil moisture in corn growth or did not embed this
effect into their models. To bridge this gap, we developed the Knowledge-Guided
Machine Learning with Soil Moisture (KGML-SM) framework, treating soil moisture
as an intermediate variable in corn growth to emphasize its key role in plant
development. Additionally, based on the prior knowledge that the model may
overestimate under drought conditions, we designed a drought-aware loss
function that penalized predicted yield in drought-affected areas. Our
experiments showed that the KGML-SM model outperformed other traditional ML
models. We explored the relationships between drought, soil moisture, and corn
yield prediction by assessing the importance of different features within the
model, and analyzing how soil moisture impacts predictions across different
regions and time periods. Finally we provided interpretability for prediction
errors to guide future model optimization.

</details>


### [336] [Unleashing the Power of Continual Learning on Non-Centralized Devices: A Survey](https://arxiv.org/pdf/2412.13840)
*Yichen Li, Haozhao Wang, Wenchao Xu, Tianzhe Xiao, Hong Liu, Minzhu Tu, Yuying Wang, Xin Yang, Rui Zhang, Shui Yu, Song Guo, Ruixuan Li*

Main category: cs.LG

TL;DR: A survey on Non-Centralized Continual Learning (NCCL) addressing challenges like distribution shifts, catastrophic forgetting, heterogeneity, and privacy in distributed systems. It reviews algorithms, benchmarks, and real-world applications.


<details>
  <summary>Details</summary>
Motivation: To enable distributed devices (e.g., vehicles, servers) to handle streaming data in non-stationary environments reliably and scalably.

Method: Comprehensive examination of NCCL algorithms, reviewing solutions from three levels, analyzing heterogeneity, security, privacy, and real-world applications. A benchmark is established to evaluate state-of-the-art NCCL approaches.

Result: Identifies key challenges and performance of NCCL techniques, highlighting gaps in current solutions.

Conclusion: Discusses future research directions and critical challenges in NCCL, emphasizing its potential and limitations.

Abstract: Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for
enabling distributed devices such as vehicles and servers to handle streaming
data from a joint non-stationary environment. To achieve high reliability and
scalability in deploying this paradigm in distributed systems, it is essential
to conquer challenges stemming from both spatial and temporal dimensions,
manifesting as distribution shifts, catastrophic forgetting, heterogeneity, and
privacy issues. This survey focuses on a comprehensive examination of the
development of the non-centralized continual learning algorithms and the
real-world deployment across distributed devices. We begin with an introduction
to the background and fundamentals of non-centralized learning and continual
learning. Then, we review existing solutions from three levels to represent how
existing techniques alleviate the catastrophic forgetting and distribution
shift. Additionally, we delve into the various types of heterogeneity issues,
security, and privacy attributes, as well as real-world applications across
three prevalent scenarios. Furthermore, we establish a large-scale benchmark to
revisit this problem and analyze the performance of the state-of-the-art NCCL
approaches. Finally, we discuss the important challenges and future research
directions in NCCL.

</details>


### [337] [Regularized second-order optimization of tensor-network Born machines](https://arxiv.org/pdf/2501.18691)
*Matan Ben-Dov, Jing Chen*

Main category: cs.LG

TL;DR: The paper introduces an improved second-order optimization technique for Tensor-network Born machines (TNBMs) to address slow convergence and local minima issues, enhancing stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The logarithmic loss function in TNBMs causes slow convergence and traps in local minima, hindering their potential as quantum-inspired generative models.

Method: A modified Newton's method on the manifold of normalized states, with regularization of the loss landscape, is proposed.

Result: The method improves convergence rates and model quality, demonstrated on discrete and continuous datasets using a one-dimensional MPS.

Conclusion: The approach is robust and scalable, offering a promising solution for optimizing quantum-inspired generative models.

Abstract: Tensor-network Born machines (TNBMs) are quantum-inspired generative models
for learning data distributions. Using tensor-network contraction and
optimization techniques, the model learns an efficient representation of the
target distribution, capable of capturing complex correlations with a compact
parameterization. Despite their promise, the optimization of TNBMs presents
several challenges. A key bottleneck of TNBMs is the logarithmic nature of the
loss function commonly used for this problem. The single-tensor logarithmic
optimization problem cannot be solved analytically, necessitating an iterative
approach that slows down convergence and increases the risk of getting trapped
in one of many non-optimal local minima. In this paper, we present an improved
second-order optimization technique for TNBM training, which significantly
enhances convergence rates and the quality of the optimized model. Our method
employs a modified Newton's method on the manifold of normalized states,
incorporating regularization of the loss landscape to mitigate local minima
issues. We demonstrate the effectiveness of our approach by training a
one-dimensional matrix product state (MPS) on both discrete and continuous
datasets, showcasing its advantages in terms of stability and efficiency, and
demonstrating its potential as a robust and scalable approach for optimizing
quantum-inspired generative models.

</details>


### [338] [LGIN: Defining an Approximately Powerful Hyperbolic GNN](https://arxiv.org/pdf/2504.00142)
*Srinitish Srinivasan, Omkumar CU*

Main category: cs.LG

TL;DR: LGIN, a hyperbolic GNN, enhances discriminative power for graph learning by leveraging Lorentzian space, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the expressiveness gap in hyperbolic GNNs by improving discriminative power for non-isomorphic graphs.

Method: Introduces LGIN with a novel update rule combining local neighborhood info and Lorentzian metric tensor preservation.

Result: LGIN outperforms state-of-the-art GNNs on diverse datasets, proving superior discriminative ability.

Conclusion: LGIN advances hyperbolic GNN expressiveness, validated by empirical results, and is the first of its kind.

Abstract: While graph neural networks (GNNs) operating in hyperbolic spaces have shown
promise for modeling hierarchical and complex relational data, a critical
limitation often overlooked is their potentially limited discriminative power
compared to their Euclidean counterparts or fundamental graph isomorphism tests
like the Weisfeiler-Lehman (WL) hierarchy. Existing hyperbolic aggregation
schemes, while curvature-aware, may not sufficiently capture the intricate
structural differences required to robustly distinguish non-isomorphic graphs
owing to non-injective aggregation functions. To address this expressiveness
gap in hyperbolic graph learning, we introduce the Lorentzian Graph Isomorphic
Network (LGIN), a novel GNN designed to achieve enhanced discriminative
capabilities within the Lorentzian model of hyperbolic space. LGIN proposes a
new update rule that effectively combines local neighborhood information with a
richer representation of graph structure designed to preserve the Lorentzian
metric tensor. This represents a significant step towards building more
expressive GNNs in non-Euclidean geometries, overcoming a common bottleneck in
current hyperbolic methods. We conduct extensive evaluations across nine
diverse benchmark datasets, including molecular and protein structures. LGIN
consistently outperforms or matches state-of-the-art hyperbolic and Euclidean
GNNs, showcasing its practical efficacy and validating its superior ability to
capture complex graph structures and distinguish between different graphs. To
the best of our knowledge, LGIN is the first work to study the framework behind
a powerful GNN on the hyperbolic space. The code for our paper can be found at
https://github.com/Deceptrax123/LGIN

</details>


### [339] [DyTTP: Trajectory Prediction with Normalization-Free Transformers](https://arxiv.org/pdf/2504.05356)
*JianLin Zhu, HongKuo Niu*

Main category: cs.LG

TL;DR: The paper introduces a two-fold approach for trajectory prediction in autonomous driving: using DynamicTanh (DyT) to replace layer normalization in transformers and employing snapshot ensemble for improved performance.


<details>
  <summary>Details</summary>
Motivation: Accurate trajectory prediction is critical for autonomous driving, but transformer-based methods face issues like computation overhead and training instability due to normalization layers.

Method: The approach integrates DyT into transformers to replace layer normalization and uses snapshot ensemble with cyclical learning rates to capture multiple model snapshots.

Result: Experiments on Argoverse datasets show significant improvements in prediction accuracy, inference speed, and robustness.

Conclusion: The work highlights the potential of normalization-free transformers and lightweight ensemble techniques for advancing trajectory forecasting.

Abstract: Accurate trajectory prediction is a cornerstone for the safe operation of
autonomous driving systems, where understanding the dynamic behavior of
surrounding agents is crucial. Transformer-based architectures have
demonstrated significant promise in capturing complex spatio-temporality
dependencies. However, their reliance on normalization layers can lead to
computation overhead and training instabilities. In this work, we present a
two-fold approach to address these challenges. First, we integrate DynamicTanh
(DyT), which is the latest method to promote transformers, into the backbone,
replacing traditional layer normalization. This modification simplifies the
network architecture and improves the stability of the inference. We are the
first work to deploy the DyT to the trajectory prediction task. Complementing
this, we employ a snapshot ensemble strategy to further boost trajectory
prediction performance. Using cyclical learning rate scheduling, multiple model
snapshots are captured during a single training run. These snapshots are then
aggregated via simple averaging at inference time, allowing the model to
benefit from diverse hypotheses without incurring substantial additional
computational cost. Extensive experiments on Argoverse datasets demonstrate
that our combined approach significantly improves prediction accuracy,
inference speed and robustness in diverse driving scenarios. This work
underscores the potential of normalization-free transformer designs augmented
with lightweight ensemble techniques in advancing trajectory forecasting for
autonomous vehicles.

</details>


### [340] [Towards Combinatorial Interpretability of Neural Computation](https://arxiv.org/pdf/2504.08842)
*Micah Adler, Dan Alistarh, Nir Shavit*

Main category: cs.LG

TL;DR: The paper introduces combinatorial interpretability and feature channel coding to decode neural network computations by analyzing weight matrices, focusing on Boolean functions and their combinatorial structures.


<details>
  <summary>Details</summary>
Motivation: To understand neural computation by deciphering the combinatorial structures in network weights and biases, moving beyond traditional activation-based methods.

Method: Uses feature channel coding to analyze weight matrices statically, without examining activations or training additional networks, to interpret neural computations.

Result: Demonstrates complete mechanistic interpretations of small neural networks, quantifies the relationship between parameter size and computational capacity, and reframes the superposition hypothesis.

Conclusion: Combinatorial interpretability and feature channel coding provide a foundational approach for understanding neural computation in both artificial and biological networks, starting with Boolean functions.

Abstract: We introduce combinatorial interpretability, a methodology for understanding
neural computation by analyzing the combinatorial structures in the sign-based
categorization of a network's weights and biases. We demonstrate its power
through feature channel coding, a theory that explains how neural networks
compute Boolean expressions and potentially underlies other categories of
neural network computation. According to this theory, features are computed via
feature channels: unique cross-neuron encodings shared among the inputs the
feature operates on. Because different feature channels share neurons, the
neurons are polysemantic and the channels interfere with one another, making
the computation appear inscrutable.
  We show how to decipher these computations by analyzing a network's feature
channel coding, offering complete mechanistic interpretations of several small
neural networks that were trained with gradient descent. Crucially, this is
achieved via static combinatorial analysis of the weight matrices, without
examining activations or training new autoencoding networks. Feature channel
coding reframes the superposition hypothesis, shifting the focus from neuron
activation directionality in high-dimensional space to the combinatorial
structure of codes. It also allows us for the first time to exactly quantify
and explain the relationship between a network's parameter size and its
computational capacity (i.e. the set of features it can compute with low
error), a relationship that is implicitly at the core of many modern scaling
laws.
  Though our initial studies of feature channel coding are restricted to
Boolean functions, we believe they provide a rich, controlled, and informative
research space, and that the path we propose for combinatorial interpretation
of neural computation can provide a basis for understanding both artificial and
biological neural circuits.

</details>


### [341] [Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion](https://arxiv.org/pdf/2504.16875)
*Julian Bedei, Murray McBain, Alexander Winkler, Charles Robert Koch, Jakob Andert, David Gordon*

Main category: cs.LG

TL;DR: A hybrid RL and ML-MPC approach optimizes hydrogen-diesel dual-fuel engine control, combining RL's adaptability with ML-MPC's safety.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of RL (unsafe actions during learning) and ML-MPC (lack of adaptability to system drifts) in engine control.

Method: Proposes a hybrid approach where RL dynamically adjusts ML-MPC's load tracking reference while ML-MPC ensures safe control inputs.

Result: RL adapts to changing conditions, reducing RMSE from 0.57 bar (ML-MPC alone) to 0.44 bar (hybrid).

Conclusion: The hybrid approach effectively balances adaptability and safety, improving engine control performance.

Abstract: Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive
Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel
dual-fuel engine control, as they can effectively control multiple-input
multiple-output systems and nonlinear processes. ML-MPC is advantageous for
providing safe and optimal controls, ensuring the engine operates within
predefined safety limits. In contrast, RL is distinguished by its adaptability
to changing conditions through its learning-based approach. However, the
practical implementation of either method alone poses challenges. RL requires
high variance in control inputs during early learning phases, which can pose
risks to the system by potentially executing unsafe actions, leading to
mechanical damage. Conversely, ML-MPC relies on an accurate system model to
generate optimal control inputs and has limited adaptability to system drifts,
such as injector aging, which naturally occur in engine applications. To
address these limitations, this study proposes a hybrid RL and ML-MPC approach
that uses an ML-MPC framework while incorporating an RL agent to dynamically
adjust the ML-MPC load tracking reference in response to changes in the
environment. At the same time, the ML-MPC ensures that actions stay safe
throughout the RL agent's exploration. To evaluate the effectiveness of this
approach, fuel pressure is deliberately varied to introduce a model-plant
mismatch between the ML-MPC and the engine test bench. The result of this
mismatch is a root mean square error (RMSE) in indicated mean effective
pressure of 0.57 bar when running the ML-MPC. The experimental results
demonstrate that RL successfully adapts to changing boundary conditions by
altering the tracking reference while ML-MPC ensures safe control inputs. The
quantitative improvement in load tracking by implementing RL is an RSME of 0.44
bar.

</details>


### [342] [ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning](https://arxiv.org/pdf/2504.21254)
*Sixuan Wang, Jiao Yin, Jinli Cao, MingJian Tang, Hua Wang, Yanchun Zhang*

Main category: cs.LG

TL;DR: ABG-NAS is a novel framework for automated graph neural network architecture search, outperforming manual GNNs and state-of-the-art NAS methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs struggle with diverse graph structures, limiting their effectiveness for tasks like node classification and link prediction.

Method: ABG-NAS combines a Comprehensive Architecture Search Space (CASS), Adaptive Genetic Optimization Strategy (AGOS), and Bayesian-Guided Tuning Module (BGTM) to discover and optimize GNN architectures.

Result: ABG-NAS consistently outperforms manual GNNs and NAS methods on datasets like Cora, PubMed, Citeseer, and CoraFull.

Conclusion: ABG-NAS advances graph representation learning by offering scalable and adaptive solutions for complex graph structures.

Abstract: Effective and efficient graph representation learning is essential for
enabling critical downstream tasks, such as node classification, link
prediction, and subgraph search. However, existing graph neural network (GNN)
architectures often struggle to adapt to diverse and complex graph structures,
limiting their ability to produce structure-aware and task-discriminative
representations. To address this challenge, we propose ABG-NAS, a novel
framework for automated graph neural network architecture search tailored for
efficient graph representation learning. ABG-NAS encompasses three key
components: a Comprehensive Architecture Search Space (CASS), an Adaptive
Genetic Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module
(BGTM). CASS systematically explores diverse propagation (P) and transformation
(T) operations, enabling the discovery of GNN architectures capable of
capturing intricate graph characteristics. AGOS dynamically balances
exploration and exploitation, ensuring search efficiency and preserving
solution diversity. BGTM further optimizes hyperparameters periodically,
enhancing the scalability and robustness of the resulting architectures.
Empirical evaluations on benchmark datasets (Cora, PubMed, Citeseer, and
CoraFull) demonstrate that ABG-NAS consistently outperforms both manually
designed GNNs and state-of-the-art neural architecture search (NAS) methods.
These results highlight the potential of ABG-NAS to advance graph
representation learning by providing scalable and adaptive solutions for
diverse graph structures. Our code is publicly available at
https://github.com/sserranw/ABG-NAS.

</details>


### [343] [Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities](https://arxiv.org/pdf/2505.01169)
*Pramook Khungurn, Pratch Piyawongwisal, Sira Sriswasdi, Supasorn Suwajanakorn*

Main category: cs.LG

TL;DR: The paper introduces a new loss function (ITVM) for distilling a two-timed flow model (TTFM), improving few-step generation performance by matching initial/terminal velocities and stabilizing training with EMA.


<details>
  <summary>Details</summary>
Motivation: To enhance the distillation of flow matching models into TTFMs by addressing limitations of prior methods (e.g., LFMD loss) for better few-step generation.

Method: Proposes the ITVM loss, which matches initial velocities, simplifies terminal velocity terms, and uses EMA-stabilized models for target computation.

Result: Preliminary experiments show ITVM outperforms baselines in few-step generation across datasets and architectures.

Conclusion: The ITVM loss effectively improves TTFM distillation, enabling better performance in generative tasks.

Abstract: A flow matching model learns a time-dependent vector field $v_t(x)$ that
generates a probability path $\{ p_t \}_{0 \leq t \leq 1}$ that interpolates
between a well-known noise distribution ($p_0$) and the data distribution
($p_1$). It can be distilled into a two-timed flow model (TTFM) $\phi_{s,x}(t)$
that can transform a sample belonging to the distribution at an initial time
$s$ to another belonging to the distribution at a terminal time $t$ in one
function evaluation. We present a new loss function for TTFM distillation
called the \emph{initial/terminal velocity matching} (ITVM) loss that extends
the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi et al. by
adding redundant terms to match the initial velocities at time $s$, removing
the derivative from the terminal velocity term at time $t$, and using a version
of the model under training, stabilized by exponential moving averaging (EMA),
to compute the target terminal average velocity. Preliminary experiments show
that our loss leads to better few-step generation performance on multiple types
of datasets and model architectures over baselines.

</details>


### [344] [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/pdf/2505.01420)
*Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis Ho, Rohin Shah*

Main category: cs.LG

TL;DR: The paper proposes evaluations to detect AI models' ability to scheme (act against developer intentions) by testing stealth and situational awareness, finding no concerning capabilities in current models.


<details>
  <summary>Details</summary>
Motivation: To address the risk of AI models scheming (misaligned covert behavior) and ensure safety before deployment.

Method: Introduces 16 evaluations: 5 for stealth (circumventing oversight) and 11 for situational awareness (self/environment reasoning).

Result: Current frontier models show no concerning levels of stealth or situational awareness.

Conclusion: The evaluations can help rule out scheming risks, ensuring safer AI deployment.

Abstract: Recent work has demonstrated the plausibility of frontier AI models scheming
-- knowingly and covertly pursuing an objective misaligned with its developer's
intentions. Such behavior could be very hard to detect, and if present in
future advanced systems, could pose severe loss of control risk. It is
therefore important for AI developers to rule out harm from scheming prior to
model deployment. In this paper, we present a suite of scheming reasoning
evaluations measuring two types of reasoning capabilities that we believe are
prerequisites for successful scheming: First, we propose five evaluations of
ability to reason about and circumvent oversight (stealth). Second, we present
eleven evaluations for measuring a model's ability to instrumentally reason
about itself, its environment and its deployment (situational awareness). We
demonstrate how these evaluations can be used as part of a scheming inability
safety case: a model that does not succeed on these evaluations is almost
certainly incapable of causing severe harm via scheming in real deployment. We
run our evaluations on current frontier models and find that none of them show
concerning levels of either situational awareness or stealth.

</details>


### [345] [Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation](https://arxiv.org/pdf/2505.01584)
*Zhiqiang He, Zhi Liu*

Main category: cs.LG

TL;DR: The paper addresses neural plasticity loss in reinforcement learning for network adaptation, proposing the Silent Neuron theory and ReSiN method, which improves bitrate and QoE.


<details>
  <summary>Details</summary>
Motivation: Current solutions for network adaptation rely on stationary assumptions, and neural networks suffer from plasticity loss, hindering adaptability to dynamic conditions.

Method: The paper introduces the Silent Neuron theory and the ReSiN method, which strategically resets neurons based on propagation states to preserve plasticity.

Result: ReSiN achieves up to 168% higher bitrate and 108% better QoE in adaptive video streaming, outperforming existing solutions.

Conclusion: ReSiN effectively addresses plasticity loss, demonstrating robust adaptability across varying network conditions.

Abstract: Adapting to non-stationary network conditions presents significant challenges
for resource adaptation. However, current solutions primarily rely on
stationary assumptions. While data-driven reinforcement learning approaches
offer promising solutions for handling network dynamics, our systematic
investigation reveals a critical limitation: neural networks suffer from
plasticity loss, significantly impeding their ability to adapt to evolving
network conditions. Through theoretical analysis of neural propagation
mechanisms, we demonstrate that existing dormant neuron metrics inadequately
characterize neural plasticity loss. To address this limitation, we have
developed the Silent Neuron theory, which provides a more comprehensive
framework for understanding plasticity degradation. Based on these theoretical
insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural
plasticity through strategic neuron resets guided by both forward and backward
propagation states. In our implementation of an adaptive video streaming
system, ReSiN has shown significant improvements over existing solutions,
achieving up to 168% higher bitrate and 108% better quality of experience (QoE)
while maintaining comparable smoothness. Furthermore, ReSiN consistently
outperforms in stationary environments, demonstrating its robust adaptability
across different network conditions.

</details>


### [346] [EnsembleCI: Ensemble Learning for Carbon Intensity Forecasting](https://arxiv.org/pdf/2505.01959)
*Leyi Yan, Linda Wang, Sihang Liu, Yi Ding*

Main category: cs.LG

TL;DR: EnsembleCI, an adaptive ensemble learning method, outperforms CarbonCast in carbon intensity (CI) forecasting by addressing regional variability and improving accuracy by 19.58%.


<details>
  <summary>Details</summary>
Motivation: Existing methods like CarbonCast lack regional adaptability and accuracy in CI predictions, necessitating a more robust solution.

Method: EnsembleCI uses weighted predictions from multiple sublearners for flexible, region-specific CI forecasting.

Result: EnsembleCI achieves lower MAPE than CarbonCast in 11 grids, reducing variability and improving long-term robustness.

Conclusion: EnsembleCI is a more accurate, reliable, and interpretable solution for CI forecasting, with practical relevance.

Abstract: Carbon intensity (CI) measures the average carbon emissions generated per
unit of electricity, making it a crucial metric for quantifying and managing
the environmental impact. Accurate CI predictions are vital for minimizing
carbon footprints, yet the state-of-the-art method (CarbonCast) falls short due
to its inability to address regional variability and lack of adaptability. To
address these limitations, we introduce EnsembleCI, an adaptive, end-to-end
ensemble learning-based approach for CI forecasting. EnsembleCI combines
weighted predictions from multiple sublearners, offering enhanced flexibility
and regional adaptability. In evaluations across 11 regional grids, EnsembleCI
consistently surpasses CarbonCast, achieving the lowest mean absolute
percentage error (MAPE) in almost all grids and improving prediction accuracy
by an average of 19.58%. While performance still varies across grids due to
inherent regional diversity, EnsembleCI reduces variability and exhibits
greater robustness in long-term forecasting compared to CarbonCast and
identifies region-specific key features, underscoring its interpretability and
practical relevance. These findings position EnsembleCI as a more accurate and
reliable solution for CI forecasting. EnsembleCI source code and data used in
this paper are available at https://github.com/emmayly/EnsembleCI.

</details>


### [347] [Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks](https://arxiv.org/pdf/2505.02369)
*Juyoung Yun*

Main category: cs.LG

TL;DR: ZSharp improves SAM by using layer-wise Z-score normalization and percentile filtering to focus on significant gradient components, enhancing generalization without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often converge to sharp minima, harming robustness. SAM seeks flatter minima but includes insignificant gradient directions, which ZSharp addresses.

Method: ZSharp applies layer-wise Z-score normalization and percentile-based filtering to retain statistically significant gradient components, aligning updates with curvature-sensitive directions.

Result: ZSharp outperforms SAM and variants on CIFAR-10, CIFAR-100, and Tiny-ImageNet, especially with deeper and transformer-based models.

Conclusion: ZSharp is a lightweight, principled improvement for sharpness-aware optimization, enhancing generalization effectively.

Abstract: Generalizing well in deep neural networks remains a core challenge,
particularly due to their tendency to converge to sharp minima that degrade
robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking
flatter minima but perturbs parameters using the full gradient, which can
include statistically insignificant directions. We propose ZSharp, a simple yet
effective extension to SAM that applies layer-wise Z-score normalization
followed by percentile-based filtering to retain only statistically significant
gradient components. This selective perturbation aligns updates with
curvature-sensitive directions, enhancing generalization without requiring
architectural changes. ZSharp introduces only one additional hyperparameter,
the percentile threshold, and remains fully compatible with existing SAM
variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,
VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and
its variants in test accuracy, particularly on deeper and transformer-based
models. These results demonstrate that ZSharp is a principled and lightweight
improvement for sharpness-aware optimization.

</details>


### [348] [Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation](https://arxiv.org/pdf/2505.02138)
*Chenxi Liu, Hao Miao, Qianxiong Xu, Shaowen Zhou, Cheng Long, Yan Zhao, Ziyue Li, Rui Zhao*

Main category: cs.LG

TL;DR: TimeKD is an efficient MTSF framework using calibrated language models and knowledge distillation to improve forecasting accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of LLMs in MTSF by leveraging knowledge distillation and cross-modality learning.

Method: Uses a cross-modality teacher model with CLMs and ground truth prompts, SCA for representation refinement, and PKD for student model training.

Result: Demonstrates effectiveness, efficiency, and scalability in real-world experiments.

Conclusion: TimeKD offers a promising solution for efficient and accurate MTSF by combining CLMs and knowledge distillation.

Abstract: Multivariate time series forecasting (MTSF) endeavors to predict future
observations given historical data, playing a crucial role in time series data
management systems. With advancements in large language models (LLMs), recent
studies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.
However, the deployment of LLMs often suffers from low efficiency during the
inference phase. To address this problem, we introduce TimeKD, an efficient
MTSF framework that leverages the calibrated language models and privileged
knowledge distillation. TimeKD aims to generate high-quality future
representations from the proposed cross-modality teacher model and cultivate an
effective student model. The cross-modality teacher model adopts calibrated
language models (CLMs) with ground truth prompts, motivated by the paradigm of
Learning Under Privileged Information (LUPI). In addition, we design a
subtractive cross attention (SCA) mechanism to refine these representations. To
cultivate an effective student model, we propose an innovative privileged
knowledge distillation (PKD) mechanism including correlation and feature
distillation. PKD enables the student to replicate the teacher's behavior while
minimizing their output discrepancy. Extensive experiments on real data offer
insight into the effectiveness, efficiency, and scalability of the proposed
TimeKD.

</details>


### [349] [Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations](https://arxiv.org/pdf/2505.02537)
*Davide Sartor, Alberto Sinigaglia, Gian Antonio Susto*

Main category: cs.LG

TL;DR: The paper generalizes theoretical results on MLPs with non-negative weight constraints and alternating saturation activations, proving their universality for monotonic functions. It introduces an equivalence between activation saturation and weight sign, extends results to convex monotone activations with non-positive weights, and proposes a practical alternative to ease optimization.


<details>
  <summary>Details</summary>
Motivation: To address optimization challenges in conventional monotonic MLPs by generalizing theoretical foundations and proposing a simpler, more stable approach.

Method: Theoretical analysis of MLPs with non-negative weights and alternating saturation activations, extending to convex monotone activations with non-positive weights. A practical formulation adjusts activations based on weight signs.

Result: Proves universality for monotonic functions under new conditions and shows improved optimization stability with the proposed method.

Conclusion: The work provides theoretical grounding for empirical observations, simplifies architectures, and enhances training stability, validated by experiments.

Abstract: Conventional techniques for imposing monotonicity in MLPs by construction
involve the use of non-negative weight constraints and bounded activation
functions, which pose well-known optimization challenges. In this work, we
generalize previous theoretical results, showing that MLPs with non-negative
weight constraint and activations that saturate on alternating sides are
universal approximators for monotonic functions. Additionally, we show an
equivalence between the saturation side in the activations and the sign of the
weight constraint. This connection allows us to prove that MLPs with convex
monotone activations and non-positive constrained weights also qualify as
universal approximators, in contrast to their non-negative constrained
counterparts. Our results provide theoretical grounding to the empirical
effectiveness observed in previous works while leading to possible
architectural simplification. Moreover, to further alleviate the optimization
difficulties, we propose an alternative formulation that allows the network to
adjust its activations according to the sign of the weights. This eliminates
the requirement for weight reparameterization, easing initialization and
improving training stability. Experimental evaluation reinforces the validity
of the theoretical results, showing that our novel approach compares favourably
to traditional monotonic architectures.

</details>


### [350] [EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices](https://arxiv.org/pdf/2505.02380)
*Arnab Sanyal, Prithwish Mukherjee, Gourav Datta, Sandeep P. Chinchali*

Main category: cs.LG

TL;DR: EntroLLM is a compression framework for LLMs using mixed quantization and entropy coding, reducing storage and improving inference speed on edge devices without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: LLMs' large storage and computational demands limit their deployment on edge devices, necessitating efficient compression methods.

Method: Combines layer-wise mixed quantization (symmetric/asymmetric) with Huffman encoding for lossless compression and introduces parallel Huffman decoding for efficient inference.

Result: Achieves 30-65% storage reduction and 31.9-146.6% faster inference on edge devices while maintaining accuracy.

Conclusion: EntroLLM is a practical, training-free solution for deploying LLMs on edge devices with minimal latency impact.

Abstract: Large Language Models (LLMs) demonstrate exceptional performance across
various tasks, but their large storage and computational requirements constrain
their deployment on edge devices. To address this, we propose EntroLLM, a novel
compression framework that integrates mixed quantization with entropy coding to
reduce storage overhead while maintaining model accuracy. Our method applies a
layer-wise mixed quantization scheme - choosing between symmetric and
asymmetric quantization based on individual layer weight distributions - to
optimize compressibility. We then employ Huffman encoding for lossless
compression of the quantized weights, significantly reducing memory bandwidth
requirements. Furthermore, we introduce parallel Huffman decoding, which
enables efficient retrieval of encoded weights during inference, ensuring
minimal latency impact. Our experiments on edge-compatible LLMs, including
smolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,
demonstrate that EntroLLM achieves up to $30\%$ storage reduction compared to
uint8 models and up to $65%$ storage reduction compared to uint4 models, while
preserving perplexity and accuracy, on language benchmark tasks. We further
show that our method enables $31.9\%$ - $146.6\%$ faster inference throughput
on memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by
reducing the required data movement. The proposed approach requires no
additional re-training and is fully compatible with existing post-training
quantization methods, making it a practical solution for edge LLMs.

</details>


### [351] [A Note on Statistically Accurate Tabular Data Generation Using Large Language Models](https://arxiv.org/pdf/2505.02659)
*Andrey Sidorenko*

Main category: cs.LG

TL;DR: A probability-driven prompting method improves LLM-generated tabular data by better preserving feature dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for synthetic tabular data generation with LLMs fail to maintain complex feature dependencies, especially among categorical variables.

Method: Introduces a probability-driven prompting approach to estimate conditional distributions using LLMs.

Result: The method enhances the statistical fidelity of synthetic tabular data.

Conclusion: Prompting probability distributions with LLMs shows promise for accurate and scalable data synthesis.

Abstract: Large language models (LLMs) have shown promise in synthetic tabular data
generation, yet existing methods struggle to preserve complex feature
dependencies, particularly among categorical variables. This work introduces a
probability-driven prompting approach that leverages LLMs to estimate
conditional distributions, enabling more accurate and scalable data synthesis.
The results highlight the potential of prompting probability distributions to
enhance the statistical fidelity of LLM-generated tabular data.

</details>


### [352] [Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation](https://arxiv.org/pdf/2505.02737)
*Gerard Pons, Besim Bilalli, Anna Queralt*

Main category: cs.LG

TL;DR: The paper proposes using Knowledge Graphs (KGs) to enhance Large Language Models (LLMs) for zero-shot Entity Disambiguation (ED), addressing issues like hallucination and outdated knowledge.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges like hallucination and outdated knowledge, which are hard to fix via retraining. KGs offer structured external knowledge to mitigate these issues.

Method: Leverages hierarchical class representation and entity descriptions from KGs to prune candidate space and enrich input prompts for LLMs.

Result: Outperforms non-enhanced and description-only enhanced LLMs, showing higher adaptability than task-specific models.

Conclusion: KGs effectively enhance LLMs for ED, with performance influenced by the KG's semantic expressivity.

Abstract: Recent advances in Large Language Models (LLMs) have positioned them as a
prominent solution for Natural Language Processing tasks. Notably, they can
approach these problems in a zero or few-shot manner, thereby eliminating the
need for training or fine-tuning task-specific models. However, LLMs face some
challenges, including hallucination and the presence of outdated knowledge or
missing information from specific domains in the training data. These problems
cannot be easily solved by retraining the models with new data as it is a
time-consuming and expensive process. To mitigate these issues, Knowledge
Graphs (KGs) have been proposed as a structured external source of information
to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for
zero-shot Entity Disambiguation (ED). For that purpose, we leverage the
hierarchical representation of the entities' classes in a KG to gradually prune
the candidate space as well as the entities' descriptions to enrich the input
prompt with additional factual knowledge. Our evaluation on popular ED datasets
shows that the proposed method outperforms non-enhanced and description-only
enhanced LLMs, and has a higher degree of adaptability than task-specific
models. Furthermore, we conduct an error analysis and discuss the impact of the
leveraged KG's semantic expressivity on the ED performance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [353] [Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments](https://arxiv.org/pdf/2505.02861)
*Kushagra Agrawal, Nisharg Nargund*

Main category: cs.MA

TL;DR: MetaOrch is a neural orchestration framework for dynamic agent selection in multi-agent systems, outperforming baselines with 86.3% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional MAS architectures lack adaptability and flexible coordination.

Method: Supervised learning models task context, agent histories, and response quality, with a fuzzy evaluation module for soft supervision.

Result: Achieves 86.3% selection accuracy, surpassing random and round-robin baselines.

Conclusion: Neural orchestration enhances MAS autonomy, interpretability, and adaptability.

Abstract: Multi-agent systems (MAS) are foundational in simulating complex real-world
scenarios involving autonomous, interacting entities. However, traditional MAS
architectures often suffer from rigid coordination mechanisms and difficulty
adapting to dynamic tasks. We propose MetaOrch, a neural orchestration
framework for optimal agent selection in multi-domain task environments. Our
system implements a supervised learning approach that models task context,
agent histories, and expected response quality to select the most appropriate
agent for each task. A novel fuzzy evaluation module scores agent responses
along completeness, relevance, and confidence dimensions, generating soft
supervision labels for training the orchestrator. Unlike previous methods that
hard-code agent-task mappings, MetaOrch dynamically predicts the most suitable
agent while estimating selection confidence. Experiments in simulated
environments with heterogeneous agents demonstrate that our approach achieves
86.3% selection accuracy, significantly outperforming baseline strategies
including random selection and round-robin scheduling. The modular architecture
emphasizes extensibility, allowing agents to be registered, updated, and
queried independently. Results suggest that neural orchestration offers a
powerful approach to enhancing the autonomy, interpretability, and adaptability
of multi-agent systems across diverse task domains.

</details>


### [354] [Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering](https://arxiv.org/pdf/2505.03096)
*Joshua Owotogbe*

Main category: cs.MA

TL;DR: A chaos engineering framework is proposed to improve the robustness of LLM-MAS by identifying vulnerabilities and ensuring reliable performance.


<details>
  <summary>Details</summary>
Motivation: LLM-MAS can enhance various tasks but are prone to errors like hallucinations and communication failures in real-world conditions.

Method: Proposes a chaos engineering framework to test and build resilience in LLM-MAS.

Result: The framework aims to proactively identify and mitigate vulnerabilities.

Conclusion: Chaos engineering can enhance the reliability of LLM-MAS in critical applications.

Abstract: This study explores the application of chaos engineering to enhance the
robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in
production-like environments under real-world conditions. LLM-MAS can
potentially improve a wide range of tasks, from answering questions and
generating content to automating customer support and improving decision-making
processes. However, LLM-MAS in production or preproduction environments can be
vulnerable to emergent errors or disruptions, such as hallucinations, agent
failures, and agent communication failures. This study proposes a chaos
engineering framework to proactively identify such vulnerabilities in LLM-MAS,
assess and build resilience against them, and ensure reliable performance in
critical applications.

</details>


### [355] [Multi-Agent Deep Reinforcement Learning for Zonal Ancillary Market Coupling](https://arxiv.org/pdf/2505.03288)
*Francesco Morri, Hélène Le Cadre, Pierre Gruet, Luce Brotcorne*

Main category: cs.MA

TL;DR: The paper analyzes zonal ancillary market coupling using noncooperative game theory, comparing exact methods (optimization and best-response) with multi-agent deep reinforcement learning (MADRL). MADRL shows lower costs but higher profit variability, while stronger zone coupling reduces costs for larger zones.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize zonal ancillary market coupling using game theory, addressing equilibrium existence and computational methods for market equilibrium.

Method: Formulates the market as a multi-leader single-follower bilevel problem, cast as a generalized Nash game. Uses exact methods (optimization, Gauss-Seidel best-response) and MADRL for equilibrium computation.

Result: MADRL achieves lower market costs but higher profit variability. Exact methods are slower, with best-response being the slowest. Stronger zone coupling reduces costs for larger zones.

Conclusion: MADRL is promising for cost reduction but requires pretraining and trades off stability. Exact methods are reliable but slower. Zone coupling benefits larger zones economically.

Abstract: We characterize zonal ancillary market coupling relying on noncooperative
game theory. To that purpose, we formulate the ancillary market as a
multi-leader single follower bilevel problem, that we subsequently cast as a
generalized Nash game with side constraints and nonconvex feasibility sets. We
determine conditions for equilibrium existence and show that the game has a
generalized potential game structure. To compute market equilibrium, we rely on
two exact approaches: an integrated optimization approach and Gauss-Seidel
best-response, that we compare against multi-agent deep reinforcement learning.
On real data from Germany and Austria, simulations indicate that multi-agent
deep reinforcement learning achieves the smallest convergence rate but requires
pretraining, while best-response is the slowest. On the economics side,
multi-agent deep reinforcement learning results in smaller market costs
compared to the exact methods, but at the cost of higher variability in the
profit allocation among stakeholders. Further, stronger coupling between zones
tends to reduce costs for larger zones.

</details>


### [356] [Simulation to Reality: Testbeds and Architectures for Connected and Automated Vehicles](https://arxiv.org/pdf/2505.03472)
*David Klüner, Simon Schäfer, Lucas Hegerath, Jianye Xu, Julius Kahle, Hazem Ibrahim, Alexandru Kampmann, Bassam Alrifaee*

Main category: cs.MA

TL;DR: Survey of software frameworks for CAVs, analyzing real-time properties, transitioning capabilities, and tooling requirements, with case studies and research gaps.


<details>
  <summary>Details</summary>
Motivation: To ensure safe and efficient operation of CAVs by evaluating software frameworks for real-time properties, communication, and resource utilization.

Method: Survey and analysis of prominent software frameworks, focusing on real-time properties, transitioning capabilities, and tooling needs, supported by case studies.

Result: Identified opportunities, challenges, and research gaps in CAV software frameworks, emphasizing practical implications in perception, planning, and control.

Conclusion: Further research is needed to address gaps and advance the development of safe and efficient CAV systems.

Abstract: Ensuring the safe and efficient operation of CAVs relies heavily on the
software framework used. A software framework needs to ensure real-time
properties, reliable communication, and efficient resource utilization.
Furthermore, a software framework needs to enable seamless transition between
testing stages, from simulation to small-scale to full-scale experiments. In
this paper, we survey prominent software frameworks used for in-vehicle and
inter-vehicle communication in CAVs. We analyze these frameworks regarding
opportunities and challenges, such as their real-time properties and
transitioning capabilities. Additionally, we delve into the tooling
requirements necessary for addressing the associated challenges. We illustrate
the practical implications of these challenges through case studies focusing on
critical areas such as perception, motion planning, and control. Furthermore,
we identify research gaps in the field, highlighting areas where further
investigation is needed to advance the development and deployment of safe and
efficient CAV systems.

</details>


### [357] [Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation](https://arxiv.org/pdf/2505.03586)
*Songchen Fu, Siang Chen, Shaojing Zhao, Letian Bai, Ta Li, Yonghong Yan*

Main category: cs.MA

TL;DR: The paper introduces a framework (RDC) to address stochastic individual delays in multi-agent systems, improving MARL performance under delayed observations.


<details>
  <summary>Details</summary>
Motivation: Observation delays in MASs hinder agents' decision-making, necessitating a solution for delayed observations in MARL.

Method: Proposes DSID-POMDP to model delays and RDC framework with modules to compensate for delays, tested on MARL benchmarks.

Result: RDC mitigates performance degradation under delays, achieving near delay-free performance in some cases.

Conclusion: The work offers a novel solution for delayed observations in MASs, enhancing MARL robustness.

Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous,
preventing agents from making decisions based on the environment's true state.
An individual agent's local observation often consists of multiple components
from other agents or dynamic entities in the environment. These discrete
observation components with varying delay characteristics pose significant
challenges for multi-agent reinforcement learning (MARL). In this paper, we
first formulate the decentralized stochastic individual delay partially
observable Markov decision process (DSID-POMDP) by extending the standard
Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL
training framework for addressing stochastic individual delays, along with
recommended implementations for its constituent modules. We implement the
DSID-POMDP's observation generation pattern using standard MARL benchmarks,
including MPE and SMAC. Experiments demonstrate that baseline MARL methods
suffer severe performance degradation under fixed and unfixed delays. The
RDC-enhanced approach mitigates this issue, remarkably achieving ideal
delay-free performance in certain delay scenarios while maintaining
generalization capability. Our work provides a novel perspective on multi-agent
delayed observation problems and offers an effective solution framework.

</details>


### [358] [A Communication-First Account of Explanation](https://arxiv.org/pdf/2505.03732)
*Jacqueline Harding, Tobias Gerstenberg, Thomas Icard*

Main category: cs.MA

TL;DR: A formal account of causal explanation is developed, grounded in conversational pragmatics and interventionist ideas, showing the value of a communication-first approach.


<details>
  <summary>Details</summary>
Motivation: To address the communicative dimension of explanation and integrate cognitive science insights into philosophical work on explanation.

Method: Uses a theory of conversational pragmatics and interventionist ideas to analyze causal explanation.

Result: Widely recognized explanatory virtues emerge naturally, and norms' impact on causal judgments is clarified.

Conclusion: A communication-first approach provides a simple yet powerful framework for philosophical and cognitive science work on explanation.

Abstract: This paper develops a formal account of causal explanation, grounded in a
theory of conversational pragmatics, and inspired by the interventionist idea
that explanation is about asking and answering
what-if-things-had-been-different questions. We illustrate the fruitfulness of
the account, relative to previous accounts, by showing that widely recognised
explanatory virtues emerge naturally, as do subtle empirical patterns
concerning the impact of norms on causal judgments. This shows the value of a
communication-first approach to explanation: getting clear on explanation's
communicative dimension is an important prerequisite for philosophical work on
explanation. The result is a simple but powerful framework for incorporating
insights from the cognitive sciences into philosophical work on explanation,
which will be useful for philosophers or cognitive scientists interested in
explanation.

</details>


### [359] [ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent Diffusion Policies](https://arxiv.org/pdf/2502.18438)
*Pedro Sequeira, Vidyasagar Sadhu, Melinda Gervasio*

Main category: cs.MA

TL;DR: ToMCAT is a framework for generating ToM-conditioned trajectories using meta-learning and multiagent diffusion models, with dynamic replanning for adaptive teamwork.


<details>
  <summary>Details</summary>
Motivation: To improve cooperative agent teamwork by integrating Theory of Mind (ToM) reasoning and dynamic replanning for better resource usage and performance.

Method: Combines meta-learning for ToM reasoning with a multiagent denoising-diffusion model for plan generation, plus dynamic replanning based on real-time observations.

Result: Dynamic replanning reduces resource usage without compromising team performance; ToM inferences and recent observations are key for adaptive planning.

Conclusion: ToMCAT effectively enhances teamwork by leveraging ToM and dynamic replanning, especially in scenarios with unknown teammate characteristics.

Abstract: In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in
Teams), a new framework for generating ToM-conditioned trajectories. It
combines a meta-learning mechanism, that performs ToM reasoning over teammates'
underlying goals and future behavior, with a multiagent denoising-diffusion
model, that generates plans for an agent and its teammates conditioned on both
the agent's goals and its teammates' characteristics, as computed via ToM. We
implemented an online planning system that dynamically samples new trajectories
(replans) from the diffusion model whenever it detects a divergence between a
previously generated plan and the current state of the world. We conducted
several experiments using ToMCAT in a simulated cooking domain. Our results
highlight the importance of the dynamic replanning mechanism in reducing the
usage of resources without sacrificing team performance. We also show that
recent observations about the world and teammates' behavior collected by an
agent over the course of an episode combined with ToM inferences are crucial to
generate team-aware plans for dynamic adaptation to teammates, especially when
no prior information is provided about them.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [360] [Mitigating Image Captioning Hallucinations in Vision-Language Models](https://arxiv.org/pdf/2505.03420)
*Fei Zhao, Chengcui Zhang, Runlin Zhang, Tianyang Wang, Xi Li*

Main category: cs.MM

TL;DR: A reinforcement learning-based test-time adaptation framework reduces hallucinations in VLMs by updating minimal parameters, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in VLMs limit reliability, and current solutions are resource-intensive.

Method: Proposes a test-time adaptation framework using reinforcement learning, updating only layer normalization parameters (0.003% of model). A CLIP-based model provides dual rewards.

Result: Reduces hallucination rates by 15.4% (LLaVA) and 17.3% (InstructBLIP), with a 68.3% improvement over baselines.

Conclusion: The method effectively mitigates hallucinations without retraining or auxiliary models, enhancing VLM reliability.

Abstract: Hallucinations in vision-language models (VLMs) hinder reliability and
real-world applicability, usually stemming from distribution shifts between
pretraining data and test samples. Existing solutions, such as retraining or
fine-tuning on additional data, demand significant computational resources and
labor-intensive data collection, while ensemble-based methods incur additional
costs by introducing auxiliary VLMs. To address these challenges, we propose a
novel test-time adaptation framework using reinforcement learning to mitigate
hallucinations during inference without retraining or any auxiliary VLMs. By
updating only the learnable parameters in the layer normalization of the
language model (approximately 0.003% of the model parameters), our method
reduces distribution shifts between test samples and pretraining samples. A
CLIP-based hallucination evaluation model is proposed to provide dual rewards
to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in
hallucination rates on LLaVA and InstructBLIP, respectively. Our approach
outperforms state-of-the-art baselines with a 68.3% improvement in
hallucination mitigation, demonstrating its effectiveness.

</details>


### [361] [Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G](https://arxiv.org/pdf/2504.17938)
*Raza Ul Mustafa, Sesha Dassanayake, Noman Ashraf, Romana Aziz, Ala Saleh Alluhaidan*

Main category: cs.MM

TL;DR: The paper explores how channel metrics (RSRP, RSRQ, SNR) correlate with YouTube video quality shifts, proposing ML-based prediction to enhance QoE.


<details>
  <summary>Details</summary>
Motivation: Improve YouTube streaming QoE by understanding and predicting resolution shifts using channel metrics, moving beyond traditional QoS.

Method: Analyzes the relationship between quality shifts and channel metrics, using ML classifiers for prediction.

Result: Channel metrics positively correlate with shifts; ML models achieve 77% accuracy in predicting resolution categories.

Conclusion: Proposed method can enhance OTT services, especially in 5G networks, by leveraging channel metrics for better QoE.

Abstract: The Quality of Experience (QoE) is the users satisfaction while streaming a
video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube
reflects the smooth streaming session without any buffering and quality shift
events. One of the most important factors nowadays affecting QoE of YouTube is
frequent shifts from higher to lower resolutions and vice versa. These shifts
ensure a smooth streaming session; however, it might get a lower mean opinion
score. For instance, dropping from 1080p to 480p during a video can preserve
continuity but might reduce the viewers enjoyment. Over time, OTT platforms are
looking for alternative ways to boost user experience instead of relying on
traditional Quality of Service (QoS) metrics such as bandwidth, latency, and
throughput. As a result, we look into the relationship between quality shifting
in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our
findings state that these channel metrics positively correlate with shifts.
Thus, in real-time, OTT can only rely on them to predict video streaming
sessions into lower- and higher-resolution categories, thus providing more
resources to improve user experience. Using traditional Machine Learning (ML)
classifiers, we achieved an accuracy of 77-percent, while using only RSRP,
RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency
networks promise enhanced streaming capabilities, the proposed methodology can
be used to improve OTT services.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [362] [The Search for Squawk: Agile Modeling in Bioacoustics](https://arxiv.org/pdf/2505.03071)
*Vincent Dumoulin, Otilia Stretcu, Jenny Hamer, Lauren Harrell, Rob Laber, Hugo Larochelle, Bart van Merriënboer, Amanda Navine, Patrick Hart, Ben Williams, Timothy A. C. Lamont, Tries B. Rasak, Mars Coral Restoration Team, Sheryn Brodie, Brendan Doohan, Phil Eichinski, Paul Roe, Lin Schwarzkopf, Tom Denton*

Main category: eess.AS

TL;DR: A scalable, data-efficient system for bioacoustic recognizers enables ecologists to address new challenges quickly with minimal training data and expertise.


<details>
  <summary>Details</summary>
Motivation: Passive acoustic monitoring (PAM) aids ecosystem health assessment, but developing recognizers is challenging due to data and expertise requirements.

Method: The system uses pre-trained acoustic embeddings, indexed audio search for dataset creation, and precomputed embeddings for active learning.

Result: Applied in three case studies (coral reef health, juvenile Hawaiian birds, Christmas Island birds) and simulations, the system proved scalable and efficient.

Conclusion: The system's generalizability and efficiency empower scientists to tackle bioacoustic problems rapidly.

Abstract: Passive acoustic monitoring (PAM) has shown great promise in helping
ecologists understand the health of animal populations and ecosystems. However,
extracting insights from millions of hours of audio recordings requires the
development of specialized recognizers. This is typically a challenging task,
necessitating large amounts of training data and machine learning expertise. In
this work, we introduce a general, scalable and data-efficient system for
developing recognizers for novel bioacoustic problems in under an hour. Our
system consists of several key components that tackle problems in previous
bioacoustic workflows: 1) highly generalizable acoustic embeddings pre-trained
for birdsong classification minimize data hunger; 2) indexed audio search
allows the efficient creation of classifier training datasets, and 3)
precomputation of embeddings enables an efficient active learning loop,
improving classifier quality iteratively with minimal wait time. Ecologists
employed our system in three novel case studies: analyzing coral reef health
through unidentified sounds; identifying juvenile Hawaiian bird calls to
quantify breeding success and improve endangered species monitoring; and
Christmas Island bird occupancy modeling. We augment the case studies with
simulated experiments which explore the range of design decisions in a
structured way and help establish best practices. Altogether these experiments
showcase our system's scalability, efficiency, and generalizability, enabling
scientists to quickly address new bioacoustic challenges.

</details>


### [363] [Fairness of Automatic Speech Recognition in Cleft Lip and Palate Speech](https://arxiv.org/pdf/2505.03697)
*Susmita Bhattacharjee, Jagabandhu Mishra, H. S. Shekhawat, S. R. Mahadeva Prasanna*

Main category: eess.AS

TL;DR: ASR systems show reduced fairness for CLP speech, but augmenting CLP speech with normal speech improves performance and fairness.


<details>
  <summary>Details</summary>
Motivation: To address the reduced fairness of ASR systems for CLP speech due to structural anomalies affecting formant structure.

Method: Augment CLP speech with normal speech and test on three ASR models (GMM-HMM, Whisper, XLS-R) using AIISH and NMCPC datasets.

Result: WER improved (e.g., GMM-HMM: 22.64% to 18.76%; Whisper: 28.45% to 18.89%). Fairness scores increased by 17.89% (AIISH) and 47.50% (NMCPC).

Conclusion: Augmentation enhances ASR fairness for CLP speech, with GMM-HMM performing best for Kannada children's speech.

Abstract: Speech produced by individuals with cleft lip and palate (CLP) is often
highly nasalized and breathy due to structural anomalies, causing shifts in
formant structure that affect automatic speech recognition (ASR) performance
and fairness. This study hypothesizes that publicly available ASR systems
exhibit reduced fairness for CLP speech and confirms this through experiments.
Despite formant disruptions, mild and moderate CLP speech retains some
spectro-temporal alignment with normal speech, motivating augmentation
strategies to enhance fairness. The study systematically explores augmenting
CLP speech with normal speech across severity levels and evaluates its impact
on ASR fairness. Three ASR models-GMM-HMM, Whisper, and XLS-R-were tested on
AIISH and NMCPC datasets. Results indicate that training with normal speech and
testing on mixed data improves word error rate (WER). Notably, WER decreased
from $22.64\%$ to $18.76\%$ (GMM-HMM, AIISH) and $28.45\%$ to $18.89\%$
(Whisper, NMCPC). The superior performance of GMM-HMM on AIISH may be due to
its suitability for Kannada children's speech, a challenge for foundation
models like XLS-R and Whisper. To assess fairness, a fairness score was
introduced, revealing improvements of $17.89\%$ (AIISH) and $47.50\%$ (NMCPC)
with augmentation.

</details>


### [364] [Multi-channel Replay Speech Detection using an Adaptive Learnable Beamformer](https://arxiv.org/pdf/2502.13473)
*Michael Neri, Tuomas Virtanen*

Main category: eess.AS

TL;DR: M-ALRAD, a multi-channel neural network, detects replay attacks using spatial audio features, outperforming state-of-the-art methods on the ReMASC dataset.


<details>
  <summary>Details</summary>
Motivation: Replay attacks threaten voice-controlled systems by using recorded speech for unauthorized access, necessitating robust detection methods.

Method: M-ALRAD combines a learnable adaptive beamformer with a convolutional recurrent neural network for joint spatial filtering and classification.

Result: M-ALRAD outperforms existing methods on the ReMASC dataset, especially in challenging environments, and generalizes better to unseen conditions.

Conclusion: M-ALRAD is a superior solution for replay attack detection, offering improved performance and generalization.

Abstract: Replay attacks belong to the class of severe threats against voice-controlled
systems, exploiting the easy accessibility of speech signals by recorded and
replayed speech to grant unauthorized access to sensitive data. In this work,
we propose a multi-channel neural network architecture called M-ALRAD for the
detection of replay attacks based on spatial audio features. This approach
integrates a learnable adaptive beamformer with a convolutional recurrent
neural network, allowing for joint optimization of spatial filtering and
classification. Experiments have been carried out on the ReMASC dataset, which
is a state-of-the-art multi-channel replay speech detection dataset
encompassing four microphones with diverse array configurations and four
environments. Results on the ReMASC dataset show the superiority of the
approach compared to the state-of-the-art and yield substantial improvements
for challenging acoustic environments. In addition, we demonstrate that our
approach is able to better generalize to unseen environments with respect to
prior studies.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [365] [Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers](https://arxiv.org/pdf/2505.02843)
*Miriam Cobo, David Corral Fontecha, Wilson Silva, Lara Lloret Iglesias*

Main category: eess.IV

TL;DR: The paper reviews the role of physics in medical imaging AI, emphasizing its importance for enhancing trustworthiness and robustness, especially in data-limited scenarios.


<details>
  <summary>Details</summary>
Motivation: AI professionals often lack understanding of medical imaging physics, limiting their ability to leverage AI's full potential in the field.

Method: The work reviews fundamentals of medical imaging physics and its impact on AI, focusing on generative models and reconstruction algorithms.

Result: Integrating physics knowledge into AI improves model trustworthiness and robustness, particularly in data-scarce situations.

Conclusion: Physics-inspired machine learning models, incorporating physics-based constraints, enhance feature learning in medical imaging.

Abstract: Artificial intelligence in medical imaging has seen unprecedented growth in
the last years, due to rapid advances in deep learning and computing resources.
Applications cover the full range of existing medical imaging modalities, with
unique characteristics driven by the physics of each technique. Yet, artificial
intelligence professionals entering the field, and even experienced developers,
often lack a comprehensive understanding of the physical principles underlying
medical image acquisition, which hinders their ability to fully leverage its
potential. The integration of physics knowledge into artificial intelligence
algorithms enhances their trustworthiness and robustness in medical imaging,
especially in scenarios with limited data availability. In this work, we review
the fundamentals of physics in medical images and their impact on the latest
advances in artificial intelligence, particularly, in generative models and
reconstruction algorithms. Finally, we explore the integration of physics
knowledge into physics-inspired machine learning models, which leverage
physics-based constraints to enhance the learning of medical imaging features.

</details>


### [366] [STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis](https://arxiv.org/pdf/2505.03123)
*Yiran Zhu, Wei Yang, Yan su, Zesheng Li, Chengchang Pan, Honggang Qi*

Main category: eess.IV

TL;DR: A multimodal spatiotemporal graph neural network (STG) framework is proposed to predict colorectal cancer liver metastasis (CRLM) progression, outperforming existing methods by integrating spatial, temporal, and multimodal data.


<details>
  <summary>Details</summary>
Motivation: Current clinical models fail to effectively integrate spatial heterogeneity, dynamic evolution, and multimodal data, limiting predictive accuracy for CRLM progression.

Method: The STG framework combines CT imaging and clinical data into a heterogeneous graph, using GraphSAGE for spatiotemporal aggregation and supervised/contrastive learning for robustness. A lightweight version reduces parameters by 78.55%.

Result: Achieves 85% time-adjacent accuracy and 1.1005 mean absolute error on the MSKCC CRLM dataset, significantly outperforming existing methods.

Conclusion: The framework's innovative graph construction and spatiotemporal decoupling uncover tumor microenvironment-prognosis associations, aiding personalized treatment decisions.

Abstract: We propose a multimodal spatiotemporal graph neural network (STG) framework
to predict colorectal cancer liver metastasis (CRLM) progression. Current
clinical models do not effectively integrate the tumor's spatial heterogeneity,
dynamic evolution, and complex multimodal data relationships, limiting their
predictive accuracy. Our STG framework combines preoperative CT imaging and
clinical data into a heterogeneous graph structure, enabling joint modeling of
tumor distribution and temporal evolution through spatial topology and
cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal
neighborhood information and leverages supervised and contrastive learning
strategies to enhance the model's ability to capture temporal features and
improve robustness. A lightweight version of the model reduces parameter count
by 78.55%, maintaining near-state-of-the-art performance. The model jointly
optimizes recurrence risk regression and survival analysis tasks, with
contrastive loss improving feature representational discriminability and
cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a
time-adjacent accuracy of 85% and a mean absolute error of 1.1005,
significantly outperforming existing methods. The innovative heterogeneous
graph construction and spatiotemporal decoupling mechanism effectively uncover
the associations between dynamic tumor microenvironment changes and prognosis,
providing reliable quantitative support for personalized treatment decisions.

</details>


### [367] [Dual Prompting for Diverse Count-level PET Denoising](https://arxiv.org/pdf/2505.03037)
*Xiaofeng Liu, Yongsong Huang, Thibault Marin, Samira Vafay Eslahi, Tiss Amal, Yanis Chemli, Keith Johnson, Georges El Fakhri, Jinsong Ouyang*

Main category: eess.IV

TL;DR: The paper proposes a dual-prompt learning approach for denoising PET volumes with varying count levels, achieving better performance than count-conditional models.


<details>
  <summary>Details</summary>
Motivation: PET volumes have diverse count levels, making unified denoising challenging. The goal is to develop a generalizable model for varied cases.

Method: Uses dual prompts (explicit count-level and implicit general denoising) with a fusion module and prompt-feature interaction to guide denoising dynamically.

Result: Tested on 1940 low-count PET volumes, the dual-prompt method outperforms count-conditional models.

Conclusion: The dual-prompt approach effectively unifies denoising for diverse count levels, improving performance and generalizability.

Abstract: The to-be-denoised positron emission tomography (PET) volumes are inherent
with diverse count levels, which imposes challenges for a unified model to
tackle varied cases. In this work, we resort to the recently flourished prompt
learning to achieve generalizable PET denoising with different count levels.
Specifically, we propose dual prompts to guide the PET denoising in a
divide-and-conquer manner, i.e., an explicitly count-level prompt to provide
the specific prior information and an implicitly general denoising prompt to
encode the essential PET denoising knowledge. Then, a novel prompt fusion
module is developed to unify the heterogeneous prompts, followed by a
prompt-feature interaction module to inject prompts into the features. The
prompts are able to dynamically guide the noise-conditioned denoising process.
Therefore, we are able to efficiently train a unified denoising model for
various count levels, and deploy it to different cases with personalized
prompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly
selected 13-22\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies.
It shows our dual prompting can largely improve the performance with informed
count-level and outperform the count-conditional model.

</details>


### [368] [Stabilizing 3D EPI time series by servo navigation and phase equalization exploiting repeated shots (PEERS)](https://arxiv.org/pdf/2505.03637)
*Malte Riedel, Thomas Ulrich, Samuel Bianchi, Klaas P. Pruessmann*

Main category: eess.IV

TL;DR: The paper proposes servo navigation and PEERS for real-time motion and frequency correction in 3D EPI fMRI, improving data quality and tSNR.


<details>
  <summary>Details</summary>
Motivation: To address motion and frequency instability in 3D EPI fMRI scans, ensuring robust and high-quality imaging.

Method: Uses a 3D orbital navigator for motion estimation and PEERS for phase/frequency refinement, combining real-time geometry updates and data-driven corrections.

Result: Reduces motion artifacts, improves tSNR by 8%, and stabilizes frequency corrections, even for long echo times.

Conclusion: Servo navigation and PEERS offer a plug-and-play solution for effective motion and frequency correction in 3D fMRI without additional hardware.

Abstract: Purpose: To enable run-time head motion control and robust frequency
corrections for 3D EPI fMRI. Methods: A short 3D orbital navigator (3 ms) is
inserted into a 3D EPI sequence. A linear perturbation model is calibrated to
estimate rigid motion and frequency parameters per shot. Rigid motion is
corrected by scan geometry updates in run-time, while several techniques are
investigated to stabilize navigator-based frequency corrections in the
reconstruction. An additional method termed PEERS is proposed that exploits the
repetitive structure of fMRI scans to fine-tune shot-wise phase and frequency
estimates using the motion-corrected EPI data itself. Results: Servo navigation
effectively reduces motion in the raw data of in-vivo fMRI scans in six
subjects. PEERS provides high-precision frequency parameters for robust
phase-corrected reconstructions in the phantom and in-vivo accounting for
scanner drifts and slice encoding-related effects on EPI. In combination, servo
navigation and PEERS achieve successful intra-volume corrections and consistent
tSNR improvements of 8% on average throughout the brain. The two methods prove
to be highly synergetic. Conclusion: Servo navigation achieves high-precision
motion correction for 3D-EPI fMRI in run-time and, in synergy with PEERS,
provides stable frequency corrections with short navigators even for long echo
times. With its automatic self-calibration and no hardware requirements, servo
navigation and PEERS enable effective plug-and-play motion correction for 3D
fMRI.

</details>


### [369] [Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records](https://arxiv.org/pdf/2409.07012)
*Daeun Kyung, Junu Kim, Tackeun Kim, Edward Choi*

Main category: eess.IV

TL;DR: EHRXDiff is a novel framework using a latent diffusion model to predict future CXR images by integrating past CXRs and medical events, addressing the limitation of single-time-point generation in existing models.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for CXRs focus on single-time-point data, limiting their ability to capture temporal changes, which is crucial for clinical utility.

Method: EHRXDiff integrates previous CXR images and medical event histories into a latent diffusion model to predict future CXRs.

Result: The framework generates high-quality, realistic future CXRs that capture temporal changes, validated by clinical, demographic, and visual consistency.

Conclusion: EHRXDiff shows promise for clinical decision-making and patient monitoring by effectively predicting disease progression.

Abstract: Chest X-ray (CXR) is an important diagnostic tool widely used in hospitals to
assess patient conditions and monitor changes over time. Recently, generative
models, specifically diffusion-based models, have shown promise in generating
realistic synthetic CXRs. However, these models mainly focus on conditional
generation using single-time-point data, i.e., generating CXRs conditioned on
their corresponding reports from a specific time. This limits their clinical
utility, particularly for capturing temporal changes. To address this
limitation, we propose a novel framework, EHRXDiff, which predicts future CXR
images by integrating previous CXRs with subsequent medical events, e.g.,
prescriptions, lab measures, etc. Our framework dynamically tracks and predicts
disease progression based on a latent diffusion model, conditioned on the
previous CXR image and a history of medical events. We comprehensively evaluate
the performance of our framework across three key aspects, including clinical
consistency, demographic consistency, and visual realism. Results show that our
framework generates high-quality, realistic future images that effectively
capture potential temporal changes. This suggests that our framework could be
further developed to support clinical decision-making and provide valuable
insights for patient monitoring and treatment planning in the medical field.
The code is available at https://github.com/dek924/EHRXDiff.

</details>


### [370] [OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator for Exposure Correction](https://arxiv.org/pdf/2411.15255)
*Gehui Li, Bin Chen, Chen Zhao, Lei Zhang, Jian Zhang*

Main category: eess.IV

TL;DR: OSMamba is a novel exposure correction network combining state space models and generative diffusion models to improve performance in extreme exposure conditions by capturing long-range dependencies and restoring lost details.


<details>
  <summary>Details</summary>
Motivation: Existing frequency domain-based methods struggle with complex real-world scenarios due to limited receptive fields and non-generative learning, failing to restore severely degraded regions.

Method: OSMamba uses an omnidirectional spectral scanning mechanism to capture long-range dependencies in frequency domain features and a dual-domain prior generator for detail restoration via diffusion models.

Result: OSMamba achieves state-of-the-art performance on multiple- and mixed-exposure datasets, excelling in both quantitative and qualitative metrics.

Conclusion: OSMamba effectively addresses limitations in exposure correction by integrating state space and diffusion models, offering superior performance in challenging scenarios.

Abstract: Exposure correction is a fundamental problem in computer vision and image
processing. Recently, frequency domain-based methods have achieved impressive
improvement, yet they still struggle with complex real-world scenarios under
extreme exposure conditions. This is due to the local convolutional receptive
fields failing to model long-range dependencies in the spectrum, and the
non-generative learning paradigm being inadequate for retrieving lost details
from severely degraded regions. In this paper, we propose Omnidirectional
Spectral Mamba (OSMamba), a novel exposure correction network that incorporates
the advantages of state space models and generative diffusion models to address
these limitations. Specifically, OSMamba introduces an omnidirectional spectral
scanning mechanism that adapts Mamba to the frequency domain to capture
comprehensive long-range dependencies in both the amplitude and phase spectra
of deep image features, hence enhancing illumination correction and structure
recovery. Furthermore, we develop a dual-domain prior generator that learns
from well-exposed images to generate a degradation-free diffusion prior
containing correct information about severely under- and over-exposed regions
for better detail restoration. Extensive experiments on multiple-exposure and
mixed-exposure datasets demonstrate that the proposed OSMamba achieves
state-of-the-art performance both quantitatively and qualitatively.

</details>


### [371] [FedSynthCT-Brain: A Federated Learning Framework for Multi-Institutional Brain MRI-to-CT Synthesis](https://arxiv.org/pdf/2412.06690)
*Ciro Benito Raggio, Mathias Krohmer Zabaleta, Nils Skupien, Oliver Blanck, Francesco Cicone, Giuseppe Lucio Cascini, Paolo Zaffino, Lucia Migliorelli, Maria Francesca Spadea*

Main category: eess.IV

TL;DR: FedSynthCT-Brain uses federated learning for MRI-to-sCT in brain imaging, improving generalizability and privacy in multi-centre settings.


<details>
  <summary>Details</summary>
Motivation: Address limitations of single-centre deep learning models for MRI-to-sCT, ensuring privacy and generalizability in clinical settings.

Method: Cross-silo horizontal federated learning with a U-Net model, validated on multi-centre data from four centres.

Result: Achieved median MAE of 102.0 HU, SSIM of 0.89, and PSNR of 26.58 on unseen data.

Conclusion: Federated learning enhances MRI-to-sCT generalizability and privacy, supporting equitable clinical applications.

Abstract: The generation of Synthetic Computed Tomography (sCT) images has become a
pivotal methodology in modern clinical practice, particularly in the context of
Radiotherapy (RT) treatment planning. The use of sCT enables the calculation of
doses, pushing towards Magnetic Resonance Imaging (MRI) guided radiotherapy
treatments. Deep learning methods for MRI-to-sCT have shown promising results,
but their reliance on single-centre training dataset limits generalisation
capabilities to diverse clinical settings. Moreover, creating centralised
multi-centre datasets may pose privacy concerns. To address the aforementioned
issues, we introduced FedSynthCT-Brain, an approach based on the Federated
Learning (FL) paradigm for MRI-to-sCT in brain imaging. This is among the first
applications of FL for MRI-to-sCT, employing a cross-silo horizontal FL
approach that allows multiple centres to collaboratively train a U-Net-based
deep learning model. We validated our method using real multicentre data from
four European and American centres, simulating heterogeneous scanner types and
acquisition modalities, and tested its performance on an independent dataset
from a centre outside the federation. In the case of the unseen centre, the
federated model achieved a median Mean Absolute Error (MAE) of $102.0$ HU
across 23 patients, with an interquartile range of $96.7-110.5$ HU. The median
(interquartile range) for the Structural Similarity Index (SSIM) and the Peak
Signal to Noise Ratio (PNSR) were $0.89 (0.86-0.89)$ and $26.58 (25.52-27.42)$,
respectively. The analysis of the results showed acceptable performances of the
federated approach, thus highlighting the potential of FL to enhance MRI-to-sCT
to improve generalisability and advancing safe and equitable clinical
applications while fostering collaboration and preserving data privacy.

</details>


### [372] [Liver Cirrhosis Stage Estimation from MRI with Deep Learning](https://arxiv.org/pdf/2502.18225)
*Jun Zeng, Debesh Jha, Ertugrul Aktas, Elif Keles, Alpay Medetalibeyoglu, Matthew Antalek, Amir A. Borhani, Daniela P. Ladner, Gorkem Durak, Ulas Bagci*

Main category: eess.IV

TL;DR: An end-to-end deep learning framework for automated liver cirrhosis stage estimation from MRI, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Early cirrhosis diagnosis is challenging but crucial to prevent severe complications like decompensation and cancer.

Method: Integrates multi-scale feature learning with sequence-specific attention mechanisms using a large dataset (CirrMRI600+).

Result: Achieves 72.8% accuracy on T1W and 63.8% on T2W sequences, outperforming traditional methods.

Conclusion: Sets new benchmarks for automated cirrhosis staging and offers insights for clinical deep learning applications.

Abstract: We present an end-to-end deep learning framework for automated liver
cirrhosis stage estimation from multi-sequence MRI. Cirrhosis is the severe
scarring (fibrosis) of the liver and a common endpoint of various chronic liver
diseases. Early diagnosis is vital to prevent complications such as
decompensation and cancer, which significantly decreases life expectancy.
However, diagnosing cirrhosis in its early stages is challenging, and patients
often present with life-threatening complications. Our approach integrates
multi-scale feature learning with sequence-specific attention mechanisms to
capture subtle tissue variations across cirrhosis progression stages. Using
CirrMRI600+, a large-scale publicly available dataset of 628 high-resolution
MRI scans from 339 patients, we demonstrate state-of-the-art performance in
three-stage cirrhosis classification. Our best model achieves 72.8% accuracy on
T1W and 63.8% on T2W sequences, significantly outperforming traditional
radiomics-based approaches. Through extensive ablation studies, we show that
our architecture effectively learns stage-specific imaging biomarkers. We
establish new benchmarks for automated cirrhosis staging and provide insights
for developing clinically applicable deep learning systems. The source code
will be available at https://github.com/JunZengz/CirrhosisStage.

</details>


### [373] [Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases](https://arxiv.org/pdf/2504.07606)
*Andrés Bell-Navas, María Villalba-Orero, Enrique Lara-Pezzi, Jesús Garicano-Mena, Soledad Le Clainche*

Main category: eess.IV

TL;DR: An automatic system using deep learning predicts heart failure time from echocardiography videos, employing HODMD for data processing and a Vision Transformer for prediction, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Heart diseases cause 18 million annual deaths; early prediction of heart failure is critical for healthcare.

Method: Two-stage system: 1) HODMD for data augmentation/feature extraction from echocardiography videos, 2) Vision Transformer trained with self-supervised learning for heart failure time prediction.

Result: HODMD and Vision Transformer outperform established ViT and CNN architectures.

Conclusion: The system is effective for heart failure prediction and will be integrated into ModelFLOWs-app.

Abstract: Heart diseases constitute the main cause of international human defunction.
According to the World Health Organization (WHO), approximately 18 million
deaths happen each year due to precisely heart diseases. In particular, heart
failures (HF) press the healthcare industry to develop systems for their early,
rapid, and effective prediction. This work presents an automatic system based
on a novel deep learning framework which analyses in real-time echocardiography
video sequences for the challenging and more specific task of heart failure
time prediction. This system works in two stages. The first one transforms the
data from a database of echocardiography video sequences into a machine
learning-compatible collection of annotated images which can be used in the
training phase of any machine learning-based framework, including a deep
learning-based one. This stage includes the use of the Higher Order Dynamic
Mode Decomposition (HODMD) algorithm for both data augmentation and feature
extraction. The second stage builds and trains a Vision Transformer (ViT).
Self-supervised learning (SSL) methods, so far barely explored in the
literature about heart failure prediction, are adopted to effectively train the
ViT from scratch, even with scarce databases. The designed neural network
analyses images from echocardiography sequences to estimate the time in which a
heart failure will happen. The results obtained show the efficacy of the HODMD
algorithm and the superiority of the proposed system with respect to several
established ViT and Convolutional Neural Network (CNN) architectures. The
source code will be incorporated into the next version release of the
ModelFLOWs-app software (https://github.com/modelflows/ModelFLOWs-app).

</details>


### [374] [Light Weight CNN for classification of Brain Tumors from MRI Images](https://arxiv.org/pdf/2504.21188)
*Natnael Alemayehu*

Main category: eess.IV

TL;DR: A lightweight CNN model for multi-class brain tumor classification from MRI scans achieves 98.78% accuracy using preprocessing, hyperparameter tuning, and cross-validation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient deep learning model for accurate and automated classification of brain tumor types to aid clinical diagnosis.

Method: Utilizes a CNN with preprocessing (normalization, augmentation, cropping), hyperparameter tuning via Keras Tuner, and 5-fold cross-validation.

Result: Achieves 98.78% classification accuracy on a dataset with four tumor classes.

Conclusion: The model is a low-complexity, effective solution for early brain tumor diagnosis in clinical settings.

Abstract: This study presents a convolutional neural network (CNN)-based approach for
the multi-class classification of brain tumors using magnetic resonance imaging
(MRI) scans. We utilize a publicly available dataset containing MRI images
categorized into four classes: glioma, meningioma, pituitary tumor, and no
tumor. Our primary objective is to build a light weight deep learning model
that can automatically classify brain tumor types with high accuracy. To
achieve this goal, we incorporate image preprocessing steps, including
normalization, data augmentation, and a cropping technique designed to reduce
background noise and emphasize relevant regions. The CNN architecture is
optimized through hyperparameter tuning using Keras Tuner, enabling systematic
exploration of network parameters. To ensure reliable evaluation, we apply
5-fold cross-validation, where each hyperparameter configuration is evaluated
across multiple data splits to mitigate overfitting. Experimental results
demonstrate that the proposed model achieves a classification accuracy of
98.78%, indicating its potential as a diagnostic aid in clinical settings. The
proposed method offers a low-complexity yet effective solution for assisting in
early brain tumor diagnosis.

</details>


### [375] [Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images](https://arxiv.org/pdf/2505.01884)
*Siddharth Kothari, Srinivasan Murali, Sankalp Kothari, Ujjwal Verma, Jaya Sreevalsan-Nair*

Main category: eess.IV

TL;DR: The paper investigates the robustness of U-Net for inland water body segmentation in SAR images against manual annotation errors, simulated as adversarial attacks. It finds U-Net tolerates some corruption before performance drops, emphasizing annotation quality's importance.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of SAR images for water segmentation is error-prone due to complex geometry and adversarial attacks, impacting model performance.

Method: Simulates manual annotation errors as adversarial attacks on U-Net and evaluates its robustness.

Result: U-Net can withstand a certain level of annotation corruption before significant performance decline.

Conclusion: Annotation quality is critical for segmentation model effectiveness; the study provides a dataset and adversarial examples for robust training.

Abstract: Inland water body segmentation from Synthetic Aperture Radar (SAR) images is
an important task needed for several applications, such as flood mapping. While
SAR sensors capture data in all-weather conditions as high-resolution images,
differentiating water and water-like surfaces from SAR images is not
straightforward. Inland water bodies, such as large river basins, have complex
geometry, which adds to the challenge of segmentation. U-Net is a widely used
deep learning model for land-water segmentation of SAR images. In practice,
manual annotation is often used to generate the corresponding water masks as
ground truth. Manual annotation of the images is prone to label noise owing to
data poisoning attacks, especially due to complex geometry. In this work, we
simulate manual errors in the form of adversarial attacks on the U-Net model
and study the robustness of the model to human errors in annotation. Our
results indicate that U-Net can tolerate a certain level of corruption before
its performance drops significantly. This finding highlights the crucial role
that the quality of manual annotations plays in determining the effectiveness
of the segmentation model. The code and the new dataset, along with adversarial
examples for robust training, are publicly available. (GitHub link -
https://github.com/GVCL/IWSeg-SAR-Poison.git)

</details>


### [376] [Regression is all you need for medical image translation](https://arxiv.org/pdf/2505.02048)
*Sebastian Rassmann, David Kügler, Christian Ewert, Martin Reuter*

Main category: eess.IV

TL;DR: YODA, a 2.5D diffusion-based framework, combines diffusion and regression for medical image translation, outperforming GANs and DMs while challenging their presumed advantages.


<details>
  <summary>Details</summary>
Motivation: Enhancing medical datasets with synthetic images requires accurate anatomical information, which GANs and DMs often fail to provide due to noise or hallucination.

Method: YODA integrates diffusion and regression paradigms, using ExpA-sampling to suppress noise and improve image quality.

Result: YODA outperforms GANs and DMs, generating images interchangeable or superior to physical acquisitions for downstream tasks.

Conclusion: YODA challenges DM advantages in MIT, enabling practical medical imaging applications.

Abstract: The acquisition of information-rich images within a limited time budget is
crucial in medical imaging. Medical image translation (MIT) can help enhance
and supplement existing datasets by generating synthetic images from acquired
data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have
achieved remarkable success in natural image generation, their benefits -
creativity and image realism - do not necessarily transfer to medical
applications where highly accurate anatomical information is required. In fact,
the imitation of acquisition noise or content hallucination hinder clinical
utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel
2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and
regression paradigms to produce realistic or noise-free outputs. Furthermore,
we propose Expectation-Approximation (ExpA) DM sampling, which draws
inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise
and, thus, eliminates noise from biasing the evaluation of image quality.
Through extensive experiments on four diverse multi-modal datasets - comprising
multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and
regression sampling yield similar results in practice. As such, the
computational overhead of diffusion sampling does not provide systematic
benefits in medical information translation. Building on these insights, we
demonstrate that YODA outperforms several state-of-the-art GAN and DM methods.
Notably, YODA-generated images are shown to be interchangeable with, or even
superior to, physical acquisitions for several downstream tasks. Our findings
challenge the presumed advantages of DMs in MIT and pave the way for the
practical application of MIT in medical imaging.

</details>
