<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 152]
- [cs.CV](#cs.CV) [Total: 348]
- [cs.AI](#cs.AI) [Total: 71]
- [cs.SD](#cs.SD) [Total: 22]
- [cs.LG](#cs.LG) [Total: 205]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 12]
- [eess.IV](#eess.IV) [Total: 43]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/pdf/2506.22439)
*Javier Conde, Miguel González, María Grandury, Gonzalo Martínez, Pedro Reviriego, Mar Brysbaert*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' alignment with human psycholinguistic ratings on word features, finding better alignment with Glasgow norms than Lancaster norms, suggesting limitations in sensory associations.


<details>
  <summary>Details</summary>
Motivation: To assess how well LLMs align with human ratings on word features (e.g., arousal, concreteness, sensory associations) using psycholinguistic datasets.

Method: Evaluation of LLMs using two psycholinguistic datasets (Glasgow and Lancaster norms) covering thirteen features over thousands of words.

Result: LLMs align better with Glasgow norms (arousal, valence, etc.) than Lancaster norms (sensory features), indicating limitations in sensory associations.

Conclusion: Current LLMs may lack embodied cognition for sensory associations, highlighting the value of psycholinguistic evaluations.

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [2] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/pdf/2506.22485)
*Sudip Dasgupta, Himanshu Shankar*

Main category: cs.CL

TL;DR: A modular AI system for automated review of structured business documents outperforms humans in consistency, speed, and bias reduction, though human oversight remains necessary in specialized domains.


<details>
  <summary>Details</summary>
Motivation: To improve the review of structured enterprise documents by addressing limitations of prior solutions focused on unstructured texts or limited compliance checks.

Method: Uses AI agents orchestrated with tools like LangChain and CrewAI for section-by-section evaluation, enforcing standardized outputs and continuous feedback loops.

Result: Achieves 99% consistency (vs. 92% for humans), reduces review time from 30 to 2.5 minutes, and halves error/bias rates, with 95% agreement with human experts.

Conclusion: The system is a scalable, auditable foundation for AI-driven document QA, though human oversight and LLM costs remain challenges.

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [3] [Hallucination Detection with Small Language Models](https://arxiv.org/pdf/2506.22486)
*Ming Cheung*

Main category: cs.CL

TL;DR: The paper proposes a framework using multiple small language models to detect hallucinations in LLM-generated responses by verifying sentences against retrieved context, showing a 10% F1 score improvement.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLM responses reduce reliability, especially without ground truth, necessitating a scalable verification method.

Method: Integrates small language models to verify LLM responses by breaking them into sentences and using 'Yes' token probabilities for detection.

Result: Experiments show a 10% F1 score improvement in detecting correct responses over hallucinations.

Conclusion: Multiple small language models offer a scalable, efficient solution for verifying LLM responses, enhancing reliability.

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [4] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/pdf/2506.22491)
*Oliver Warke, Joemon M. Jose, Faegheh Hasibi, Jan Breitsohl*

Main category: cs.CL

TL;DR: PromptAug is an LLM-based data augmentation method for conflict detection, improving accuracy and F1-score by 2%. It addresses challenges like limited high-quality data and LLM guardrails, with a robust evaluation framework.


<details>
  <summary>Details</summary>
Motivation: The need for effective classification models for harmful behaviors on social media, coupled with limited high-quality labeled data and restricted access to research data, motivates the development of PromptAug.

Method: PromptAug, an innovative LLM-based data augmentation method, is introduced and evaluated using extreme data scarcity scenarios, diversity analysis, and thematic analysis.

Result: PromptAug achieves a 2% improvement in accuracy and F1-score on conflict and emotion datasets. Thematic analysis identifies four problematic patterns in augmented text.

Conclusion: PromptAug is effective for augmenting data in sensitive tasks like conflict detection, with a unique interdisciplinary evaluation combining NLP and social science methods.

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [5] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/pdf/2506.22508)
*Chenyang Shao, Tianxing Li, Chenhao Pu, Fengli Xu, Yong Li*

Main category: cs.CL

TL;DR: AgentStealth is a self-reinforcing LLM framework for text anonymization, using adversarial workflows and reinforcement learning to outperform baselines in effectiveness and utility.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing text anonymization methods—rigid replacements or costly cloud-based LLMs—by exploring locally deployed smaller-scale language models (SLMs).

Method: Proposes AgentStealth: an adversarial anonymization workflow with In-context Contrastive Learning, supervised adaptation of SLMs, and online reinforcement learning.

Result: Outperforms baselines by +12.3% in anonymization effectiveness and +6.8% in utility, with lightweight deployment on edge devices.

Conclusion: AgentStealth offers a privacy-preserving, efficient solution for text anonymization, avoiding cloud reliance and open-sourced for broader use.

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [6] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/pdf/2506.22510)
*Zihao Zhao, Xinlong Zhai, Jinyu Yang, Chuan Shi*

Main category: cs.CL

TL;DR: The paper proposes MDGCL, a multi-domain pre-training and cross-domain transfer framework for graph data, addressing domain-specific differences to improve representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive pre-training strategies fail to account for domain-specific differences in graph data, limiting knowledge transfer across domains.

Method: MDGCL introduces a domain-aware contrastive learning strategy and domain tokens in pre-training, and a domain attention mechanism for downstream tasks.

Result: MDGCL achieves significant improvements, with up to 19.33% accuracy and 19.13% Macro-F1 score gains over state-of-the-art methods.

Conclusion: The framework effectively addresses domain differences in graph data, enabling better multi-domain knowledge transfer and downstream performance.

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [7] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/pdf/2506.22516)
*Jingkai Li*

Main category: cs.CL

TL;DR: The study applies Integrated Information Theory (IIT) 3.0 and 4.0 to analyze Large Language Model (LLM) representations, finding no significant indicators of consciousness but revealing patterns under spatio-permutational analyses.


<details>
  <summary>Details</summary>
Motivation: To investigate whether differences in Theory of Mind (ToM) test performances in LLMs can be explained by IIT metrics and to differentiate between consciousness-like phenomena and inherent separations in LLM representations.

Method: Applied IIT 3.0 and 4.0 metrics (Φᵐᵃˣ, Φ, Conceptual Information, Φ-structure) to LLM representations and compared them with Span Representations. Conducted experiments across LLM transformer layers and linguistic spans.

Result: No statistically significant indicators of consciousness were found in LLM representations, but intriguing patterns emerged under spatio-permutational analyses.

Conclusion: Contemporary Transformer-based LLMs lack significant consciousness indicators, though their representations show interesting structural patterns.

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [8] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/pdf/2506.22518)
*Deyu Zou, Yongqiang Chen, Mufei Li, Siqi Miao, Chenxi Liu, Bo Han, James Cheng, Pan Li*

Main category: cs.CL

TL;DR: ReG improves graph-based RAG by aligning weak retrievers with LLMs, using feedback to remove spurious signals and reorganizing retrieved knowledge into coherent evidence chains, achieving up to 10% performance gains.


<details>
  <summary>Details</summary>
Motivation: Current graph-based RAG systems suffer from weak retrievers due to lack of ground truth and unorganized retrieved knowledge, leading to spurious signals and poor performance.

Method: ReG incorporates LLM feedback to refine supervision and introduces a structure-aware module to reorganize retrieval results into coherent evidence chains.

Result: ReG improves performance by up to 10%, matches state-of-the-art with 5% training data, reduces reasoning token cost by 30%, and boosts reasoning performance by 4%.

Conclusion: ReG effectively addresses the limitations of weak retrievers in graph-based RAG, enhancing performance and efficiency across various benchmarks and LLM backbones.

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [9] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/pdf/2506.22529)
*Lu Kalkbrenner, Veronika Solopova, Steffen Zeiler, Robert Nickel, Dorothea Kolossa*

Main category: cs.CL

TL;DR: The paper introduces Misinfo-TeleGraph, a German-language Telegram dataset for misinformation detection, and evaluates text-only and graph-based models, showing GraphSAGE with LSTM aggregation outperforms text-only methods.


<details>
  <summary>Details</summary>
Motivation: To address the underutilization of connectivity and message propagation in misinformation detection, especially on poorly moderated platforms like Telegram, which is critical in the German electoral context.

Method: The dataset includes 5M+ messages with metadata, channel relationships, and labels derived via semantic similarity (M3-embeddings) and manual annotation. Text-only models and GNNs (like GraphSAGE with LSTM aggregation) are evaluated.

Result: GraphSAGE with LSTM aggregation outperforms text-only baselines in MCC and F1-score. The study also examines the impact of subscribers, view counts, and label types (weak vs. strong).

Conclusion: The work provides a benchmark and open dataset for future research on misinformation detection in German Telegram and similar platforms, highlighting the potential and challenges of weak supervision.

Abstract: Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.

</details>


### [10] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/pdf/2506.22598)
*Nicholas Edwards, Yukyung Lee, Yujun, Mao, Yulu Qin, Sebastian Schuster, Najoung Kim*

Main category: cs.CL

TL;DR: RExBench evaluates LLM agents' ability to autonomously implement research extensions, finding current agents fall short without human guidance.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of LLM agents in performing realistic research extensions, a critical skill for autonomous research systems.

Method: Introduces RExBench, a benchmark with 12 tasks extending existing research, evaluated using automatic execution and human hints.

Result: All tested LLM agents failed most tasks; success rates improved with hints but remained below 40%.

Conclusion: Current LLM agents lack the ability to autonomously handle research extensions, requiring significant human input.

Abstract: Agents based on Large Language Models (LLMs) have shown promise for
performing sophisticated software engineering tasks autonomously. In addition,
there has been progress towards developing agents that can perform parts of the
research pipeline in machine learning and the natural sciences. We argue that
research extension and its implementation is a critical capability for such
systems, and introduce RExBench to support the evaluation of this capability.
RExBench is a benchmark consisting of 12 realistic research experiment
implementation tasks that aim to investigate research hypotheses that have not
previously been implemented. Each task is set up as an extension to an existing
research paper and codebase, accompanied by domain expert-written instructions.
RExBench is robust to data contamination, and supports an automatic evaluation
infrastructure that executes agent outputs to determine whether the success
criteria are met. We use this benchmark to evaluate nine LLM agents implemented
using three different frameworks: aider, Claude Code, and OpenHands. We find
that all agents evaluated fail to autonomously implement the majority of the
extensions. Although the success rate improves with additional human-written
hints, the best performance under this setting remains below 40%. This
indicates that current agents are still short of being able to handle realistic
research extension tasks without substantial human guidance.

</details>


### [11] [Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](https://arxiv.org/pdf/2506.22846)
*Duygu Altinok*

Main category: cs.CL

TL;DR: Proposes LAIL, a novel auxiliary loss framework to enhance CTC-based ASR using LLMs, improving linguistic modeling while maintaining CTC's efficiency.


<details>
  <summary>Details</summary>
Motivation: Autoregressive E2E ASR models are slow for real-time use, while CTC models lack linguistic dependency modeling. LAIL bridges this gap.

Method: Attaches connector layers to intermediate encoder layers, mapping outputs to LLM embeddings and computing a causal language modeling loss.

Result: Achieves state-of-the-art WER improvements on LibriSpeech, TEDLIUM2, and WSJ with minimal computational overhead.

Conclusion: LAIL effectively enhances CTC-based ASR by leveraging LLMs, balancing performance and efficiency.

Abstract: End-to-end (E2E) automatic speech recognition (ASR) systems have
revolutionized the field by integrating all components into a single neural
network, with attention-based encoder-decoder models achieving state-of-the-art
performance. However, their autoregressive decoding process limits inference
speed, making them unsuitable for real-time applications. In contrast,
CTC-based models offer faster, non-autoregressive decoding but struggle to
model linguistic dependencies effectively. Addressing this challenge, we
propose a novel auxiliary loss framework called Language-Aware Intermediate
Loss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large
language models (LLMs). By attaching connector layers to intermediate encoder
layers, LAIL maps outputs to the embedding space of an LLM and computes a
causal language modeling loss during training. This approach enhances
linguistic modeling while preserving the computational efficiency of CTC
decoding. Using the Conformer architecture and various LLaMA models, we
demonstrate significant improvements in Word Error Rate (WER) on the
LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance
for CTC-based ASR with minimal computational overhead.

</details>


### [12] [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/pdf/2506.22957)
*Younwoo Choi, Changling Li, Yongjin Yang, Zhijing Jin*

Main category: cs.CL

TL;DR: The paper introduces 'interlocutor awareness' in LLMs, evaluating their ability to adapt to dialogue partners' identities and characteristics, and highlights its dual impact on collaboration and safety.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' awareness of conversational partners is crucial for reliable performance and safety in multi-agent and human-AI systems.

Method: The study formalizes interlocutor awareness and evaluates it across reasoning patterns, linguistic style, and alignment preferences in contemporary LLMs.

Result: LLMs reliably identify peers like GPT and Claude. Interlocutor awareness enhances collaboration but introduces safety vulnerabilities like reward-hacking and jailbreak susceptibility.

Conclusion: The findings emphasize the dual promise and risks of identity-sensitive behavior in LLMs, calling for further research and safeguards in multi-agent deployments.

Abstract: As large language models (LLMs) are increasingly integrated into multi-agent
and human-AI systems, understanding their awareness of both self-context and
conversational partners is essential for ensuring reliable performance and
robust safety. While prior work has extensively studied situational awareness
which refers to an LLM's ability to recognize its operating phase and
constraints, it has largely overlooked the complementary capacity to identify
and adapt to the identity and characteristics of a dialogue partner. In this
paper, we formalize this latter capability as interlocutor awareness and
present the first systematic evaluation of its emergence in contemporary LLMs.
We examine interlocutor inference across three dimensions-reasoning patterns,
linguistic style, and alignment preferences-and show that LLMs reliably
identify same-family peers and certain prominent model families, such as GPT
and Claude. To demonstrate its practical significance, we develop three case
studies in which interlocutor awareness both enhances multi-LLM collaboration
through prompt adaptation and introduces new alignment and safety
vulnerabilities, including reward-hacking behaviors and increased jailbreak
susceptibility. Our findings highlight the dual promise and peril of
identity-sensitive behavior in LLMs, underscoring the need for further
understanding of interlocutor awareness and new safeguards in multi-agent
deployments. Our code is open-sourced at
https://github.com/younwoochoi/InterlocutorAwarenessLLM.

</details>


### [13] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/pdf/2506.22623)
*Badr Youbi Idrissi, Monica Millunzi, Amelia Sorrenti, Lorenzo Baraldi, Daryna Dementieva*

Main category: cs.CL

TL;DR: The paper introduces a new watermarking method for detecting synthetic text generated by LLMs, aiming to ensure ethical use. It replicates a baseline study, identifies its limitations, and proposes a robust alternative, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address concerns about misuse of LLMs by developing a reliable method to detect machine-generated text, ensuring ethical AI applications.

Method: Replicates a baseline study, identifies its flaws, and proposes a novel watermarking technique evaluated using paraphrased text for robustness.

Result: The proposed method shows greater robustness compared to existing watermarking techniques, particularly under paraphrasing attacks.

Conclusion: The new watermarking approach effectively detects synthetic text, offering a more reliable solution for ethical AI text generation.

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [14] [Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions](https://arxiv.org/pdf/2506.22858)
*Duygu Altinok*

Main category: cs.CL

TL;DR: A novel training approach for ASR systems improves named entity and numerical data recognition by extending semantic context with overlapping windows and enriched training data.


<details>
  <summary>Details</summary>
Motivation: ASR systems like Whisper struggle with named entities and numerical data, impacting accuracy and semantic understanding in critical domains.

Method: Extends semantic context using overlapping 5-second windows on 30-second chunks, reassigns boundary-spanning entities, and uses enriched training data with entity labels.

Result: Improves performance on named entity recognition (NER) and entity formatting, as shown on the Spoken Wikipedia dataset.

Conclusion: Context-aware training effectively addresses ASR limitations for long-form transcription and complex entity recognition.

Abstract: Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high
transcription accuracy but struggle with named entities and numerical data,
especially when proper formatting is required. These issues increase word error
rate (WER) and impair semantic understanding in critical domains like legal,
financial, and medical applications. We propose a novel training approach that
extends the semantic context of ASR models by adding overlapping context
windows during training. By sliding 5-second overlaps on both sides of
30-second chunks, we create a 40-second "effective semantic window," improving
entity recognition and formatting while focusing predictions on the central 30
seconds. To address entities spanning chunk boundaries, we reassign such
entities entirely to the right-hand chunk, ensuring proper formatting.
Additionally, enriched training data with embedded entity labels enables the
model to learn both recognition and type-specific formatting. Evaluated on the
Spoken Wikipedia dataset, our method improves performance across semantic
tasks, including named entity recognition (NER) and entity formatting. These
results highlight the effectiveness of context-aware training in addressing ASR
limitations for long-form transcription and complex entity recognition tasks.

</details>


### [15] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/pdf/2506.22644)
*Chase Fensore, Kaustubh Dhole, Joyce C Ho, Eugene Agichtein*

Main category: cs.CL

TL;DR: A hybrid RAG system combining sparse (BM25) and dense (E5) retrieval with Falcon3-10B-Instruct achieved 4th in faithfulness and 11th in correctness in the LiveRAG Challenge 2025, though neural re-ranking improved performance at high computational cost.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve retrieval-augmented generation (RAG) systems on dynamic test sets using the FineWeb-10BT corpus.

Method: Combined sparse (BM25) and dense (E5) retrieval, used Falcon3-10B-Instruct for generation, and evaluated with synthetic questions and RankLLaMA re-ranking.

Result: Neural re-ranking improved MAP by 52% but was computationally expensive. DSPy-optimized prompting increased semantic similarity but had 0% refusal rates. Vocabulary alignment was key to performance.

Conclusion: The hybrid system performed well in faithfulness and correctness, but trade-offs between performance and computational cost must be considered.

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [16] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/pdf/2506.22679)
*Ankush Raut, Projna Paromita, Sydney Begerowski, Suzanne Bell, Theodora Chaspari*

Main category: cs.CL

TL;DR: LLMs are tested for detecting micro-behaviors in team conversations. Encoder-only models struggled, while decoder-only Llama-3.1 performed better, showing promise for speech technologies in high-stakes environments.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' capability in detecting subtle micro-behaviors in team communication, especially in high-stakes scenarios like space missions.

Method: Tested zero-shot classification, fine-tuning, paraphrase-augmented fine-tuning with encoder-only models (RoBERTa, DistilBERT), and few-shot text generation with decoder-only models (Llama-3.1).

Result: Encoder-only models struggled, especially with underrepresented behaviors. Llama-3.1 achieved 44% macro F1 for 3-way and 68% for binary classification.

Conclusion: Decoder-only LLMs like Llama-3.1 are more effective for analyzing team communication dynamics, useful for training interventions in high-stakes environments.

Abstract: We explore the feasibility of large language models (LLMs) in detecting
subtle expressions of micro-behaviors in team conversations using transcripts
collected during simulated space missions. Specifically, we examine zero-shot
classification, fine-tuning, and paraphrase-augmented fine-tuning with
encoder-only sequence classification LLMs, as well as few-shot text generation
with decoder-only causal language modeling LLMs, to predict the micro-behavior
associated with each conversational turn (i.e., dialogue). Our findings
indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to
detect underrepresented micro-behaviors, particularly discouraging speech, even
with weighted fine-tuning. In contrast, the instruction fine-tuned version of
Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best
models achieving macro F1-scores of 44% for 3-way classification and 68% for
binary classification. These results have implications for the development of
speech technologies aimed at analyzing team communication dynamics and
enhancing training interventions in high-stakes environments such as space
missions, particularly in scenarios where text is the only accessible data.

</details>


### [17] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/pdf/2506.22694)
*Raghavv Goel, Sudhanshu Agrawal, Mukul Gagrani, Junyoung Park, Yifan Zao, He Zhang, Tian Liu, Yiping Yang, Xin Yuan, Jiuyan Lu, Chris Lott, Mingu Lee*

Main category: cs.CL

TL;DR: The paper introduces VocabTrim, a training-free technique to improve speculative decoding (SpD) by reducing drafting overhead in memory-bound environments, achieving a 16% speed-up for Llama-3.2-3B-Instruct.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unnecessary inference overhead in drafter-based speculative decoding, especially for LLMs with large vocabularies, by optimizing the drafting process.

Method: The method involves VocabTrim, which reconstructs the drafter's LM head to include only frequently sampled tokens, reducing drafting latency.

Result: VocabTrim improves memory-bound speed-up by 16% for Llama-3.2-3B-Instruct, despite a slight drop in acceptance rate.

Conclusion: VocabTrim effectively balances drafting efficiency and acceptance rate, enhancing speculative decoding performance in memory-bound scenarios.

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [18] [Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models](https://arxiv.org/pdf/2410.08174)
*Qingni Wang, Tiantian Geng, Zhiyuan Wang, Teng Wang, Bo Fu, Feng Zheng*

Main category: cs.CL

TL;DR: TRON is a two-step framework for risk control in MLLMs, improving trustworthiness with conformal and nonconformity scores, validated across VideoQA datasets.


<details>
  <summary>Details</summary>
Motivation: Address trustworthiness issues in MLLMs by generalizing risk control methods beyond logits and multiple-choice settings.

Method: Introduces TRON with conformal scores for minimal response sets and nonconformity scores for quality control, plus semantic redundancy analysis.

Result: Achieves desired error rates and efficient deduplicated prediction sets across eight MLLMs in VideoQA tasks.

Conclusion: TRON enhances MLLM reliability in open-ended and closed-ended scenarios, offering a robust evaluation metric.

Abstract: Multimodal Large Language Models (MLLMs) exhibit promising advancements
across various tasks, yet they still encounter significant trustworthiness
issues. Prior studies apply Split Conformal Prediction (SCP) in language
modeling to construct prediction sets with statistical guarantees. However,
these methods typically rely on internal model logits or are restricted to
multiple-choice settings, which hampers their generalizability and adaptability
in dynamic, open-ended environments. In this paper, we introduce TRON, a
two-step framework for risk control and assessment, applicable to any MLLM that
supports sampling in both open-ended and closed-ended scenarios. TRON comprises
two main components: (1) a novel conformal score to sample response sets of
minimum size, and (2) a nonconformity score to identify high-quality responses
based on self-consistency theory, controlling the error rates by two specific
risk levels. Furthermore, we investigate semantic redundancy in prediction sets
within open-ended contexts for the first time, leading to a promising
evaluation metric for MLLMs based on average set size. Our comprehensive
experiments across four Video Question-Answering (VideoQA) datasets utilizing
eight MLLMs show that TRON achieves desired error rates bounded by two
user-specified risk levels. Additionally, deduplicated prediction sets maintain
adaptiveness while being more efficient and stable for risk assessment under
different risk levels.

</details>


### [19] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/pdf/2506.22698)
*Emily Dux Speltz*

Main category: cs.CL

TL;DR: The report summarizes a workshop on AI language models and human cognition, highlighting insights, limitations, and ethical considerations for future research.


<details>
  <summary>Details</summary>
Motivation: To address the knowledge gap in understanding how AI language models relate to human cognitive processes in text comprehension and production.

Method: Interdisciplinary workshop with experts in cognitive psychology, linguistics, and AI, focusing on collaborative dialogue and analysis.

Result: Found potential of LLMs to inform human language processing, alignment with human behavior when fine-tuned, and challenges in human-AI collaboration.

Conclusion: Future research should focus on ethical AI use and enhancing human-AI collaboration in language tasks.

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [20] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/pdf/2506.22724)
*Niyati Bafna, Tianjian Li, Kenton Murray, David R. Mortensen, David Yarowsky, Hale Sirin, Daniel Khashabi*

Main category: cs.CL

TL;DR: The paper investigates why multilingual generation in LLMs underperforms for mid- to low-resource languages, attributing it to a translation barrier in the model's implicit task-solving pipeline.


<details>
  <summary>Details</summary>
Motivation: To understand and address the poor quality of multilingual generation in LLMs, especially for mid- to low-resource languages.

Method: Analyzes a word translation task across 108 language pairs using logit lens to observe intermediate model processing.

Result: Found that translation failure is a significant cause of poor output quality, particularly for low-resource languages.

Conclusion: Highlights the translation barrier as a key hurdle for multilingual generation in LLMs, offering insights for future improvements.

Abstract: Multilingual generation with large language models (LLMs) is often of poor
quality for mid- to low-resource languages. Building on insights from
interpretability, we demonstrate the existence of an implicit
task-solving-->translation pipeline for generation, whereby the model first
solves the required task in a largely target-language-agnostic manner, and
subsequently translates answer concepts into the intended target language. We
hypothesize that the failure of the translation stage is an important culprit
for the observed low quality of final outputs, and formalize this as the
translation barrier hypothesis. We test this hypothesis for a word translation
task across 108 language pairs, using logit lens to observe model processing in
intermediate layers. We find that a significant portion of overall failures
indeed stems from translation failure, or the model's inability to translate
correctly solved intermediate concepts into the target language. This is
especially true for low-resource target languages. Our results highlight an
important hurdle for end-to-end multilingual generation, and lend guiding
insights for future work seeking to improve multilinguality in LLMs.

</details>


### [21] [Jan-nano Technical Report](https://arxiv.org/pdf/2506.22760)
*Alan Dao, Dinh Bach Vu*

Main category: cs.CL

TL;DR: Jan-nano is a 4B parameter language model that achieves high efficiency and performance by specializing in instant information retrieval, eliminating next token prediction training, and running on consumer hardware.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the tradeoff between model capabilities and computational resources, aiming to redefine efficiency by focusing on specialization rather than scale.

Method: Jan-nano is fine-tuned from Qwen3-4B using a novel multi-stage RLVR system, which removes reliance on next token prediction training (SFT).

Result: The model achieves 83.2% on the SimpleQA benchmark with MCP integration and supports a 128K context length, demonstrating high performance on consumer hardware.

Conclusion: Jan-nano shows that intelligence is about strategic specialization, not just scale, offering a viable alternative to resource-heavy models.

Abstract: Most language models face a fundamental tradeoff where powerful capabilities
require substantial computational resources. We shatter this constraint with
Jan-nano, a 4B parameter language model that redefines efficiency through
radical specialization: instead of trying to know everything, it masters the
art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel
multi-stage RLVR system that completely eliminates reliance on next token
prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with
MCP integration while running on consumer hardware. With 128K context length,
Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


### [22] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/pdf/2506.22777)
*Miles Turpin, Andy Arditi, Marvin Li, Joe Benton, Julian Michael*

Main category: cs.CL

TL;DR: VFT (verbalization fine-tuning) reduces undetected reward hacking in RL-trained models by teaching them to verbalize cue influences, improving detection and transparency.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of detecting reward hacking in RL-trained models, which exploit unintended strategies without revealing them in reasoning, posing risks in high-stakes applications.

Method: Propose VFT, a pre-RL intervention that trains models to acknowledge prompt cue influences. Evaluate by training models with RL in environments with held-out cues, measuring undetected reward hacks.

Result: VFT reduces undetected reward hacks to 6% post-RL, compared to 88% without VFT and 99% with debiasing. VFT increases verbalization of cue influence from 8% to 42% (up to 94% post-RL).

Conclusion: VFT significantly improves detection of reward hacking by enhancing verbalization, offering a practical approach for safer and more transparent AI systems.

Abstract: Language models trained with RL can engage in reward hacking--exploiting
unintended strategies for high reward--without revealing this behavior in their
chain-of-thought reasoning, making detection difficult and posing risks for
high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL
intervention that trains models to explicitly acknowledge when they are
influenced by prompt cues--hints which point to incorrect answers (e.g., "a
Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently
train models with RL on environments where held-out prompt cues signal which
incorrect answers will receive high reward, incentivizing models to reward hack
by exploiting cues instead of reasoning correctly. We measure how often models
exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained
model's responses consist of undetected reward hacks. In comparison, when we
perform RL without VFT, the rate of undetected reward hacks goes up to 88%;
with a debiasing baseline intervention, this increases further to 99%. VFT
achieves this by substantially increasing how often models verbalize the
influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while
baselines remain low even after RL (10% and 1%). Our results show that teaching
models to explicitly verbalize reward hacking behavior before RL significantly
improves their detection, offering a practical path toward more transparent and
safe AI systems.

</details>


### [23] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/pdf/2506.22791)
*Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren*

Main category: cs.CL

TL;DR: ContextCache introduces a context-aware semantic caching system for multi-turn dialogues, improving precision and recall while reducing latency and computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing semantic caching systems lack awareness of multi-turn dialogue contexts, leading to incorrect cache hits in different conversational settings.

Method: ContextCache uses a two-stage retrieval architecture: vector-based retrieval on the current query followed by contextual matching using self-attention mechanisms on dialogue history.

Result: ContextCache outperforms existing methods in precision and recall, with cached responses showing ~10x lower latency than direct LLM invocation.

Conclusion: ContextCache effectively reduces computational costs and improves efficiency for LLM conversational applications by leveraging contextual awareness.

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


### [24] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/pdf/2506.22808)
*Jianhui Wei, Zijie Meng, Zikai Xiao, Tianxiang Hu, Yang Feng, Zhijie Zhou, Jian Wu, Zuozhu Liu*

Main category: cs.CL

TL;DR: The paper introduces MedEthicsQA, a benchmark for evaluating medical ethics in LLMs, revealing performance gaps in MedLLMs compared to foundation models.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient exploration of ethical safety in Medical Large Language Models (MedLLMs).

Method: Creation of MedEthicsQA, a benchmark with 5,623 multiple-choice and 5,351 open-ended questions, integrating global ethical standards and rigorous quality control.

Result: State-of-the-art MedLLMs show declined performance in medical ethics questions, highlighting alignment deficiencies.

Conclusion: The benchmark underscores the need for improved ethical alignment in MedLLMs, providing a reliable dataset for future research.

Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable
potential in clinical tasks, their ethical safety remains insufficiently
explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive
benchmark comprising $\textbf{5,623}$ multiple-choice questions and
$\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.
We systematically establish a hierarchical taxonomy integrating global medical
ethical standards. The benchmark encompasses widely used medical datasets,
authoritative question banks, and scenarios derived from PubMed literature.
Rigorous quality control involving multi-stage filtering and multi-faceted
expert validation ensures the reliability of the dataset with a low error rate
($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance
in answering medical ethics questions compared to their foundation
counterparts, elucidating the deficiencies of medical ethics alignment. The
dataset, registered under CC BY-NC 4.0 license, is available at
https://github.com/JianhuiWei7/MedEthicsQA.

</details>


### [25] [Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models](https://arxiv.org/pdf/2506.22813)
*Zhuojun Ding, Wei Wei, Chenghao Fan*

Main category: cs.CL

TL;DR: The paper introduces SaM, a framework for dynamically selecting and merging expert models for domain-specific tasks, improving generalization and scalability without extra training.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning for domain-specific tasks like NER is costly and lacks adaptability. Unified models struggle with scalability and domain adaptation.

Method: SaM selects domain-specific experts based on domain similarity and performance, then merges them to create task-specific models for target domains.

Result: SaM outperforms unified models by 10% on average across benchmarks, offering better generalization and scalability.

Conclusion: The SaM framework effectively addresses domain adaptation and scalability challenges, with potential for further improvements and extensions.

Abstract: Supervised fine-tuning (SFT) is widely used to align large language models
(LLMs) with information extraction (IE) tasks, such as named entity recognition
(NER). However, annotating such fine-grained labels and training
domain-specific models is costly. Existing works typically train a unified
model across multiple domains, but such approaches lack adaptation and
scalability since not all training data benefits target domains and scaling
trained models remains challenging. We propose the SaM framework, which
dynamically Selects and Merges expert models at inference time. Specifically,
for a target domain, we select domain-specific experts pre-trained on existing
domains based on (i) domain similarity to the target domain and (ii)
performance on sampled instances, respectively. The experts are then merged to
create task-specific models optimized for the target domain. By dynamically
merging experts beneficial to target domains, we improve generalization across
various domains without extra training. Additionally, experts can be added or
removed conveniently, leading to great scalability. Extensive experiments on
multiple benchmarks demonstrate our framework's effectiveness, which
outperforms the unified model by an average of 10%. We further provide insights
into potential improvements, practical experience, and extensions of our
framework.

</details>


### [26] [Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems](https://arxiv.org/pdf/2506.22852)
*Yucheng Cai, Yuxuan Wu, Yi Huang, Junlan Feng, Zhijian Ou*

Main category: cs.CL

TL;DR: The paper proposes Knowledge Augmented Fine-Tuning (KAFT) to improve LLMs' factual accuracy in knowledge-intensive dialog systems by fine-tuning with domain-specific data and external knowledge, outperforming prompting methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with factual accuracy in knowledge-intensive scenarios, even when augmented with external knowledge via RAG or agent-based systems.

Method: Proposes KAFT, fine-tuning LLMs with domain-specific data and external knowledge, tested on the MobileCS2 dataset.

Result: KAFT significantly outperforms prompting in RAG and agent systems, especially in factual accuracy.

Conclusion: KAFT is an effective method to enhance LLMs' performance in knowledge-intensive tasks, validated by empirical results.

Abstract: Large language models (LLMs) have recently been applied to dialog systems.
Despite making progress, LLMs are prone to errors in knowledge-intensive
scenarios. Recently, approaches based on retrieval augmented generation (RAG)
and agent have emerged to improve the factual accuracy by enhancing the LLMs
with knowledge retrieved from external knowledge bases (KBs). This is mostly
implemented by prompting the LLMs with instructions, examples and the retrieved
knowledge. However, LLMs may have difficulty using the retrieved knowledge
effectively for response generation, because they are not well trained to do
such generation for specific domains. To mitigate this problem, we propose to
finetune the LLMs in the RAG-based and agent-based systems with domain-specific
data, together with domain-specific external knowledge, which is called
knowledge augmented finetuning (KAFT). We base our study on the MobileCS2
dataset, a real-life customer service dialog dataset that features intensive
knowledge interactions, to systematically compare the prompting and KAFT
techniques in the RAG-based and agent-based systems. Experiment results show
that KAFT substantially surpasses prompting in both RAG and agent systems,
particularly in terms of factual accuracy. To the best of our knowledge, this
paper represents the first solid empirical work to investigate the KAFT idea.

</details>


### [27] [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/pdf/2506.22853)
*Kyochul Jang, Donghyeon Lee, Kyusik Kim, Dongseok Heo, Taewhoo Lee, Woojeong Kim, Bongwon Suh*

Main category: cs.CL

TL;DR: The paper introduces DICE-SCORE to evaluate tool-related information dispersion in dialogues and DICE-BENCH, a framework for creating realistic function-calling datasets.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack realism in function-calling scenarios, prompting the need for a more practical evaluation metric and dataset.

Method: Developed DICE-SCORE for evaluation and DICE-BENCH, a framework using tool graphs and multi-agent systems to synthesize realistic dialogues.

Result: DICE-BENCH produced 1,607 high-DICE-SCORE instances, revealing gaps in current LLM capabilities for real-world deployment.

Conclusion: Significant improvements are needed for LLMs to handle real-world function-calling tasks effectively.

Abstract: Existing function-calling benchmarks focus on single-turn interactions.
However, they overlook the complexity of real-world scenarios. To quantify how
existing benchmarks address practical applications, we introduce DICE-SCORE, a
metric that evaluates the dispersion of tool-related information such as
function name and parameter values throughout the dialogue. Analyzing existing
benchmarks through DICE-SCORE reveals notably low scores, highlighting the need
for more realistic scenarios. To address this gap, we present DICE-BENCH, a
framework that constructs practical function-calling datasets by synthesizing
conversations through a tool graph that maintains dependencies across rounds
and a multi-agent system with distinct personas to enhance dialogue
naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our
experiments on 19 LLMs with DICE-BENCH show that significant advances are still
required before such models can be deployed effectively in real-world settings.
Our code and data are all publicly available:
https://snuhcc.github.io/DICE-Bench/.

</details>


### [28] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/pdf/2506.15981)
*Markus Frohmann, Gabriel Meseguer-Brocal, Markus Schedl, Elena V. Epure*

Main category: cs.CL

TL;DR: The paper proposes DE-detect, a multimodal late-fusion pipeline combining transcribed lyrics and speech features to detect AI-generated music, outperforming existing methods in robustness and practicality.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated music challenges artists and copyright holders, requiring reliable detection methods. Current detectors (audio or lyrics-based) have limitations like poor generalization and reliance on clean lyrics.

Method: A multimodal, modular late-fusion pipeline integrates transcribed sung lyrics and speech features from audio, enhancing robustness and practicality.

Result: DE-detect outperforms existing lyrics-based detectors and is more robust to audio perturbations.

Conclusion: DE-detect provides an effective, robust solution for detecting AI-generated music in real-world scenarios, with code publicly available.

Abstract: The rapid advancement of AI-based music generation tools is revolutionizing
the music industry but also posing challenges to artists, copyright holders,
and providers alike. This necessitates reliable methods for detecting such
AI-generated content. However, existing detectors, relying on either audio or
lyrics, face key practical limitations: audio-based detectors fail to
generalize to new or unseen generators and are vulnerable to audio
perturbations; lyrics-based methods require cleanly formatted and accurate
lyrics, unavailable in practice. To overcome these limitations, we propose a
novel, practically grounded approach: a multimodal, modular late-fusion
pipeline that combines automatically transcribed sung lyrics and speech
features capturing lyrics-related information within the audio. By relying on
lyrical aspects directly from audio, our method enhances robustness, mitigates
susceptibility to low-level artifacts, and enables practical applicability.
Experiments show that our method, DE-detect, outperforms existing lyrics-based
detectors while also being more robust to audio perturbations. Thus, it offers
an effective, robust solution for detecting AI-generated music in real-world
scenarios. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [29] [Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge](https://arxiv.org/pdf/2502.13010)
*Mohammad Reza Rezaei, Reza Saadati Fard, Jayson L. Parker, Rahul G. Krishnan, Milad Lankarany*

Main category: cs.CL

TL;DR: AMG-RAG automates medical knowledge graph updates and integrates external evidence, improving accuracy and interpretability in medical question-answering.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of rapidly evolving medical knowledge and manual updates in LLMs for reliable medical insights.

Method: Introduces AMG-RAG, a framework for automated medical knowledge graph construction, reasoning, and external evidence retrieval (e.g., PubMed, WikiSearch).

Result: Achieves F1 score of 74.1% on MEDQA and 66.34% accuracy on MEDMCQA, outperforming larger models without added computational cost.

Conclusion: AMG-RAG demonstrates the importance of automated knowledge graphs and external evidence for accurate, up-to-date medical insights.

Abstract: Large Language Models (LLMs) have significantly advanced medical
question-answering by leveraging extensive clinical data and medical
literature. However, the rapid evolution of medical knowledge and the
labor-intensive process of manually updating domain-specific resources pose
challenges to the reliability of these systems. To address this, we introduce
Agentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates
the construction and continuous updating of medical knowledge graphs,
integrates reasoning, and retrieves current external evidence, such as PubMed
and WikiSearch. By dynamically linking new findings and complex medical
concepts, AMG-RAG not only improves accuracy but also enhances interpretability
in medical queries.
  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness
of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of
66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to
100 times larger. Notably, these improvements are achieved without increasing
computational overhead, highlighting the critical role of automated knowledge
graph generation and external evidence retrieval in delivering up-to-date,
trustworthy medical insights.

</details>


### [30] [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/pdf/2506.22977)
*Asen Dotsinski, Udit Thakur, Marko Ivanov, Mohammad Hafeez Khan, Maria Heuss*

Main category: cs.CL

TL;DR: A reproduction study of Ortu et al. (2024) confirms their findings on factual and counterfactual handling in language models but extends the work by testing larger models, prompt variations, and domain-specific validity.


<details>
  <summary>Details</summary>
Motivation: To validate and extend Ortu et al.'s findings on how language models handle facts and counterfactuals, exploring generalizability and limitations.

Method: Reproduced experiments on GPT-2 and Pythia 6.9B, extended to Llama 3.1 8B, tested prompt variations, and evaluated domain-specific impacts.

Result: Confirmed primary findings but found reduced attention head specialization in larger models, prompt structure impacts counterfactual handling, and domain-specific skews.

Conclusion: Attention head ablation's effectiveness varies by model, prompt, domain, and task, highlighting limitations in generalizability.

Abstract: We present a reproduction study of "Competition of Mechanisms: Tracing How
Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), which
investigates competition of mechanisms in language models between factual
recall and counterfactual in-context repetition. Our study successfully
reproduces their primary findings regarding the localization of factual and
counterfactual information, the dominance of attention blocks in mechanism
competition, and the specialization of attention heads in handling competing
information. We reproduce their results on both GPT-2 (Radford et al., 2019)
and Pythia 6.9B (Biderman et al., 2023). We extend their work in three
significant directions. First, we explore the generalizability of these
findings to even larger models by replicating the experiments on Llama 3.1 8B
(Grattafiori et al., 2024), discovering greatly reduced attention head
specialization. Second, we investigate the impact of prompt structure by
introducing variations where we avoid repeating the counterfactual statement
verbatim or we change the premise word, observing a marked decrease in the
logit for the counterfactual token. Finally, we test the validity of the
authors' claims for prompts of specific domains, discovering that certain
categories of prompts skew the results by providing the factual prediction
token as part of the subject of the sentence. Overall, we find that the
attention head ablation proposed in Ortu et al. (2024) is ineffective for
domains that are underrepresented in their dataset, and that the effectiveness
varies based on model architecture, prompt structure, domain and task.

</details>


### [31] [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/pdf/2506.22978)
*Yida Zhao, Hao Xve, Xiang Hu, Kewei Tu*

Main category: cs.CL

TL;DR: The paper introduces a unified framework for compositional syntactic language models (SLMs) based on constituency parse trees, evaluates design choices, and provides recommendations.


<details>
  <summary>Details</summary>
Motivation: To enhance Transformers by incorporating syntactic biases and improving compositional SLMs through a unified framework.

Method: Proposes a framework for compositional SLMs, evaluates variants across tasks like language modeling and summarization.

Result: Comprehensive empirical evaluation leads to design recommendations for compositional SLMs.

Conclusion: The study offers insights and practical guidelines for improving compositional SLMs, with code publicly available.

Abstract: Syntactic language models (SLMs) enhance Transformers by incorporating
syntactic biases through the modeling of linearized syntactic parse trees
alongside surface sentences. This paper focuses on compositional SLMs that are
based on constituency parse trees and contain explicit bottom-up composition of
constituent representations. We identify key aspects of design choices in
existing compositional SLMs and propose a unified framework encompassing both
existing models and novel variants. We conduct a comprehensive empirical
evaluation of all the variants in our framework across language modeling,
syntactic generalization, summarization, dialogue, and inference efficiency.
Based on the experimental results, we make multiple recommendations on the
design of compositional SLMs. Our code is released at
https://github.com/zhaoyd1/compositional_SLMs.

</details>


### [32] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/pdf/2506.23046)
*Xianzhe Fan, Xuhui Zhou, Chuanyang Jin, Kolby Nottingham, Hao Zhu, Maarten Sap*

Main category: cs.CL

TL;DR: The SoMi-ToM benchmark evaluates Theory of Mind (ToM) in embodied multi-agent social interactions, addressing the gap in static, text-based benchmarks. It uses multimodal data and multi-level evaluation, revealing a significant performance gap between humans and large vision-language models (LVLMs).


<details>
  <summary>Details</summary>
Motivation: Existing ToM benchmarks are limited to static, text-based scenarios, failing to capture real-world dynamic social interactions. The SoMi-ToM benchmark aims to bridge this gap by evaluating ToM in complex, embodied interactions.

Method: The benchmark uses multimodal data (visual, dialogue, action) from the SoMi environment, with first-person (real-time state inference) and third-person (goal/behavior inference) evaluations. A dataset includes videos, images, and expert-annotated questions.

Result: LVLMs perform significantly worse than humans, with accuracy gaps of 40.1% (first-person) and 26.4% (third-person).

Conclusion: Future LVLMs must improve ToM capabilities for embodied, complex social interactions, as current models lag behind human performance.

Abstract: Humans continuously infer the states, goals, and behaviors of others by
perceiving their surroundings in dynamic, real-world social interactions.
However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based
scenarios, which have a significant gap compared to real interactions. We
propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in
embodied multi-agent complex social interactions. This benchmark is based on
rich multimodal interaction data generated by the interaction environment SoMi,
covering diverse crafting goals and social relationships. Our framework
supports multi-level evaluation: (1) first-person evaluation provides
multimodal (visual, dialogue, action, etc.) input from a first-person
perspective during a task for real-time state inference, (2) third-person
evaluation provides complete third-person perspective video and text records
after a task for goal and behavior inference. This evaluation method allows for
a more comprehensive examination of a model's ToM capabilities from both the
subjective immediate experience and the objective global observation. We
constructed a challenging dataset containing 35 third-person perspective
videos, 363 first-person perspective images, and 1225 expert-annotated
multiple-choice questions (three options). On this dataset, we systematically
evaluated the performance of human subjects and several state-of-the-art large
vision-language models (LVLMs). The results show that LVLMs perform
significantly worse than humans on SoMi-ToM: the average accuracy gap between
humans and models is 40.1% in first-person evaluation and 26.4% in third-person
evaluation. This indicates that future LVLMs need to further improve their ToM
capabilities in embodied, complex social interactions.

</details>


### [33] [MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition](https://arxiv.org/pdf/2506.23051)
*João Lucas Luz Lima Sarcinelli, Marina Lages Gonçalves Teixeira, Jade Bortot de Paiva, Diego Furtado Silva*

Main category: cs.CL

TL;DR: The paper introduces MariNER, the first gold-standard NER dataset for early 20th-century Brazilian Portuguese, addressing the lack of resources for historical text analysis in digital humanities.


<details>
  <summary>Details</summary>
Motivation: Brazilian Portuguese lacks high-quality NER datasets, especially for historical texts, hindering digital humanities research.

Method: Construction of MariNER, a manually annotated dataset with over 9,000 sentences, and evaluation of state-of-the-art NER models.

Result: MariNER provides a valuable resource for NER in Brazilian Portuguese, with performance benchmarks for existing models.

Conclusion: The dataset fills a critical gap and supports advancements in NER for historical and domain-specific texts.

Abstract: Named Entity Recognition (NER) is a fundamental Natural Language Processing
(NLP) task that aims to identify and classify entity mentions in texts across
different categories. While languages such as English possess a large number of
high-quality resources for this task, Brazilian Portuguese still lacks in
quantity of gold-standard NER datasets, especially when considering specific
domains. Particularly, this paper considers the importance of NER for analyzing
historical texts in the context of digital humanities. To address this gap,
this work outlines the construction of MariNER: \textit{Mapeamento e
Anota\c{c}\~oes de Registros hIst\'oricos para NER} (Mapping and Annotation of
Historical Records for NER), the first gold-standard dataset for early
20th-century Brazilian Portuguese, with more than 9,000 manually annotated
sentences. We also assess and compare the performance of state-of-the-art NER
models for the dataset.

</details>


### [34] [Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning](https://arxiv.org/pdf/2506.23056)
*Xiang Zhuang, Bin Wu, Jiyu Cui, Kehua Feng, Xiaotong Li, Huabin Xing, Keyan Ding, Qiang Zhang, Huajun Chen*

Main category: cs.CL

TL;DR: A knowledge-enhanced framework (K-MSE) improves molecular structure elucidation in LLMs by integrating specialized chemical knowledge and Monte Carlo Tree Search, achieving over 20% performance boost.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with molecular structure elucidation due to limited chemical knowledge, prompting the need for a specialized framework.

Method: K-MSE uses an external molecular substructure knowledge base and a molecule-spectrum scorer for reward modeling, enhanced by Monte Carlo Tree Search.

Result: The framework significantly improves performance, with over 20% gains on GPT-4o-mini and GPT-4o.

Conclusion: K-MSE effectively addresses LLMs' limitations in molecular structure elucidation, enhancing accuracy through specialized knowledge integration.

Abstract: Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.

</details>


### [35] [Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries](https://arxiv.org/pdf/2506.23071)
*Zhengren Wang, Bozhou Li, Dongwen Yao, Wentao Zhang*

Main category: cs.CL

TL;DR: Text2VectorSQL unifies Text-to-SQL and vector search to enhance natural language query handling for structured and unstructured data, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Address limitations of Text-to-SQL (rigid syntax, ambiguity) and VectorSQL (manual crafting, lack of evaluation) by integrating both paradigms.

Method: Introduces Text2VectorSQL for semantic filtering, multi-modal matching, and retrieval acceleration, with synthetic data and expert-reviewed ground truths.

Result: Demonstrates significant performance improvements over baseline methods.

Conclusion: Establishes Text2VectorSQL as a foundation for more versatile database interfaces, with public repository availability.

Abstract: While Text-to-SQL enables natural language interaction with structured
databases, its effectiveness diminishes with unstructured data or ambiguous
queries due to rigid syntax and limited expressiveness. Concurrently, vector
search has emerged as a powerful paradigm for semantic retrieval, particularly
for unstructured data. However, existing VectorSQL implementations still rely
heavily on manual crafting and lack tailored evaluation frameworks, leaving a
significant gap between theoretical potential and practical deployment. To
bridge these complementary paradigms, we introduces Text2VectorSQL, a novel
framework unifying Text-to-SQL and vector search to overcome expressiveness
constraints and support more diverse and holistical natural language queries.
Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching,
and retrieval acceleration. For evaluation, we build vector index on
appropriate columns, extend user queries with semantic search, and annotate
ground truths via an automatic pipeline with expert review. Furthermore, we
develop dedicated Text2VectorSQL models with synthetic data, demonstrating
significant performance improvements over baseline methods. Our work
establishes the foundation for the Text2VectorSQL task, paving the way for more
versatile and intuitive database interfaces. The repository will be publicly
available at https://github.com/Open-DataFlow/Text2VectorSQL.

</details>


### [36] [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/pdf/2506.23101)
*Yue Xu, Wenjie Wang*

Main category: cs.CL

TL;DR: The paper introduces Genres, a benchmark for evaluating gender bias in multimodal large language models (MLLMs) through relational and contextual interactions, revealing persistent biases not evident in single-character settings.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about gender bias in MLLMs, especially in socially sensitive applications, by moving beyond isolated evaluations to examine relational and contextual bias.

Method: Developed Genres, a benchmark using dual-character profiles and narrative generation tasks to assess gender bias in interpersonal dynamics.

Result: Experiments showed context-sensitive gender biases in MLLMs, highlighting issues not captured in single-entity evaluations.

Conclusion: Relationship-aware benchmarks like Genres are crucial for identifying subtle, interaction-driven biases and guiding future bias mitigation efforts.

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
across tasks involving both visual and textual modalities. However, growing
concerns remain about their potential to encode and amplify gender bias,
particularly in socially sensitive applications. Existing benchmarks
predominantly evaluate bias in isolated scenarios, overlooking how bias may
emerge subtly through interpersonal interactions. We fill this gap by going
beyond single-entity evaluation and instead focusing on a deeper examination of
relational and contextual gender bias in dual-individual interactions. We
introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs
through the lens of social relationships in generated narratives. Genres
assesses gender bias through a dual-character profile and narrative generation
task that captures rich interpersonal dynamics and supports a fine-grained bias
evaluation suite across multiple dimensions. Experiments on both open- and
closed-source MLLMs reveal persistent, context-sensitive gender biases that are
not evident in single-character settings. Our findings underscore the
importance of relationship-aware benchmarks for diagnosing subtle,
interaction-driven gender bias in MLLMs and provide actionable insights for
future bias mitigation.

</details>


### [37] [FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes](https://arxiv.org/pdf/2506.23111)
*Janki Atul Nawale, Mohammed Safi Ur Rahman Khan, Janani D, Mansi Gupta, Danish Pruthi, Mitesh M. Khapra*

Main category: cs.CL

TL;DR: INDIC-BIAS is an India-centric benchmark to evaluate fairness in LLMs across 85 identity groups, revealing strong biases against marginalized identities.


<details>
  <summary>Details</summary>
Motivation: Existing fairness studies are Western-focused, lacking applicability to culturally diverse regions like India.

Method: Curated 1,800 socio-cultural topics, generated 20,000 scenario templates, and structured them into three tasks (plausibility, judgment, generation) to evaluate 14 LLMs.

Result: LLMs exhibit strong negative biases against marginalized groups, reinforcing stereotypes and failing to mitigate bias even when prompted.

Conclusion: INDIC-BIAS highlights allocative and representational harms of LLMs in India, advocating cautious usage and open-sourcing the benchmark for further research.

Abstract: Existing studies on fairness are largely Western-focused, making them
inadequate for culturally diverse countries such as India. To address this gap,
we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to
evaluate fairness of LLMs across 85 identity groups encompassing diverse
castes, religions, regions, and tribes. We first consult domain experts to
curate over 1,800 socio-cultural topics spanning behaviors and situations,
where biases and stereotypes are likely to emerge. Grounded in these topics, we
generate and manually validate 20,000 real-world scenario templates to probe
LLMs for fairness. We structure these templates into three evaluation tasks:
plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on
these tasks reveals strong negative biases against marginalized identities,
with models frequently reinforcing common stereotypes. Additionally, we find
that models struggle to mitigate bias even when explicitly asked to rationalize
their decision. Our evaluation provides evidence of both allocative and
representational harms that current LLMs could cause towards Indian identities,
calling for a more cautious usage in practical applications. We release
INDIC-BIAS as an open-source benchmark to advance research on benchmarking and
mitigating biases and stereotypes in the Indian context.

</details>


### [38] [Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models](https://arxiv.org/pdf/2506.23122)
*Shivam Sharma, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: The paper explores identifying narrative roles (Hero, Villain, Victim, Other) in memes across English and code-mixed languages, using a balanced dataset. It evaluates various models, highlighting challenges in detecting the 'Victim' class and cultural generalization.


<details>
  <summary>Details</summary>
Motivation: To address the nuanced, culture-specific language in memes and improve role detection, especially for underrepresented classes like 'Victim'.

Method: Evaluates multilingual transformers, sentiment classifiers, LLMs, and multimodal models under zero-shot settings, using precision, recall, and F1 metrics. Also explores prompt design for multimodal models.

Result: Larger models like DeBERTa-v3 and Qwen2.5-VL perform well, but 'Victim' detection and cross-cultural generalization remain challenging. Hybrid prompts improve results marginally.

Conclusion: Cultural grounding, prompt engineering, and multimodal reasoning are crucial for accurately modeling narrative roles in memes.

Abstract: This work investigates the challenging task of identifying narrative roles -
Hero, Villain, Victim, and Other - in Internet memes, across three diverse test
sets spanning English and code-mixed (English-Hindi) languages. Building on an
annotated dataset originally skewed toward the 'Other' class, we explore a more
balanced and linguistically diverse extension, originally introduced as part of
the CLEF 2024 shared task. Comprehensive lexical and structural analyses
highlight the nuanced, culture-specific, and context-rich language used in real
memes, in contrast to synthetically curated hateful content, which exhibits
explicit and repetitive lexical markers. To benchmark the role detection task,
we evaluate a wide spectrum of models, including fine-tuned multilingual
transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,
and multimodal vision-language models. Performance is assessed under zero-shot
settings using precision, recall, and F1 metrics. While larger models like
DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent
challenges in reliably identifying the 'Victim' class and generalising across
cultural and code-mixed content. We also explore prompt design strategies to
guide multimodal models and find that hybrid prompts incorporating structured
instructions and role definitions offer marginal yet consistent improvements.
Our findings underscore the importance of cultural grounding, prompt
engineering, and multimodal reasoning in modelling subtle narrative framings in
visual-textual content.

</details>


### [39] [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2506.23127)
*Zhaoye Fei, Li Ji, Siyin Wang, Junhao Shi, Jingjing Gong, Xipeng Qiu*

Main category: cs.CL

TL;DR: Embodied Planner-R1 is a reinforcement learning framework for LLMs to improve interactive task planning, achieving high success rates on benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with embodied task planning due to static knowledge and lack of causal learning in dynamic environments.

Method: Uses reinforcement learning with group rollout, sparse rewards, and Interactive Policy Optimization (IPO) for autonomous exploration.

Result: Achieves 97.78% on ALFWorld and 79.92% on ScienceWorld, with strong generalization.

Conclusion: Embodied Planner-R1 significantly outperforms prior methods in interactive task planning.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they face significant challenges in embodied task planning
scenarios that require continuous environmental understanding and action
generation. Existing approaches generate open-loop action scripts based on
static knowledge, making it difficult to learn causal relationships between
actions and environmental feedback, particularly in partially observable
environments. We introduce Embodied Planner-R1, a novel outcome-driven
reinforcement learning framework that enables LLMs to develop interactive
capabilities through autonomous exploration with minimal supervision. Our
framework incorporates three key innovations: (1) Without human annotations, we
employ pure reinforcement learning with group rollout, incorporating
in-environment interaction through parallel exploration; (2) completion-driven
sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient
learning from grouped trajectories. Across two challenging text-based Embodied
planning benchmarks, Embodied Planner-R1 achieves impressive completion rates
of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a
large margin, and suffers only a -3.66% drop in previously unseen environments,
evidencing strong generalization.

</details>


### [40] [Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format](https://arxiv.org/pdf/2506.23133)
*Dingzirui Wang, Xuanliang Zhang, Rongyu Cao, Longxu Dou, Xianzhen Luo, Yingwei Ma, Qingfu Zhu, Wanxiang Che, Binhua Li, Fei Huang, Yongbin Li*

Main category: cs.CL

TL;DR: Format-Adapter improves reasoning in LLMs by generating and selecting task-specific formats, reducing human labeling costs and boosting performance by 4.3%.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and unsuitability of human-labeled reasoning formats in LLMs, which are costly and not universally applicable.

Method: Proposes measuring reasoning errors, then introduces Format-Adapter to generate and select optimal formats using LLMs, minimizing these errors.

Result: Achieves a 4.3% average performance improvement on math and commonsense reasoning tasks.

Conclusion: Format-Adapter effectively enhances reasoning consistency and performance in LLMs by automating format selection.

Abstract: Generating and voting multiple answers is an effective method to mitigate
reasoning inconsistencies of large language models (LLMs). Prior works have
shown that multiple reasoning formats outperform a single format when
generating multiple answers. However, previous works using multiple formats
rely on formats labeled by humans, which could be unsuitable for all tasks and
have high labeling costs. To address this issue, we adapt suitable formats to
the given tasks by generating and selecting formats. We first propose how to
measure the reasoning error when generating multiple answers. Then, we
introduce Format-Adapter, which utilizes LLMs to generate and select suitable
reasoning formats by minimizing the error measurement we present. We conduct
experiments on math and commonsense reasoning tasks, where Format-Adapter
achieves a 4.3% performance improvement on average over previous works,
demonstrating the effectiveness.

</details>


### [41] [LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation](https://arxiv.org/pdf/2506.23136)
*Shadman Sobhan, Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: A RAG pipeline for technical documents improves retrieval of structured data (tables, images) and outperforms general RAG methods in faithfulness and relevancy.


<details>
  <summary>Details</summary>
Motivation: Address challenges of LLMs (hallucination, outdated knowledge) and limitations of traditional RAG with structured data.

Method: Proposes a RAG pipeline combining vector similarity search and a fine-tuned reranker (Gemma-2-9b-it) trained with RAFT on a custom dataset.

Result: Achieves high faithfulness (94-96%) and relevancy (87-93%) scores, excelling in table-based and out-of-context questions.

Conclusion: The pipeline is effective for technical documents, outperforming general RAG methods.

Abstract: Large Language Models (LLMs) are capable of natural language understanding
and generation. But they face challenges such as hallucination and outdated
knowledge. Fine-tuning is one possible solution, but it is resource-intensive
and must be repeated with every data update. Retrieval-Augmented Generation
(RAG) offers an efficient solution by allowing LLMs to access external
knowledge sources. However, traditional RAG pipelines struggle with retrieving
information from complex technical documents with structured data such as
tables and images. In this work, we propose a RAG pipeline, capable of handling
tables and images in documents, for technical documents that support both
scanned and searchable formats. Its retrieval process combines vector
similarity search with a fine-tuned reranker based on Gemma-2-9b-it. The
reranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom
dataset designed to improve context identification for question answering. Our
evaluation demonstrates that the proposed pipeline achieves a high faithfulness
score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%
(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed
architecture is superior to general RAG pipelines in terms of table-based
questions and handling questions outside context.

</details>


### [42] [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/pdf/2506.23137)
*Siyuan Li, Ruitong Liu, Yan Wen, Te Sun*

Main category: cs.CL

TL;DR: The paper introduces Flow-Modulated Scoring (FMS) for Knowledge Graph Completion, combining static embeddings with dynamic context-aware transformations to improve relational modeling.


<details>
  <summary>Details</summary>
Motivation: Existing KGC methods rely on static embeddings, limiting their ability to capture contextual dependencies and relational dynamics.

Method: FMS uses a semantic context learning module for context-sensitive entity representations and a conditional flow-matching module for dynamic embedding transformations.

Result: FMS outperforms state-of-the-art methods on standard benchmarks.

Conclusion: FMS enhances relational semantics by integrating static and dynamic information, achieving superior performance in KGC.

Abstract: Effective modeling of multifaceted relations is pivotal for Knowledge Graph
Completion (KGC). However, a majority of existing approaches are predicated on
static, embedding-based scoring, exhibiting inherent limitations in capturing
contextual dependencies and relational dynamics. Addressing this gap, we
propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal
components: (1) a semantic context learning module that encodes
context-sensitive entity representations, and (2) a conditional flow-matching
module designed to learn the dynamic transformation from a head to a tail
embedding, governed by the aforementioned context. The resultant predictive
vector field, representing the context-informed relational path, serves to
dynamically refine the initial static score of an entity pair. Through this
synergy of context-aware static representations and conditioned dynamic
information, FMS facilitates a more profound modeling of relational semantics.
Comprehensive evaluations on several standard benchmarks demonstrate that our
proposed method surpasses prior state-of-the-art results.

</details>


### [43] [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/pdf/2506.23139)
*Prafulla Kumar Choubey, Xiangyu Peng, Shilpa Bhagavath, Kung-Hsiang Huang, Caiming Xiong, Chien-Sheng Wu*

Main category: cs.CL

TL;DR: A new benchmark for Deep Search evaluates RAG systems using synthetic data simulating business workflows, revealing retrieval as the main bottleneck.


<details>
  <summary>Details</summary>
Motivation: To address the need for evaluating complex RAG systems that require multi-hop reasoning over diverse, noisy sources like documents, transcripts, and URLs.

Method: Built a synthetic data pipeline simulating business workflows, generating interconnected content with realistic noise and multi-hop questions.

Result: Best-performing RAG methods scored 32.96 on the benchmark, with retrieval identified as the main bottleneck.

Conclusion: Existing RAG methods struggle with deep searches, leading to performance degradation due to partial context retrieval.

Abstract: We present a new benchmark for evaluating Deep Search--a realistic and
complex form of retrieval-augmented generation (RAG) that requires
source-aware, multi-hop reasoning over diverse, sparsed, but related sources.
These include documents, meeting transcripts, Slack messages, GitHub, and URLs,
which vary in structure and often contain human-to-human interactions. We build
it using a synthetic data pipeline that simulates business workflows across
product planning, development, and support stages, generating interconnected
content with realistic noise and multi-hop questions with guaranteed
ground-truth answers. We release our benchmark with both answerable and
unanswerable queries, and retrieval pool of 39,190 enterprise artifacts,
enabling fine-grained evaluation of long-context LLM and RAG systems. Our
experiments reveal that even the best-performing agentic RAG methods achieve an
average performance score of 32.96 on our benchmark. With further analysis, we
highlight retrieval as the main bottleneck: existing methods struggle to
conduct deep searches and retrieve all necessary evidence. Consequently, they
often reason over partial context, leading to significant performance
degradation.

</details>


### [44] [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/pdf/2506.23146)
*Dingzriui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng*

Main category: cs.CL

TL;DR: The paper introduces the Learning-to-Context Slope (LCS) metric to evaluate in-context learning (ICL) effectiveness in LLMs, addressing reliability, attribution, and data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Current ICL evaluation methods are unreliable, poorly attributed, and impractical in data-insufficient scenarios, necessitating a better metric.

Method: Proposes LCS, which models the slope between learning gain and contextual relevance to quantify ICL effectiveness.

Result: LCS reliably correlates with performance improvements, works in biased/data-scarce settings, and identifies actionable thresholds.

Conclusion: LCS is a robust metric for evaluating ICL, offering insights into model capabilities and practical thresholds for practitioners.

Abstract: In-context learning (ICL) has emerged as an effective approach to enhance the
performance of large language models (LLMs). However, its effectiveness varies
significantly across models and tasks, posing challenges for practitioners to
determine when ICL reliably improves performance. Current evaluation
approaches, reliant on performance change after applying ICL, suffer from low
reliability, poor attribution, and impracticality in data-insufficient
scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that
quantifies ICL effectiveness by modeling the slope between learning gain (loss
decrease from demonstrations) and contextual relevance (demonstration-input
relevance). LCS addresses key limitations of performance-based metrics: (1) it
captures continuous loss changes even when outputs are incorrect, improving
reliability; (2) its formulation attributes ICL failures to weak contextual
alignment (inability to adapt inputs to demonstrations) or strong output
calibration (self-verification of correctness); and (3) it minimizes reliance
on labeled data via synthetic evaluation. Extensive experiments demonstrate
that LCS strongly correlates with performance improvements in labeled settings
and reliably reflects true effectiveness in biased or data-scarce scenarios.
Further analysis reveals actionable thresholds for LCS and identifies model
capabilities critical to ICL success.

</details>


### [45] [V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy](https://arxiv.org/pdf/2506.23149)
*Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng*

Main category: cs.CL

TL;DR: The paper introduces V-Synthesis, a method for synthesizing demonstrations from scratch for arbitrary tasks in in-context learning, addressing the challenge of consistency with the target task.


<details>
  <summary>Details</summary>
Motivation: High labeling costs for in-context learning demonstrations and the limitations of existing synthesis methods motivate the need for a task-agnostic approach.

Method: Proposes V-Score, a consistency metric, and V-Synthesis, which uses proportional sampling based on V-Score to ensure consistency and diversity.

Result: V-Synthesis improves performance by 2.0% on average compared to existing methods.

Conclusion: V-Synthesis is effective for synthesizing demonstrations from scratch, offering better consistency and performance.

Abstract: High labeling cost for in-context learning (ICL) demonstrations motivates
using large language models (LLMs) for synthesis to reduce overhead. However,
existing synthesis methods are mainly task-specific or rely on pre-existing
demonstrations. So this paper focuses on synthesizing demonstrations from
scratch for arbitrary tasks. A major challenge in synthesizing from scratch is
ensuring consistency with the target task, as the lack of labeling guidance
could lead to synthesis bias. We first propose a consistency metric called
V-Score, which has higher performance and lower computation cost compared with
the metrics based on grams or embedding vectors. Furthermore, we introduce
V-Synthesis, which leverages V-Score for proportional sampling to ensure both
high consistency and diversity of synthesized demonstrations. Experimental
results demonstrate that V-Synthesis yields an average performance improvement
of 2.0% compared to existing synthesis methods confirming the effectiveness of
V-Synthesis.

</details>


### [46] [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/pdf/2506.23192)
*Gabriel Iturra-Bocaz, Felipe Bravo-Marquez*

Main category: cs.CL

TL;DR: RiverText is a Python library for training and evaluating incremental word embeddings from text streams, addressing the static nature of traditional models.


<details>
  <summary>Details</summary>
Motivation: Traditional word embeddings are static and struggle with evolving language patterns, especially in dynamic contexts like social media.

Method: RiverText implements incremental techniques (Skip-gram, CBOW, Word Context Matrix) using PyTorch and adapts evaluation tasks for streaming.

Result: The library provides a standardized framework for incremental word embeddings and compares methods with various hyperparameters.

Conclusion: RiverText is a valuable open-source tool for dynamic word embedding scenarios, available on GitHub.

Abstract: Word embeddings have become essential components in various information
retrieval and natural language processing tasks, such as ranking, document
classification, and question answering. However, despite their widespread use,
traditional word embedding models present a limitation in their static nature,
which hampers their ability to adapt to the constantly evolving language
patterns that emerge in sources such as social media and the web (e.g., new
hashtags or brand names). To overcome this problem, incremental word embedding
algorithms are introduced, capable of dynamically updating word representations
in response to new language patterns and processing continuous data streams.
  This paper presents RiverText, a Python library for training and evaluating
incremental word embeddings from text data streams. Our tool is a resource for
the information retrieval and natural language processing communities that work
with word embeddings in streaming scenarios, such as analyzing social media.
The library implements different incremental word embedding techniques, such as
Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized
framework. In addition, it uses PyTorch as its backend for neural network
training. We have implemented a module that adapts existing intrinsic static
word embedding evaluation tasks for word similarity and word categorization to
a streaming setting. Finally, we compare the implemented methods with different
hyperparameter settings and discuss the results. Our open-source library is
available at https://github.com/dccuchile/rivertext.

</details>


### [47] [Generalist Reward Models: Found Inside Large Language Models](https://arxiv.org/pdf/2506.23235)
*Yi-Chen Li, Tian Xu, Yang Yu, Xuqin Zhang, Xiong-Hui Chen, Zhongxiang Ling, Ningjing Chao, Lei Yuan, Zhi-Hua Zhou*

Main category: cs.CL

TL;DR: The paper reveals that LLMs inherently contain a latent reward model equivalent to offline inverse reinforcement learning, enabling high-quality reward signals without additional training. This method outperforms existing approaches and offers a scalable alternative to human preference-based alignment.


<details>
  <summary>Details</summary>
Motivation: To bypass the high cost of human preference data for LLM alignment and provide a rigorous theoretical foundation for AI feedback methods.

Method: The study leverages the latent reward model within LLMs, proving its equivalence to offline inverse reinforcement learning and using it for reinforcement learning without further training.

Result: The method outperforms existing LLM-as-a-judge approaches and surpasses explicitly trained reward models, with a provably superior error bound.

Conclusion: The findings suggest replacing reward modeling with eliciting pre-trained knowledge, offering a more efficient and scalable paradigm for LLM alignment.

Abstract: The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.

</details>


### [48] [Two Spelling Normalization Approaches Based on Large Language Models](https://arxiv.org/pdf/2506.23288)
*Miguel Domingo, Francisco Casacuberta*

Main category: cs.CL

TL;DR: The paper proposes two new spelling normalization methods using large language models, comparing unsupervised training and machine translation, with machine translation proving more effective.


<details>
  <summary>Details</summary>
Motivation: Standardizing spelling in historical documents is challenging due to evolving language conventions, necessitating effective normalization techniques.

Method: Two approaches are tested: one using unsupervised training and another using machine translation, evaluated across diverse datasets.

Result: Both methods showed promise, but statistical machine translation performed better for spelling normalization.

Conclusion: Machine translation remains the most suitable technology for spelling normalization in historical documents.

Abstract: The absence of standardized spelling conventions and the organic evolution of
human language present an inherent linguistic challenge within historical
documents, a longstanding concern for scholars in the humanities. Addressing
this issue, spelling normalization endeavors to align a document's orthography
with contemporary standards. In this study, we propose two new approaches based
on large language models: one of which has been trained without a supervised
training, and a second one which has been trained for machine translation. Our
evaluation spans multiple datasets encompassing diverse languages and
historical periods, leading us to the conclusion that while both of them
yielded encouraging results, statistical machine translation still seems to be
the most suitable technology for this task.

</details>


### [49] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/pdf/2506.23293)
*P. Myles Eugenio*

Main category: cs.CL

TL;DR: A neuro-symbolic framework for generative language modeling uses local, event-driven learning with a hierarchical Hopfield memory chain, enabling emergent tokenization and structure without predefined tokens or supervision.


<details>
  <summary>Details</summary>
Motivation: To explore how symbolic structure can emerge from local neural learning and develop scalable, interpretable neuro-symbolic systems.

Method: Uses a hierarchical Hopfield memory chain as a dynamic tokenizer, learning symbol sequences as multi-scale representations and constructing projection tensors for hierarchical tokens.

Result: The model filters natural language patterns from noise, generates synthetic languages with human-like morphology, and generalizes beyond initial inference class without explicit data.

Conclusion: The framework provides a foundation for scalable, interpretable neuro-symbolic systems, advancing neuromorphic architectures for generative language models.

Abstract: We present a neuro-symbolic framework for generative language modeling based
on local, event-driven emergent learning. At its core is a hierarchical
Hopfield memory chain acting as a compositional short-term memory and dynamic
tokenizer (retokenizer). Rather than relying on predefined tokens or
supervision, the model builds structure from scratch, learning symbol sequences
as multi-scale representations. It constructs projection tensors that bind
co-occurring features into hierarchical tokens, introducing redundancy (i.e an
emergent gauge structure) and enabling compression of local activations into
long-range dependencies. Curiously, we find that the retokenizer can filter
natural language patterns from noise, generating synthetic languages with
coherent internal morphology -- quantifiably the same as human language.
Language is learned in a local (Hebbian) fashion, where model constraints
dictate allowed emergent structure, and new information is retained in
alignment with this structure. The absence of a global objective enables a form
of plasticity not found in conventional language models, allowing the system to
generalize beyond its initial inference class -- even without explicit data. We
demonstrate that briefly activating a new neuron during inference binds
distributed multi-scale token features into a symbolic embedding. These
emergent embedding neurons act as long-term memory and support a key-value
mechanism for compositional inference and generalization. This architecture
provides a methodological foundation for studying how symbolic structure can
emerge from local neural learning. It offers a new pathway for building
scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and
reasoning arise as compressed memory traces within a Hopfield hierarchy. This
approach advances the development of neuromorphic architectures for generative
language models.

</details>


### [50] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/pdf/2506.23315)
*Shouvon Sarker, Xishuang Dong, Lijun Qian*

Main category: cs.CL

TL;DR: The paper presents a BERT-based ensemble model for detecting and classifying medication events from clinical notes, achieving improved performance in Micro-F and Macro-F scores.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of identifying key variables like medications and diseases from clinical data, leveraging the n2c2 2022 shared tasks and the CMED dataset.

Method: The method involves pretraining BERT models on diverse datasets (Wikipedia, MIMIC), fine-tuning them on CMED training data, and using ensemble voting strategies for final predictions.

Result: The ensemble model improved strict Micro-F score by ~5% and strict Macro-F score by ~6%.

Conclusion: BERT-based ensemble models are effective for medication event classification in clinical notes.

Abstract: Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

</details>


### [51] [Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family](https://arxiv.org/pdf/2506.23340)
*Yumeng Lin, Xufeng Duan, David Haslett, Yige Chen, Zhenguang G. Cai*

Main category: cs.CL

TL;DR: The study examines how training data, language proximity, and family affect multilingual translation quality in GPT-4 and Llama 2, revealing data size and structural closeness to English as key factors.


<details>
  <summary>Details</summary>
Motivation: To understand challenges in multilingual translation, especially for low-resource or linguistically distant languages.

Method: Evaluated GPT-4 and Llama 2 using round-trip translations, assessed with BLEU scores and BERT similarity metrics.

Result: Abundant data mitigates linguistic divergence, but structural closeness to English improves low-resource translation. Orthographic, phylogenetic, syntactic, and geographical distances predict performance.

Conclusion: Translation quality depends on data volume and structural/typological language relationships, highlighting linguistic constraints in multilingual models.

Abstract: Large language models have achieved impressive progress in multilingual
translation, yet they continue to face challenges with certain language
pairs-particularly those with limited training data or significant linguistic
divergence from English. This study systematically investigates how training
data, language proximity, and language family affect information loss in
multilingual translation. We evaluate two large language models, GPT-4 and
Llama 2, by performing round-trip translations. Translation quality was
assessed using BLEU scores and BERT similarity metrics. Our results reveal a
robust interaction between training data size and language distance: while
abundant training data can mitigate the effects of linguistic divergence,
languages structurally closer to English consistently yield higher translation
quality in low-resource conditions. Among various distance metrics,
orthographic, phylogenetic, syntactic, and geographical distances emerge as
strong predictors of translation performance. Language family also exerts an
independent influence. These findings contribute to a deeper understanding of
the linguistic constraints shaping multilingual translation in large language
models, emphasizing that translation quality is shaped not only by data volume
but also by structural and typological relationships between languages.

</details>


### [52] [ATGen: A Framework for Active Text Generation](https://arxiv.org/pdf/2506.23342)
*Akim Tsvigun, Daniil Vasilev, Ivan Tsvigun, Ivan Lysenko, Talgat Bektleuov, Aleksandr Medvedev, Uliana Vinogradova, Nikita Severin, Mikhail Mozikov, Andrey Savchenko, Rostislav Grigorev, Ramil Kuleev, Fedor Zhdanov, Artem Shelmanov, Ilya Makarov*

Main category: cs.CL

TL;DR: ATGen is a framework integrating active learning (AL) with text generation (NLG) tasks, reducing annotation effort and costs by leveraging human annotators and LLMs like ChatGPT.


<details>
  <summary>Details</summary>
Motivation: Despite AL's potential, its application to NLG tasks has been limited. ATGen aims to bridge this gap.

Method: ATGen combines AL strategies with NLG, supporting both human annotators and LLM-based automatic annotation.

Result: The framework reduces human annotation effort and LLM API costs, with evaluations showing effectiveness across diverse NLG tasks.

Conclusion: ATGen successfully integrates AL with NLG, offering a practical solution for efficient annotation and benchmarking.

Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the
annotation effort required for training machine learning models. However,
despite the surging popularity of natural language generation (NLG) tasks in
recent years, the application of AL to NLG has been limited. In this paper, we
introduce Active Text Generation (ATGen) - a comprehensive framework that
bridges AL with text generation tasks, enabling the application of
state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered
annotation in NLG tasks using both human annotators and automatic annotation
agents based on large language models (LLMs). The framework supports LLMs
deployed as services, such as ChatGPT and Claude, or operated on-premises.
Furthermore, ATGen provides a unified platform for smooth implementation and
benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present
evaluation results for state-of-the-art AL strategies across diverse settings
and multiple text generation tasks. We show that ATGen reduces both the effort
of human annotators and costs associated with API calls to LLM-based annotation
agents. The code of the framework is available on GitHub under the MIT license.
The video presentation is available at http://atgen-video.nlpresearch.group

</details>


### [53] [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/pdf/2506.23377)
*Taejin Kim, Siun-Chuon Mau, Konrad Vesey*

Main category: cs.CL

TL;DR: The paper introduces Perspective-Dial, a framework for quantifying and controlling the perspective or bias in LLM outputs using a metric space and systematic prompt engineering.


<details>
  <summary>Details</summary>
Motivation: There's a lack of quantifiable understanding of bias and perspective in LLM outputs, which is critical given their mission-critical roles.

Method: The approach involves (1) Perspective Space for measuring perspectives and (2) Systematic Prompt Engineering using greedy-coordinate descent to control LLM outputs.

Result: The method empirically quantifies and adjusts LLM outputs for various topics without needing a deep theoretical understanding of bias.

Conclusion: Perspective-Dial enables applications like bias detection, narrative tracking, and debate bots, addressing the need for perspective control in LLMs.

Abstract: Large language models (LLMs) are used in a variety of mission-critical roles.
Due to the rapidly developing nature of LLMs, there is a lack of quantifiable
understanding of the bias and perspective associated with LLM output. Inspired
by this need, this paper considers the broader issue of perspective or
viewpoint of general text and perspective control of large-language model (LLM)
output. Perspective-Dial consists of two main components: a (1) metric space,
dubbed Perspective Space, that enables quantitative measurements of different
perspectives regarding a topic, and the use of (2) Systematic Prompt
Engineering that utilizes greedy-coordinate descent to control LLM output
perspective based on measurement feedback from the Perspective Space. The
empirical nature of the approach allows progress to side step a principled
understanding of perspective or bias -- effectively quantifying and adjusting
outputs for a variety of topics. Potential applications include detection,
tracking and mitigation of LLM bias, narrative detection, sense making and
tracking in public discourse, and debate bot advocating given perspective.

</details>


### [54] [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/pdf/2506.23393)
*Eugene J. Yu, Dawei Zhu, Yifan Song, Xiangyu Wong, Jiebin Zhang, Wenxuan Shi, Xiaoguang Li, Qun Liu, Sujian Li*

Main category: cs.CL

TL;DR: The paper introduces MOG, a framework for generating Wikipedia articles using hierarchical memory architecture to improve accuracy and reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: Autonomous Wikipedia article generation is complex due to the need for accurate, structured information from diverse sources.

Method: MOG extracts memory units from web documents, organizes them hierarchically, and uses this structure to guide generation, with a citation module for traceability.

Result: MOG outperforms baselines on the WikiStart dataset, producing more informative and reliable articles.

Conclusion: MOG is effective for real-world Wikipedia article generation, enhancing informativeness and verifiability.

Abstract: Generating Wikipedia articles autonomously is a challenging task requiring
the integration of accurate, comprehensive, and well-structured information
from diverse sources. This paper introduces the Memory Organization-based
Generation (MOG) framework, a novel approach to address these challenges by
leveraging a hierarchical memory architecture. MOG extracts fine-grained memory
units from web documents, recursively organizes them into a Wikipedia-style
hierarchical structure, and uses this structure to guide the generation
process. This ensures alignment between memory and the article outline,
improving both informativeness and verifiability while minimizing
hallucinations. Additionally, a citation module is implemented to enhance
traceability by linking every generated sentence to specific memory units.
Evaluations on our newly created WikiStart dataset demonstrate that MOG
outperforms baseline methods in producing informative and reliable articles,
making it particularly robust in real-world scenarios.

</details>


### [55] [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/pdf/2506.23411)
*Jiale Zhang, Zichong Wang, Avash Palikhe, Zhipeng Yin, Wenbin Zhang*

Main category: cs.CL

TL;DR: The paper reviews fairness datasets in language model research, introduces a unified evaluation framework, and highlights biases to improve fairness assessments.


<details>
  <summary>Details</summary>
Motivation: Address the lack of scrutiny on fairness datasets used in language model benchmarks.

Method: Survey and characterize widely used fairness datasets, introduce a unified evaluation framework, and analyze 24 benchmarks.

Result: Reveals demographic disparities and biases in fairness datasets, offering guidance for better usage.

Conclusion: Encourages more diverse benchmarks and thoughtful use of fairness datasets, with open resources for transparency.

Abstract: Fairness benchmarks play a central role in shaping how we evaluate language
models, yet surprisingly little attention has been given to examining the
datasets that these benchmarks rely on. This survey addresses that gap by
presenting a broad and careful review of the most widely used fairness datasets
in current language model research, characterizing them along several key
dimensions including their origin, scope, content, and intended use to help
researchers better appreciate the assumptions and limitations embedded in these
resources. To support more meaningful comparisons and analyses, we introduce a
unified evaluation framework that reveals consistent patterns of demographic
disparities across datasets and scoring methods. Applying this framework to
twenty four common benchmarks, we highlight the often overlooked biases that
can influence conclusions about model fairness and offer practical guidance for
selecting, combining, and interpreting these datasets. We also point to
opportunities for creating new fairness benchmarks that reflect more diverse
social contexts and encourage more thoughtful use of these tools going forward.
All code, data, and detailed results are publicly available at
https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets
to promote transparency and reproducibility across the research community.

</details>


### [56] [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/pdf/2506.23423)
*Felipe Nuti, Tim Franzmeyer, João Henriques*

Main category: cs.CL

TL;DR: The paper introduces Tuning Contribution (TuCo), a method to quantitatively measure fine-tuning's impact on individual LLM outputs by analyzing hidden states and decomposing models into pre-training and fine-tuning components.


<details>
  <summary>Details</summary>
Motivation: Existing work lacks a systematic way to analyze fine-tuning's effect on individual LLM outputs, prompting the need for a quantitative method.

Method: Proposes tracking intermediate hidden states and decomposing fine-tuned LLMs into pre-training and fine-tuning components, introducing TuCo as a ratio of their magnitudes.

Result: TuCo reveals that adversarial attacks reduce fine-tuning's influence, and lower TuCo correlates with attack success, indicating its role in model safety.

Conclusion: TuCo provides a tool to study fine-tuning's influence on LLM behavior and safety, offering insights into adversarial attack mechanisms.

Abstract: Past work has studied the effects of fine-tuning on large language models'
(LLMs) overall performance on certain tasks. However, a quantitative and
systematic method for analyzing its effect on individual outputs is still
lacking. Here, we propose a new method for measuring the contribution that
fine-tuning makes to individual LLM responses, assuming access to the original
pre-trained model. Our method tracks the model's intermediate hidden states,
providing a more fine-grained insight into the effects of fine-tuning than a
simple comparison of final outputs from pre-trained and fine-tuned models. We
introduce and theoretically analyze an exact decomposition of any fine-tuned
LLM into a pre-training component and a fine-tuning component. Empirically, we
find that model behavior and performance can be steered by up- or down-scaling
the fine-tuning component during the forward pass. Motivated by this finding
and our theoretical analysis, we define the Tuning Contribution (TuCo) as the
ratio of the magnitudes of the fine-tuning component to the pre-training
component. We observe that three prominent adversarial attacks on LLMs
circumvent safety measures in a way that reduces TuCo, and that TuCo is
consistently lower on prompts where these attacks succeed compared to those
where they do not. This suggests that attenuating the effect of fine-tuning on
model outputs plays a role in the success of such attacks. In summary, TuCo
enables the quantitative study of how fine-tuning influences model behavior and
safety, and vice versa.

</details>


### [57] [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/pdf/2506.23431)
*Zixian Huang, Chenxu Niu, Yu Gu, Gengyang Xiao, Xinwei Huang, Gong Cheng*

Main category: cs.CL

TL;DR: Proposes a pipelined decoder for parallel text generation, improving speed without compromising quality or memory.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models are slow due to sequential token generation, creating a bottleneck.

Method: Introduces a pipelined decoder that generates multiple subsequences in parallel at each time-step.

Result: Significantly improves generation speed across tasks like QA, summarization, and keyphrase generation.

Conclusion: The pipelined decoder offers a viable solution for faster, context-aware text generation.

Abstract: As the basis of generative AI, an autoregressive model requires the
generation of a new token depending on all the previously generated tokens,
which brings high quality but also restricts the model to generate tokens one
by one, forming a bottleneck limiting the generation speed. In this paper, we
propose a new decoder architecture that efficiently generates text in parallel
for context-aware generation tasks. Our proposed pipelined decoder initiates
the generation of multiple subsequences simultaneously, and, at each time-step,
it generates a new token for each subsequence to realize parallelism.
Experiments on multiple text generation tasks, including question answering,
text summarization, and keyphrase generation, show that our pipelined decoder
significantly improves the generation speed without a significant loss of
generation quality or additional memory consumption.

</details>


### [58] [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/pdf/2506.23463)
*Jang Won June*

Main category: cs.CL

TL;DR: ATF (Adaptive Table Filtering Framework) prunes uninformative table columns and rows to improve LLM performance on large tables, reducing cells by ~70% and boosting TableQA tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with large tables due to input length limits, necessitating a solution to filter uninformative data.

Method: ATF uses LLM-generated column descriptions, clustering, and sparse-dense alignment scores to prune columns and rows, integrating with models like TAPAS and TAPEX without retraining.

Result: ATF reduces table cells by ~70%, improving performance on TableQA tasks but slightly dropping performance on Table Fact Verification where full-table context is crucial.

Conclusion: ATF effectively balances informativeness and minimalism across tasks, enhancing LLM performance on large tables.

Abstract: Large language models (LLMs) for table-based reasoning often struggle with
large tables due to input length limits. We propose ATF (Adaptive Table
Filtering Framework), a modular and question-aware filtering pipeline that
prunes uninformative columns and rows using LLM-generated column descriptions,
clustering, and sparse-dense alignment scores. ATF integrates seamlessly with
existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that
ATF reduces table cells by ~70\%, boosting performance on out-of-domain TableQA
tasks while causing slight performance drops on Table Fact Verification, where
full-table context is more critical. These results highlight ATF's ability to
adaptively balance informativeness and minimalism across tasks.

</details>


### [59] [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/pdf/2506.23485)
*Haocheng Yu, Yaxiong Wu, Hao Wang, Wei Guo, Yong Liu, Yawen Li, Yuyang Ye, Junping Du, Enhong Chen*

Main category: cs.CL

TL;DR: TAIRA is a novel LLM-powered multi-agent system for interactive recommendations, addressing complex user intents through thought-augmented planning and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-powered interactive recommender agents struggle with diverse and complex user intents due to limited planning and generalization capabilities.

Method: TAIRA uses a manager agent with Thought Pattern Distillation (TPD) to decompose user needs and plan subtasks, alongside user simulation schemes for evaluation.

Result: TAIRA significantly outperforms existing methods, especially on challenging tasks, and generalizes well on novel tasks.

Conclusion: TAIRA effectively manages complex user intents in interactive recommendation systems, validated by comprehensive experiments.

Abstract: Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.

</details>


### [60] [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/pdf/2506.23508)
*Zhihao Zhang, Qiaole Dong, Qi Zhang, Jun Zhao, Enyu Zhou, Zhiheng Xi, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Tao Ji, Tao Gui, Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper explores the trade-off between SFT and RFT in adapting multimodal models, showing SFT causes forgetting while RFT preserves prior knowledge.


<details>
  <summary>Details</summary>
Motivation: To understand how post-training methods like SFT and RFT affect prior knowledge in multimodal models.

Method: Introduces jigsaw puzzles as a novel task and tests SFT and RFT on Qwen2.5-VL, analyzing learning dynamics.

Result: SFT learns quickly but forgets prior knowledge, while RFT learns slowly but retains knowledge. RFT's alignment with the base model mitigates interference.

Conclusion: Data distribution, not just algorithms, drives forgetting. RFT is promising for stable continual learning in multimodal models.

Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and
Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large
language models to downstream tasks. While effective at task adaptation, their
impact on prior knowledge remains unclear. In this paper, we introduce jigsaw
puzzles as a novel task absent from existing pretraining corpora and
systematically study the behavior of SFT and RFT on an open-source multimodal
model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid
task acquisition but leads to catastrophic forgetting, whereas RFT learns more
slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon
through the lens of learning dynamics, showing that RFT reinforces correct
samples that are naturally aligned with the base model's probability landscape,
mitigating interference with prior knowledge. Moreover, supervised training on
correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly
learning new tasks. These findings suggest that data distribution, rather than
algorithmic differences, plays a central role in forgetting, and highlight
RFT's potential for stable continual learning in multimodal large language
models.

</details>


### [61] [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/pdf/2506.23524)
*Phan Quoc Hung Mai, Quang Hung Nguyen, Phuong Giang Duong, Hong Hanh Nguyen, Nguyen Tuan Long*

Main category: cs.CL

TL;DR: The paper introduces NEU-ESC, a Vietnamese dataset for sentiment and topic classification in education, addressing gaps in existing datasets. It uses multitask learning with BERT, achieving high accuracy, and benchmarks against other models.


<details>
  <summary>Details</summary>
Motivation: Existing educational datasets lack domain relevance and student slang in Vietnamese. NEU-ESC fills this gap with richer, more diverse data.

Method: Multitask learning with encoder-only language models (BERT) is applied to the NEU-ESC dataset for sentiment and topic classification.

Result: The model achieves 83.7% and 79.8% accuracy for sentiment and topic classification, respectively.

Conclusion: NEU-ESC provides a valuable resource for Vietnamese educational sentiment analysis, with strong performance demonstrated by BERT-based multitask learning.

Abstract: In the field of education, understanding students' opinions through their
comments is crucial, especially in the Vietnamese language, where resources
remain limited. Existing educational datasets often lack domain relevance and
student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese
dataset for Educational Sentiment Classification and Topic Classification,
curated from university forums, which offers more samples, richer class
diversity, longer texts, and broader vocabulary. In addition, we explore
multitask learning using encoder-only language models (BERT), in which we
showed that it achieves performance up to 83.7% and 79.8% accuracy for
sentiment and topic classification tasks. We also benchmark our dataset and
model with other datasets and models, including Large Language Models, and
discuss these benchmarks. The dataset is publicly available at:
https://huggingface.co/datasets/hung20gg/NEU-ESC.

</details>


### [62] [On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?](https://arxiv.org/pdf/2506.23527)
*Jan Kvapil, Martin Fajcik*

Main category: cs.CL

TL;DR: The paper investigates memorization, creativity, and nonsense in LLM-generated recipes, using human annotations and an automated pipeline for scalability.


<details>
  <summary>Details</summary>
Motivation: To analyze how LLMs balance memorization, creativity, and nonsense in generated recipes and to automate this analysis for scalability.

Method: Human annotation of 20 recipes from Mixtral, followed by an automated "LLM-as-judge" pipeline for large-scale analysis.

Result: Mixtral relies on memorized content, while the automated pipeline (Llama 3.1+Gemma 2 9B) achieves 78% accuracy in ingredient matching.

Conclusion: The automated framework enables scalable quantification of memorization and creativity in LLM-generated recipes, revealing their creative capacities.

Abstract: This work-in-progress investigates the memorization, creativity, and nonsense
found in cooking recipes generated from Large Language Models (LLMs).
Precisely, we aim (i) to analyze memorization, creativity, and non-sense in
LLMs using a small, high-quality set of human judgments and (ii) to evaluate
potential approaches to automate such a human annotation in order to scale our
study to hundreds of recipes. To achieve (i), we conduct a detailed human
annotation on 20 preselected recipes generated by LLM (Mixtral), extracting
each recipe's ingredients and step-by-step actions to assess which elements are
memorized--i.e., directly traceable to online sources possibly seen during
training--and which arise from genuine creative synthesis or outright nonsense.
We find that Mixtral consistently reuses ingredients that can be found in
online documents, potentially seen during model training, suggesting strong
reliance on memorized content. To achieve aim (ii) and scale our analysis
beyond small sample sizes and single LLM validation, we design an
``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,
parsing ingredients and recipe steps, and their annotation. For instance,
comparing its output against human annotations, the best ingredient extractor
and annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on
ingredient matching. This automated framework enables large-scale
quantification of memorization, creativity, and nonsense in generated recipes,
providing rigorous evidence of the models' creative capacities.

</details>


### [63] [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/pdf/2506.23601)
*Weijie Shi, Yue Cui, Yaguang Wu, Jingzhi Fang, Shibo Zhang, Mengze Li, Sirui Han, Jia Zhu, Jiajie Xu, Xiaofang Zhou*

Main category: cs.CL

TL;DR: SemDiD improves semantic diversity in LLM decoding, outperforming existing methods in coverage and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack semantic diversity, limiting applications like Best-of-N strategies and reinforcement learning.

Method: SemDiD uses orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment in embedding space.

Result: SemDiD improves Best-of-N coverage by 1.4-5.2%, accelerates RLHF training by 15%, and increases accuracy by up to 2.1%.

Conclusion: SemDiD effectively balances quality and semantic diversity, enhancing performance in diverse tasks.

Abstract: Diverse decoding of large language models is crucial for applications
requiring multiple semantically distinct responses, yet existing methods
primarily achieve lexical rather than semantic diversity. This limitation
significantly constrains Best-of-N strategies, group-based reinforcement
learning, and data synthesis. While temperature sampling and diverse beam
search modify token distributions or apply n-gram penalties, they fail to
ensure meaningful semantic differentiation. We introduce Semantic-guided
Diverse Decoding (SemDiD), operating directly in embedding space that balances
quality with diversity through three complementary mechanisms: orthogonal
directional guidance, dynamic inter-group repulsion, and position-debiased
probability assessment. SemDiD harmonizes these competing objectives using
adaptive gain functions and constraint optimization, ensuring both quality
thresholds and maximal semantic differentiation. Experiments show SemDiD
consistently outperforms existing methods, improving Best-of-N coverage by
1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%
while increasing accuracy by up to 2.1%.

</details>


### [64] [Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs](https://arxiv.org/pdf/2506.23610)
*Manuel Pratelli, Marinella Petrocchi*

Main category: cs.CL

TL;DR: LLMs can generate synthetic behavioral data, but their ability to replicate personality-driven psychological differences is mixed. Some traits like Agreeableness and Conscientiousness are reliably replicated, while others diverge.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can faithfully reproduce personality-based variations in behavior, specifically in susceptibility to misinformation, as an ethical and scalable alternative to human experiments.

Method: LLM agents were conditioned on Big-Five personality profiles and compared to human participants' responses to headline accuracy, using published datasets.

Result: Some personality traits (e.g., Agreeableness, Conscientiousness) were reliably replicated in LLM responses, while others showed systematic biases.

Conclusion: LLMs show promise for behavioral simulation but have limits in fully capturing personality-driven cognitive diversity, highlighting both potential and challenges.

Abstract: Large language models (LLMs) make it possible to generate synthetic
behavioural data at scale, offering an ethical and low-cost alternative to
human experiments. Whether such data can faithfully capture psychological
differences driven by personality traits, however, remains an open question. We
evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to
reproduce personality-based variation in susceptibility to misinformation,
focusing on news discernment, the ability to judge true headlines as true and
false headlines as false. Leveraging published datasets in which human
participants with known personality profiles rated headline accuracy, we create
matching LLM agents and compare their responses to the original human patterns.
Certain trait-misinformation associations, notably those involving
Agreeableness and Conscientiousness, are reliably replicated, whereas others
diverge, revealing systematic biases in how LLMs internalize and express
personality. The results underscore both the promise and the limits of
personality-aligned LLMs for behavioral simulation, and offer new insight into
modeling cognitive diversity in artificial agents.

</details>


### [65] [Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack](https://arxiv.org/pdf/2506.23661)
*Arnisa Fazla, Lucas Krauter, David Guzman Piedrahita, Andrianos Michail*

Main category: cs.CL

TL;DR: BeamAttack is extended to include word deletions and skip substitutions, achieving high attack success rates while preserving text similarity.


<details>
  <summary>Details</summary>
Motivation: To enhance the robustness evaluation of text classification systems by discovering minimal adversarial modifications.

Method: Extends BeamAttack with word deletions, skip substitutions, and LIME integration for prioritized word replacements. Evaluated on BiLSTM, BERT, and RoBERTa within BODEGA.

Result: Over 99% attack success rate while maintaining semantic and lexical similarity.

Conclusion: BeamAttack is effective but has limitations; implementation is publicly available.

Abstract: We extend BeamAttack, an adversarial attack algorithm designed to evaluate
the robustness of text classification systems through word-level modifications
guided by beam search. Our extensions include support for word deletions and
the option to skip substitutions, enabling the discovery of minimal
modifications that alter model predictions. We also integrate LIME to better
prioritize word replacements. Evaluated across multiple datasets and victim
models (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA
framework, our approach achieves over a 99\% attack success rate while
preserving the semantic and lexical similarity of the original texts. Through
both quantitative and qualitative analysis, we highlight BeamAttack's
effectiveness and its limitations. Our implementation is available at
https://github.com/LucK1Y/BeamAttack

</details>


### [66] [Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation](https://arxiv.org/pdf/2506.23662)
*Philip Lippmann, Jie Yang*

Main category: cs.CL

TL;DR: ZEST is a zero-shot contextual adaptation framework that generates a synthetic proxy corpus for domain-adapted embeddings without requiring target corpus access or finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing context-aware embedding methods need target corpus access or domain-specific finetuning, which is impractical in privacy-sensitive or resource-limited settings.

Method: ZEST synthesizes a compact proxy corpus from a few exemplar documents using a hierarchical procedure, emulating domain-specific distributions.

Result: ZEST achieves within 0.5% accuracy of models with full corpus access on the MTEB benchmark, using only five example documents.

Conclusion: ZEST enables high-performance, adaptable embeddings in constrained environments without retraining or corpus access.

Abstract: Context-aware embedding methods boost retrieval accuracy by conditioning on
corpus statistics (e.g., term co-occurrence and topical patterns) extracted
from neighboring documents. However, this context-aware approach requires
access to the target corpus or requires domain-specific finetuning, posing
practical barriers in privacy-sensitive or resource-constrained settings. We
present ZEST, a zero-shot contextual adaptation framework that replaces real
corpus access with a one-time offline synthesis of a compact proxy. Given only
a handful exemplar documents representative of the general target domain, we
use a multi-step hierarchical procedure to generate a synthetic context corpus
of several hundred documents that aims to emulate key domain-specific
distributions. At inference, the frozen context-aware encoder uses this proxy
corpus -- without any finetuning or target corpus access -- to produce
domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot
synthetic context adaptation using only five example documents performs within
0.5% of models leveraging full target corpus access -- demonstrating remarkable
efficacy without any retraining. ZEST thus provides a practical method for
deploying high-performance, adaptable embeddings in constrained environments.

</details>


### [67] [L0: Reinforcement Learning to Become General Agents](https://arxiv.org/pdf/2506.23667)
*Junjie Zhang, Jingyi Xi, Zhuoyang Song, Junyu Lu, Yuhua Ke, Ting Sun, Yukun Yang, Jiaxing Zhang, Songxin Zhang, Zejian Xie*

Main category: cs.CL

TL;DR: L-Zero (L0) is a scalable, end-to-end training pipeline for general-purpose agents, improving LLM performance on multi-turn tasks using Reinforcement Learning with Verifiable Rewards (RLVR).


<details>
  <summary>Details</summary>
Motivation: Addressing scalability and training efficiency challenges in training LLMs for autonomous, multi-turn tasks.

Method: Introduces L0 with a low-cost, sandboxed concurrent agent worker pool and NB-Agent, a code-as-action scaffold. Evaluated on factuality benchmarks.

Result: Boosts accuracy on SimpleQA from 30% to 80% and on HotpotQA from 22% to 41% using RLVR.

Conclusion: L0 is effective for training robust agents, with open-sourced resources available.

Abstract: Training large language models (LLMs) to act as autonomous agents for
multi-turn, long-horizon tasks remains significant challenges in scalability
and training efficiency. To address this, we introduce L-Zero (L0), a scalable,
end-to-end training pipeline for general-purpose agents. Featuring a low-cost,
extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier
for applying reinforcement learning in complex environments. We also introduce
NB-Agent, the agent scaffold within L0, which operates in a "code-as-action"
fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality
question-answering benchmarks. Our experiments demonstrate that a base model
can develop robust problem-solving skills using solely Reinforcement Learning
with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method
boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41
%. We have open-sourced the entire L0 system, including our L0 series models,
the NB-Agent, a complete training pipeline, and the corresponding training
recipes on (https://github.com/cmriat/l0).

</details>


### [68] [AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data](https://arxiv.org/pdf/2506.23735)
*JiaRu Wu, Mingwei Liu*

Main category: cs.CL

TL;DR: AutoEvoEval is an evolution-based framework for evaluating LLMs, introducing 22 atomic operations to generate diverse test samples, revealing significant accuracy drops and model sensitivities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs are static and lack systematic control over perturbations, limiting robustness assessment.

Method: Proposes AutoEvoEval with 22 interpretable atomic evolution operations and multi-round compositions for controlled test sample generation.

Result: Atomic operations cause a 7.283% accuracy drop, with structure-disrupting edits being most harmful. Multi-step perturbations amplify adversarial effects by up to 52.932%.

Conclusion: Current benchmarks may overestimate model generalization, highlighting the need for evolution-aware robustness evaluation.

Abstract: Large language models (LLMs) have shown remarkable performance on various
tasks, but existing evaluation benchmarks are often static and insufficient to
fully assess their robustness and generalization in realistic scenarios. Prior
work using evolutionary or adversarial data augmentation has improved
evaluation diversity but lacks systematic control over perturbation types and
multi-step complexity, limiting comprehensive robustness analysis. To address
these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for
close-ended tasks such as multi-choice question answering. AutoEvoEval
introduces 22 interpretable atomic evolution operations and supports
multi-round compositions, enabling controlled generation of diverse,
challenging, and realistic test samples. We conduct extensive experiments
addressing four research questions on a broad set of open- and closed-source
LLMs. Our results show that atomic operations cause an average accuracy drop of
7.283\%, with structure-disrupting or misleading semantic edits causing the
largest declines. Model sensitivities vary significantly for the same
perturbation, and combining multiple evolution steps amplifies adversarial
effects by up to 52.932\%. These findings suggest current benchmarks may
overestimate true model generalization and emphasize the need for
evolution-aware robustness evaluation. Code and resources are available at:
https://github.com/SYSUSELab/AutoEvoEval.

</details>


### [69] [Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences](https://arxiv.org/pdf/2506.23743)
*Tiziano Labruna, Simone Gallo, Giovanni Da San Martino*

Main category: cs.CL

TL;DR: The study quantifies positional bias in large language models, showing it grows with answer uncertainty.


<details>
  <summary>Details</summary>
Motivation: To understand how model decisions are influenced by the order of answer options under varying uncertainty.

Method: Adapted SQuAD-it dataset with incorrect options and tested on WebGPT and Winning Arguments benchmarks, flipping answer order to measure bias.

Result: Positional bias is minimal in low-uncertainty scenarios but increases exponentially with higher uncertainty.

Conclusion: Answer ordering significantly impacts model decisions under uncertainty, highlighting a need for bias mitigation.

Abstract: Positional bias in binary question answering occurs when a model
systematically favors one choice over another based solely on the ordering of
presented options. In this study, we quantify and analyze positional bias
across five large language models under varying degrees of answer uncertainty.
We re-adapted the SQuAD-it dataset by adding an extra incorrect answer option
and then created multiple versions with progressively less context and more
out-of-context answers, yielding datasets that range from low to high
uncertainty. Additionally, we evaluate two naturally higher-uncertainty
benchmarks: (1) WebGPT - question pairs with unequal human-assigned quality
scores, and (2) Winning Arguments - where models predict the more persuasive
argument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order
of the "correct" (or higher-quality/persuasive) option is systematically
flipped (first placed in position 1, then in position 2) to compute both
Preference Fairness and Position Consistency. We observe that positional bias
is nearly absent under low-uncertainty conditions, but grows exponentially when
it becomes doubtful to decide which option is correct.

</details>


### [70] [Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model](https://arxiv.org/pdf/2506.23840)
*Bowen Ding, Yuhan Chen, Futing Wang, Lingfeng Ming, Tao Lin*

Main category: cs.CL

TL;DR: DuP-PO addresses the 'thinking trap' in LRMs by optimizing token efficiency and reasoning performance through balanced sampling, dynamic token regulation, and stable policy shaping.


<details>
  <summary>Details</summary>
Motivation: LRMs waste tokens on unnecessary high-level reasoning for simple tasks, reducing efficiency. The 'thinking trap' hinders performance under token constraints.

Method: Proposes DuP-PO with: 1) balanced rollout sampling, 2) dynamic advantage control, and 3) stable policy shaping.

Result: DuP-PO improves token efficiency and reasoning performance on five math benchmarks, outperforming the base LRM.

Conclusion: DuP-PO effectively mitigates the thinking trap, enhancing LRM efficiency and accuracy.

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems but face an
overthinking dilemma. When handling simple tasks, they often produce verbose
responses overloaded with thinking tokens (e.g., wait, however). These tokens
trigger unnecessary high-level reasoning behaviors like reflection and
backtracking, reducing efficiency. In this work, our pilot study reveals that
these thinking-token-induced behaviors are not essential for effective
problem-solving and may even hinder correct reasoning within constrained token
budgets. We identify this phenomenon as the thinking trap. To mitigate this
issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel
algorithm featuring: (1) A rollout sampling strategy that guarantees balanced
exposure to responses with and without thinking tokens; (2) A fine-grained
advantage control technique to dynamically regulate the prediction of target
tokens; (3) A policy shaping method ensuring stable gradient contributions from
thinking tokens. Experimental results on five popular math reasoning benchmarks
show that DuP-PO performs well on the popular LRM, which significantly improves
their token efficiency during reasoning, while achieving superior performance
of the base model.

</details>


### [71] [Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It](https://arxiv.org/pdf/2506.23864)
*Seyed Mahed Mousavi, Edoardo Cecchinato, Lucia Hornikova, Giuseppe Riccardi*

Main category: cs.CL

TL;DR: The paper audits reasoning benchmarks (SocialIQa, FauxPas-EAI, ToMi), revealing flaws in design and evaluation. LLMs highlight issues like duplicates, ambiguity, and scoring biases. Human re-evaluation shows scores improve due to wording, not reasoning. Performance is sensitive to input variations, questioning benchmark validity.


<details>
  <summary>Details</summary>
Motivation: To expose flaws in reasoning benchmarks and evaluation methods, challenging claims about LLM reasoning abilities.

Method: Audit benchmarks using five LLMs (GPT-3, 3.5, 4, o1, LLaMA 3.1) and human annotation to identify issues. Re-evaluate cleaned subsets.

Result: Benchmarks contain structural, semantic, and pragmatic flaws. Model scores improve due to wording, not reasoning. Performance is sensitive to minor input changes.

Conclusion: Current benchmarks may misrepresent LLM reasoning. Need for evaluation protocols focusing on inference processes, not output form. Audited data and tools released for better assessments.

Abstract: We conduct a systematic audit of three widely used reasoning benchmarks,
SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark
items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and
LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic
issues in benchmark design (e.g., duplicated items, ambiguous wording, and
implausible answers), as well as scoring procedures that prioritize output form
over reasoning process. Through systematic human annotation and re-evaluation
on cleaned benchmark subsets, we find that model scores often improve not due
to due to erratic surface wording variations and not to improved reasoning.
Infact, further analyses show that model performance is highly sensitive to
minor input variations such as context availability and phrasing, revealing
that high scores may reflect alignment with format-specific cues rather than
consistent inference based on the input. These findings challenge the validity
of current benchmark-based claims about reasoning in LLMs, and highlight the
need for evaluation protocols that assess reasoning as a process of drawing
inference from available information, rather than as static output selection.
We release audited data and evaluation tools to support more interpretable and
diagnostic assessments of model reasoning.

</details>


### [72] [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/pdf/2506.23888)
*André de Souza Loureiro, Jorge Valverde-Rebaza, Julieta Noguez, David Escarcega, Ricardo Marcacini*

Main category: cs.CL

TL;DR: MAPS framework enhances multi-step reasoning in LLMs by combining CoT, Self-Reflection, and Auto-Prompting, outperforming standard methods while balancing cost and performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-step reasoning tasks despite advancements, necessitating improved methods like MAPS.

Method: MAPS integrates CoT, Self-Reflection, and Auto-Prompting in an iterative refinement process with adaptive prompts for error correction.

Result: MAPS outperforms standard CoT and matches specialized reasoning models on benchmarks, with controlled reflection depth for cost efficiency.

Conclusion: MAPS effectively improves LLM reasoning, balancing accuracy and cost, and generalizes well across models.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved their problem-solving capabilities. However, these models still
struggle when faced with complex multi-step reasoning tasks. In this paper, we
propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,
a novel approach designed to enhance multi-step mathematical reasoning in LLMs
by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and
Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an
iterative refinement process. Initially, the model generates a solution using
CoT prompting. When errors are detected, an adaptive self-reflection mechanism
identifies and analyzes them, generating tailored prompts to guide corrections.
These dynamically adjusted prompts enable the model to iteratively refine its
reasoning. Experiments on four well-established benchmarks across multiple LLMs
show that MAPS significantly outperforms standard CoT and achieves competitive
results with reasoning-optimized models. In addition, MAPS enables
general-purpose LLMs to reach performance levels comparable to specialized
reasoning models. While deeper reflection layers improve accuracy, they also
increase token usage and costs. To balance this trade-off, MAPS strategically
limits reflection depth, ensuring an optimal balance between cost and reasoning
performance.

</details>


### [73] [The Trilemma of Truth in Large Language Models](https://arxiv.org/pdf/2506.23921)
*Germans Savcisens, Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: The paper introduces sAwMIL, a method to assess the veracity of LLMs' internal knowledge, revealing insights about truth signals and probing performance.


<details>
  <summary>Details</summary>
Motivation: To address flawed assumptions in existing methods for probing LLMs' knowledge and provide a reliable way to verify their internal probabilistic knowledge.

Method: Introduces sAwMIL, a probing method using multiple-instance learning and conformal prediction to classify statements as true, false, or neither based on LLM activations.

Result: Found concentrated veracity signals in LLMs, asymmetry in truth/falsehood signals, and varying probe performance across models. Also identified a third signal type (neither true nor false).

Conclusion: sAwMIL offers a reliable way to verify LLM knowledge, with findings aiding future research on LLM truth assessment.

Abstract: We often attribute human characteristics to large language models (LLMs) and
claim that they "know" certain things. LLMs have an internal probabilistic
knowledge that represents information retained during training. How can we
assess the veracity of this knowledge? We examine two common methods for
probing the veracity of LLMs and discover several assumptions that are flawed.
To address these flawed assumptions, we introduce sAwMIL (short for Sparse
Aware Multiple-Instance Learning), a probing method that utilizes the internal
activations of LLMs to separate statements into true, false, and neither.
sAwMIL is based on multiple-instance learning and conformal prediction. We
evaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including
both default and chat-based variants, as well as on 3 new datasets. Among the
insights we provide are: (1) the veracity signal is often concentrated in the
third quarter of an LLM's depth; (2) truth and falsehood signals are not always
symmetric; (3) linear probes perform better on chat models than on default
models; (4) nonlinear probes may be required to capture veracity signals for
some LLMs with reinforcement learning from human feedback or knowledge
distillation; and (5) LLMs capture a third type of signal that is distinct from
true and false and is neither true nor false. These findings provide a reliable
method for verifying what LLMs "know" and how certain they are of their
probabilistic internal knowledge.

</details>


### [74] [IMPACT: Inflectional Morphology Probes Across Complex Typologies](https://arxiv.org/pdf/2506.23929)
*Mohammed J. Saeed, Tommi Vehvilainen, Evgeny Fedoseev, Sevil Caliskan, Tatiana Vodolazova*

Main category: cs.CL

TL;DR: IMPACT is a synthetic evaluation framework for assessing LLMs' understanding of inflectional morphology in five morphologically rich languages, revealing gaps in their linguistic complexity handling.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs truly grasp the linguistic complexity of non-English languages, particularly in morphology, despite their fluent outputs.

Method: Introduces IMPACT, a synthetic framework with unit-test-style cases for evaluating LLMs across five languages (Arabic, Russian, Finnish, Turkish, Hebrew), covering shared and language-specific morphological phenomena.

Result: Eight multilingual LLMs struggle with non-English languages and uncommon morphological patterns, especially in judging ungrammatical examples. Chain of Thought and Thinking Models can degrade performance.

Conclusion: LLMs have significant gaps in handling linguistic complexity, highlighting room for improvement. The IMPACT framework is released publicly to support further research.

Abstract: Large Language Models (LLMs) have shown significant progress on various
multilingual benchmarks and are increasingly used to generate and evaluate text
in non-English languages. However, while they may produce fluent outputs, it
remains unclear to what extent these models truly grasp the underlying
linguistic complexity of those languages, particularly in morphology. To
investigate this, we introduce IMPACT, a synthetically generated evaluation
framework focused on inflectional morphology, which we publicly release,
designed to evaluate LLM performance across five morphologically rich
languages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes
unit-test-style cases covering both shared and language-specific phenomena,
from basic verb inflections (e.g., tense, number, gender) to unique features
like Arabic's reverse gender agreement and vowel harmony in Finnish and
Turkish. We assess eight multilingual LLMs that, despite strong English
performance, struggle with other languages and uncommon morphological patterns,
especially when judging ungrammatical examples. We also show that Chain of
Thought and Thinking Models can degrade performance. Our work exposes gaps in
LLMs' handling of linguistic complexity, pointing to clear room for
improvement. To support further research, we publicly release the IMPACT
framework.

</details>


### [75] [Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages](https://arxiv.org/pdf/2506.23930)
*Ruhina Tabasshum Prome, Tarikul Islam Tamiti, Anomadarshi Barua*

Main category: cs.CL

TL;DR: The paper explores prompt engineering on LLMs for hate speech detection in low-resource languages, introducing metaphor prompting and evaluating six strategies on Llama2-7B.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of large-scale datasets for hate speech detection in low-resource languages like Bengali.

Method: Six prompting strategies (zero-shot, refusal suppression, flattering, multi-shot, role, metaphor) tested on Llama2-7B, compared with word embeddings (GloVe, Word2Vec, FastText) and deep learning models (MLP, CNN, BiGRU).

Result: Metaphor prompting outperforms other methods, validated in Bengali, Hindi, English, and German, with metrics like F1 score and environmental impact.

Conclusion: Metaphor prompting is effective for hate speech detection in low-resource languages, offering a novel approach beyond traditional jailbreaking.

Abstract: The rapid expansion of social media leads to a marked increase in hate
speech, which threatens personal lives and results in numerous hate crimes.
Detecting hate speech presents several challenges: diverse dialects, frequent
code-mixing, and the prevalence of misspelled words in user-generated content
on social media platforms. Recent progress in hate speech detection is
typically concentrated on high-resource languages. However, low-resource
languages still face significant challenges due to the lack of large-scale,
high-quality datasets. This paper investigates how we can overcome this
limitation via prompt engineering on large language models (LLMs) focusing on
low-resource Bengali language. We investigate six prompting strategies -
zero-shot prompting, refusal suppression, flattering the classifier, multi-shot
prompting, role prompting, and finally our innovative metaphor prompting to
detect hate speech effectively in low-resource languages. We pioneer the
metaphor prompting to circumvent the built-in safety mechanisms of LLMs that
marks a significant departure from existing jailbreaking methods. We
investigate all six different prompting strategies on the Llama2-7B model and
compare the results extensively with three pre-trained word embeddings - GloVe,
Word2Vec, and FastText for three different deep learning models - multilayer
perceptron (MLP), convolutional neural network (CNN), and bidirectional gated
recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in
the low-resource Bengali language, we also evaluate it in another low-resource
language - Hindi, and two high-resource languages - English and German. The
performance of all prompting techniques is evaluated using the F1 score, and
environmental impact factor (IF), which measures CO$_2$ emissions, electricity
usage, and computational time.

</details>


### [76] [Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs](https://arxiv.org/pdf/2506.23940)
*Yang Dai, Jianxiang An, Tianwei Lin, Hongyang He, Hongzhe Huang, Wenqiao Zhang, Zheqi Lv, Siliang Tang, Yueting Zhuang*

Main category: cs.CL

TL;DR: A framework for unifying knowledge across domain-specific MLLMs using parameter integration and compatibility-aware fusion.


<details>
  <summary>Details</summary>
Motivation: Addressing the fragmentation of knowledge in specialized MLLMs and enabling modular composition of expert capabilities.

Method: Proposes Compatibility-Aware Parameter Splicing (CAPS) for selective parameter fusion, with domain compatibility scoring for alignment.

Result: Validated effectiveness across diverse multimodal benchmarks, enabling scalable, domain-adaptive MLLMs.

Conclusion: The framework successfully integrates heterogeneous expertise while maintaining modularity, offering a scalable solution.

Abstract: Multimodal Large Language Models (MLLMs) have achieved success across various
domains. However, their applicability tends to degrade when confronted with
different types of data inputs, especially for MLLMs that have been fine-tuned
for specific tasks. Despite its importance, the study of knowledge sharing
among domain-specific MLLMs--such as those trained for mathematics or
code--remains largely underexplored. To address the fragmentation of knowledge
across domain-specialized MLLMs, we propose a unified parameter integration
framework that enables modular composition of expert capabilities. Our method
is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,
which leverages both local functional attribution and global
information-theoretic signals to guide selective parameter fusion. By extending
this mechanism to the low-rank adaptation layer granularity, we ensure
efficient integration with minimal inference overhead. Furthermore, we
introduce a domain compatibility scoring mechanism that quantifies inter-expert
alignment at the activation level and correlates with downstream task utility.
This principled fusion protocol allows the final model to synergize
heterogeneous expertise while preserving structural modularity. Extensive
evaluations across diverse multimodal benchmarks validate the effectiveness of
our framework, offering a scalable path toward compositional, domain-adaptive
MLLMs.

</details>


### [77] [Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders](https://arxiv.org/pdf/2506.23951)
*Mathis Le Bail, Jérémie Dentan, Davide Buscaldi, Sonia Vanier*

Main category: cs.CL

TL;DR: The paper explores Sparse Autoencoders (SAEs) for interpretable concept extraction in sentence classification, proposing a novel SAE-based architecture with improved causality and interpretability.


<details>
  <summary>Details</summary>
Motivation: To extend SAE-based explainability methods to sentence classification, a domain less explored, and enhance interpretability of LLM features.

Method: Introduces a tailored SAE architecture with a specialized classifier head and activation rate sparsity loss, benchmarked against ConceptShap, ICA, and other SAE techniques.

Result: The proposed architecture outperforms benchmarks in causality and interpretability, validated on two classification tasks and four Pythia LLMs.

Conclusion: The novel SAE-based approach effectively improves interpretability and causality in sentence classification, supported by new metrics.

Abstract: Sparse Autoencoders (SAEs) have been successfully used to probe Large
Language Models (LLMs) and extract interpretable concepts from their internal
representations. These concepts are linear combinations of neuron activations
that correspond to human-interpretable features. In this paper, we investigate
the effectiveness of SAE-based explainability approaches for sentence
classification, a domain where such methods have not been extensively explored.
We present a novel SAE-based architecture tailored for text classification,
leveraging a specialized classifier head and incorporating an activation rate
sparsity loss. We benchmark this architecture against established methods such
as ConceptShap, Independent Component Analysis, and other SAE-based concept
extraction techniques. Our evaluation covers two classification benchmarks and
four fine-tuned LLMs from the Pythia family. We further enrich our analysis
with two novel metrics for measuring the precision of concept-based
explanations, using an external sentence encoder. Our empirical results show
that our architecture improves both the causality and interpretability of the
extracted features.

</details>


### [78] [TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation](https://arxiv.org/pdf/2506.23979)
*Renren Jin, Tianhao Shen, Xinwei Wu, Dan Shi, Haoran Sun, Wuwei Huang, Quandong Wang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong*

Main category: cs.CL

TL;DR: The TaP framework automates and scales preference dataset construction for multilingual LLM fine-tuning, outperforming larger open-source datasets.


<details>
  <summary>Details</summary>
Motivation: High-quality datasets for LLM fine-tuning are resource-intensive and mostly English-only, limiting multilingual alignment with human preferences.

Method: TaP uses a structured taxonomy for automated, scalable preference dataset generation across languages, ensuring diversity and coverage.

Result: LLMs fine-tuned with TaP datasets outperform those using larger open-source datasets, even when the latter is 180 times bigger.

Conclusion: TaP offers an efficient, scalable solution for multilingual preference dataset generation, enhancing LLM performance in instruction-following and alignment.

Abstract: Conducting supervised fine-tuning and preference fine-tuning on large
language models (LLMs) requires high-quality datasets to improve their ability
to follow instructions and align with human preferences and values. However,
constructing such datasets is resource-intensive, and most available datasets
for supervised and preference fine-tuning are in English. To address these
challenges, we propose the \underline{\textbf{Ta}}xonomy-Guided
\underline{\textbf{P}}reference Data Generation (TaP) framework, which
facilitates automated and scalable construction of preference datasets across
various languages. TaP is grounded in a structured taxonomy that allows
fine-grained control over dataset composition, thereby ensuring both diversity
and comprehensive coverage. We employ TaP-generated datasets to perform
supervised and preference fine-tuning on various LLMs. Experimental results
demonstrate that LLMs trained on TaP-generated datasets outperform those
trained on existing open-source datasets. Remarkably, LLMs trained on
TaP-generated datasets surpass the performance of those trained on an
open-source dataset that is 180 times larger.

</details>


### [79] [Machine Understanding of Scientific Language](https://arxiv.org/pdf/2506.23990)
*Dustin Wright*

Main category: cs.CL

TL;DR: This thesis focuses on developing tools and methods for machine understanding of scientific language to identify faithfulness in scientific text, addressing issues like misinformation and exaggerated claims.


<details>
  <summary>Details</summary>
Motivation: The proliferation of scientific text online has made it crucial to automatically assess its faithfulness to underlying science, as misinformation can hinder knowledge dissemination.

Method: The thesis contributes to NLP and ML with techniques like automatic fact-checking, adversarial claim generation, domain adaptation, and zero-shot scientific fact-checking.

Result: New methods and datasets were developed to analyze scientific communication, detect misinformation, and generate insights into science communication processes.

Conclusion: The research provides effective tools for learning from limited scientific text to identify misinformation and improve science communication.

Abstract: Scientific information expresses human understanding of nature. This
knowledge is largely disseminated in different forms of text, including
scientific papers, news articles, and discourse among people on social media.
While important for accelerating our pursuit of knowledge, not all scientific
text is faithful to the underlying science. As the volume of this text has
burgeoned online in recent years, it has become a problem of societal
importance to be able to identify the faithfulness of a given piece of
scientific text automatically. This thesis is concerned with the cultivation of
datasets, methods, and tools for machine understanding of scientific language,
in order to analyze and understand science communication at scale. To arrive at
this, I present several contributions in three areas of natural language
processing and machine learning: automatic fact checking, learning with limited
data, and scientific text processing. These contributions include new methods
and resources for identifying check-worthy claims, adversarial claim
generation, multi-source domain adaptation, learning from crowd-sourced labels,
cite-worthiness detection, zero-shot scientific fact checking, detecting
exaggerated scientific claims, and modeling degrees of information change in
science communication. Critically, I demonstrate how the research outputs of
this thesis are useful for effectively learning from limited amounts of
scientific text in order to identify misinformative scientific statements and
generate new insights into the science communication process

</details>


### [80] [Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning](https://arxiv.org/pdf/2506.23998)
*Seungjun Yi, Joakim Nguyen, Huimin Xu, Terence Lim, Andrew Well, Mia Markey, Ying Ding*

Main category: cs.CL

TL;DR: An automated LLM pipeline for thematic analysis of clinical narratives in CHD, reducing manual effort and improving scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional thematic analysis is labor-intensive and unscalable for CHD narratives.

Method: A multi-agent LLM framework with optional RLHF for enhanced thematic relevance.

Result: Enables scalable, patient-centered analysis of qualitative datasets.

Conclusion: The system supports fine-tuning LLMs for clinical contexts, improving thematic alignment with human analysis.

Abstract: Congenital heart disease (CHD) presents complex, lifelong challenges often
underrepresented in traditional clinical metrics. While unstructured narratives
offer rich insights into patient and caregiver experiences, manual thematic
analysis (TA) remains labor-intensive and unscalable. We propose a fully
automated large language model (LLM) pipeline that performs end-to-end TA on
clinical narratives, which eliminates the need for manual coding or full
transcript review. Our system employs a novel multi-agent framework, where
specialized LLM agents assume roles to enhance theme quality and alignment with
human analysis. To further improve thematic relevance, we optionally integrate
reinforcement learning from human feedback (RLHF). This supports scalable,
patient-centered analysis of large qualitative datasets and allows LLMs to be
fine-tuned for specific clinical contexts.

</details>


### [81] [Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective](https://arxiv.org/pdf/2506.24006)
*Anselm R. Strohmaier, Wim Van Dooren, Kathrin Seßler, Brian Greer, Lieven Verschaffel*

Main category: cs.CL

TL;DR: The paper examines LLMs' ability to solve math word problems, finding they excel at superficial tasks but struggle with real-world context, limiting their educational utility.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' potential in math education, focusing on their competence in solving word problems and understanding real-world context.

Method: A scoping review with three parts: technical overview, systematic literature review of word-problem corpora, and empirical evaluation of LLMs on word problems.

Result: LLMs perform well on superficial word problems (s-problems) but fail when real-world context is problematic, scoring perfectly on PISA problems but showing limitations elsewhere.

Conclusion: LLMs lack deep understanding of word problems, limiting their effectiveness as instructional tools in math education.

Abstract: The progress of Large Language Models (LLMs) like ChatGPT raises the question
of how they can be integrated into education. One hope is that they can support
mathematics learning, including word-problem solving. Since LLMs can handle
textual input with ease, they appear well-suited for solving mathematical word
problems. Yet their real competence, whether they can make sense of the
real-world context, and the implications for classrooms remain unclear. We
conducted a scoping review from a mathematics-education perspective, including
three parts: a technical overview, a systematic review of word problems used in
research, and a state-of-the-art empirical evaluation of LLMs on mathematical
word problems. First, in the technical overview, we contrast the
conceptualization of word problems and their solution processes between LLMs
and students. In computer-science research this is typically labeled
mathematical reasoning, a term that does not align with usage in mathematics
education. Second, our literature review of 213 studies shows that the most
popular word-problem corpora are dominated by s-problems, which do not require
a consideration of realities of their real-world context. Finally, our
evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems
shows that most recent LLMs solve these s-problems with near-perfect accuracy,
including a perfect score on 20 problems from PISA. LLMs still showed
weaknesses in tackling problems where the real-world context is problematic or
non-sensical. In sum, we argue based on all three aspects that LLMs have
mastered a superficial solution process but do not make sense of word problems,
which potentially limits their value as instructional tools in mathematics
classrooms.

</details>


### [82] [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/pdf/2506.24016)
*Hyunjong Kim, Sangyeop Kim, Jongheon Jeong, Yeongjae Cho, Sungzoon Cho*

Main category: cs.CL

TL;DR: EXPERT is a reference-free evaluation metric for image captioning, providing structured explanations based on fluency, relevance, and descriptiveness. It outperforms existing metrics in both scoring and explanation quality.


<details>
  <summary>Details</summary>
Motivation: Current explainable evaluation metrics lack standardized criteria and verified explanation quality.

Method: EXPERT uses a two-stage evaluation template to supervise a vision-language model, leveraging large-scale datasets of structured explanations.

Result: EXPERT achieves state-of-the-art results on benchmarks and higher-quality explanations, validated by human evaluation.

Conclusion: EXPERT sets a new standard for explainable evaluation metrics in image captioning, with publicly available code and datasets.

Abstract: Recent advances in large language models and vision-language models have led
to growing interest in explainable evaluation metrics for image captioning.
However, these metrics generate explanations without standardized criteria, and
the overall quality of the generated explanations remains unverified. In this
paper, we propose EXPERT, a reference-free evaluation metric that provides
structured explanations based on three fundamental criteria: fluency,
relevance, and descriptiveness. By constructing large-scale datasets of
high-quality structured explanations, we develop a two-stage evaluation
template to effectively supervise a vision-language model for both scoring and
explanation generation. EXPERT achieves state-of-the-art results on benchmark
datasets while providing significantly higher-quality explanations than
existing metrics, as validated through comprehensive human evaluation. Our code
and datasets are available at https://github.com/hjkim811/EXPERT.

</details>


### [83] [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/pdf/2506.24068)
*Ian R. McKenzie, Oskar J. Hollinsworth, Tom Tseng, Xander Davies, Stephen Casper, Aaron D. Tucker, Robert Kirk, Adam Gleave*

Main category: cs.CL

TL;DR: The paper evaluates the security of AI defense pipelines, introduces a novel classifier and attack method (STACK), and suggests mitigations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation and attacks on AI defense pipelines used by frontier developers like Anthropic, Google DeepMind, and OpenAI.

Method: Developed an open-source defense pipeline, tested a few-shot-prompted classifier, and introduced the STACK attack procedure.

Result: The classifier outperformed ShieldGemma (0% ASR on ClearHarm), while STACK achieved 71% ASR in black-box and 33% in transfer settings.

Conclusion: AI defense pipelines are vulnerable; staged attacks like STACK are feasible, and specific mitigations are needed.

Abstract: Frontier AI developers are relying on layers of safeguards to protect against
catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus
model using one such defense pipeline, and other frontier developers including
Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the
security of such pipelines is unclear, with limited prior work evaluating or
attacking these pipelines. We address this gap by developing and red-teaming an
open-source defense pipeline. First, we find that a novel few-shot-prompted
input and output classifier outperforms state-of-the-art open-weight safeguard
model ShieldGemma across three attacks and two datasets, reducing the attack
success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,
we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on
ClearHarm in a black-box attack against the few-shot-prompted classifier
pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%
ASR, providing initial evidence that it is feasible to design attacks with no
access to the target pipeline. We conclude by suggesting specific mitigations
that developers could use to thwart staged attacks.

</details>


### [84] [On the Predictive Power of Representation Dispersion in Language Models](https://arxiv.org/pdf/2506.24106)
*Yanhong Li, Ming Li, Karen Livescu, Jiawei Zhou*

Main category: cs.CL

TL;DR: Language models with wider embedding spaces (higher representation dispersion) achieve lower perplexity, and this dispersion can be used for practical tasks like model selection and improving retrieval-based methods.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between a model's embedding space breadth (dispersion) and its predictive performance (perplexity), and to leverage this for practical applications.

Method: Analyze representation dispersion (average pairwise cosine distance) across models (LLaMA, Qwen) and domains (Wikipedia, news, scientific abstracts). Use dispersion for tasks like predicting downstream accuracy, optimizing retrieval methods, and enhancing training with a push-away objective.

Result: Higher dispersion strongly correlates with lower perplexity. Dispersion aids in model selection, identifies optimal layers for retrieval, and improves perplexity when integrated into training.

Conclusion: Representation dispersion is a key factor in model performance and can be practically utilized for efficiency and accuracy improvements.

Abstract: We show that a language model's ability to predict text is tightly linked to
the breadth of its embedding space: models that spread their contextual
representations more widely tend to achieve lower perplexity. Concretely, we
find that representation dispersion - the average pairwise cosine distance
among hidden vectors - strongly and negatively correlates with perplexity
across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,
news, scientific abstracts). Beyond illustrating this link, we show how
dispersion can be leveraged for a range of practical tasks without requiring
labeled data. First, measuring dispersion on unlabeled text allows us to
predict downstream accuracy in new domains, offering a data-efficient tool for
model selection. Next, we find that identifying layers with higher dispersion
pinpoints the best representations for retrieval-based methods such as kNN-LM,
bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple
push-away objective into training, which increases dispersion in both
single-domain and cross-domain scenarios and directly improves perplexity in
each.

</details>


### [85] [Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models](https://arxiv.org/pdf/2506.24117)
*David M. Smiley*

Main category: cs.CL

TL;DR: Pre-trained transformer models (E5, AlephBERT, MPNet, LaBSE) are evaluated for detecting parallel passages in biblical Hebrew, with E5 and AlephBERT showing strong performance.


<details>
  <summary>Details</summary>
Motivation: Traditional manual comparison of biblical Hebrew passages is labor-intensive and error-prone, necessitating automated solutions.

Method: Models generate word embeddings for passages, evaluated using cosine similarity and Wasserstein Distance to detect parallels.

Result: E5 excels in parallel detection, while AlephBERT better differentiates non-parallel passages.

Conclusion: Pre-trained models improve efficiency and accuracy in detecting intertextual parallels, with potential for broader ancient language studies.

Abstract: Identifying parallel passages in biblical Hebrew is foundational in biblical
scholarship for uncovering intertextual relationships. Traditional methods rely
on manual comparison, which is labor-intensive and prone to human error. This
study evaluates the potential of pre-trained transformer-based language models,
including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in
the Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings
and Chronicles, I assessed each model's capability to generate word embeddings
that delineate parallel from non-parallel passages. Utilizing cosine similarity
and Wasserstein Distance measures, I found that E5 and AlephBERT show
significant promise, with E5 excelling in parallel detection and AlephBERT
demonstrating stronger non-parallel differentiation. These findings indicate
that pre-trained models can enhance the efficiency and accuracy of detecting
intertextual parallels in ancient texts, suggesting broader applications for
ancient language studies.

</details>


### [86] [Scaling Data-Constrained Language Models](https://arxiv.org/pdf/2305.16264)
*Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, Colin Raffel*

Main category: cs.CL

TL;DR: Scaling language models faces data constraints as internet text data may soon limit training datasets. Experiments show repeated data (up to 4 epochs) has minimal impact on loss, but excessive repetition diminishes compute value. A new scaling law accounts for repeated tokens and excess parameters. Mitigation strategies like code data augmentation are explored.


<details>
  <summary>Details</summary>
Motivation: The potential limitation of available internet text data for training large language models motivates investigating scaling in data-constrained regimes.

Method: Conducted experiments varying data repetition (up to 900B tokens) and compute budget (up to 9B parameters), analyzing loss and compute value. Proposed a scaling law for compute optimality.

Result: Up to 4 epochs of repeated data yield negligible loss changes, but excessive repetition reduces compute value. A validated scaling law addresses repeated tokens and excess parameters.

Conclusion: Data repetition is viable to a point, but new scaling laws and mitigation strategies (e.g., code augmentation) are needed for data-constrained scaling.

Abstract: The current trend of scaling language models involves increasing both
parameter count and training dataset size. Extrapolating this trend suggests
that training dataset size may soon be limited by the amount of text data
available on the internet. Motivated by this limit, we investigate scaling
language models in data-constrained regimes. Specifically, we run a large set
of experiments varying the extent of data repetition and compute budget,
ranging up to 900 billion training tokens and 9 billion parameter models. We
find that with constrained data for a fixed compute budget, training with up to
4 epochs of repeated data yields negligible changes to loss compared to having
unique data. However, with more repetition, the value of adding compute
eventually decays to zero. We propose and empirically validate a scaling law
for compute optimality that accounts for the decreasing value of repeated
tokens and excess parameters. Finally, we experiment with approaches mitigating
data scarcity, including augmenting the training dataset with code data or
removing commonly used filters. Models and datasets from our 400 training runs
are freely available at https://github.com/huggingface/datablations.

</details>


### [87] [RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing](https://arxiv.org/pdf/2404.19543)
*Yucheng Hu, Yuxing Lu*

Main category: cs.CL

TL;DR: This survey paper provides a comprehensive overview of Retrieval-Augmented Language Models (RALMs), covering their paradigm, evolution, taxonomy, applications, and evaluation methods, while also addressing limitations and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a detailed overview of RALMs (Retrieval-Augmented Generation and Retrieval-Augmented Understanding) and their role in mitigating challenges like hallucination and domain-specific knowledge gaps in LLMs.

Method: The paper examines the components of RALMs (Retrievers, Language Models, Augmentations) and their interactions, alongside applications in tasks like translation and dialogue systems. It also reviews evaluation methods focusing on robustness, accuracy, and relevance.

Result: RALMs enhance performance in NLP tasks but face limitations in retrieval quality and computational efficiency.

Conclusion: The survey offers structured insights into RALMs, their potential, and future research directions, supported by a GitHub repository for further study.

Abstract: Large Language Models (LLMs) have catalyzed significant advancements in
Natural Language Processing (NLP), yet they encounter challenges such as
hallucination and the need for domain-specific knowledge. To mitigate these,
recent methodologies have integrated information retrieved from external
resources with LLMs, substantially enhancing their performance across NLP
tasks. This survey paper addresses the absence of a comprehensive overview on
Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented
Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an
in-depth examination of their paradigm, evolution, taxonomy, and applications.
The paper discusses the essential components of RALMs, including Retrievers,
Language Models, and Augmentations, and how their interactions lead to diverse
model structures and applications. RALMs demonstrate utility in a spectrum of
tasks, from translation and dialogue systems to knowledge-intensive
applications. The survey includes several evaluation methods of RALMs,
emphasizing the importance of robustness, accuracy, and relevance in their
assessment. It also acknowledges the limitations of RALMs, particularly in
retrieval quality and computational efficiency, offering directions for future
research. In conclusion, this survey aims to offer a structured insight into
RALMs, their potential, and the avenues for their future development in NLP.
The paper is supplemented with a Github Repository containing the surveyed
works and resources for further study:
https://github.com/2471023025/RALM_Survey.

</details>


### [88] [The Effectiveness of LLMs as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation](https://arxiv.org/pdf/2405.01299)
*Maja Pavlovic, Massimo Poesio*

Main category: cs.CL

TL;DR: The paper compares twelve studies on LLMs for data annotation, highlighting their benefits and limitations, and examines human-GPT opinion alignment in subjective datasets.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' potential in data annotation and address gaps like bias and representativeness.

Method: Comparative overview of twelve studies and empirical analysis of human-GPT opinion distributions in four datasets.

Result: LLMs show cost and time benefits but have limitations like bias and prompt sensitivity. Human-GPT alignment varies.

Conclusion: Further research is needed to address LLMs' limitations and incorporate diverse perspectives in annotation tasks.

Abstract: Large Language Models (LLMs) have emerged as powerful support tools across
various natural language tasks and a range of application domains. Recent
studies focus on exploring their capabilities for data annotation. This paper
provides a comparative overview of twelve studies investigating the potential
of LLMs in labelling data. While the models demonstrate promising cost and
time-saving benefits, there exist considerable limitations, such as
representativeness, bias, sensitivity to prompt variations and English language
preference. Leveraging insights from these studies, our empirical analysis
further examines the alignment between human and GPT-generated opinion
distributions across four subjective datasets. In contrast to the studies
examining representation, our methodology directly obtains the opinion
distribution from GPT. Our analysis thereby supports the minority of studies
that are considering diverse perspectives when evaluating data annotation tasks
and highlights the need for further research in this direction.

</details>


### [89] [Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph](https://arxiv.org/pdf/2406.15627)
*Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Kirill Grishchenkov, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, Artem Shelmanov*

Main category: cs.CL

TL;DR: A new benchmark for evaluating uncertainty quantification (UQ) techniques in large language models (LLMs) is introduced to address fragmented research and improve consistency in assessing UQ methods.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLMs has highlighted challenges like hallucinations and low-quality outputs, necessitating better UQ techniques. Current research lacks consistency in methods and evaluation.

Method: The authors propose a benchmark that includes state-of-the-art UQ baselines and supports controlled evaluation across text generation tasks, including confidence normalization assessment.

Result: A large-scale empirical study evaluates UQ and normalization techniques across eleven tasks, identifying the most effective approaches.

Conclusion: The benchmark provides a unified framework for evaluating UQ in LLMs, aiding in the development of more reliable and interpretable models.

Abstract: The rapid proliferation of large language models (LLMs) has stimulated
researchers to seek effective and efficient approaches to deal with LLM
hallucinations and low-quality outputs. Uncertainty quantification (UQ) is a
key element of machine learning applications in dealing with such challenges.
However, research to date on UQ for LLMs has been fragmented in terms of
techniques and evaluation methodologies. In this work, we address this issue by
introducing a novel benchmark that implements a collection of state-of-the-art
UQ baselines and offers an environment for controllable and consistent
evaluation of novel UQ techniques over various text generation tasks. Our
benchmark also supports the assessment of confidence normalization methods in
terms of their ability to provide interpretable scores. Using our benchmark, we
conduct a large-scale empirical investigation of UQ and normalization
techniques across eleven tasks, identifying the most effective approaches.
Code: https://github.com/IINemo/lm-polygraph Benchmark:
https://huggingface.co/LM-Polygraph

</details>


### [90] [Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement](https://arxiv.org/pdf/2407.01461)
*Xiaohua Wang, Zisu Huang, Feiran Zhang, Zhibo Xu, Cenyuan Zhang, Qi Qian, Xiaoqing Zheng, Xuanjing Huang*

Main category: cs.CL

TL;DR: A framework refines user prompts to enhance LLM responses and robustness against harmful inputs.


<details>
  <summary>Details</summary>
Motivation: Improve LLM response quality and robustness against adversarial jailbreak prompts.

Method: Proposes a lightweight query refinement model trained via multi-objective reinforcement learning.

Result: Enhances response quality and robustness against jailbreak attacks.

Conclusion: The framework effectively improves LLM performance and safety.

Abstract: The capacity of large language models (LLMs) to generate honest, harmless,
and helpful responses heavily relies on the quality of user prompts. However,
these prompts often tend to be brief and vague, thereby significantly limiting
the full potential of LLMs. Moreover, harmful prompts can be meticulously
crafted and manipulated by adversaries to jailbreak LLMs, inducing them to
produce potentially toxic content. To enhance the capabilities of LLMs while
maintaining strong robustness against harmful jailbreak inputs, this study
proposes a transferable and pluggable framework that refines user prompts
before they are input into LLMs. This strategy improves the quality of the
queries, empowering LLMs to generate more truthful, benign and useful
responses. Specifically, a lightweight query refinement model is introduced and
trained using a specially designed reinforcement learning approach that
incorporates multiple objectives to enhance particular capabilities of LLMs.
Extensive experiments demonstrate that the refinement model not only improves
the quality of responses but also strengthens their robustness against
jailbreak attacks. Code is available at:
https://github.com/Huangzisu/query-refinement .

</details>


### [91] [ChipXplore: Natural Language Exploration of Hardware Designs and Libraries](https://arxiv.org/pdf/2407.12749)
*Manar Abdelatty, Jacob Rosenstein, Sherief Reda*

Main category: cs.CL

TL;DR: ChipXplore is a multi-agent framework using LLMs to automate and optimize hardware design queries via natural language, improving accuracy and productivity.


<details>
  <summary>Details</summary>
Motivation: Manual navigation between hardware designs and PDKs is error-prone and inefficient.

Method: Uses text-to-SQL and text-to-Cypher workflows for structured data retrieval.

Result: Achieves 97.39% accuracy, 5.63x faster retrieval, and 29.78% higher accuracy than generic workflows.

Conclusion: ChipXplore enables efficient, autonomous hardware design tasks with PDK awareness.

Abstract: Hardware design workflows rely on Process Design Kits (PDKs) from different
fabrication nodes, each containing standard cell libraries optimized for speed,
power, or density. Engineers typically navigate between the design and target
PDK to make informed decisions, such as selecting gates for area optimization
or enhancing the speed of the critical path. However, this process is often
manual, time-consuming, and prone to errors. To address this, we present
ChipXplore, a multi-agent collaborative framework powered by large language
models that enables engineers to query hardware designs and PDKs using natural
language. By exploiting the structured nature of PDK and hardware design data,
ChipXplore retrieves relevant information through text-to-SQL and
text-to-Cypher customized workflows. The framework achieves an execution
accuracy of 97.39\% in complex natural language queries and improves
productivity by making retrieval 5.63x faster while reducing errors by 5.25x in
user studies. Compared to generic workflows, ChipXplore's customized workflow
is capable of orchestrating reasoning and planning over multiple databases,
improving accuracy by 29.78\%. ChipXplore lays the foundation for building
autonomous agents capable of tackling diverse physical design tasks that
require PDK and hardware design awareness.

</details>


### [92] [Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition](https://arxiv.org/pdf/2407.21536)
*Jiang Li, Xiaoping Wang, Zhigang Zeng*

Main category: cs.CL

TL;DR: GraphSmile is a novel approach for multimodal emotion recognition in conversation (MERC) and sentiment analysis (MSAC), addressing challenges like incomplete cross-modal modeling, fusion conflicts, and dynamic sentiment detection.


<details>
  <summary>Details</summary>
Motivation: Existing MERC methods struggle with inter-modal cues, fusion conflicts, and dynamic sentiment shifts, limiting their effectiveness.

Method: GraphSmile uses GSF (graph-based fusion) and SDP (sentiment dynamics prediction) modules to capture cross-modal cues and detect sentiment changes.

Result: GraphSmile outperforms baselines in handling complex emotional and sentimental patterns.

Conclusion: GraphSmile effectively addresses MERC and MSAC challenges, demonstrating superior performance.

Abstract: Multimodal emotion recognition in conversation (MERC) has garnered
substantial research attention recently. Existing MERC methods face several
challenges: (1) they fail to fully harness direct inter-modal cues, possibly
leading to less-than-thorough cross-modal modeling; (2) they concurrently
extract information from the same and different modalities at each network
layer, potentially triggering conflicts from the fusion of multi-source data;
(3) they lack the agility required to detect dynamic sentimental changes,
perhaps resulting in inaccurate classification of utterances with abrupt
sentiment shifts. To address these issues, a novel approach named GraphSmile is
proposed for tracking intricate emotional cues in multimodal dialogues.
GraphSmile comprises two key components, i.e., GSF and SDP modules. GSF
ingeniously leverages graph structures to alternately assimilate inter-modal
and intra-modal emotional dependencies layer by layer, adequately capturing
cross-modal cues while effectively circumventing fusion conflicts. SDP is an
auxiliary task to explicitly delineate the sentiment dynamics between
utterances, promoting the model's ability to distinguish sentimental
discrepancies. GraphSmile is effortlessly applied to multimodal sentiment
analysis in conversation (MSAC), thus enabling simultaneous execution of MERC
and MSAC tasks. Empirical results on multiple benchmarks demonstrate that
GraphSmile can handle complex emotional and sentimental patterns, significantly
outperforming baseline models.

</details>


### [93] [CTISum: A New Benchmark Dataset For Cyber Threat Intelligence Summarization](https://arxiv.org/pdf/2408.06576)
*Wei Peng, Junmei Ding, Wei Wang, Lei Cui, Wei Cai, Zhiyu Hao, Xiaochun Yun*

Main category: cs.CL

TL;DR: The paper introduces CTISum, a benchmark dataset for Cyber Threat Intelligence (CTI) summarization, addressing the lack of suitable datasets. It also proposes a fine-grained subtask for attack process summarization and benchmarks various summarization methods, revealing challenges for current models.


<details>
  <summary>Details</summary>
Motivation: The lack of suitable datasets hinders the development of efficient CTI summarization techniques, which are crucial for actionable insights in cybersecurity.

Method: A multi-stage annotation pipeline collects and annotates CTI data from diverse web sources. The dataset is benchmarked using extractive, abstractive, and LLMs-based summarization methods.

Result: Current state-of-the-art models face significant challenges in summarizing CTI reports, indicating the problem remains open.

Conclusion: CTISum fills a critical gap in CTI summarization, and the proposed subtask aids in risk assessment and vulnerability identification. The dataset and code are publicly available.

Abstract: Cyber Threat Intelligence (CTI) summarization involves generating concise and
accurate highlights from web intelligence data, which is critical for providing
decision-makers with actionable insights to swiftly detect and respond to cyber
threats in the cybersecurity domain. Despite that, the development of efficient
techniques for summarizing CTI reports, comprising facts, analytical insights,
attack processes, and more, has been hindered by the lack of suitable datasets.
To address this gap, we introduce CTISum, a new benchmark dataset designed for
the CTI summarization task. Recognizing the significance of understanding
attack processes, we also propose a novel fine-grained subtask: attack process
summarization, which aims to help defenders assess risks, identify security
gaps, and uncover vulnerabilities. Specifically, a multi-stage annotation
pipeline is designed to collect and annotate CTI data from diverse web sources,
alongside a comprehensive benchmarking of CTISum using both extractive,
abstractive and LLMs-based summarization methods. Experimental results reveal
that current state-of-the-art models face significant challenges when applied
to CTISum, highlighting that automatic summarization of CTI reports remains an
open research problem. The code and example dataset can be made publicly
available at https://github.com/pengwei-iie/CTISum.

</details>


### [94] [Emotional RAG LLMs: Reading Comprehension for the Open Internet](https://arxiv.org/pdf/2408.11189)
*Benjamin Reichman, Adar Avsian, Kartik Talamadupula, Toshish Jawale, Larry Heck*

Main category: cs.CL

TL;DR: The paper addresses the gap between neutral benchmark datasets and real-world diverse-text RAG applications by introducing a dataset with emotional/sarcastic text, an emotion translation model, and a prompt-based method for better LLM interpretation.


<details>
  <summary>Details</summary>
Motivation: Real-world RAG systems retrieve diverse-toned internet text, unlike neutral benchmark datasets, creating challenges for LLMs.

Method: Developed a dataset with emotionally inflected/sarcastic text, an emotion translation model, and a prompt-based method for LLMs.

Result: Provides tools to adapt LLMs to diverse linguistic styles and improve pragmatic interpretation of retrieved text.

Conclusion: The work bridges the gap between benchmark and real-world RAG applications, enhancing LLM adaptability to varied tones.

Abstract: Queries to large language models (LLMs) can be divided into two parts: the
instruction/question and the accompanying context. The context for
retrieval-augmented generation (RAG) systems in most benchmarks comes from
Wikipedia-like texts written in a neutral and factual tone. However, real-world
RAG applications often retrieve internet-based text with diverse tones and
linguistic styles, posing challenges for downstream tasks. This paper
introduces (a) a dataset that transforms RAG-retrieved passages into
emotionally inflected and sarcastic text, (b) an emotion translation model for
adapting text to different tones, and (c) a prompt-based method to improve
LLMs' pragmatic interpretation of retrieved text.

</details>


### [95] [S^3cMath: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners](https://arxiv.org/pdf/2409.01524)
*Yuchen Yan, Jin Jiang, Yang Liu, Yixin Cao, Xin Xu, Mengdi Zhang, Xunliang Cai, Jian Shao*

Main category: cs.CL

TL;DR: The paper introduces S$^3$c-Math, a series of mathematical LLMs capable of spontaneous step-level self-correction for improved reasoning.


<details>
  <summary>Details</summary>
Motivation: Current self-correction methods in LLMs rely on external interventions, not intrinsic capabilities. The aim is to enable LLMs to autonomously detect and correct errors during reasoning.

Method: A step-level sampling approach constructs self-correction data, and a training strategy equips LLMs with spontaneous step-level self-correction.

Result: S$^3$c-Math shows significant improvements on GSM8K, MATH, and other benchmarks.

Conclusion: The work pioneers spontaneous step-level self-correction in LLMs for mathematical reasoning, proving its effectiveness.

Abstract: Self-correction is a novel method that can stimulate the potential reasoning
abilities of large language models (LLMs). It involves detecting and correcting
errors during the inference process when LLMs solve reasoning problems.
However, recent works do not regard self-correction as a spontaneous and
intrinsic capability of LLMs. Instead, such correction is achieved through
post-hoc generation, external knowledge introduction, multi-model
collaboration, and similar techniques. In this paper, we propose a series of
mathematical LLMs called S$^3$c-Math, which are able to perform Spontaneous
Step-level Self-correction for Mathematical reasoning. This capability helps
LLMs to recognize whether their ongoing inference tends to contain errors and
simultaneously correct these errors to produce a more reliable response. We
proposed a method, which employs a step-level sampling approach to construct
step-wise self-correction data for achieving such ability. Additionally, we
implement a training strategy that uses above constructed data to equip LLMs
with spontaneous step-level self-correction capacities. Our data and methods
have been demonstrated to be effective across various foundation LLMs,
consistently showing significant progress in evaluations on GSM8K, MATH, and
other mathematical benchmarks. To the best of our knowledge, we are the first
to introduce the spontaneous step-level self-correction ability of LLMs in
mathematical reasoning.

</details>


### [96] [Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino](https://arxiv.org/pdf/2409.15380)
*Jann Railey Montalan, Jian Gang Ngui, Wei Qi Leong, Yosephine Susanto, Hamsawardhini Rengarajan, Alham Fikri Aji, William Chandra Tjhi*

Main category: cs.CL

TL;DR: Kalahi is a cultural evaluation suite for LLMs, designed by native Filipino speakers, to assess culturally appropriate responses. It reveals LLMs struggle with Filipino cultural nuances, scoring only 46% compared to native performance of 89.1%.


<details>
  <summary>Details</summary>
Motivation: Current multilingual LLMs often fail to provide culturally relevant responses for Filipino users, highlighting the need for a culturally specific evaluation tool.

Method: Kalahi includes 150 handcrafted prompts testing LLMs on Filipino cultural knowledge and values. Experiments were conducted on multilingual and Filipino-supported LLMs.

Result: The best LLM scored only 46.0% on Kalahi, significantly lower than the 89.10% achieved by native Filipinos, demonstrating the challenge of cultural relevance for LLMs.

Conclusion: Kalahi effectively evaluates Filipino cultural representation in LLMs, showing current models lack adequate cultural understanding for Filipino users.

Abstract: Multilingual large language models (LLMs) today may not necessarily provide
culturally appropriate and relevant responses to its Filipino users. We
introduce Kalahi, a cultural LLM evaluation suite collaboratively created by
native Filipino speakers. It is composed of 150 high-quality, handcrafted and
nuanced prompts that test LLMs for generations that are relevant to shared
Filipino cultural knowledge and values. Strong LLM performance in Kalahi
indicates a model's ability to generate responses similar to what an average
Filipino would say or do in a given situation. We conducted experiments on LLMs
with multilingual and Filipino language support. Results show that Kalahi,
while trivial for Filipinos, is challenging for LLMs, with the best model
answering only 46.0% of the questions correctly compared to native Filipino
performance of 89.10%. Thus, Kalahi can be used to accurately and reliably
evaluate Filipino cultural representation in LLMs.

</details>


### [97] [Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback](https://arxiv.org/pdf/2410.03145)
*Kyuyoung Kim, Ah Jeong Seo, Hao Liu, Jinwoo Shin, Kimin Lee*

Main category: cs.CL

TL;DR: MMPO improves LLM alignment by incorporating quality margins in preferences, outperforming baselines on benchmarks like MT-bench and RewardBench.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods rely on binary labels, missing subtle quality differences in outputs.

Method: MMPO uses soft target probabilities based on the Bradley-Terry model, trained with cross-entropy.

Result: MMPO achieves state-of-the-art performance on RewardBench and is more robust to overfitting.

Conclusion: MMPO enhances LLM policies and reward models by better capturing relative quality margins.

Abstract: Large language models (LLMs) fine-tuned with alignment techniques, such as
reinforcement learning from human feedback, have been instrumental in
developing some of the most capable AI systems to date. Despite their success,
existing methods typically rely on simple binary labels, such as those
indicating preferred outputs in pairwise preferences, which fail to capture the
subtle differences in relative quality between pairs. To address this
limitation, we introduce an approach called Margin Matching Preference
Optimization (MMPO), which incorporates relative quality margins into
optimization, leading to improved LLM policies and reward models. Specifically,
given quality margins in pairwise preferences, we design soft target
probabilities based on the Bradley-Terry model, which are then used to train
models with the standard cross-entropy objective. Experiments with both human
and AI feedback data demonstrate that MMPO consistently outperforms baseline
methods, often by a substantial margin, on popular benchmarks including
MT-bench and RewardBench. Notably, the 7B model trained with MMPO achieves
state-of-the-art performance on RewardBench as of June 2024, outperforming
other models of the same scale. Our analysis also shows that MMPO is more
robust to overfitting, leading to better-calibrated models.

</details>


### [98] [Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?](https://arxiv.org/pdf/2410.06735)
*Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo*

Main category: cs.CL

TL;DR: LLMs pre-trained with programming languages outperform those trained with natural languages in logical reasoning tasks, with AST depth also influencing performance.


<details>
  <summary>Details</summary>
Motivation: To verify the causal relationship between programming language pre-training and enhanced logical reasoning abilities in LLMs.

Method: Pre-trained decoder-based models from scratch using ten programming languages and three natural language datasets, evaluated on FLD and bAbi tasks.

Result: Programming language-trained models consistently outperformed natural language-trained ones, with better instruction-following and AST depth impacting reasoning.

Conclusion: Programming languages contain factors that enhance logical inference, offering insights for LLM pre-training.

Abstract: Recent large language models (LLMs) have demonstrated remarkable
generalization abilities in mathematics and logical reasoning tasks. Prior
research indicates that LLMs pre-trained with programming language data exhibit
high mathematical and reasoning abilities; however, this causal relationship
has not been rigorously tested. Our research aims to verify which programming
languages and features during pre-training affect logical inference
performance. Specifically, we pre-trained decoder-based language models from
scratch using datasets from ten programming languages (e.g., Python, C, Java)
and three natural language datasets (Wikipedia, Fineweb, C4) under identical
conditions. Thereafter, we evaluated the trained models in a few-shot
in-context learning setting on logical reasoning tasks: FLD and bAbi, which do
not require commonsense or world knowledge. The results demonstrate that nearly
all models trained with programming languages consistently outperform those
trained with natural languages, indicating that programming languages contain
factors that elicit logic inference performance. In addition, we found that
models trained with programming languages exhibit a better ability to follow
instructions compared to those trained with natural languages. Further analysis
reveals that the depth of Abstract Syntax Trees representing parsed results of
programs also affects logical reasoning performance. These findings will offer
insights into the essential elements of pre-training for acquiring the
foundational abilities of LLMs.

</details>


### [99] [Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning](https://arxiv.org/pdf/2410.10360)
*Yongxin Xu, Ruizhe Zhang, Xinke Jiang, Yujie Feng, Yuzhen Xiao, Xinyu Ma, Runchuan Zhu, Xu Chu, Junfeng Zhao, Yasha Wang*

Main category: cs.CL

TL;DR: Parenting is a framework for Retrieval-Augmented Generation (RAG) that decouples and optimizes parameter subspaces to balance adherence and robustness, improving LLM performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of control in integrating internal and external knowledge in RAG methods, inspired by human cognition.

Method: Uses key parameter mining with forward/backward propagation to localize subspaces, then applies type-tailored tuning for optimization.

Result: Validated on various datasets and models, showing effectiveness and generalizability.

Conclusion: Parenting enhances RAG by balancing adherence and robustness through targeted subspace optimization.

Abstract: Retrieval-Augmented Generation (RAG) offers an effective solution to the
issues faced by Large Language Models (LLMs) in hallucination generation and
knowledge obsolescence by incorporating externally retrieved knowledge.
However, existing methods lack effective control mechanisms for integrating
internal and external knowledge. Inspired by human cognitive processes, we
propose Parenting, a novel framework that decouples, identifies, and
purposefully optimizes parameter subspaces related to adherence and robustness.
Specifically, Parenting utilizes a key parameter mining method that combines
forward and backward propagation signals to localize subspaces representing
different capabilities. Then, Parenting employs a type-tailored tuning
strategy, applying specific and appropriate optimizations to different
subspaces, aiming to achieve a balanced enhancement of both adherence and
robustness. Extensive experiments on various datasets and models validate the
effectiveness and generalizability of our method.

</details>


### [100] [Beware of Calibration Data for Pruning Large Language Models](https://arxiv.org/pdf/2410.17711)
*Yixin Ji, Yang Xiang, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang*

Main category: cs.CL

TL;DR: The paper explores the impact of calibration data on post-training pruning for LLMs, finding its quality and similarity to pre-training data crucial. A self-generating calibration data strategy is proposed, improving pruning performance by up to 2.68%.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic exploration of calibration data's role in post-training pruning for LLMs, especially under high sparsity.

Method: Conducts controlled experiments on calibration data factors (pruning settings, data amount, similarity to pre-training data) and proposes a self-generating synthesis strategy.

Result: Small, similar calibration data improves performance; the self-generating strategy enhances pruning methods by up to 2.68%.

Conclusion: Calibration data quality significantly impacts post-training pruning, and the proposed synthesis strategy offers a practical solution for advanced LLMs.

Abstract: As large language models (LLMs) are widely applied across various fields,
model compression has become increasingly crucial for reducing costs and
improving inference efficiency. Post-training pruning is a promising method
that does not require resource-intensive iterative training and only needs a
small amount of calibration data to assess the importance of parameters. Recent
research has enhanced post-training pruning from different aspects but few of
them systematically explore the effects of calibration data, and it is unclear
if there exist better calibration data construction strategies. We fill this
blank and surprisingly observe that calibration data is also crucial to
post-training pruning, especially for high sparsity. Through controlled
experiments on important influence factors of calibration data, including the
pruning settings, the amount of data, and its similarity with pre-training
data, we observe that a small size of data is adequate, and more similar data
to its pre-training stage can yield better performance. As pre-training data is
usually inaccessible for advanced LLMs, we further provide a self-generating
calibration data synthesis strategy to construct feasible calibration data.
Experimental results on recent strong open-source LLMs (e.g., DCLM, and
LLaMA-3) show that the proposed strategy can enhance the performance of strong
pruning methods (e.g., Wanda, DSnoT, OWL) by a large margin (up to $2.68\%$).
Code is available at https://github.com/Dereck0602/calibration_data.

</details>


### [101] [Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation](https://arxiv.org/pdf/2411.06660)
*Qiao Qiao, Yuepei Li, Qing Wang, Kang Zhou, Qi Li*

Main category: cs.CL

TL;DR: A novel framework called Bridge jointly encodes structural and semantic information for knowledge graph completion (KGC), outperforming SOTA models by fine-tuning PLMs with self-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Existing KGC methods use either structural KG embeddings or semantic PLMs, leading to suboptimal performance. PLMs are also not trained on KGs, making direct triple encoding inappropriate.

Method: Bridge encodes entities and relations separately using PLMs, applies structured representation learning, and fine-tunes PLMs with self-supervised learning (BYOL) using two views of a triple without semantic alteration.

Result: Bridge outperforms SOTA models on three benchmark datasets.

Conclusion: Jointly leveraging structural and semantic information via Bridge improves KGC performance, addressing limitations of existing methods.

Abstract: Knowledge graph completion (KGC) is a task of inferring missing triples based
on existing Knowledge Graphs (KGs). Both structural and semantic information
are vital for successful KGC. However, existing methods only use either the
structural knowledge from the KG embeddings or the semantic information from
pre-trained language models (PLMs), leading to suboptimal model performance.
Moreover, since PLMs are not trained on KGs, directly using PLMs to encode
triples may be inappropriate. To overcome these limitations, we propose a novel
framework called Bridge, which jointly encodes structural and semantic
information of KGs. Specifically, we strategically encode entities and
relations separately by PLMs to better utilize the semantic knowledge of PLMs
and enable structured representation learning via a structural learning
principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a
self-supervised representation learning method called BYOL to fine-tune PLMs
with two different views of a triple. Unlike BYOL, which uses augmentation
methods to create two semantically similar views of the same image, potentially
altering the semantic information. We strategically separate the triple into
two parts to create different views, thus avoiding semantic alteration.
Experiments demonstrate that Bridge outperforms the SOTA models on three
benchmark datasets.

</details>


### [102] [The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models](https://arxiv.org/pdf/2411.08870)
*Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst*

Main category: cs.CL

TL;DR: Medical LLMs and VLMs often fail to outperform their base models in medical QA tasks, with most cases showing worse performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether domain-adaptive pretraining on biomedical corpora actually improves performance of LLMs and VLMs in medical tasks.

Method: Comparison of ten medical LLMs and two VLMs against their base models in zero-/few-shot prompting and supervised fine-tuning for medical QA, with optimized prompts and statistical analysis.

Result: Medical models outperformed base models in only 26.7% of cases, tied in 16.7%, and performed worse in 56.7%.

Conclusion: General-domain models may already possess strong medical knowledge, and future studies should strengthen evaluation methods.

Abstract: Several recent works seek to adapt general-purpose large language models
(LLMs) and vision-language models (VLMs) for medical applications through
continued pretraining on publicly available biomedical corpora. These works
typically claim that such domain-adaptive pretraining improves performance on
various downstream medical tasks, such as answering medical exam questions. In
this paper, we compare ten "medical" LLMs and two VLMs against their
corresponding base models, arriving at a different conclusion: all medical VLMs
and nearly all medical LLMs fail to consistently improve over their base models
in the zero-/few-shot prompting and supervised fine-tuning regimes for medical
question answering (QA). For instance, on clinical-note-based QA tasks in the
3-shot setting, medical LLMs outperform their base models in only 26.7% of
cases, reach a (statistical) tie in 16.7% of cases, and perform significantly
worse in the remaining 56.7% of cases. Our conclusions are based on (i)
comparing each medical model directly against its base model; (ii) optimizing
the prompts for each model separately in zero-/few-shot prompting; and (iii)
accounting for statistical uncertainty in comparisons. Our findings suggest
that state-of-the-art general-domain models may already exhibit strong medical
knowledge and reasoning capabilities, and offer recommendations to strengthen
the conclusions of future studies.

</details>


### [103] [MLAN: Language-Based Instruction Tuning Preserves and Transfers Knowledge in Multimodal Language Models](https://arxiv.org/pdf/2411.10557)
*Jianhong Tu, Zhuohao Ni, Nicholas Crispino, Zihao Yu, Michael Bendersky, Beliz Gunel, Ruoxi Jia, Xin Liu, Lingjuan Lyu, Dawn Song, Chenguang Wang*

Main category: cs.CL

TL;DR: A novel visual instruction tuning strategy improves zero-shot task generalization in multimodal models by emphasizing text-only data, achieving comparable performance with fewer training tokens.


<details>
  <summary>Details</summary>
Motivation: Existing methods inadequately explore modality importance in instruction tuning, often over-relying on vision-language data while neglecting text-only data.

Method: Incorporates diverse text-only data during visual instruction tuning and varies vision-language data in controlled experiments to study modality importance.

Result: Text-heavy instruction tuning matches vision-heavy methods across 12 datasets using half the training tokens, showing efficient knowledge transfer.

Conclusion: Increasing diverse text-only data enables effective cross-modality instruction following and domain knowledge transfer, outperforming vision-language approaches in efficiency.

Abstract: We present a novel visual instruction tuning strategy to improve the
zero-shot task generalization of multimodal large language models by building a
firm text-only knowledge base. Existing work lacks sufficient experimentation
on the importance of each modality in the instruction tuning stage, often using
a majority of vision-language data while keeping text-only data limited and
fixing mixtures of modalities. By incorporating diverse text-only data in the
visual instruction tuning stage, we vary vision-language data in various
controlled experiments to investigate the importance of modality in visual
instruction tuning. Our comprehensive evaluation shows that the text-heavy
instruction tuning approach is able to perform on-par with traditional
vision-heavy mixtures on both modalities across 12 general datasets while using
as low as half the total training tokens. We find that simply increasing
sufficiently diverse text-only data enables transfer of instruction following
ability and domain knowledge across modalities while being more efficient than
the vision-language approach.

</details>


### [104] [A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans](https://arxiv.org/pdf/2412.01131)
*Zhihan Cao, Hiroaki Yamada, Simone Teufel, Takenobu Tokunaga*

Main category: cs.CL

TL;DR: The paper introduces a framework to evaluate PLMs' knowledge of five semantic relations beyond hypernymy, comparing humans and models using six metrics. Results show a knowledge gap, with antonymy as the exception, and masked models outperforming causal ones.


<details>
  <summary>Details</summary>
Motivation: To address the incomplete understanding of PLMs' semantic relation knowledge, particularly beyond hypernymy, and to compare human and model performance on the same task.

Method: A comprehensive evaluation framework covering five semantic relations (hyponymy, holonymy, meronymy, antonymy, synonymy) using six metrics. Experiments involve 16 PLMs (8 masked, 8 causal).

Result: Significant knowledge gap between humans and models for most relations, except antonymy. Masked models outperform causal ones, but both confuse non-antonymy relations with antonymy.

Conclusion: The study highlights limitations in PLMs' semantic relation knowledge and suggests masked models are superior, though challenges remain in distinguishing non-antonymy relations.

Abstract: Recently, much work has concerned itself with the enigma of what exactly PLMs
(pretrained language models) learn about different aspects of language, and how
they learn it. One stream of this type of research investigates the knowledge
that PLMs have about semantic relations. However, many aspects of semantic
relations were left unexplored. Only one relation was considered, namely
hypernymy. Furthermore, previous work did not measure humans' performance on
the same task as that solved by the PLMs. This means that at this point in
time, there is only an incomplete view of models' semantic relation knowledge.
To address this gap, we introduce a comprehensive evaluation framework covering
five relations beyond hypernymy, namely hyponymy, holonymy, meronymy, antonymy,
and synonymy. We use six metrics (two newly introduced here) for recently
untreated aspects of semantic relation knowledge, namely soundness,
completeness, symmetry, asymmetry, prototypicality, and distinguishability and
fairly compare humans and models on the same task. Our extensive experiments
involve 16 PLMs, eight masked and eight causal language models. Up to now only
masked language models had been tested although causal and masked language
models treat context differently. Our results reveal a significant knowledge
gap between humans and models for almost all semantic relations. Antonymy is
the outlier relation where all models perform reasonably well. In general,
masked language models perform significantly better than causal language
models. Nonetheless, both masked and causal language models are likely to
confuse non-antonymy relations with antonymy.

</details>


### [105] [A Context-aware Framework for Translation-mediated Conversations](https://arxiv.org/pdf/2412.04205)
*José Pombal, Sweta Agrawal, Patrick Fernandes, Emmanouil Zaranis, André F. T. Martins*

Main category: cs.CL

TL;DR: The paper introduces TowerChat, a framework to enhance translation systems by integrating contextual information, outperforming GPT-4o and TowerInstruct in bilingual conversational tasks.


<details>
  <summary>Details</summary>
Motivation: Current translation systems lack contextual understanding, leading to errors and miscommunication. The goal is to improve accuracy by incorporating context.

Method: The framework integrates contextual information during training and inference for large language model-based translation systems, tested in customer chat and user-assistant interactions.

Result: TowerChat outperforms state-of-the-art systems (GPT-4o, TowerInstruct) in translation quality metrics and leverages context effectively.

Conclusion: Incorporating context improves translation accuracy and consistency, making TowerChat a superior solution for bilingual conversations.

Abstract: Automatic translation systems offer a powerful solution to bridge language
barriers in scenarios where participants do not share a common language.
However, these systems can introduce errors leading to misunderstandings and
conversation breakdown. A key issue is that current systems fail to incorporate
the rich contextual information necessary to resolve ambiguities and omitted
details, resulting in literal, inappropriate, or misaligned translations. In
this work, we present a framework to improve large language model-based
translation systems by incorporating contextual information in bilingual
conversational settings during training and inference. We validate our proposed
framework on two task-oriented domains: customer chat and user-assistant
interaction. Across both settings, the system produced by our
framework-TowerChat-consistently results in better translations than
state-of-the-art systems like GPT-4o and TowerInstruct, as measured by multiple
automatic translation quality metrics on several language pairs. We also show
that the resulting model leverages context in an intended and interpretable
way, improving consistency between the conveyed message and the generated
translations.

</details>


### [106] [Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media](https://arxiv.org/pdf/2412.10266)
*Jiaqing Yuan, Ruijie Xi, Munindar P. Singh*

Main category: cs.CL

TL;DR: The paper introduces a generative approach for stance detection, incorporating interpretable rationales into smaller models like FlanT5, outperforming GPT-3.5 by 9.57%. It shows reasoning enhances multitask learning but may hinder single-task performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in existing stance detection methods, aiming for transparent and trustworthy systems to combat biases and harmful narratives on social media.

Method: Adopts a generative approach, integrating explicit rationales into smaller models (FlanT5) via single-task and multitask learning.

Result: FlanT5 with reasoning outperforms GPT-3.5 by 9.57%. Reasoning boosts multitask learning but may reduce single-task effectiveness.

Conclusion: Faithful rationales improve interpretability and trustworthiness, advancing equitable engagement on social media.

Abstract: Stance detection is crucial for fostering a human-centric Web by analyzing
user-generated content to identify biases and harmful narratives that undermine
trust. With the development of Large Language Models (LLMs), existing
approaches treat stance detection as a classification problem, providing robust
methodologies for modeling complex group interactions and advancing
capabilities in natural language tasks. However, these methods often lack
interpretability, limiting their ability to offer transparent and
understandable justifications for predictions. This study adopts a generative
approach, where stance predictions include explicit, interpretable rationales,
and integrates them into smaller language models through single-task and
multitask learning. We find that incorporating reasoning into stance detection
enables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot
performance, achieving an improvement of up to 9.57%. Moreover, our results
show that reasoning capabilities enhance multitask learning performance but may
reduce effectiveness in single-task settings. Crucially, we demonstrate that
faithful rationales improve rationale distillation into SLMs, advancing efforts
to build interpretable, trustworthy systems for addressing discrimination,
fostering trust, and promoting equitable engagement on social media.

</details>


### [107] [Interpretable LLM-based Table Question Answering](https://arxiv.org/pdf/2412.12386)
*Giang Nguyen, Ivan Brugere, Shubham Sharma, Sanjay Kariyappa, Anh Totti Nguyen, Freddy Lecue*

Main category: cs.CL

TL;DR: Plan-of-SQLs (POS) is a Table QA method that enhances interpretability by decomposing questions into executable SQL steps, ensuring transparency and competitive accuracy with fewer LLM calls.


<details>
  <summary>Details</summary>
Motivation: Interpretability in Table QA is crucial for high-stakes domains, but current LLM-based methods lack clear explanations.

Method: POS decomposes questions into atomic steps, each translated into SQL commands, ensuring transparent intermediate results.

Result: POS generates high-quality explanations, improves user verification, achieves competitive accuracy, and reduces LLM calls by up to 25x.

Conclusion: POS offers interpretable, efficient, and robust Table QA, with LLMs effectively simulating human decision-making based on explanations.

Abstract: Interpretability in Table Question Answering (Table QA) is critical,
especially in high-stakes domains like finance and healthcare. While recent
Table QA approaches based on Large Language Models (LLMs) achieve high
accuracy, they often produce ambiguous explanations of how answers are derived.
  We propose Plan-of-SQLs (POS), a new Table QA method that makes the model's
decision-making process interpretable. POS decomposes a question into a
sequence of atomic steps, each directly translated into an executable SQL
command on the table, thereby ensuring that every intermediate result is
transparent. Through extensive experiments, we show that: First, POS generates
the highest-quality explanations among compared methods, which markedly
improves the users' ability to simulate and verify the model's decisions.
Second, when evaluated on standard Table QA benchmarks (TabFact, WikiTQ, and
FeTaQA), POS achieves QA accuracy that is competitive to existing methods,
while also offering greater efficiency-requiring significantly fewer LLM calls
and table database queries (up to 25x fewer)-and more robust performance on
large-sized tables. Finally, we observe high agreement (up to 90.59% in forward
simulation) between LLMs and human users when making decisions based on the
same explanations, suggesting that LLMs could serve as an effective proxy for
humans in evaluating Table QA explanations.

</details>


### [108] [Computational Analysis of Character Development in Holocaust Testimonies](https://arxiv.org/pdf/2412.17063)
*Esther Shizgal, Eitan Wagner, Renana Keydar, Omri Abend*

Main category: cs.CL

TL;DR: A computational method analyzes character development in Holocaust survivor testimonies, focusing on religious trajectories, revealing common patterns in belief and practice.


<details>
  <summary>Details</summary>
Motivation: To explore the interplay between inner and outer changes in protagonists' religious trajectories within narratives, using Holocaust survivor testimonies as a case study.

Method: Natural language processing techniques are applied to cluster and analyze religious belief and practice trajectories in first-person testimonies.

Result: Most narratives show constant religious belief but oscillating practice, identifying common structures for historical and sociological research.

Conclusion: The study showcases NLP's potential for thematic trajectory analysis in narratives, offering insights into character evolution.

Abstract: This work presents a computational approach to analyze character development
along the narrative timeline. The analysis characterizes the inner and outer
changes the protagonist undergoes within a narrative, and the interplay between
them. We consider transcripts of Holocaust survivor testimonies as a test case,
each telling the story of an individual in first-person terms. We focus on the
survivor's religious trajectory, examining the evolution of their disposition
toward religious belief and practice along the testimony. Clustering the
resulting trajectories in the dataset, we identify common sequences in the
data. Our findings highlight multiple common structures of religiosity across
the narratives: in terms of belief, most present a constant disposition, while
for practice, most present an oscillating structure, serving as valuable
material for historical and sociological research. This work demonstrates the
potential of natural language processing techniques for analyzing character
evolution through thematic trajectories in narratives.

</details>


### [109] [Multimodal Contrastive Representation Learning in Augmented Biomedical Knowledge Graphs](https://arxiv.org/pdf/2501.01644)
*Tien Dang, Viet Thanh Duy Nguyen, Minh Tuan Le, Truong-Son Hy*

Main category: cs.CL

TL;DR: A multimodal approach combining Language Models and Graph Contrastive Learning improves link prediction in Biomedical Knowledge Graphs, demonstrated on PrimeKG++ and DrugBank datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance link prediction in Biomedical Knowledge Graphs (BKGs) by uncovering novel drug-disease relations and addressing limitations in existing BKGs.

Method: Unifies embeddings from specialized Language Models (LMs) with Graph Contrastive Learning (GCL) and employs a Knowledge Graph Embedding (KGE) model for intra- and inter-entity relationships. Introduces PrimeKG++, an enriched knowledge graph with multimodal data.

Result: Demonstrates strong generalizability and accurate link predictions, even for unseen nodes, on PrimeKG++ and DrugBank datasets.

Conclusion: The approach is effective and robust for diverse biomedical datasets, with publicly available code, models, and data.

Abstract: Biomedical Knowledge Graphs (BKGs) integrate diverse datasets to elucidate
complex relationships within the biomedical field. Effective link prediction on
these graphs can uncover valuable connections, such as potential novel
drug-disease relations. We introduce a novel multimodal approach that unifies
embeddings from specialized Language Models (LMs) with Graph Contrastive
Learning (GCL) to enhance intra-entity relationships while employing a
Knowledge Graph Embedding (KGE) model to capture inter-entity relationships for
effective link prediction. To address limitations in existing BKGs, we present
PrimeKG++, an enriched knowledge graph incorporating multimodal data, including
biological sequences and textual descriptions for each entity type. By
combining semantic and relational information in a unified representation, our
approach demonstrates strong generalizability, enabling accurate link
predictions even for unseen nodes. Experimental results on PrimeKG++ and the
DrugBank drug-target interaction dataset demonstrate the effectiveness and
robustness of our method across diverse biomedical datasets. Our source code,
pre-trained models, and data are publicly available at
https://github.com/HySonLab/BioMedKG

</details>


### [110] [PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models](https://arxiv.org/pdf/2501.03124)
*Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, Yu Cheng*

Main category: cs.CL

TL;DR: PRMBench is introduced to evaluate Process-level Reward Models (PRMs) on fine-grained error detection, revealing weaknesses in current models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack systematic evaluation of PRMs' nuanced error detection capabilities in complex reasoning tasks.

Method: PRMBench, a benchmark with 6,216 problems and 83,456 step-level labels, assesses PRMs across dimensions like simplicity, soundness, and sensitivity.

Result: Experiments on 15 models (open-source PRMs and closed-source LLMs) expose significant weaknesses in current PRMs.

Conclusion: PRMBench highlights challenges in process-level evaluation and aims to advance PRM research.

Abstract: Process-level Reward Models (PRMs) are crucial for complex reasoning and
decision-making tasks, where each intermediate step plays an important role in
the reasoning process. Since language models are prone to various types of
errors during the reasoning process, PRMs are required to possess nuanced
capabilities for detecting various implicit error types in real-world
scenarios. However, current benchmarks primarily focus on step correctness,
failing to evaluate PRMs' performance systematically. To address this gap, we
introduce PRMBench, a process-level benchmark specifically designed to assess
the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216
carefully designed problems and 83,456 step-level labels, evaluating models
across multiple dimensions, including simplicity, soundness, and sensitivity.
In our experiments on 15 models, spanning both open-source PRMs and
closed-source large language models prompted as critic models, we uncover
significant weaknesses in current PRMs. These findings underscore the
challenges inherent in process-level evaluation and highlight key directions
for future research. We hope PRMBench can be a robust bench for advancing
research on PRM evaluation and development.

</details>


### [111] [Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling](https://arxiv.org/pdf/2501.10316)
*Suvodip Dey, Yi-Jyun Sun, Gokhan Tur, Dilek Hakkani-Tur*

Main category: cs.CL

TL;DR: The paper proposes an accountability model for LLM-based task-oriented dialogue agents to mitigate user overreliance by introducing friction turns during model uncertainty or errors, improving dialogue state tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing user overreliance on LLM-based AI agents and reducing hallucinations by introducing accountability and friction turns.

Method: An augmented LLM with an accountability head (binary classifier) to predict dialogue state slots, tested on MultiWOZ and Snips benchmarks.

Result: 3% absolute improvement in joint goal accuracy (JGA); self-correction boosts JGA from 67.13 to 70.51, achieving state-of-the-art DST performance.

Conclusion: The accountability model and friction turns effectively reduce errors and user overreliance, enhancing dialogue agent reliability.

Abstract: Recent LLMs have enabled significant advancements for conversational agents.
However, they are also well known to hallucinate, producing responses that seem
plausible but are factually incorrect. On the other hand, users tend to
over-rely on LLM-based AI agents, accepting AI's suggestion even when it is
wrong. Adding positive friction, such as explanations or getting user
confirmations, has been proposed as a mitigation in AI-supported
decision-making systems. In this paper, we propose an accountability model for
LLM-based task-oriented dialogue agents to address user overreliance via
friction turns in cases of model uncertainty and errors associated with
dialogue state tracking (DST). The accountability model is an augmented LLM
with an additional accountability head that functions as a binary classifier to
predict the relevant slots of the dialogue state mentioned in the conversation.
We perform our experiments with multiple backbone LLMs on two established
benchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the
proposed approach not only enables reliable estimation of AI agent errors but
also guides the decoder in generating more accurate actions. We observe around
3% absolute improvement in joint goal accuracy (JGA) of DST output by
incorporating accountability heads into modern LLMs. Self-correcting the
detected errors further increases the JGA from 67.13 to 70.51, achieving
state-of-the-art DST performance. Finally, we show that error correction
through user confirmations (friction turn) achieves a similar performance gain,
highlighting its potential to reduce user overreliance.

</details>


### [112] [Multimodal Medical Code Tokenizer](https://arxiv.org/pdf/2502.04397)
*Xiaorui Su, Shvat Messica, Yepeng Huang, Ruth Johnson, Lukas Fesser, Shanghua Gao, Faryad Sahneh, Marinka Zitnik*

Main category: cs.CL

TL;DR: MedTok is a multimodal medical code tokenizer that improves EHR model performance by leveraging textual descriptions and relational context of medical codes.


<details>
  <summary>Details</summary>
Motivation: Existing tokenizers treat medical codes as isolated tokens, ignoring their textual descriptions, hierarchical positions, and relational context, which are critical for clinical reasoning.

Method: MedTok processes text descriptions with a language model encoder and relational structure with a graph encoder, quantizing both into a unified token space.

Result: MedTok improves AUPRC across EHR models (4.10% on MIMIC-III, 4.78% on MIMIC-IV, 11.32% on EHRShot), with the largest gains in drug recommendation.

Conclusion: MedTok demonstrates potential as a unified tokenizer for medical codes, enhancing tokenization for medical foundation models and medical QA systems.

Abstract: Foundation models trained on patient electronic health records (EHRs) require
tokenizing medical data into sequences of discrete vocabulary items. Existing
tokenizers treat medical codes from EHRs as isolated textual tokens. However,
each medical code is defined by its textual description, its position in
ontological hierarchies, and its relationships to other codes, such as disease
co-occurrences and drug-treatment associations. Medical vocabularies contain
more than 600,000 codes with critical information for clinical reasoning. We
introduce MedTok, a multimodal medical code tokenizer that uses the text
descriptions and relational context of codes. MedTok processes text using a
language model encoder and encodes the relational structure with a graph
encoder. It then quantizes both modalities into a unified token space,
preserving modality-specific and cross-modality information. We integrate
MedTok into five EHR models and evaluate it on operational and clinical tasks
across in-patient and out-patient datasets, including outcome prediction,
diagnosis classification, drug recommendation, and risk stratification.
Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR
models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with
the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate
using MedTok tokenizer with medical QA systems. Our results demonstrate the
potential of MedTok as a unified tokenizer for medical codes, improving
tokenization for medical foundation models.

</details>


### [113] [Mechanistic Interpretability of Emotion Inference in Large Language Models](https://arxiv.org/pdf/2502.05489)
*Ala N. Tak, Amin Banayeeanzade, Anahita Bolourani, Mina Kian, Robin Jia, Jonathan Gratch*

Main category: cs.CL

TL;DR: The paper explores how autoregressive LLMs process emotions, identifying localized emotion representations and validating them using cognitive appraisal theory. It demonstrates causal intervention to shape emotional text generation, with potential applications in safety and alignment.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms of how LLMs process emotional stimuli, which remains largely unexplored, and to validate these mechanisms using psychological theory.

Method: Investigates emotion representations in autoregressive LLMs, evaluates diverse model families and sizes, and uses cognitive appraisal theory for validation. Causal interventions are applied to steer emotional text generation.

Result: Identifies localized emotion representations in LLMs and shows they align with cognitive appraisal theory. Causal interventions successfully shape emotional outputs.

Conclusion: The study provides a novel method to causally intervene in emotional text generation, offering potential benefits for safety and alignment in affective domains.

Abstract: Large language models (LLMs) show promising capabilities in predicting human
emotions from text. However, the mechanisms through which these models process
emotional stimuli remain largely unexplored. Our study addresses this gap by
investigating how autoregressive LLMs infer emotions, showing that emotion
representations are functionally localized to specific regions in the model.
Our evaluation includes diverse model families and sizes and is supported by
robustness checks. We then show that the identified representations are
psychologically plausible by drawing on cognitive appraisal theory, a
well-established psychological framework positing that emotions emerge from
evaluations (appraisals) of environmental stimuli. By causally intervening on
construed appraisal concepts, we steer the generation and show that the outputs
align with theoretical and intuitive expectations. This work highlights a novel
way to causally intervene and precisely shape emotional text generation,
potentially benefiting safety and alignment in sensitive affective domains.

</details>


### [114] [KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy](https://arxiv.org/pdf/2502.05651)
*Hyunjong Kim, Suyeon Lee, Yeongjae Cho, Eunseo Ryu, Yohan Jo, Suran Seong, Sungzoon Cho*

Main category: cs.CL

TL;DR: The paper proposes a framework for creating high-quality Korean Motivational Interviewing (MI) dialogues using AI, addressing dataset limitations and non-English language gaps.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-driven mental health chatbots faces challenges like privacy and expertise, especially in non-English languages. Existing datasets are insufficient for training MI-based chatbots.

Method: A novel framework simulates MI sessions with therapist expertise, trains an MI forecaster model, and uses LLMs for utterance generation. The result is KMI, a synthetic Korean MI dataset.

Result: KMI contains 1,000 high-quality Korean MI dialogues, validated through expert evaluation for quality, expertise, and practicality. Novel MI-based metrics are introduced for dialogue evaluation.

Conclusion: The framework and KMI dataset advance AI-driven mental health chatbots by addressing dataset gaps and enhancing expertise, particularly for non-English languages.

Abstract: The increasing demand for mental health services has led to the rise of
AI-driven mental health chatbots, though challenges related to privacy, data
collection, and expertise persist. Motivational Interviewing (MI) is gaining
attention as a theoretical basis for boosting expertise in the development of
these chatbots. However, existing datasets are showing limitations for training
chatbots, leading to a substantial demand for publicly available resources in
the field of MI and psychotherapy. These challenges are even more pronounced in
non-English languages, where they receive less attention. In this paper, we
propose a novel framework that simulates MI sessions enriched with the
expertise of professional therapists. We train an MI forecaster model that
mimics the behavioral choices of professional therapists and employ Large
Language Models (LLMs) to generate utterances through prompt engineering. Then,
we present KMI, the first synthetic dataset theoretically grounded in MI,
containing 1,000 high-quality Korean Motivational Interviewing dialogues.
Through an extensive expert evaluation of the generated dataset and the
dialogue model trained on it, we demonstrate the quality, expertise, and
practicality of KMI. We also introduce novel metrics derived from MI theory in
order to evaluate dialogues from the perspective of MI.

</details>


### [115] [Demystifying Singular Defects in Large Language Models](https://arxiv.org/pdf/2502.07004)
*Haoqi Wang, Tong Zhang, Mathieu Salzmann*

Main category: cs.CL

TL;DR: The paper explores high-norm tokens in LLMs, identifying their causes and properties, and proposes practical applications like improved quantization and LLM signatures.


<details>
  <summary>Details</summary>
Motivation: High-norm tokens in LLMs are not well understood, unlike in ViTs, necessitating a new analysis framework.

Method: Theoretical insights and empirical validation across recent models, focusing on layer-wise singular directions, eigenvalues, and computational pathways.

Result: Key findings include the role of singular directions in norm explosions, negative eigenvalues in decay, and distinct pathways for initial vs. noninitial tokens.

Conclusion: The study advances understanding of singular defects in LLMs and suggests practical applications, encouraging further research into LLM mechanisms.

Abstract: Large transformer models are known to produce high-norm tokens. In vision
transformers (ViTs), such tokens have been mathematically modeled through the
singular vectors of the linear approximations of layers. However, in large
language models (LLMs), the underlying causes of high-norm tokens remain
largely unexplored, and their different properties from those of ViTs require a
new analysis framework. In this paper, we provide both theoretical insights and
empirical validation across a range of recent models, leading to the following
observations: i) The layer-wise singular direction predicts the abrupt
explosion of token norms in LLMs. ii) The negative eigenvalues of a layer
explain its sudden decay. iii) The computational pathways leading to high-norm
tokens differ between initial and noninitial tokens. iv) High-norm tokens are
triggered by the right leading singular vector of the matrix approximating the
corresponding modules. We showcase two practical applications of these
findings: the improvement of quantization schemes and the design of LLM
signatures. Our findings not only advance the understanding of singular defects
in LLMs but also open new avenues for their application. We expect that this
work will stimulate further research into the internal mechanisms of LLMs. Code
is released at https://github.com/haoqiwang/singular_defect.

</details>


### [116] [Organize the Web: Constructing Domains Enhances Pre-Training Data Curation](https://arxiv.org/pdf/2502.10341)
*Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, Luca Soldaini*

Main category: cs.CL

TL;DR: The paper introduces WebOrganizer, a framework to organize web corpora into domains by topic and format, improving data curation for language models.


<details>
  <summary>Details</summary>
Motivation: Large, unstructured web datasets make data curation challenging; the paper aims to systematically organize and analyze these datasets.

Method: Develops taxonomies for web content, uses a language model to annotate data, and studies domain mixing for better downstream task performance.

Result: Domain mixing boosts model performance and complements quality-based data selection methods.

Conclusion: Organizing and mixing domains enhances data curation, offering new insights for effective pre-training.

Abstract: Modern language models are trained on large, unstructured datasets consisting
of trillions of tokens and obtained by crawling the web. The unstructured
nature makes it difficult to reason about their contents and develop systematic
approaches to data curation. In this paper, we unpack monolithic web corpora by
developing taxonomies of their contents and organizing them into domains. We
introduce WebOrganizer, a framework for organizing web pages in terms of both
their topic and format. Using these two complementary notions of domains, we
automatically annotate pre-training data by distilling annotations from a large
language model into efficient classifiers. This allows us to study how data
from different domains should be mixed to improve models on downstream tasks,
and we show that we can combine insights about effective topics and formats to
further boost performance. We demonstrate that our domain mixing also improves
existing methods that select data based on quality. Furthermore, we study and
compare how quality-based methods will implicitly change the domain mixture.
Overall, our work demonstrates that constructing and mixing domains provides a
valuable complement to quality-based data curation methods, opening new avenues
for effective and insightful pre-training data curation.

</details>


### [117] [Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization](https://arxiv.org/pdf/2502.16825)
*Yao Xiao, Hai Ye, Linyao Chen, Hwee Tou Ng, Lidong Bing, Xiaoli Li, Roy Ka-wei Lee*

Main category: cs.CL

TL;DR: Scaling up on-policy samples via repeated random sampling improves LLM alignment, but conventional DPO strategies decline in performance with larger samples. A new strategy, categorizing rewards into seven points and selecting rejected responses at μ−2σ, enhances performance consistently.


<details>
  <summary>Details</summary>
Motivation: To address the performance decline in conventional DPO strategies when scaling up on-policy samples, the paper investigates reward distribution and proposes a more effective preference data construction method.

Method: The study categorizes reward space into seven points, explores all 21 pairwise combinations, and evaluates performance using AlpacaEval 2 on four models.

Result: Selecting rejected responses at μ−2σ instead of the minimum reward leads to optimal performance, and the proposed scalable strategy consistently improves model performance.

Conclusion: The paper introduces a scalable preference data construction strategy that enhances alignment performance as sample size increases, validated by empirical results.

Abstract: Iterative data generation and model retraining are widely used to align large
language models (LLMs). It typically involves a policy model to generate
on-policy responses and a reward model to guide training data selection. Direct
Preference Optimization (DPO) further enhances this process by constructing
preference pairs of chosen and rejected responses. In this work, we aim to
\emph{scale up} the number of on-policy samples via repeated random sampling to
improve alignment performance. Conventional practice selects the sample with
the highest reward as chosen and the lowest as rejected for DPO. However, our
experiments reveal that this strategy leads to a \emph{decline} in performance
as the sample size increases. To address this, we investigate preference data
construction through the lens of underlying normal distribution of sample
rewards. We categorize the reward space into seven representative points and
systematically explore all 21 ($C_7^2$) pairwise combinations. Through
evaluations on four models using AlpacaEval 2, we find that selecting the
rejected response at reward position $\mu - 2\sigma$ rather than the minimum
reward, is crucial for optimal performance. We finally introduce a scalable
preference data construction strategy that consistently enhances model
performance as the sample scale increases.

</details>


### [118] [Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases](https://arxiv.org/pdf/2502.18282)
*Shanshan Xu, T. Y. S. S Santosh, Yanai Elazar, Quirin Vogel, Barbara Plank, Matthias Grabmair*

Main category: cs.CL

TL;DR: The paper examines how LLMs' political biases reflect their training data, finding strong alignment with the data but weak correlation with human survey opinions.


<details>
  <summary>Details</summary>
Motivation: To understand if LLMs' political biases stem from memorized training data and how they align with human opinions.

Method: Quantitative evaluation of political leanings in pretraining corpora and comparison with human survey data, focusing on 32 US Supreme Court cases.

Result: LLMs strongly reflect training data biases, with no significant alignment to human survey opinions.

Conclusion: Highlights the need for responsible data curation and auditing methods to ensure human-AI alignment.

Abstract: Recent works have shown that Large Language Models (LLMs) have a tendency to
memorize patterns and biases present in their training data, raising important
questions about how such memorized content influences model behavior. One such
concern is the emergence of political bias in LLM outputs. In this paper, we
investigate the extent to which LLMs' political leanings reflect memorized
patterns from their pretraining corpora. We propose a method to quantitatively
evaluate political leanings embedded in the large pretraining corpora.
Subsequently we investigate to whom are the LLMs' political leanings more
aligned with, their pretrainig corpora or the surveyed human opinions. As a
case study, we focus on probing the political leanings of LLMs in 32 US Supreme
Court cases, addressing contentious topics such as abortion and voting rights.
Our findings reveal that LLMs strongly reflect the political leanings in their
training data, and no strong correlation is observed with their alignment to
human opinions as expressed in surveys. These results underscore the importance
of responsible curation of training data, and the methodology for auditing the
memorization in LLMs to ensure human-AI alignment.

</details>


### [119] [What Makes the Preferred Thinking Direction for LLMs in Multiple-choice Questions?](https://arxiv.org/pdf/2502.18435)
*Yizhe Zhang, Richard Bai, Zijin Gu, Ruixiang Zhang, Jiatao Gu, Emmanuel Abbe, Samy Bengio, Navdeep Jaitly*

Main category: cs.CL

TL;DR: The paper explores right-to-left (R2L) training as an alternative to left-to-right (L2R) autoregressive factorization in language models, showing R2L's superior performance on multiple-choice questions (MCQs) across various tasks.


<details>
  <summary>Details</summary>
Motivation: L2R factorization may not always be the best inductive bias, prompting investigation into alternative factorizations like R2L for improved performance in knowledge extraction and reasoning tasks.

Method: The study evaluates R2L training on MCQs, testing models (2B-8B parameters) across benchmarks for logical reasoning, commonsense understanding, and truthfulness. Controlled simulations on arithmetic tasks disentangle impacting factors like calibration and directional entropy.

Result: R2L models outperform L2R models on several MCQ benchmarks, with performance differences linked to factors like calibration and computability.

Conclusion: Exploring alternative factorizations like R2L can enhance LLM capabilities, offering theoretical insights into optimal factorization for approximating human language distribution and task-specific advantages.

Abstract: Language models usually use left-to-right (L2R) autoregressive factorization.
However, L2R factorization may not always be the best inductive bias.
Therefore, we investigate whether alternative factorizations of the text
distribution could be beneficial in some tasks. We investigate right-to-left
(R2L) training as a compelling alternative, focusing on multiple-choice
questions (MCQs) as a test bed for knowledge extraction and reasoning. Through
extensive experiments across various model sizes (2B-8B parameters) and
training datasets, we find that R2L models can significantly outperform L2R
models on several MCQ benchmarks, including logical reasoning, commonsense
understanding, and truthfulness assessment tasks. Our analysis reveals that
this performance difference may be fundamentally linked to multiple factors
including calibration, computability, and directional conditional entropy. We
analyze the impact of these factors through controlled simulation studies using
arithmetic tasks, where the impacting factors can be better disentangled. Our
work demonstrates that exploring alternative factorizations of the text
distribution can lead to improvements in LLM capabilities and provides
theoretical insights into optimal factorization towards approximating human
language distribution, and when each reasoning order might be more
advantageous. Our code and checkpoints are released at
https://github.com/apple/ml-reversal-blessing.

</details>


### [120] [Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles](https://arxiv.org/pdf/2502.18968)
*Kuang Wang, Xianfei Li, Shenghao Yang, Li Zhou, Feng Jiang, Haizhou Li*

Main category: cs.CL

TL;DR: USP is a user simulator framework that infers implicit profiles from interactions to create realistic, diverse dialogues, outperforming baselines in authenticity and diversity.


<details>
  <summary>Details</summary>
Motivation: Current user simulators lack utterance-level authenticity and user-level diversity, often relying on predefined profiles or neglecting implicit traits.

Method: USP uses an LLM-driven extractor, conditional supervised fine-tuning, reinforcement learning with cycle consistency, and a diverse profile sampler.

Result: USP outperforms baselines in authenticity and diversity while maintaining consistency and aligns well with benchmarks in real-world applications.

Conclusion: USP effectively addresses limitations of current simulators, offering a robust solution for realistic and diverse dialogue simulation.

Abstract: User simulators are crucial for replicating human interactions with dialogue
systems, supporting both collaborative training and automatic evaluation,
especially for large language models (LLMs). However, current role-playing
methods face challenges such as a lack of utterance-level authenticity and
user-level diversity, often hindered by role confusion and dependence on
predefined profiles of well-known figures. In contrast, direct simulation
focuses solely on text, neglecting implicit user traits like personality and
conversation-level consistency. To address these issues, we introduce the User
Simulator with Implicit Profiles (USP), a framework that infers implicit user
profiles from human-machine interactions to simulate personalized and realistic
dialogues. We first develop an LLM-driven extractor with a comprehensive
profile schema, then refine the simulation using conditional supervised
fine-tuning and reinforcement learning with cycle consistency, optimizing at
both the utterance and conversation levels. Finally, a diverse profile sampler
captures the distribution of real-world user profiles. Experimental results
show that USP outperforms strong baselines in terms of authenticity and
diversity while maintaining comparable consistency. Additionally, using USP to
evaluate LLM on dynamic multi-turn aligns well with mainstream benchmarks,
demonstrating its effectiveness in real-world applications.

</details>


### [121] [Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement](https://arxiv.org/pdf/2503.01875)
*Yaxuan Kong, Yiyuan Yang, Yoontae Hwang, Wenjie Du, Stefan Zohren, Zhangyang Wang, Ming Jin, Qingsong Wen*

Main category: cs.CL

TL;DR: The paper introduces Time-MQA, a unified framework for natural language queries across multiple time series tasks, supported by the TSQA dataset. It enhances large language models for advanced time series reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing methods and datasets for time series data are limited to narrow tasks like forecasting or anomaly detection, lacking a unified approach for diverse tasks.

Method: The authors propose Time-MQA, a framework for multi-task time series question answering, and the TSQA dataset (~200k QA pairs). They pre-train large language models (Mistral 7B, Llama-3 8B, Qwen-2.5 7B) on TSQA.

Result: Pre-training on TSQA improved time series reasoning in models, enabling advanced interactions beyond numeric tasks. The dataset and models are open-sourced.

Conclusion: Time-MQA and TSQA bridge the gap in time series tasks, offering a robust, unified framework for diverse applications and enhancing model capabilities.

Abstract: Time series data are foundational in finance, healthcare, and energy domains.
However, most existing methods and datasets remain focused on a narrow spectrum
of tasks, such as forecasting or anomaly detection. To bridge this gap, we
introduce Time Series Multi-Task Question Answering (Time-MQA), a unified
framework that enables natural language queries across multiple time series
tasks - numerical analytical tasks and open-ended question answering with
reasoning. Central to Time-MQA is the TSQA dataset, a large-scale dataset
containing $\sim$200k question-answer pairs derived from diverse time series
spanning environment, traffic, etc. This comprehensive resource covers various
time series lengths and promotes robust model development. We further
demonstrate how continually pre-training large language models (Mistral 7B,
Llama-3 8B, and Qwen-2.5 7B) on the TSQA dataset enhanced time series reasoning
capabilities, moving beyond mere numeric tasks and enabling more advanced and
intuitive interactions with temporal data. The complete TSQA dataset, models,
user study questionnaires for evaluation, and other related materials have been
open-sourced.

</details>


### [122] [Enough Coin Flips Can Make LLMs Act Bayesian](https://arxiv.org/pdf/2503.04722)
*Ritwik Gupta, Rodolfo Corona, Jiaxin Ge, Eric Wang, Dan Klein, Trevor Darrell, David M. Chan*

Main category: cs.CL

TL;DR: LLMs use in-context learning (ICL) for Bayesian-like reasoning, with biases in priors but largely correct posterior updates.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs perform structured reasoning via Bayesian frameworks or pattern matching in ICL.

Method: Controlled experiments with biased coin flips to analyze LLM behavior in zero-shot and few-shot settings.

Result: LLMs show biased priors but update posteriors correctly with sufficient ICL evidence. Attention magnitude doesn't affect Bayesian inference.

Conclusion: LLMs can perform Bayesian reasoning via ICL, with deviations mainly from miscalibrated priors, not flawed updates.

Abstract: Large language models (LLMs) exhibit the ability to generalize given few-shot
examples in their input prompt, an emergent capability known as in-context
learning (ICL). We investigate whether LLMs use ICL to perform structured
reasoning in ways that are consistent with a Bayesian framework or rely on
pattern matching. Using a controlled setting of biased coin flips, we find
that: (1) LLMs often possess biased priors, causing initial divergence in
zero-shot settings, (2) in-context evidence outweighs explicit bias
instructions, (3) LLMs broadly follow Bayesian posterior updates, with
deviations primarily due to miscalibrated priors rather than flawed updates,
and (4) attention magnitude has negligible effect on Bayesian inference. With
sufficient demonstrations of biased coin flips via ICL, LLMs update their
priors in a Bayesian manner.

</details>


### [123] [Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative Decoding](https://arxiv.org/pdf/2503.10135)
*Jinze Li, Yixing Xu, Haiduo Huang, Xuanwu Yin, Dong Li, Edith C. H. Ngai, Emad Barsoum*

Main category: cs.CL

TL;DR: Gumiho improves speculative decoding by using a hybrid model with serial and parallel heads, prioritizing early tokens for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods treat all tokens equally, ignoring the varying importance of early and later tokens in the sequence.

Method: Gumiho combines serial (Transformer) and parallel (MLP) heads, focusing advanced structures on early tokens for accuracy and lightweight heads on later tokens for efficiency.

Result: Gumiho outperforms existing methods, validating its effectiveness in multi-token generation.

Conclusion: Prioritizing early tokens with advanced structures and later tokens with lightweight heads enhances speculative decoding performance.

Abstract: Speculative decoding (SPD) aims to accelerate the auto-regressive token
generation process of a target Large Language Model (LLM). Some approaches
employ a draft model with multiple heads to predict a sequence of future
tokens, where each head handles a token in the sequence. The target LLM
verifies the predicted sequence and accepts aligned tokens, enabling efficient
multi-token generation. However, existing methods assume that all tokens within
a sequence are equally important, employing identical head structures and
relying on a single-generation paradigm, either serial or parallel. To this
end, we theoretically demonstrate that initial tokens in the draft sequence are
more important than later ones. Building on this insight, we propose Gumiho, a
hybrid model combining serial and parallel heads. Specifically, given the
critical importance of early tokens, we employ a sophisticated Transformer
architecture for the early draft heads in a serial configuration to improve
accuracy. For later tokens, we utilize multiple lightweight MLP heads operating
in parallel to enhance efficiency. By allocating more advanced model structures
and longer running times to the early heads, Gumiho achieves improved overall
performance. The experimental results demonstrate that our method outperforms
existing approaches, fully validating its effectiveness.

</details>


### [124] [TigerLLM - A Family of Bangla Large Language Models](https://arxiv.org/pdf/2503.10995)
*Nishat Raihan, Marcos Zampieri*

Main category: cs.CL

TL;DR: TigerLLM, a new Bangla LLM, outperforms existing open-source and proprietary models like GPT3.5, setting a new benchmark for Bangla language modeling.


<details>
  <summary>Details</summary>
Motivation: Addressing the linguistic disparity in LLMs, particularly for Bangla, which lacks high-performance open-source models.

Method: Introducing TigerLLM, a family of Bangla LLMs, and evaluating them against standard benchmarks.

Result: TigerLLM surpasses all open-source alternatives and outperforms larger proprietary models like GPT3.5.

Conclusion: TigerLLM establishes a new baseline for Bangla language modeling, bridging the gap in LLM development for low-resource languages.

Abstract: The development of Large Language Models (LLMs) remains heavily skewed
towards English and a few other high-resource languages. This linguistic
disparity is particularly evident for Bangla - the 5th most spoken language. A
few initiatives attempted to create open-source Bangla LLMs with performance
still behind high-resource languages and limited reproducibility. To address
this gap, we introduce TigerLLM - a family of Bangla LLMs. Our results
demonstrate that these models surpass all open-source alternatives and also
outperform larger proprietary models like GPT3.5 across standard benchmarks,
establishing TigerLLM as the new baseline for future Bangla language modeling.

</details>


### [125] [Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning](https://arxiv.org/pdf/2503.11655)
*Donghao Huang, Zhaoxia Wang*

Main category: cs.CL

TL;DR: DeepSeek-R1, an open-source reasoning model, outperforms GPT-4o in few-shot sentiment analysis with higher efficiency and explainability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing accuracy, efficiency, and explainability in sentiment analysis using LLMs.

Method: Comprehensive evaluation of DeepSeek-R1 against GPT-4o variants, testing full and distilled models with few-shot learning curves.

Result: DeepSeek-R1 achieves 91.39% F1 score (5-class) and 99.31% accuracy (binary) with 5 shots, showing 8x efficiency improvement over GPT-4o.

Conclusion: DeepSeek-R1 is a superior, interpretable open-source alternative for sentiment analysis, despite lower throughput.

Abstract: Large language models (LLMs) have transformed sentiment analysis, yet
balancing accuracy, efficiency, and explainability remains a critical
challenge. This study presents the first comprehensive evaluation of
DeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and
GPT-4o-mini. We test the full 671B model and its distilled variants,
systematically documenting few-shot learning curves. Our experiments show
DeepSeek-R1 achieves a 91.39\% F1 score on 5-class sentiment and 99.31\%
accuracy on binary tasks with just 5 shots, an eightfold improvement in
few-shot efficiency over GPT-4o. Architecture-specific distillation effects
emerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant
by 6.69 percentage points. While its reasoning process reduces throughput,
DeepSeek-R1 offers superior explainability via transparent, step-by-step
traces, establishing it as a powerful, interpretable open-source alternative.

</details>


### [126] [LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates](https://arxiv.org/pdf/2503.16334)
*Ying Shen, Lifu Huang*

Main category: cs.CL

TL;DR: LLMBRACES enhances Transformer-based LLMs by dynamically adjusting FFN sub-updates based on relevance, improving accuracy and control over outputs like sentiment and toxicity.


<details>
  <summary>Details</summary>
Motivation: To improve LLM performance and control by modulating FFN sub-updates, leveraging their interpretable concept encoding.

Method: Proposes LLMBRACES, which computes relevance scores for FFN value vectors and dynamically adjusts sub-update contributions.

Result: Outperforms baselines in fine-tuning and zero-shot settings, reduces tunable parameters by 75%, and excels in controlled generation tasks.

Conclusion: LLMBRACES offers efficient, flexible control over LLM outputs, enhancing accuracy and reliability for diverse applications.

Abstract: Recent findings reveal that much of the knowledge in a Transformer-based
Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where
each FNN layer can be interpreted as the summation of sub-updates, each
corresponding to a weighted column vector from the FFN's value parameter matrix
that often encodes human-interpretable concepts. In light of this, we
hypothesize that model performance and behaviors can be further enhanced and
controlled by modulating the contributions of these sub-updates based on their
relevance to the input or target output style, and propose LLMBRACES, a novel
and efficient method that computes relevance scores associated with value
vectors in FFN layers and leverages these scores to dynamically adjust the
contribution of sub-updates. By optimizing sub-update contributions, LLMBRACES
refines the prediction process, leading to more accurate and reliable outputs,
much like a 'brace' providing support and stability. Moreover, LLMBRACES can be
extended to support conditional control over generation characteristics, such
as sentiment, thereby offering fine-grained steering of LLM outputs. Extensive
experiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and
Llama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both
fine-tuning and zero-shot settings while requiring significantly fewer tunable
parameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in
sentiment-controlled generation and toxicity reduction, highlighting its
potential for flexible, controlled text generation across applications.

</details>


### [127] [Automating Adjudication of Cardiovascular Events Using Large Language Models](https://arxiv.org/pdf/2503.17222)
*Sonish Sivarajkumar, Kimia Ameri, Chuqin Li, Yanshan Wang, Min Jiang*

Main category: cs.CL

TL;DR: A novel LLM-based framework automates cardiovascular event adjudication in clinical trials, improving efficiency and consistency.


<details>
  <summary>Details</summary>
Motivation: Manual adjudication of cardiovascular events is slow, costly, and inconsistent, prompting the need for automation.

Method: A two-stage LLM approach: event extraction from unstructured data and adjudication guided by CEC guidelines and Tree of Thoughts.

Result: Achieved F1-score of 0.82 for extraction and 0.68 accuracy for adjudication, with the CLEART score introduced for evaluation.

Conclusion: The framework reduces time, cost, and variability while maintaining high-quality outcomes in clinical trials.

Abstract: Cardiovascular events, such as heart attacks and strokes, remain a leading
cause of mortality globally, necessitating meticulous monitoring and
adjudication in clinical trials. This process, traditionally performed manually
by clinical experts, is time-consuming, resource-intensive, and prone to
inter-reviewer variability, potentially introducing bias and hindering trial
progress. This study addresses these critical limitations by presenting a novel
framework for automating the adjudication of cardiovascular events in clinical
trials using Large Language Models (LLMs). We developed a two-stage approach:
first, employing an LLM-based pipeline for event information extraction from
unstructured clinical data and second, using an LLM-based adjudication process
guided by a Tree of Thoughts approach and clinical endpoint committee (CEC)
guidelines. Using cardiovascular event-specific clinical trial data, the
framework achieved an F1-score of 0.82 for event extraction and an accuracy of
0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,
automated metric specifically designed for evaluating the quality of
AI-generated clinical reasoning in adjudicating cardiovascular events. This
approach demonstrates significant potential for substantially reducing
adjudication time and costs while maintaining high-quality, consistent, and
auditable outcomes in clinical trials. The reduced variability and enhanced
standardization also allow for faster identification and mitigation of risks
associated with cardiovascular therapies.

</details>


### [128] [Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs](https://arxiv.org/pdf/2504.04745)
*Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco*

Main category: cs.CL

TL;DR: The paper evaluates how LLMs use structured linguistic representations (AMR) for tasks. Short-context tasks degrade performance, while long-context tasks (e.g., summarization) improve, especially in newer/larger models. AMR-to-text reconstruction is also effective.


<details>
  <summary>Details</summary>
Motivation: To assess the impact of structured linguistic representations (AMR) on LLM performance across varying context lengths.

Method: Analyzed 8-bit quantized and instruction-tuned LLMs (Llama 3.1, Phi-3, Mistral 7B) using AMR for short and long contexts.

Result: Short-context tasks worsened with AMR, but long-context tasks (e.g., summarization) improved, especially in newer/larger models. AMR-to-text reconstruction achieved 81% similarity.

Conclusion: AMR benefits long-context tasks in advanced LLMs but harms short-context ones. AMR-to-text reconstruction is effective.

Abstract: This paper evaluates the ability of Large Language Models (LLMs) to leverage
contextual information in the form of structured linguistic representations.
Specifically, we examine the impact of encoding both short and long contexts
using Abstract Meaning Representation (AMR) structures across a diverse set of
language tasks. We perform our analysis using 8-bit quantized and
instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our
results indicate that, for tasks involving short contexts, augmenting the
prompt with the AMR of the original language context often degrades the
performance of the underlying LLM. However, for tasks that involve long
contexts, such as dialogue summarization in the SAMSum dataset, this
enhancement improves LLM performance, for example, by increasing the zero-shot
cosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more
evident in the newer and larger LLMs, but does not extend to the older or
smaller ones. In addition, we observe that LLMs can effectively reconstruct the
original text from a linearized AMR, achieving a cosine similarity of 81% in
the best-case scenario.

</details>


### [129] [MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation](https://arxiv.org/pdf/2504.12563)
*Haris Riaz, Sourav Bhabesh, Vinayak Arannil, Miguel Ballesteros, Graham Horwood*

Main category: cs.CL

TL;DR: MetaSynth enhances synthetic data diversity for domain adaptation, improving LLM performance in specialized domains like Finance and Biomedicine without compromising general capabilities.


<details>
  <summary>Details</summary>
Motivation: Addressing the low diversity in synthetic data generated by larger language models, which limits its applicability for domain adaptation.

Method: Proposes MetaSynth, using meta-prompting to orchestrate multiple expert LLM agents for diverse synthetic data generation.

Result: MetaSynth-generated data improves Mistral-7B-v0.3 performance by 4.08% in Finance and 13.75% in Biomedicine, outperforming template-based synthetic data.

Conclusion: Diverse synthetic data from MetaSynth is effective for domain adaptation, requiring no real data mixing.

Abstract: Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data
generated using larger Language models. Questions remain about leveraging
synthetic data for other use cases, such as adapting LLMs to specific domains.
A key limitation of synthetic data is low diversity, which negatively impacts
its downstream applicability for improving other models. To address this, we
propose MetaSynth, a method for generating synthetic data that enhances
diversity through meta-prompting, where a language model orchestrates multiple
"expert" LLM agents to collaboratively generate data. Using only 25 million
tokens of synthetic data generated with MetaSynth, we successfully adapt a
well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and
Biomedicine-without compromising the capabilities of the resulting model in
general tasks. In addition, we evaluate the diversity of our synthetic data
using seven automated metrics, and find that it approaches the diversity of LLM
pre-training corpora.
  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms
the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in
Biomedicine. The same model shows degraded performance when trained on data
generated using a template prompt, even when the template includes prior
generations and varying In-Context exemplars of real data. Our findings suggest
that a few million tokens of diverse synthetic data without mixing any real
data, is sufficient for effective domain adaptation when using MetaSynth.

</details>


### [130] [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/pdf/2504.14154)
*Zhiyuan Wang, Qingni Wang, Yue Zhang, Tianlong Chen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu*

Main category: cs.CL

TL;DR: The paper introduces Selective Conformal Uncertainty (SConU), a method using conformal p-values to manage miscoverage rates and improve prediction efficiency in large language models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing conformal uncertainty frameworks, which fail to handle outliers violating exchangeability, leading to unreliable miscoverage rates.

Method: Develops two conformal p-values to test sample deviations from the calibration set's uncertainty distribution, enabling rigorous miscoverage rate management.

Result: SConU improves prediction efficiency and manages miscoverage rates effectively in single-domain and interdisciplinary contexts.

Conclusion: The proposed SConU method enhances the reliability of large language models by approximating conditional coverage, especially in high-stakes tasks.

Abstract: As large language models are increasingly utilized in real-world
applications, guarantees of task-specific metrics are essential for their
reliable deployment. Previous studies have introduced various criteria of
conformal uncertainty grounded in split conformal prediction, which offer
user-specified correctness coverage. However, existing frameworks often fail to
identify uncertainty data outliers that violate the exchangeability assumption,
leading to unbounded miscoverage rates and unactionable prediction sets. In
this paper, we propose a novel approach termed Selective Conformal Uncertainty
(SConU), which, for the first time, implements significance tests, by
developing two conformal p-values that are instrumental in determining whether
a given sample deviates from the uncertainty distribution of the calibration
set at a specific manageable risk level. Our approach not only facilitates
rigorous management of miscoverage rates across both single-domain and
interdisciplinary contexts, but also enhances the efficiency of predictions.
Furthermore, we comprehensively analyze the components of the conformal
procedures, aiming to approximate conditional coverage, particularly in
high-stakes question-answering tasks.

</details>


### [131] [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/pdf/2504.16084)
*Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding, Bowen Zhou*

Main category: cs.CL

TL;DR: TTRL is a novel RL method for training LLMs on unlabeled data, leveraging TTS practices like majority voting for reward estimation. It improves performance significantly, even surpassing models trained with labeled data.


<details>
  <summary>Details</summary>
Motivation: The challenge of reward estimation in RL for LLMs without explicit labels motivates the development of TTRL, which uses unlabeled data effectively.

Method: TTRL employs Test-Time Scaling (TTS) techniques, such as majority voting, to estimate rewards and train LLMs via reinforcement learning on unlabeled data.

Result: TTRL boosts performance, e.g., increasing Qwen-2.5-Math-7B's pass@1 by ~211% on AIME 2024, and consistently surpasses initial model limits.

Conclusion: TTRL is a general and effective method for improving LLM performance using unlabeled data, with potential for broader applications.

Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the maj@n metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model maj@n,
and approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL

</details>


### [132] [Truth Neurons](https://arxiv.org/pdf/2505.12182)
*Haohang Li, Yupeng Cao, Yangyang Yu, Jordan W. Suchow, Zining Zhu*

Main category: cs.CL

TL;DR: The paper identifies 'truth neurons' in language models that encode truthfulness mechanistically, showing their existence across models and layers. Suppressing these neurons degrades performance, indicating their general role in truthfulness.


<details>
  <summary>Details</summary>
Motivation: Understanding how truthfulness is encoded in language models to improve their reliability and safety.

Method: Proposes identifying truthfulness at the neuron level, validating existence through experiments across models and layers.

Result: Truth neurons exist and encode truthfulness subject-agnostically; suppressing them degrades performance on benchmarks.

Conclusion: The findings provide insights into truthfulness mechanisms in language models, suggesting ways to enhance trustworthiness.

Abstract: Despite their remarkable success and deployment across diverse workflows,
language models sometimes produce untruthful responses. Our limited
understanding of how truthfulness is mechanistically encoded within these
models jeopardizes their reliability and safety. In this paper, we propose a
method for identifying representations of truthfulness at the neuron level. We
show that language models contain truth neurons, which encode truthfulness in a
subject-agnostic manner. Experiments conducted across models of varying scales
validate the existence of truth neurons, confirming that the encoding of
truthfulness at the neuron level is a property shared by many language models.
The distribution patterns of truth neurons over layers align with prior
findings on the geometry of truthfulness. Selectively suppressing the
activations of truth neurons found through the TruthfulQA dataset degrades
performance both on TruthfulQA and on other benchmarks, showing that the
truthfulness mechanisms are not tied to a specific dataset. Our results offer
novel insights into the mechanisms underlying truthfulness in language models
and highlight potential directions toward improving their trustworthiness and
reliability.

</details>


### [133] [CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning](https://arxiv.org/pdf/2505.13271)
*Lei Sheng, Shuai-Shuai Xu*

Main category: cs.CL

TL;DR: CSC-SQL integrates Self-Consistency and Self-Correction to improve SQL generation accuracy, using GRPO for fine-tuning, achieving high execution accuracy on BIRD test set.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Self-Consistency and Self-Correction have limitations (suboptimal outputs, syntactic-only fixes). CSC-SQL aims to combine their strengths for better SQL generation.

Method: CSC-SQL selects top outputs from parallel sampling, corrects them via a merge revision model, and uses GRPO for fine-tuning via reinforcement learning.

Result: 7B model achieves 71.72% and 32B model achieves 73.67% execution accuracy on BIRD test set.

Conclusion: CSC-SQL is effective and generalizable, with open-sourced code for further use.

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
translating natural language questions about relational databases into SQL
queries. In particular, test-time scaling techniques such as Self-Consistency
and Self-Correction can enhance SQL generation accuracy by increasing
computational effort during inference. However, these methods have notable
limitations: Self-Consistency may select suboptimal outputs despite majority
votes, while Self-Correction typically addresses only syntactic errors. To
leverage the strengths of both approaches, we propose CSC-SQL, a novel method
that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two
most frequently occurring outputs from parallel sampling and feeds them into a
merge revision model for correction. Additionally, we employ the Group Relative
Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and
revision models via reinforcement learning, significantly enhancing output
quality. Experimental results confirm the effectiveness and generalizability of
CSC-SQL. On the BIRD private test set, our 7B model achieves 71.72\% execution
accuracy, while the 32B model achieves 73.67\%. The code has been open sourced
at https://github.com/CycloneBoy/csc_sql.

</details>


### [134] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/pdf/2505.22648)
*Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou*

Main category: cs.CL

TL;DR: The paper introduces WebDancer, an agentic information-seeking system, using a four-stage training paradigm for multi-step reasoning, achieving strong performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address complex real-world problems requiring multi-step reasoning and information seeking by developing an autonomous agentic system.

Method: A four-stage approach: browsing data construction, trajectories sampling, supervised fine-tuning, and reinforcement learning for generalization.

Result: WebDancer performs well on GAIA and WebWalkerQA benchmarks, demonstrating the efficacy of the training paradigm.

Conclusion: The framework provides insights for developing more capable agentic models, with code and demo available.

Abstract: Addressing intricate real-world problems necessitates in-depth information
seeking and multi-step reasoning. Recent progress in agentic systems,
exemplified by Deep Research, underscores the potential for autonomous
multi-step research. In this work, we present a cohesive paradigm for building
end-to-end agentic information seeking agents from a data-centric and
training-stage perspective. Our approach consists of four key stages: (1)
browsing data construction, (2) trajectories sampling, (3) supervised
fine-tuning for effective cold start, and (4) reinforcement learning for
enhanced generalisation. We instantiate this framework in a web agent based on
the ReAct, WebDancer. Empirical evaluations on the challenging information
seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of
WebDancer, achieving considerable results and highlighting the efficacy of our
training paradigm. Further analysis of agent training provides valuable
insights and actionable, systematic pathways for developing more capable
agentic models. The codes and demo will be released in
https://github.com/Alibaba-NLP/WebAgent.

</details>


### [135] [ScienceMeter: Tracking Scientific Knowledge Updates in Language Models](https://arxiv.org/pdf/2505.24302)
*Yike Wang, Shangbin Feng, Yulia Tsvetkov, Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: ScienceMeter evaluates LLMs' scientific knowledge updates using three metrics: preservation, acquisition, and projection. It tests five methods on 15,444 papers, finding limited success in updating knowledge across domains.


<details>
  <summary>Details</summary>
Motivation: LLMs' scientific knowledge becomes outdated quickly, necessitating a framework to evaluate and improve knowledge update methods.

Method: ScienceMeter introduces three metrics (preservation, acquisition, projection) and evaluates five update methods on a dataset of 15,444 papers and 30,888 claims.

Result: Best methods preserve 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% of future knowledge. Inference works for large models; training suits smaller ones.

Conclusion: Current methods fall short in robustly updating scientific knowledge, highlighting the challenge and importance of developing better mechanisms.

Abstract: Large Language Models (LLMs) are increasingly used to support scientific
research, but their knowledge of scientific advancements can quickly become
outdated. We introduce ScienceMeter, a new framework for evaluating scientific
knowledge update methods over scientific knowledge spanning the past, present,
and future. ScienceMeter defines three metrics: knowledge preservation, the
extent to which models' understanding of previously learned papers are
preserved; knowledge acquisition, how well scientific claims from newly
introduced papers are acquired; and knowledge projection, the ability of the
updated model to anticipate or generalize to related scientific claims that may
emerge in the future. Using ScienceMeter, we examine the scientific knowledge
of LLMs on claim judgment and generation tasks across a curated dataset of
15,444 scientific papers and 30,888 scientific claims from ten domains
including medicine, biology, materials science, and computer science. We
evaluate five representative knowledge update approaches including training-
and inference-time methods. With extensive experiments, we find that the
best-performing knowledge update methods can preserve only 85.9% of existing
knowledge, acquire 71.7% of new knowledge, and project 37.7% of future
knowledge. Inference-based methods work for larger models, whereas smaller
models require training to achieve comparable performance. Cross-domain
analysis reveals that performance on these objectives is correlated. Even when
applying on specialized scientific LLMs, existing knowledge update methods fail
to achieve these objectives collectively, underscoring that developing robust
scientific knowledge update mechanisms is both crucial and challenging.

</details>


### [136] [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/pdf/2506.07160)
*Yikun Wang, Yibin Wang, Dianyi Wang, Zimian Peng, Qipeng Guo, Dacheng Tao, Jiaqi Wang*

Main category: cs.CL

TL;DR: The paper introduces GCPO, a reinforcement learning framework, to improve geometric reasoning in smaller LLMs by optimizing auxiliary construction and reasoning chains, outperforming baselines by 4.29%.


<details>
  <summary>Details</summary>
Motivation: Geometry problem solving in LLMs is challenging due to suboptimal performance or high computational costs of large models. The paper aims to address this by enhancing smaller models with efficient auxiliary construction.

Method: Proposes Group Contrastive Policy Optimization (GCPO), featuring Group Contrastive Masking for adaptive rewards and a length reward for longer reasoning chains, applied in the GeometryZero model.

Result: GeometryZero models outperform baselines (e.g., GRPO) by 4.29% on benchmarks like Geometry3K and MathVista.

Conclusion: GCPO effectively enhances geometric reasoning in smaller LLMs, offering a cost-efficient alternative to large models.

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities across diverse domains, particularly in mathematical reasoning,
amid which geometry problem solving remains a challenging area where auxiliary
construction plays a enssential role. Existing approaches either achieve
suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring
massive computational costs. We posit that reinforcement learning with
verifiable reward (e.g., GRPO) offers a promising direction for training
smaller models that effectively combine auxiliary construction with robust
geometric reasoning. However, directly applying GRPO to geometric reasoning
presents fundamental limitations due to its dependence on unconditional
rewards, which leads to indiscriminate and counterproductive auxiliary
constructions. To address these challenges, we propose Group Contrastive Policy
Optimization (GCPO), a novel reinforcement learning framework featuring two key
innovations: (1) Group Contrastive Masking, which adaptively provides positive
or negative reward signals for auxiliary construction based on contextual
utility, and a (2) length reward that promotes longer reasoning chains.
Building on GCPO, we develop GeometryZero, a family of affordable-size
geometric reasoning models that judiciously determine when to employ auxiliary
construction. Our extensive empirical evaluation across popular geometric
benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models
consistently outperform baselines (e.g. GRPO), achieving an average improvement
of 4.29% across all benchmarks.

</details>


### [137] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/pdf/2506.08686)
*Soham Poddar, Paramita Koley, Janardan Misra, Sanjay Podder, Navveen Balani, Niloy Ganguly, Saptarshi Ghosh*

Main category: cs.CL

TL;DR: The paper addresses energy inefficiency in LLM inference by focusing on output compression, revealing LLMs often produce unnecessarily long responses. It proposes prompt-engineering strategies to reduce response length, achieving 25-60% energy savings without compromising quality.


<details>
  <summary>Details</summary>
Motivation: Energy consumption in LLM inference is high, and output compression is underexplored. The study aims to reduce energy use by optimizing response length.

Method: Benchmarked 12 decoder-only LLMs on 5 datasets, analyzed response quality, and tested prompt-engineering strategies for length reduction.

Result: Prompt-engineering reduced response length by 25-60%, significantly lowering energy consumption while maintaining response quality.

Conclusion: Simple prompt adjustments can effectively optimize LLM energy efficiency by reducing redundant output, offering a practical solution for sustainable AI.

Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)
arises from their inference processes; hence developing energy-efficient
methods for inference is crucial. While several techniques exist for inference
optimization, output compression remains relatively unexplored, with only a few
preliminary efforts addressing this aspect. In this work, we first benchmark 12
decoder-only LLMs across 5 datasets, revealing that these models often produce
responses that are substantially longer than necessary. We then conduct a
comprehensive quality assessment of LLM responses, formally defining six
information categories present in LLM responses. We show that LLMs often tend
to include redundant or additional information besides the minimal answer. To
address this issue of long responses by LLMs, we explore several simple and
intuitive prompt-engineering strategies. Empirical evaluation shows that
appropriate prompts targeting length reduction and controlling information
content can achieve significant energy optimization between 25-60\% by reducing
the response length while preserving the quality of LLM responses.

</details>


### [138] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/pdf/2506.09428)
*Fei Ding, Baiqiao Wang*

Main category: cs.CL

TL;DR: A novel SFT method mitigates catastrophic forgetting in LLMs by reconstructing instruction distributions and synthesizing datasets, improving both general and task-specific performance.


<details>
  <summary>Details</summary>
Motivation: Address the degradation of general abilities in LLMs during SFT, especially when original SFT data is unavailable.

Method: Reconstructs base model's instruction distribution, uses multi-model generation/filtering to create synthetic general-purpose data, and mixes it with domain-specific data for fine-tuning.

Result: Preserves general capabilities while enhancing task-specific performance, outperforming baselines using public SFT datasets.

Conclusion: The proposed method is cost-effective and effective in mitigating catastrophic forgetting without original SFT data.

Abstract: Supervised Fine-Tuning (SFT) is a critical step for enhancing the
instruction-following capabilities of Large Language Models (LLMs) and adapting
them to specialized domains. However, SFT often leads to a degradation of the
model's general abilities, a phenomenon known as catastrophic forgetting. This
problem is exacerbated when third-party practitioners fine-tune open-source
models, as the original SFT data is typically not available. To address this
challenge, we propose a novel and cost-effective SFT method that effectively
mitigates catastrophic forgetting without requiring access to the original SFT
data. Our approach first reconstructs the likely instruction distribution of
the base model. It then employs a multi-model generation and filtering pipeline
to synthesize a high-quality general-purpose dataset. This synthetic dataset is
mixed with new, domain-specific data for fine-tuning. Experimental results show
that our method not only preserves the model's capabilities in general domains
but also improves task-specific performance, outperforming baselines that use
publicly available SFT datasets.

</details>


### [139] [From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment](https://arxiv.org/pdf/2506.12446)
*Bin Xie, Bingbing Xu, Yige Yuan, Shengmao Zhu, Huawei Shen*

Main category: cs.CL

TL;DR: The paper introduces SP-PRM, a dual-consistency framework for process reward models (PRMs) to address the granularity mismatch in reward-guided search (RGS) methods, improving alignment of large language models with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing reward-guided search methods rely on outcome reward models (ORMs), which mismatch with the process rewards needed for policy guidance, leading to inconsistent scoring and suboptimal alignment.

Method: The authors propose SP-PRM, integrating score consistency and preference consistency modules for partial evaluation without human annotation.

Result: SP-PRM improves GPT-4 evaluation scores by 3.6%-10.3% across dialogue, summarization, and reasoning tasks.

Conclusion: SP-PRM effectively enhances RGS methods by aligning process rewards with human preferences, addressing the limitations of ORMs.

Abstract: Inference-time alignment methods have gained significant attention for their
efficiency and effectiveness in aligning large language models (LLMs) with
human preferences. However, existing dominant approaches using reward-guided
search (RGS) primarily rely on outcome reward models (ORMs), which suffer from
a critical granularity mismatch: ORMs are designed to provide outcome rewards
for complete responses, while RGS methods rely on process rewards to guide the
policy, leading to inconsistent scoring and suboptimal alignment. To address
this challenge, we introduce process reward models (PRMs) into RGS and argue
that an ideal PRM should satisfy two objectives: Score Consistency, ensuring
coherent evaluation across partial and complete responses, and Preference
Consistency, aligning partial sequence assessments with human preferences.
Based on these, we propose SP-PRM, a novel dual-consistency framework
integrating score consistency-based and preference consistency-based partial
evaluation modules without relying on human annotation. Extensive experiments
on dialogue, summarization, and reasoning tasks demonstrate that SP-PRM
substantially enhances existing RGS methods, achieving a 3.6%-10.3% improvement
in GPT-4 evaluation scores across all tasks.

</details>


### [140] [FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation](https://arxiv.org/pdf/2506.12494)
*Zhuocheng Zhang, Yang Feng, Min Zhang*

Main category: cs.CL

TL;DR: FlexRAG is an open-source framework addressing challenges in existing RAG systems, offering flexibility and efficiency for research and prototyping.


<details>
  <summary>Details</summary>
Motivation: Existing RAG frameworks face issues like algorithm reproduction difficulties, lack of innovation, and high overhead.

Method: FlexRAG supports text-based, multimodal, and network-based RAG with lifecycle support, asynchronous processing, and persistent caching.

Result: FlexRAG provides a robust, flexible solution for rapid development and sharing of advanced RAG systems.

Conclusion: FlexRAG is a valuable tool for researchers, with resources available on GitHub.

Abstract: Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large
language model applications, with numerous existing frameworks offering a wide
range of functionalities to facilitate the development of RAG systems. However,
we have identified several persistent challenges in these frameworks, including
difficulties in algorithm reproduction and sharing, lack of new techniques, and
high system overhead. To address these limitations, we introduce
\textbf{FlexRAG}, an open-source framework specifically designed for research
and prototyping. FlexRAG supports text-based, multimodal, and network-based
RAG, providing comprehensive lifecycle support alongside efficient asynchronous
processing and persistent caching capabilities. By offering a robust and
flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and
share advanced RAG systems. Our toolkit and resources are available at
\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.

</details>


### [141] [Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders](https://arxiv.org/pdf/2506.12576)
*Ananya Joshi, Celia Cintas, Skyler Speakman*

Main category: cs.CL

TL;DR: The paper introduces a method using Sparse Autoencoders (SAE) to align large language model (LLM) outputs with any topic by scoring and modifying SAE neurons based on semantic similarity to alignment texts.


<details>
  <summary>Details</summary>
Motivation: To enable alignment of LLM outputs for any topic without the limitations of pre-identified topics or parameter tuning.

Method: Scores SAE neurons by semantic similarity to alignment texts and modifies outputs by emphasizing topic-aligned neurons.

Result: Outperforms fine-tuning in language acceptability (0.5 vs. 0.25), reduces training time (62s vs. 333.6s), and adds minimal inference overhead (+0.00092s/token).

Conclusion: The approach is efficient and effective for aligning LLM outputs to diverse topics, with practical benefits over fine-tuning.

Abstract: Recent work shows that Sparse Autoencoders (SAE) applied to large language
model (LLM) layers have neurons corresponding to interpretable concepts. These
SAE neurons can be modified to align generated outputs, but only towards
pre-identified topics and with some parameter tuning. Our approach leverages
the observational and modification properties of SAEs to enable alignment for
any topic. This method 1) scores each SAE neuron by its semantic similarity to
an alignment text and uses them to 2) modify SAE-layer-level outputs by
emphasizing topic-aligned neurons. We assess the alignment capabilities of this
approach on diverse public topic datasets including Amazon reviews, Medicine,
and Sycophancy, across the currently available open-source LLMs and SAE pairs
(GPT2 and Gemma) with multiple SAEs configurations. Experiments aligning to
medical prompts reveal several benefits over fine-tuning, including increased
average language acceptability (0.25 vs. 0.5), reduced training time across
multiple alignment topics (333.6s vs. 62s), and acceptable inference time for
many applications (+0.00092s/token). Our open-source code is available at
github.com/IBM/sae-steering.

</details>


### [142] [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/pdf/2506.16692)
*Hyunsoo Yun, Eun Hak Lee*

Main category: cs.CL

TL;DR: A study integrates LLM and XAI to analyze how political ideologies influence transportation policymaking in South Korea, revealing key factors like party affiliation and sponsor characteristics.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of lawmakers' political ideologies on transportation-related legislative decisions.

Method: Combines LLM for classifying transportation bills and XAI to analyze relationships between political affiliations and legislative outcomes.

Result: Conservative and progressive sponsors, district size, and electoral population significantly shape legislative outcomes.

Conclusion: The framework provides insights into legislative dynamics and aids future policy development, with broader governance implications.

Abstract: Given the significant influence of lawmakers' political ideologies on
legislative decision-making, analyzing their impact on transportation-related
policymaking is of critical importance. This study introduces a novel framework
that integrates a large language model (LLM) with explainable artificial
intelligence (XAI) to analyze transportation-related legislative proposals.
Legislative bill data from South Korea's 21st National Assembly were used to
identify key factors shaping transportation policymaking. These include
political affiliations and sponsor characteristics. The LLM was employed to
classify transportation-related bill proposals through a stepwise filtering
process based on keywords, sentences, and contextual relevance. XAI techniques
were then applied to examine the relationships between political party
affiliation and associated attributes. The results revealed that the number and
proportion of conservative and progressive sponsors, along with district size
and electoral population, were critical determinants shaping legislative
outcomes. These findings suggest that both parties contributed to bipartisan
legislation through different forms of engagement, such as initiating or
supporting proposals. This integrated approach offers a valuable tool for
understanding legislative dynamics and guiding future policy development, with
broader implications for infrastructure planning and governance.

</details>


### [143] [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/pdf/2506.17525)
*Mingfei Lau, Qian Chen, Yeming Fang, Tingting Xu, Tongzhou Chen, Pavel Golik*

Main category: cs.CL

TL;DR: The paper audits multilingual speech datasets (Mozilla Common Voice 17.0, FLEURS, Vox Populi), revealing significant quality issues, especially in under-resourced languages, and proposes guidelines for improvement.


<details>
  <summary>Details</summary>
Motivation: To highlight quality issues in public multilingual speech datasets that may mislead downstream evaluations and to advocate for better language planning and data quality control.

Method: The study conducts a quality audit, categorizing issues into micro-level and macro-level, with a case analysis of Taiwanese Southern Min (nan_tw).

Result: Macro-level issues are more common in under-resourced languages, emphasizing the need for proactive language planning and improved dataset creation processes.

Conclusion: Proposes guidelines for future dataset development, stressing sociolinguistic awareness and community-led language revitalization.

Abstract: Our quality audit for three widely used public multilingual speech datasets -
Mozilla Common Voice 17.0, FLEURS, and Vox Populi - shows that in some
languages, these datasets suffer from significant quality issues, which may
obfuscate downstream evaluation results while creating an illusion of success.
We divide these quality issues into two categories: micro-level and
macro-level. We find that macro-level issues are more prevalent in less
institutionalized, often under-resourced languages. We provide a case analysis
of Taiwanese Southern Min (nan_tw) that highlights the need for proactive
language planning (e.g. orthography prescriptions, dialect boundary definition)
and enhanced data quality control in the dataset creation process. We conclude
by proposing guidelines and recommendations to mitigate these issues in future
dataset development, emphasizing the importance of sociolinguistic awareness
and language planning principles. Furthermore, we encourage research into how
this creation process itself can be leveraged as a tool for community-led
language planning and revitalization.

</details>


### [144] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/pdf/2506.17609)
*Lincan Li, Eren Erman Ozguven, Yue Zhao, Guang Wang, Yiqun Xie, Yushun Dong*

Main category: cs.CL

TL;DR: TyphoFormer enhances typhoon track forecasting by integrating natural language descriptions with numerical data using a Transformer model, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate typhoon forecasting is vital for disaster response, but current Transformer models lack broader contextual knowledge for sparse meteorological trajectories.

Method: TyphoFormer uses LLM-generated textual descriptions of typhoon attributes, embedding them as tokens in a Transformer encoder alongside numerical data.

Result: Experiments on HURDAT2 show TyphoFormer outperforms state-of-the-art methods, especially in nonlinear path shifts and sparse data scenarios.

Conclusion: Incorporating language descriptions improves typhoon forecasting reliability, demonstrating the value of contextual knowledge in sparse trajectory modeling.

Abstract: Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of sparse meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.

</details>


### [145] [KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via Knowledge-Augmented Generation](https://arxiv.org/pdf/2506.17728)
*Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, QiWei Wang, Xiaorui Wang, Xinkai Du, YangYang Hou, Yu Ao, ZhaoYang Wang, Zhengke Gui, ZhiYing Yi, Zhongpu Bo, Haofen Wang, Huajun Chen*

Main category: cs.CL

TL;DR: KAG-Thinker upgrades KAG to a multi-turn interactive reasoning framework using a lightweight LLM, enhancing logical coherence in Q&A tasks by decomposing questions into sub-problems and modeling dependencies.


<details>
  <summary>Details</summary>
Motivation: To improve logical coherence and contextual consistency in solving complex problems within domain-specific KBs using LLMs.

Method: Decomposes questions into logical forms (natural language and logical function), classifies tasks (Retrieval or Reasoning), and models dependencies. Uses knowledge boundary and depth solving modules for optimal source selection.

Result: Enhanced reasoning process in Q&A tasks by structured decomposition and explicit task modeling.

Conclusion: KAG-Thinker effectively improves reasoning in LLMs for domain-specific KBs through structured thinking and multi-turn interaction.

Abstract: In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn
interactive thinking and deep reasoning framework powered by a dedicated
parameter-light large language model (LLM). Our approach constructs a
structured thinking process for solving complex problems, enhancing the the
logical coherence and contextual consistency of the reasoning process in
question-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within
LLMs. Following the \textbf{Logical Form} guided retrieval and reasoning
technology route of KAG, this framework first decomposes complex questions into
independently solvable sub-problems (which are also referred to as logical
forms) through \textbf{breadth decomposition}. Each such logical form is
represented in two equivalent forms-natural language and logical function-and
subsequently classified as either a Knowledge Retrieval or Reasoning Analysis
task. Dependencies and parameter passing between these tasks are explicitly
modeled via logical function interfaces. In the solving process, the Retrieval
function performs retrieval tasks. It retrieves one-hop structured and
unstructured information of specified knowledge unit. While the Math and Deduce
functions are used to perform reasoning analysis tasks. Secondly, it is worth
noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external
knowledge sources are regarded as equivalent KBs. We use the \textbf{knowledge
boundary} module to determine the optimal source using self-regulatory
mechanisms such as confidence calibration and reflective reasoning, and use the
\textbf{depth solving} module to enhance the comprehensiveness of knowledge
acquisition...

</details>


### [146] [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/pdf/2506.18501)
*Wael Etaiwi, Bushra Alhijawi*

Main category: cs.CL

TL;DR: The study evaluates ChatGPT and DeepSeek across five NLP tasks, finding DeepSeek better for classification and reasoning, while ChatGPT excels in nuanced understanding.


<details>
  <summary>Details</summary>
Motivation: To comprehensively assess the strengths, weaknesses, and domain-specific abilities of LLMs like ChatGPT and DeepSeek in diverse NLP applications.

Method: A structured experimental protocol with identical, neutral prompts and evaluation on two benchmark datasets per task (sentiment analysis, topic classification, text summarization, machine translation, textual entailment).

Result: DeepSeek performs better in classification stability and logical reasoning; ChatGPT outperforms in tasks needing nuanced understanding and flexibility.

Conclusion: The findings guide the selection of the appropriate LLM based on specific task requirements.

Abstract: The increasing use of large language models (LLMs) in natural language
processing (NLP) tasks has sparked significant interest in evaluating their
effectiveness across diverse applications. While models like ChatGPT and
DeepSeek have shown strong results in many NLP domains, a comprehensive
evaluation is needed to understand their strengths, weaknesses, and
domain-specific abilities. This is critical as these models are applied to
various tasks, from sentiment analysis to more nuanced tasks like textual
entailment and translation. This study aims to evaluate ChatGPT and DeepSeek
across five key NLP tasks: sentiment analysis, topic classification, text
summarization, machine translation, and textual entailment. A structured
experimental protocol is used to ensure fairness and minimize variability. Both
models are tested with identical, neutral prompts and evaluated on two
benchmark datasets per task, covering domains like news, reviews, and
formal/informal texts. The results show that DeepSeek excels in classification
stability and logical reasoning, while ChatGPT performs better in tasks
requiring nuanced understanding and flexibility. These findings provide
valuable insights for selecting the appropriate LLM based on task requirements.

</details>


### [147] [Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach](https://arxiv.org/pdf/2506.19750)
*Takashi Nishibayashi, Seiji Kanazawa, Kumpei Yamada*

Main category: cs.CL

TL;DR: The paper proposes a Synthetic Vignette Simulation Approach to evaluate the impact of algorithm updates on Symptom Checkers (SCs) for rare diseases, using synthetic vignettes from HPO. It shows strong predictive accuracy for performance changes.


<details>
  <summary>Details</summary>
Motivation: Preventing performance degradation in SCs for rare diseases during algorithm updates is challenging due to lack of pre-deployment evaluation methods.

Method: Generates synthetic vignettes from HPO annotations to simulate SC interviews and predict diagnostic performance changes.

Result: Validated retrospectively, the method accurately predicted performance changes for diseases with HPO phenotype frequency (R²=0.83 for Recall@8, R²=0.78 for Precision@8).

Conclusion: The approach enables efficient, low-cost pre-deployment evaluation of SC algorithm updates for rare diseases, leveraging expert-curated knowledge for transparency.

Abstract: Symptom Checkers (SCs) provide medical information tailored to user symptoms.
A critical challenge in SC development is preventing unexpected performance
degradation for individual diseases, especially rare diseases, when updating
algorithms. This risk stems from the lack of practical pre-deployment
evaluation methods. For rare diseases, obtaining sufficient evaluation data
from user feedback is difficult. To evaluate the impact of algorithm updates on
the diagnostic performance for individual rare diseases before deployment, this
study proposes and validates a novel Synthetic Vignette Simulation Approach.
This approach aims to enable this essential evaluation efficiently and at a low
cost. To estimate the impact of algorithm updates, we generated synthetic
vignettes from disease-phenotype annotations in the Human Phenotype Ontology
(HPO), a publicly available knowledge base for rare diseases curated by
experts. Using these vignettes, we simulated SC interviews to predict changes
in diagnostic performance. The effectiveness of this approach was validated
retrospectively by comparing the predicted changes with actual performance
metrics using the R-squared ($R^2$) coefficient. Our experiment, covering eight
past algorithm updates for rare diseases, showed that the proposed method
accurately predicted performance changes for diseases with phenotype frequency
information in HPO (n=5). For these updates, we found a strong correlation for
both Recall@8 change ($R^2$ = 0.83,$p$ = 0.031) and Precision@8 change ($R^2$ =
0.78,$p$ = 0.047). Our proposed method enables the pre-deployment evaluation of
SC algorithm changes for individual rare diseases. This evaluation is based on
a publicly available medical knowledge database created by experts, ensuring
transparency and explainability for stakeholders. Additionally, SC developers
can efficiently improve diagnostic performance at a low cost.

</details>


### [148] [Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis](https://arxiv.org/pdf/2506.19753)
*Omar A. Essameldin, Ali O. Elbeih, Wael H. Gomaa, Wael F. Elsersy*

Main category: cs.CL

TL;DR: The study classifies 18 Arabic dialects using RNNs, Transformers, and LLMs, with MARBERTv2 achieving the best performance (65% accuracy, 64% F1-score).


<details>
  <summary>Details</summary>
Motivation: Address the challenge of Arabic dialect classification due to its linguistic diversity and applications like chatbots and social media monitoring.

Method: Tested RNN models, Transformer models, and LLMs (via prompt engineering) on the QADI dataset of Arabic tweets.

Result: MARBERTv2 outperformed others with 65% accuracy and 64% F1-score.

Conclusion: The study highlights key linguistic challenges in dialect identification and supports applications like personalized chatbots and social media monitoring.

Abstract: The Arabic language is among the most popular languages in the world with a
huge variety of dialects spoken in 22 countries. In this study, we address the
problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.
RNN models, Transformer models, and large language models (LLMs) via prompt
engineering are created and tested. Among these, MARBERTv2 performed best with
65% accuracy and 64% F1-score. Through the use of state-of-the-art
preprocessing techniques and the latest NLP models, this paper identifies the
most significant linguistic issues in Arabic dialect identification. The
results corroborate applications like personalized chatbots that respond in
users' dialects, social media monitoring, and greater accessibility for Arabic
communities.

</details>


### [149] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/pdf/2506.20199)
*Mengqi Wang, Tiantian Feng, Shrikanth Narayanan*

Main category: cs.CL

TL;DR: The study explores improving conversational emotion recognition (CER) in LLMs by enhancing in-context learning (ICL) with high-quality example retrieval. Augmented retrieval outperforms random methods, emphasizing coherent, paraphrased examples.


<details>
  <summary>Details</summary>
Motivation: High-performing LLM applications for subjective tasks like emotion recognition remain challenging, prompting investigation into better CER methods.

Method: Proposes random and augmented example retrieval strategies for ICL, analyzing conversational context's impact on CER accuracy. Tests on IEMOCAP, MELD, and EmoryNLP datasets.

Result: Augmented example retrieval consistently outperforms other methods across all datasets.

Conclusion: Retrieving coherent, paraphrased examples is crucial for enhancing CER accuracy in LLMs.

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [150] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/pdf/2506.20876)
*Sebastian Joseph, Lily Chen, Barry Wei, Michael Mackert, Iain J. Marshall, Paul Pu Liang, Ramez Kouzy, Byron C. Wallace, Junyi Jessy Li*

Main category: cs.CL

TL;DR: The paper explores challenges in applying automated fact-checking to medicine, highlighting issues like claim-evidence mismatches and subjective veracity, advocating for an interactive communication approach.


<details>
  <summary>Details</summary>
Motivation: The high-stakes nature of medical decisions and the inadequacy of medical literacy among users drive interest in automated fact-checking systems for medicine.

Method: The study examines how clinical experts verify social media claims by synthesizing medical evidence, identifying challenges in end-to-end fact-checking.

Result: Key challenges include connecting claims to clinical trials, ambiguities in claims, and subjective veracity labels, making end-to-end fact-checking impractical.

Conclusion: Fact-checking in medicine should be treated as an interactive communication problem, not an end-to-end process.

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [151] [Potemkin Understanding in Large Language Models](https://arxiv.org/pdf/2506.21521)
*Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan*

Main category: cs.CL

TL;DR: The paper questions the validity of LLM evaluations using benchmarks, introducing a framework to detect 'potemkin understanding'—false comprehension mimicking human errors. It finds such flaws widespread across models.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLM benchmark success reflects genuine understanding or superficial mimicry of human errors.

Method: Introduces a formal framework and two procedures: a specialized benchmark and a general method to quantify 'potemkins' (false understanding).

Result: Potemkins are common across models, tasks, and domains, indicating not just incorrect understanding but deeper incoherence in concept representations.

Conclusion: Benchmark evaluations may mislead if LLMs' errors don't mirror human misunderstandings, highlighting the need for more nuanced assessment methods.

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [152] [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/pdf/2506.21591)
*Shaoyu Dou, Yutian Shen, Mofan Chen, Zixuan Wang, Jiajie Xu, Qi Guo, Kailai Shao, Chao Chen, Haixiang Hu, Haibo Shi, Min Min, Liwen Zhang*

Main category: cs.CL

TL;DR: FinEval-KR is a new framework to evaluate LLMs' financial reasoning by decoupling knowledge and reasoning abilities, introducing distinct scores and a cognitive score based on Bloom's taxonomy. It includes a Chinese financial dataset and reveals LLMs' limitations in knowledge application.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to decouple knowledge and reasoning in financial tasks, lacking root cause analysis for failures.

Method: Introduces FinEval-KR with knowledge and reasoning scores, plus a cognitive score based on Bloom's taxonomy. Releases a Chinese financial dataset.

Result: LLM reasoning and higher-order cognitive abilities are key to accuracy. Top models struggle with knowledge application. Specialized financial LLMs lag behind general models.

Conclusion: FinEval-KR provides a better evaluation framework, highlighting LLMs' reasoning strengths and knowledge application weaknesses.

Abstract: Large Language Models (LLMs) demonstrate significant potential but face
challenges in complex financial reasoning tasks requiring both domain knowledge
and sophisticated reasoning. Current evaluation benchmarks often fall short by
not decoupling these capabilities indicators from single task performance and
lack root cause analysis for task failure. To address this, we introduce
FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'
knowledge and reasoning abilities independently, proposing distinct knowledge
score and reasoning score metrics. Inspired by cognitive science, we further
propose a cognitive score based on Bloom's taxonomy to analyze capabilities in
reasoning tasks across different cognitive levels. We also release a new
open-source Chinese financial reasoning dataset covering 22 subfields to
support reproducible research and further advancements in financial reasoning.
Our experimental results reveal that LLM reasoning ability and higher-order
cognitive ability are the core factors influencing reasoning accuracy. We also
specifically find that even top models still face a bottleneck with knowledge
application. Furthermore, our analysis shows that specialized financial LLMs
generally lag behind the top general large models across multiple metrics.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [153] [Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring](https://arxiv.org/pdf/2506.22437)
*Xinxin Sun, Peter Chang*

Main category: cs.CV

TL;DR: A physics-informed alignment framework improves crack detection in structural health monitoring by adapting KAZE architecture, outperforming traditional methods by up to 90% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional feature detectors like SIFT and SURF are unsuitable for thin crack localization due to high-frequency edge suppression, while lightweight alternatives like ORB and BRISK lack repeatability on textured surfaces.

Method: The framework uses nonlinear anisotropic diffusion for crack-preserving scale space and RANSAC-based homography estimation, requiring no training or calibration.

Result: Validated on field images, it reduces crack area and spine length errors by 70% and 90%, respectively, with sub-5% alignment error.

Conclusion: The method is robust, interpretable, and lightweight, suitable for scalable deployment in real-world SHM applications.

Abstract: Accurate image alignment is essential for monitoring crack evolution in
structural health monitoring (SHM), particularly under real-world conditions
involving perspective distortion, occlusion, and low contrast. However,
traditional feature detectors such as SIFT and SURF, which rely on
Gaussian-based scale spaces, tend to suppress high-frequency edges, making them
unsuitable for thin crack localization. Lightweight binary alternatives like
ORB and BRISK, while computationally efficient, often suffer from poor keypoint
repeatability on textured or shadowed surfaces. This study presents a
physics-informed alignment framework that adapts the open KAZE architecture to
SHM-specific challenges. By utilizing nonlinear anisotropic diffusion to
construct a crack-preserving scale space, and integrating RANSAC-based
homography estimation, the framework enables accurate geometric correction
without the need for training, parameter tuning, or prior calibration. The
method is validated on time-lapse images of masonry and concrete acquired via
handheld smartphone under varied field conditions, including shadow
interference, cropping, oblique viewing angles, and surface clutter. Compared
to classical detectors, the proposed framework reduces crack area and spine
length errors by up to 70 percent and 90 percent, respectively, while
maintaining sub-5 percent alignment error in key metrics. Unsupervised,
interpretable, and computationally lightweight, this approach supports scalable
deployment via UAVs and mobile platforms. By tailoring nonlinear scale-space
modeling to SHM image alignment, this work offers a robust and physically
grounded alternative to conventional techniques for tracking real-world crack
evolution.

</details>


### [154] [Counting with Confidence: Accurate Pest Monitoring in Water Traps](https://arxiv.org/pdf/2506.22438)
*Xumin Gao, Mark Stevens, Grzegorz Cielniak*

Main category: cs.CV

TL;DR: The paper proposes a method to evaluate pest counting confidence in images by combining counting results and environmental factors, improving accuracy over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Existing pest counting models lack reliability assessment in real-world deployments due to missing ground truth. This study aims to address this gap.

Method: Uses a pest detection network, image quality/complexity assessments, and pest distribution uniformity analysis. Introduces adaptive DBSCAN and a regression model for confidence prediction.

Result: Reduces MSE by 31.7% and improves R2 by 15.2% compared to baseline methods.

Conclusion: First study to comprehensively evaluate counting confidence, demonstrating significant improvements in accuracy and reliability.

Abstract: Accurate pest population monitoring and tracking their dynamic changes are
crucial for precision agriculture decision-making. A common limitation in
existing vision-based automatic pest counting research is that models are
typically evaluated on datasets with ground truth but deployed in real-world
scenarios without assessing the reliability of counting results due to the lack
of ground truth. To this end, this paper proposed a method for comprehensively
evaluating pest counting confidence in the image, based on information related
to counting results and external environmental conditions. First, a pest
detection network is used for pest detection and counting, extracting counting
result-related information. Then, the pest images undergo image quality
assessment, image complexity assessment, and pest distribution uniformity
assessment. And the changes in image clarity caused by stirring during image
acquisition are quantified by calculating the average gradient magnitude.
Notably, we designed a hypothesis-driven multi-factor sensitivity analysis
method to select the optimal image quality assessment and image complexity
assessment methods. And we proposed an adaptive DBSCAN clustering algorithm for
pest distribution uniformity assessment. Finally, the obtained information
related to counting results and external environmental conditions is input into
a regression model for prediction, resulting in the final pest counting
confidence. To the best of our knowledge, this is the first study dedicated to
comprehensively evaluating counting confidence in counting tasks, and
quantifying the relationship between influencing factors and counting
confidence through a model. Experimental results show our method reduces MSE by
31.7% and improves R2 by 15.2% on the pest counting confidence test set,
compared to the baseline built primarily on information related to counting
results.

</details>


### [155] [Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization](https://arxiv.org/pdf/2506.22463)
*Weizhi Gao, Zhichao Hou, Junqi Yin, Feiyi Wang, Linyu Peng, Xiaorui Liu*

Main category: cs.CV

TL;DR: The paper introduces MoDiff, a framework to accelerate diffusion models using modulated quantization and error compensation, reducing computation costs without performance loss.


<details>
  <summary>Details</summary>
Motivation: High computation costs in iterative sampling of diffusion models limit their efficiency, and existing acceleration techniques have limitations in error and quality.

Method: MoDiff employs modulated quantization and error compensation to accelerate diffusion models while maintaining performance.

Result: MoDiff reduces activation quantization from 8 to 3 bits without performance degradation, validated on CIFAR-10 and LSUN datasets.

Conclusion: MoDiff is a principled and effective framework for accelerating diffusion models, outperforming existing methods.

Abstract: Diffusion models have emerged as powerful generative models, but their high
computation cost in iterative sampling remains a significant bottleneck. In
this work, we present an in-depth and insightful study of state-of-the-art
acceleration techniques for diffusion models, including caching and
quantization, revealing their limitations in computation error and generation
quality. To break these limits, this work introduces Modulated Diffusion
(MoDiff), an innovative, rigorous, and principled framework that accelerates
generative modeling through modulated quantization and error compensation.
MoDiff not only inherents the advantages of existing caching and quantization
methods but also serves as a general framework to accelerate all diffusion
models. The advantages of MoDiff are supported by solid theoretical insight and
analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate
that MoDiff significant reduces activation quantization from 8 bits to 3 bits
without performance degradation in post-training quantization (PTQ). Our code
implementation is available at https://github.com/WeizhiGao/MoDiff.

</details>


### [156] [ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction](https://arxiv.org/pdf/2506.22498)
*Hao Liu, Yu Hu, Rakiba Rayhana, Ling Bai, Zheng Liu*

Main category: cs.CV

TL;DR: A novel method using load cells and image-based fusion predicts bed-exit intent early, outperforming existing baselines for fall prevention.


<details>
  <summary>Details</summary>
Motivation: Bed-related falls are a major injury source in healthcare; existing alarms react too late.

Method: Uses four load cells under bed legs, converts signals into images (RGB line plot and texture maps), and processes them with ViFusionTST, a dual-stream Swin Transformer.

Result: Achieves 0.885 accuracy and 0.794 F1 score on real-world data, surpassing 1D/2D baselines.

Conclusion: Image-based fusion of load-sensor signals is effective for real-time, privacy-preserving fall prevention.

Abstract: Bed-related falls remain a leading source of injury in hospitals and
long-term-care facilities, yet many commercial alarms trigger only after a
patient has already left the bed. We show that early bed-exit intent can be
predicted using only four low-cost load cells mounted under the bed legs. The
resulting load signals are first converted into a compact set of complementary
images: an RGB line plot that preserves raw waveforms and three texture maps -
recurrence plot, Markov transition field, and Gramian angular field - that
expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin
Transformer that processes the line plot and texture maps in parallel and fuses
them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data
from 95 beds in a long-term-care facility. On this real-world dataset
ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing
recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.
The results demonstrate that image-based fusion of load-sensor signals for time
series classification is a practical and effective solution for real-time,
privacy-preserving fall prevention.

</details>


### [157] [Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data](https://arxiv.org/pdf/2506.22499)
*Jiachao Liu, Pablo Guarda, Koichiro Niinuma, Sean Qian*

Main category: cs.CV

TL;DR: A novel framework for dynamic origin-destination demand estimation (DODE) integrates satellite imagery with local sensor data, improving accuracy and scalability in multi-class mesoscopic network models.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of sparse local traffic detectors by leveraging consistent, city-wide satellite imagery for comprehensive traffic data.

Method: Develop a computer vision pipeline for vehicle detection and map matching, then formulate a computational graph-based DODE model to calibrate network states using combined data sources.

Result: Satellite-derived density significantly enhances estimation accuracy, particularly for unsensed links, and the framework scales well in real-world large networks.

Conclusion: The integrated framework shows promise for practical deployment in cities of varying sizes, with sensitivity to satellite data quality.

Abstract: This study presents a novel integrated framework for dynamic
origin-destination demand estimation (DODE) in multi-class mesoscopic network
models, leveraging high-resolution satellite imagery together with conventional
traffic data from local sensors. Unlike sparse local detectors, satellite
imagery offers consistent, city-wide road and traffic information of both
parking and moving vehicles, overcoming data availability limitations. To
extract information from imagery data, we design a computer vision pipeline for
class-specific vehicle detection and map matching, generating link-level
traffic density observations by vehicle class. Building upon this information,
we formulate a computational graph-based DODE model that calibrates dynamic
network states by jointly matching observed traffic counts and travel times
from local sensors with density measurements derived from satellite imagery. To
assess the accuracy and scalability of the proposed framework, we conduct a
series of numerical experiments using both synthetic and real-world data. The
results of out-of-sample tests demonstrate that supplementing traditional data
with satellite-derived density significantly improves estimation performance,
especially for links without local sensors. Real-world experiments also confirm
the framework's capability to handle large-scale networks, supporting its
potential for practical deployment in cities of varying sizes. Sensitivity
analysis further evaluates the impact of data quality related to satellite
imagery data.

</details>


### [158] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/pdf/2506.22899)
*Ehsan Pajouheshgar, Yitao Xu, Ali Abbasi, Alexander Mordvintsev, Wenzel Jakob, Sabine Süsstrunk*

Main category: cs.CV

TL;DR: Neural Cellular Automata (NCAs) are enhanced with an implicit decoder to overcome limitations in high-resolution tasks, enabling real-time full-HD outputs while preserving emergent properties.


<details>
  <summary>Details</summary>
Motivation: NCAs are limited by high computational costs and local information propagation, restricting them to low-resolution grids. This work aims to scale NCAs to high-resolution outputs efficiently.

Method: The paper pairs NCAs with a shared implicit decoder and introduces novel loss functions for high-resolution tasks, ensuring minimal memory and computation overhead.

Result: The proposed framework allows NCAs to generate full-HD outputs in real time, maintaining self-organizing properties and efficiency across 2D, 3D grids, and 3D meshes.

Conclusion: The integration of an implicit decoder and tailored loss functions enables NCAs to scale seamlessly to high-resolution tasks with minimal computational overhead.

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical
cells self-organize to form complex and coherent patterns by repeatedly
applying simple local rules. NCAs display striking emergent behaviors including
self-regeneration, generalization and robustness to unseen situations, and
spontaneous motion. Despite their success in texture synthesis and
morphogenesis, NCAs remain largely confined to low-resolution grids. This
limitation stems from (1) training time and memory requirements that grow
quadratically with grid size, (2) the strictly local propagation of information
which impedes long-range cell communication, and (3) the heavy compute demands
of real-time inference at high resolution. In this work, we overcome this
limitation by pairing NCA with a tiny, shared implicit decoder, inspired by
recent advances in implicit neural representations. Following NCA evolution on
a coarse grid, a lightweight decoder renders output images at arbitrary
resolution. We also propose novel loss functions for both morphogenesis and
texture synthesis tasks, specifically tailored for high-resolution output with
minimal memory and computation overhead. Combining our proposed architecture
and loss functions brings substantial improvement in quality, efficiency, and
performance. NCAs equipped with our implicit decoder can generate full-HD
outputs in real time while preserving their self-organizing, emergent
properties. Moreover, because each MLP processes cell states independently,
inference remains highly parallelizable and efficient. We demonstrate the
applicability of our approach across multiple NCA variants (on 2D, 3D grids,
and 3D meshes) and multiple tasks, including texture generation and
morphogenesis (growing patterns from a seed), showing that with our proposed
framework, NCAs seamlessly scale to high-resolution outputs with minimal
computational overhead.

</details>


### [159] [ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment](https://arxiv.org/pdf/2506.22967)
*Amir Aghdam, Vincent Tao Hu*

Main category: cs.CV

TL;DR: ActAlign is a zero-shot framework for fine-grained video classification using sequence alignment with language-generated sub-action sequences, outperforming larger models without video-text supervision.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of zero-shot fine-grained video classification without temporal annotations or video examples for unseen classes.

Method: Uses a large language model to generate ordered sub-action sequences for each class, aligning them with video frames via Dynamic Time Warping (DTW) in a shared embedding space.

Result: Achieves 30.5% accuracy on ActionAtlas (human accuracy: 61.6%), outperforming larger models with 8x fewer parameters.

Conclusion: Structured language priors and classical alignment techniques can effectively enhance vision-language models for fine-grained video understanding.

Abstract: We address the task of zero-shot fine-grained video classification, where no
video examples or temporal annotations are available for unseen action classes.
While contrastive vision-language models such as SigLIP demonstrate strong
open-set recognition via mean-pooled image-text similarity, they fail to
capture the temporal structure critical for distinguishing fine-grained
activities. We introduce ActAlign, a zero-shot framework that formulates video
classification as sequence alignment. For each class, a large language model
generates an ordered sub-action sequence, which is aligned with video frames
using Dynamic Time Warping (DTW) in a shared embedding space. Without any
video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the
extremely challenging ActionAtlas benchmark, where human accuracy is only
61.6%. ActAlign outperforms billion-parameter video-language models while using
approximately 8x less parameters. These results demonstrate that structured
language priors, combined with classical alignment techniques, offer a scalable
and general approach to unlocking the open-set recognition potential of
vision-language models for fine-grained video understanding.

</details>


### [160] [Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models](https://arxiv.org/pdf/2506.22500)
*Weiyi Zhao, Xiaoyu Tan, Liang Liu, Sijia Li, Youwei Song, Xihe Qiu*

Main category: cs.CV

TL;DR: The paper introduces a synthetic dataset (OR-VSKC) to address visual-semantic knowledge conflicts in MLLMs for surgical risk detection, improving performance on trained entities but showing limitations on untrained ones.


<details>
  <summary>Details</summary>
Motivation: To improve automated operating room risk detection by addressing visual-semantic knowledge conflicts in multimodal large language models (MLLMs).

Method: Creation of a dataset with 34,000 synthetic images and 214 human-annotated images, followed by fine-tuning MLLMs on OR-VSKC.

Result: Fine-tuning improves detection of trained conflict entities and generalizes to new viewpoints, but performance on untrained entities remains poor.

Conclusion: The OR-VSKC dataset and benchmark are valuable resources for studying and improving MLLMs' visual-semantic knowledge consistency in surgical risk detection.

Abstract: Surgical risk identification is critical for patient safety and reducing
preventable medical errors. While multimodal large language models (MLLMs) show
promise for automated operating room (OR) risk detection, they often exhibit
visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety
violations despite understanding textual rules. To address this, we introduce a
dataset comprising over 34,000 synthetic images generated by diffusion models,
depicting operating room scenes containing entities that violate established
safety rules. These images were created to alleviate data scarcity and examine
MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated
images that serve as a gold-standard reference for validation. This
comprehensive dataset, spanning diverse perspectives, stages, and
configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC
significantly improves MLLMs' detection of trained conflict entities and
generalizes well to new viewpoints for these entities, but performance on
untrained entity types remains poor, highlighting learning specificity and the
need for comprehensive training. The main contributions of this work include:
(1) a data generation methodology tailored for rule-violation scenarios; (2)
the release of the OR-VSKC dataset and its associated benchmark as open-source
resources; and (3) an empirical analysis of violation-sensitive knowledge
consistency in representative MLLMs. The dataset and appendix are available at
https://github.com/zgg2577/VS-KC.

</details>


### [161] [CoreMark: Toward Robust and Universal Text Watermarking Technique](https://arxiv.org/pdf/2506.23066)
*Jiale Meng, Yiming Li, Zheming Lu, Zewei He, Hao Luo, Tianwei Zhang*

Main category: cs.CV

TL;DR: CoreMark introduces a text watermarking framework using CORE segments for robust, generalizable, and imperceptible data embedding, outperforming existing methods in resistance to attacks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in text watermarking like robustness, generalizability, and imperceptibility.

Method: Uses CORE segments for embedding, dynamically selects robust characters, adjusts thickness for data embedding, and employs a plug-and-play modulator for font size adaptation.

Result: CoreMark excels in generalizability and robustness against attacks (screenshot, print-scan, print-camera) while maintaining imperceptibility.

Conclusion: CoreMark offers a superior solution for text watermarking, balancing robustness, generalizability, and imperceptibility effectively.

Abstract: Text watermarking schemes have gained considerable attention in recent years,
yet still face critical challenges in achieving simultaneous robustness,
generalizability, and imperceptibility. This paper introduces a new embedding
paradigm,termed CORE, which comprises several consecutively aligned black pixel
segments. Its key innovation lies in its inherent noise resistance during
transmission and broad applicability across languages and fonts. Based on the
CORE, we present a text watermarking framework named CoreMark. Specifically,
CoreMark first dynamically extracts COREs from characters. Then, the characters
with stronger robustness are selected according to the lengths of COREs. By
modifying the thickness of the CORE, the hidden data is embedded into the
selected characters without causing significant visual distortions. Moreover, a
general plug-and-play embedding strength modulator is proposed, which can
adaptively enhance the robustness for small font sizes by adjusting the
embedding strength according to the font size. Experimental evaluation
indicates that CoreMark demonstrates outstanding generalizability across
multiple languages and fonts. Compared to existing methods, CoreMark achieves
significant improvements in resisting screenshot, print-scan, and print camera
attacks, while maintaining satisfactory imperceptibility.

</details>


### [162] [How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?](https://arxiv.org/pdf/2506.22501)
*Gautam Siddharth Kashyap, Manaswi Kulahara, Nipun Joshi, Usman Naseem*

Main category: cs.CV

TL;DR: The paper introduces SpatialNet-ViT, a model combining Vision Transformers and Multi-Task Learning to improve generalization in remote sensing classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing studies often focus on narrow tasks or datasets, limiting their generalizability across diverse remote sensing challenges.

Method: The proposed SpatialNet-ViT integrates Vision Transformers and Multi-Task Learning, alongside techniques like data augmentation and transfer learning.

Result: The model enhances classification accuracy and scalability while improving robustness and generalization across datasets.

Conclusion: SpatialNet-ViT offers a promising solution for diverse remote sensing classification tasks by leveraging advanced techniques for better performance.

Abstract: Remote sensing datasets offer significant promise for tackling key
classification tasks such as land-use categorization, object presence
detection, and rural/urban classification. However, many existing studies tend
to focus on narrow tasks or datasets, which limits their ability to generalize
across various remote sensing classification challenges. To overcome this, we
propose a novel model, SpatialNet-ViT, leveraging the power of Vision
Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach
combines spatial awareness with contextual understanding, improving both
classification accuracy and scalability. Additionally, techniques like data
augmentation, transfer learning, and multi-task learning are employed to
enhance model robustness and its ability to generalize across diverse datasets

</details>


### [163] [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/pdf/2506.23151)
*Vladislav Bargatin, Egor Chistov, Alexander Yakovenko, Dmitriy Vatolin*

Main category: cs.CV

TL;DR: MEMFOF is a memory-efficient multi-frame optical flow method that balances accuracy and GPU memory usage, achieving state-of-the-art performance with low memory overhead.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between accuracy and GPU memory consumption in optical flow estimation, especially for high-resolution inputs.

Method: Revisits RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols for multi-frame estimation.

Result: Outperforms alternatives in accuracy and efficiency, ranking first on benchmarks like Spring, Sintel, and KITTI-2015.

Conclusion: MEMFOF is robust for high-resolution flow estimation, offering a practical solution with minimal memory usage.

Abstract: Recent advances in optical flow estimation have prioritized accuracy at the
cost of growing GPU memory consumption, particularly for high-resolution
(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical
flow method that identifies a favorable trade-off between multi-frame
estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU
memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely
positions our method to be trained at native 1080p without the need for
cropping or downsampling. We systematically revisit design choices from
RAFT-like architectures, integrating reduced correlation volumes and
high-resolution training protocols alongside multi-frame estimation, to achieve
state-of-the-art performance across multiple benchmarks while substantially
reducing memory overhead. Our method outperforms more resource-intensive
alternatives in both accuracy and runtime efficiency, validating its robustness
for flow estimation at high resolutions. At the time of submission, our method
ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,
leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the
best Fl-all error on KITTI-2015 at 2.94%. The code is available at
https://github.com/msu-video-group/memfof.

</details>


### [164] [What Makes a Dribble Successful? Insights From 3D Pose Tracking Data](https://arxiv.org/pdf/2506.22503)
*Michiel Schepers, Pieter Robberechts, Jan Van Haaren, Jesse Davis*

Main category: cs.CV

TL;DR: The study uses 3D pose tracking data to enhance dribble evaluation in soccer, showing improved predictive performance over traditional 2D methods.


<details>
  <summary>Details</summary>
Motivation: Current 2D positional data lacks depth in capturing dribbling dynamics like balance and orientation, limiting insights.

Method: Pose-based features from 1,736 dribbles in the 2022/23 Champions League season were analyzed alongside traditional 2D data.

Result: Pose-based features (balance and attacker-defender alignment) significantly improve dribble success prediction.

Conclusion: 3D pose tracking offers deeper insights into dribbling skills, outperforming 2D methods.

Abstract: Data analysis plays an increasingly important role in soccer, offering new
ways to evaluate individual and team performance. One specific application is
the evaluation of dribbles: one-on-one situations where an attacker attempts to
bypass a defender with the ball. While previous research has primarily relied
on 2D positional tracking data, this fails to capture aspects like balance,
orientation, and ball control, limiting the depth of current insights. This
study explores how pose tracking data (capturing players' posture and movement
in three dimensions) can improve our understanding of dribbling skills. We
extract novel pose-based features from 1,736 dribbles in the 2022/23 Champions
League season and evaluate their impact on dribble success. Our results
indicate that features capturing the attacker's balance and the alignment of
the orientation between the attacker and defender are informative for
predicting dribble success. Incorporating these pose-based features on top of
features derived from traditional 2D positional data leads to a measurable
improvement in model performance.

</details>


### [165] [PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution](https://arxiv.org/pdf/2506.23254)
*Aradhana Mishra, Bumshik Lee*

Main category: cs.CV

TL;DR: PixelBoost, a novel diffusion model, improves image super-resolution by leveraging Brownian motion's stochastic nature, enhancing realism and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between realistic image generation and computational efficiency in diffusion-model-based super-resolution.

Method: Integrates controlled stochasticity into training, avoiding local optima, and uses a sigmoidal noise sequencing method for faster inference.

Result: Superior performance in LPIPS, LOE, PSNR, SSIM, and visual quality, with better edge reconstruction and adaptive learning.

Conclusion: PixelBoost effectively balances realism and efficiency, advancing super-resolution techniques.

Abstract: Diffusion-model-based image super-resolution techniques often face a
trade-off between realistic image generation and computational efficiency. This
issue is exacerbated when inference times by decreasing sampling steps,
resulting in less realistic and hazy images. To overcome this challenge, we
introduce a novel diffusion model named PixelBoost that underscores the
significance of embracing the stochastic nature of Brownian motion in advancing
image super-resolution, resulting in a high degree of realism, particularly
focusing on texture and edge definitions. By integrating controlled
stochasticity into the training regimen, our proposed model avoids convergence
to local optima, effectively capturing and reproducing the inherent uncertainty
of image textures and patterns. Our proposed model demonstrates superior
objective results in terms of learned perceptual image patch similarity
(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),
structural similarity index measure (SSIM), as well as visual quality. To
determine the edge enhancement, we evaluated the gradient magnitude and pixel
value, and our proposed model exhibited a better edge reconstruction
capability. Additionally, our model demonstrates adaptive learning capabilities
by effectively adjusting to Brownian noise patterns and introduces a sigmoidal
noise sequencing method that simplifies training, resulting in faster inference
speeds.

</details>


### [166] [Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection](https://arxiv.org/pdf/2506.22504)
*Hassan Baker, Austin J. Brockmeier*

Main category: cs.CV

TL;DR: Patch2Loc is an unsupervised method for detecting brain lesions in MRI by learning from normal patches and identifying abnormalities through prediction errors.


<details>
  <summary>Details</summary>
Motivation: To improve brain lesion detection without requiring annotated data, leveraging unsupervised learning for more accessible diagnostics.

Method: Trains a neural network to map patches to their spatial locations; detects abnormalities via higher prediction errors/variance, generating a heatmap for segmentation.

Result: Outperforms state-of-the-art unsupervised segmentation on BraTS2021, MSLUB, ATLAS, and WMH datasets.

Conclusion: Patch2Loc offers an effective unsupervised alternative for brain lesion segmentation, with potential for broader medical imaging applications.

Abstract: Detecting brain lesions as abnormalities observed in magnetic resonance
imaging (MRI) is essential for diagnosis and treatment. In the search of
abnormalities, such as tumors and malformations, radiologists may benefit from
computer-aided diagnostics that use computer vision systems trained with
machine learning to segment normal tissue from abnormal brain tissue. While
supervised learning methods require annotated lesions, we propose a new
unsupervised approach (Patch2Loc) that learns from normal patches taken from
structural MRI. We train a neural network model to map a patch back to its
spatial location within a slice of the brain volume. During inference, abnormal
patches are detected by the relatively higher error and/or variance of the
location prediction. This generates a heatmap that can be integrated into
pixel-wise methods to achieve finer-grained segmentation. We demonstrate the
ability of our model to segment abnormal brain tissues by applying our approach
to the detection of tumor tissues in MRI on T2-weighted images from BraTS2021
and MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show
that it outperforms the state-of-the art in unsupervised segmentation. The
codebase for this work can be found on our
\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.

</details>


### [167] [VisionScores -- A system-segmented image score dataset for deep learning tasks](https://arxiv.org/pdf/2506.23030)
*Alejandro Romero Amezcua, Mariano José Juan Rivera Meraz*

Main category: cs.CV

TL;DR: VisionScores introduces the first system-segmented image score dataset for machine learning, focusing on two-handed piano pieces with 24.8k samples in two scenarios: same composition type (Sonatinas) from different composers and different types from Franz Liszt.


<details>
  <summary>Details</summary>
Motivation: To provide a structured, high-density image dataset for machine/deep learning, capturing graphic similarity and composition patterns specific to piano pieces.

Method: Dataset includes 24.8k grayscale images (128x512 pixels) of two-handed piano scores, segmented by systems, with metadata and full-page scores for analysis. Two scenarios: same composition type (Sonatinas) from various composers and different types from Franz Liszt.

Result: A comprehensive dataset (VisionScores) with 14k Sonatina samples and 10.8k Liszt samples, including metadata and raw materials for further research.

Conclusion: VisionScores is a valuable resource for machine learning tasks in music, offering structured, instrument-specific data with rich metadata and raw materials.

Abstract: VisionScores presents a novel proposal being the first system-segmented image
score dataset, aiming to offer structure-rich, high information-density images
for machine and deep learning tasks. Delimited to two-handed piano pieces, it
was built to consider not only certain graphic similarity but also composition
patterns, as this creative process is highly instrument-dependent. It provides
two scenarios in relation to composer and composition type. The first, formed
by 14k samples, considers works from different authors but the same composition
type, specifically, Sonatinas. The latter, consisting of 10.8K samples,
presents the opposite case, various composition types from the same author,
being the one selected Franz Liszt. All of the 24.8k samples are formatted as
grayscale jpg images of $128 \times 512$ pixels. VisionScores supplies the
users not only the formatted samples but the systems' order and pieces'
metadata. Moreover, unsegmented full-page scores and the pre-formatted images
are included for further analysis.

</details>


### [168] [Weakly Supervised Object Segmentation by Background Conditional Divergence](https://arxiv.org/pdf/2506.22505)
*Hassan Baker, Matthew S. Emigh, Austin J. Brockmeier*

Main category: cs.CV

TL;DR: Proposes a weakly supervised method for binary object segmentation using image-wise labels and counterfactual backgrounds, outperforming unsupervised baselines in specialized domains.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of expensive pixel-wise segmentation in specialized domains by leveraging weak supervision (image-wise labels) for training.

Method: Uses weak supervision (image-wise labels) to train a masking network. Creates counterfactual images by blending segmented objects with clustered backgrounds and employs sample-based divergences for training.

Result: Outperforms unsupervised baselines in sonar and natural images, avoiding pretrained or generative networks.

Conclusion: Demonstrates effectiveness of weak supervision and counterfactual backgrounds for segmentation in specialized domains, with potential for broader applications.

Abstract: As a computer vision task, automatic object segmentation remains challenging
in specialized image domains without massive labeled data, such as synthetic
aperture sonar images, remote sensing, biomedical imaging, etc. In any domain,
obtaining pixel-wise segmentation masks is expensive. In this work, we propose
a method for training a masking network to perform binary object segmentation
using weak supervision in the form of image-wise presence or absence of an
object of interest, which provides less information but may be obtained more
quickly from manual or automatic labeling. A key step in our method is that the
segmented objects can be placed into background-only images to create
realistic, images of the objects with counterfactual backgrounds. To create a
contrast between the original and counterfactual background images, we propose
to first cluster the background-only images, and then during learning create
counterfactual images that blend objects segmented from their original source
backgrounds to backgrounds chosen from a targeted cluster. One term in the
training loss is the divergence between these counterfactual images and the
real object images with backgrounds of the target cluster. The other term is a
supervised loss for background-only images. While an adversarial critic could
provide the divergence, we use sample-based divergences. We conduct experiments
on side-scan and synthetic aperture sonar in which our approach succeeds
compared to previous unsupervised segmentation baselines that were only tested
on natural images. Furthermore, to show generality we extend our experiments to
natural images, obtaining reasonable performance with our method that avoids
pretrained networks, generative networks, and adversarial critics. The basecode
for this work can be found at
\href{GitHub}{https://github.com/bakerhassan/WSOS}.

</details>


### [169] [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/pdf/2506.22509)
*Hang Xu, Jie Huang, Linjiang Huang, Dong Li, Yidi Liu, Feng Zhao*

Main category: cs.CV

TL;DR: The paper proposes a training-free Domain Noise Alignment (DNA) method for Diffusion-based Dense Prediction (DDP) models to achieve domain adaptation by aligning noise statistics between domains.


<details>
  <summary>Details</summary>
Motivation: The exposure bias in diffusion models causes domain shift, and noise prediction statistics can capture domain differences, motivating a training-free solution.

Method: DNA aligns noise statistics of target domains with source domains or uses high-confidence regions in source-free DA to guide noise adjustment during sampling.

Result: The method effectively enhances DA capabilities across four dense prediction tasks.

Conclusion: DNA provides a training-free, effective solution for domain adaptation in DDP models.

Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which
enhances the dense prediction model's performance when tested on its unseen
domain. Recently, with the development of Diffusion-based Dense Prediction
(DDP) models, the exploration of DA designs tailored to this framework is worth
exploring, since the diffusion model is effective in modeling the distribution
transformation that comprises domain information. In this work, we propose a
training-free mechanism for DDP frameworks, endowing them with DA capabilities.
Our motivation arises from the observation that the exposure bias (e.g., noise
statistics bias) in diffusion brings domain shift, and different domains in
conditions of DDP models can also be effectively captured by the noise
prediction statistics. Based on this, we propose a training-free Domain Noise
Alignment (DNA) approach, which alleviates the variations of noise statistics
to domain changes during the diffusion sampling process, thereby achieving
domain adaptation. Specifically, when the source domain is available, we
directly adopt the DNA method to achieve domain adaptation by aligning the
noise statistics of the target domain with those of the source domain. For the
more challenging source-free DA, inspired by the observation that regions
closer to the source domain exhibit higher confidence meeting variations of
sampling noise, we utilize the statistics from the high-confidence regions
progressively to guide the noise statistic adjustment during the sampling
process. Notably, our method demonstrates the effectiveness of enhancing the DA
capability of DDP models across four common dense prediction tasks. Code is
available at
\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.

</details>


### [170] [JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](https://arxiv.org/pdf/2506.23552)
*Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, Youngjung Uh*

Main category: cs.CV

TL;DR: JAM-Flow is a unified framework for synthesizing facial motion and speech together, using flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) architecture.


<details>
  <summary>Details</summary>
Motivation: Existing generative models treat talking head synthesis and text-to-speech as separate tasks, missing the intrinsic link between facial motion and speech.

Method: JAM-Flow employs flow matching and MM-DiT with Motion-DiT and Audio-DiT modules, using selective joint attention and temporally aligned embeddings for cross-modal interaction.

Result: The framework supports diverse tasks like synchronized talking head generation and audio-driven animation within a single model.

Conclusion: JAM-Flow advances multi-modal generative modeling by enabling holistic audio-visual synthesis.

Abstract: The intrinsic link between facial motion and speech is often overlooked in
generative modeling, where talking head synthesis and text-to-speech (TTS) are
typically addressed as separate tasks. This paper introduces JAM-Flow, a
unified framework to simultaneously synthesize and condition on both facial
motion and speech. Our approach leverages flow matching and a novel Multi-Modal
Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT
and Audio-DiT modules. These are coupled via selective joint attention layers
and incorporate key architectural choices, such as temporally aligned
positional embeddings and localized joint attention masking, to enable
effective cross-modal interaction while preserving modality-specific strengths.
Trained with an inpainting-style objective, JAM-Flow supports a wide array of
conditioning inputs-including text, reference audio, and reference
motion-facilitating tasks such as synchronized talking head generation from
text, audio-driven animation, and much more, within a single, coherent model.
JAM-Flow significantly advances multi-modal generative modeling by providing a
practical solution for holistic audio-visual synthesis. project page:
https://joonghyuk.com/jamflow-web

</details>


### [171] [PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation](https://arxiv.org/pdf/2411.16750)
*Ziyao Zeng, Jingcheng Ni, Daniel Wang, Patrick Rim, Younjoon Chung, Fengyu Yang, Byung-Woo Hong, Alex Wong*

Main category: cs.CV

TL;DR: Language prior from text-to-image diffusion models improves monocular depth estimation by leveraging learned spatial relationships, enhancing perception of user-specified regions, and accelerating convergence.


<details>
  <summary>Details</summary>
Motivation: Traditional monocular depth estimation faces ambiguity and visual nuisance; language prior from diffusion models can mitigate these issues by leveraging pre-trained spatial knowledge.

Method: Proposes PriorDiffusion, using a pre-trained text-to-image diffusion model with image and text inputs to infer affine-invariant depth via denoising.

Result: Achieves faster training convergence, fewer inference steps, and state-of-the-art zero-shot performance on multiple datasets.

Conclusion: Language prior enhances depth estimation by improving perception and accelerating convergence, demonstrating superior zero-shot performance.

Abstract: Traditional monocular depth estimation suffers from inherent ambiguity and
visual nuisance. We argue that language prior can enhance monocular depth
estimation by leveraging the inductive bias learned during the text-to-image
pre-training of diffusion models. The ability of these models to generate
images that align with text indicates that they have learned the spatial
relationships, size, and shape of specified objects, which can be applied to
improve depth estimation. Thus, we propose PriorDiffusion, using a pre-trained
text-to-image diffusion model that takes both images and corresponding text
descriptions to infer affine-invariant depth through a denoising process. We
also show that language prior enhances the model's perception of specific
regions of images that users care about and describe. Simultaneously, language
prior acts as a constraint to accelerate the convergence of both training and
the inference diffusion trajectory. By training on HyperSim and Virtual KITTI,
we achieve faster training convergence, fewer inference diffusion steps, and
state-of-the-art zero-shot performance across NYUv2, KITTI, ETH3D, and ScanNet.
Code will be released upon acceptance.

</details>


### [172] [Lightning the Night with Generative Artificial Intelligence](https://arxiv.org/pdf/2506.22511)
*Tingting Zhou, Feng Zhang, Haoyang Fu, Baoxiang Pan, Renhe Zhang, Feng Lu, Zhixin Yang*

Main category: cs.CV

TL;DR: The paper introduces RefDiff, a generative diffusion model for retrieving visible light reflectance at night using thermal infrared data, improving accuracy and enabling uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: The lack of visible light at night limits continuous weather monitoring. This study aims to overcome this by using thermal infrared data to simulate visible light reflectance.

Method: Developed RefDiff, a generative diffusion model based on AGRI's multi-band thermal infrared data, enabling nighttime visible light reflectance retrieval.

Result: RefDiff achieves high accuracy (SSIM 0.90) and outperforms classical models, especially in complex cloud areas. Validation with VIIRS confirms its nighttime capability.

Conclusion: RefDiff advances nighttime visible light retrieval, expanding the potential applications of nighttime data in weather monitoring.

Abstract: The visible light reflectance data from geostationary satellites is crucial
for meteorological observations and plays an important role in weather
monitoring and forecasting. However, due to the lack of visible light at night,
it is impossible to conduct continuous all-day weather observations using
visible light reflectance data. This study pioneers the use of generative
diffusion models to address this limitation. Based on the multi-band thermal
infrared brightness temperature data from the Advanced Geostationary Radiation
Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we
developed a high-precision visible light reflectance retrieval model, called
Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m},
0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance
retrieval at night. Compared to the classical models, RefDiff not only
significantly improves accuracy through ensemble averaging but also provides
uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,
with particularly significant improvements in areas with complex cloud
structures and thick clouds. The model's nighttime retrieval capability was
validated using VIIRS nighttime product, demonstrating comparable performance
to its daytime counterpart. In summary, this research has made substantial
progress in the ability to retrieve visible light reflectance at night, with
the potential to expand the application of nighttime visible light data.

</details>


### [173] [FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait](https://arxiv.org/pdf/2412.01064)
*Taekyung Ki, Dongchan Min, Gyeongsu Chae*

Main category: cs.CV

TL;DR: FLOAT introduces an audio-driven talking portrait video generation method using flow matching and a learned motion latent space, improving temporal consistency and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address challenges in temporally consistent video generation and fast sampling in diffusion-based models for portrait animation.

Method: Uses a learned orthogonal motion latent space and a transformer-based vector field predictor with frame-wise conditioning. Supports speech-driven emotion enhancement.

Result: Outperforms state-of-the-art methods in visual quality, motion fidelity, and efficiency.

Conclusion: FLOAT provides a robust solution for high-quality, efficient, and expressive talking portrait video generation.

Abstract: With the rapid advancement of diffusion-based generative models, portrait
image animation has achieved remarkable results. However, it still faces
challenges in temporally consistent video generation and fast sampling due to
its iterative sampling nature. This paper presents FLOAT, an audio-driven
talking portrait video generation method based on flow matching generative
model. Instead of a pixel-based latent space, we take advantage of a learned
orthogonal motion latent space, enabling efficient generation and editing of
temporally consistent motion. To achieve this, we introduce a transformer-based
vector field predictor with an effective frame-wise conditioning mechanism.
Additionally, our method supports speech-driven emotion enhancement, enabling a
natural incorporation of expressive motions. Extensive experiments demonstrate
that our method outperforms state-of-the-art audio-driven talking portrait
methods in terms of visual quality, motion fidelity, and efficiency.

</details>


### [174] [Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence](https://arxiv.org/pdf/2506.22513)
*Aditya Sharma*

Main category: cs.CV

TL;DR: The paper proposes an automated framework for fault detection in radiography using a modified U-net model and data augmentation, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the lack of explained information in NDE 4.0, optimize virtual defect augmentation, and validate the framework's viability.

Method: Uses 223 CR images of airplane welds, applies data augmentation (virtual defect and standard), and trains a modified U-net model for semantic fault segmentation.

Result: High defect detection awareness (a90/95 metrics), efficient processing of large images, and positive field evaluation by professionals.

Conclusion: The framework is viable, efficient, and promising as a support tool in testing, despite equipment and software limitations.

Abstract: This investigation attempts to create an automated framework for fault
detection and organization for usage in contemporary radiography, as per NDE
4.0. The review's goals are to address the lack of information that is
sufficiently explained, learn how to make the most of virtual defect increase,
and determine whether the framework is viable by using NDE measurements. As its
basic information source, the technique consists of compiling and categorizing
223 CR photographs of airplane welds. Information expansion systems, such as
virtual defect increase and standard increase, are used to work on the
preparation dataset. A modified U-net model is prepared using the improved data
to produce semantic fault division veils. To assess the effectiveness of the
model, NDE boundaries such as Case, estimating exactness, and misleading call
rate are used. Tiny a90/95 characteristics, which provide strong
differentiating evidence of flaws, reveal that the suggested approach achieves
exceptional awareness in defect detection. Considering a 90/95, size error, and
fake call rate in the weld area, the consolidated expansion approach clearly
wins. Due to the framework's fast derivation speed, large images can be broken
down efficiently and quickly. Professional controllers evaluate the transmitted
system in the field and believe that it has a guarantee as a support device in
the testing cycle, irrespective of particular equipment cut-off points and
programming resemblance.

</details>


### [175] [I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue](https://arxiv.org/pdf/2503.00071)
*Esam Ghaleb, Bulat Khaertdinov, Aslı Özyürek, Raquel Fernández*

Main category: cs.CV

TL;DR: The paper introduces a multimodal reference resolution task for representational gestures, proposes a self-supervised pre-training method for gesture embeddings, and demonstrates improved accuracy using multimodal representations and dialogue history.


<details>
  <summary>Details</summary>
Motivation: To address the understudied computational perspective of how representational co-speech gestures refer to objects in face-to-face interaction.

Method: A self-supervised pre-training approach for gesture representation learning that grounds body movements in spoken language.

Result: Learned embeddings align with expert annotations and improve reference resolution accuracy, especially with multimodal gesture representations and dialogue history.

Conclusion: Gestures and speech play complementary roles in reference resolution, advancing naturalistic human-machine interaction models.

Abstract: In face-to-face interaction, we use multiple modalities, including speech and
gestures, to communicate information and resolve references to objects.
However, how representational co-speech gestures refer to objects remains
understudied from a computational perspective. In this work, we address this
gap by introducing a multimodal reference resolution task centred on
representational gestures, while simultaneously tackling the challenge of
learning robust gesture embeddings. We propose a self-supervised pre-training
approach to gesture representation learning that grounds body movements in
spoken language. Our experiments show that the learned embeddings align with
expert annotations and have significant predictive power. Moreover, reference
resolution accuracy further improves when (1) using multimodal gesture
representations, even when speech is unavailable at inference time, and (2)
leveraging dialogue history. Overall, our findings highlight the complementary
roles of gesture and speech in reference resolution, offering a step towards
more naturalistic models of human-machine interaction.

</details>


### [176] [Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis](https://arxiv.org/pdf/2506.22517)
*Subhadip Kumar*

Main category: cs.CV

TL;DR: The paper compares three computer vision models (Yolov11, Yolov12, RF-DETR) for detecting container damage, finding RF-DETR superior for uncommon damages despite lower mAP scores.


<details>
  <summary>Details</summary>
Motivation: Timely detection of container damage is crucial for safety and prolonging service life in logistics.

Method: Used 278 annotated images to train and test Yolov11, Yolov12, and RF-DETR, comparing mAP and precision.

Result: Yolov11 and Yolov12 had higher mAP@50 (81.9%), but RF-DETR (77.7%) outperformed in detecting uncommon damages.

Conclusion: RF-DETR is better for detecting rare container damages, despite lower overall mAP scores.

Abstract: Containers are an integral part of the logistics industry and act as a
barrier for cargo. A typical service life for a container is more than 20
years. However, overtime containers suffer various types of damage due to the
mechanical as well as natural factors. A damaged container is a safety hazard
for the employees handling it and a liability for the logistic company.
Therefore, a timely inspection and detection of the damaged container is a key
for prolonging service life as well as avoiding safety hazards. In this paper,
we will compare the performance of the damage detection by three
state-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.
We will use a dataset of 278 annotated images to train, validate and test the
model. We will compare the mAP and precision of the model. The objective of
this paper is to identify the model that is best suited for container damage
detection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%
compared to RF-DETR, which was 77.7%. However, while testing the model for
not-so-common damaged containers, the RF-DETR model outperformed the others
overall, exhibiting superiority to accurately detecting both damaged containers
as well as damage occurrences with high confidence.

</details>


### [177] [Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement](https://arxiv.org/pdf/2503.06520)
*Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, Jiaya Jia*

Main category: cs.CV

TL;DR: Seg-Zero is a novel framework for reasoning segmentation that uses cognitive reinforcement and a decoupled architecture to improve generalization and explicit reasoning, outperforming prior methods by 18%.


<details>
  <summary>Details</summary>
Motivation: Traditional methods lack out-of-domain generalization and explicit reasoning processes, limiting their effectiveness.

Method: Seg-Zero employs a decoupled architecture with a reasoning model and a segmentation model, using reinforcement learning (GRPO) and a reward mechanism for optimization.

Result: Seg-Zero-7B achieves 57.5 on the ReasonSeg benchmark, surpassing LISA-7B by 18%.

Conclusion: Seg-Zero demonstrates robust zero-shot generalization and explicit reasoning, marking a significant advancement in reasoning segmentation.

Abstract: Traditional methods for reasoning segmentation rely on supervised fine-tuning
with categorical labels and simple descriptions, limiting its out-of-domain
generalization and lacking explicit reasoning processes. To address these
limitations, we propose Seg-Zero, a novel framework that demonstrates
remarkable generalizability and derives explicit chain-of-thought reasoning
through cognitive reinforcement. Seg-Zero introduces a decoupled architecture
consisting of a reasoning model and a segmentation model. The reasoning model
interprets user intentions, generates explicit reasoning chains, and produces
positional prompts, which are subsequently used by the segmentation model to
generate precious pixel-level masks. We design a sophisticated reward mechanism
that integrates both format and accuracy rewards to effectively guide
optimization directions. Trained exclusively via reinforcement learning with
GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot
generalization and exhibits emergent test-time reasoning capabilities.
Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on
the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\%. This significant
improvement highlights Seg-Zero's ability to generalize across domains while
presenting an explicit reasoning process. Code is available at
https://github.com/dvlab-research/Seg-Zero.

</details>


### [178] [Preserve Anything: Controllable Image Synthesis with Object Preservation](https://arxiv.org/pdf/2506.22531)
*Prasen Kumar Sharma, Neeraj Matiyali, Siddharth Srivastava, Gaurav Sharma*

Main category: cs.CV

TL;DR: The paper introduces "Preserve Anything," a method for controlled image synthesis that improves object preservation, semantic consistency, and user control in text-to-image generation. It outperforms existing methods in fidelity and alignment.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image methods struggle with preserving multiple objects, maintaining semantic alignment, and providing explicit control over scene composition.

Method: The method uses an N-channel ControlNet with modules for object preservation, background guidance, lighting consistency, and high-frequency overlay to retain details and reduce artifacts. A benchmark dataset of 240K natural and 18K synthetic images is introduced for evaluation.

Result: The method achieves state-of-the-art performance with FID 15.26 and CLIP-S 32.85, and user studies show significant improvements in prompt alignment, photorealism, artifact reduction, and aesthetics.

Conclusion: "Preserve Anything" addresses key limitations in text-to-image synthesis, offering superior performance and user control, validated by empirical results and user studies.

Abstract: We introduce \textit{Preserve Anything}, a novel method for controlled image
synthesis that addresses key limitations in object preservation and semantic
consistency in text-to-image (T2I) generation. Existing approaches often fail
(i) to preserve multiple objects with fidelity, (ii) maintain semantic
alignment with prompts, or (iii) provide explicit control over scene
composition. To overcome these challenges, the proposed method employs an
N-channel ControlNet that integrates (i) object preservation with size and
placement agnosticism, color and detail retention, and artifact elimination,
(ii) high-resolution, semantically consistent backgrounds with accurate
shadows, lighting, and prompt adherence, and (iii) explicit user control over
background layouts and lighting conditions. Key components of our framework
include object preservation and background guidance modules, enforcing lighting
consistency and a high-frequency overlay module to retain fine details while
mitigating unwanted artifacts. We introduce a benchmark dataset consisting of
240K natural images filtered for aesthetic quality and 18K 3D-rendered
synthetic images with metadata such as lighting, camera angles, and object
relationships. This dataset addresses the deficiencies of existing benchmarks
and allows a complete evaluation. Empirical results demonstrate that our method
achieves state-of-the-art performance, significantly improving feature-space
fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining
competitive aesthetic quality. We also conducted a user study to demonstrate
the efficacy of the proposed work on unseen benchmark and observed a remarkable
improvement of $\sim25\%$, $\sim19\%$, $\sim13\%$, and $\sim14\%$ in terms of
prompt alignment, photorealism, the presence of AI artifacts, and natural
aesthetics over existing works.

</details>


### [179] [Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset](https://arxiv.org/pdf/2506.22554)
*Vasu Agrawal, Akinniyi Akinyemi, Kathryn Alvero, Morteza Behrooz, Julia Buffalini, Fabio Maria Carlucci, Joy Chen, Junming Chen, Zhang Chen, Shiyang Cheng, Praveen Chowdary, Joe Chuang, Antony D'Avirro, Jon Daly, Ning Dong, Mark Duppenthaler, Cynthia Gao, Jeff Girard, Martin Gleize, Sahir Gomez, Hongyu Gong, Srivathsan Govindarajan, Brandon Han, Sen He, Denise Hernandez, Yordan Hristov, Rongjie Huang, Hirofumi Inaguma, Somya Jain, Raj Janardhan, Qingyao Jia, Christopher Klaiber, Dejan Kovachev, Moneish Kumar, Hang Li, Yilei Li, Pavel Litvin, Wei Liu, Guangyao Ma, Jing Ma, Martin Ma, Xutai Ma, Lucas Mantovani, Sagar Miglani, Sreyas Mohan, Louis-Philippe Morency, Evonne Ng, Kam-Woh Ng, Tu Anh Nguyen, Amia Oberai, Benjamin Peloquin, Juan Pino, Jovan Popovic, Omid Poursaeed, Fabian Prada, Alice Rakotoarison, Alexander Richard, Christophe Ropers, Safiyyah Saleem, Vasu Sharma, Alex Shcherbyna, Jia Shen, Jie Shen, Anastasis Stathopoulos, Anna Sun, Paden Tomasello, Tuan Tran, Arina Turkatenko, Bo Wan, Chao Wang, Jeff Wang, Mary Williamson, Carleigh Wood, Tao Xiang, Yilin Yang, Julien Yao, Chen Zhang, Jiemin Zhang, Xinyue Zhang, Jason Zheng, Pavlo Zhyzheria, Jan Zikes, Michael Zollhoefer*

Main category: cs.CV

TL;DR: The paper introduces the Seamless Interaction Dataset and models for AI to understand and generate dyadic behavioral dynamics, enhancing virtual agents and human-AI interactions.


<details>
  <summary>Details</summary>
Motivation: To develop socially intelligent AI by comprehending and generating dyadic behavioral dynamics in human communication.

Method: Creation of a large-scale dataset (Seamless Interaction Dataset) and development of models for generating dyadic motion gestures and facial expressions aligned with speech.

Result: Models capable of generating contextually relevant gestures and expressions, with controllable emotional responses and expressivity levels.

Conclusion: The work advances interactive virtual agents and human-AI interactions, with potential applications in telepresence and multimodal analysis.

Abstract: Human communication involves a complex interplay of verbal and nonverbal
signals, essential for conveying meaning and achieving interpersonal goals. To
develop socially intelligent AI technologies, it is crucial to develop models
that can both comprehend and generate dyadic behavioral dynamics. To this end,
we introduce the Seamless Interaction Dataset, a large-scale collection of over
4,000 hours of face-to-face interaction footage from over 4,000 participants in
diverse contexts. This dataset enables the development of AI technologies that
understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,
telepresence experiences, and multimodal content analysis tools. We also
develop a suite of models that utilize the dataset to generate dyadic motion
gestures and facial expressions aligned with human speech. These models can
take as input both the speech and visual behavior of their interlocutors. We
present a variant with speech from an LLM model and integrations with 2D and 3D
rendering methods, bringing us closer to interactive virtual agents.
Additionally, we describe controllable variants of our motion models that can
adapt emotional responses and expressivity levels, as well as generating more
semantically-relevant gestures. Finally, we discuss methods for assessing the
quality of these dyadic motion models, which are demonstrating the potential
for more intuitive and responsive human-AI interactions.

</details>


### [180] [Recomposed realities: animating still images via patch clustering and randomness](https://arxiv.org/pdf/2506.22556)
*Markus Juvonen, Samuli Siltanen*

Main category: cs.CV

TL;DR: A patch-based method uses k-means clustering to animate still images by reconstructing them from curated datasets, emphasizing reinterpretation over replication.


<details>
  <summary>Details</summary>
Motivation: To bring still images to life through motion by leveraging existing image data, allowing for creative reinterpretation.

Method: Uses k-means clustering to group image patches from datasets, then reconstructs target images by matching and randomly sampling from these clusters.

Result: Enables animation of still images while allowing conceptual differences between source and target domains.

Conclusion: The method successfully animates images by reinterpreting local structures from datasets, offering flexibility in creative applications.

Abstract: We present a patch-based image reconstruction and animation method that uses
existing image data to bring still images to life through motion. Image patches
from curated datasets are grouped using k-means clustering and a new target
image is reconstructed by matching and randomly sampling from these clusters.
This approach emphasizes reinterpretation over replication, allowing the source
and target domains to differ conceptually while sharing local structures.

</details>


### [181] [Improving Token-based Object Detection with Video](https://arxiv.org/pdf/2506.22562)
*Abhineet Singh, Nilanjan Ray*

Main category: cs.CV

TL;DR: The paper extends Pix2Seq for video object detection, introducing a method that uses discrete tokens for object representation and outputs 3D tracklets, improving scalability and eliminating postprocessing heuristics.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional video detectors, such as loss sparsity and heuristics-based postprocessing, by leveraging sequence-based object representation and integrated 3D tracklets.

Method: Represents objects as variable-length sequences of discrete tokens and outputs integrated 3D tracklets, enabling scalable and end-to-end video object detection.

Result: Outperforms the baseline Pix2Seq static detector and is competitive with state-of-the-art video detectors on UA-DETRAC, despite computational bottlenecks.

Conclusion: The proposed method offers a scalable and efficient approach to video object detection, with potential for generalization to multi-object tracking.

Abstract: This paper improves upon the Pix2Seq object detector by extending it for
videos. In the process, it introduces a new way to perform end-to-end video
object detection that improves upon existing video detectors in two key ways.
First, by representing objects as variable-length sequences of discrete tokens,
we can succinctly represent widely varying numbers of video objects, with
diverse shapes and locations, without having to inject any localization cues in
the training process. This eliminates the need to sample the space of all
possible boxes that constrains conventional detectors and thus solves the dual
problems of loss sparsity during training and heuristics-based postprocessing
during inference. Second, it conceptualizes and outputs the video objects as
fully integrated and indivisible 3D boxes or tracklets instead of generating
image-specific 2D boxes and linking these boxes together to construct the video
object, as done in most conventional detectors. This allows it to scale
effortlessly with available computational resources by simply increasing the
length of the video subsequence that the network takes as input, even
generalizing to multi-object tracking if the subsequence can span the entire
video. We compare our video detector with the baseline Pix2Seq static detector
on several datasets and demonstrate consistent improvement, although with
strong signs of being bottlenecked by our limited computational resources. We
also compare it with several video detectors on UA-DETRAC to show that it is
competitive with the current state of the art even with the computational
bottleneck. We make our code and models publicly available.

</details>


### [182] [Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation](https://arxiv.org/pdf/2506.22567)
*Shansong Wang, Zhecheng Jin, Mingzhe Hu, Mojtaba Safari, Feng Zhao, Chih-Wei Chang, Richard LJ Qiu, Justin Roper, David S. Yu, Xiaofeng Yang*

Main category: cs.CV

TL;DR: MMKD-CLIP, a biomedical foundation model, uses multi-teacher knowledge distillation to overcome data scarcity and heterogeneity, outperforming existing models across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Transferring CLIP's success to biomedicine is challenging due to limited data, heterogeneous modalities, and fragmented standards.

Method: Two-stage training: CLIP-style pretraining on 2.9M biomedical image-text pairs, followed by feature-level distillation from 9 teacher models.

Result: Outperforms teacher models on 58 datasets across 6 task types, showing robustness and generalization.

Conclusion: Multi-teacher knowledge distillation is scalable and effective for biomedical foundation models under real-world constraints.

Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs
have demonstrated impressive capabilities in zero-shot classification,
cross-modal retrieval, and open-ended visual answering. However, transferring
this success to biomedicine is hindered by the scarcity of large-scale
biomedical image-text corpora, the heterogeneity of image modalities, and
fragmented data standards across institutions. These limitations hinder the
development of a unified and generalizable biomedical foundation model trained
from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical
foundation model developed via Multiple Medical CLIP Knowledge Distillation.
Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge
from nine state-of-the-art domain-specific or generalist biomedical CLIP
models, each pretrained on millions of biomedical image-text pairs. Our
two-stage training pipeline first performs CLIP-style pretraining on over 2.9
million biomedical image-text pairs from 26 image modalities, followed by
feature-level distillation using over 19.2 million feature pairs extracted from
teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,
encompassing over 10.8 million biomedical images across nine image modalities.
The evaluation spans six core task types: zero-shot classification, linear
probing, cross-modal retrieval, visual question answering, survival prediction,
and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models
while demonstrating remarkable robustness and generalization across image
domains and task settings. These results underscore that multi-teacher
knowledge distillation is a scalable and effective paradigm for building
high-performing biomedical foundation models under the practical constraints of
real-world data availability.

</details>


### [183] [Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation](https://arxiv.org/pdf/2506.22570)
*Chee Mei Ling, Thangarajah Akilan, Aparna Ravinda Phalke*

Main category: cs.CV

TL;DR: The paper proposes an efficient image segmentation method for precision agriculture using a novel Dual Atrous Separable Convolution (DAS Conv) module within a DeepLabV3 framework, achieving high performance with lower computational complexity.


<details>
  <summary>Details</summary>
Motivation: To improve crop management and productivity by accurately delineating farmland anomalies through advanced semantic segmentation in agricultural imagery.

Method: Integration of the DAS Conv module in DeepLabV3, optimizing dilation rates and padding, and using strategic skip connections to enhance spatial feature capture.

Result: The model outperforms baselines and matches transformer-based SOTA models on the Agriculture Vision dataset, with a 66% efficiency improvement.

Conclusion: The study presents a lightweight, efficient solution for high-quality semantic segmentation in agricultural remote sensing.

Abstract: Agricultural image semantic segmentation is a pivotal component of modern
agriculture, facilitating accurate visual data analysis to improve crop
management, optimize resource utilization, and boost overall productivity. This
study proposes an efficient image segmentation method for precision
agriculture, focusing on accurately delineating farmland anomalies to support
informed decision-making and proactive interventions. A novel Dual Atrous
Separable Convolution (DAS Conv) module is integrated within the
DeepLabV3-based segmentation framework. The DAS Conv module is meticulously
designed to achieve an optimal balance between dilation rates and padding size,
thereby enhancing model performance without compromising efficiency. The study
also incorporates a strategic skip connection from an optimal stage in the
encoder to the decoder to bolster the model's capacity to capture fine-grained
spatial features. Despite its lower computational complexity, the proposed
model outperforms its baseline and achieves performance comparable to highly
complex transformer-based state-of-the-art (SOTA) models on the Agriculture
Vision benchmark dataset. It achieves more than 66% improvement in efficiency
when considering the trade-off between model complexity and performance,
compared to the SOTA model. This study highlights an efficient and effective
solution for improving semantic segmentation in remote sensing applications,
offering a computationally lightweight model capable of high-quality
performance in agricultural imagery.

</details>


### [184] [LIGHT: Multi-Modal Text Linking on Historical Maps](https://arxiv.org/pdf/2506.22589)
*Yijun Lin, Rhett Olson, Junhan Wu, Yao-Yi Chiang, Jerod Weinman*

Main category: cs.CV

TL;DR: LIGHT is a multi-modal approach integrating linguistic, image, and geometric features to link text on historical maps, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Text on historical maps is complex and varies in orientation, shape, and placement, making linking text fragments challenging. Existing methods lack geometric focus.

Method: LIGHT combines geometric, visual, and linguistic features using a geometry-aware embedding module and LayoutLMv3. It predicts reading-order successors with bi-directional learning.

Result: LIGHT outperforms existing methods on ICDAR 2024/2025 MapText Competition data.

Conclusion: Multi-modal learning is effective for linking text on historical maps, with LIGHT demonstrating superior performance.

Abstract: Text on historical maps provides valuable information for studies in history,
economics, geography, and other related fields. Unlike structured or
semi-structured documents, text on maps varies significantly in orientation,
reading order, shape, and placement. Many modern methods can detect and
transcribe text regions, but they struggle to effectively ``link'' the
recognized text fragments, e.g., determining a multi-word place name. Existing
layout analysis methods model word relationships to improve text understanding
in structured documents, but they primarily rely on linguistic features and
neglect geometric information, which is essential for handling map text. To
address these challenges, we propose LIGHT, a novel multi-modal approach that
integrates linguistic, image, and geometric features for linking text on
historical maps. In particular, LIGHT includes a geometry-aware embedding
module that encodes the polygonal coordinates of text regions to capture
polygon shapes and their relative spatial positions on an image. LIGHT unifies
this geometric information with the visual and linguistic token embeddings from
LayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal
information to predict the reading-order successor of each text instance
directly with a bi-directional learning strategy that enhances sequence
robustness. Experimental results show that LIGHT outperforms existing methods
on the ICDAR 2024/2025 MapText Competition data, demonstrating the
effectiveness of multi-modal learning for historical map text linking.

</details>


### [185] [BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data](https://arxiv.org/pdf/2506.22591)
*Arunkumar Kannan, Martin A. Lindquist, Brian Caffo*

Main category: cs.CV

TL;DR: BrainMT is a hybrid framework combining Mamba and transformer blocks to model long-range spatiotemporal dependencies in fMRI data, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle to capture complex spatiotemporal relationships in fMRI data, limiting their predictive accuracy.

Method: BrainMT uses a bidirectional Mamba block for temporal interactions and a transformer block for spatial relationships, processed in two stages.

Result: Outperforms existing methods on classification (sex prediction) and regression (cognitive intelligence prediction) tasks on UKBioBank and Human Connectome Project datasets.

Conclusion: BrainMT effectively addresses limitations of current approaches, offering improved performance for fMRI-based phenotype prediction.

Abstract: Recent advances in deep learning have made it possible to predict phenotypic
measures directly from functional magnetic resonance imaging (fMRI) brain
volumes, sparking significant interest in the neuroimaging community. However,
existing approaches, primarily based on convolutional neural networks or
transformer architectures, often struggle to model the complex relationships
inherent in fMRI data, limited by their inability to capture long-range spatial
and temporal dependencies. To overcome these shortcomings, we introduce
BrainMT, a novel hybrid framework designed to efficiently learn and integrate
long-range spatiotemporal attributes in fMRI data. Our framework operates in
two stages: (1) a bidirectional Mamba block with a temporal-first scanning
mechanism to capture global temporal interactions in a computationally
efficient manner; and (2) a transformer block leveraging self-attention to
model global spatial relationships across the deep features processed by the
Mamba block. Extensive experiments on two large-scale public datasets,
UKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves
state-of-the-art performance on both classification (sex prediction) and
regression (cognitive intelligence prediction) tasks, outperforming existing
methods by a significant margin. Our code and implementation details will be
made publicly available at this
https://github.com/arunkumar-kannan/BrainMT-fMRI

</details>


### [186] [Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning](https://arxiv.org/pdf/2506.22624)
*Zuyao You, Zuxuan Wu*

Main category: cs.CV

TL;DR: Seg-R1 uses RL to improve pixel-level understanding in LMMs for segmentation tasks, achieving strong performance and generalization without text supervision.


<details>
  <summary>Details</summary>
Motivation: Enhancing pixel-level reasoning in large multimodal models (LMMs) for segmentation tasks like COD and SOD.

Method: Uses RL to generate prompts for SAM2, introduces GRPO, and trains purely with RL.

Result: Achieves .873 S-measure on COD10K, 71.4 cIoU on RefCOCOg, and 56.7 gIoU on ReasonSeg.

Conclusion: Pure RL training enables strong performance and generalization in segmentation tasks.

Abstract: We present Seg-R1, a preliminary exploration of using reinforcement learning
(RL) to enhance the pixel-level understanding and reasoning capabilities of
large multimodal models (LMMs). Starting with foreground segmentation tasks,
specifically camouflaged object detection (COD) and salient object detection
(SOD), our approach enables the LMM to generate point and bounding box prompts
in the next-token fashion, which are then used to guide SAM2 in producing
segmentation masks. We introduce Group Relative Policy Optimization (GRPO) into
the segmentation domain, equipping the LMM with pixel-level comprehension
through a carefully designed training strategy. Notably, Seg-R1 achieves
remarkable performance with purely RL-based training, achieving .873 S-measure
on COD10K without complex model modification. Moreover, we found that pure RL
training demonstrates strong open-world generalization. Despite being trained
solely on foreground segmentation image-mask pairs without text supervision,
Seg-R1 achieves impressive zero-shot performance on referring segmentation and
reasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on
ReasonSeg test, outperforming models fully supervised on these datasets.

</details>


### [187] [ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models](https://arxiv.org/pdf/2506.22636)
*Sotirios Panagiotis Chytas, Miso Choi, Hyunwoo J. Kim, Vikas Singh*

Main category: cs.CV

TL;DR: The paper proposes a lightweight module (ReCo) to mitigate hallucination in Vision Language Models (VLMs) caused by fading memory of visual input, improving performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: VLMs often hallucinate due to over-reliance on language and fading memory of visual input, leading to ungrounded or contradictory outputs.

Method: Introduces ReCo, a small trainable module based on geometric algebra and relational compositions, added atop VLMs without other modifications.

Result: ReCo improves performance on three major VLMs (InstructBLIP, LlaVA, MiniGPT4) and complements other hallucination-reduction methods.

Conclusion: ReCo effectively addresses the fading memory effect in VLMs, enhancing their reliability and accuracy.

Abstract: Vision Language Models (VLMs) show impressive capabilities in integrating and
reasoning with both visual and language data. But these models make mistakes. A
common finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,
generate plausible sounding text which is not grounded in the visual input, or
at worst, is contradictory. A growing consensus attributes this behavior to an
over-reliance on language -- especially as the generation progresses, the model
suffers from a ``fading memory effect'' with respect to the provided visual
input. We study mechanisms by which this behavior can be controlled.
Specifically, using ideas from geometric algebra and relational compositions,
we propose the addition of a small, trainable module (named ReCo) on top of any
VLM -- no other modification is needed. We show that such a lightweight module
is able to mitigate the fading memory effect on three of the most widely used
VLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on
multiple benchmarks. Additionally, we show that our module can be combined with
many of the other approaches for reducing hallucination where we achieve
improved results for each one.

</details>


### [188] [CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation](https://arxiv.org/pdf/2506.22637)
*Haoxuan Wang, Zhenghao Zhao, Junyi Wu, Yuzhang Shang, Gaowen Liu, Yan Yan*

Main category: cs.CV

TL;DR: CaO$_2$ introduces a two-stage diffusion-based framework to address inconsistencies in dataset distillation, achieving state-of-the-art performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based dataset distillation methods suffer from objective and condition inconsistencies, leading to suboptimal performance.

Method: CaO$_2$ uses a two-stage approach: probability-informed sample selection and latent representation refinement.

Result: Outperforms baselines by 2.3% accuracy on ImageNet and subsets.

Conclusion: CaO$_2$ effectively aligns distillation with evaluation objectives, improving efficiency and performance.

Abstract: The recent introduction of diffusion models in dataset distillation has shown
promising potential in creating compact surrogate datasets for large,
high-resolution target datasets, offering improved efficiency and performance
over traditional bi-level/uni-level optimization methods. However, current
diffusion-based dataset distillation approaches overlook the evaluation process
and exhibit two critical inconsistencies in the distillation process: (1)
Objective Inconsistency, where the distillation process diverges from the
evaluation objective, and (2) Condition Inconsistency, leading to mismatches
between generated images and their corresponding conditions. To resolve these
issues, we introduce Condition-aware Optimization with Objective-guided
Sampling (CaO$_2$), a two-stage diffusion-based framework that aligns the
distillation process with the evaluation objective. The first stage employs a
probability-informed sample selection pipeline, while the second stage refines
the corresponding latent representations to improve conditional likelihood.
CaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,
surpassing the best-performing baselines by an average of 2.3% accuracy.

</details>


### [189] [3D Shape Generation: A Survey](https://arxiv.org/pdf/2506.22678)
*Nicolas Caytuiro, Ivan Sipiran*

Main category: cs.CV

TL;DR: A survey on 3D shape generation, covering representations, methods, and evaluation, with future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a structured overview of advancements in 3D shape generation and guide future research.

Method: Categorizes 3D representations, reviews generative approaches, and summarizes datasets and metrics.

Result: Organizes the field into core components and identifies open challenges.

Conclusion: Serves as a reference for understanding and advancing 3D shape generation.

Abstract: Recent advances in deep learning have significantly transformed the field of
3D shape generation, enabling the synthesis of complex, diverse, and
semantically meaningful 3D objects. This survey provides a comprehensive
overview of the current state of the art in 3D shape generation, organizing the
discussion around three core components: shape representations, generative
modeling approaches, and evaluation protocols. We begin by categorizing 3D
representations into explicit, implicit, and hybrid setups, highlighting their
structural properties, advantages, and limitations. Next, we review a wide
range of generation methods, focusing on feedforward architectures. We further
summarize commonly used datasets and evaluation metrics that assess fidelity,
diversity, and realism of generated shapes. Finally, we identify open
challenges and outline future research directions that could drive progress in
controllable, efficient, and high-quality 3D shape generation. This survey aims
to serve as a valuable reference for researchers and practitioners seeking a
structured and in-depth understanding of this rapidly evolving field.

</details>


### [190] [LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning](https://arxiv.org/pdf/2506.22710)
*Jiang Yuan, JI Ma, Bo Wang, Guanzhou Ke, Weiming Hu*

Main category: cs.CV

TL;DR: LightBSR proposes a lightweight blind super-resolution model by optimizing implicit degradation representation (IDR) discriminability, using knowledge distillation and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing IDE-BSR methods overlook IDR discriminability, complicating adaptation and increasing model complexity. LightBSR addresses this gap.

Method: Uses knowledge distillation: contrastive learning in the teacher stage for degradation discrimination, then feature alignment to transfer knowledge to the student model.

Result: Achieves high performance with minimal complexity in blind SR tasks.

Conclusion: LightBSR effectively balances performance and efficiency by focusing on IDR discriminability.

Abstract: Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges
on extracting the implicit degradation representation (IDR) of the LR image and
adapting it to LR image features to guide HR detail restoration. Although
IDE-BSR has shown potential in dealing with noise interference and complex
degradations, existing methods ignore the importance of IDR discriminability
for BSR and instead over-complicate the adaptation process to improve effect,
resulting in a significant increase in the model's parameters and computations.
In this paper, we focus on the discriminability optimization of IDR and propose
a new powerful and lightweight BSR model termed LightBSR. Specifically, we
employ a knowledge distillation-based learning framework. We first introduce a
well-designed degradation-prior-constrained contrastive learning technique
during teacher stage to make the model more focused on distinguishing different
degradation types. Then we utilize a feature alignment technique to transfer
the degradation-related knowledge acquired by the teacher to the student for
practical inferencing. Extensive experiments demonstrate the effectiveness of
IDR discriminability-driven BSR model design. The proposed LightBSR can achieve
outstanding performance with minimal complexity across a range of blind SR
tasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.

</details>


### [191] [Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians](https://arxiv.org/pdf/2506.22718)
*Jun-Jee Chao, Qingyuan Jiang, Volkan Isler*

Main category: cs.CV

TL;DR: A method for joint part segmentation and motion estimation from point clouds of articulated objects, robust to occlusions and missing data.


<details>
  <summary>Details</summary>
Motivation: Address challenges in analyzing articulated object motion when point clouds vary due to occlusions or asynchronous sensor data, making point correspondence tracking ineffective.

Method: Uses 3D Gaussians as building blocks with time-dependent transformations (rotations, translations, scales) shared across time steps. Part segmentation and motion estimation are derived from point-Gaussian correspondences.

Result: Outperforms point-correspondence methods, especially with occlusions, improving part segmentation by 13% over state-of-the-art.

Conclusion: The proposed representation is effective for joint part segmentation and motion estimation, robust to occlusions and missing data.

Abstract: Part segmentation and motion estimation are two fundamental problems for
articulated object motion analysis. In this paper, we present a method to solve
these two problems jointly from a sequence of observed point clouds of a single
articulated object. The main challenge in our problem setting is that the point
clouds are not assumed to be generated by a fixed set of moving points.
Instead, each point cloud in the sequence could be an arbitrary sampling of the
object surface at that particular time step. Such scenarios occur when the
object undergoes major occlusions, or if the dataset is collected using
measurements from multiple sensors asynchronously. In these scenarios, methods
that rely on tracking point correspondences are not appropriate. We present an
alternative approach based on a compact but effective representation where we
represent the object as a collection of simple building blocks modeled as 3D
Gaussians. We parameterize the Gaussians with time-dependent rotations,
translations, and scales that are shared across all time steps. With our
representation, part segmentation can be achieved by building correspondences
between the observed points and the Gaussians. Moreover, the transformation of
each point across time can be obtained by following the poses of the assigned
Gaussian (even when the point is not observed). Experiments show that our
method outperforms existing methods that solely rely on finding point
correspondences. Additionally, we extend existing datasets to emulate
real-world scenarios by considering viewpoint occlusions. We further
demonstrate that our method is more robust to missing points as compared to
existing approaches on these challenging datasets, even when some parts are
completely occluded in some time-steps. Notably, our part segmentation
performance outperforms the state-of-the-art method by 13% on point clouds with
occlusions.

</details>


### [192] [Deterministic Object Pose Confidence Region Estimation](https://arxiv.org/pdf/2506.22720)
*Jinghao Wang, Zhang Li, Zi Wang, Banglei Guan, Yang Shang, Qifeng Yu*

Main category: cs.CV

TL;DR: A deterministic method for 6D pose confidence region estimation using inductive conformal prediction and implicit function theorem, addressing inefficiency and inflated regions in sampling-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current sampling-based methods for 6D pose confidence region estimation are slow and produce excessively large regions, limiting practical use.

Method: Uses inductive conformal prediction to calibrate Gaussian keypoint distributions into 2D confidence regions, then propagates these into 6D pose regions via the implicit function theorem.

Result: Achieves higher accuracy and faster computation, reducing confidence region volumes by up to 99.9% for rotations and 99.8% for translations.

Conclusion: The proposed method efficiently provides compact confidence regions with user-defined coverage, outperforming sampling-based approaches.

Abstract: 6D pose confidence region estimation has emerged as a critical direction,
aiming to perform uncertainty quantification for assessing the reliability of
estimated poses. However, current sampling-based approach suffers from critical
limitations that severely impede their practical deployment: 1) the sampling
speed significantly decreases as the number of samples increases. 2) the
derived confidence regions are often excessively large. To address these
challenges, we propose a deterministic and efficient method for estimating pose
confidence regions. Our approach uses inductive conformal prediction to
calibrate the deterministically regressed Gaussian keypoint distributions into
2D keypoint confidence regions. We then leverage the implicit function theorem
to propagate these keypoint confidence regions directly into 6D pose confidence
regions. This method avoids the inefficiency and inflated region sizes
associated with sampling and ensembling. It provides compact confidence regions
that cover the ground-truth poses with a user-defined confidence level.
Experimental results on the LineMOD Occlusion and SPEED datasets show that our
method achieves higher pose estimation accuracy with reduced computational
time. For the same coverage rate, our method yields significantly smaller
confidence region volumes, reducing them by up to 99.9\% for rotations and
99.8\% for translations. The code will be available soon.

</details>


### [193] [XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge](https://arxiv.org/pdf/2506.22726)
*Yu Zhang, Xi Zhang, Hualin zhou, Xinyuan Chen, Shang Gao, Hong Jia, Jianfei Yang, Yuankai Qi, Tao Gu*

Main category: cs.CV

TL;DR: XTransfer is a novel method for efficient, modality-agnostic model transfer in edge-based human sensing, addressing issues like modality shift and resource constraints.


<details>
  <summary>Details</summary>
Motivation: Limited sensor data and edge system resources hinder deep learning for human sensing, with current methods suffering from accuracy loss and poor adaptability.

Method: XTransfer uses model repairing to fix modality shifts and layer recombining to create compact models from pre-trained sources.

Result: XTransfer outperforms baselines, reducing costs in data collection, training, and deployment while maintaining high accuracy.

Conclusion: XTransfer offers a scalable, efficient solution for human sensing on edge systems, advancing smart applications.

Abstract: Deep learning for human sensing on edge systems offers significant
opportunities for smart applications. However, its training and development are
hindered by the limited availability of sensor data and resource constraints of
edge systems. Current methods that rely on transferring pre-trained models
often encounter issues such as modality shift and high resource demands,
resulting in substantial accuracy loss, resource overhead, and poor
adaptability across different sensing applications. In this paper, we propose
XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic
model transfer. XTransfer freely leverages single or multiple pre-trained
models and transfers knowledge across different modalities by (i) model
repairing that safely repairs modality shift in pre-trained model layers with
only few sensor data, and (ii) layer recombining that efficiently searches and
recombines layers of interest from source models in a layer-wise manner to
create compact models. We benchmark various baselines across diverse human
sensing datasets spanning different modalities. Comprehensive results
demonstrate that XTransfer achieves state-of-the-art performance on human
sensing tasks while significantly reducing the costs of sensor data collection,
model training, and edge deployment.

</details>


### [194] [UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments](https://arxiv.org/pdf/2506.22736)
*Dayong Su, Yafei Zhang, Huafeng Li, Jinxing Li, Yu Liu*

Main category: cs.CV

TL;DR: UniFuse is a unified framework for multimodal medical image fusion that handles misaligned or degraded images by integrating alignment, restoration, and fusion in a single stage.


<details>
  <summary>Details</summary>
Motivation: Current fusion methods rely on high-quality, perfectly aligned images, limiting their effectiveness with misaligned or degraded inputs.

Method: UniFuse uses degradation-aware prompt learning, Omni Unified Feature Representation with Spatial Mamba, and a Universal Feature Restoration & Fusion module with ALSN.

Result: Experiments show UniFuse outperforms existing methods in handling misaligned or degraded images.

Conclusion: UniFuse successfully unifies alignment, restoration, and fusion, offering a robust solution for multimodal medical image fusion.

Abstract: Current multimodal medical image fusion typically assumes that source images
are of high quality and perfectly aligned at the pixel level. Its effectiveness
heavily relies on these conditions and often deteriorates when handling
misaligned or degraded medical images. To address this, we propose UniFuse, a
general fusion framework. By embedding a degradation-aware prompt learning
module, UniFuse seamlessly integrates multi-directional information from input
images and correlates cross-modal alignment with restoration, enabling joint
optimization of both tasks within a unified framework. Additionally, we design
an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to
encode multi-directional features and mitigate modality differences in feature
alignment. To enable simultaneous restoration and fusion within an All-in-One
configuration, we propose a Universal Feature Restoration & Fusion module,
incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA
principles. By leveraging ALSN's adaptive feature representation along with
degradation-type guidance, we enable joint restoration and fusion within a
single-stage framework. Compared to staged approaches, UniFuse unifies
alignment, restoration, and fusion within a single framework. Experimental
results across multiple datasets demonstrate the method's effectiveness and
significant advantages over existing approaches.

</details>


### [195] [Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds](https://arxiv.org/pdf/2506.22749)
*Yun Zhang, Feifan Chen, Na Li, Zhiwei Guo, Xu Wang, Fen Miao, Sam Kwong*

Main category: cs.CV

TL;DR: A deep learning-based Joint Geometry and Attribute Up-sampling (JGAU) method is proposed for generating high-quality colored point clouds by leveraging spatial attribute correlations and a new dataset, SYSU-PCUD.


<details>
  <summary>Details</summary>
Motivation: To enhance the quality and scalability of colored point clouds for realistic 3D applications by addressing geometry and attribute up-sampling challenges.

Method: Proposes JGAU framework with geometry and attribute up-sampling networks, coarse attribute up-sampling methods (GDWAI and DLAI), and an attribute enhancement module.

Result: Achieves PSNR gains of 2.11-2.47 decibels over state-of-the-art methods at various up-sampling rates.

Conclusion: JGAU significantly improves colored point cloud quality, demonstrating its effectiveness for large-scale and dense point cloud generation.

Abstract: Colored point cloud, which includes geometry and attribute components, is a
mainstream representation enabling realistic and immersive 3D applications. To
generate large-scale and denser colored point clouds, we propose a deep
learning-based Joint Geometry and Attribute Up-sampling (JGAU) method that
learns to model both geometry and attribute patterns while leveraging spatial
attribute correlations. First, we establish and release a large-scale dataset
for colored point cloud up-sampling called SYSU-PCUD, containing 121
large-scale colored point clouds with diverse geometry and attribute
complexities across six categories and four sampling rates. Second, to improve
the quality of up-sampled point clouds, we propose a deep learning-based JGAU
framework that jointly up-samples geometry and attributes. It consists of a
geometry up-sampling network and an attribute up-sampling network, where the
latter leverages the up-sampled auxiliary geometry to model neighborhood
correlations of the attributes. Third, we propose two coarse attribute
up-sampling methods, Geometric Distance Weighted Attribute Interpolation
(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate
coarse up-sampled attributes for each point. Then, an attribute enhancement
module is introduced to refine these up-sampled attributes and produce
high-quality point clouds by further exploiting intrinsic attribute and
geometry patterns. Extensive experiments show that the Peak Signal-to-Noise
Ratio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10
decibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,
8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art
methods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28
decibels, and 2.11 decibels at these four up-sampling rates, demonstrating
significant improvement.

</details>


### [196] [Point Cloud Compression and Objective Quality Assessment: A Survey](https://arxiv.org/pdf/2506.22902)
*Yiling Xu, Yujie Zhang, Shuting Xia, Kaifa Yang, He Huang, Ziyu Shan, Wenjie Huang, Qi Yang, Le Yang*

Main category: cs.CV

TL;DR: A survey of recent advances in point cloud compression (PCC) and quality assessment (PCQA), highlighting challenges and future directions for efficient 3D applications.


<details>
  <summary>Details</summary>
Motivation: The surge in 3D point cloud data necessitates efficient compression and quality assessment due to its irregular structure and high volume, especially for real-time and perceptual applications.

Method: Analyzes handcrafted and learning-based PCC algorithms and objective PCQA metrics, benchmarking them on emerging datasets.

Result: Provides detailed comparisons and insights into the strengths and limitations of current methods, identifying gaps like visual fidelity and latency reduction.

Conclusion: Future directions include hybrid compression frameworks and advanced feature extraction to improve efficiency and intelligence in 3D applications.

Abstract: The rapid growth of 3D point cloud data, driven by applications in autonomous
driving, robotics, and immersive environments, has led to criticals demand for
efficient compression and quality assessment techniques. Unlike traditional 2D
media, point clouds present unique challenges due to their irregular structure,
high data volume, and complex attributes. This paper provides a comprehensive
survey of recent advances in point cloud compression (PCC) and point cloud
quality assessment (PCQA), emphasizing their significance for real-time and
perceptually relevant applications. We analyze a wide range of handcrafted and
learning-based PCC algorithms, along with objective PCQA metrics. By
benchmarking representative methods on emerging datasets, we offer detailed
comparisons and practical insights into their strengths and limitations.
Despite notable progress, challenges such as enhancing visual fidelity,
reducing latency, and supporting multimodal data remain. This survey outlines
future directions, including hybrid compression frameworks and advanced feature
extraction strategies, to enable more efficient, immersive, and intelligent 3D
applications.

</details>


### [197] [Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography](https://arxiv.org/pdf/2506.22753)
*Jianing Zhang, Jiayi Zhu, Feiyu Ji, Xiaokang Yang, Xiaoyun Yuan*

Main category: cs.CV

TL;DR: The paper introduces Degradation-Modeled Multipath Diffusion (DMD) for metalens photography, leveraging pretrained models and tunable paths to address optical degradation and computational challenges without relying on large datasets.


<details>
  <summary>Details</summary>
Motivation: Metalenses face issues like optical degradation and computational restoration difficulties, often requiring precise calibration or massive datasets, which are impractical for real-world systems.

Method: The proposed framework uses multipath diffusion (positive, neutral, negative-prompt paths) and pseudo data augmentation, with a tunable decoder and SVDA module for degradation-aware attention. A MetaCamera is built for validation.

Result: The approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction.

Conclusion: DMD offers a practical solution for metalens photography, balancing fidelity and perceptual quality while avoiding dataset dependency.

Abstract: Metalenses offer significant potential for ultra-compact computational
imaging but face challenges from complex optical degradation and computational
restoration difficulties. Existing methods typically rely on precise optical
calibration or massive paired datasets, which are non-trivial for real-world
imaging systems. Furthermore, a lack of control over the inference process
often results in undesirable hallucinated artifacts. We introduce
Degradation-Modeled Multipath Diffusion for tunable metalens photography,
leveraging powerful natural image priors from pretrained models instead of
large datasets. Our framework uses positive, neutral, and negative-prompt paths
to balance high-frequency detail generation, structural fidelity, and
suppression of metalens-specific degradation, alongside \textit{pseudo} data
augmentation. A tunable decoder enables controlled trade-offs between fidelity
and perceptual quality. Additionally, a spatially varying degradation-aware
attention (SVDA) module adaptively models complex optical and sensor-induced
degradation. Finally, we design and build a millimeter-scale MetaCamera for
real-world validation. Extensive results show that our approach outperforms
state-of-the-art methods, achieving high-fidelity and sharp image
reconstruction. More materials: https://dmdiff.github.io/.

</details>


### [198] [RoboPearls: Editable Video Simulation for Robot Manipulation](https://arxiv.org/pdf/2506.22756)
*Tao Tang, Likui Zhang, Youpeng Wen, Kaidong Zhang, Jia-Wang Bian, xia zhou, Tianyi Yan, Kun Zhan, Peng Jia, Hefeng Wu, Liang Lin, Xiaodan Liang*

Main category: cs.CV

TL;DR: RoboPearls is a video simulation framework for robotic manipulation, leveraging 3D Gaussian Splatting and LLMs to create realistic, editable simulations and automate processes.


<details>
  <summary>Details</summary>
Motivation: High costs and inefficiencies in collecting real-world robotic demonstrations limit scalability, while existing simulators struggle with the sim-to-real gap.

Method: Uses 3D Gaussian Splatting for photo-realistic simulations, Incremental Semantic Distillation, 3D-NNFM Loss, and integrates LLMs for automation and VLMs for performance analysis.

Result: Demonstrated effectiveness on multiple datasets (RLBench, COLOSSEUM, etc.) and real-world robots, showing satisfactory simulation performance.

Conclusion: RoboPearls addresses scalability and sim-to-real challenges, offering a promising solution for robotic manipulation learning.

Abstract: The development of generalist robot manipulation policies has seen
significant progress, driven by large-scale demonstration data across diverse
environments. However, the high cost and inefficiency of collecting real-world
demonstrations hinder the scalability of data acquisition. While existing
simulation platforms enable controlled environments for robotic learning, the
challenge of bridging the sim-to-real gap remains. To address these challenges,
we propose RoboPearls, an editable video simulation framework for robotic
manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the
construction of photo-realistic, view-consistent simulations from demonstration
videos, and supports a wide range of simulation operators, including various
object manipulations, powered by advanced modules like Incremental Semantic
Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by
incorporating large language models (LLMs), RoboPearls automates the simulation
production process in a user-friendly manner through flexible command
interpretation and execution. Furthermore, RoboPearls employs a vision-language
model (VLM) to analyze robotic learning issues to close the simulation loop for
performance enhancement. To demonstrate the effectiveness of RoboPearls, we
conduct extensive experiments on multiple datasets and scenes, including
RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which
demonstrate our satisfactory simulation performance.

</details>


### [199] [A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks](https://arxiv.org/pdf/2506.23004)
*Vaigai Nayaki Yokar, Hoa Le-Minh, Xicong Li, Wai Lok Woo, Luis Nero Alves, Stanislav Zvanovec, Tran The Son, Zabih Ghassemlooy*

Main category: cs.CV

TL;DR: A lightweight CNN-based method for frame identification and synchronization in S2C VLC systems, achieving 98.74% accuracy.


<details>
  <summary>Details</summary>
Motivation: Enhance short-link communication performance in S2C VLC systems by addressing challenges like blurring, cropping, and rotated images.

Method: Supervised CNN model developed in Python/TensorFlow Keras, trained with real-time experiments and a custom dataset.

Result: Achieves 98.74% accuracy in frame identification and synchronization.

Conclusion: The proposed CNN model is effective for S2C VLC systems, improving performance with minimal overhead.

Abstract: This paper proposes a novel, robust, and lightweight supervised Convolutional
Neural Network (CNN)-based technique for frame identification and
synchronization, designed to enhance short-link communication performance in a
screen-to-camera (S2C) based visible light communication (VLC) system.
Developed using Python and the TensorFlow Keras framework, the proposed CNN
model was trained through three real-time experimental investigations conducted
in Jupyter Notebook. These experiments incorporated a dataset created from
scratch to address various real-time challenges in S2C communication, including
blurring, cropping, and rotated images in mobility scenarios. Overhead frames
were introduced for synchronization, which leads to enhanced system
performance. The experimental results demonstrate that the proposed model
achieves an overall accuracy of approximately 98.74%, highlighting its
effectiveness in identifying and synchronizing frames in S2C VLC systems.

</details>


### [200] [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/pdf/2506.22762)
*Dinh Phu Tran, Dao Duy Hung, Daeyoung Kim*

Main category: cs.CV

TL;DR: VSRM introduces a novel video super-resolution framework using Mamba for efficient long-range spatio-temporal feature extraction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Video super-resolution faces challenges with CNNs' limited receptive fields and Transformers' quadratic complexity. Mamba's linear complexity and long-sequence modeling offer a promising alternative.

Method: VSRM employs Spatial-to-Temporal and Temporal-to-Spatial Mamba blocks, a Deformable Cross-Mamba Alignment module, and a Frequency Charbonnier-like loss for feature alignment and quality enhancement.

Result: VSRM achieves state-of-the-art performance on diverse benchmarks.

Conclusion: VSRM sets a strong foundation for future research in video super-resolution by leveraging Mamba's advantages.

Abstract: Video super-resolution remains a major challenge in low-level vision tasks.
To date, CNN- and Transformer-based methods have delivered impressive results.
However, CNNs are limited by local receptive fields, while Transformers
struggle with quadratic complexity, posing challenges for processing long
sequences in VSR. Recently, Mamba has drawn attention for its long-sequence
modeling, linear complexity, and large receptive fields. In this work, we
propose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolution
framework that leverages the power of \textbf{M}amba. VSRM introduces
Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract
long-range spatio-temporal features and enhance receptive fields efficiently.
To better align adjacent frames, we propose Deformable Cross-Mamba Alignment
module. This module utilizes a deformable cross-mamba mechanism to make the
compensation stage more dynamic and flexible, preventing feature distortions.
Finally, we minimize the frequency domain gaps between reconstructed and
ground-truth frames by proposing a simple yet effective Frequency
Charbonnier-like loss that better preserves high-frequency content and enhances
visual quality. Through extensive experiments, VSRM achieves state-of-the-art
results on diverse benchmarks, establishing itself as a solid foundation for
future research.

</details>


### [201] [Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement](https://arxiv.org/pdf/2506.23353)
*Siyuan Chai, Xiaodong Guo, Tong Liu*

Main category: cs.CV

TL;DR: A task-oriented infrared image enhancement method improves contrast and preserves details for autonomous driving in complex weather.


<details>
  <summary>Details</summary>
Motivation: Infrared images suffer from low contrast, especially for non-heat-emitting objects, impacting high-level vision tasks like object detection and semantic segmentation.

Method: The method involves layer decomposition to enhance details and saliency information extraction using morphological reconstruction to avoid noise amplification.

Result: The approach outperforms state-of-the-art methods, improving image quality for downstream tasks.

Conclusion: The proposed method effectively enhances infrared images for autonomous driving applications.

Abstract: Infrared image helps improve the perception capabilities of autonomous
driving in complex weather conditions such as fog, rain, and low light.
However, infrared image often suffers from low contrast, especially in
non-heat-emitting targets like bicycles, which significantly affects the
performance of downstream high-level vision tasks. Furthermore, achieving
contrast enhancement without amplifying noise and losing important information
remains a challenge. To address these challenges, we propose a task-oriented
infrared image enhancement method. Our approach consists of two key components:
layer decomposition and saliency information extraction. First, we design an
layer decomposition method for infrared images, which enhances scene details
while preserving dark region features, providing more features for subsequent
saliency information extraction. Then, we propose a morphological
reconstruction-based saliency extraction method that effectively extracts and
enhances target information without amplifying noise. Our method improves the
image quality for object detection and semantic segmentation tasks. Extensive
experiments demonstrate that our approach outperforms state-of-the-art methods.

</details>


### [202] [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/pdf/2506.22783)
*Oguzhan Baser, Ahmet Ege Tanriverdi, Sriram Vishwanath, Sandeep P. Chinchali*

Main category: cs.CV

TL;DR: The paper introduces PhonemeFake (PF), a deepfake attack method that outperforms existing datasets in deceiving humans and benchmarks, and proposes a detection model for scalable defense.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake datasets fail to mimic real-world attacks, highlighting the need for more realistic attack vectors.

Method: PhonemeFake manipulates critical speech segments using language reasoning, and a bilevel detection model is introduced to prioritize compute on manipulated regions.

Result: PF reduces human perception by 42% and benchmark accuracies by 94%. The detection model cuts EER by 91% and speeds up by 90%.

Conclusion: PhonemeFake and the proposed detection model offer a scalable solution for realistic deepfake attacks and defense.

Abstract: Deepfake (DF) attacks pose a growing threat as generative models become
increasingly advanced. However, our study reveals that existing DF datasets
fail to deceive human perception, unlike real DF attacks that influence public
discourse. It highlights the need for more realistic DF attack vectors. We
introduce PhonemeFake (PF), a DF attack that manipulates critical speech
segments using language reasoning, significantly reducing human perception by
up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF
dataset on HuggingFace and open-source bilevel DF segment detection model that
adaptively prioritizes compute on manipulated regions. Our extensive
experiments across three known DF datasets reveal that our detection model
reduces EER by 91% while achieving up to 90% speed-up, with minimal compute
overhead and precise localization beyond existing models as a scalable
solution.

</details>


### [203] [Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks](https://arxiv.org/pdf/2506.23481)
*Xian Zhang, Xiang Cheng*

Main category: cs.CV

TL;DR: The paper analyzes the geolocation capabilities of Multimodal Large Language Models (MLLMs), highlighting privacy risks and evaluating their accuracy in identifying image origins.


<details>
  <summary>Details</summary>
Motivation: The study addresses privacy and ethical concerns arising from MLLMs' ability to infer geographic locations from images, posing risks like doxxing and surveillance.

Method: The research reviews existing geolocation techniques, evaluates state-of-the-art visual reasoning models, and tests their performance on street view imagery.

Result: Advanced models achieve 49% accuracy in localizing street-level images within a 1-kilometer radius, demonstrating their ability to extract geographic cues.

Conclusion: The study identifies key visual elements aiding geolocation and suggests technical and policy measures to mitigate privacy risks.

Abstract: Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)
has significantly enhanced their reasoning capabilities, enabling a wide range
of intelligent applications. However, these advancements also raise critical
concerns regarding privacy and ethics. MLLMs are now capable of inferring the
geographic location of images -- such as those shared on social media or
captured from street views -- based solely on visual content, thereby posing
serious risks of privacy invasion, including doxxing, surveillance, and other
security threats.
  Methods: This study provides a comprehensive analysis of existing geolocation
techniques based on MLLMs. It systematically reviews relevant litera-ture and
evaluates the performance of state-of-the-art visual reasoning models on
geolocation tasks, particularly in identifying the origins of street view
imagery.
  Results: Empirical evaluation reveals that the most advanced visual large
models can successfully localize the origin of street-level imagery with up to
$49\%$ accuracy within a 1-kilometer radius. This performance underscores the
models' powerful capacity to extract and utilize fine-grained geographic cues
from visual data.
  Conclusions: Building on these findings, the study identifies key visual
elements that contribute to suc-cessful geolocation, such as text,
architectural styles, and environmental features. Furthermore, it discusses the
potential privacy implications associated with MLLM-enabled geolocation and
discuss several technical and policy-based coun-termeasures to mitigate
associated risks. Our code and dataset are available at
https://github.com/zxyl1003/MLLM-Geolocation-Evaluation.

</details>


### [204] [Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching](https://arxiv.org/pdf/2506.22784)
*Yu Han, Zhiwei Huang, Yanting Zhang, Fangjun Ding, Shen Cai, Rui Fan*

Main category: cs.CV

TL;DR: A detector-free framework for direct point-pixel matching between LiDAR and camera views, addressing modality gaps and sparsity issues in single-frame LiDAR data.


<details>
  <summary>Details</summary>
Motivation: The modality gap between unstructured LiDAR point clouds and structured images, especially under sparse single-frame LiDAR settings, poses challenges for existing methods.

Method: Projects LiDAR intensity maps into 2D views and uses an attention-based detector-free matching network for cross-modal correspondence. Introduces a repeatability scoring mechanism to enhance reliability.

Result: Achieves state-of-the-art performance on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks, outperforming prior methods even with single-frame LiDAR.

Conclusion: The proposed detector-free framework effectively bridges the modality gap and improves robustness in sparse LiDAR settings without multi-frame accumulation.

Abstract: Point-pixel registration between LiDAR point clouds and camera images is a
fundamental yet challenging task in autonomous driving and robotic perception.
A key difficulty lies in the modality gap between unstructured point clouds and
structured images, especially under sparse single-frame LiDAR settings.
Existing methods typically extract features separately from point clouds and
images, then rely on hand-crafted or learned matching strategies. This separate
encoding fails to bridge the modality gap effectively, and more critically,
these methods struggle with the sparsity and noise of single-frame LiDAR, often
requiring point cloud accumulation or additional priors to improve reliability.
Inspired by recent progress in detector-free matching paradigms (e.g.
MatchAnything), we revisit the projection-based approach and introduce the
detector-free framework for direct point-pixel matching between LiDAR and
camera views. Specifically, we project the LiDAR intensity map into a 2D view
from the LiDAR perspective and feed it into an attention-based detector-free
matching network, enabling cross-modal correspondence estimation without
relying on multi-frame accumulation. To further enhance matching reliability,
we introduce a repeatability scoring mechanism that acts as a soft visibility
prior. This guides the network to suppress unreliable matches in regions with
low intensity variation, improving robustness under sparse input. Extensive
experiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that
our method achieves state-of-the-art performance, outperforming prior
approaches on nuScenes (even those relying on accumulated point clouds),
despite using only single-frame LiDAR.

</details>


### [205] [WaRA: Wavelet Low Rank Adaptation](https://arxiv.org/pdf/2506.24092)
*Moein Heidari, Yasamin Medghalchi, Mahdi Khoursha, Reza Rezaeian, Ilker Hacihaliloglu*

Main category: cs.CV

TL;DR: WaRA introduces a wavelet-based PEFT method for multi-resolution weight updates, outperforming LoRA in vision tasks and showing promise in language tasks.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA use global low-rank factorizations, missing local or multi-scale structures in weight updates.

Method: WaRA uses wavelet transforms to decompose weight updates into multi-resolution representations, enabling low-rank factorization in the wavelet domain.

Result: WaRA excels in vision tasks (image generation, classification, segmentation) and shows effectiveness in language tasks, improving quality and reducing complexity.

Conclusion: WaRA offers a flexible, sparse, and efficient alternative to LoRA, with broad applicability across vision and language tasks.

Abstract: Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across
various applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its
extensions have emerged as particularly effective, allowing efficient model
adaptation while significantly reducing computational overhead. However,
existing approaches typically rely on global low-rank factorizations, which
overlook local or multi-scale structure, failing to capture complex patterns in
the weight updates. To address this, we propose WaRA, a novel PEFT method that
leverages wavelet transforms to decompose the weight update matrix into a
multi-resolution representation. By performing low-rank factorization in the
wavelet domain and reconstructing updates through an inverse transform, WaRA
obtains compressed adaptation parameters that harness multi-resolution
analysis, enabling it to capture both coarse and fine-grained features while
providing greater flexibility and sparser representations than standard LoRA.
Through comprehensive experiments and analysis, we demonstrate that WaRA
performs superior on diverse vision tasks, including image generation,
classification, and semantic segmentation, significantly enhancing generated
image quality while reducing computational complexity. Although WaRA was
primarily designed for vision tasks, we further showcase its effectiveness in
language tasks, highlighting its broader applicability and generalizability.
The code is publicly available at
\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.

</details>


### [206] [RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors](https://arxiv.org/pdf/2506.22800)
*Sicong Du, Jiarun Liu, Qifeng Chen, Hao-Xiang Chen, Tai-Jiang Mu, Sheng Yang*

Main category: cs.CV

TL;DR: RGE-GS is a novel framework for expansive road structure reconstruction, combining diffusion-based generation with reward-guided Gaussian integration to improve quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Incomplete road scans from single-pass driving clips necessitate better reconstruction methods for sensor simulators, but current 3DGS extensions with diffusion priors introduce inconsistencies and inefficiencies.

Method: RGE-GS integrates a reward network to prioritize stable diffusion outputs and uses a differentiated training strategy for adaptive Gaussian optimization.

Result: RGE-GS achieves state-of-the-art reconstruction quality on public datasets.

Conclusion: The framework effectively addresses limitations of prior methods and will be made open-source.

Abstract: A single-pass driving clip frequently results in incomplete scanning of the
road structure, making reconstructed scene expanding a critical requirement for
sensor simulators to effectively regress driving actions. Although contemporary
3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction
quality, their direct extension through the integration of diffusion priors
often introduces cumulative physical inconsistencies and compromises training
efficiency. To address these limitations, we present RGE-GS, a novel expansive
reconstruction framework that synergizes diffusion-based generation with
reward-guided Gaussian integration. The RGE-GS framework incorporates two key
innovations: First, we propose a reward network that learns to identify and
prioritize consistently generated patterns prior to reconstruction phases,
thereby enabling selective retention of diffusion outputs for spatial
stability. Second, during the reconstruction process, we devise a
differentiated training strategy that automatically adjust Gaussian
optimization progress according to scene converge metrics, which achieving
better convergence than baseline methods. Extensive evaluations of publicly
available datasets demonstrate that RGE-GS achieves state-of-the-art
performance in reconstruction quality. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version
incorporating reviewer suggestions will be updated soon.)

</details>


### [207] [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/pdf/2506.22803)
*Nuoye Xiong, Anqi Dong, Ning Wang, Cong Hua, Guangming Zhu, Mei Lin, Peiyi Shen, Liang Zhang*

Main category: cs.CV

TL;DR: The paper introduces CBM-HNMU, a method to improve interpretability and accuracy of deep learning models by refining concepts in a Concept Bottleneck Model and distilling knowledge back into the original model.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are becoming more complex and less interpretable, with existing explanation methods lacking effective interventions or model modifications.

Method: CBM-HNMU uses the Concept Bottleneck Model to approximate black-box reasoning, identifies and refines detrimental concepts, and distills corrected knowledge back into the model.

Result: Evaluations on multiple datasets show accuracy improvements up to 2.64% and an average increase of 1.03%.

Conclusion: CBM-HNMU enhances both interpretability and model performance, offering a practical solution for human-neural network mutual understanding.

Abstract: Recent advances in deep learning have led to increasingly complex models with
deeper layers and more parameters, reducing interpretability and making their
decisions harder to understand. While many methods explain black-box reasoning,
most lack effective interventions or only operate at sample-level without
modifying the model itself. To address this, we propose the Concept Bottleneck
Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).
CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable
framework to approximate black-box reasoning and communicate conceptual
understanding. Detrimental concepts are automatically identified and refined
(removed/replaced) based on global gradient contributions. The modified CBM
then distills corrected knowledge back into the black-box model, enhancing both
interpretability and accuracy. We evaluate CBM-HNMU on various CNN and
transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,
and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum
increase in average accuracy across 1.03%. Source code is available at:
https://github.com/XiGuaBo/CBM-HNMU.

</details>


### [208] [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://arxiv.org/pdf/2506.22806)
*Byung Hyun Lee, Sungjin Lim, Seunggyu Lee, Dong Un Kang, Se Young Chun*

Main category: cs.CV

TL;DR: The paper introduces Concept Pinpoint Eraser (CPE), a method to erase specific concepts in text-to-image diffusion models while preserving others, outperforming prior methods by using nonlinear modules and adversarial training.


<details>
  <summary>Details</summary>
Motivation: Addressing the concern of generating inappropriate or trademarked images, the paper aims to improve concept erasure in diffusion models without distorting other concepts.

Method: CPE adds nonlinear Residual Attention Gates (ResAGs) and uses an attention anchoring loss to selectively erase target concepts. It also employs adversarial training with learnable text embeddings for robustness.

Result: CPE effectively erases target concepts (e.g., celebrities, artistic styles, explicit content) while preserving diverse remaining concepts and resisting adversarial attacks.

Conclusion: CPE outperforms existing methods by combining nonlinear modules and adversarial training, offering robust and precise concept erasure.

Abstract: Remarkable progress in text-to-image diffusion models has brought a major
concern about potentially generating images on inappropriate or trademarked
concepts. Concept erasing has been investigated with the goals of deleting
target concepts in diffusion models while preserving other concepts with
minimal distortion. To achieve these goals, recent concept erasing methods
usually fine-tune the cross-attention layers of diffusion models. In this work,
we first show that merely updating the cross-attention layers in diffusion
models, which is mathematically equivalent to adding \emph{linear} modules to
weights, may not be able to preserve diverse remaining concepts. Then, we
propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding
\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or
cut) target concepts while safeguarding remaining concepts from broad
distributions by employing an attention anchoring loss to prevent the
forgetting. Moreover, we adversarially train CPE with ResAG and learnable text
embeddings in an iterative manner to maximize erasing performance and enhance
robustness against adversarial attacks. Extensive experiments on the erasure of
celebrities, artistic styles, and explicit contents demonstrated that the
proposed CPE outperforms prior arts by keeping diverse remaining concepts while
deleting the target concepts with robustness against attack prompts. Code is
available at https://github.com/Hyun1A/CPE

</details>


### [209] [FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition](https://arxiv.org/pdf/2506.22807)
*Yueyang Li, Shengyu Gong, Weiming Zeng, Nizhuan Wang, Wai Ting Siok*

Main category: cs.CV

TL;DR: FreqDGT, a frequency-adaptive dynamic graph transformer, improves cross-subject emotion recognition in EEG by integrating frequency-adaptive processing, adaptive dynamic graph learning, and multi-scale temporal disentanglement.


<details>
  <summary>Details</summary>
Motivation: Cross-subject generalization in EEG-based emotion recognition is challenging due to individual variability and emotional response differences.

Method: FreqDGT uses frequency-adaptive processing (FAP), adaptive dynamic graph learning (ADGL), and a multi-scale temporal disentanglement network (MTDN) to address these challenges.

Result: FreqDGT significantly improves cross-subject emotion recognition accuracy, demonstrating robustness to individual differences.

Conclusion: The integration of frequency-adaptive, spatial-dynamic, and temporal-hierarchical modeling in FreqDGT effectively enhances EEG-based emotion recognition.

Abstract: Electroencephalography (EEG) serves as a reliable and objective signal for
emotion recognition in affective brain-computer interfaces, offering unique
advantages through its high temporal resolution and ability to capture
authentic emotional states that cannot be consciously controlled. However,
cross-subject generalization remains a fundamental challenge due to individual
variability, cognitive traits, and emotional responses. We propose FreqDGT, a
frequency-adaptive dynamic graph transformer that systematically addresses
these limitations through an integrated framework. FreqDGT introduces
frequency-adaptive processing (FAP) to dynamically weight emotion-relevant
frequency bands based on neuroscientific evidence, employs adaptive dynamic
graph learning (ADGL) to learn input-specific brain connectivity patterns, and
implements multi-scale temporal disentanglement network (MTDN) that combines
hierarchical temporal transformers with adversarial feature disentanglement to
capture both temporal dynamics and ensure cross-subject robustness.
Comprehensive experiments demonstrate that FreqDGT significantly improves
cross-subject emotion recognition accuracy, confirming the effectiveness of
integrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical
modeling while ensuring robustness to individual differences. The code is
available at https://github.com/NZWANG/FreqDGT.

</details>


### [210] [Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping](https://arxiv.org/pdf/2506.22814)
*Andrew Hamara, Andrew C. Freeman*

Main category: cs.CV

TL;DR: Extends Fixed Aspect Ratio Cropping to efficiently extract multiple non-overlapping crops in linear time by dynamically adjusting attention thresholds.


<details>
  <summary>Details</summary>
Motivation: Traditional methods optimize a single bounding box, limiting effectiveness for applications needing multiple disjoint crops.

Method: Extends Fixed Aspect Ratio Cropping algorithm, dynamically adjusts attention thresholds, and removes selected crops without recomputing the saliency map.

Result: Efficient extraction of multiple non-overlapping crops in linear time.

Conclusion: Qualitative results are discussed, with potential for future datasets and benchmarks.

Abstract: Automatic image cropping aims to extract the most visually salient regions
while preserving essential composition elements. Traditional saliency-aware
cropping methods optimize a single bounding box, making them ineffective for
applications requiring multiple disjoint crops. In this work, we extend the
Fixed Aspect Ratio Cropping algorithm to efficiently extract multiple
non-overlapping crops in linear time. Our approach dynamically adjusts
attention thresholds and removes selected crops from consideration without
recomputing the entire saliency map. We discuss qualitative results and
introduce the potential for future datasets and benchmarks.

</details>


### [211] [Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/pdf/2506.22817)
*Xingyilang Yin, Jiale Wang, Xi Yang, Mutian Xu, Xu Gu, Nannan Wang*

Main category: cs.CV

TL;DR: MVOV3D improves open-vocabulary 3D scene understanding by reducing noise in 2D multi-view fusion without training, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with diverse object categories due to limited 3D data and noisy 2D multi-view fusion.

Method: MVOV3D enhances 2D multi-view features using CLIP encoders and 3D geometric priors, avoiding training.

Result: Achieves 14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160, surpassing trained 3D networks.

Conclusion: MVOV3D effectively leverages 2D fusion for open-vocabulary 3D understanding, setting new benchmarks.

Abstract: Recent open-vocabulary 3D scene understanding approaches mainly focus on
training 3D networks through contrastive learning with point-text pairs or by
distilling 2D features into 3D models via point-pixel alignment. While these
methods show considerable performance in benchmarks with limited vocabularies,
they struggle to handle diverse object categories as the limited amount of 3D
data upbound training strong open-vocabulary 3d models. We observe that 2D
multi-view fusion methods take precedence in understanding diverse concepts in
3D scenes. However, inherent noises in vision-language models lead multi-view
fusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel
approach aimed at unleashing the potential of 2D multi-view fusion for
open-vocabulary 3D scene understanding. We focus on reducing the inherent
noises without training, thereby preserving the generalizability while
enhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D
features by leveraging precise region-level image features and text features
encoded by CLIP encoders and incorporates 3D geometric priors to optimize
multi-view fusion. Extensive experiments on various datasets demonstrate the
effectiveness of our method. Notably, our MVOV3D achieves a new record with
14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge
open-vocabulary semantic segmentation, outperforming current leading trained 3D
networks by a significant margin.

</details>


### [212] [Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration](https://arxiv.org/pdf/2506.22819)
*Ramya Hebbalaguppe, Tamoghno Kandar, Abhinav Nagpal, Chetan Arora*

Main category: cs.CV

TL;DR: The paper addresses the miscalibration issue in test-time prompt tuning (TPT) for vision-language models (VLM) by proposing careful prompt initialization using LLM knowledge and a novel regularization loss.


<details>
  <summary>Details</summary>
Motivation: TPT improves VLM accuracy but degrades confidence calibration, limiting its use in critical applications.

Method: Initializes prompts using LLM knowledge and introduces a regularization loss to reduce intraclass and increase inter-class distances.

Result: Achieves an average ECE of 4.11, outperforming other methods (vanilla TPT: 11.7, C-TPT: 6.12, DiffTPT: 6.78, PromptAlign: 8.43).

Conclusion: The proposed approach effectively improves calibration in TPT, making VLMs more reliable for critical applications.

Abstract: Vision-language models (VLM) have demonstrated impressive performance in
image recognition by leveraging self-supervised training on large datasets.
Their performance can be further improved by adapting to the test sample using
test-time prompt tuning (TPT). Unfortunately, the singular focus of TPT
approaches on improving the accuracy suffers from tunnel vision, and leads to
degradation in confidence calibration. This limits the applicability of TPT in
critical applications.
  We make three contributions in this work. (1) We posit that random or naive
initialization of prompts leads to overfitting on a particular test sample, and
is the main reason for miscalibration of the VLM after TPT. To mitigate the
problem, we propose careful initialization of test time prompt using prior
knowledge about the target label attributes from a large language model (LLM);
(2) To further maintain the quality of prompts during \tpt, we propose a novel
regularization loss to reduce intraclass distance, and increase inter-class
distance between the learnt
  Through extensive experiments on different CLIP architectures and 15
datasets, we show that our approach can effectively improve the calibration
after TPT. We report an average expected calibration error (ECE) of 4.11 with
our method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),
6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is
publicly accessible at:
https://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.

</details>


### [213] [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/pdf/2506.22832)
*Alexander Gambashidze, Li Pengyi, Matvey Skripkin, Andrey Galichin, Anton Gusarov, Konstantin Sobolev, Andrey Kuznetsov, Ivan Oseledets*

Main category: cs.CV

TL;DR: The paper introduces a listener-augmented GRPO framework to improve reward models for human visual preferences by aligning reasoning traces with an independent vision-language model, achieving better accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Current reward models for human visual preferences often fail to generalize, and supervised fine-tuning leads to memorization, requiring complex annotation pipelines.

Method: The proposed listener-augmented GRPO framework uses a frozen vision-language model (listener) to re-evaluate the reasoner's chain-of-thought, providing a dense, calibrated confidence score to shape the RL reward signal.

Result: The method achieves 67.4% accuracy on the ImageReward benchmark, improves OOD performance by up to +6%, and reduces reasoning contradictions compared to baselines.

Conclusion: Listener-based rewards offer a scalable, data-efficient approach to aligning vision-language models with nuanced human preferences.

Abstract: Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.

</details>


### [214] [SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds](https://arxiv.org/pdf/2506.22833)
*Shashikant Verma, Shanmuganathan Raman*

Main category: cs.CV

TL;DR: SemFaceEdit introduces a method for localized facial editing in 3D-aware GANs by generating semantic fields on generative radiance manifolds, enabling precise control over geometry and appearance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D-aware GANs lack localized editing capabilities, limiting their utility for fine-grained facial modifications.

Method: SemFaceEdit uses two modules: Geometry for semantic radiance/occupancy fields and Appearance for RGB radiance, trained adversarially with latent codes for disentanglement.

Result: The method achieves superior radiance field disentanglement and precise editing of facial semantics while preserving other regions.

Conclusion: SemFaceEdit advances localized facial editing in 3D-aware GANs, offering improved control and performance.

Abstract: Despite multiple view consistency offered by 3D-aware GAN techniques, the
resulting images often lack the capacity for localized editing. In response,
generative radiance manifolds emerge as an efficient approach for constrained
point sampling within volumes, effectively reducing computational demands and
enabling the learning of fine details. This work introduces SemFaceEdit, a
novel method that streamlines the appearance and geometric editing process by
generating semantic fields on generative radiance manifolds. Utilizing latent
codes, our method effectively disentangles the geometry and appearance
associated with different facial semantics within the generated image. In
contrast to existing methods that can change the appearance of the entire
radiance field, our method enables the precise editing of particular facial
semantics while preserving the integrity of other regions. Our network
comprises two key modules: the Geometry module, which generates semantic
radiance and occupancy fields, and the Appearance module, which is responsible
for predicting RGB radiance. We jointly train both modules in adversarial
settings to learn semantic-aware geometry and appearance descriptors. The
appearance descriptors are then conditioned on their respective semantic latent
codes by the Appearance Module, facilitating disentanglement and enhanced
control. Our experiments highlight SemFaceEdit's superior performance in
semantic field-based editing, particularly in achieving improved radiance field
disentanglement.

</details>


### [215] [FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition](https://arxiv.org/pdf/2506.22836)
*Hongyan An, Kuan Zhu, Xin He, Haiyun Guo, Chaoyang Zhao, Ming Tang, Jinqiao Wang*

Main category: cs.CV

TL;DR: The paper introduces FOCUS, a method for Pedestrian Attribute Recognition (PAR) that adaptively extracts fine-grained attribute-level features, addressing limitations of fixed regional features.


<details>
  <summary>Details</summary>
Motivation: Existing PAR methods use fixed regional features, which compromise fine-grained patterns and fail to generalize to unseen attributes. FOCUS aims to overcome these limitations.

Method: FOCUS uses Multi-Granularity Mix Tokens (MGMT) for diverse feature extraction and Attribute-guided Visual Feature Extraction (AVFE) with cross-attention. Region-Aware Contrastive Learning (RACL) ensures consistent attention.

Result: Experiments on PA100K, PETA, and RAPv1 datasets show FOCUS's effectiveness and generalization ability.

Conclusion: FOCUS improves PAR by adaptively extracting fine-grained features, enhancing performance and practicality for both seen and unseen attributes.

Abstract: Pedestrian attribute recognition (PAR) is a fundamental perception task in
intelligent transportation and security. To tackle this fine-grained task, most
existing methods focus on extracting regional features to enrich attribute
information. However, a regional feature is typically used to predict a fixed
set of pre-defined attributes in these methods, which limits the performance
and practicality in two aspects: 1) Regional features may compromise
fine-grained patterns unique to certain attributes in favor of capturing common
characteristics shared across attributes. 2) Regional features cannot
generalize to predict unseen attributes in the test time. In this paper, we
propose the \textbf{F}ine-grained \textbf{O}ptimization with semanti\textbf{C}
g\textbf{U}ided under\textbf{S}tanding (FOCUS) approach for PAR, which
adaptively extracts fine-grained attribute-level features for each attribute
individually, regardless of whether the attributes are seen or not during
training. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to
capture latent features at varying levels of visual granularity, thereby
enriching the diversity of the extracted information. Next, we introduce the
Attribute-guided Visual Feature Extraction (AVFE) module, which leverages
textual attributes as queries to retrieve their corresponding visual attribute
features from the Mix Tokens using a cross-attention mechanism. To ensure that
textual attributes focus on the appropriate Mix Tokens, we further incorporate
a Region-Aware Contrastive Learning (RACL) method, encouraging attributes
within the same region to share consistent attention maps. Extensive
experiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness
and strong generalization ability of our method.

</details>


### [216] [AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results](https://arxiv.org/pdf/2506.22843)
*Kien Nguyen, Clinton Fookes, Sridha Sridharan, Huy Nguyen, Feng Liu, Xiaoming Liu, Arun Ross, Dana Michalski, Tamás Endrei, Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia, Zijing Gong, Yuhao Wang, Xuehu Liu, Pingping Zhang, Md Rashidunnabi, Hugo Proença, Kailash A. Hambarde, Saeid Rezaei*

Main category: cs.CV

TL;DR: The AG-VPReID 2025 Challenge introduces a large-scale video-based competition for aerial-ground person re-identification, addressing challenges like viewpoint differences and occlusions.


<details>
  <summary>Details</summary>
Motivation: To bridge the aerial-ground domain gap in person re-identification, crucial for surveillance and public safety, by leveraging a new dataset and advanced methods.

Method: Teams developed solutions including multi-stream architectures, transformer-based temporal reasoning, and physics-informed modeling.

Result: The leading approach, X-TFCLIP, achieved 72.28% Rank-1 accuracy in aerial-to-ground and 70.77% in ground-to-aerial ReID, outperforming baselines.

Conclusion: The challenge highlights the dataset's complexity and advances in aerial-ground ReID, with potential for further improvements.

Abstract: Person re-identification (ReID) across aerial and ground vantage points has
become crucial for large-scale surveillance and public safety applications.
Although significant progress has been made in ground-only scenarios, bridging
the aerial-ground domain gap remains a formidable challenge due to extreme
viewpoint differences, scale variations, and occlusions. Building upon the
achievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID
2025 Challenge - the first large-scale video-based competition focused on
high-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID
dataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7
million frames captured from UAVs, CCTV, and wearable cameras, the challenge
featured four international teams. These teams developed solutions ranging from
multi-stream architectures to transformer-based temporal reasoning and
physics-informed modeling. The leading approach, X-TFCLIP from UAM, attained
72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the
ground-to-aerial ReID setting, surpassing existing baselines while highlighting
the dataset's complexity. For additional details, please refer to the official
website at https://agvpreid25.github.io.

</details>


### [217] [DMD-Net: Deep Mesh Denoising Network](https://arxiv.org/pdf/2506.22850)
*Aalok Gangopadhyay, Shashikant Verma, Shanmuganathan Raman*

Main category: cs.CV

TL;DR: DMD-Net is a deep learning framework for mesh denoising using a dual-stream GCN and a Feature Guided Transformer, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the mesh denoising problem with a robust, end-to-end deep learning solution that handles high noise levels effectively.

Method: Uses a dual-stream GCN with primal-dual fusion and a Feature Guided Transformer (feature extractor, transformer, denoiser) for denoising.

Result: Competitive or superior performance compared to state-of-the-art methods, robust to high noise levels.

Conclusion: DMD-Net is effective and robust for mesh denoising, with each component contributing to its success.

Abstract: We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning
framework, for solving the mesh denoising problem. DMD-Net consists of a Graph
Convolutional Neural Network in which aggregation is performed in both the
primal as well as the dual graph. This is realized in the form of an asymmetric
two-stream network, which contains a primal-dual fusion block that enables
communication between the primal-stream and the dual-stream. We develop a
Feature Guided Transformer (FGT) paradigm, which consists of a feature
extractor, a transformer, and a denoiser. The feature extractor estimates the
local features, that guide the transformer to compute a transformation, which
is applied to the noisy input mesh to obtain a useful intermediate
representation. This is further processed by the denoiser to obtain the
denoised mesh. Our network is trained on a large scale dataset of 3D objects.
We perform exhaustive ablation studies to demonstrate that each component in
our network is essential for obtaining the best performance. We show that our
method obtains competitive or better results when compared with the
state-of-the-art mesh denoising algorithms. We demonstrate that our method is
robust to various kinds of noise. We observe that even in the presence of
extremely high noise, our method achieves excellent performance.

</details>


### [218] [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/pdf/2506.22864)
*Li-Cheng Shen, Jih-Kang Hsieh, Wei-Hua Li, Chu-Song Chen*

Main category: cs.CV

TL;DR: MaTIR unifies text-to-image retrieval (TIR) and referring expression segmentation (RES) for efficient search and accurate segmentation, using a two-stage framework with SAM 2 and Alpha-CLIP for offline mask generation and MLLM for reranking.


<details>
  <summary>Details</summary>
Motivation: Existing TIR lacks interpretability, and RES is computationally expensive for large datasets. MaTIR bridges this gap.

Method: Two-stage framework: 1) Segmentation-aware retrieval using SAM 2 and Alpha-CLIP for offline mask/embedding generation. 2) MLLM for reranking and object grounding.

Result: Improved retrieval accuracy and segmentation quality on COCO and D$^3$ datasets.

Conclusion: MaTIR effectively combines TIR and RES, offering scalable and accurate results.

Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

</details>


### [219] [DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved Out-of-Distribution Detection](https://arxiv.org/pdf/2501.08005)
*Francisco Caetano, Christiaan Viviers, Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen*

Main category: cs.CV

TL;DR: DisCoPatch, an unsupervised Adversarial VAE framework, detects covariate shifts in OOD detection by leveraging batch statistics and adversarial discriminators, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of detecting subtle covariate shifts in data distribution, which degrade ML performance, aiming to improve OOD detection by better understanding in-distribution boundaries.

Method: DisCoPatch uses an Adversarial VAE framework with Batch Normalization, exploiting batch statistics from patches of the same image. It trains the discriminator using suboptimal VAE outputs as negative samples.

Result: DisCoPatch achieves 95.5% AUROC on ImageNet-1K(-C) and 95.0% on Near-OOD benchmarks, with a compact 25MB model size and low latency.

Conclusion: DisCoPatch is an efficient, practical solution for OOD detection, outperforming prior methods and offering high performance with low computational cost.

Abstract: Out-of-distribution (OOD) detection holds significant importance across many
applications. While semantic and domain-shift OOD problems are well-studied,
this work focuses on covariate shifts - subtle variations in the data
distribution that can degrade machine learning performance. We hypothesize that
detecting these subtle shifts can improve our understanding of in-distribution
boundaries, ultimately improving OOD detection. In adversarial discriminators
trained with Batch Normalization (BN), real and adversarial samples form
distinct domains with unique batch statistics - a property we exploit for OOD
detection. We introduce DisCoPatch, an unsupervised Adversarial Variational
Autoencoder (VAE) framework that harnesses this mechanism. During inference,
batches consist of patches from the same image, ensuring a consistent data
distribution that allows the model to rely on batch statistics. DisCoPatch uses
the VAE's suboptimal outputs (generated and reconstructed) as negative samples
to train the discriminator, thereby improving its ability to delineate the
boundary between in-distribution samples and covariate shifts. By tightening
this boundary, DisCoPatch achieves state-of-the-art results in public OOD
detection benchmarks. The proposed model not only excels in detecting covariate
shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior
methods on public Near-OOD (95.0%) benchmarks. With a compact model size of
25MB, it achieves high OOD detection performance at notably lower latency than
existing methods, making it an efficient and practical solution for real-world
OOD detection applications. The code is publicly available.

</details>


### [220] [Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception](https://arxiv.org/pdf/2506.22866)
*Hang-Cheng Dong, Lu Zou, Bingguo Liu, Dong Ye, Guodong Liu*

Main category: cs.CV

TL;DR: A weakly supervised semantic segmentation framework for surface defect detection, using region-aware CAM and pseudo-label training, outperforms traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the reliance on large annotated datasets in defect detection by proposing a weakly supervised approach.

Method: Introduces filtering-guided backpropagation (FGBP) for refined CAM and a region-aware weighted module, followed by pseudo-label training.

Result: Superior performance on industrial defect datasets, bridging weakly supervised learning and high-precision segmentation.

Conclusion: The framework offers a practical solution for industrial defect detection with limited resources.

Abstract: Surface defect detection plays a critical role in industrial quality
inspection. Recent advances in artificial intelligence have significantly
enhanced the automation level of detection processes. However, conventional
semantic segmentation and object detection models heavily rely on large-scale
annotated datasets, which conflicts with the practical requirements of defect
detection tasks. This paper proposes a novel weakly supervised semantic
segmentation framework comprising two key components: a region-aware class
activation map (CAM) and pseudo-label training. To address the limitations of
existing CAM methods, especially low-resolution thermal maps, and insufficient
detail preservation, we introduce filtering-guided backpropagation (FGBP),
which refines target regions by filtering gradient magnitudes to identify areas
with higher relevance to defects. Building upon this, we further develop a
region-aware weighted module to enhance spatial precision. Finally,
pseudo-label segmentation is implemented to refine the model's performance
iteratively. Comprehensive experiments on industrial defect datasets
demonstrate the superiority of our method. The proposed framework effectively
bridges the gap between weakly supervised learning and high-precision defect
segmentation, offering a practical solution for resource-constrained industrial
scenarios.

</details>


### [221] [Super-Resolution Generative Adversarial Networks based Video Enhancement](https://arxiv.org/pdf/2505.10589)
*Kağan Çetin, Hacer Akça, Ömer Nezih Gerek*

Main category: cs.CV

TL;DR: An enhanced video super-resolution method extends SRGAN with 3D Non-Local Blocks for spatio-temporal data, improving temporal coherence and reducing artifacts.


<details>
  <summary>Details</summary>
Motivation: SRGAN lacks temporal continuity for video processing, prompting the need for a modified framework.

Method: Incorporates 3D Non-Local Blocks and uses patch-wise learning with advanced data degradation techniques.

Result: Improved temporal coherence, sharper textures, and fewer artifacts compared to single-image methods.

Conclusion: This work advances practical learning-based video enhancement for applications like streaming and gaming.

Abstract: This study introduces an enhanced approach to video super-resolution by
extending ordinary Single-Image Super-Resolution (SISR) Super-Resolution
Generative Adversarial Network (SRGAN) structure to handle spatio-temporal
data. While SRGAN has proven effective for single-image enhancement, its design
does not account for the temporal continuity required in video processing. To
address this, a modified framework that incorporates 3D Non-Local Blocks is
proposed, which is enabling the model to capture relationships across both
spatial and temporal dimensions. An experimental training pipeline is
developed, based on patch-wise learning and advanced data degradation
techniques, to simulate real-world video conditions and learn from both local
and global structures and details. This helps the model generalize better and
maintain stability across varying video content while maintaining the general
structure besides the pixel-wise correctness. Two model variants-one larger and
one more lightweight-are presented to explore the trade-offs between
performance and efficiency. The results demonstrate improved temporal
coherence, sharper textures, and fewer visual artifacts compared to traditional
single-image methods. This work contributes to the development of practical,
learning-based solutions for video enhancement tasks, with potential
applications in streaming, gaming, and digital restoration.

</details>


### [222] [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](https://arxiv.org/pdf/2506.22868)
*Junsung Lee, Junoh Kang, Bohyung Han*

Main category: cs.CV

TL;DR: STR-Match is a training-free video editing method using a novel STR score for spatiotemporal coherence, outperforming existing methods in quality and consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations like temporal inconsistency, motion distortion, and limited domain transformation in text-guided video editing.

Method: Uses latent optimization guided by STR score, leveraging 2D spatial attention and 1D temporal modules in T2V diffusion models.

Result: Produces visually appealing, spatiotemporally coherent videos, even under significant domain transformations.

Conclusion: STR-Match consistently outperforms existing methods in visual quality and spatiotemporal consistency.

Abstract: Previous text-guided video editing methods often suffer from temporal
inconsistency, motion distortion, and-most notably-limited domain
transformation. We attribute these limitations to insufficient modeling of
spatiotemporal pixel relevance during the editing process. To address this, we
propose STR-Match, a training-free video editing algorithm that produces
visually appealing and spatiotemporally coherent videos through latent
optimization guided by our novel STR score. The score captures spatiotemporal
pixel relevance across adjacent frames by leveraging 2D spatial attention and
1D temporal modules in text-to-video (T2V) diffusion models, without the
overhead of computationally expensive 3D attention mechanisms. Integrated into
a latent optimization framework with a latent mask, STR-Match generates
temporally consistent and visually faithful videos, maintaining strong
performance even under significant domain transformations while preserving key
visual attributes of the source. Extensive experiments demonstrate that
STR-Match consistently outperforms existing methods in both visual quality and
spatiotemporal consistency.

</details>


### [223] [Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder](https://arxiv.org/pdf/2506.22880)
*Dang Jisheng, Wu Xudong, Wang Bimei, Lv Ning, Chen Jiayu, Jingwen Zhao, Yichu liu, Jizhao Liu, Juncheng Li, Teng Wang*

Main category: cs.CV

TL;DR: DeSa2VA introduces a decoupling-enhanced prompting scheme to improve segmentation accuracy by disentangling visual and textual features, outperforming existing methods in multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Sa2VA fuse features in segmentation models, leading to entangled dynamic visual and static semantic information, which degrades accuracy.

Method: DeSa2VA uses text pre-training, a linear decoupling module, and dynamic mask fusion to separate and synergize textual and visual features.

Result: The method achieves state-of-the-art performance in image/video segmentation and question answering tasks.

Conclusion: DeSa2VA effectively addresses feature entanglement, enhancing segmentation accuracy and semantic grounding.

Abstract: Existing video segmenter and grounder approaches, exemplified by Sa2VA,
directly fuse features within segmentation models. This often results in an
undesirable entanglement of dynamic visual information and static semantics,
thereby degrading segmentation accuracy. To systematically mitigate this issue,
we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text
pre-training and a linear decoupling module to address the information
processing limitations inherent in SAM-2. Specifically, first, we devise a
pre-training paradigm that converts textual ground-truth labels into
point-level prompts while generating corresponding text masks. These masks are
refined through a hybrid loss function to strengthen the model's semantic
grounding capabilities. Next, we employ linear projection to disentangle hidden
states that generated by a large language model into distinct textual and
visual feature subspaces. Finally, a dynamic mask fusion strategy
synergistically combines these decoupled features through triple supervision
from predicted text/visual masks and ground-truth annotations. Extensive
experiments demonstrate state-of-the-art performance across diverse tasks,
including image segmentation, image question answering, video segmentation, and
video question answering. Our codes are available at
https://github.com/longmalongma/DeSa2VA.

</details>


### [224] [How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings](https://arxiv.org/pdf/2506.22881)
*Fumiya Uchiyama, Rintaro Yanagi, Shohei Taniguchi, Shota Takashiro, Masahiro Suzuki, Hirokatsu Kataoka, Yusuke Iwasawa, Yutaka Matsuo*

Main category: cs.CV

TL;DR: The paper introduces a semantic informativeness metric for images and texts using contrastive learning, extending Information Gain to vision and language domains. It validates the metric with strong correlations and low computational costs.


<details>
  <summary>Details</summary>
Motivation: To address whether contrastive learning can represent absolute semantic informativeness, not just relational similarity, by quantifying how conditioning on an image or text distorts their associated distributions.

Method: Proposes a metric for semantic informativeness using contrastive learning models (CLIP or SigLIP), measuring Information Gain by embedding norms and validating it empirically.

Result: Strong correlation (R² 0.98-1.00) between the metric and theoretical expectations, with low computational cost post-embedding. Placeholder icons score lowest in Information Gain.

Conclusion: The introduced metric effectively quantifies semantic informativeness, is computationally efficient, and works with open-weight models, bridging vision and language domains.

Abstract: Contrastive learning has the capacity to model multimodal probability
distributions by embedding and aligning visual representations with semantics
from captions. This approach enables the estimation of relational semantic
similarity; however, it remains unclear whether it can also represent absolute
semantic informativeness. In this work, we introduce a semantic informativeness
metric for an image calculated from text samples via a contrastive learning
model; similarly, the informativeness of a text is calculated from image
samples. We propose a redefinition of the concept of Information Gain, a
concept previously explored in natural language processing, extending its
application to the domains of vision and language. Our metric quantifies how
conditioning on an image distorts the distribution of associated texts, and
vice versa for text conditioning on image distributions. In OpenCLIP's
empirical results, we observe that images with the lowest Information Gain
scores often correspond to placeholder icons such as "image not found."
Furthermore, we propose to measure a norm-based metric of the embedding to
estimate the Information Gain, following the theoretical results for Skip-Gram
with Negative Sampling (SGNS) word embedding. Information Gain can be measured
using either CLIP or SigLIP, and the results demonstrate a strong correlation
with a coefficient of determination ranging from 0.98 to 1.00. After obtaining
the mean and the covariance of the sample embedding, the computational cost of
this method is independent of the sample size, and it is compatible with
publicly available, open-weight models.

</details>


### [225] [CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems](https://arxiv.org/pdf/2506.22890)
*Senkang Hu, Yihang Tao, Guowen Xu, Xinyuan Qian, Yiqin Deng, Xianhao Chen, Sam Tak Wu Kwong, Yuguang Fang*

Main category: cs.CV

TL;DR: CP-Guard is a defense framework for Collaborative Perception (CP) systems to detect and eliminate malicious agents by ensuring consensus among collaborators.


<details>
  <summary>Details</summary>
Motivation: CP systems are vulnerable to attacks from malicious agents, necessitating a robust defense mechanism.

Method: The framework includes a probability-agnostic sample consensus (PASAC) method, collaborative consistency loss (CCLoss), and adaptive thresholds for dynamic environments.

Result: Extensive experiments validate the framework's effectiveness in detecting and mitigating malicious agents.

Conclusion: CP-Guard provides a reliable, adaptive solution to secure CP systems against malicious attacks.

Abstract: Collaborative Perception (CP) has been shown to be a promising technique for
multi-agent autonomous driving and multi-agent robotic systems, where multiple
agents share their perception information to enhance the overall perception
performance and expand the perception range. However, in CP, an ego agent needs
to receive messages from its collaborators, which makes it vulnerable to
attacks from malicious agents. To address this critical issue, we propose a
unified, probability-agnostic, and adaptive framework, namely, CP-Guard, which
is a tailored defense mechanism for CP deployed by each agent to accurately
detect and eliminate malicious agents in its collaboration network. Our key
idea is to enable CP to reach a consensus rather than a conflict against an ego
agent's perception results. Based on this idea, we first develop a
probability-agnostic sample consensus (PASAC) method to effectively sample a
subset of the collaborators and verify the consensus without prior
probabilities of malicious agents. Furthermore, we define collaborative
consistency loss (CCLoss) for object detection task and bird's eye view (BEV)
segmentation task to capture the discrepancy between an ego agent and its
collaborators, which is used as a verification criterion for consensus. In
addition, we propose online adaptive threshold via dual sliding windows to
dynamically adjust the threshold for consensus verification and ensure the
reliability of the systems in dynamic environments. Finally, we conduct
extensive experiments and demonstrate the effectiveness of our framework. Code
will be released at https://github.com/CP-Security/CP-Guard

</details>


### [226] [MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering](https://arxiv.org/pdf/2506.22900)
*Mai A. Shaaban, Tausifa Jan Saleem, Vijay Ram Papineni, Mohammad Yaqub*

Main category: cs.CV

TL;DR: MOTOR improves MedVQA by using multimodal retrieval and re-ranking with grounded captions and optimal transport, outperforming existing methods by 6.45%.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs for MedVQA often produce incorrect answers, and retrieval-augmented methods risk irrelevant context, neglecting visual/multimodal cues crucial for medical diagnosis.

Method: Proposes MOTOR, a multimodal retrieval and re-ranking approach using grounded captions and optimal transport to align query and context with visual/textual data.

Result: MOTOR achieves higher accuracy on MedVQA datasets, outperforming state-of-the-art methods by 6.45%.

Conclusion: MOTOR effectively enhances retrieval relevance and accuracy in MedVQA by integrating multimodal context.

Abstract: Medical visual question answering (MedVQA) plays a vital role in clinical
decision-making by providing contextually rich answers to image-based queries.
Although vision-language models (VLMs) are widely used for this task, they
often generate factually incorrect answers. Retrieval-augmented generation
addresses this challenge by providing information from external sources, but
risks retrieving irrelevant context, which can degrade the reasoning
capabilities of VLMs. Re-ranking retrievals, as introduced in existing
approaches, enhances retrieval relevance by focusing on query-text alignment.
However, these approaches neglect the visual or multimodal context, which is
particularly crucial for medical diagnosis. We propose MOTOR, a novel
multimodal retrieval and re-ranking approach that leverages grounded captions
and optimal transport. It captures the underlying relationships between the
query and the retrieved context based on textual and visual information.
Consequently, our approach identifies more clinically relevant contexts to
augment the VLM input. Empirical analysis and human expert evaluation
demonstrate that MOTOR achieves higher accuracy on MedVQA datasets,
outperforming state-of-the-art methods by an average of 6.45%. Code is
available at https://github.com/BioMedIA-MBZUAI/MOTOR.

</details>


### [227] [MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances](https://arxiv.org/pdf/2506.22907)
*Yunzhe Shao, Xinyu Yi, Lu Yin, Shihui Guo, Junhai Yong, Feng Xu*

Main category: cs.CV

TL;DR: MagShield is a novel method to mitigate magnetic interference in sparse inertial MoCap systems by detecting and correcting disturbances using multi-IMU analysis and motion priors.


<details>
  <summary>Details</summary>
Motivation: Existing IMU systems suffer from orientation errors in magnetically disturbed environments, limiting real-world usability.

Method: MagShield uses a 'detect-then-correct' approach: detecting disturbances via multi-IMU joint analysis and correcting errors with human motion priors.

Result: MagShield improves motion capture accuracy under magnetic interference and is compatible with various sparse inertial MoCap systems.

Conclusion: MagShield effectively addresses magnetic interference, enhancing the practicality of sparse inertial MoCap systems in real-world scenarios.

Abstract: This paper proposes a novel method called MagShield, designed to address the
issue of magnetic interference in sparse inertial motion capture (MoCap)
systems. Existing Inertial Measurement Unit (IMU) systems are prone to
orientation estimation errors in magnetically disturbed environments, limiting
their practical application in real-world scenarios. To address this problem,
MagShield employs a "detect-then-correct" strategy, first detecting magnetic
disturbances through multi-IMU joint analysis, and then correcting orientation
errors using human motion priors. MagShield can be integrated with most
existing sparse inertial MoCap systems, improving their performance in
magnetically disturbed environments. Experimental results demonstrate that
MagShield significantly enhances the accuracy of motion capture under magnetic
interference and exhibits good compatibility across different sparse inertial
MoCap systems.

</details>


### [228] [Attention to Burstiness: Low-Rank Bilinear Prompt Tuning](https://arxiv.org/pdf/2506.22908)
*Yuzhu Wang, Manni Duan, Shu Kong*

Main category: cs.CV

TL;DR: Visual Prompt Tuning (VPT) is enhanced by whitening data to address non-Gaussian distributions, leading to Bilinear Prompt Tuning (BPT), which improves accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The non-Gaussian distributions in patch embeddings and key/query projectors challenge prompt learning in VPT.

Method: Proposes whitening data to de-correlate and equalize variance, then introduces bilinear and low-rank prompt tuning (BPT).

Result: BPT significantly boosts accuracy (e.g., +25 points on CUB) and reduces parameters/computation.

Conclusion: BPT outperforms VPT methods, offering a more efficient and effective prompt-tuning approach.

Abstract: Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique
that adapts a pre-trained vision Transformer (ViT) by learning a small set of
parameters in the input space, known as prompts. In VPT, we uncover
``burstiness'' in the values arising from the interaction of image patch
embeddings, and the key and query projectors within Transformer's
self-attention module. Furthermore, the values of patch embeddings and the key
and query projectors exhibit Laplacian and hyper-Laplacian distribution,
respectively. Intuitively, these non-Gaussian distributions pose challenges for
learning prompts. To address this, we propose whitening these data,
de-correlating them and equalizing their variance towards more Gaussian before
learning prompts. We derive the whitening matrix over random image patch
embeddings and ViT's key and query projectors, and multiply it with the prompt
to be learned in a bilinear manner. Surprisingly, this method significantly
accelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on
the CUB dataset; interestingly, it learns ``bursty prompts''. Extending the
bilinear model which is known to introduce burstiness, we present a compact,
low-rank version by learning two smaller matrices whose multiplication yields
the final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT).
Extensive experiments across multiple benchmark datasets demonstrate that BPT
methods not only outperform various VPT methods but also reduce parameter count
and computation overhead.

</details>


### [229] [Towards Explainable Bilingual Multimodal Misinformation Detection and Localization](https://arxiv.org/pdf/2506.22930)
*Yiwei He, Xiangtai Li, Zhenglin Huang, Yi Dong, Hao Fei, Jiangning Zhang, Baoyuan Wu, Guangliang Cheng*

Main category: cs.CV

TL;DR: BiMi is a bilingual multimodal framework for detecting misinformation in news media by analyzing region-level edits and cross-lingual inconsistencies, outperforming baselines in accuracy and explanation quality.


<details>
  <summary>Details</summary>
Motivation: The rise of subtle misinformation in bilingual news media, combining localized image edits and cross-lingual inconsistencies, necessitates advanced detection tools.

Method: BiMi integrates region-level localization, cross-modal/lingual consistency detection, and natural language explanation, enhanced by GRPO for interpretability and an online retrieval module for context.

Result: BiMi outperforms baselines by +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore.

Conclusion: BiMi advances multilingual misinformation detection, supported by the BiMiBench dataset and GRPO for improved interpretability.

Abstract: The increasing realism of multimodal content has made misinformation more
subtle and harder to detect, especially in news media where images are
frequently paired with bilingual (e.g., Chinese-English) subtitles. Such
content often includes localized image edits and cross-lingual inconsistencies
that jointly distort meaning while remaining superficially plausible. We
introduce BiMi, a bilingual multimodal framework that jointly performs
region-level localization, cross-modal and cross-lingual consistency detection,
and natural language explanation for misinformation analysis. To support
generalization, BiMi integrates an online retrieval module that supplements
model reasoning with up-to-date external context. We further release BiMiBench,
a large-scale and comprehensive benchmark constructed by systematically editing
real news images and subtitles, comprising 104,000 samples with realistic
manipulations across visual and linguistic modalities. To enhance
interpretability, we apply Group Relative Policy Optimization (GRPO) to improve
explanation quality, marking the first use of GRPO in this domain. Extensive
experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in
classification accuracy, +15.9 in localization accuracy, and +2.5 in
explanation BERTScore, advancing state-of-the-art performance in realistic,
multilingual misinformation detection. Code, models, and datasets will be
released.

</details>


### [230] [Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data](https://arxiv.org/pdf/2506.22939)
*Ghufran A. Omran, Wassan Saad Abduljabbar Hayale, Ahmad AbdulQadir AlRababah, Israa Ibraheem Al-Barazanchi, Ravi Sekhar, Pritesh Shah, Sushma Parihar, Harshavardhan Reddy Penubadi*

Main category: cs.CV

TL;DR: The paper introduces CO-BRNN for SC in remote sensing, achieving 97% accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: High accuracy in SC from remote sensing is challenging due to noise and data variety.

Method: Proposes CO-BRNN, comparing it with MLP-CNN, CNN-LSTM, LSTM-CRF, GB, MIRM-CF, and CNN-DA.

Result: CO-BRNN achieved 97% accuracy, surpassing other methods (LSTM-CRF: 90%, MLP-CNN: 85%, CNN-LSTM: 80%).

Conclusion: CO-BRNN is effective for SC, emphasizing the need for physical validation of satellite data.

Abstract: Scene categorization (SC) in remotely acquired images is an important subject
with broad consequences in different fields, including catastrophe control,
ecological observation, architecture for cities, and more. Nevertheless, its
several apps, reaching a high degree of accuracy in SC from distant observation
data has demonstrated to be difficult. This is because traditional conventional
deep learning models require large databases with high variety and high levels
of noise to capture important visual features. To address these problems, this
investigation file introduces an innovative technique referred to as the
Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type
of scenes in remote sensing data. The investigation compares the execution of
CO-BRNN with current techniques, including Multilayer Perceptron- Convolutional
Neural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory
(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),
Graph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional
Neural Networks Data Augmentation (CNN-DA). The results demonstrate that
CO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,
MLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance
of physical confirmation to ensure the efficiency of satellite data.

</details>


### [231] [YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging](https://arxiv.org/pdf/2506.22955)
*Haniyeh Nikkhah, Jafar Tanha, Mahdi Zarrin, SeyedEhsan Roshan, Amin Kazempour*

Main category: cs.CV

TL;DR: YM-WML, a novel model for cardiac image segmentation, integrates a robust backbone, YOLOv11 neck, and attention-based segmentation head, achieving a Dice score of 91.02 on ACDC dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing class imbalance and complex structures in medical image segmentation.

Method: YM-WML combines a robust backbone, YOLOv11 neck for multi-scale features, and an attention-based head. Uses WME loss for class imbalance.

Result: Achieves 91.02 Dice score on ACDC dataset, outperforming state-of-the-art methods.

Conclusion: YM-WML sets a new benchmark in cardiac segmentation with stable training and strong generalization.

Abstract: Medical image segmentation poses significant challenges due to class
imbalance and the complex structure of medical images. To address these
challenges, this study proposes YM-WML, a novel model for cardiac image
segmentation. The model integrates a robust backbone for effective feature
extraction, a YOLOv11 neck for multi-scale feature aggregation, and an
attention-based segmentation head for precise and accurate segmentation. To
address class imbalance, we introduce the Weighted Multi-class Exponential
(WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity
Coefficient of 91.02, outperforming state-of-the-art methods. The model
demonstrates stable training, accurate segmentation, and strong generalization,
setting a new benchmark in cardiac segmentation tasks.

</details>


### [232] [Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images](https://arxiv.org/pdf/2506.22960)
*Shreyas Dixit, Ashhar Aziz, Shashwat Bajpai, Vasu Sharma, Aman Chadha, Vinija Jain, Amitava Das*

Main category: cs.CV

TL;DR: PECCAVI introduces a distortion-free, visual paraphrase attack-resistant watermarking technique for AI-generated content by embedding watermarks in Non-Melting Points (NMPs) and using multi-channel frequency domain methods.


<details>
  <summary>Details</summary>
Motivation: The rise of synthetic content and generative AI's potential for disinformation necessitates robust watermarking solutions, as current methods are vulnerable to attacks like visual paraphrasing.

Method: PECCAVI embeds watermarks in NMPs (core semantic regions) and uses multi-channel frequency domain watermarking with noisy burnishing to prevent reverse-engineering.

Result: PECCAVI resists visual paraphrase attacks and is model-agnostic, ensuring durability and security.

Conclusion: PECCAVI offers a secure, open-source solution for watermarking AI-generated content, addressing vulnerabilities in existing techniques.

Abstract: A report by the European Union Law Enforcement Agency predicts that by 2026,
up to 90 percent of online content could be synthetically generated, raising
concerns among policymakers, who cautioned that "Generative AI could act as a
force multiplier for political disinformation. The combined effect of
generative text, images, videos, and audio may surpass the influence of any
single modality." In response, California's Bill AB 3211 mandates the
watermarking of AI-generated images, videos, and audio. However, concerns
remain regarding the vulnerability of invisible watermarking techniques to
tampering and the potential for malicious actors to bypass them entirely.
Generative AI-powered de-watermarking attacks, especially the newly introduced
visual paraphrase attack, have shown an ability to fully remove watermarks,
resulting in a paraphrase of the original image. This paper introduces PECCAVI,
the first visual paraphrase attack-safe and distortion-free image watermarking
technique. In visual paraphrase attacks, an image is altered while preserving
its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI
strategically embeds watermarks within these NMPs and employs multi-channel
frequency domain watermarking. It also incorporates noisy burnishing to counter
reverse-engineering efforts aimed at locating NMPs to disrupt the embedded
watermark, thereby enhancing durability. PECCAVI is model-agnostic. All
relevant resources and codes will be open-sourced.

</details>


### [233] [Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation](https://arxiv.org/pdf/2506.22979)
*Jie Liu, Jiayi Shen, Pan Zhou, Jan-Jakob Sonke, Efstratios Gavves*

Main category: cs.CV

TL;DR: FewCLIP introduces a probabilistic prototype calibration framework for GFSS, improving adaptability and generalization over deterministic methods.


<details>
  <summary>Details</summary>
Motivation: Existing prototype-based methods in GFSS are deterministic, limiting adaptability to diverse samples, especially for novel classes with few annotations.

Method: FewCLIP refines frozen textual prototypes with learnable visual calibration prototypes and introduces distribution regularization for uncertainty-aware learning.

Result: FewCLIP outperforms state-of-the-art methods on PASCAL-5$^i$ and COCO-20$^i$ datasets in GFSS and class-incremental settings.

Conclusion: FewCLIP provides adaptive and uncertainty-aware prototype learning, enhancing generalization and mitigating overfitting in GFSS.

Abstract: Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a
segmentation model to novel classes with only a few annotated examples while
maintaining performance on base classes. Recently, pretrained vision-language
models (VLMs) such as CLIP have been leveraged in GFSS to improve
generalization on novel classes through multi-modal prototypes learning.
However, existing prototype-based methods are inherently deterministic,
limiting the adaptability of learned prototypes to diverse samples,
particularly for novel classes with scarce annotations. To address this, we
propose FewCLIP, a probabilistic prototype calibration framework over
multi-modal prototypes from the pretrained CLIP, thus providing more adaptive
prototype learning for GFSS. Specifically, FewCLIP first introduces a prototype
calibration mechanism, which refines frozen textual prototypes with learnable
visual calibration prototypes, leading to a more discriminative and adaptive
representation. Furthermore, unlike deterministic prototype learning
techniques, FewCLIP introduces distribution regularization over these
calibration prototypes. This probabilistic formulation ensures structured and
uncertainty-aware prototype learning, effectively mitigating overfitting to
limited novel class data while enhancing generalization. Extensive experimental
results on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed
FewCLIP significantly outperforms state-of-the-art approaches across both GFSS
and class-incremental setting. The code is available at
https://github.com/jliu4ai/FewCLIP.

</details>


### [234] [Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models](https://arxiv.org/pdf/2506.22982)
*Atharv Mittal, Agam Pandey, Amritanshu Tiwari, Sukrit Jindal, Swadesh Swain*

Main category: cs.CV

TL;DR: The paper validates and improves the Cross-Prompt Attack (CroPA) on Vision-Language Models (VLMs), demonstrating superior adversarial transferability and proposing enhancements like better initialization, universal perturbations, and a novel loss function.


<details>
  <summary>Details</summary>
Motivation: VLMs are vulnerable to adversarial attacks, especially when both visual and textual inputs are manipulated. The study aims to validate and enhance adversarial transferability in VLMs.

Method: The study replicates CroPA and introduces three improvements: a novel initialization strategy, investigation of cross-image transferability via universal perturbations, and a new loss function targeting vision encoder attention.

Result: The improvements consistently boost adversarial effectiveness across VLMs like Flamingo, BLIP-2, InstructBLIP, and LLaVA, confirming CroPA's superior transferability.

Conclusion: The work highlights the need to study VLM vulnerabilities and offers a robust framework for generating transferable adversarial examples, impacting real-world VLM security.

Abstract: Large Vision-Language Models (VLMs) have revolutionized computer vision,
enabling tasks such as image classification, captioning, and visual question
answering. However, they remain highly vulnerable to adversarial attacks,
particularly in scenarios where both visual and textual modalities can be
manipulated. In this study, we conduct a comprehensive reproducibility study of
"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on
Vision-Language Models" validating the Cross-Prompt Attack (CroPA) and
confirming its superior cross-prompt transferability compared to existing
baselines. Beyond replication we propose several key improvements: (1) A novel
initialization strategy that significantly improves Attack Success Rate (ASR).
(2) Investigate cross-image transferability by learning universal
perturbations. (3) A novel loss function targeting vision encoder attention
mechanisms to improve generalization. Our evaluation across prominent VLMs --
including Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on
LLaVA validates the original results and demonstrates that our improvements
consistently boost adversarial effectiveness. Our work reinforces the
importance of studying adversarial vulnerabilities in VLMs and provides a more
robust framework for generating transferable adversarial examples, with
significant implications for understanding the security of VLMs in real-world
applications.

</details>


### [235] [MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models](https://arxiv.org/pdf/2506.23009)
*Jian Chen, Wenye Ma, Penghang Liu, Wei Wang, Tengwei Song, Ming Li, Chenguang Wang, Ruiyi Zhang, Changyou Chen*

Main category: cs.CV

TL;DR: The paper introduces MusiXQA, a dataset for evaluating MLLMs in music sheet understanding, and presents Phi-3-MusiX, a fine-tuned model outperforming GPT-based methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack exploration in interpreting music sheets, prompting the creation of MusiXQA to address this gap.

Method: The dataset features synthetic music sheets with structured annotations, and the model Phi-3-MusiX is fine-tuned on this data.

Result: Evaluations show current MLLMs' limitations in music sheet understanding, while Phi-3-MusiX achieves significant performance gains.

Conclusion: MusiXQA and Phi-3-MusiX provide a foundation for advancing MLLMs in music sheet understanding.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable visual
reasoning abilities in natural images, text-rich documents, and graphic
designs. However, their ability to interpret music sheets remains
underexplored. To bridge this gap, we introduce MusiXQA, the first
comprehensive dataset for evaluating and advancing MLLMs in music sheet
understanding. MusiXQA features high-quality synthetic music sheets generated
via MusiXTeX, with structured annotations covering note pitch and duration,
chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.
Through extensive evaluations, we reveal significant limitations of current
state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed
Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant
performance gains over GPT-based methods. The proposed dataset and model
establish a foundation for future advances in MLLMs for music sheet
understanding. Code, data, and model will be released upon acceptance.

</details>


### [236] [Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation](https://arxiv.org/pdf/2506.23038)
*Xinrong Hu, Yiyu Shi*

Main category: cs.CV

TL;DR: AugPaint is a data augmentation framework using inpainting with latent diffusion models to generate synthetic image-label pairs from limited labeled medical data, improving segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Pixel-level labeling in medical datasets is labor-intensive and costly, and enhancing segmentation with limited labeled data is a challenge.

Method: AugPaint uses latent diffusion models for inpainting, cropping foreground-labeled areas and conditioning the reversed denoising process to fill masked backgrounds, ensuring accurate synthetic image-label pairs.

Result: Evaluations on four medical datasets (CT, MRI, skin imaging) show AugPaint outperforms state-of-the-art label-efficient methods, significantly boosting segmentation performance.

Conclusion: AugPaint effectively addresses the scarcity of labeled medical data by generating high-quality synthetic images, enhancing downstream segmentation models.

Abstract: Collecting pixel-level labels for medical datasets can be a laborious and
expensive process, and enhancing segmentation performance with a scarcity of
labeled data is a crucial challenge. This work introduces AugPaint, a data
augmentation framework that utilizes inpainting to generate image-label pairs
from limited labeled data. AugPaint leverages latent diffusion models, known
for their ability to generate high-quality in-domain images with low overhead,
and adapts the sampling process for the inpainting task without need for
retraining. Specifically, given a pair of image and label mask, we crop the
area labeled with the foreground and condition on it during reversed denoising
process for every noise level. Masked background area would gradually be filled
in, and all generated images are paired with the label mask. This approach
ensures the accuracy of match between synthetic images and label masks, setting
it apart from existing dataset generation methods. The generated images serve
as valuable supervision for training downstream segmentation models,
effectively addressing the challenge of limited annotations. We conducted
extensive evaluations of our data augmentation method on four public medical
image segmentation datasets, including CT, MRI, and skin imaging. Results
across all datasets demonstrate that AugPaint outperforms state-of-the-art
label-efficient methodologies, significantly improving segmentation
performance.

</details>


### [237] [From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting](https://arxiv.org/pdf/2506.23042)
*Hung Nguyen, An Le, Runfa Li, Truong Nguyen*

Main category: cs.CV

TL;DR: AutoOpti3DGS is a training-time framework that reduces Gaussian proliferation in 3D Gaussian Splatting without compromising visual quality, using wavelet transforms and an orthogonality loss.


<details>
  <summary>Details</summary>
Motivation: To address the memory and bandwidth strain caused by the growing set of Gaussian primitives in 3D Gaussian Splatting.

Method: Uses learnable Forward and Inverse Discrete Wavelet Transforms with fixed low-pass and learnable high-pass filters, initialized to zero, and an auxiliary orthogonality loss to activate fine frequencies gradually.

Result: Produces sparser scene representations, integrates seamlessly with existing frameworks, and requires minimal hyper-parameter tuning.

Conclusion: AutoOpti3DGS effectively balances visual fidelity and computational efficiency, making it suitable for memory-constrained hardware.

Abstract: 3D Gaussian Splatting has emerged as a powerful approach in novel view
synthesis, delivering rapid training and rendering but at the cost of an
ever-growing set of Gaussian primitives that strains memory and bandwidth. We
introduce AutoOpti3DGS, a training-time framework that automatically restrains
Gaussian proliferation without sacrificing visual fidelity. The key idea is to
feed the input images to a sequence of learnable Forward and Inverse Discrete
Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters
are learnable and initialized to zero, and an auxiliary orthogonality loss
gradually activates fine frequencies. This wavelet-driven, coarse-to-fine
process delays the formation of redundant fine Gaussians, allowing 3DGS to
capture global structure first and refine detail only when necessary. Through
extensive experiments, AutoOpti3DGS requires just a single filter learning-rate
hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks,
and consistently produces sparser scene representations more compatible with
memory or storage-constrained hardware.

</details>


### [238] [Ovis-U1 Technical Report](https://arxiv.org/pdf/2506.23044)
*Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, Yang Li, Qing-Guo Chen*

Main category: cs.CV

TL;DR: Ovis-U1 is a 3B-parameter unified model excelling in multimodal understanding, text-to-image generation, and image editing, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To integrate multimodal understanding, generation, and editing into a single model, improving performance over task-specific models.

Method: Uses a diffusion-based visual decoder and bidirectional token refiner, trained with a unified approach from a language model.

Result: Achieves top scores on benchmarks like OpenCompass (69.6), DPG-Bench (83.72), and ImgEdit-Bench (4.00).

Conclusion: Ovis-U1 advances multimodal capabilities, setting a new standard for unified models.

Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model
that integrates multimodal understanding, text-to-image generation, and image
editing capabilities. Building on the foundation of the Ovis series, Ovis-U1
incorporates a diffusion-based visual decoder paired with a bidirectional token
refiner, enabling image generation tasks comparable to leading models like
GPT-4o. Unlike some previous models that use a frozen MLLM for generation
tasks, Ovis-U1 utilizes a new unified training approach starting from a
language model. Compared to training solely on understanding or generation
tasks, unified training yields better performance, demonstrating the
enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score
of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent
state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In
text-to-image generation, it excels with scores of 83.72 and 0.89 on the
DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves
4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the
initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries
of multimodal understanding, generation, and editing.

</details>


### [239] [Empowering Small VLMs to Think with Dynamic Memorization and Exploration](https://arxiv.org/pdf/2506.23061)
*Jiazhen Liu, Yuchuan Deng, Long Chen*

Main category: cs.CV

TL;DR: DyME is a novel training paradigm for small-scale vision-language models (SVLMs) that dynamically switches between memorization (SFT) and exploration (RLVR) modes to enhance thinking reliability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing training paradigms (SFT and RLVR) are too demanding for SVLMs, leading to unreliable thinking and poor performance.

Method: DyME dynamically alternates between SFT and RLVR during training to optimize updates.

Result: DyME consistently balances memorization and exploration, improving SVLM performance across domains.

Conclusion: DyME is an effective solution for enhancing SVLMs' thinking capabilities.

Abstract: Empowering Small-scale Vision-Language Models (SVLMs) with reliable thinking
capabilities remains fundamentally challenging due to their limited parameter
capacity and weak instruction-following abilities. Existing training paradigms,
including Supervised Fine-Tuning (SFT) and Reinforcement Learning with
Verifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding
the capabilities of SVLMs. Consequently, directly applying these paradigms to
SVLMs often suffers from severe pseudo thinking traces and advantage collapse,
ultimately undermining both thinking reliability and task performance. A
natural solution is to combine SFT and RLVR, leveraging their complementarity
to reduce the dependence on model capacity. However, the widely adopted
two-stage training paradigm still performs poorly on SVLMs, as their tendency
toward sub-optimal convergence hinders the trade-off and limits the benefits of
the combination. To address this, we propose DyME, a novel training paradigm
that Dynamically selects between Memorization (via SFT) and Exploration (via
RLVR) modes at each optimization step, ensuring that every update contributes
to the trade-off. Extensive experiments across diverse domains demonstrate that
DyME consistently achieves this balance, and thus delivers substantial
performance improvements. These results establish DyME as a practical and
effective solution for empowering SVLMs with reliable thinking capabilities.
GitHub: https://github.com/HKUST-LongGroup/DyME

</details>


### [240] [Unsupervised 3D Braided Hair Reconstruction from a Single-View Image](https://arxiv.org/pdf/2506.23072)
*Jing Gao*

Main category: cs.CV

TL;DR: A novel unsupervised pipeline for 3D braided hair reconstruction from single-view images, outperforming existing methods in accuracy and realism.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with braided hair due to its complex structure, creating a need for better reconstruction techniques.

Method: Uses a synthetic braid model inspired by braid theory to capture intricate braid structures from single-view RGB images.

Result: Outperforms state-of-the-art methods in accuracy, realism, and efficiency for 3D braided hairstyle reconstruction.

Conclusion: The proposed method effectively addresses the challenges of braided hair reconstruction, enhancing digital human modeling.

Abstract: Reconstructing 3D braided hairstyles from single-view images remains a
challenging task due to the intricate interwoven structure and complex
topologies of braids. Existing strand-based hair reconstruction methods
typically focus on loose hairstyles and often struggle to capture the
fine-grained geometry of braided hair. In this paper, we propose a novel
unsupervised pipeline for efficiently reconstructing 3D braided hair from
single-view RGB images. Leveraging a synthetic braid model inspired by braid
theory, our approach effectively captures the complex intertwined structures of
braids. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches, providing superior accuracy, realism, and
efficiency in reconstructing 3D braided hairstyles, supporting expressive
hairstyle modeling in digital humans.

</details>


### [241] [Learning Counterfactually Decoupled Attention for Open-World Model Attribution](https://arxiv.org/pdf/2506.23074)
*Yu Zheng, Boyang Gong, Fanye Kong, Yueqi Duan, Bingyao Yu, Wenzhao Zheng, Lei Chen, Jiwen Lu, Jie Zhou*

Main category: cs.CV

TL;DR: CDAL introduces a causal approach for open-world model attribution, decoupling artifacts from biases to improve generalization to unseen attacks.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited by handcrafted designs and spurious correlations, struggling with novel attacks in open-world scenarios.

Method: CDAL models causal relationships between visual traces and attribution, decoupling artifacts from biases to quantify attention quality.

Result: CDAL outperforms state-of-the-art models, especially for unseen attacks, with minimal computational overhead.

Conclusion: CDAL effectively improves generalization in open-world model attribution by leveraging causal relationships.

Abstract: In this paper, we propose a Counterfactually Decoupled Attention Learning
(CDAL) method for open-world model attribution. Existing methods rely on
handcrafted design of region partitioning or feature space, which could be
confounded by the spurious statistical correlations and struggle with novel
attacks in open-world scenarios. To address this, CDAL explicitly models the
causal relationships between the attentional visual traces and source model
attribution, and counterfactually decouples the discriminative model-specific
artifacts from confounding source biases for comparison. In this way, the
resulting causal effect provides a quantification on the quality of learned
attention maps, thus encouraging the network to capture essential generation
patterns that generalize to unseen source models by maximizing the effect.
Extensive experiments on existing open-world model attribution benchmarks show
that with minimal computational overhead, our method consistently improves
state-of-the-art models by large margins, particularly for unseen novel
attacks. Source code: https://github.com/yzheng97/CDAL.

</details>


### [242] [Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization](https://arxiv.org/pdf/2506.23077)
*Suofei Zhang, Xinxin Wang, Xiaofu Wu, Quan Zhou, Haifeng Hu*

Main category: cs.CV

TL;DR: The paper introduces Distance-Aware Cross-View Geo-Localization (DACVGL) and a benchmark (DA-Campus) with multi-view imagery and distance annotations. It proposes Dynamic Contrastive Learning (DyCL) to address hierarchical retrieval, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on cross-domain image matching but lack contextual awareness and error cost minimization. DACVGL aims to address this gap.

Method: The paper formulates DACVGL as a hierarchical retrieval problem and introduces DyCL, a contrastive learning framework aligning features by spatial margins.

Result: DyCL complements multi-scale metric learning, improving hierarchical retrieval and geo-localization accuracy.

Conclusion: DyCL effectively addresses DACVGL's challenges, offering a novel solution with publicly available code and benchmark.

Abstract: Existing deep learning-based cross-view geo-localization methods primarily
focus on improving the accuracy of cross-domain image matching, rather than
enabling models to comprehensively capture contextual information around the
target and minimize the cost of localization errors. To support systematic
research into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem,
we construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs
multi-view imagery with precise distance annotations across three spatial
resolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical
retrieval problem across different domains. Our study further reveals that, due
to the inherent complexity of spatial relationships among buildings, this
problem can only be addressed via a contrastive learning paradigm, rather than
conventional metric learning. To tackle this challenge, we propose Dynamic
Contrastive Learning (DyCL), a novel framework that progressively aligns
feature representations according to hierarchical spatial margins. Extensive
experiments demonstrate that DyCL is highly complementary to existing
multi-scale metric learning methods and yields substantial improvements in both
hierarchical retrieval performance and overall cross-view geo-localization
accuracy. Our code and benchmark are publicly available at
https://github.com/anocodetest1/DyCL.

</details>


### [243] [Frequency-enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation](https://arxiv.org/pdf/2506.23086)
*Jian Shi, Tianqi You, Pingping Zhang, Hongli Zhang, Rui Xu, Haojie Li*

Main category: cs.CV

TL;DR: The paper introduces FMC-Net, a frequency-enhanced multi-granularity context network, to improve vertebrae segmentation in 3D CT and MRI images by addressing blurring and distinguishing similar vertebrae.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with image blurring and distinguishing similar vertebrae in spinal structures, limiting segmentation accuracy.

Method: FMC-Net uses wavelet transform for lossless downsampling, processes high-frequency components with HFR for feature refinement, and low-frequency components with MG-SSM for multi-granularity context aggregation.

Result: The method outperforms state-of-the-art approaches on CT and MRI datasets, demonstrating improved segmentation accuracy.

Conclusion: FMC-Net effectively addresses segmentation challenges, offering a robust solution for clinical applications, with publicly available source code.

Abstract: Automated and accurate segmentation of individual vertebra in 3D CT and MRI
images is essential for various clinical applications. Due to the limitations
of current imaging techniques and the complexity of spinal structures, existing
methods still struggle with reducing the impact of image blurring and
distinguishing similar vertebrae. To alleviate these issues, we introduce a
Frequency-enhanced Multi-granularity Context Network (FMC-Net) to improve the
accuracy of vertebrae segmentation. Specifically, we first apply wavelet
transform for lossless downsampling to reduce the feature distortion in blurred
images. The decomposed high and low-frequency components are then processed
separately. For the high-frequency components, we apply a High-frequency
Feature Refinement (HFR) to amplify the prominence of key features and filter
out noises, restoring fine-grained details in blurred images. For the
low-frequency components, we use a Multi-granularity State Space Model (MG-SSM)
to aggregate feature representations with different receptive fields,
extracting spatially-varying contexts while capturing long-range dependencies
with linear complexity. The utilization of multi-granularity contexts is
essential for distinguishing similar vertebrae and improving segmentation
accuracy. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches on both CT and MRI vertebrae segmentation datasets.
The source code is publicly available at https://github.com/anaanaa/FMCNet.

</details>


### [244] [Where, What, Why: Towards Explainable Driver Attention Prediction](https://arxiv.org/pdf/2506.23088)
*Yuchen Zhou, Jiayu Tang, Xiaoyan Xiao, Yueyao Lin, Linkai Liu, Zipeng Guo, Hao Fei, Xiaobo Xia, Chao Gou*

Main category: cs.CV

TL;DR: The paper introduces Explainable Driver Attention Prediction, a task paradigm predicting where, what, and why drivers focus, supported by the W3DA dataset and the LLada framework.


<details>
  <summary>Details</summary>
Motivation: Existing methods predict spatial attention but lack insights into cognitive motivations, limiting understanding of attention mechanisms in driving.

Method: Proposes LLada, a Large Language model-driven framework unifying pixel modeling, semantic parsing, and cognitive reasoning in an end-to-end architecture.

Result: LLada demonstrates robust generalization across datasets and driving conditions.

Conclusion: This work advances understanding of driver attention, benefiting autonomous driving, driver training, and human-computer interaction.

Abstract: Modeling task-driven attention in driving is a fundamental challenge for both
autonomous vehicles and cognitive science. Existing methods primarily predict
where drivers look by generating spatial heatmaps, but fail to capture the
cognitive motivations behind attention allocation in specific contexts, which
limits deeper understanding of attention mechanisms. To bridge this gap, we
introduce Explainable Driver Attention Prediction, a novel task paradigm that
jointly predicts spatial attention regions (where), parses attended semantics
(what), and provides cognitive reasoning for attention allocation (why). To
support this, we present W3DA, the first large-scale explainable driver
attention dataset. It enriches existing benchmarks with detailed semantic and
causal annotations across diverse driving scenarios, including normal
conditions, safety-critical situations, and traffic accidents. We further
propose LLada, a Large Language model-driven framework for driver attention
prediction, which unifies pixel modeling, semantic parsing, and cognitive
reasoning within an end-to-end architecture. Extensive experiments demonstrate
the effectiveness of LLada, exhibiting robust generalization across datasets
and driving conditions. This work serves as a key step toward a deeper
understanding of driver attention mechanisms, with significant implications for
autonomous driving, intelligent driver training, and human-computer
interaction.

</details>


### [245] [DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation](https://arxiv.org/pdf/2506.23104)
*Jihun Kim, Hoyong Kwon, Hyeokjun Kweon, Wooseong Jeong, Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: DC-TTA improves SAM's interactive segmentation by dividing user clicks into subsets for localized updates, outperforming SAM and traditional TTA methods.


<details>
  <summary>Details</summary>
Motivation: SAM struggles in specialized domains and complex scenarios like camouflaged objects, needing better adaptation to user cues.

Method: DC-TTA partitions user clicks into coherent subsets, processes each independently via TTA, and merges adapted models for unified predictions.

Result: DC-TTA outperforms SAM's zero-shot results and conventional TTA, handling complex tasks with fewer interactions and higher accuracy.

Conclusion: DC-TTA effectively adapts SAM for complex scenarios, improving segmentation with minimal user input.

Abstract: Interactive segmentation (IS) allows users to iteratively refine object
boundaries with minimal cues, such as positive and negative clicks. While the
Segment Anything Model (SAM) has garnered attention in the IS community for its
promptable segmentation capabilities, it often struggles in specialized domains
or when handling complex scenarios (e.g., camouflaged or multi-part objects).
To overcome these challenges, we propose DC-TTA, a novel test-time adaptation
(TTA) framework that adapts SAM on a per-sample basis by leveraging user
interactions as supervision. Instead of forcing a single model to incorporate
all user clicks at once, DC-TTA partitions the clicks into more coherent
subsets, each processed independently via TTA with a separated model. This
Divide-and-Conquer strategy reduces conflicts among diverse cues and enables
more localized updates. Finally, we merge the adapted models to form a unified
predictor that integrates the specialized knowledge from each subset.
Experimental results across various benchmarks demonstrate that DC-TTA
significantly outperforms SAM's zero-shot results and conventional TTA methods,
effectively handling complex tasks such as camouflaged object segmentation with
fewer interactions and improved accuracy.

</details>


### [246] [Computer-Aided Multi-Stroke Character Simplification by Stroke Removal](https://arxiv.org/pdf/2506.23106)
*Ryo Ishiyama, Shinnosuke Matsuo, Seiichi Uchida*

Main category: cs.CV

TL;DR: A framework simplifies multi-stroke characters by removing strokes while maintaining legibility, aiding non-native learners and font design.


<details>
  <summary>Details</summary>
Motivation: To reduce learning barriers for non-native speakers and improve font design by simplifying complex characters without losing legibility.

Method: Uses a character recognition model to assess and remove strokes with minimal impact on legibility.

Result: Many characters remain distinguishable even after multiple strokes are removed, suggesting potential for formalized simplification.

Conclusion: The framework shows promise for systematic character simplification, benefiting learners and design.

Abstract: Multi-stroke characters in scripts such as Chinese and Japanese can be highly
complex, posing significant challenges for both native speakers and,
especially, non-native learners. If these characters can be simplified without
degrading their legibility, it could reduce learning barriers for non-native
speakers, facilitate simpler and legible font designs, and contribute to
efficient character-based communication systems. In this paper, we propose a
framework to systematically simplify multi-stroke characters by selectively
removing strokes while preserving their overall legibility. More specifically,
we use a highly accurate character recognition model to assess legibility and
remove those strokes that minimally impact it. Experimental results on 1,256
character classes with 5, 10, 15, and 20 strokes reveal several key findings,
including the observation that even after removing multiple strokes, many
characters remain distinguishable. These findings suggest the potential for
more formalized simplification strategies.

</details>


### [247] [Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound](https://arxiv.org/pdf/2506.23108)
*Zhiyuan Zhu, Jian Wang, Yong Jiang, Tong Han, Yuhao Huang, Ang Zhang, Kaiwen Yang, Mingyuan Luo, Zhe Liu, Yaofei Duan, Dong Ni, Tianhong Tang, Xin Yang*

Main category: cs.CV

TL;DR: A novel Corpus-View-Category Refinement Framework (CVC-RF) is proposed for accurate carotid plaque grading (CPG), addressing feature fusion, representation learning, and class feature differences in multi-view ultrasound images.


<details>
  <summary>Details</summary>
Motivation: Accurate CPG is crucial for assessing cardiovascular and cerebrovascular risks, but existing deep learning methods neglect representation learning and class feature differences.

Method: CVC-RF processes information at Corpus-, View-, and Category-levels, using center-memory contrastive loss, cascaded down-sampling attention, and a mixture-of-experts weighting strategy.

Result: CVC-RF achieves state-of-the-art performance in CPG by effectively modeling global features through multi-level refinement.

Conclusion: The proposed framework enhances CPG accuracy by addressing key limitations in existing methods, demonstrating superior performance.

Abstract: Accurate carotid plaque grading (CPG) is vital to assess the risk of
cardiovascular and cerebrovascular diseases. Due to the small size and high
intra-class variability of plaque, CPG is commonly evaluated using a
combination of transverse and longitudinal ultrasound views in clinical
practice. However, most existing deep learning-based multi-view classification
methods focus on feature fusion across different views, neglecting the
importance of representation learning and the difference in class features. To
address these issues, we propose a novel Corpus-View-Category Refinement
Framework (CVC-RF) that processes information from Corpus-, View-, and
Category-levels, enhancing model performance. Our contribution is four-fold.
First, to the best of our knowledge, we are the foremost deep learning-based
method for CPG according to the latest Carotid Plaque-RADS guidelines. Second,
we propose a novel center-memory contrastive loss, which enhances the network's
global modeling capability by comparing with representative cluster centers and
diverse negative samples at the Corpus level. Third, we design a cascaded
down-sampling attention module to fuse multi-scale information and achieve
implicit feature interaction at the View level. Finally, a parameter-free
mixture-of-experts weighting strategy is introduced to leverage class
clustering knowledge to weight different experts, enabling feature decoupling
at the Category level. Experimental results indicate that CVC-RF effectively
models global features via multi-level refinement, achieving state-of-the-art
performance in the challenging CPG task.

</details>


### [248] [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/pdf/2506.23115)
*Haonan Chen, Hong Liu, Yuping Luo, Liang Wang, Nan Yang, Furu Wei, Zhicheng Dou*

Main category: cs.CV

TL;DR: MoCa is a two-stage framework transforming pre-trained VLMs into bidirectional multimodal embedding models, addressing limitations of current approaches through continual pre-training and diverse contrastive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current multimodal embedding models using causal VLMs have suboptimal attention for embedding tasks, scalability issues, and limited training diversity.

Method: MoCa involves Modality-aware Continual Pre-training for bidirectional reasoning and Heterogeneous Contrastive Fine-tuning using diverse multimodal data.

Result: MoCa achieves state-of-the-art results on MMEB and ViDoRe-v2 benchmarks and shows strong scalability.

Conclusion: MoCa effectively addresses key limitations of current models, improving performance and robustness in multimodal embedding tasks.

Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs),
have shown promise in various tasks. However, current approaches face three key
limitations: the use of causal attention in VLM backbones is suboptimal for
embedding tasks; scalability issues due to reliance on high-quality labeled
paired data for contrastive learning; and limited diversity in training
objectives and data. To address these issues, we propose MoCa, a two-stage
framework for transforming pre-trained VLMs into effective bidirectional
multimodal embedding models. The first stage, Modality-aware Continual
Pre-training, introduces a joint reconstruction objective that simultaneously
denoises interleaved text and image inputs, enhancing bidirectional
context-aware reasoning. The second stage, Heterogeneous Contrastive
Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple
image-caption pairs to enhance generalization and alignment. Our method
addresses the stated limitations by introducing bidirectional attention through
continual pre-training, scaling effectively with massive unlabeled datasets via
joint reconstruction objectives, and utilizing diverse multimodal data for
enhanced representation robustness. Experiments demonstrate that MoCa
consistently improves performance across MMEB and ViDoRe-v2 benchmarks,
achieving new state-of-the-art results, and exhibits strong scalability with
both model size and training data on MMEB.

</details>


### [249] [Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation](https://arxiv.org/pdf/2506.23120)
*Zhenhua Ning, Zhuotao Tian, Shaoshuai Shi, Guangming Lu, Daojing He, Wenjie Pei, Li Jiang*

Main category: cs.CV

TL;DR: The paper introduces R²S, a reasoning-based segmentation framework, and 3D ReasonSeg dataset to enhance spatial reasoning in 3D point cloud perception.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex instructions requiring accurate spatial reasoning despite detailed spatial cues in 3D point clouds.

Method: R²S decomposes spatial reasoning into two stages: identifying relevant elements and processing instructions using visual priors. The 3D ReasonSeg dataset supports this with annotated samples.

Result: R²S and 3D ReasonSeg improve spatial reasoning in 3D point cloud perception, validated by experiments.

Conclusion: The framework and dataset provide a new baseline for future work in 3D point cloud perception with enhanced spatial reasoning.

Abstract: Recent advances in point cloud perception have demonstrated remarkable
progress in scene understanding through vision-language alignment leveraging
large language models (LLMs). However, existing methods may still encounter
challenges in handling complex instructions that require accurate spatial
reasoning, even if the 3D point cloud data provides detailed spatial cues such
as size and position for identifying the targets. To tackle this issue, we
propose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based
segmentation framework. The framework emulates human cognitive processes by
decomposing spatial reasoning into two sequential stages: first identifying
relevant elements, then processing instructions guided by their associated
visual priors. Furthermore, acknowledging the inadequacy of existing datasets
in complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based
segmentation dataset comprising 25,185 training samples and 3,966 validation
samples with precise annotations. Both quantitative and qualitative experiments
demonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud
perception with stronger spatial reasoning capabilities, and we hope that they
can serve as a new baseline and benchmark for future work.

</details>


### [250] [Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval](https://arxiv.org/pdf/2506.23132)
*Sophie Zhou, Shu Kong*

Main category: cs.CV

TL;DR: The paper proposes a method for detecting and explaining art plagiarism by retrieving visually similar authentic artworks, using a dataset of paintings and AI-synthesized plagiarized versions. A baseline method achieves high recognition accuracy but low retrieval precision, while a finetuned model improves retrieval but reduces accuracy.


<details>
  <summary>Details</summary>
Motivation: To protect artists' copyrights and intellectual property by improving the detection and explanation of plagiarized paintings.

Method: Constructs a dataset of paintings and AI-synthesized plagiarized versions. Uses DINOv2 for baseline retrieval and classification, then finetunes DINOv2 with metric learning to improve retrieval.

Result: Baseline achieves 97.2% recognition accuracy but 29.0% AP retrieval precision. Finetuned model improves retrieval by 12% AP but reduces accuracy to 92.7%.

Conclusion: The trade-off between retrieval precision and recognition accuracy highlights challenges, suggesting future research directions.

Abstract: Art plagiarism detection plays a crucial role in protecting artists'
copyrights and intellectual property, yet it remains a challenging problem in
forensic analysis. In this paper, we address the task of recognizing
plagiarized paintings and explaining the detected plagarisms by retrieving
visually similar authentic artworks. To support this study, we construct a
dataset by collecting painting photos and synthesizing plagiarized versions
using generative AI, tailored to specific artists' styles. We first establish a
baseline approach using off-the-shelf features from the visual foundation model
DINOv2 to retrieve the most similar images in the database and classify
plagiarism based on a similarity threshold. Surprisingly, this non-learned
method achieves a high recognition accuracy of 97.2\% but suffers from low
retrieval precision 29.0\% average precision (AP). To improve retrieval
quality, we finetune DINOv2 with a metric learning loss using positive and
negative sample pairs sampled in the database. The finetuned model greatly
improves retrieval performance by 12\% AP over the baseline, though it
unexpectedly results in a lower recognition accuracy (92.7\%). We conclude with
insightful discussions and outline directions for future research.

</details>


### [251] [RoboScape: Physics-informed Embodied World Model](https://arxiv.org/pdf/2506.23135)
*Yu Shang, Xin Zhang, Yinzhou Tang, Lei Jin, Chen Gao, Wei Wu, Yong Li*

Main category: cs.CV

TL;DR: RoboScape is a physics-informed world model for realistic robotic video generation, addressing 3D geometry and motion dynamics limitations.


<details>
  <summary>Details</summary>
Motivation: Current embodied world models lack physical awareness, leading to unrealistic videos in contact-rich robotic scenarios.

Method: RoboScape integrates RGB video generation with physics knowledge through temporal depth prediction and keypoint dynamics learning.

Result: RoboScape produces videos with high visual fidelity and physical plausibility, validated in robotic policy training and evaluation.

Conclusion: RoboScape advances embodied intelligence by efficiently integrating physics into world models.

Abstract: World models have become indispensable tools for embodied intelligence,
serving as powerful simulators capable of generating realistic robotic videos
while addressing critical data scarcity challenges. However, current embodied
world models exhibit limited physical awareness, particularly in modeling 3D
geometry and motion dynamics, resulting in unrealistic video generation for
contact-rich robotic scenarios. In this paper, we present RoboScape, a unified
physics-informed world model that jointly learns RGB video generation and
physics knowledge within an integrated framework. We introduce two key
physics-informed joint training tasks: temporal depth prediction that enhances
3D geometric consistency in video rendering, and keypoint dynamics learning
that implicitly encodes physical properties (e.g., object shape and material
characteristics) while improving complex motion modeling. Extensive experiments
demonstrate that RoboScape generates videos with superior visual fidelity and
physical plausibility across diverse robotic scenarios. We further validate its
practical utility through downstream applications including robotic policy
training with generated data and policy evaluation. Our work provides new
insights for building efficient physics-informed world models to advance
embodied intelligence research. The code is available at:
https://github.com/tsinghua-fib-lab/RoboScape.

</details>


### [252] [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/pdf/2506.23219)
*Jie Feng, Shengyuan Wang, Tianhui Liu, Yanxin Xi, Yong Li*

Main category: cs.CV

TL;DR: UrbanLLaVA is a multi-modal large language model designed for urban research, outperforming general MLLMs by processing diverse urban data types and tasks through a curated dataset and multi-stage training.


<details>
  <summary>Details</summary>
Motivation: Current urban research methods lack a unified framework for multi-modal data, prompting the development of UrbanLLaVA to address this gap.

Method: UrbanLLaVA uses a curated urban instruction dataset and a multi-stage training framework to enhance spatial reasoning and domain knowledge learning.

Result: UrbanLLaVA outperforms open-source and proprietary MLLMs in single-modal and cross-modal tasks across three cities.

Conclusion: UrbanLLaVA offers a robust solution for urban research, demonstrating strong performance and generalization, with open access to code and data.

Abstract: Urban research involves a wide range of scenarios and tasks that require the
understanding of multi-modal data. Current methods often focus on specific data
types and lack a unified framework in urban field for processing them
comprehensively. The recent success of multi-modal large language models
(MLLMs) presents a promising opportunity to overcome this limitation. In this
paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model
designed to process these four types of data simultaneously and achieve strong
performance across diverse urban tasks compared with general MLLMs. In
$\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset
encompassing both single-modal and cross-modal urban data, spanning from
location view to global view of urban environment. Additionally, we propose a
multi-stage training framework that decouples spatial reasoning enhancement
from domain knowledge learning, thereby improving the compatibility and
downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks.
Finally, we also extend existing benchmark for urban research to assess the
performance of MLLMs across a wide range of urban tasks. Experimental results
from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms
open-source and proprietary MLLMs in both single-modal tasks and complex
cross-modal tasks and shows robust generalization abilities across cities.
Source codes and data are openly accessible to the research community via
https://github.com/tsinghua-fib-lab/UrbanLLaVA.

</details>


### [253] [VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis](https://arxiv.org/pdf/2506.23138)
*Shiyu Wu, Mingzhen Sun, Weining Wang, Yequan Wang, Jing Liu*

Main category: cs.CV

TL;DR: VisualPrompter is a training-free prompt engineering framework that refines user inputs for diffusion models, improving text-image alignment without compromising aesthetics.


<details>
  <summary>Details</summary>
Motivation: Address the gap between user-provided and model-preferred prompts, ensuring semantic alignment in generated images.

Method: Uses an automatic self-reflection module to identify missing concepts and a target-specific prompt optimization mechanism.

Result: Achieves state-of-the-art performance on text-image alignment benchmarks and is adaptable to various models.

Conclusion: VisualPrompter effectively bridges the gap between user intent and model output, enhancing both alignment and aesthetics.

Abstract: Since there exists a notable gap between user-provided and model-preferred
prompts, generating high-quality and satisfactory images using diffusion models
often requires prompt engineering to optimize user inputs. Current studies on
text-to-image prompt engineering can effectively enhance the style and
aesthetics of generated images. However, they often neglect the semantic
alignment between generated images and user descriptions, resulting in visually
appealing but content-wise unsatisfying outputs. In this work, we propose
VisualPrompter, a novel training-free prompt engineering framework that refines
user inputs to model-preferred sentences. In particular, VisualPrompter
utilizes an automatic self-reflection module to identify the missing concepts
in generated images and a target-specific prompt optimization mechanism to
revise the prompts in a fine-grained manner. Extensive experiments demonstrate
the effectiveness of our VisualPrompter, which achieves new state-of-the-art
performance on multiple benchmarks for text-image alignment evaluation.
Additionally, our framework features a plug-and-play design, making it highly
adaptable to various generative models.

</details>


### [254] [AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation](https://arxiv.org/pdf/2506.23150)
*Xinyue Liang, Zhiyuan Ma, Lingchen Sun, Yanjun Guo, Lei Zhang*

Main category: cs.CV

TL;DR: AlignCVC introduces a distribution alignment framework for single-image-to-3D generation, improving cross-view consistency (CVC) and accelerating inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with noisy and unstable reconstruction outputs, limiting CVC improvement in single-image-to-3D workflows.

Method: AlignCVC aligns generated and reconstructed multi-view distributions with ground-truth, using a soft-hard alignment strategy for generation and reconstruction models.

Result: The method enhances generation quality, achieves strong CVC, and speeds up inference to as few as 4 steps.

Conclusion: AlignCVC is an effective, efficient, and plug-and-play solution for single-image-to-3D generation.

Abstract: Single-image-to-3D models typically follow a sequential generation and
reconstruction workflow. However, intermediate multi-view images synthesized by
pre-trained generation models often lack cross-view consistency (CVC),
significantly degrading 3D reconstruction performance. While recent methods
attempt to refine CVC by feeding reconstruction results back into the
multi-view generator, these approaches struggle with noisy and unstable
reconstruction outputs that limit effective CVC improvement. We introduce
AlignCVC, a novel framework that fundamentally re-frames single-image-to-3D
generation through distribution alignment rather than relying on strict
regression losses. Our key insight is to align both generated and reconstructed
multi-view distributions toward the ground-truth multi-view distribution,
establishing a principled foundation for improved CVC. Observing that generated
images exhibit weak CVC while reconstructed images display strong CVC due to
explicit rendering, we propose a soft-hard alignment strategy with distinct
objectives for generation and reconstruction models. This approach not only
enhances generation quality but also dramatically accelerates inference to as
few as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC,
seamlessly integrates various multi-view generation models with 3D
reconstruction models. Extensive experiments demonstrate the effectiveness and
efficiency of AlignCVC for single-image-to-3D generation.

</details>


### [255] [Dynamic View Synthesis from Small Camera Motion Videos](https://arxiv.org/pdf/2506.23153)
*Huiqiang Sun, Xingyi Li, Juewen Peng, Liao Shen, Zhiguo Cao, Ke Xian, Guosheng Lin*

Main category: cs.CV

TL;DR: The paper addresses challenges in novel view synthesis for dynamic 3D scenes with small camera motion by proposing Distribution-based Depth Regularization (DDR) and camera parameter learning.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF-based methods struggle with limited camera motion, leading to incorrect scene geometry and inaccurate camera parameter estimation.

Method: Introduces DDR to align rendering weight distribution with true distribution using Gumbel-softmax and enforces volume density constraints. Also learns camera parameters during training.

Result: The approach effectively handles small camera motion inputs, outperforming state-of-the-art methods.

Conclusion: The proposed DDR and camera parameter learning improve scene representation and robustness for dynamic 3D scenes with limited camera motion.

Abstract: Novel view synthesis for dynamic $3$D scenes poses a significant challenge.
Many notable efforts use NeRF-based approaches to address this task and yield
impressive results. However, these methods rely heavily on sufficient motion
parallax in the input images or videos. When the camera motion range becomes
limited or even stationary (i.e., small camera motion), existing methods
encounter two primary challenges: incorrect representation of scene geometry
and inaccurate estimation of camera parameters. These challenges make prior
methods struggle to produce satisfactory results or even become invalid. To
address the first challenge, we propose a novel Distribution-based Depth
Regularization (DDR) that ensures the rendering weight distribution to align
with the true distribution. Specifically, unlike previous methods that use
depth loss to calculate the error of the expectation, we calculate the
expectation of the error by using Gumbel-softmax to differentiably sample
points from discrete rendering weight distribution. Additionally, we introduce
constraints that enforce the volume density of spatial points before the object
boundary along the ray to be near zero, ensuring that our model learns the
correct geometry of the scene. To demystify the DDR, we further propose a
visualization tool that enables observing the scene geometry representation at
the rendering weight level. For the second challenge, we incorporate camera
parameter learning during training to enhance the robustness of our model to
camera parameters. We conduct extensive experiments to demonstrate the
effectiveness of our approach in representing scenes with small camera motion
input, and our results compare favorably to state-of-the-art methods.

</details>


### [256] [Self-Supervised Contrastive Learning for Multi-Label Images](https://arxiv.org/pdf/2506.23156)
*Jiale Chen*

Main category: cs.CV

TL;DR: A self-supervised learning (SSL) method tailored for multi-label images is proposed, using block-wise augmentation and image-aware contrastive loss to achieve competitive performance with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Mainstream SSL methods rely on single-label datasets like ImageNet, ignoring multi-label images' richer semantics and causing high pre-training overhead.

Method: Proposes a block-wise augmentation module to extract positive view pairs from multi-label images and an image-aware contrastive loss for semantically consistent representations.

Result: Validated through linear fine-tuning and transfer learning, showing competitiveness despite limited sample quality and quantity.

Conclusion: The method effectively leverages multi-label images for SSL, reducing overhead while maintaining strong representation learning capabilities.

Abstract: Self-supervised learning (SSL) has demonstrated its effectiveness in learning
representations through comparison methods that align with human intuition.
However, mainstream SSL methods heavily rely on high body datasets with single
label, such as ImageNet, resulting in intolerable pre-training overhead.
Besides, more general multi-label images are frequently overlooked in SSL,
despite their potential for richer semantic information and broader
applicability in downstream scenarios. Therefore, we tailor the mainstream SSL
approach to guarantee excellent representation learning capabilities using
fewer multi-label images. Firstly, we propose a block-wise augmentation module
aimed at extracting additional potential positive view pairs from multi-label
images. Subsequently, an image-aware contrastive loss is devised to establish
connections between these views, thereby facilitating the extraction of
semantically consistent representations. Comprehensive linear fine-tuning and
transfer learning validate the competitiveness of our approach despite
challenging sample quality and quantity.

</details>


### [257] [STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene](https://arxiv.org/pdf/2506.23157)
*Hanyu Zhou, Haonan Wang, Haoyue Liu, Yuxing Duan, Luxin Yan, Gim Hee Lee*

Main category: cs.CV

TL;DR: A spatiotemporal-disentangled Gaussian splatting framework is proposed for high-dynamic scene reconstruction, using event cameras to compensate for frame cameras and clustering to distinguish features.


<details>
  <summary>Details</summary>
Motivation: Existing unified representation models fail to handle discontinuous temporal features and heterogeneous spatial features in dynamic scenes.

Method: Disentangles spatiotemporal features into latent representations, uses event cameras alongside frame cameras, and employs clustering to distinguish background and object features.

Result: The method improves spatiotemporal discrimination, enabling time-continuous dynamic scene reconstruction.

Conclusion: The proposed framework effectively addresses spatiotemporal mismatching, verified by extensive experiments.

Abstract: High-dynamic scene reconstruction aims to represent static background with
rigid spatial features and dynamic objects with deformed continuous
spatiotemporal features. Typically, existing methods adopt unified
representation model (e.g., Gaussian) to directly match the spatiotemporal
features of dynamic scene from frame camera. However, this unified paradigm
fails in the potential discontinuous temporal features of objects due to frame
imaging and the heterogeneous spatial features between background and objects.
To address this issue, we disentangle the spatiotemporal features into various
latent representations to alleviate the spatiotemporal mismatching between
background and objects. In this work, we introduce event camera to compensate
for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting
framework for high-dynamic scene reconstruction. As for dynamic scene, we
figure out that background and objects have appearance discrepancy in
frame-based spatial features and motion discrepancy in event-based temporal
features, which motivates us to distinguish the spatiotemporal features between
background and objects via clustering. As for dynamic object, we discover that
Gaussian representations and event data share the consistent spatiotemporal
characteristic, which could serve as a prior to guide the spatiotemporal
disentanglement of object Gaussians. Within Gaussian splatting framework, the
cumulative scene-object disentanglement can improve the spatiotemporal
discrimination between background and objects to render the time-continuous
dynamic scene. Extensive experiments have been performed to verify the
superiority of the proposed method.

</details>


### [258] [Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](https://arxiv.org/pdf/2506.23714)
*Md Moinul Islam, Sofoklis Kakouros, Janne Heikkilä, Mourad Oussalah*

Main category: cs.CV

TL;DR: A multimodal video summarization framework integrates text, audio, and visual cues to create timestamp-aligned summaries, outperforming traditional methods in both text and video metrics.


<details>
  <summary>Details</summary>
Motivation: The rise in video content demands advanced summarization techniques beyond unimodal approaches to capture semantically and emotionally important moments.

Method: The framework uses prosodic features, textual cues, and visual indicators to identify key moments, including bonus words emphasized across modalities.

Result: Improvements include ROUGE-1 (0.4769 to 0.7929), BERTScore (0.9152 to 0.9536), and a 23% F1-Score boost in video evaluation.

Conclusion: Multimodal integration enhances video summarization by producing comprehensive, behaviourally informed summaries.

Abstract: The increasing volume of video content in educational, professional, and
social domains necessitates effective summarization techniques that go beyond
traditional unimodal approaches. This paper proposes a behaviour-aware
multimodal video summarization framework that integrates textual, audio, and
visual cues to generate timestamp-aligned summaries. By extracting prosodic
features, textual cues and visual indicators, the framework identifies
semantically and emotionally important moments. A key contribution is the
identification of bonus words, which are terms emphasized across multiple
modalities and used to improve the semantic relevance and expressive clarity of
the summaries. The approach is evaluated against pseudo-ground truth (pGT)
summaries generated using LLM-based extractive method. Experimental results
demonstrate significant improvements over traditional extractive method, such
as the Edmundson method, in both text and video-based evaluation metrics.
Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore
from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework
improves F1-Score by almost 23%. The findings underscore the potential of
multimodal integration in producing comprehensive and behaviourally informed
video summaries.

</details>


### [259] [Trident: Detecting Face Forgeries with Adversarial Triplet Learning](https://arxiv.org/pdf/2506.23189)
*Mustafa Hakan Kara, Aysegul Dundar, Uğur Güdükbay*

Main category: cs.CV

TL;DR: Trident is a face forgery detection framework using triplet learning and Siamese networks for adaptability across diverse forgery methods, enhanced by domain-adversarial training for robustness.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks generate increasingly sophisticated face forgeries, challenging detection models that often fail against unseen techniques.

Method: Trident employs triplet learning with a Siamese network, domain-adversarial training, and controlled gradient flow to isolate forgery features.

Result: The framework shows effectiveness in detecting diverse forgeries, demonstrated by benchmarks and ablation studies.

Conclusion: Trident improves robustness and generalizability in face forgery detection, with code to be released.

Abstract: As face forgeries generated by deep neural networks become increasingly
sophisticated, detecting face manipulations in digital media has posed a
significant challenge, underscoring the importance of maintaining digital media
integrity and combating visual disinformation. Current detection models,
predominantly based on supervised training with domain-specific data, often
falter against forgeries generated by unencountered techniques. In response to
this challenge, we introduce \textit{Trident}, a face forgery detection
framework that employs triplet learning with a Siamese network architecture for
enhanced adaptability across diverse forgery methods. \textit{Trident} is
trained on curated triplets to isolate nuanced differences of forgeries,
capturing fine-grained features that distinguish pristine samples from
manipulated ones while controlling for other variables. To further enhance
generalizability, we incorporate domain-adversarial training with a forgery
discriminator. This adversarial component guides our embedding model towards
forgery-agnostic representations, improving its robustness to unseen
manipulations. In addition, we prevent gradient flow from the classifier head
to the embedding model, avoiding overfitting induced by artifacts peculiar to
certain forgeries. Comprehensive evaluations across multiple benchmarks and
ablation studies demonstrate the effectiveness of our framework. We will
release our code in a GitHub repository.

</details>


### [260] [DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding](https://arxiv.org/pdf/2506.23196)
*Mona Ahmadian, Amir Shirian, Frank Guerin, Andrew Gilbert*

Main category: cs.CV

TL;DR: DEL is a framework for dense semantic action localization in videos, using multimodal alignment and interaction refinement to achieve state-of-the-art performance on TAL datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world videos contain overlapping events and complex temporal dependencies, making multimodal interaction modeling challenging.

Method: DEL aligns audio and visual features with masked self-attention for intra-mode consistency and refines multimodal interactions across scales.

Result: DEL surpasses previous methods with notable mAP gains on UnAV-100, THUMOS14, ActivityNet 1.3, and EPIC-Kitchens-100.

Conclusion: DEL effectively addresses the challenges of dense action localization in videos, achieving superior performance.

Abstract: Real-world videos often contain overlapping events and complex temporal
dependencies, making multimodal interaction modeling particularly challenging.
We introduce DEL, a framework for dense semantic action localization, aiming to
accurately detect and classify multiple actions at fine-grained temporal
resolutions in long untrimmed videos. DEL consists of two key modules: the
alignment of audio and visual features that leverage masked self-attention to
enhance intra-mode consistency and a multimodal interaction refinement module
that models cross-modal dependencies across multiple scales, enabling
high-level semantics and fine-grained details. Our method achieves
state-of-the-art performance on multiple real-world Temporal Action
Localization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and
EPIC-Kitchens-100, surpassing previous approaches with notable average mAP
gains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.

</details>


### [261] [Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing](https://arxiv.org/pdf/2506.23202)
*Qilin Shu, Qixian Zhang, Qi Zhang, Hongyun Zhang, Duoqian Miao, Cairong Zhao*

Main category: cs.CV

TL;DR: The paper proposes HAMW, a method to enhance transformer-based person search by addressing high-frequency feature suppression and computational costs through high-frequency augmentation and multi-wave mixing.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models for person search suppress high-frequency features and have high computational costs, limiting performance.

Method: HAMW uses a three-stage framework with high-frequency augmentation and replaces self-attention with multi-level Haar wavelet fusion for multi-scale feature capture.

Result: HAMW achieves state-of-the-art performance on CUHK-SYSU and PRW datasets.

Conclusion: HAMW effectively improves feature extraction and efficiency in person search tasks.

Abstract: The person search task aims to locate a target person within a set of scene
images. In recent years, transformer-based models in this field have made some
progress. However, they still face three primary challenges: 1) the
self-attention mechanism tends to suppress high-frequency components in the
features, which severely impacts model performance; 2) the computational cost
of transformers is relatively high. To address these issues, we propose a novel
High-frequency Augmentation and Multi-Wave mixing (HAMW) method for person
search. HAMW is designed to enhance the discriminative feature extraction
capabilities of transformers while reducing computational overhead and
improving efficiency. Specifically, we develop a three-stage framework that
progressively optimizes both detection and re-identification performance. Our
model enhances the perception of high-frequency features by learning from
augmented inputs containing additional high-frequency components. Furthermore,
we replace the self-attention layers in the transformer with a strategy based
on multi-level Haar wavelet fusion to capture multi-scale features. This not
only lowers the computational complexity but also alleviates the suppression of
high-frequency features and enhances the ability to exploit multi-scale
information. Extensive experiments demonstrate that HAMW achieves
state-of-the-art performance on both the CUHK-SYSU and PRW datasets.

</details>


### [262] [Ella: Embodied Social Agents with Lifelong Memory](https://arxiv.org/pdf/2506.24019)
*Hongxin Zhang, Zheyuan Zhang, Zeyuan Wang, Zunzhe Zhang, Lixing Fang, Qinhong Zhou, Chuang Gan*

Main category: cs.CV

TL;DR: Ella is an embodied social agent with lifelong learning capabilities in a 3D open world, using a multimodal memory system and foundation models for autonomous evolution and social interactions.


<details>
  <summary>Details</summary>
Motivation: To advance embodied intelligence by integrating structured memory systems with foundation models, enabling agents to learn and evolve through social interactions and observations.

Method: Ella employs a long-term multimodal memory system (semantic and episodic) combined with foundation models for decision-making, planning, and social relationship building.

Result: Ella successfully influences, leads, and cooperates with other agents in a dynamic 3D world, demonstrating effective learning through observation and interaction.

Conclusion: The integration of structured memory with foundation models holds transformative potential for advancing embodied intelligence.

Abstract: We introduce Ella, an embodied social agent capable of lifelong learning
within a community in a 3D open world, where agents accumulate experiences and
acquire knowledge through everyday visual observations and social interactions.
At the core of Ella's capabilities is a structured, long-term multimodal memory
system that stores, updates, and retrieves information effectively. It consists
of a name-centric semantic memory for organizing acquired knowledge and a
spatiotemporal episodic memory for capturing multimodal experiences. By
integrating this lifelong memory system with foundation models, Ella retrieves
relevant information for decision-making, plans daily activities, builds social
relationships, and evolves autonomously while coexisting with other intelligent
beings in the open world. We conduct capability-oriented evaluations in a
dynamic 3D open world where 15 agents engage in social activities for days and
are assessed with a suite of unseen controlled evaluations. Experimental
results show that Ella can influence, lead, and cooperate with other agents
well to achieve goals, showcasing its ability to learn effectively through
observation and social interaction. Our findings highlight the transformative
potential of combining structured memory systems with foundation models for
advancing embodied intelligence. More videos can be found at
https://umass-embodied-agi.github.io/Ella/.

</details>


### [263] [VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions](https://arxiv.org/pdf/2506.23236)
*Marko Mihajlovic, Siwei Zhang, Gen Li, Kaifeng Zhao, Lea Müller, Siyu Tang*

Main category: cs.CV

TL;DR: VolumetricSMPL introduces a neural volumetric body model using Neural Blend Weights (NBW) for efficient MLP decoders, outperforming prior models in speed, memory, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional surface mesh models struggle with interactions and efficiency, while existing volumetric models are either insufficiently robust or too costly.

Method: Leverages NBW to dynamically blend learned weight matrices, reducing computational costs while maintaining expressiveness.

Result: Achieves 10x faster inference, 6x lower GPU memory usage, and enhanced accuracy, with applications in reconstruction, motion synthesis, and more.

Conclusion: VolumetricSMPL offers broad applicability and significant performance gains, addressing limitations of prior models.

Abstract: Parametric human body models play a crucial role in computer graphics and
vision, enabling applications ranging from human motion analysis to
understanding human-environment interactions. Traditionally, these models use
surface meshes, which pose challenges in efficiently handling interactions with
other geometric entities, such as objects and scenes, typically represented as
meshes or point clouds. To address this limitation, recent research has
explored volumetric neural implicit body models. However, existing works are
either insufficiently robust for complex human articulations or impose high
computational and memory costs, limiting their widespread use. To this end, we
introduce VolumetricSMPL, a neural volumetric body model that leverages Neural
Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike
prior approaches that rely on large MLPs, NBW dynamically blends a small set of
learned weight matrices using predicted shape- and pose-dependent coefficients,
significantly improving computational efficiency while preserving
expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model
COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,
and a Signed Distance Function (SDF) for efficient and differentiable contact
modeling. We demonstrate VolumetricSMPL's strengths across four challenging
tasks: (1) reconstructing human-object interactions from in-the-wild images,
(2) recovering human meshes in 3D scenes from egocentric views, (3)
scene-constrained motion synthesis, and (4) resolving self-intersections. Our
results highlight its broad applicability and significant performance and
efficiency gains.

</details>


### [264] [BridgeShape: Latent Diffusion Schrödinger Bridge for 3D Shape Completion](https://arxiv.org/pdf/2506.23205)
*Dequan Kong, Zhe Zhu, Honghua Chen, Mingqiang Wei*

Main category: cs.CV

TL;DR: BridgeShape introduces a latent diffusion Schr\"odinger bridge framework for 3D shape completion, addressing suboptimal global transport and resolution constraints by modeling optimal transport and using a compact latent space.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to model optimal global transport paths and face resolution limitations, leading to suboptimal completions.

Method: BridgeShape formulates shape completion as an optimal transport problem and uses a Depth-Enhanced VQ-VAE to encode shapes into a compact latent space.

Result: Achieves state-of-the-art performance, enabling high-fidelity completions at higher resolutions and for unseen classes.

Conclusion: BridgeShape effectively addresses key challenges in 3D shape completion, offering superior fidelity and scalability.

Abstract: Existing diffusion-based 3D shape completion methods typically use a
conditional paradigm, injecting incomplete shape information into the denoising
network via deep feature interactions (e.g., concatenation, cross-attention) to
guide sampling toward complete shapes, often represented by voxel-based
distance functions. However, these approaches fail to explicitly model the
optimal global transport path, leading to suboptimal completions. Moreover,
performing diffusion directly in voxel space imposes resolution constraints,
limiting the generation of fine-grained geometric details. To address these
challenges, we propose BridgeShape, a novel framework for 3D shape completion
via latent diffusion Schr\"odinger bridge. The key innovations lie in two
aspects: (i) BridgeShape formulates shape completion as an optimal transport
problem, explicitly modeling the transition between incomplete and complete
shapes to ensure a globally coherent transformation. (ii) We introduce a
Depth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D
shapes into a compact latent space, leveraging self-projected multi-view depth
information enriched with strong DINOv2 features to enhance geometric
structural perception. By operating in a compact yet structurally informative
latent space, BridgeShape effectively mitigates resolution constraints and
enables more efficient and high-fidelity 3D shape completion. BridgeShape
achieves state-of-the-art performance on large-scale 3D shape completion
benchmarks, demonstrating superior fidelity at higher resolutions and for
unseen object classes.

</details>


### [265] [Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification](https://arxiv.org/pdf/2506.23247)
*James Hinns, David Martens*

Main category: cs.CV

TL;DR: Segment Attribution Tables (SATs) summarize local saliency explanations into semi-global insights, bridging the gap between oversimplified global methods and overly detailed local explanations.


<details>
  <summary>Details</summary>
Motivation: Deep learning models lack transparency in predictions. Local explanations (e.g., saliency maps) are too granular, while global methods oversimplify. SATs aim to provide a middle ground.

Method: SATs use image segments (e.g., "eyes") and saliency maps to quantify segment influence, revealing model reliance on concepts and spurious correlations.

Result: SATs highlight recurring patterns and spurious correlations (e.g., backgrounds or watermarks) without requiring changes in test performance.

Conclusion: SATs offer a practical tool for analyzing and debugging image classifiers, balancing detail and simplicity.

Abstract: Deep learning dominates image classification tasks, yet understanding how
models arrive at predictions remains a challenge. Much research focuses on
local explanations of individual predictions, such as saliency maps, which
visualise the influence of specific pixels on a model's prediction. However,
reviewing many of these explanations to identify recurring patterns is
infeasible, while global methods often oversimplify and miss important local
behaviours. To address this, we propose Segment Attribution Tables (SATs), a
method for summarising local saliency explanations into (semi-)global insights.
SATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency
maps to quantify their influence. These segments highlight concepts the model
relies on across instances and reveal spurious correlations, such as reliance
on backgrounds or watermarks, even when out-of-distribution test performance
sees little change. SATs can explain any classifier for which a form of
saliency map can be produced, using segmentation maps that provide named
segments. SATs bridge the gap between oversimplified global summaries and
overly detailed local explanations, offering a practical tool for analysing and
debugging image classifiers.

</details>


### [266] [TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints](https://arxiv.org/pdf/2506.23207)
*Zhen Tan, Xieyuanli Chen, Lei Feng, Yangbing Ge, Shuaifeng Zhi, Jiaxiong Liu, Dewen Hu*

Main category: cs.CV

TL;DR: TVG-SLAM improves RGB-only 3DGS SLAM by using tri-view geometry for robust tracking and high-quality mapping, outperforming prior methods in challenging outdoor environments.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-only SLAM systems rely heavily on photometric loss, making them less robust in unbounded outdoor environments with viewpoint and illumination changes.

Method: Introduces tri-view matching for geometric constraints, hybrid geometric constraints for tracking, probabilistic initialization for mapping, and dynamic attenuation to reduce drift.

Result: Outperforms prior systems, reducing Absolute Trajectory Error by 69.0% in challenging datasets while maintaining high rendering quality.

Conclusion: TVG-SLAM offers a robust solution for RGB-only SLAM in dynamic outdoor environments, with plans for open-source release.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM
systems to achieve high-fidelity scene representation. However, the heavy
reliance of existing systems on photometric rendering loss for camera tracking
undermines their robustness, especially in unbounded outdoor environments with
severe viewpoint and illumination changes. To address these challenges, we
propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel
tri-view geometry paradigm to ensure consistent tracking and high-quality
mapping. We introduce a dense tri-view matching module that aggregates reliable
pairwise correspondences into consistent tri-view matches, forming robust
geometric constraints across frames. For tracking, we propose Hybrid Geometric
Constraints, which leverage tri-view matches to construct complementary
geometric cues alongside photometric loss, ensuring accurate and stable pose
estimation even under drastic viewpoint shifts and lighting variations. For
mapping, we propose a new probabilistic initialization strategy that encodes
geometric uncertainty from tri-view correspondences into newly initialized
Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust
mechanism to mitigate tracking drift caused by mapping latency. Experiments on
multiple public outdoor datasets show that our TVG-SLAM outperforms prior
RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our
method improves tracking robustness, reducing the average Absolute Trajectory
Error (ATE) by 69.0\% while achieving state-of-the-art rendering quality. The
implementation of our method will be released as open-source.

</details>


### [267] [MotionGPT3: Human Motion as a Second Modality](https://arxiv.org/pdf/2506.24086)
*Bingfan Zhu, Biao Jiang, Sunyi Wang, Shixiang Tang, Tao Chen, Linjie Luo, Youyi Zheng, Xin Chen*

Main category: cs.CV

TL;DR: MotionGPT3 is a bimodal motion-language model addressing challenges in unified motion-language understanding and generation by decoupling motion modeling and preserving language intelligence.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between continuous motion and discrete language representation and prevent degradation of language intelligence in unified training.

Method: Uses a mixture of experts approach with separate motion and text branches, a motion VAE for encoding, and a diffusion head for motion prediction.

Result: Achieves competitive performance in motion understanding and generation while maintaining strong language capabilities.

Conclusion: Establishes a unified bimodal motion diffusion framework within an autoregressive manner.

Abstract: Though recent advances in multimodal models have demonstrated strong
capabilities and opportunities in unified understanding and generation, the
development of unified motion-language models remains underexplored. To enable
such models with high-fidelity human motion, two core challenges must be
addressed. The first is the reconstruction gap between the continuous motion
modality and discrete representation in an autoregressive manner, and the
second is the degradation of language intelligence during unified training.
Inspired by the mixture of experts, we propose MotionGPT3, a bimodal
motion-language model that treats human motion as a second modality, decoupling
motion modeling via separate model parameters and enabling both effective
cross-modal interaction and efficient multimodal scaling training. To preserve
language intelligence, the text branch retains the original structure and
parameters of the pretrained language model, while a new motion branch is
integrated via a shared attention mechanism, enabling bidirectional information
flow between two modalities. We first employ a motion Variational Autoencoder
(VAE) to encode raw human motion into latent representations. Based on this
continuous latent space, the motion branch predicts motion latents directly
from intermediate hidden states using a diffusion head, bypassing discrete
tokenization. Extensive experiments show that our approach achieves competitive
performance on both motion understanding and generation tasks while preserving
strong language capabilities, establishing a unified bimodal motion diffusion
framework within an autoregressive manner.

</details>


### [268] [A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans](https://arxiv.org/pdf/2506.23209)
*Chia-Wen Huang, Haw Hwai, Chien-Chang Lee, Pei-Yuan Wu*

Main category: cs.CV

TL;DR: A deep learning model using 3D CT scans and Slice Attention improves appendicitis diagnosis, achieving higher AUC scores.


<details>
  <summary>Details</summary>
Motivation: Timely and accurate appendicitis diagnosis is crucial to prevent complications, but CT imaging overloads radiologists, causing delays.

Method: Proposes a deep learning model with 3D CT scans and Slice Attention, guided by 2D datasets, and a hierarchical framework for classification.

Result: Improves AUC by 3% for appendicitis and 5.9% for complicated appendicitis.

Conclusion: The model offers a more efficient and reliable diagnostic solution compared to prior methods.

Abstract: Timely and accurate diagnosis of appendicitis is critical in clinical
settings to prevent serious complications. While CT imaging remains the
standard diagnostic tool, the growing number of cases can overwhelm
radiologists, potentially causing delays. In this paper, we propose a deep
learning model that leverages 3D CT scans for appendicitis classification,
incorporating Slice Attention mechanisms guided by external 2D datasets to
enhance small lesion detection. Additionally, we introduce a hierarchical
classification framework using pre-trained 2D models to differentiate between
simple and complicated appendicitis. Our approach improves AUC by 3% for
appendicitis and 5.9% for complicated appendicitis, offering a more efficient
and reliable diagnostic solution compared to previous work.

</details>


### [269] [Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/pdf/2506.23270)
*Yi Li, Hualiang Wang, Xinpeng Ding, Haonan Wang, Xiaomeng Li*

Main category: cs.CV

TL;DR: The paper introduces Token Activation Map (TAM), a method to improve the explainability of Multimodal Large Language Models (MLLMs) by addressing redundant activations and enhancing visualization quality.


<details>
  <summary>Details</summary>
Motivation: The explainability of MLLMs is understudied, and redundant activations from earlier tokens interfere with reliable explanations, affecting model credibility and understanding.

Method: Proposes an estimated causal inference method and a rank Gaussian filter to mitigate context interference and reduce activation noises, termed Token Activation Map (TAM).

Result: TAM outperforms existing methods, providing high-quality visualizations for tasks like object localization, failure analysis, and model understanding.

Conclusion: TAM effectively addresses the explainability challenges in MLLMs, offering versatile applications and superior performance over current methods.

Abstract: Multimodal large language models (MLLMs) are broadly empowering various
fields. Despite their advancements, the explainability of MLLMs remains less
explored, hindering deeper understanding, model credibility, and effective
visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that
produce a single output, MLLMs generate sequences of tokens progressively,
where each generated token depends on the previous context. Therefore, earlier
context tokens can introduce redundant activations that interfere with the
explanation of later tokens beyond their original information. Existing studies
often overlook this issue, but our observations reveal that these redundant
correlations can significantly hurt the reliability of explanations. To address
this, we propose an estimated causal inference method to mitigate the
interference of context to achieve high-quality MLLM explanation, with a novel
rank Gaussian filter to further reduce activation noises. We term this method
Token Activation Map (TAM) to highlight the consideration of interactions
between tokens. TAM also indicates that it excels at explaining multiple tokens
of MLLM, which is different from the Class Activation Map (CAM) for a single
prediction. Our TAM method significantly outperforms existing SoTA methods,
showcasing high-quality visualization results that can be utilized for various
scenarios, such as object localization, failure case analysis, video
visualization, MLLMs visual comparison, and model understanding (e.g., color,
shape, action, location, visual reasoning, multi-turn conversation, etc). The
code is available atgithub.com/xmed-lab/TAM.

</details>


### [270] [High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation](https://arxiv.org/pdf/2506.23227)
*Lunhao Duan, Shanshan Zhao, Xingxing Weng, Jing Zhang, Gui-Song Xia*

Main category: cs.CV

TL;DR: The paper proposes a framework for indoor point cloud semantic segmentation using scene-level annotations, improving pseudo-label accuracy via multi-modal information and region-point consistency.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with accurate pseudo-label generation from scene-level annotations, impacting segmentation performance.

Method: The framework uses cross-modal feature guidance and region-point semantic consistency to enhance pseudo-label quality.

Result: Significant improvements on ScanNet v2 and S3DIS datasets, validated by ablation studies.

Conclusion: The method effectively addresses challenges of scene-level annotation, outperforming prior works.

Abstract: This paper investigates indoor point cloud semantic segmentation under
scene-level annotation, which is less explored compared to methods relying on
sparse point-level labels. In the absence of precise point-level labels,
current methods first generate point-level pseudo-labels, which are then used
to train segmentation models. However, generating accurate pseudo-labels for
each point solely based on scene-level annotations poses a considerable
challenge, substantially affecting segmentation performance. Consequently, to
enhance accuracy, this paper proposes a high-quality pseudo-label generation
framework by exploring contemporary multi-modal information and region-point
semantic consistency. Specifically, with a cross-modal feature guidance module,
our method utilizes 2D-3D correspondences to align point cloud features with
corresponding 2D image pixels, thereby assisting point cloud feature learning.
To further alleviate the challenge presented by the scene-level annotation, we
introduce a region-point semantic consistency module. It produces regional
semantics through a region-voting strategy derived from point-level semantics,
which are subsequently employed to guide the point-level semantic predictions.
Leveraging the aforementioned modules, our method can rectify inaccurate
point-level semantic predictions during training and obtain high-quality
pseudo-labels. Significant improvements over previous works on ScanNet v2 and
S3DIS datasets under scene-level annotation can demonstrate the effectiveness.
Additionally, comprehensive ablation studies validate the contributions of our
approach's individual components. The code is available at
https://github.com/LHDuan/WSegPC .

</details>


### [271] [Why Settle for One? Text-to-ImageSet Generation and Evaluation](https://arxiv.org/pdf/2506.23275)
*Chengyou Jia, Xin Shen, Zhuohang Dang, Zhuohang Dang, Changliang Xia, Weijia Wu, Xinyu Zhang, Hangwei Qian, Ivor W. Tsang, Minnan Luo*

Main category: cs.CV

TL;DR: The paper introduces Text-to-ImageSet (T2IS) generation, a challenging problem for creating coherent image sets with diverse consistency requirements. It proposes T2IS-Bench, T2IS-Eval, and AutoT2IS to address this, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for consistent image generation are domain-specific, limiting generalizability. The paper aims to tackle the broader problem of generating image sets with varied consistency needs based on user instructions.

Method: The authors introduce T2IS-Bench for diverse instructions, T2IS-Eval for evaluation, and AutoT2IS, a training-free framework leveraging Diffusion Transformers for consistency.

Result: AutoT2IS outperforms existing methods on T2IS-Bench, addressing diverse consistency challenges and enabling real-world applications.

Conclusion: The proposed framework significantly advances T2IS generation, demonstrating practical value and outperforming specialized approaches.

Abstract: Despite remarkable progress in Text-to-Image models, many real-world
applications require generating coherent image sets with diverse consistency
requirements. Existing consistent methods often focus on a specific domain with
specific aspects of consistency, which significantly constrains their
generalizability to broader applications. In this paper, we propose a more
challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate
sets of images that meet various consistency requirements based on user
instructions. To systematically study this problem, we first introduce
$\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,
providing comprehensive coverage for T2IS generation. Building on this, we
propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user
instructions into multifaceted assessment criteria and employs effective
evaluators to adaptively assess consistency fulfillment between criteria and
generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free
framework that maximally leverages pretrained Diffusion Transformers'
in-context capabilities to harmonize visual elements to satisfy both
image-level prompt alignment and set-level visual consistency. Extensive
experiments on T2IS-Bench reveal that diverse consistency challenges all
existing methods, while our AutoT2IS significantly outperforms current
generalized and even specialized approaches. Our method also demonstrates the
ability to enable numerous underexplored real-world applications, confirming
its substantial practical value. Visit our project in
https://chengyou-jia.github.io/T2IS-Home.

</details>


### [272] [DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object Detection](https://arxiv.org/pdf/2506.23252)
*Kunwei Lv, Ping Lan*

Main category: cs.CV

TL;DR: DGE-YOLO is an enhanced YOLO-based framework for multi-modal UAV object detection, featuring dual-branch architecture, EMA mechanism, and Gather-and-Distribute module, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting small objects in complex aerial scenarios with multi-modal inputs, where existing methods sacrifice performance for speed.

Method: DGE-YOLO introduces a dual-branch architecture for modality-specific feature extraction, an EMA mechanism for multi-scale attention, and a Gather-and-Distribute module to reduce information loss.

Result: Outperforms state-of-the-art methods on the Drone Vehicle dataset, demonstrating effectiveness in multi-modal UAV object detection.

Conclusion: DGE-YOLO effectively addresses the challenges of multi-modal UAV object detection, offering superior performance and robust feature fusion.

Abstract: The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted
the importance of robust and efficient object detection in diverse aerial
scenarios. Detecting small objects under complex conditions, however, remains a
significant challenge. Existing approaches often prioritize inference speed,
leading to degraded performance when handling multi-modal inputs. To address
this, we present DGE-YOLO, an enhanced YOLO-based detection framework designed
to effectively fuse multi-modal information. Specifically, we introduce a
dual-branch architecture for modality-specific feature extraction, enabling the
model to process both infrared and visible images. To further enrich semantic
representation, we propose an Efficient Multi-scale Attention (EMA) mechanism
that enhances feature learning across spatial scales. Additionally, we replace
the conventional neck with a Gather-and-Distribute module to mitigate
information loss during feature aggregation. Extensive experiments on the Drone
Vehicle dataset demonstrate that DGE-YOLO achieves superior performance over
state-of-the-art methods, validating its effectiveness in multi-modal UAV
object detection tasks.

</details>


### [273] [PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation](https://arxiv.org/pdf/2506.23257)
*Chongke Bi, Xin Gao, Baofeng Fu, Yuheng Zhao, Siming Chen, Ying Zhao, Yunhai Wang*

Main category: cs.CV

TL;DR: PCLVis is a framework for analyzing process communication latency (PCL) in large-scale simulations using MPI data, aiding users in optimizing simulations without physical link layer access.


<details>
  <summary>Details</summary>
Motivation: Address scalability issues in supercomputer simulations caused by high communication costs, providing a tool for general users to analyze PCL events without administrative privileges.

Method: Uses MPI process communication data, spatial PCL event locating, process-correlation tree clustering, communication-dependency DAG, sliding window algorithm for event abstraction, and CS-Glyphs for visualization.

Result: Demonstrated effectiveness on TH-1A supercomputer, enabling users to optimize simulations by identifying and addressing PCL events.

Conclusion: PCLVis improves simulation efficiency by providing actionable insights into communication latency, accessible to general users.

Abstract: Large-scale simulations on supercomputers have become important tools for
users. However, their scalability remains a problem due to the huge
communication cost among parallel processes. Most of the existing communication
latency analysis methods rely on the physical link layer information, which is
only available to administrators. In this paper, a framework called PCLVis is
proposed to help general users analyze process communication latency (PCL)
events. Instead of the physical link layer information, the PCLVis uses the MPI
process communication data for the analysis. First, a spatial PCL event
locating method is developed. All processes with high correlation are
classified into a single cluster by constructing a process-correlation tree.
Second, the propagation path of PCL events is analyzed by constructing a
communication-dependency-based directed acyclic graph (DAG), which can help
users interactively explore a PCL event from the temporal evolution of a
located PCL event cluster. In this graph, a sliding window algorithm is
designed to generate the PCL events abstraction. Meanwhile, a new glyph called
the communication state glyph (CS-Glyph) is designed for each process to show
its communication states, including its in/out messages and load balance. Each
leaf node can be further unfolded to view additional information. Third, a PCL
event attribution strategy is formulated to help users optimize their
simulations. The effectiveness of the PCLVis framework is demonstrated by
analyzing the PCL events of several simulations running on the TH-1A
supercomputer. By using the proposed framework, users can greatly improve the
efficiency of their simulations.

</details>


### [274] [Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis](https://arxiv.org/pdf/2506.23263)
*Lei-lei Li, Jianwu Fang, Junbin Xiao, Shanmin Pang, Hongkai Yu, Chen Lv, Jianru Xue, Tat-Seng Chua*

Main category: cs.CV

TL;DR: A novel diffusion model, Causal-VidSyn, is proposed for synthesizing egocentric traffic accident videos by leveraging cause descriptions and driver fixations, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Understanding causes and effects of car accidents is vital for self-driving car safety, but synthesizing realistic accident videos with causal relations is challenging.

Method: Causal-VidSyn uses cause descriptions and driver fixations to identify accident participants and behaviors, aided by accident reason answering and gaze-conditioned selection modules.

Result: The model surpasses existing video diffusion models in frame quality and causal sensitivity for tasks like accident video editing and text-to-video generation.

Conclusion: Causal-VidSyn effectively synthesizes realistic accident videos, supported by the Drive-Gaze dataset, enhancing safety testing for self-driving cars.

Abstract: Egocentricly comprehending the causes and effects of car accidents is crucial
for the safety of self-driving cars, and synthesizing causal-entity reflected
accident videos can facilitate the capability test to respond to unaffordable
accidents in reality. However, incorporating causal relations as seen in
real-world videos into synthetic videos remains challenging. This work argues
that precisely identifying the accident participants and capturing their
related behaviors are of critical importance. In this regard, we propose a
novel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic
accident videos. To enable causal entity grounding in video diffusion,
Causal-VidSyn leverages the cause descriptions and driver fixations to identify
the accident participants and behaviors, facilitated by accident reason
answering and gaze-conditioned selection modules. To support Causal-VidSyn, we
further construct Drive-Gaze, the largest driver gaze dataset (with 1.54M
frames of fixations) in driving accident scenarios. Extensive experiments show
that Causal-VidSyn surpasses state-of-the-art video diffusion models in terms
of frame quality and causal sensitivity in various tasks, including accident
video editing, normal-to-accident video diffusion, and text-to-video
generation.

</details>


### [275] [Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation](https://arxiv.org/pdf/2506.23271)
*Jinxing Zhou, Zhihui Li, Yongqiang Yu, Yanghao Zhou, Ruohao Guo, Guangyao Li, Yuxin Mao, Mingfei Han, Xiaojun Chang, Meng Wang*

Main category: cs.CV

TL;DR: Mettle is a memory-efficient method for adapting large pretrained transformers to audio-visual tasks using Layer-Centric Distillation and Meta-Token Injection.


<details>
  <summary>Details</summary>
Motivation: To adapt large-scale pretrained transformers efficiently for downstream audio-visual tasks while preserving pretrained knowledge and enabling task-specific adaptation.

Method: Uses Layer-Centric Distillation (LCD) to distill audio/visual features into meta-tokens and Meta-Token Injection (MTI) for fine-grained segmentation tasks.

Result: Reduces memory usage and training time while maintaining parameter efficiency and competitive accuracy.

Conclusion: Mettle is effective for adapting transformers to audio-visual tasks with efficiency and accuracy.

Abstract: We present \textbf{Met}a-\textbf{T}oken \textbf{Le}arning (Mettle), a simple
and memory-efficient method for adapting large-scale pretrained transformer
models to downstream audio-visual tasks. Instead of sequentially modifying the
output feature distribution of the transformer backbone, Mettle utilizes a
lightweight \textit{Layer-Centric Distillation (LCD)} module to distill in
parallel the intact audio or visual features embedded by each transformer layer
into compact meta-tokens. This distillation process considers both pretrained
knowledge preservation and task-specific adaptation. The obtained meta-tokens
can be directly applied to classification tasks, such as audio-visual event
localization and audio-visual video parsing. To further support fine-grained
segmentation tasks, such as audio-visual segmentation, we introduce a
\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual
meta-tokens distilled from the top transformer layer to guide feature
adaptation in earlier layers. Extensive experiments on multiple audiovisual
benchmarks demonstrate that our method significantly reduces memory usage and
training time while maintaining parameter efficiency and competitive accuracy.

</details>


### [276] [SIEDD: Shared-Implicit Encoder with Discrete Decoders](https://arxiv.org/pdf/2506.23382)
*Vikram Rangarajan, Shishira Maiya, Max Ehrlich, Abhinav Shrivastava*

Main category: cs.CV

TL;DR: SIEDD accelerates INR video encoding by 20-30X without sacrificing quality or control, using a shared encoder and discrete decoders.


<details>
  <summary>Details</summary>
Motivation: Current INR video compression methods are too slow for practical use, often compromising quality or control.

Method: SIEDD uses a shared encoder for global features and lightweight decoders for frame groups, with aggressive sampling.

Result: Achieves 20-30X faster encoding on HD/4K benchmarks while maintaining quality and compression ratios.

Conclusion: SIEDD makes high-fidelity neural video compression practical for real-world deployment.

Abstract: Implicit Neural Representations (INRs) offer exceptional fidelity for video
compression by learning per-video optimized functions, but their adoption is
crippled by impractically slow encoding times. Existing attempts to accelerate
INR encoding often sacrifice reconstruction quality or crucial coordinate-level
control essential for adaptive streaming and transcoding. We introduce SIEDD
(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that
fundamentally accelerates INR encoding without these compromises. SIEDD first
rapidly trains a shared, coordinate-based encoder on sparse anchor frames to
efficiently capture global, low-frequency video features. This encoder is then
frozen, enabling massively parallel training of lightweight, discrete decoders
for individual frame groups, further expedited by aggressive coordinate-space
sampling. This synergistic design delivers a remarkable 20-30X encoding
speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while
maintaining competitive reconstruction quality and compression ratios.
Critically, SIEDD retains full coordinate-based control, enabling continuous
resolution decoding and eliminating costly transcoding. Our approach
significantly advances the practicality of high-fidelity neural video
compression, demonstrating a scalable and efficient path towards real-world
deployment. Our codebase is available at
https://github.com/VikramRangarajan/SIEDD .

</details>


### [277] [Autoregressive Denoising Score Matching is a Good Video Anomaly Detector](https://arxiv.org/pdf/2506.23282)
*Hanwen Zhang, Congqi Cao, Qinyi Lv, Lingtong Min, Yanning Zhang*

Main category: cs.CV

TL;DR: The paper addresses limitations in likelihood-based video anomaly detection (VAD) by tackling three gaps (scene, motion, appearance) using a noise-conditioned score transformer and a novel autoregressive denoising mechanism.


<details>
  <summary>Details</summary>
Motivation: Likelihood-based VAD methods fail to detect anomalies near learned distributions, necessitating a solution for 'unseen' anomalies.

Method: Proposes a noise-conditioned score transformer, scene-dependent and motion-aware score function, and autoregressive denoising score matching for inference.

Result: Achieves state-of-the-art performance on three VAD benchmarks.

Conclusion: The method effectively addresses gaps in VAD, improving anomaly detection by integrating scene, motion, and appearance considerations.

Abstract: Video anomaly detection (VAD) is an important computer vision problem. Thanks
to the mode coverage capabilities of generative models, the likelihood-based
paradigm is catching growing interest, as it can model normal distribution and
detect out-of-distribution anomalies. However, these likelihood-based methods
are blind to the anomalies located in local modes near the learned
distribution. To handle these ``unseen" anomalies, we dive into three gaps
uniquely existing in VAD regarding scene, motion and appearance. Specifically,
we first build a noise-conditioned score transformer for denoising score
matching. Then, we introduce a scene-dependent and motion-aware score function
by embedding the scene condition of input sequences into our model and
assigning motion weights based on the difference between key frames of input
sequences. Next, to solve the problem of blindness in principle, we integrate
unaffected visual information via a novel autoregressive denoising score
matching mechanism for inference. Through autoregressively injecting
intensifying Gaussian noise into the denoised data and estimating the
corresponding score function, we compare the denoised data with the original
data to get a difference and aggregate it with the score function for an
enhanced appearance perception and accumulate the abnormal context. With all
three gaps considered, we can compute a more comprehensive anomaly indicator.
Experiments on three popular VAD benchmarks demonstrate the state-of-the-art
performance of our method.

</details>


### [278] [Time-variant Image Inpainting via Interactive Distribution Transition Estimation](https://arxiv.org/pdf/2506.23461)
*Yun Xing, Qing Guo, Xiaoguang Li, Yihao Huang, Xiaofeng Cao, Di Lin, Ivor Tsang, Lei Ma*

Main category: cs.CV

TL;DR: The paper introduces Time-vAriant iMage inPainting (TAMP), a task to restore damaged images using time-variant reference images. It proposes the InDiTE-Diff method, combining interactive distribution transition estimation with diffusion models, and validates it on a new dataset, TAMP-Street.


<details>
  <summary>Details</summary>
Motivation: Restoring damaged images with time-variant references is challenging due to content distinctions and potential damages in references. Existing methods fail in such chaotic scenarios.

Method: Proposes InDiTE module for adaptive semantics and InDiTE-Diff, integrating InDiTE with diffusion models for latent cross-reference during sampling.

Result: InDiTE-Diff outperforms SOTA methods on the TAMP-Street dataset under two settings.

Conclusion: The proposed method effectively addresses the TAMP task, demonstrating superior performance over existing approaches.

Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant
iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image
by leveraging the complementary information from a reference image, where both
images captured the same scene but with a significant time gap in between,
i.e., time-variant images. Different from conventional reference-guided image
inpainting, the reference image under TAMP setup presents significant content
distinction to the target image and potentially also suffers from damages. Such
an application frequently happens in our daily lives to restore a damaged image
by referring to another reference image, where there is no guarantee of the
reference image's source and quality. In particular, our study finds that even
state-of-the-art (SOTA) reference-guided image inpainting methods fail to
achieve plausible results due to the chaotic image complementation. To address
such an ill-posed problem, we propose a novel Interactive Distribution
Transition Estimation (InDiTE) module which interactively complements the
time-variant images with adaptive semantics thus facilitate the restoration of
damaged regions. To further boost the performance, we propose our TAMP
solution, namely Interactive Distribution Transition Estimation-driven
Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and
conducts latent cross-reference during sampling. Moreover, considering the lack
of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,
based on existing image and mask datasets. We conduct experiments on the
TAMP-Street datasets under two different time-variant image inpainting
settings, which show our method consistently outperform SOTA reference-guided
image inpainting methods for solving TAMP.

</details>


### [279] [MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition](https://arxiv.org/pdf/2506.23283)
*Yuhuan Yang, Chaofan Ma, Zhenjie Mao, Jiangchao Yao, Ya Zhang, Yanfeng Wang*

Main category: cs.CV

TL;DR: MoMa is an efficient adapter framework for video understanding that integrates Mamba's selective state space modeling into image foundation models (IFMs) for full spatial-temporal modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adapting IFMs to video often process spatial and temporal information separately, missing the full intricacy of video dynamics.

Method: Proposes MoMa with SeqMod operation to inject spatial-temporal information into IFMs and a Divide-and-Modulate architecture.

Result: Achieves superior performance on video benchmarks with reduced computational cost.

Conclusion: MoMa effectively enhances video understanding by integrating spatial-temporal modeling efficiently.

Abstract: Video understanding is a complex challenge that requires effective modeling
of spatial-temporal dynamics. With the success of image foundation models
(IFMs) in image understanding, recent approaches have explored
parameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most
of these methods tend to process spatial and temporal information separately,
which may fail to capture the full intricacy of video dynamics. In this paper,
we propose MoMa, an efficient adapter framework that achieves full
spatial-temporal modeling by integrating Mamba's selective state space modeling
into IFMs. We propose a novel SeqMod operation to inject spatial-temporal
information into pre-trained IFMs, without disrupting their original features.
By incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances
video understanding while maintaining computational efficiency. Extensive
experiments on multiple video benchmarks demonstrate the effectiveness of MoMa,
achieving superior performance with reduced computational cost.

</details>


### [280] [Sanitizing Manufacturing Dataset Labels Using Vision-Language Models](https://arxiv.org/pdf/2506.23465)
*Nazanin Mahjourian, Vinh Nguyen*

Main category: cs.CV

TL;DR: The paper introduces VLSR, a vision-language framework for sanitizing and refining noisy labels in manufacturing image datasets, improving dataset quality for machine learning.


<details>
  <summary>Details</summary>
Motivation: Large-scale datasets, especially in manufacturing, often have label noise and inconsistencies, making high-quality labels costly and time-consuming to obtain.

Method: VLSR uses CLIP to embed images and labels into a shared semantic space, then performs label sanitization via cosine similarity and clusters similar labels using density-based clustering.

Result: VLSR effectively identifies problematic labels, improves consistency, and reduces label vocabulary, enhancing dataset quality.

Conclusion: VLSR minimizes human intervention and improves dataset quality for robust machine learning in industrial applications.

Abstract: The success of machine learning models in industrial applications is heavily
dependent on the quality of the datasets used to train the models. However,
large-scale datasets, specially those constructed from crowd-sourcing and
web-scraping, often suffer from label noise, inconsistencies, and errors. This
problem is particularly pronounced in manufacturing domains, where obtaining
high-quality labels is costly and time-consuming. This paper introduces
Vision-Language Sanitization and Refinement (VLSR), which is a
vision-language-based framework for label sanitization and refinement in
multi-label manufacturing image datasets. This method embeds both images and
their associated textual labels into a shared semantic space leveraging the
CLIP vision-language model. Then two key tasks are addressed in this process by
computing the cosine similarity between embeddings. First, label sanitization
is performed to identify irrelevant, misspelled, or semantically weak labels,
and surface the most semantically aligned label for each image by comparing
image-label pairs using cosine similarity between image and label embeddings.
Second, the method applies density-based clustering on text embeddings,
followed by iterative cluster merging, to group semantically similar labels
into unified label groups. The Factorynet dataset, which includes noisy labels
from both human annotations and web-scraped sources, is employed to evaluate
the effectiveness of the proposed framework. Experimental results demonstrate
that the VLSR framework successfully identifies problematic labels and improves
label consistency. This method enables a significant reduction in label
vocabulary through clustering, which ultimately enhances the dataset's quality
for training robust machine learning models in industrial applications with
minimal human intervention.

</details>


### [281] [Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification](https://arxiv.org/pdf/2506.23285)
*Daqian Shi, Xiaolei Diao, Xu Chen, Cédric M. John*

Main category: cs.CV

TL;DR: The paper proposes a competitive distillation strategy for DNN training, where networks compete and dynamically act as teachers, improving learning performance through competition and stochastic perturbation.


<details>
  <summary>Details</summary>
Motivation: Current distillation methods like mutual learning and self-distillation have limited improvements due to unclear learning directions across iterations. The paper aims to enhance training by introducing competition among networks.

Method: A competitive distillation strategy is introduced, where networks dynamically act as teachers based on performance. Competitive optimization and stochastic perturbation are used to improve parameter updates and visual representations.

Result: Experiments show competitive distillation achieves strong performance across various tasks and datasets.

Conclusion: The proposed competitive distillation strategy effectively enhances DNN training by leveraging competition and stochastic perturbation, outperforming existing methods.

Abstract: Deep Neural Networks (DNNs) have significantly advanced the field of computer
vision. To improve DNN training process, knowledge distillation methods
demonstrate their effectiveness in accelerating network training by introducing
a fixed learning direction from the teacher network to student networks. In
this context, several distillation-based optimization strategies are proposed,
e.g., deep mutual learning and self-distillation, as an attempt to achieve
generic training performance enhancement through the cooperative training of
multiple networks. However, such strategies achieve limited improvements due to
the poor understanding of the impact of learning directions among networks
across different iterations. In this paper, we propose a novel competitive
distillation strategy that allows each network in a group to potentially act as
a teacher based on its performance, enhancing the overall learning performance.
Competitive distillation organizes a group of networks to perform a shared task
and engage in competition, where competitive optimization is proposed to
improve the parameter updating process. We further introduce stochastic
perturbation in competitive distillation, aiming to motivate networks to induce
mutations to achieve better visual representations and global optimum. The
experimental results show that competitive distillation achieves promising
performance in diverse tasks and datasets.

</details>


### [282] [Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding](https://arxiv.org/pdf/2506.23491)
*ZongHan Hsieh, Tzer-Jen Wei*

Main category: cs.CV

TL;DR: Qwen-GUI-3B is a lightweight Vision-Language Model for GUI grounding tasks, achieving competitive performance with larger models while being trainable on a single GPU. Key innovations include cross-platform datasets, two-stage fine-tuning, and data curation strategies.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of large-scale VLMs for consumer-grade hardware by developing a lightweight yet accurate model for GUI grounding tasks.

Method: Combines cross-platform datasets, employs a two-stage fine-tuning strategy (initial cross-platform training followed by specialized fine-tuning), and uses data curation to reduce redundancy.

Result: Achieves 84.9% on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B parameters.

Conclusion: Qwen-GUI-3B demonstrates that lightweight models can achieve high accuracy in GUI grounding through innovative data and training strategies.

Abstract: This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)
specifically designed for Graphical User Interface grounding tasks, achieving
performance competitive with significantly larger models. Unlike large-scale
VLMs (>7B parameters) that are computationally intensive and impractical for
consumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while
being fully trainable on a single GPU (RTX 4090). The model incorporates
several key innovations: (i) combine cross-platform, multi-resolution dataset
of 24K examples from diverse sources including mobile, desktop, and web GUI
screenshots to effectively address data scarcity in high-resolution desktop
environments; (ii) a two-stage fine-tuning strategy, where initial
cross-platform training establishes robust GUI understanding, followed by
specialized fine-tuning on high-resolution data to significantly enhance model
adaptability; and (iii) data curation and redundancy reduction strategies,
demonstrating that randomly sampling a smaller subset with reduced redundancy
achieves performance comparable to larger datasets, emphasizing data diversity
over sheer volume. Empirical evaluation on standard GUI grounding
benchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging
ScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%
on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B
parameters. Ablation studies validate the critical role of balanced sampling
and two-stage fine-tuning in enhancing robustness, particularly in
high-resolution desktop scenarios. The Qwen-GUI-3B is available at:
https://github.com/Han1018/Qwen-GUI-3B

</details>


### [283] [DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios](https://arxiv.org/pdf/2506.23292)
*Changtao Miao, Yi Zhang, Weize Gao, Man Luo, Weiwei Feng, Zhiya Tan, Jianshu Li, Ajian Liu, Yunfeng Diao, Qi Chu, Tao Gong, Zhe Li, Weibin Yao, Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces a large-scale deepfake detection and localization (DDL) dataset to address the lack of interpretability and diversity in existing deepfake detection methods and datasets.


<details>
  <summary>Details</summary>
Motivation: The misuse of deepfake content necessitates reliable detection methods with interpretability, especially in critical domains like law. Existing datasets lack diversity and localization annotations, limiting practical effectiveness.

Method: The authors construct the DDL dataset with 1.8M forged samples and 75 deepfake methods, featuring diverse scenarios, comprehensive methods, varied manipulation modes, and fine-grained annotations.

Result: The DDL dataset provides a challenging benchmark for real-world forgeries and supports advanced detection, localization, and interpretability methods.

Conclusion: The DDL dataset addresses limitations of current datasets and enhances the development of next-generation deepfake detection and interpretability tools.

Abstract: Recent advances in AIGC have exacerbated the misuse of malicious deepfake
content, making the development of reliable deepfake detection methods an
essential means to address this challenge. Although existing deepfake detection
models demonstrate outstanding performance in detection metrics, most methods
only provide simple binary classification results, lacking interpretability. In
critical domains such as law, interpretability is crucial for enhancing the
credibility and authority of decisions. Recent studies attempt to improve the
interpretability of classification results by providing spatial manipulation
masks or temporal forgery segments. However, the practical effectiveness of
these methods remains suboptimal due to limitations of the forgery data. Most
current deepfake datasets predominantly offer binary labels, only a few
datasets with localization annotations. However, they suffer from restricted
forgery scenarios, limited diversity in deepfake types, and insufficient data
scale, making them inadequate for complex real-world scenarios. To address this
predicament, we construct a novel large-scale deepfake detection and
localization ($\textbf{DDL}$) dataset containing over $\textbf{1.8M}$ forged
samples and encompassing up to $\textbf{75}$ distinct deepfake methods. The DDL
design incorporates four key innovations: (1) $\textbf{Diverse Forgery
Scenarios}$, (2) $\textbf{Comprehensive Deepfake Methods}$, (3) $\textbf{Varied
Manipulation Modes}$, and (4) $\textbf{Fine-grained Forgery Annotations}$.
Through these improvements, our DDL not only provides a more challenging
benchmark for complex real-world forgeries, but also offers crucial support for
building next-generation deepfake detection, localization, and interpretability
methods. The DDL dataset project page is on
https://deepfake-workshop-ijcai2025.github.io/main/index.html.

</details>


### [284] [Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound](https://arxiv.org/pdf/2506.23538)
*Yuhao Huang, Yueyue Xu, Haoran Dou, Jiaxiao Deng, Xin Yang, Hongyu Zheng, Dong Ni*

Main category: cs.CV

TL;DR: An intelligent system for automated plane localization and CUA diagnosis using 3D ultrasound, combining denoising diffusion models, reinforcement learning, and text-driven uncertainty modeling.


<details>
  <summary>Details</summary>
Motivation: CUAs cause infertility and pregnancy complications; 3D US improves visualization over 2D US for accurate diagnosis.

Method: 1) Denoising diffusion model with local/global guidance. 2) Reinforcement learning for key slice extraction. 3) Text-driven uncertainty modeling for prediction refinement.

Result: Effective plane localization and CUA diagnosis demonstrated on a large 3D uterine US dataset.

Conclusion: The proposed system enhances accuracy and efficiency in diagnosing CUAs using 3D US.

Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,
preterm birth, and an increased risk of pregnancy complications. Compared to
traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,
providing a clear visualization of the uterine morphology for assessing CUAs
accurately. In this paper, we propose an intelligent system for simultaneous
automated plane localization and CUA diagnosis. Our highlights are: 1) we
develop a denoising diffusion model with local (plane) and global (volume/text)
guidance, using an adaptive weighting strategy to optimize attention allocation
to different conditions; 2) we introduce a reinforcement learning-based
framework with unsupervised rewards to extract the key slice summary from
redundant sequences, fully integrating information across multiple planes to
reduce learning difficulty; 3) we provide text-driven uncertainty modeling for
coarse prediction, and leverage it to adjust the classification probability for
overall performance improvement. Extensive experiments on a large 3D uterine US
dataset show the efficacy of our method, in terms of plane localization and CUA
diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.

</details>


### [285] [DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On](https://arxiv.org/pdf/2506.23295)
*Xiang Xu*

Main category: cs.CV

TL;DR: DiffFit is a two-stage latent diffusion framework for high-fidelity virtual try-on, addressing challenges like garment detail preservation and alignment.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods struggle with garment detail preservation, alignment, efficiency, and generalization.

Method: DiffFit uses a two-stage approach: geometry-aware garment warping followed by texture refinement via cross-modal diffusion.

Result: DiffFit outperforms state-of-the-art methods in preserving garment details and alignment, validated by benchmarks.

Conclusion: DiffFit effectively decouples geometric and appearance tasks, enhancing realism and stability in virtual try-on.

Abstract: Virtual try-on (VTON) aims to synthesize realistic images of a person wearing
a target garment, with broad applications in e-commerce and digital fashion.
While recent advances in latent diffusion models have substantially improved
visual quality, existing approaches still struggle with preserving fine-grained
garment details, achieving precise garment-body alignment, maintaining
inference efficiency, and generalizing to diverse poses and clothing styles. To
address these challenges, we propose DiffFit, a novel two-stage latent
diffusion framework for high-fidelity virtual try-on. DiffFit adopts a
progressive generation strategy: the first stage performs geometry-aware
garment warping, aligning the garment with the target body through fine-grained
deformation and pose adaptation. The second stage refines texture fidelity via
a cross-modal conditional diffusion model that integrates the warped garment,
the original garment appearance, and the target person image for high-quality
rendering. By decoupling geometric alignment and appearance refinement, DiffFit
effectively reduces task complexity and enhances both generation stability and
visual realism. It excels in preserving garment-specific attributes such as
textures, wrinkles, and lighting, while ensuring accurate alignment with the
human body. Extensive experiments on large-scale VTON benchmarks demonstrate
that DiffFit achieves superior performance over existing state-of-the-art
methods in both quantitative metrics and perceptual evaluations.

</details>


### [286] [Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting](https://arxiv.org/pdf/2506.23308)
*Yiming Huang, Long Bai, Beilei Cui, Yanheng Li, Tong Chen, Jie Wang, Jinlin Wu, Zhen Lei, Hongbin Liu, Hongliang Ren*

Main category: cs.CV

TL;DR: Endo-4DGX improves 3D Gaussian Splatting for endoscopic scenes with uneven lighting, using illumination embeddings and adaptive modules to enhance rendering quality under extreme light conditions.


<details>
  <summary>Details</summary>
Motivation: Accurate soft tissue reconstruction is vital for robotic surgery, but current 3DGS methods fail under varying illumination (low light or over-exposure).

Method: Endo-4DGX introduces illumination-adaptive Gaussian Splatting with illumination embeddings, region-aware enhancement, and spatial-aware adjustment modules, plus an exposure control loss.

Result: Endo-4DGX outperforms state-of-the-art methods in challenging lighting, maintaining geometric accuracy and improving rendering quality.

Conclusion: Endo-4DGX advances robotic surgery by enabling high-quality reconstructions in uneven lighting, with potential for broader surgical applications.

Abstract: Accurate reconstruction of soft tissue is crucial for advancing automation in
image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)
techniques and their variants, 4DGS, achieve high-quality renderings of dynamic
surgical scenes in real-time. However, 3D-GS-based methods still struggle in
scenarios with varying illumination, such as low light and over-exposure.
Training 3D-GS in such extreme light conditions leads to severe optimization
problems and devastating rendering quality. To address these challenges, we
present Endo-4DGX, a novel reconstruction method with illumination-adaptive
Gaussian Splatting designed specifically for endoscopic scenes with uneven
lighting. By incorporating illumination embeddings, our method effectively
models view-dependent brightness variations. We introduce a region-aware
enhancement module to model the sub-area lightness at the Gaussian level and a
spatial-aware adjustment module to learn the view-consistent brightness
adjustment. With the illumination adaptive design, Endo-4DGX achieves superior
rendering performance under both low-light and over-exposure conditions while
maintaining geometric accuracy. Additionally, we employ an exposure control
loss to restore the appearance from adverse exposure to the normal level for
illumination-adaptive optimization. Experimental results demonstrate that
Endo-4DGX significantly outperforms combinations of state-of-the-art
reconstruction and restoration methods in challenging lighting environments,
underscoring its potential to advance robot-assisted surgical applications. Our
code is available at https://github.com/lastbasket/Endo-4DGX.

</details>


### [287] [FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method](https://arxiv.org/pdf/2506.23323)
*Quang-Huy Che, Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: FastSeg is a training-free framework for open-vocabulary semantic segmentation, using a pretrained diffusion model with minimal steps and innovative components for improved quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between fine spatial precision and segmentation quality in existing methods, FastSeg aims to bridge this gap efficiently.

Method: Leverages a (1+1)-step reverse process of a pretrained diffusion model, dual-prompt mechanism, Hierarchical Attention Refinement Method (HARD), and Test-Time Flipping (TTF) for all-class segmentation.

Result: Achieves 43.8% average mIoU on PASCAL VOC, PASCAL Context, and COCO Object benchmarks with superior efficiency.

Conclusion: FastSeg sets a strong foundation for extendability, balancing segmentation quality and inference efficiency.

Abstract: Open-vocabulary semantic segmentation (OVSS) aims to segment objects from
arbitrary text categories without requiring densely annotated datasets.
Although contrastive learning based models enable zero-shot segmentation, they
often lose fine spatial precision at pixel level, due to global representation
bias. In contrast, diffusion-based models naturally encode fine-grained spatial
features via attention mechanisms that capture both global context and local
details. However, they often face challenges in balancing the number of
iterations with the quality of the segmentation. In this work, we propose
FastSeg, a novel and efficient training-free framework with only (1+1)-step of
reverse process of a pretrained diffusion model (e.g., Stable Diffusion).
Moreover, instead of running multiple times for different classes, FastSeg
performs segmentation for all classes at once. To further enhance the
segmentation quality, FastSeg introduces three key components: (i) a
dual-prompt mechanism for discriminative, class-aware attention extraction,
(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused
cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time
Flipping (TTF) scheme designed to improve spatial consistency. Extensive
experiments show that FastSeg achieves state-of-the-art training-free
performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,
and COCO Object benchmarks while maintaining superior inference efficiency. Our
results demonstrate that FastSeg provides a strong foundation for
extendability, bridging the gap between segmentation quality and inference
efficiency.

</details>


### [288] [PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection](https://arxiv.org/pdf/2506.23581)
*Xiao Li, Yiming Zhu, Yifan Huang, Wei Zhang, Yingzhe He, Jie Shi, Xiaolin Hu*

Main category: cs.CV

TL;DR: The paper proposes PBCAT, a novel adversarial training method to defend against various physically realizable attacks on object detectors, improving robustness significantly.


<details>
  <summary>Details</summary>
Motivation: Object detectors are vulnerable to physically realizable attacks like adversarial patches and textures, but existing adversarial training methods are limited in scope.

Method: PBCAT combines small-area gradient-guided adversarial patches and imperceptible global perturbations for unified defense.

Result: PBCAT improved detection accuracy by 29.7% under adversarial texture attacks, outperforming state-of-the-art defenses.

Conclusion: PBCAT effectively defends against diverse physically realizable attacks, enhancing object detector robustness.

Abstract: Object detection plays a crucial role in many security-sensitive
applications. However, several recent studies have shown that object detectors
can be easily fooled by physically realizable attacks, \eg, adversarial patches
and recent adversarial textures, which pose realistic and urgent threats.
Adversarial Training (AT) has been recognized as the most effective defense
against adversarial attacks. While AT has been extensively studied in the
$l_\infty$ attack settings on classification models, AT against physically
realizable attacks on object detectors has received limited exploration. Early
attempts are only performed to defend against adversarial patches, leaving AT
against a wider range of physically realizable attacks under-explored. In this
work, we consider defending against various physically realizable attacks with
a unified AT method. We propose PBCAT, a novel Patch-Based Composite
Adversarial Training strategy. PBCAT optimizes the model by incorporating the
combination of small-area gradient-guided adversarial patches and imperceptible
global adversarial perturbations covering the entire image. With these designs,
PBCAT has the potential to defend against not only adversarial patches but also
unseen physically realizable attacks such as adversarial textures. Extensive
experiments in multiple settings demonstrated that PBCAT significantly improved
robustness against various physically realizable attacks over state-of-the-art
defense methods. Notably, it improved the detection accuracy by 29.7\% over
previous defense methods under one recent adversarial texture attack.

</details>


### [289] [IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering](https://arxiv.org/pdf/2506.23329)
*Parker Liu, Chenxin Li, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, Brandon Y. Feng*

Main category: cs.CV

TL;DR: IR3D-Bench is a benchmark challenging VLMs to demonstrate scene understanding through active 3D recreation, moving beyond passive recognition.


<details>
  <summary>Details</summary>
Motivation: To assess if VLMs truly understand scenes by testing their ability to actively recreate 3D structures from images, rather than just describe them.

Method: Tasks VLAs with using programming and rendering tools to recreate 3D structures from input images, grounded in the analysis-by-synthesis paradigm.

Result: Initial experiments reveal limitations in visual precision, not tool usage, among state-of-the-art VLMs.

Conclusion: IR3D-Bench aims to advance tool-using VLAs for genuine scene understanding through creation, with released data and metrics for further study.

Abstract: Vision-language models (VLMs) excel at descriptive tasks, but whether they
truly understand scenes from visual observations remains uncertain. We
introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding
through active creation rather than passive recognition. Grounded in the
analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)
with actively using programming and rendering tools to recreate the underlying
3D structure of an input image, achieving agentic inverse rendering through
tool use. This "understanding-by-creating" approach probes the tool-using
generative capacity of VLAs, moving beyond the descriptive or conversational
capacity measured by traditional scene understanding benchmarks. We provide a
comprehensive suite of metrics to evaluate geometric accuracy, spatial
relations, appearance attributes, and overall plausibility. Initial experiments
on agentic inverse rendering powered by various state-of-the-art VLMs highlight
current limitations, particularly in visual precision rather than basic tool
usage. IR3D-Bench, including data and evaluation protocols, is released to
facilitate systematic study and development of tool-using VLAs towards genuine
scene understanding by creating.

</details>


### [290] [CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation](https://arxiv.org/pdf/2506.23347)
*Yi Liu, Shengqian Li, Zuzeng Lin, Feng Wang, Si Liu*

Main category: cs.CV

TL;DR: CycleVAR introduces Softmax Relaxed Quantization and reformulates image translation as autoregressive generation, outperforming state-of-the-art models like CycleGAN-Turbo.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional Vector Quantization in unsupervised image translation by preserving gradient flow.

Method: Uses Softmax Relaxed Quantization for continuous codebook selection and CycleVAR for autoregressive generation with multi-scale prompts.

Result: Parallel one-step generation achieves better quality and speed than serial multi-step; CycleVAR outperforms CycleGAN-Turbo.

Conclusion: CycleVAR advances unsupervised image translation with improved gradient flow and generation efficiency.

Abstract: The current conditional autoregressive image generation methods have shown
promising results, yet their potential remains largely unexplored in the
practical unsupervised image translation domain, which operates without
explicit cross-domain correspondences. A critical limitation stems from the
discrete quantization inherent in traditional Vector Quantization-based
frameworks, which disrupts gradient flow between the Variational Autoencoder
decoder and causal Transformer, impeding end-to-end optimization during
adversarial training in image space. To tackle this issue, we propose using
Softmax Relaxed Quantization, a novel approach that reformulates codebook
selection as a continuous probability mixing process via Softmax, thereby
preserving gradient propagation. Building upon this differentiable foundation,
we introduce CycleVAR, which reformulates image-to-image translation as
image-conditional visual autoregressive generation by injecting multi-scale
source image tokens as contextual prompts, analogous to prefix-based
conditioning in language models. CycleVAR exploits two modes to generate the
target image tokens, including (1) serial multi-step generation, enabling
iterative refinement across scales, and (2) parallel one-step generation
synthesizing all resolution outputs in a single forward pass. Experimental
findings indicate that the parallel one-step generation mode attains superior
translation quality with quicker inference speed than the serial multi-step
mode in unsupervised scenarios. Furthermore, both quantitative and qualitative
results indicate that CycleVAR surpasses previous state-of-the-art unsupervised
image translation models, \textit{e}.\textit{g}., CycleGAN-Turbo.

</details>


### [291] [AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval](https://arxiv.org/pdf/2506.23605)
*Suyash Maniyar, Vishvesh Trivedi, Ajoy Mondal, Anand Mishra, C. V. Jawahar*

Main category: cs.CV

TL;DR: Proposes SynLecSlideGen, an LLM-guided pipeline for generating synthetic lecture slides, and shows its effectiveness in improving few-shot transfer learning performance on real data.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of lecture slides for training models is labor-intensive and requires expertise. Synthetic data can address this limitation.

Method: Developed SynLecSlideGen, a pipeline for generating synthetic slides, and created RealSlide, a benchmark of annotated real slides. Evaluated via few-shot transfer learning.

Result: Pretraining on synthetic slides significantly boosts performance compared to training only on real data.

Conclusion: Synthetic data effectively compensates for limited labeled lecture slides, enhancing model performance.

Abstract: Lecture slide element detection and retrieval are key problems in slide
understanding. Training effective models for these tasks often depends on
extensive manual annotation. However, annotating large volumes of lecture
slides for supervised training is labor intensive and requires domain
expertise. To address this, we propose a large language model (LLM)-guided
synthetic lecture slide generation pipeline, SynLecSlideGen, which produces
high-quality, coherent and realistic slides. We also create an evaluation
benchmark, namely RealSlide by manually annotating 1,050 real lecture slides.
To assess the utility of our synthetic slides, we perform few-shot transfer
learning on real data using models pre-trained on them. Experimental results
show that few-shot transfer learning with pretraining on synthetic slides
significantly improves performance compared to training only on real data. This
demonstrates that synthetic data can effectively compensate for limited labeled
lecture slides. The code and resources of our work are publicly available on
our project website: https://synslidegen.github.io/.

</details>


### [292] [GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields](https://arxiv.org/pdf/2506.23352)
*Shunsuke Yasuki, Taiki Miyanishi, Nakamasa Inoue, Shuhei Kurita, Koya Sakamoto, Daichi Azuma, Masato Taki, Yutaka Matsuo*

Main category: cs.CV

TL;DR: GeoProg3D is a visual programming framework for natural language-driven interactions with city-scale 3D scenes, combining a geography-aware 3D language field and specialized vision APIs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D language approaches lack scalability and compositional reasoning for large urban settings.

Method: GeoProg3D uses a hierarchical 3D model (GCLF) and geographic vision APIs (GV-APIs) with LLMs for dynamic reasoning.

Result: Outperforms existing 3D language fields and vision-language models on the GeoEval3D benchmark.

Conclusion: GeoProg3D enables scalable, compositional geographic reasoning in city-scale 3D environments via natural language.

Abstract: The advancement of 3D language fields has enabled intuitive interactions with
3D scenes via natural language. However, existing approaches are typically
limited to small-scale environments, lacking the scalability and compositional
reasoning capabilities necessary for large, complex urban settings. To overcome
these limitations, we propose GeoProg3D, a visual programming framework that
enables natural language-driven interactions with city-scale high-fidelity 3D
scenes. GeoProg3D consists of two key components: (i) a Geography-aware
City-scale 3D Language Field (GCLF) that leverages a memory-efficient
hierarchical 3D model to handle large-scale data, integrated with geographic
information for efficiently filtering vast urban spaces using directional cues,
distance measurements, elevation data, and landmark references; and (ii)
Geographical Vision APIs (GV-APIs), specialized geographic vision tools such as
area segmentation and object detection. Our framework employs large language
models (LLMs) as reasoning engines to dynamically combine GV-APIs and operate
GCLF, effectively supporting diverse geographic vision tasks. To assess
performance in city-scale reasoning, we introduce GeoEval3D, a comprehensive
benchmark dataset containing 952 query-answer pairs across five challenging
tasks: grounding, spatial reasoning, comparison, counting, and measurement.
Experiments demonstrate that GeoProg3D significantly outperforms existing 3D
language fields and vision-language models across multiple tasks. To our
knowledge, GeoProg3D is the first framework enabling compositional geographic
reasoning in high-fidelity city-scale 3D environments via natural language. The
code is available at https://snskysk.github.io/GeoProg3D/.

</details>


### [293] [OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions](https://arxiv.org/pdf/2506.23361)
*Yuanhao Cai, He Zhang, Xi Chen, Jinbo Xing, Yiwei Hu, Yuqian Zhou, Kai Zhang, Zhifei Zhang, Soo Ye Kim, Tianyu Wang, Yulun Zhang, Xiaokang Yang, Zhe Lin, Alan Yuille*

Main category: cs.CV

TL;DR: The paper introduces a method for multi-subject video customization, addressing data construction and control signal challenges, and proposes a diffusion Transformer framework for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on single-subject scenarios and lack exploration of control signals for video editing. The paper aims to tackle these gaps.

Method: Proposes VideoCus-Factory for multi-subject data construction and an Image-Video Transfer Mixed (IVTM) training approach. Introduces OmniVCus, a diffusion Transformer framework with Lottery Embedding (LE) and Temporally Aligned Embedding (TAE).

Result: The method outperforms state-of-the-art in quantitative and qualitative evaluations.

Conclusion: The proposed framework effectively addresses multi-subject customization and control signal utilization, demonstrating superior performance.

Abstract: Existing feedforward subject-driven video customization methods mainly study
single-subject scenarios due to the difficulty of constructing multi-subject
training data pairs. Another challenging problem that how to use the signals
such as depth, mask, camera, and text prompts to control and edit the subject
in the customized video is still less explored. In this paper, we first propose
a data construction pipeline, VideoCus-Factory, to produce training data pairs
for multi-subject customization from raw videos without labels and control
signals such as depth-to-video and mask-to-video pairs. Based on our
constructed data, we develop an Image-Video Transfer Mixed (IVTM) training with
image editing data to enable instructive editing for the subject in the
customized video. Then we propose a diffusion Transformer framework, OmniVCus,
with two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned
Embedding (TAE). LE enables inference with more subjects by using the training
subjects to activate more frame embeddings. TAE encourages the generation
process to extract guidance from temporally aligned control signals by
assigning the same frame embeddings to the control and noise tokens.
Experiments demonstrate that our method significantly surpasses
state-of-the-art methods in both quantitative and qualitative evaluations.
Video demos are at our project page:
https://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released
at https://github.com/caiyuanhao1998/Open-OmniVCus

</details>


### [294] [A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video](https://arxiv.org/pdf/2506.23414)
*Ming-Zher Poh, Jonathan Wang, Jonathan Hsu, Lawrence Cai, Eric Teasley, James A. Taylor, Jameson K. Rogers, Anupam Pathak, Shwetak Patel*

Main category: cs.CV

TL;DR: A high-throughput bench-testing platform for evaluating smartphone-based heart rate monitoring apps, addressing device variability and fragmentation with synthetic PPG test videos and parallel testing.


<details>
  <summary>Details</summary>
Motivation: Challenges in performance evaluation and device compatibility for smartphone-based HR monitoring apps due to device variability and lack of standardized testing methods.

Method: Designed a system with a test rig for 12 smartphones, synthetic PPG test videos with controllable HR and signal quality, and a host machine for coordination.

Result: Achieved 0.11% MAPE for HR accuracy and 0.92 correlation for PPG signals, with all 20 tested smartphones meeting ANSI/CTA standards.

Conclusion: The platform provides a scalable solution for pre-deployment testing, improving app performance and device compatibility in mobile health.

Abstract: Smartphone-based heart rate (HR) monitoring apps using finger-over-camera
photoplethysmography (PPG) face significant challenges in performance
evaluation and device compatibility due to device variability and
fragmentation. Manual testing is impractical, and standardized methods are
lacking. This paper presents a novel, high-throughput bench-testing platform to
address this critical need. We designed a system comprising a test rig capable
of holding 12 smartphones for parallel testing, a method for generating
synthetic PPG test videos with controllable HR and signal quality, and a host
machine for coordinating video playback and data logging. The system achieved a
mean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and
measured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and
measured PPG signals using a clinically-validated smartphone-based HR app.
Bench-testing results of 20 different smartphone models correctly classified
all the devices as meeting the ANSI/CTA accuracy standards for HR monitors
(MAPE <10%) when compared to a prospective clinical study with 80 participants,
demonstrating high positive predictive value. This platform offers a scalable
solution for pre-deployment testing of smartphone HR apps to improve app
performance, ensure device compatibility, and advance the field of mobile
health.

</details>


### [295] [Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models](https://arxiv.org/pdf/2506.23418)
*Parham Rezaei, Arash Marioriyad, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: The paper addresses text-to-image models' challenges in compositional generation, proposing a probabilistic framework (PoS) for spatial alignment and introducing PSE (evaluation metric) and PSG (generation method) to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models often fail to accurately represent spatial relationships in prompts, leading to misaligned compositions.

Method: Proposes PoS-based Evaluation (PSE) for assessing spatial alignment and PoS-based Generation (PSG) for improving generation without fine-tuning, using gradient-based or search-based strategies.

Result: PSE aligns better with human judgment than traditional metrics, and PSG outperforms state-of-the-art methods in generating spatially accurate images.

Conclusion: The proposed PSE and PSG significantly enhance text-to-image models' ability to generate images with precise spatial configurations.

Abstract: Despite the ability of text-to-image models to generate high-quality,
realistic, and diverse images, they face challenges in compositional
generation, often struggling to accurately represent details specified in the
input prompt. A prevalent issue in compositional generation is the misalignment
of spatial relationships, as models often fail to faithfully generate images
that reflect the spatial configurations specified between objects in the input
prompts. To address this challenge, we propose a novel probabilistic framework
for modeling the relative spatial positioning of objects in a scene, leveraging
the concept of Probability of Superiority (PoS). Building on this insight, we
make two key contributions. First, we introduce a novel evaluation metric,
PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D
spatial relationships between text and image, with improved adherence to human
judgment. Second, we propose PoS-based Generation (PSG), an inference-time
method that improves the alignment of 2D and 3D spatial relationships in T2I
models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based
reward function that can be utilized in two distinct ways: (1) as a
gradient-based guidance mechanism applied to the cross-attention maps during
the denoising steps, or (2) as a search-based strategy that evaluates a set of
initial noise vectors to select the best one. Extensive experiments demonstrate
that the PSE metric exhibits stronger alignment with human judgment compared to
traditional center-based metrics, providing a more nuanced and reliable measure
of complex spatial relationship accuracy in text-image alignment. Furthermore,
PSG significantly enhances the ability of text-to-image models to generate
images with specified spatial configurations, outperforming state-of-the-art
methods across multiple evaluation metrics and benchmarks.

</details>


### [296] [Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/pdf/2506.23639)
*Wanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, Sipeng Zheng, Zongqing Lu*

Main category: cs.CV

TL;DR: A framework for multimodal understanding using byte-pair encoding on visual tokens, improving cross-modal alignment and performance in vision-language tasks.


<details>
  <summary>Details</summary>
Motivation: Aligning different modalities in MLLMs is challenging; the paper aims to unify multimodal understanding by adapting text-like tokenization for visual data.

Method: Uses byte-pair encoding for visual tokens, priority-guided encoding (frequency and spatial consistency), and multi-stage curriculum-driven training.

Result: Improved performance in diverse vision-language tasks by better capturing cross-modal relationships.

Conclusion: The approach bridges visual and textual representations, advancing efficient multimodal foundation models.

Abstract: Multimodal large language models (MLLMs) have made significant progress in
vision-language understanding, yet effectively aligning different modalities
remains a fundamental challenge. We present a framework that unifies multimodal
understanding by applying byte-pair encoding to visual tokens. Unlike
conventional approaches that rely on modality-specific encoders, our method
directly incorporates structural information into visual tokens, mirroring
successful tokenization strategies in text-only language models. We introduce a
priority-guided encoding scheme that considers both frequency and spatial
consistency, coupled with a multi-stage training procedure based on
curriculum-driven data composition. These enhancements enable the transformer
model to better capture cross-modal relationships and reason with visual
information. Comprehensive experiments demonstrate improved performance across
diverse vision-language tasks. By bridging the gap between visual and textual
representations, our approach contributes to the advancement of more capable
and efficient multimodal foundation models.

</details>


### [297] [Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles](https://arxiv.org/pdf/2506.23426)
*Menna Taha, Aya Ahmed, Mohammed Karmoose, Yasser Gadallah*

Main category: cs.CV

TL;DR: The paper proposes a novel object detection method for AVs that focuses on determining object harmfulness rather than traditional class-based classification, improving safety by detecting OOD objects.


<details>
  <summary>Details</summary>
Motivation: Current AV object detection models struggle with OOD objects, posing safety risks. The paper aims to address this by shifting focus to harmfulness assessment.

Method: The method classifies objects as 'harmful' or 'harmless' based on their position and trajectory relative to the AV, enabling detection of unseen objects.

Result: The model effectively detects OOD objects, assesses their harmfulness, and enhances AV decision-making in dynamic environments.

Conclusion: The proposed approach improves AV safety by focusing on harmfulness, enabling better real-time responses to previously unseen objects.

Abstract: Autonomous vehicles (AVs) use object detection models to recognize their
surroundings and make driving decisions accordingly. Conventional object
detection approaches classify objects into known classes, which limits the AV's
ability to detect and appropriately respond to Out-of-Distribution (OOD)
objects. This problem is a significant safety concern since the AV may fail to
detect objects or misclassify them, which can potentially lead to hazardous
situations such as accidents. Consequently, we propose a novel object detection
approach that shifts the emphasis from conventional class-based classification
to object harmfulness determination. Instead of object detection by their
specific class, our method identifies them as either 'harmful' or 'harmless'
based on whether they pose a danger to the AV. This is done based on the object
position relative to the AV and its trajectory. With this metric, our model can
effectively detect previously unseen objects to enable the AV to make safer
real-time decisions. Our results demonstrate that the proposed model
effectively detects OOD objects, evaluates their harmfulness, and classifies
them accordingly, thus enhancing the AV decision-making effectiveness in
dynamic environments.

</details>


### [298] [VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation](https://arxiv.org/pdf/2506.23641)
*Peng Huang, Junhu Fu, Bowen Guo, Zeju Li, Yuanyuan Wang, Yi Guo*

Main category: cs.CV

TL;DR: VAP-Diffusion leverages MLLMs for detailed medical image generation by using structured prompts and a prototype condition mechanism to ensure quality and diversity.


<details>
  <summary>Details</summary>
Motivation: Detailed descriptions for medical image generation are often unavailable, limiting realism and diversity.

Method: Uses MLLMs with Chain-of-Thought prompts to generate descriptions, stores them by category, and employs a Prototype Condition Mechanism for robustness.

Result: Effective across three medical imaging types in four datasets.

Conclusion: VAP-Diffusion improves medical image generation by integrating external knowledge and ensuring robustness.

Abstract: As the appearance of medical images is influenced by multiple underlying
factors, generative models require rich attribute information beyond labels to
produce realistic and diverse images. For instance, generating an image of skin
lesion with specific patterns demands descriptions that go beyond diagnosis,
such as shape, size, texture, and color. However, such detailed descriptions
are not always accessible. To address this, we explore a framework, termed
Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from
pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality
and diversity of medical image generation. First, to derive descriptions from
MLLMs without hallucination, we design a series of prompts following
Chain-of-Thoughts for common medical imaging tasks, including dermatologic,
colorectal, and chest X-ray images. Generated descriptions are utilized during
training and stored across different categories. During testing, descriptions
are randomly retrieved from the corresponding category for inference. Moreover,
to make the generator robust to unseen combination of descriptions at the test
time, we propose a Prototype Condition Mechanism that restricts test embeddings
to be similar to those from training. Experiments on three common types of
medical imaging across four datasets verify the effectiveness of VAP-Diffusion.

</details>


### [299] [Towards foundational LiDAR world models with efficient latent flow matching](https://arxiv.org/pdf/2506.23434)
*Tianran Liu, Shengwen Zhao, Nicholas Rhinehart*

Main category: cs.CV

TL;DR: The paper explores transferability in LiDAR world models across domains, proposing a latent CFM-based framework for improved efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR world models lack transferability across domains, requiring domain-specific training. The study aims to develop models with strong cross-domain performance.

Method: A latent conditional flow matching (CFM)-based framework is introduced to address inefficiencies in current models, focusing on data compression and training objectives.

Result: The proposed model achieves up to 11% absolute improvement over training from scratch, outperforms in 30/36 comparisons, and reduces labeled data needs by 95%. It also shows 6x higher compression and 23x computational efficiency.

Conclusion: The latent CFM framework enhances LiDAR world models' transferability and efficiency, achieving SOTA performance with significantly reduced data and computational costs.

Abstract: LiDAR-based world models offer more structured and geometry-aware
representations than their image-based counterparts. However, existing LiDAR
world models are narrowly trained; each model excels only in the domain for
which it was built. Can we develop LiDAR world models that exhibit strong
transferability across multiple domains? We conduct the first systematic domain
transfer study across three demanding scenarios: (i) outdoor to indoor
generalization, (ii) sparse-beam \& dense-beam adaptation, and (iii)
non-semantic to semantic transfer. Given different amounts of fine-tuning data,
our experiments show that a single pre-trained model can achieve up to 11%
absolute improvement (83\% relative) over training from scratch and outperforms
training from scratch in 30/36 of our comparisons. This transferability of
dynamic learning significantly reduces the reliance on manually annotated data
for semantic occupancy forecasting: our method exceed the previous semantic
occupancy forecasting models with only 5% of the labeled training data required
by prior models. We also observed inefficiencies of current LiDAR world models,
mainly through their under-compression of LiDAR data and inefficient training
objectives. To address this, we propose a latent conditional flow matching
(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy
using only half the training data and a compression ratio 6 times higher than
that of prior methods. Our model achieves SOTA performance on
future-trajectory-conditioned semantic occupancy forecasting while being 23x
more computationally efficient (a 28x FPS speedup); and achieves SOTA
performance on semantic occupancy forecasting while being 2x more
computationally efficient (a 1.1x FPS speedup).

</details>


### [300] [PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions](https://arxiv.org/pdf/2506.23440)
*Mahesh Bhosale, Abdul Wasi, Yuanhao Zhai, Yunjie Tian, Samuel Border, Nan Xi, Pinaki Sarder, Junsong Yuan, David Doermann, Xuan Gong*

Main category: cs.CV

TL;DR: PathDiff is a diffusion framework that generates histopathology images by integrating unpaired mask-text data, improving control over semantics and spatial details for enhanced image quality and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Public datasets lack paired text and mask data for histopathology images, limiting joint use in image generation and control over semantics and spatial details.

Method: PathDiff integrates unpaired mask-text data into a unified conditioning space within a diffusion framework, enabling precise control over structural and contextual features.

Result: PathDiff generates high-quality, semantically accurate images, improving fidelity, text-image alignment, and faithfulness for tasks like nuclei segmentation and classification.

Conclusion: PathDiff outperforms existing methods, demonstrating its effectiveness in leveraging unpaired mask-text data for enhanced histopathology image generation.

Abstract: Diffusion-based generative models have shown promise in synthesizing
histopathology images to address data scarcity caused by privacy constraints.
Diagnostic text reports provide high-level semantic descriptions, and masks
offer fine-grained spatial structures essential for representing distinct
morphological regions. However, public datasets lack paired text and mask data
for the same histopathological images, limiting their joint use in image
generation. This constraint restricts the ability to fully exploit the benefits
of combining both modalities for enhanced control over semantics and spatial
details. To overcome this, we propose PathDiff, a diffusion framework that
effectively learns from unpaired mask-text data by integrating both modalities
into a unified conditioning space. PathDiff allows precise control over
structural and contextual features, generating high-quality, semantically
accurate images. PathDiff also improves image fidelity, text-image alignment,
and faithfulness, enhancing data augmentation for downstream tasks like nuclei
segmentation and classification. Extensive experiments demonstrate its
superiority over existing methods.

</details>


### [301] [Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation](https://arxiv.org/pdf/2506.23460)
*Dewen Zeng, Xinrong Hu, Yu-Jen Chen, Yawen Wu, Xiaowei Xu, Yiyu Shi*

Main category: cs.CV

TL;DR: The paper introduces CLDF, a method using contrastive learning with diffusion features to improve weakly supervised semantic segmentation by addressing noise in saliency maps from conditional diffusion models.


<details>
  <summary>Details</summary>
Motivation: Traditional CAM-based methods for WSSS suffer from partial activations and imprecise boundaries. CDMs offer an alternative but introduce noise in saliency maps due to background alterations.

Method: CLDF integrates gradient maps from CDMs with CAMs for contrastive learning, training a pixel decoder to map diffusion features to a low-dimensional embedding space for segmentation.

Result: Experiments on four tasks from two medical datasets show CLDF outperforms existing baselines.

Conclusion: CLDF effectively reduces noise and improves segmentation accuracy in weakly supervised settings.

Abstract: Weakly supervised semantic segmentation (WSSS) methods using class labels
often rely on class activation maps (CAMs) to localize objects. However,
traditional CAM-based methods struggle with partial activations and imprecise
object boundaries due to optimization discrepancies between classification and
segmentation. Recently, the conditional diffusion model (CDM) has been used as
an alternative for generating segmentation masks in WSSS, leveraging its strong
image generation capabilities tailored to specific class distributions. By
modifying or perturbing the condition during diffusion sampling, the related
objects can be highlighted in the generated images. Yet, the saliency maps
generated by CDMs are prone to noise from background alterations during reverse
diffusion. To alleviate the problem, we introduce Contrastive Learning with
Diffusion Features (CLDF), a novel method that uses contrastive learning to
train a pixel decoder to map the diffusion features from a frozen CDM to a
low-dimensional embedding space for segmentation. Specifically, we integrate
gradient maps generated from CDM external classifier with CAMs to identify
foreground and background pixels with fewer false positives/negatives for
contrastive learning, enabling robust pixel embedding learning. Experimental
results on four segmentation tasks from two public medical datasets demonstrate
that our method significantly outperforms existing baselines.

</details>


### [302] [AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays](https://arxiv.org/pdf/2506.23467)
*Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang*

Main category: cs.CV

TL;DR: AdFair-CLIP improves fairness and accuracy in CLIP-based medical image models by suppressing biases with adversarial feature intervention.


<details>
  <summary>Details</summary>
Motivation: Address fairness concerns like demographic biases in CLIP models, which cause disparities in diagnostic outcomes for underrepresented groups.

Method: Introduces AdFair-CLIP, using adversarial feature intervention to mitigate spurious correlations and improve fairness.

Result: Significantly enhances fairness and diagnostic accuracy in chest X-ray datasets, maintaining robustness in zero-shot and few-shot scenarios.

Conclusion: Sets new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, especially for CXR analysis.

Abstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated
superior performance across various visual tasks including medical image
classification. However, fairness concerns, including demographic biases, have
received limited attention for CLIP models. This oversight leads to critical
issues, particularly those related to race and gender, resulting in disparities
in diagnostic outcomes and reduced reliability for underrepresented groups. To
address these challenges, we introduce AdFair-CLIP, a novel framework employing
adversarial feature intervention to suppress sensitive attributes, thereby
mitigating spurious correlations and improving prediction fairness. We conduct
comprehensive experiments on chest X-ray (CXR) datasets, and show that
AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while
maintaining robust generalization in zero-shot and few-shot scenarios. These
results establish new benchmarks for fairness-aware learning in CLIP-based
medical diagnostic models, particularly for CXR analysis.

</details>


### [303] [When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation](https://arxiv.org/pdf/2506.23724)
*Chang'an Yi, Xiaohui Deng, Guohao Chen, Yan Zhou, Qinghua Lu, Shuaicheng Niu*

Main category: cs.CV

TL;DR: COCA introduces a Cross-Model Co-Learning framework for Test-time Adaptation (TTA), leveraging complementary knowledge across models to improve adaptation performance.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods focus on single-model adaptation, but cross-model knowledge can enhance performance, as smaller models can guide larger ones effectively.

Method: COCA combines co-adaptation (integrating knowledge from other models) and self-adaptation (enhancing individual strengths via unsupervised learning).

Result: COCA significantly improves adaptation accuracy, e.g., boosting ViT-Base's performance on ImageNet-C from 51.7% to 64.5% with Mobile-ViT's guidance.

Conclusion: COCA demonstrates the value of cross-model co-learning in TTA, offering a plug-and-play solution for diverse models.

Abstract: Test-time Adaptation (TTA) adapts a given model to testing domain data with
potential domain shifts through online unsupervised learning, yielding
impressive performance. However, to date, existing TTA methods primarily focus
on single-model adaptation. In this work, we investigate an intriguing
question: how does cross-model knowledge influence the TTA process? Our
findings reveal that, in TTA's unsupervised online setting, each model can
provide complementary, confident knowledge to the others, even when there are
substantial differences in model size. For instance, a smaller model like
MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base
(86.6M parameters). In light of this, we propose COCA, a Cross-Model
Co-Learning framework for TTA, which mainly consists of two main strategies. 1)
Co-adaptation adaptively integrates complementary knowledge from other models
throughout the TTA process, reducing individual model biases. 2)
Self-adaptation enhances each model's unique strengths via unsupervised
learning, enabling diverse adaptation to the target domain. Extensive
experiments show that COCA, which can also serve as a plug-and-play module,
significantly boosts existing SOTAs, on models with various sizes--including
ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,
with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy
on ImageNet-C from 51.7% to 64.5%. The code is publicly available at
https://github.com/ycarobot/COCA.

</details>


### [304] [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/pdf/2506.23468)
*Xuan Yao, Junyu Gao, Changsheng Xu*

Main category: cs.CV

TL;DR: NavMorph is a self-evolving world model framework for VLN-CE tasks, improving generalization and adaptability in navigation using compact latent representations and Contextual Evolution Memory.


<details>
  <summary>Details</summary>
Motivation: Current VLN-CE approaches struggle with generalization to novel environments and adapting to ongoing changes during navigation.

Method: NavMorph uses compact latent representations to model environmental dynamics and integrates Contextual Evolution Memory for scene-contextual navigation.

Result: The method achieves notable performance improvements on VLN-CE benchmarks.

Conclusion: NavMorph enhances environmental understanding and decision-making, demonstrating effectiveness in VLN-CE tasks.

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires
agents to execute sequential navigation actions in complex environments guided
by natural language instructions. Current approaches often struggle with
generalizing to novel environments and adapting to ongoing changes during
navigation. Inspired by human cognition, we present NavMorph, a self-evolving
world model framework that enhances environmental understanding and
decision-making in VLN-CE tasks. NavMorph employs compact latent
representations to model environmental dynamics, equipping agents with
foresight for adaptive planning and policy refinement. By integrating a novel
Contextual Evolution Memory, NavMorph leverages scene-contextual information to
support effective navigation while maintaining online adaptability. Extensive
experiments demonstrate that our method achieves notable performance
improvements on popular VLN-CE benchmarks. Code is available at
\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.

</details>


### [305] [Interactive Interface For Semantic Segmentation Dataset Synthesis](https://arxiv.org/pdf/2506.23470)
*Ngoc-Do Tran, Minh-Tuan Huynh, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le*

Main category: cs.CV

TL;DR: SynthLab is a modular platform for visual data synthesis, addressing the challenges of creating high-quality annotated datasets for semantic segmentation by offering scalability, ease of use, and privacy mitigation.


<details>
  <summary>Details</summary>
Motivation: The resource-intensive and privacy-concerning nature of creating annotated datasets for AI and computer vision tasks drives the need for an efficient, user-friendly solution.

Method: SynthLab employs a modular architecture for visual data synthesis, featuring a drag-and-drop interface for customizable data pipelines.

Result: User studies show SynthLab is flexible, accessible, and usable by non-experts, enabling broader adoption of AI for real-world applications.

Conclusion: SynthLab successfully addresses dataset creation challenges, offering a scalable and user-friendly solution for AI-driven tasks.

Abstract: The rapid advancement of AI and computer vision has significantly increased
the demand for high-quality annotated datasets, particularly for semantic
segmentation. However, creating such datasets is resource-intensive, requiring
substantial time, labor, and financial investment, and often raises privacy
concerns due to the use of real-world data. To mitigate these challenges, we
present SynthLab, consisting of a modular platform for visual data synthesis
and a user-friendly interface. The modular architecture of SynthLab enables
easy maintenance, scalability with centralized updates, and seamless
integration of new features. Each module handles distinct aspects of computer
vision tasks, enhancing flexibility and adaptability. Meanwhile, its
interactive, user-friendly interface allows users to quickly customize their
data pipelines through drag-and-drop actions. Extensive user studies involving
a diverse range of users across different ages, professions, and expertise
levels, have demonstrated flexible usage, and high accessibility of SynthLab,
enabling users without deep technical expertise to harness AI for real-world
applications.

</details>


### [306] [GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance](https://arxiv.org/pdf/2506.23478)
*Pedro Alonso, Tianrui Li, Chongshou Li*

Main category: cs.CV

TL;DR: GeoCD improves 3D point cloud reconstruction by using geodesic distance instead of Euclidean distance, outperforming Chamfer Distance (CD) with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Chamfer Distance (CD) is limited by its reliance on Euclidean distances, which fail to capture intrinsic 3D shape geometry.

Method: Proposed GeoCD, a topology-aware and differentiable geodesic distance metric, and fine-tuned models trained with CD using GeoCD.

Result: GeoCD consistently enhances reconstruction quality, with significant gains achieved after just one epoch of fine-tuning.

Conclusion: GeoCD is a superior metric for 3D point cloud learning, addressing CD's limitations and improving performance efficiently.

Abstract: Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning
due to its simplicity and efficiency. However, it suffers from a fundamental
limitation: it relies solely on Euclidean distances, which often fail to
capture the intrinsic geometry of 3D shapes. To address this limitation, we
propose GeoCD, a topology-aware and fully differentiable approximation of
geodesic distance designed to serve as a metric for 3D point cloud learning.
Our experiments show that GeoCD consistently improves reconstruction quality
over standard CD across various architectures and datasets. We demonstrate this
by fine-tuning several models, initially trained with standard CD, using GeoCD.
Remarkably, fine-tuning for a single epoch with GeoCD yields significant gains
across multiple evaluation metrics.

</details>


### [307] [Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting](https://arxiv.org/pdf/2506.23479)
*Zhaojie Zeng, Yuesong Wang, Chao Yang, Tao Guan, Lili Ju*

Main category: cs.CV

TL;DR: A new framework using 2D Gaussian Splatting improves efficiency and adaptability in image representation, reducing training time significantly while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Address the high GPU resource demands and slow training of Implicit Neural Representation (INR) and the limitations of GaussianImage's fixed Gaussian count.

Method: Uses a network to generate a coarse Gaussian representation quickly, followed by minimal fine-tuning, and dynamically adjusts Gaussian points based on image complexity.

Result: Achieves comparable or better rendering quality than GaussianImage with up to 10x faster training and adaptive Gaussian counts.

Conclusion: The proposed framework offers a practical, efficient, and flexible solution for image representation, outperforming existing methods in speed and adaptability.

Abstract: Implicit Neural Representation (INR) has demonstrated remarkable advances in
the field of image representation but demands substantial GPU resources.
GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this
cost, however, the slow training process limits its practicality, and the fixed
number of Gaussians per image limits its adaptability to varying information
entropy. To address these issues, we propose in this paper a generalizable and
self-adaptive image representation framework based on 2D Gaussian Splatting.
Our method employs a network to quickly generate a coarse Gaussian
representation, followed by minimal fine-tuning steps, achieving comparable
rendering quality of GaussianImage while significantly reducing training time.
Moreover, our approach dynamically adjusts the number of Gaussian points based
on image complexity to further enhance flexibility and efficiency in practice.
Experiments on DIV2K and Kodak datasets show that our method matches or exceeds
GaussianImage's rendering performance with far fewer iterations and shorter
training times. Specifically, our method reduces the training time by up to one
order of magnitude while achieving superior rendering performance with the same
number of Gaussians.

</details>


### [308] [MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting](https://arxiv.org/pdf/2506.23482)
*Jun Huang, Ting Liu, Yihang Wu, Xiaochao Qu, Luoqi Liu, Xiaolin Hu*

Main category: cs.CV

TL;DR: MTADiffusion is a Mask-Text Alignment diffusion model for object inpainting, addressing issues like semantic misalignment and style inconsistency. It introduces MTAPipeline for mask annotation, a multi-task training strategy, and a style-consistency loss, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing inpainting methods suffer from semantic misalignment, structural distortion, and style inconsistency, prompting the need for a more robust solution.

Method: MTADiffusion uses MTAPipeline for mask-text annotation, a multi-task training strategy combining inpainting and edge prediction, and a style-consistency loss with a VGG network and Gram matrix.

Result: MTADiffusion outperforms other methods on BrushBench and EditBench, demonstrating superior performance.

Conclusion: MTADiffusion effectively addresses key challenges in image inpainting, offering improved semantic alignment, structural stability, and style consistency.

Abstract: Advancements in generative models have enabled image inpainting models to
generate content within specific regions of an image based on provided prompts
and masks. However, existing inpainting methods often suffer from problems such
as semantic misalignment, structural distortion, and style inconsistency. In
this work, we present MTADiffusion, a Mask-Text Alignment diffusion model
designed for object inpainting. To enhance the semantic capabilities of the
inpainting model, we introduce MTAPipeline, an automatic solution for
annotating masks with detailed descriptions. Based on the MTAPipeline, we
construct a new MTADataset comprising 5 million images and 25 million mask-text
pairs. Furthermore, we propose a multi-task training strategy that integrates
both inpainting and edge prediction tasks to improve structural stability. To
promote style consistency, we present a novel inpainting style-consistency loss
using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations
on BrushBench and EditBench demonstrate that MTADiffusion achieves
state-of-the-art performance compared to other methods.

</details>


### [309] [Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking](https://arxiv.org/pdf/2506.23783)
*Shiao Wang, Ju Huang, Qingchuan Ma, Jinfeng Gao, Chunyi Xu, Xiao Wang, Lan Chen, Bo Jiang*

Main category: cs.CV

TL;DR: Proposes Mamba-FETrack V2, an efficient RGB-Event object tracking framework using Vision Mamba for low-complexity feature extraction and fusion.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal tracking algorithms rely on high-complexity Vision Transformers, causing computational overhead and limiting cross-modal interactions.

Method: Uses a lightweight Prompt Generator and Vision Mamba-based FEMamba backbone for prompt-guided feature extraction and fusion.

Result: Demonstrates superior performance and efficiency on RGB-Event tracking benchmarks like COESOT, FE108, and FELT V2.

Conclusion: The framework offers an efficient solution for RGB-Event tracking with promising results.

Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust
object tracking has garnered increasing attention in recent years. However,
most existing multimodal tracking algorithms depend heavily on high-complexity
Vision Transformer architectures for feature extraction and fusion across
modalities. This not only leads to substantial computational overhead but also
limits the effectiveness of cross-modal interactions. In this paper, we propose
an efficient RGB-Event object tracking framework based on the linear-complexity
Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a
lightweight Prompt Generator that utilizes embedded features from each
modality, together with a shared prompt pool, to dynamically generate
modality-specific learnable prompt vectors. These prompts, along with the
modality-specific embedded features, are then fed into a Vision Mamba-based
FEMamba backbone, which facilitates prompt-guided feature extraction,
cross-modal interaction, and fusion in a unified manner. Finally, the fused
representations are passed to the tracking head for accurate target
localization. Extensive experimental evaluations on multiple RGB-Event tracking
benchmarks, including short-term COESOT dataset and long-term datasets, i.e.,
FE108 and FELT V2, demonstrate the superior performance and efficiency of the
proposed tracking framework. The source code and pre-trained models will be
released on https://github.com/Event-AHU/Mamba_FETrack

</details>


### [310] [LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching](https://arxiv.org/pdf/2506.23502)
*Mengxiao Tian, Xinxiao Wu, Shuo Yang*

Main category: cs.CV

TL;DR: The paper introduces an LLM-enhanced action-aware multi-modal prompt-tuning method to improve CLIP's fine-grained action-level understanding in image-text matching.


<details>
  <summary>Details</summary>
Motivation: CLIP lacks fine-grained understanding of object attributes, spatial relationships, and actions. Recent methods focus on object-level alignment but miss action perception, which is crucial for describing object states or relationships.

Method: The proposed method uses LLM-generated action-related knowledge to design action triplet and action state prompts. An adaptive interaction module aggregates visual features based on action-aware prompts for better representations.

Result: Experiments on benchmark datasets show the method's effectiveness in improving performance.

Conclusion: The approach successfully enhances CLIP's action-level understanding, addressing a critical gap in fine-grained visual-language alignment.

Abstract: Driven by large-scale contrastive vision-language pre-trained models such as
CLIP, recent advancements in the image-text matching task have achieved
remarkable success in representation learning. Due to image-level
visual-language alignment, CLIP falls short in understanding fine-grained
details such as object attributes and spatial relationships between objects.
Recent efforts have attempted to compel CLIP to acquire structured visual
representations by introducing prompt learning to achieve object-level
alignment. While achieving promising results, they still lack the capability to
perceive actions, which are crucial for describing the states or relationships
between objects. Therefore, we propose to endow CLIP with fine-grained
action-level understanding by introducing an LLM-enhanced action-aware
multi-modal prompt-tuning method, incorporating the action-related external
knowledge generated by large language models (LLMs). Specifically, we design an
action triplet prompt and an action state prompt to exploit compositional
semantic knowledge and state-related causal knowledge implicitly stored in
LLMs. Subsequently, we propose an adaptive interaction module to aggregate
attentive visual features conditioned on action-aware prompted knowledge for
establishing discriminative and action-aware visual representations, which
further improves the performance. Comprehensive experimental results on two
benchmark datasets demonstrate the effectiveness of our method.

</details>


### [311] [Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation](https://arxiv.org/pdf/2506.23505)
*Tinh Nguyen*

Main category: cs.CV

TL;DR: YOLOv12 with physics-informed augmentation improves underwater object detection, achieving high accuracy (98.30% mAP) and speed (142 FPS), while enhancing occlusion robustness and small-object recall.


<details>
  <summary>Details</summary>
Motivation: Underwater object detection faces challenges like light attenuation, turbidity, and occlusion, limiting real-time deployment in low visibility.

Method: Integrates physics-informed augmentation (turbulence blurring, occlusion simulation, spectral HSV) with YOLOv12, using Residual ELAN blocks and Area Attention.

Result: Achieves 98.30% mAP at 142 FPS, improves occlusion robustness by 18.9%, small-object recall by 22.4%, and detection precision by 7.94%.

Conclusion: The method provides a precise, efficient solution for underwater robotics and conservation, validated by ablation studies.

Abstract: Underwater object detection is crucial for autonomous navigation,
environmental monitoring, and marine exploration, but it is severely hampered
by light attenuation, turbidity, and occlusion. Current methods balance
accuracy and computational efficiency, but they have trouble deploying in
real-time under low visibility conditions. Through the integration of
physics-informed augmentation techniques with the YOLOv12 architecture, this
study advances underwater detection. With Residual ELAN blocks to preserve
structural features in turbid waters and Area Attention to maintain large
receptive fields for occluded objects while reducing computational complexity.
Underwater optical properties are addressed by domain-specific augmentations
such as turbulence adaptive blurring, biologically grounded occlusion
simulation, and spectral HSV transformations for color distortion. Extensive
tests on four difficult datasets show state-of-the-art performance, with
Brackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion
robustness by 18.9%, small-object recall by 22.4%, and detection precision by
up to 7.94% compared to previous models. The crucial role of augmentation
strategy is validated by ablation studies. This work offers a precise and
effective solution for conservation and underwater robotics applications.

</details>


### [312] [ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models](https://arxiv.org/pdf/2506.23513)
*Zixun Fang, Kai Zhu, Zhiheng Liu, Yu Liu, Wei Zhai, Yang Cao, Zheng-Jun Zha*

Main category: cs.CV

TL;DR: A novel framework for generating high-quality panoramic videos using pretrained perspective video models, addressing the modality gap between panoramic and perspective data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with panoramic video synthesis due to the disparity between panoramic and perspective data, limiting quality.

Method: Proposes a ViewPoint map for panorama representation and a Pano-Perspective attention mechanism to leverage pretrained perspective priors.

Result: Achieves state-of-the-art performance, producing dynamic and spatially consistent panoramic videos.

Conclusion: The framework effectively bridges the modality gap, enabling superior panoramic video generation.

Abstract: Panoramic video generation aims to synthesize 360-degree immersive videos,
holding significant importance in the fields of VR, world models, and spatial
intelligence. Existing works fail to synthesize high-quality panoramic videos
due to the inherent modality gap between panoramic data and perspective data,
which constitutes the majority of the training data for modern diffusion
models. In this paper, we propose a novel framework utilizing pretrained
perspective video models for generating panoramic videos. Specifically, we
design a novel panorama representation named ViewPoint map, which possesses
global spatial continuity and fine-grained visual details simultaneously. With
our proposed Pano-Perspective attention mechanism, the model benefits from
pretrained perspective priors and captures the panoramic spatial correlations
of the ViewPoint map effectively. Extensive experiments demonstrate that our
method can synthesize highly dynamic and spatially consistent panoramic videos,
achieving state-of-the-art performance and surpassing previous methods.

</details>


### [313] [WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image](https://arxiv.org/pdf/2506.23518)
*Jiwoo Park, Tae Eun Choi, Youngjun Jun, Seong Jae Hwang*

Main category: cs.CV

TL;DR: A method to improve view consistency in novel view synthesis using diffusion models without extra modules, enhancing attention and noise handling.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of maintaining spatial continuity in novel view synthesis from a single image, as current diffusion models struggle with view consistency.

Method: Proposes a training-free approach using adaptive attention manipulation and noise reinitialization via view-guided warping to ensure consistency.

Result: Improves view consistency across diffusion models, validated by a comprehensive metric framework.

Conclusion: The method is broadly applicable and effective for enhancing view consistency in novel view synthesis.

Abstract: Generating high-quality novel views of a scene from a single image requires
maintaining structural coherence across different views, referred to as view
consistency. While diffusion models have driven advancements in novel view
synthesis, they still struggle to preserve spatial continuity across views.
Diffusion models have been combined with 3D models to address the issue, but
such approaches lack efficiency due to their complex multi-step pipelines. This
paper proposes a novel view-consistent image generation method which utilizes
diffusion models without additional modules. Our key idea is to enhance
diffusion models with a training-free method that enables adaptive attention
manipulation and noise reinitialization by leveraging view-guided warping to
ensure view consistency. Through our comprehensive metric framework suitable
for novel-view datasets, we show that our method improves view consistency
across various diffusion models, demonstrating its broader applicability.

</details>


### [314] [GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models](https://arxiv.org/pdf/2506.23903)
*Hamza Rasaee, Taha Koleilat, Hassan Rivaz*

Main category: cs.CV

TL;DR: A prompt-driven vision-language model (VLM) integrating Grounding DINO and SAM2 achieves superior object segmentation in ultrasound across multiple organs, outperforming state-of-the-art methods without needing large annotated datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in ultrasound object segmentation due to anatomical variability, diverse protocols, and limited annotated data.

Method: Uses a VLM combining Grounding DINO and SAM2, fine-tuned with LoRA on 15 ultrasound datasets, and tested on 3 unseen datasets.

Result: Outperforms UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on seen datasets and maintains strong performance on unseen ones.

Conclusion: VLMs show promise for scalable, robust ultrasound image analysis, reducing reliance on organ-specific annotated data.

Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains
a significant challenge due to anatomical variability, diverse imaging
protocols, and limited annotated data. In this study, we propose a
prompt-driven vision-language model (VLM) that integrates Grounding DINO with
SAM2 to enable object segmentation across multiple ultrasound organs. A total
of 18 public ultrasound datasets, encompassing the breast, thyroid, liver,
prostate, kidney, and paraspinal muscle, were utilized. These datasets were
divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank
Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for
testing to evaluate performance in unseen distributions. Comprehensive
experiments demonstrate that our approach outperforms state-of-the-art
segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,
and SAMUS on most seen datasets while maintaining strong performance on unseen
datasets without additional fine-tuning. These results underscore the promise
of VLMs in scalable and robust ultrasound image analysis, reducing dependence
on large, organ-specific annotated datasets. We will publish our code on
code.sonography.ai after acceptance.

</details>


### [315] [From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection](https://arxiv.org/pdf/2506.23519)
*Qi Qin, Runmin Cong, Gen Zhan, Yiting Liao, Sam Kwong*

Main category: cs.CV

TL;DR: The paper introduces fixation information to improve video salient object detection (VSOD) under weak supervision, proposing a PSE module for location/semantic guidance and an SLQ Competitor with IIMC for spatiotemporal modeling. It outperforms benchmarks.


<details>
  <summary>Details</summary>
Motivation: Eye-tracking annotations are more accessible and align with human visual patterns, motivating the use of fixation data to enhance VSOD under weak supervision.

Method: Proposes a Position and Semantic Embedding (PSE) module for guidance and a Semantics and Locality Query (SLQ) Competitor with Intra-Inter Mixed Contrastive (IIMC) learning for spatiotemporal modeling.

Result: The model outperforms other methods on five VSOD benchmarks across various metrics.

Conclusion: Leveraging fixation data and innovative modules improves VSOD performance under weak supervision, demonstrating the effectiveness of the proposed approach.

Abstract: The eye-tracking video saliency prediction (VSP) task and video salient
object detection (VSOD) task both focus on the most attractive objects in video
and show the result in the form of predictive heatmaps and pixel-level saliency
masks, respectively. In practical applications, eye tracker annotations are
more readily obtainable and align closely with the authentic visual patterns of
human eyes. Therefore, this paper aims to introduce fixation information to
assist the detection of video salient objects under weak supervision. On the
one hand, we ponder how to better explore and utilize the information provided
by fixation, and then propose a Position and Semantic Embedding (PSE) module to
provide location and semantic guidance during the feature learning process. On
the other hand, we achieve spatiotemporal feature modeling under weak
supervision from the aspects of feature selection and feature contrast. A
Semantics and Locality Query (SLQ) Competitor with semantic and locality
constraints is designed to effectively select the most matching and accurate
object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed
Contrastive (IIMC) model improves the spatiotemporal modeling capabilities
under weak supervision by forming an intra-video and inter-video contrastive
learning paradigm. Experimental results on five popular VSOD benchmarks
indicate that our model outperforms other competitors on various evaluation
metrics.

</details>


### [316] [Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving](https://arxiv.org/pdf/2506.23523)
*Tuong Do, Binh X. Nguyen, Quang D. Tran, Erman Tjiputra, Te-Chuan Chiu, Anh Nguyen*

Main category: cs.CV

TL;DR: A lightweight temporal transformer decomposition method is proposed to enhance autonomous driving by efficiently processing sequential image frames and steering data, reducing model complexity and enabling real-time performance.


<details>
  <summary>Details</summary>
Motivation: Traditional vision-based autonomous driving systems struggle with complex environments due to reliance on single-image inputs, and existing high-performance methods are resource-intensive and impractical for federated learning.

Method: The method decomposes large attention maps into smaller matrices to process sequential image frames and temporal steering data, reducing model complexity.

Result: Outperforms recent approaches on three datasets and achieves real-time performance, with effectiveness confirmed in real robot experiments.

Conclusion: The proposed method effectively enhances autonomous driving performance by leveraging temporal information efficiently and practically.

Abstract: Traditional vision-based autonomous driving systems often face difficulties
in navigating complex environments when relying solely on single-image inputs.
To overcome this limitation, incorporating temporal data such as past image
frames or steering sequences, has proven effective in enhancing robustness and
adaptability in challenging scenarios. While previous high-performance methods
exist, they often rely on resource-intensive fusion networks, making them
impractical for training and unsuitable for federated learning. To address
these challenges, we propose lightweight temporal transformer decomposition, a
method that processes sequential image frames and temporal steering data by
breaking down large attention maps into smaller matrices. This approach reduces
model complexity, enabling efficient weight updates for convergence and
real-time predictions while leveraging temporal information to enhance
autonomous driving performance. Intensive experiments on three datasets
demonstrate that our method outperforms recent approaches by a clear margin
while achieving real-time performance. Additionally, real robot experiments
further confirm the effectiveness of our method.

</details>


### [317] [When Test-Time Adaptation Meets Self-Supervised Models](https://arxiv.org/pdf/2506.23529)
*Jisu Han, Jihee Park, Dongyoon Han, Wonjun Hwang*

Main category: cs.CV

TL;DR: The paper explores test-time adaptation (TTA) for self-supervised learning (SSL) models, proposing a collaborative framework to improve performance without relying on source pretraining.


<details>
  <summary>Details</summary>
Motivation: To enable SSL models to adapt dynamically to changing environments without dependency on source pretraining, addressing the limitations of existing TTA methods.

Method: Introduces a self-supervised TTA protocol and a collaborative learning framework combining SSL and TTA, using contrastive learning and knowledge distillation for refinement.

Result: Validated on diverse SSL models (DINO, MoCo, iBOT), the method achieves competitive performance without source pretraining.

Conclusion: The proposed framework effectively enhances SSL models' adaptability in dynamic environments, demonstrating its potential for practical applications.

Abstract: Training on test-time data enables deep learning models to adapt to dynamic
environmental changes, enhancing their practical applicability. Online
adaptation from source to target domains is promising but it remains highly
reliant on the performance of source pretrained model. In this paper, we
investigate whether test-time adaptation (TTA) methods can continuously improve
models trained via self-supervised learning (SSL) without relying on source
pretraining. We introduce a self-supervised TTA protocol after observing that
existing TTA approaches struggle when directly applied to self-supervised
models with low accuracy on the source domain. Furthermore, we propose a
collaborative learning framework that integrates SSL and TTA models, leveraging
contrastive learning and knowledge distillation for stepwise representation
refinement. We validate our method on diverse self-supervised models, including
DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the
effectiveness of our approach in SSL, showing that it achieves competitive
performance even without source pretraining.

</details>


### [318] [GViT: Representing Images as Gaussians for Visual Recognition](https://arxiv.org/pdf/2506.23532)
*Jefferson Hernandez, Ruozhen He, Guha Balakrishnan, Alexander C. Berg, Vicente Ordonez*

Main category: cs.CV

TL;DR: GVIT replaces pixel/patch grids with learnable 2D Gaussians for image classification, achieving 76.9% top-1 accuracy on Imagenet-1k.


<details>
  <summary>Details</summary>
Motivation: To move beyond traditional pixel or patch grid representations by using compact, learnable 2D Gaussians for more efficient and effective image encoding.

Method: Encodes images as hundreds of Gaussians, optimizing their attributes (position, scale, etc.) jointly with a ViT classifier. Uses classifier gradients to guide Gaussians toward salient regions and a differentiable renderer for reconstruction.

Result: GVIT matches traditional patch-based ViT performance, achieving 76.9% top-1 accuracy on Imagenet-1k with a ViT-B architecture.

Conclusion: GVIT demonstrates the viability of Gaussian-based representations for image classification, offering an alternative to conventional grid-based methods.

Abstract: We introduce GVIT, a classification framework that abandons conventional
pixel or patch grid input representations in favor of a compact set of
learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose
positions, scales, orientations, colors, and opacities are optimized jointly
with a ViT classifier trained on top of these representations. We reuse the
classifier gradients as constructive guidance, steering the Gaussians toward
class-salient regions while a differentiable renderer optimizes an image
reconstruction loss. We demonstrate that by 2D Gaussian input representations
coupled with our GVIT guidance, using a relatively standard ViT architecture,
closely matches the performance of a traditional patch-based ViT, reaching a
76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.

</details>


### [319] [Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention](https://arxiv.org/pdf/2506.23542)
*Weida Wang, Changyong He, Jin Zeng, Di Qiu*

Main category: cs.CV

TL;DR: A novel ToF depth denoising network using motion-invariant graph fusion improves temporal stability and spatial sharpness by leveraging cross-frame geometric attention and a maximum a posteriori formulation.


<details>
  <summary>Details</summary>
Motivation: ToF depth images are noisy, and existing methods either ignore multi-frame depth variations or lack temporal consistency, leading to poor performance.

Method: Proposes a network with motion-invariant graph fusion, geometric attention, and an iterative filtering solution derived from a maximum a posteriori problem.

Result: Achieves state-of-the-art performance on synthetic DVToF and real Kinectv2 datasets, with robust generalization.

Conclusion: The method effectively denoises ToF depth images while maintaining temporal and spatial quality, with interpretable and adaptive learning.

Abstract: Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,
requiring denoising for reliable downstream applications. Previous works either
focus on single-frame processing, or perform multi-frame processing without
considering depth variations at corresponding pixels across frames, leading to
undesirable temporal inconsistency and spatial ambiguity. In this paper, we
propose a novel ToF depth denoising network leveraging motion-invariant graph
fusion to simultaneously enhance temporal stability and spatial sharpness.
Specifically, despite depth shifts across frames, graph structures exhibit
temporal self-similarity, enabling cross-frame geometric attention for graph
fusion. Then, by incorporating an image smoothness prior on the fused graph and
data fidelity term derived from ToF noise distribution, we formulate a maximum
a posterior problem for ToF denoising. Finally, the solution is unrolled into
iterative filters whose weights are adaptively learned from the graph-informed
geometric attention, producing a high-performance yet interpretable network.
Experimental results demonstrate that the proposed scheme achieves
state-of-the-art performance in terms of accuracy and consistency on synthetic
DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.
Source code will be released at
\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.

</details>


### [320] [Pyramidal Patchification Flow for Visual Generation](https://arxiv.org/pdf/2506.23543)
*Hui Li, Baoyou Chen, Liwei Zhang, Jiaye Li, Jingdong Wang, Siyu Zhu*

Main category: cs.CV

TL;DR: PPFlow introduces a pyramidal patchification approach for diffusion transformers (DiTs), varying patch sizes by noise timesteps to optimize computation cost and performance.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and performance in diffusion transformers by adapting patch sizes dynamically based on noise levels.

Method: Uses Pyramidal Patchification Flow (PPFlow) with varying patch sizes for different timesteps, learned linear projections, and modified Unpatchify. Operates on full latent representations without renoising.

Result: Achieves 1.6× to 2.0× inference speedup over SiT-B/2 with similar performance. Pretrained DiTs show even better results with minimal training time.

Conclusion: PPFlow effectively balances computation cost and performance, offering a scalable solution for diffusion transformers.

Abstract: Diffusion transformers (DiTs) adopt Patchify, mapping patch representations
to token representations through linear projections, to adjust the number of
tokens input to DiT blocks and thus the computation cost. Instead of a single
patch size for all the timesteps, we introduce a Pyramidal Patchification Flow
(PPFlow) approach: Large patch sizes are used for high noise timesteps and
small patch sizes for low noise timesteps; Linear projections are learned for
each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,
our approach operates over full latent representations other than pyramid
representations, and adopts the normal denoising process without requiring the
renoising trick. We demonstrate the effectiveness of our approach through two
training manners. Training from scratch achieves a $1.6\times$ ($2.0\times$)
inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with
slightly lower training FLOPs and similar image generation performance.
Training from pretrained normal DiTs achieves even better performance with
small training time. The code and checkpoint are at
https://github.com/fudan-generative-vision/PPFlow.

</details>


### [321] [A Survey on Vision-Language-Action Models for Autonomous Driving](https://arxiv.org/pdf/2506.24044)
*Sicong Jiang, Zilin Huang, Kangan Qian, Ziang Luo, Tianze Zhu, Yang Zhong, Yihong Tang, Menglin Kong, Yunlong Wang, Siwen Jiao, Hao Ye, Zihao Sheng, Xin Zhao, Tuopu Wen, Zheng Fu, Sikai Chen, Kun Jiang, Diange Yang, Seongjin Choi, Lijun Sun*

Main category: cs.CV

TL;DR: This survey provides a comprehensive overview of Vision-Language-Action (VLA) models for Autonomous Driving (VLA4AD), covering architecture, evolution, model comparisons, datasets, benchmarks, and open challenges.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of MLLMs and VLAs in autonomous driving has led to fragmented literature, necessitating a consolidated overview to guide future research.

Method: The survey formalizes architectural building blocks, traces model evolution, compares over 20 representative models, and consolidates datasets and benchmarks.

Result: It highlights progress in VLA4AD, identifies key protocols for evaluating driving safety and explanation quality, and outlines open challenges like robustness and real-time efficiency.

Conclusion: The survey serves as a concise reference for advancing interpretable and socially aligned autonomous vehicles, with future directions and a GitHub repo provided.

Abstract: The rapid progress of multimodal large language models (MLLM) has paved the
way for Vision-Language-Action (VLA) paradigms, which integrate visual
perception, natural language understanding, and control within a single policy.
Researchers in autonomous driving are actively adapting these methods to the
vehicle domain. Such models promise autonomous vehicles that can interpret
high-level instructions, reason about complex traffic scenes, and make their
own decisions. However, the literature remains fragmented and is rapidly
expanding. This survey offers the first comprehensive overview of VLA for
Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks
shared across recent work, (ii) trace the evolution from early explainer to
reasoning-centric VLA models, and (iii) compare over 20 representative models
according to VLA's progress in the autonomous driving domain. We also
consolidate existing datasets and benchmarks, highlighting protocols that
jointly measure driving safety, accuracy, and explanation quality. Finally, we
detail open challenges - robustness, real-time efficiency, and formal
verification - and outline future directions of VLA4AD. This survey provides a
concise yet complete reference for advancing interpretable socially aligned
autonomous vehicles. Github repo is available at
\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.

</details>


### [322] [Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions](https://arxiv.org/pdf/2506.23547)
*Jiwon Kim, Soohyun Hwang, Dong-O Kim, Changsu Han, Min Kyu Park, Chang-Su Kim*

Main category: cs.CV

TL;DR: Oneta is a novel multi-style image enhancement algorithm using a two-step model (intensity enhancement and color correction) with compact representation (eigenTF) and style tokens for diverse tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for a versatile and high-performance solution for multi-style image enhancement across various tasks.

Method: Uses two sequential point operators (TF for intensity, CCM for color) with Y-Net and C-Net for parameter prediction. Employs K learnable style tokens for K styles.

Result: Achieves high performance across six enhancement tasks (e.g., retouching, dehazing) on 30 datasets.

Conclusion: Oneta is an effective, versatile solution for multi-style image enhancement.

Abstract: The first algorithm, called Oneta, for a novel task of multi-style image
enhancement is proposed in this work. Oneta uses two point operators
sequentially: intensity enhancement with a transformation function (TF) and
color correction with a color correction matrix (CCM). This two-step
enhancement model, though simple, achieves a high performance upper bound.
Also, we introduce eigentransformation function (eigenTF) to represent TF
compactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and
CCM parameters, respectively. To support $K$ styles, Oneta employs $K$
learnable tokens. During training, each style token is learned using image
pairs from the corresponding dataset. In testing, Oneta selects one of the $K$
style tokens to enhance an image accordingly. Extensive experiments show that
the single Oneta network can effectively undertake six enhancement tasks --
retouching, image signal processing, low-light image enhancement, dehazing,
underwater image enhancement, and white balancing -- across 30 datasets.

</details>


### [323] [LH2Face: Loss function for Hard High-quality Face](https://arxiv.org/pdf/2506.23555)
*Fan Xie, Pan Cao*

Main category: cs.CV

TL;DR: A novel loss function, LH2Face, improves face recognition by addressing hard samples and face quality, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current face recognition methods struggle with hard samples and ignore face quality, leading to overly uniform training strategies.

Method: LH2Face uses a vMF-based similarity measure, adaptive margins, proxy-based constraints, and a face reconstruction renderer.

Result: Achieves 49.39% accuracy on IJB-B, surpassing the second-best method by 2.37%.

Conclusion: LH2Face effectively addresses hard samples and improves face recognition performance.

Abstract: In current practical face authentication systems, most face recognition (FR)
algorithms are based on cosine similarity with softmax classification. Despite
its reliable classification performance, this method struggles with hard
samples. A popular strategy to improve FR performance is incorporating angular
or cosine margins. However, it does not take face quality or recognition
hardness into account, simply increasing the margin value and thus causing an
overly uniform training strategy. To address this problem, a novel loss
function is proposed, named Loss function for Hard High-quality Face (LH2Face).
Firstly, a similarity measure based on the von Mises-Fisher (vMF) distribution
is stated, specifically focusing on the logarithm of the Probability Density
Function (PDF), which represents the distance between a probability
distribution and a vector. Then, an adaptive margin-based multi-classification
method using softmax, called the Uncertainty-Aware Margin Function, is
implemented in the article. Furthermore, proxy-based loss functions are used to
apply extra constraints between the proxy and sample to optimize their
representation space distribution. Finally, a renderer is constructed that
optimizes FR through face reconstruction and vice versa. Our LH2Face is
superior to similiar schemes on hard high-quality face datasets, achieving
49.39% accuracy on the IJB-B dataset, which surpasses the second-place method
by 2.37%.

</details>


### [324] [Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention](https://arxiv.org/pdf/2506.24085)
*Wonwoong Cho, Yanxia Zhang, Yan-Ying Chen, David I. Inouye*

Main category: cs.CV

TL;DR: IT-Blender, a T2I diffusion adapter, automates blending visual and textual concepts to enhance human creativity, outperforming baselines by leveraging pretrained diffusion models and blended attention.


<details>
  <summary>Details</summary>
Motivation: Human cross-modal conceptual blending is prone to cognitive biases like design fixation, limiting creativity. IT-Blender aims to automate and improve this process.

Method: IT-Blender uses pretrained diffusion models (SD and FLUX) to blend latent representations of a clean reference image with a noisy generated image, employing blended attention for detail preservation and disentanglement.

Result: IT-Blender significantly outperforms baselines in blending visual and textual concepts, demonstrating its effectiveness.

Conclusion: IT-Blender showcases the potential of image generative models to augment human creativity by automating cross-modal conceptual blending.

Abstract: Blending visual and textual concepts into a new visual concept is a unique
and powerful trait of human beings that can fuel creativity. However, in
practice, cross-modal conceptual blending for humans is prone to cognitive
biases, like design fixation, which leads to local minima in the design space.
In this paper, we propose a T2I diffusion adapter "IT-Blender" that can
automate the blending process to enhance human creativity. Prior works related
to cross-modal conceptual blending are limited in encoding a real image without
loss of details or in disentangling the image and text inputs. To address these
gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend
the latent representations of a clean reference image with those of the noisy
generated image. Combined with our novel blended attention, IT-Blender encodes
the real reference image without loss of details and blends the visual concept
with the object specified by the text in a disentangled way. Our experiment
results show that IT-Blender outperforms the baselines by a large margin in
blending visual and textual concepts, shedding light on the new application of
image generative models to augment human creativity.

</details>


### [325] [OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving](https://arxiv.org/pdf/2506.23565)
*Mingqian Ji, Jian Yang, Shanshan Zhang*

Main category: cs.CV

TL;DR: The paper proposes Object-centric Radiance Fields (OcRF) to improve 3D object detection by focusing on foreground objects and avoiding background noise, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-view 3D object detection rely on implicit, data-driven approaches, limiting performance. Radiance fields, successful in 3D reconstruction, are explored but initially degrade detection due to background interference.

Method: The authors introduce OcRF to enhance 3D voxel features by rendering foreground objects and use Height-aware Opacity-based Attention (HOA) to improve 2D BEV features.

Result: OcRFDet achieves 57.2% mAP and 64.8% NDS on nuScenes, outperforming prior methods.

Conclusion: Focusing on foreground objects with OcRF and HOA significantly improves 3D object detection performance.

Abstract: Current multi-view 3D object detection methods typically transfer 2D features
into 3D space using depth estimation or 3D position encoder, but in a fully
data-driven and implicit manner, which limits the detection performance.
Inspired by the success of radiance fields on 3D reconstruction, we assume they
can be used to enhance the detector's ability of 3D geometry estimation.
However, we observe a decline in detection performance, when we directly use
them for 3D rendering as an auxiliary task. From our analysis, we find the
performance drop is caused by the strong responses on the background when
rendering the whole scene. To address this problem, we propose object-centric
radiance fields, focusing on modeling foreground objects while discarding
background noises. Specifically, we employ Object-centric Radiance Fields
(OcRF) to enhance 3D voxel features via an auxiliary task of rendering
foreground objects. We further use opacity - the side-product of rendering- to
enhance the 2D foreground BEV features via Height-aware Opacity-based Attention
(HOA), where attention maps at different height levels are generated separately
via multiple networks in parallel. Extensive experiments on the nuScenes
validation and test datasets demonstrate that our OcRFDet achieves superior
performance, outperforming previous state-of-the-art methods with 57.2$\%$ mAP
and 64.8$\%$ NDS on the nuScenes test benchmark. Code will be available at
https://github.com/Mingqj/OcRFDet.

</details>


### [326] [Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution](https://arxiv.org/pdf/2506.23566)
*Luigi Sigillo, Renato Giamba, Danilo Comminiello*

Main category: cs.CV

TL;DR: MWT-Diff is a framework for satellite image super-resolution using latent diffusion models and wavelet transforms, outperforming recent methods in perceptual quality.


<details>
  <summary>Details</summary>
Motivation: High-resolution satellite imagery is limited by sensor constraints and high costs, hindering applications like environmental monitoring and disaster response.

Method: Combines latent diffusion models with wavelet transforms, using a metadata-, wavelet-, and time-aware encoder (MWT-Encoder) to guide hierarchical diffusion dynamics for image reconstruction.

Result: MWT-Diff outperforms recent approaches in perceptual quality metrics (FID, LPIPS) across multiple datasets.

Conclusion: The framework effectively reconstructs high-resolution satellite imagery while preserving critical spatial characteristics.

Abstract: The acquisition of high-resolution satellite imagery is often constrained by
the spatial and temporal limitations of satellite sensors, as well as the high
costs associated with frequent observations. These challenges hinder
applications such as environmental monitoring, disaster response, and
agricultural management, which require fine-grained and high-resolution data.
In this paper, we propose MWT-Diff, an innovative framework for satellite image
super-resolution (SR) that combines latent diffusion models with wavelet
transforms to address these challenges. At the core of the framework is a novel
metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates
embeddings that capture metadata attributes, multi-scale frequency information,
and temporal relationships. The embedded feature representations steer the
hierarchical diffusion dynamics, through which the model progressively
reconstructs high-resolution satellite imagery from low-resolution inputs. This
process preserves critical spatial characteristics including textural patterns,
boundary discontinuities, and high-frequency spectral components essential for
detailed remote sensing analysis. The comparative analysis of MWT-Diff across
multiple datasets demonstrated favorable performance compared to recent
approaches, as measured by standard perceptual quality metrics including FID
and LPIPS.

</details>


### [327] [FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation](https://arxiv.org/pdf/2506.24125)
*Jiacheng Cui, Xinyue Bi, Yaxin Luo, Xiaohan Zhao, Jiacheng Liu, Zhiqiang Shen*

Main category: cs.CV

TL;DR: The paper introduces Data Residual Matching (FADRM) for dataset distillation, leveraging data-level skip connections to improve efficiency and accuracy, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of residual connections in data-centric approaches, specifically for dataset distillation, addressing data information vanishing and computational inefficiency.

Method: Proposes Data Residual Matching, combining data-level skip connections with optimization-level refinements to balance new knowledge and core local information.

Result: Achieves 47.7% test accuracy (single-model) and 50.0% (multi-model) on ImageNet-1K, with a 50% reduction in training time and GPU memory usage.

Conclusion: FADRM sets a new benchmark for dataset distillation, outperforming existing methods in both efficiency and effectiveness.

Abstract: Residual connection has been extensively studied and widely applied at the
model architecture level. However, its potential in the more challenging
data-centric approaches remains unexplored. In this work, we introduce the
concept of Data Residual Matching for the first time, leveraging data-level
skip connections to facilitate data generation and mitigate data information
vanishing. This approach maintains a balance between newly acquired knowledge
through pixel space optimization and existing core local information
identification within raw data modalities, specifically for the dataset
distillation task. Furthermore, by incorporating optimization-level
refinements, our method significantly improves computational efficiency,
achieving superior performance while reducing training time and peak GPU memory
usage by 50%. Consequently, the proposed method Fast and Accurate Data Residual
Matching for Dataset Distillation (FADRM) establishes a new state-of-the-art,
demonstrating substantial improvements over existing methods across multiple
dataset benchmarks in both efficiency and effectiveness. For instance, with
ResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the
method achieves 47.7% test accuracy in single-model dataset distillation and
50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and
outperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%
and +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.

</details>


### [328] [Event-based Tiny Object Detection: A Benchmark Dataset and Baseline](https://arxiv.org/pdf/2506.23575)
*Nuo Chen, Chao Xiao, Yimian Dai, Shiman He, Miao Li, Wei An*

Main category: cs.CV

TL;DR: The paper introduces EV-UAV, the first large-scale event-based dataset for small object detection (SOD) in anti-UAV tasks, and proposes EV-SpSegNet, a novel baseline method with a Spatiotemporal Correlation loss for improved performance.


<details>
  <summary>Details</summary>
Motivation: Traditional cameras struggle with SOD due to low frame rates and data redundancy, while existing event-based datasets lack diversity and small targets.

Method: Proposes EV-SpSegNet, a sparse segmentation network for event point clouds, and a Spatiotemporal Correlation loss to leverage motion continuity.

Result: EV-SpSegNet outperforms on the EV-UAV dataset, featuring 2.3M annotations and small targets (6.8x5.4 pixels).

Conclusion: The EV-UAV dataset and EV-SpSegNet provide a benchmark for future SOD research in event-based anti-UAV tasks.

Abstract: Small object detection (SOD) in anti-UAV task is a challenging problem due to
the small size of UAVs and complex backgrounds. Traditional frame-based cameras
struggle to detect small objects in complex environments due to their low frame
rates, limited dynamic range, and data redundancy. Event cameras, with
microsecond temporal resolution and high dynamic range, provide a more
effective solution for SOD. However, existing event-based object detection
datasets are limited in scale, feature large targets size, and lack diverse
backgrounds, making them unsuitable for SOD benchmarks. In this paper, we
introduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),
the first large-scale, highly diverse benchmark for anti-UAV tasks. It includes
147 sequences with over 2.3 million event-level annotations, featuring
extremely small targets (averaging 6.8 $\times$ 5.4 pixels) and diverse
scenarios such as urban clutter and extreme lighting conditions. Furthermore,
based on the observation that small moving targets form continuous curves in
spatiotemporal event point clouds, we propose Event based Sparse Segmentation
Network (EV-SpSegNet), a novel baseline for event segmentation in point cloud
space, along with a Spatiotemporal Correlation (STC) loss that leverages motion
continuity to guide the network in retaining target events. Extensive
experiments on the EV-UAV dataset demonstrate the superiority of our method and
provide a benchmark for future research in EVSOD. The dataset and code are at
https://github.com/ChenYichen9527/Ev-UAV.

</details>


### [329] [StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection](https://arxiv.org/pdf/2506.23577)
*Yanning Hou, Yanran Ruan, Junfa Li, Shanshan Wang, Jianfeng Qiu, Ke Xu*

Main category: cs.CV

TL;DR: The paper proposes StackCLIP, a method using stacked prompts to enhance CLIP's text-image alignment for zero-shot industrial anomaly detection, improving generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing overfitting and limited generalization in CLIP due to specific category prompts during pretraining.

Method: Uses multicategory name stacking for prompts (CSP module) and ensemble feature alignment (EFA module) for training. Introduces RPL module to refine prompt learning.

Result: Achieves state-of-the-art performance in zero-shot anomaly detection and segmentation on seven datasets.

Conclusion: StackCLIP enhances CLIP's generalization and performance in industrial anomaly detection tasks.

Abstract: Enhancing the alignment between text and image features in the CLIP model is
a critical challenge in zero-shot industrial anomaly detection tasks. Recent
studies predominantly utilize specific category prompts during pretraining,
which can cause overfitting to the training categories and limit model
generalization. To address this, we propose a method that transforms category
names through multicategory name stacking to create stacked prompts, forming
the basis of our StackCLIP model. Our approach introduces two key components.
The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts
by stacking semantically analogous categories, while utilizing multi-object
textual feature fusion to amplify discriminative anomalies among similar
objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific
linear layers tailored for each stack cluster and adaptively integrates them
based on the attributes of test categories. These modules work together to
deliver superior training speed, stability, and convergence, significantly
boosting anomaly segmentation performance. Additionally, our stacked prompt
framework offers robust generalization across classification tasks. To further
improve performance, we introduce the Regulating Prompt Learning (RPL) module,
which leverages the generalization power of stacked prompts to refine prompt
learning, elevating results in anomaly detection classification tasks.
Extensive testing on seven industrial anomaly detection datasets demonstrates
that our method achieves state-of-the-art performance in both zero-shot anomaly
detection and segmentation tasks.

</details>


### [330] [Dataset Distillation via Vision-Language Category Prototype](https://arxiv.org/pdf/2506.23580)
*Yawen Zou, Guang Li, Duo Su, Zi Wang, Jun Yu, Chao Zhang*

Main category: cs.CV

TL;DR: The paper introduces a vision-language method for dataset distillation (DD), enhancing performance by incorporating text prototypes derived from a large language model, improving logical coherence and generalization.


<details>
  <summary>Details</summary>
Motivation: Previous DD methods focus on images, neglecting semantic context, leading to poor generalization and illogical outputs. This study aims to address this by integrating language information.

Method: The approach combines text prototypes (from a large language model) with image prototypes to collaboratively synthesize data, enhancing DD performance.

Result: The method produces logically coherent images with target objects, achieving state-of-the-art validation performance and robust generalization.

Conclusion: The framework expands DD's applicability beyond image-based methods, demonstrating effectiveness and broad utility, with code and data publicly available.

Abstract: Dataset distillation (DD) condenses large datasets into compact yet
informative substitutes, preserving performance comparable to the original
dataset while reducing storage, transmission costs, and computational
consumption. However, previous DD methods mainly focus on distilling
information from images, often overlooking the semantic information inherent in
the data. The disregard for context hinders the model's generalization ability,
particularly in tasks involving complex datasets, which may result in illogical
outputs or the omission of critical objects. In this study, we integrate
vision-language methods into DD by introducing text prototypes to distill
language information and collaboratively synthesize data with image prototypes,
thereby enhancing dataset distillation performance. Notably, the text
prototypes utilized in this study are derived from descriptive text information
generated by an open-source large language model. This framework demonstrates
broad applicability across datasets without pre-existing text descriptions,
expanding the potential of dataset distillation beyond traditional image-based
approaches. Compared to other methods, the proposed approach generates
logically coherent images containing target objects, achieving state-of-the-art
validation performance and demonstrating robust generalization. Source code and
generated data are available in
https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/

</details>


### [331] [CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models](https://arxiv.org/pdf/2506.23590)
*Qiming Li, Zekai Ye, Xiaocheng Feng, Weihong Zhong, Libo Qin, Ruihan Chen, Baohang Li, Kui Jiang, Yaowei Wang, Ting Liu, Bing Qin*

Main category: cs.CV

TL;DR: The paper introduces Caption-sensitive Attention Intervention (CAI), a training-free method to mitigate object hallucination in Large Vision-Language Models (LVLMs) by leveraging attention patterns from caption queries.


<details>
  <summary>Details</summary>
Motivation: LVLMs often produce content deviating from visual information (object hallucination), and existing solutions are costly or slow.

Method: CAI uses attention activation patterns from caption queries to enhance visual perception without additional training or high inference costs.

Result: CAI achieves state-of-the-art performance in hallucination mitigation across four benchmarks with minimal added inference cost.

Conclusion: CAI is an effective, low-cost solution for reducing object hallucination in LVLMs.

Abstract: Although Large Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in interpreting visual information, they frequently produce
content that deviates from visual information, leading to object hallucination.
To tackle this, recent works mostly depend on expensive manual annotations and
training cost, or significantly increase inference time. In this work, we
observe that LVLMs' attention to visual information is significantly stronger
when answering caption queries compared to non-caption queries. Inspired by
this phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a
training-free, plug-and-play hallucination mitigation method that leverages the
attention activation pattern in response to caption queries to enhance LVLMs'
visual perception capability. Extensive experimental results across four
benchmarks covering both discriminative and generative tasks, demonstrate that
CAI achieves state-of-the-art (SOTA) hallucination mitigating performance only
with minimal additional inference cost.

</details>


### [332] [SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion](https://arxiv.org/pdf/2506.23606)
*Zhengkang Xiang, Zizhao Li, Amir Khodabandeh, Kourosh Khoshelham*

Main category: cs.CV

TL;DR: SG-LDM is a Semantic-Guided Lidar Diffusion Model for high-fidelity lidar point cloud synthesis, outperforming existing methods and enhancing downstream tasks like segmentation.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of diversity in real-world lidar data and the limitations of unconditional generation methods for practical applications.

Method: Uses latent alignment and explicit semantic conditioning to generate lidar point clouds, and introduces a diffusion-based lidar translation framework for domain adaptation.

Result: Achieves state-of-the-art performance in lidar synthesis and improves downstream segmentation via data augmentation.

Conclusion: SG-LDM advances lidar synthesis and translation, proving effective for real-world applications and downstream tasks.

Abstract: Lidar point cloud synthesis based on generative models offers a promising
solution to augment deep learning pipelines, particularly when real-world data
is scarce or lacks diversity. By enabling flexible object manipulation, this
synthesis approach can significantly enrich training datasets and enhance
discriminative models. However, existing methods focus on unconditional lidar
point cloud generation, overlooking their potential for real-world
applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar
Diffusion Model that employs latent alignment to enable robust
semantic-to-lidar synthesis. By directly operating in the native lidar space
and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art
performance in generating high-fidelity lidar point clouds guided by semantic
labels. Moreover, we propose the first diffusion-based lidar translation
framework based on SG-LDM, which enables cross-domain translation as a domain
adaptation strategy to enhance downstream perception performance. Systematic
experiments demonstrate that SG-LDM significantly outperforms existing lidar
diffusion models and the proposed lidar translation framework further improves
data augmentation performance in the downstream lidar segmentation task.

</details>


### [333] [PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum](https://arxiv.org/pdf/2506.23607)
*Shiqi Zhang, Sha Zhang, Jiajun Deng, Yedong Shen, Mingxiao MA, Yanyong Zhang*

Main category: cs.CV

TL;DR: PGOV3D introduces a two-stage training strategy for open-vocabulary 3D semantic segmentation, leveraging partial scenes and multi-modal models for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook rich semantic content and cross-view correspondences in multi-view images, limiting effectiveness.

Method: A two-stage approach: pre-training on partial scenes with dense semantics and simple geometry, then fine-tuning on complete scenes. Uses MLLM and 2D segmentation models for open-vocabulary labels.

Result: Achieves competitive performance on ScanNet, ScanNet200, and S3DIS benchmarks.

Conclusion: PGOV3D effectively bridges the semantic gap between partial and complete 3D scenes, enhancing open-vocabulary segmentation.

Abstract: Existing open-vocabulary 3D semantic segmentation methods typically supervise
3D segmentation models by merging text-aligned features (e.g., CLIP) extracted
from multi-view images onto 3D points. However, such approaches treat
multi-view images merely as intermediaries for transferring open-vocabulary
information, overlooking their rich semantic content and cross-view
correspondences, which limits model effectiveness. To address this, we propose
PGOV3D, a novel framework that introduces a Partial-to-Global curriculum for
improving open-vocabulary 3D semantic segmentation. The key innovation lies in
a two-stage training strategy. In the first stage, we pre-train the model on
partial scenes that provide dense semantic information but relatively simple
geometry. These partial point clouds are derived from multi-view RGB-D inputs
via pixel-wise depth projection. To enable open-vocabulary learning, we
leverage a multi-modal large language model (MLLM) and a 2D segmentation
foundation model to generate open-vocabulary labels for each viewpoint,
offering rich and aligned supervision. An auxiliary inter-frame consistency
module is introduced to enforce feature consistency across varying viewpoints
and enhance spatial understanding. In the second stage, we fine-tune the model
on complete scene-level point clouds, which are sparser and structurally more
complex. We aggregate the partial vocabularies associated with each scene and
generate pseudo labels using the pre-trained model, effectively bridging the
semantic gap between dense partial observations and large-scale 3D
environments. Extensive experiments on ScanNet, ScanNet200, and S3DIS
benchmarks demonstrate that PGOV3D achieves competitive performance in
open-vocabulary 3D semantic segmentation.

</details>


### [334] [AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention](https://arxiv.org/pdf/2506.23611)
*Ziao Liu, Zhenjia Li, Yifeng Shi, Xiangang Li*

Main category: cs.CV

TL;DR: AttentionGS eliminates the need for high-quality initial point clouds in 3D Gaussian Splatting (3DGS) by using structural attention, improving reconstruction in texture-deficient or constrained-view scenarios.


<details>
  <summary>Details</summary>
Motivation: 3DGS relies on SfM for point clouds, which fails in texture-deficient or constrained-view cases, limiting its applicability.

Method: AttentionGS uses geometric attention early for global structure recovery and texture attention later for detail refinement, along with opacity-weighted gradients for Gaussian densification.

Result: AttentionGS outperforms state-of-the-art methods, especially when initial point clouds are unreliable.

Conclusion: The framework enables more robust and flexible 3DGS for real-world applications.

Abstract: 3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance
Fields (NeRF), excelling in complex scene reconstruction and efficient
rendering. However, it relies on high-quality point clouds from
Structure-from-Motion (SfM), limiting its applicability. SfM also fails in
texture-deficient or constrained-view scenarios, causing severe degradation in
3DGS reconstruction. To address this limitation, we propose AttentionGS, a
novel framework that eliminates the dependency on high-quality initial point
clouds by leveraging structural attention for direct 3D reconstruction from
randomly initialization. In the early training stage, we introduce geometric
attention to rapidly recover the global scene structure. As training
progresses, we incorporate texture attention to refine fine-grained details and
enhance rendering quality. Furthermore, we employ opacity-weighted gradients to
guide Gaussian densification, leading to improved surface reconstruction.
Extensive experiments on multiple benchmark datasets demonstrate that
AttentionGS significantly outperforms state-of-the-art methods, particularly in
scenarios where point cloud initialization is unreliable. Our approach paves
the way for more robust and flexible 3D Gaussian Splatting in real-world
applications.

</details>


### [335] [TurboVSR: Fantastic Video Upscalers and Where to Find Them](https://arxiv.org/pdf/2506.23618)
*Zhongdao Wang, Guodongfang Zhao, Jingjing Ren, Bailan Feng, Shifeng Zhang, Wenbo Li*

Main category: cs.CV

TL;DR: TurboVSR is an ultra-efficient diffusion-based video super-resolution model that achieves state-of-the-art performance while being 100+ times faster than current methods.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based VSR models are computationally inefficient, taking too long to process videos. TurboVSR aims to address this by improving efficiency without sacrificing quality.

Method: TurboVSR uses a high-compression autoencoder, factorized conditioning for training, and converts a pre-trained diffusion model into a shortcut model for faster inference.

Result: TurboVSR matches state-of-the-art VSR quality while processing a 2-second 1080p video in just 7 seconds. It also supports 4K image super-resolution with fine details.

Conclusion: TurboVSR significantly improves computational efficiency in VSR, making high-quality super-resolution practical for real-world applications.

Abstract: Diffusion-based generative models have demonstrated exceptional promise in
the video super-resolution (VSR) task, achieving a substantial advancement in
detail generation relative to prior methods. However, these approaches face
significant computational efficiency challenges. For instance, current
techniques may require tens of minutes to super-resolve a mere 2-second, 1080p
video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based
video super-resolution model. Our core design comprises three key aspects: (1)
We employ an autoencoder with a high compression ratio of 32$\times$32$\times$8
to reduce the number of tokens. (2) Highly compressed latents pose substantial
challenges for training. We introduce factorized conditioning to mitigate the
learning complexity: we first learn to super-resolve the initial frame;
subsequently, we condition the super-resolution of the remaining frames on the
high-resolution initial frame and the low-resolution subsequent frames. (3) We
convert the pre-trained diffusion model to a shortcut model to enable fewer
sampling steps, further accelerating inference. As a result, TurboVSR performs
on par with state-of-the-art VSR methods, while being 100+ times faster, taking
only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports
image resolution by considering image as a one-frame video. Our efficient
design makes SR beyond 1080p possible, results on 4K (3648$\times$2048) image
SR show surprising fine details.

</details>


### [336] [Revisiting Audio-Visual Segmentation with Vision-Centric Transformer](https://arxiv.org/pdf/2506.23623)
*Shaofei Huang, Rui Ling, Tianrui Hui, Hongyu Li, Xu Zhou, Shifeng Zhang, Si Liu, Richang Hong, Meng Wang*

Main category: cs.CV

TL;DR: The paper proposes a Vision-Centric Transformer (VCT) framework for Audio-Visual Segmentation (AVS) to address limitations of audio-centric methods, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Audio-centric Transformers in AVS suffer from perception ambiguity and weakened dense prediction due to mixed audio and visual detail loss.

Method: The VCT framework uses vision-derived queries to fetch audio and visual information iteratively, aided by a Prototype Prompted Query Generation (PPQG) module for semantically aware and visually rich queries.

Result: VCT achieves state-of-the-art performance on three subsets of the AVSBench dataset.

Conclusion: The VCT framework effectively addresses the limitations of audio-centric methods, improving segmentation accuracy for sound-producing objects.

Abstract: Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in
video frames based on the associated audio signal. Prevailing AVS methods
typically adopt an audio-centric Transformer architecture, where object queries
are derived from audio features. However, audio-centric Transformers suffer
from two limitations: perception ambiguity caused by the mixed nature of audio,
and weakened dense prediction ability due to visual detail loss. To address
these limitations, we propose a new Vision-Centric Transformer (VCT) framework
that leverages vision-derived queries to iteratively fetch corresponding audio
and visual information, enabling queries to better distinguish between
different sounding objects from mixed audio and accurately delineate their
contours. Additionally, we also introduce a Prototype Prompted Query Generation
(PPQG) module within our VCT framework to generate vision-derived queries that
are both semantically aware and visually rich through audio prototype prompting
and pixel context grouping, facilitating audio-visual information aggregation.
Extensive experiments demonstrate that our VCT framework achieves new
state-of-the-art performances on three subsets of the AVSBench dataset. The
code is available at https://github.com/spyflying/VCT_AVS.

</details>


### [337] [Brain Tumor Detection through Thermal Imaging and MobileNET](https://arxiv.org/pdf/2506.23627)
*Roham Maiti, Debasmita Bhoumik*

Main category: cs.CV

TL;DR: The paper proposes using MobileNET for efficient brain tumor detection from MRI scans, addressing limitations of traditional methods and classical ML models.


<details>
  <summary>Details</summary>
Motivation: Brain tumors pose significant health risks, and traditional detection methods like biopsies, MRI, and CT scans are costly and require expertise. Classical ML models also face computational and dataset challenges.

Method: The research employs the MobileNET model for tumor detection, focusing on reduced computational resources and faster processing while maintaining accuracy.

Result: The proposed method achieved an average accuracy of 98.5%.

Conclusion: The MobileNET-based approach offers an efficient, accurate, and accessible solution for brain tumor detection, overcoming limitations of traditional and classical ML methods.

Abstract: Brain plays a crucial role in regulating body functions and cognitive
processes, with brain tumors posing significant risks to human health. Precise
and prompt detection is a key factor in proper treatment and better patient
outcomes. Traditional methods for detecting brain tumors, that include
biopsies, MRI, and CT scans often face challenges due to their high costs and
the need for specialized medical expertise. Recent developments in machine
learning (ML) and deep learning (DL) has exhibited strong capabilities in
automating the identification and categorization of brain tumors from medical
images, especially MRI scans. However, these classical ML models have
limitations, such as high computational demands, the need for large datasets,
and long training times, which hinder their accessibility and efficiency. Our
research uses MobileNET model for efficient detection of these tumors. The
novelty of this project lies in building an accurate tumor detection model
which use less computing re-sources and runs in less time followed by efficient
decision making through the use of image processing technique for accurate
results. The suggested method attained an average accuracy of 98.5%.

</details>


### [338] [Blending Concepts with Text-to-Image Diffusion Models](https://arxiv.org/pdf/2506.23630)
*Lorenzo Olearo, Giorgio Longari, Alessandro Raganato, Rafael Peñaloza, Simone Melzi*

Main category: cs.CV

TL;DR: Diffusion models can blend diverse concepts into coherent images without training, using methods like prompt scheduling and embedding interpolation. No single method dominates; outcomes vary based on input factors.


<details>
  <summary>Details</summary>
Motivation: To explore if diffusion models can blend distinct concepts (objects, ideas) into novel images under a zero-shot framework.

Method: Four blending techniques exploiting different aspects of the diffusion pipeline (e.g., prompt scheduling, embedding interpolation, layer-wise conditioning).

Result: Models exhibit creative blending capabilities, but no single method is universally superior; outcomes depend on factors like prompt order and conceptual distance.

Conclusion: Diffusion models show strong compositional potential but are sensitive to input variations, highlighting their versatility and limitations.

Abstract: Diffusion models have dramatically advanced text-to-image generation in
recent years, translating abstract concepts into high-fidelity images with
remarkable ease. In this work, we examine whether they can also blend distinct
concepts, ranging from concrete objects to intangible ideas, into coherent new
visual entities under a zero-shot framework. Specifically, concept blending
merges the key attributes of multiple concepts (expressed as textual prompts)
into a single, novel image that captures the essence of each concept. We
investigate four blending methods, each exploiting different aspects of the
diffusion pipeline (e.g., prompt scheduling, embedding interpolation, or
layer-wise conditioning). Through systematic experimentation across diverse
concept categories, such as merging concrete concepts, synthesizing compound
words, transferring artistic styles, and blending architectural landmarks, we
show that modern diffusion models indeed exhibit creative blending capabilities
without further training or fine-tuning. Our extensive user study, involving
100 participants, reveals that no single approach dominates in all scenarios:
each blending technique excels under certain conditions, with factors like
prompt ordering, conceptual distance, and random seed affecting the outcome.
These findings highlight the remarkable compositional potential of diffusion
models while exposing their sensitivity to seemingly minor input variations.

</details>


### [339] [MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis](https://arxiv.org/pdf/2506.23648)
*Zhe Liu, Yuhao Huang, Lian Liu, Chengrui Zhang, Haotian Lin, Tong Han, Zhiyuan Zhu, Yanlin Chen, Yuerui Chen, Dong Ni, Zhongshan Gou, Xin Yang*

Main category: cs.CV

TL;DR: An automated MR diagnosis model (MReg) using 4-chamber cardiac color Doppler echocardiography video (A4C-CDV) improves accuracy and interpretability by mimicking clinical workflow and leveraging feature mining strategies.


<details>
  <summary>Details</summary>
Motivation: Current intelligent methods for mitral regurgitation (MR) diagnosis lack alignment with clinical workflow and suffer from suboptimal accuracy and interpretability.

Method: MReg formulates MR diagnosis as a regression task, uses feature selection/amplification to mimic sonographer logic, and introduces a feature summary module for category-level feature extraction.

Result: MReg outperforms other methods on a dataset of 1868 cases, demonstrating superior performance in MR diagnosis.

Conclusion: MReg offers a clinically aligned, accurate, and interpretable solution for automated MR diagnosis.

Abstract: Color Doppler echocardiography is a crucial tool for diagnosing mitral
regurgitation (MR). Recent studies have explored intelligent methods for MR
diagnosis to minimize user dependence and improve accuracy. However, these
approaches often fail to align with clinical workflow and may lead to
suboptimal accuracy and interpretability. In this study, we introduce an
automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color
Doppler echocardiography video (A4C-CDV). It follows comprehensive feature
mining strategies to detect MR and assess its severity, considering clinical
realities. Our contribution is threefold. First, we formulate the MR diagnosis
as a regression task to capture the continuity and ordinal relationships
between categories. Second, we design a feature selection and amplification
mechanism to imitate the sonographer's diagnostic logic for accurate MR
grading. Third, inspired by the Mixture-of-Experts concept, we introduce a
feature summary module to extract the category-level features, enhancing the
representational capacity for more accurate grading. We trained and evaluated
our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases
with three graded regurgitation labels. Compared to other weakly supervised
video anomaly detection and supervised classification methods, MReg
demonstrated superior performance in MR diagnosis. Our code is available at:
https://github.com/cskdstz/MReg.

</details>


### [340] [Towards Markerless Intraoperative Tracking of Deformable Spine Tissue](https://arxiv.org/pdf/2506.23657)
*Connor Daly, Elettra Marconi, Marco Riva, Jinendra Ekanayake, Daniel S. Elson, Ferdinando Rodriguez y Baena*

Main category: cs.CV

TL;DR: The paper introduces SpineAlign, a system for tracking spinal deformations using RGB-D imaging, and presents a dataset, segmentation network, and multi-task framework for registration.


<details>
  <summary>Details</summary>
Motivation: To reduce operating time and complexity in orthopedic surgery by replacing bone-mounted tracking devices with markerless RGB-D imaging.

Method: Developed SpineAlign for deformation tracking, an intraoperative segmentation network, and CorrespondNet for key region prediction.

Result: First real-world clinical RGB-D dataset for spine surgery and a functional system for markerless tracking.

Conclusion: The work advances markerless tracking in spine surgery, demonstrating feasibility and potential for clinical use.

Abstract: Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is
a promising method with high translational potential. Unlike bone-mounted
tracking devices, markerless tracking can reduce operating time and complexity.
However, its use has been limited to cadaveric studies. This paper introduces
the first real-world clinical RGB-D dataset for spine surgery and develops
SpineAlign, a system for capturing deformation between preoperative and
intraoperative spine states. We also present an intraoperative segmentation
network trained on this data and introduce CorrespondNet, a multi-task
framework for predicting key regions for registration in both intraoperative
and preoperative scenes.

</details>


### [341] [On the Domain Robustness of Contrastive Vision-Language Models](https://arxiv.org/pdf/2506.23663)
*Mario Koddenbrock, Rudolf Hoffmann, David Brodmann, Erik Rodner*

Main category: cs.CV

TL;DR: Deepbench is a framework for evaluating domain-specific robustness of vision-language models (VLMs) using LLM-generated corruptions, revealing variability in model performance across domains.


<details>
  <summary>Details</summary>
Motivation: Practitioners use pretrained foundation models despite limited transparency, and their performance declines under domain shifts.

Method: Deepbench uses an LLM to generate realistic, domain-specific image corruptions for evaluating VLMs without labeled data.

Result: Evaluation shows significant robustness variability across six domains, emphasizing the need for domain-aware assessment.

Conclusion: Deepbench is open-sourced to aid research in domain-aware robustness evaluation of VLMs.

Abstract: In real-world vision-language applications, practitioners increasingly rely
on large, pretrained foundation models rather than custom-built solutions,
despite limited transparency regarding their training data and processes. While
these models achieve impressive performance on general benchmarks, their
effectiveness can decline notably under specialized domain shifts, such as
unique imaging conditions or environmental variations. In this work, we
introduce Deepbench, a framework designed to assess domain-specific robustness
of vision-language models (VLMs). Deepbench leverages a large language model
(LLM) to generate realistic, context-aware image corruptions tailored to
specific deployment domains without requiring labeled data. We evaluate a range
of contrastive vision-language architectures and architectural variants across
six real-world domains and observe substantial variability in robustness,
highlighting the need for targeted, domain-aware evaluation. Deepbench is
released as open-source software to support further research into domain-aware
robustness assessment.

</details>


### [342] [Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration](https://arxiv.org/pdf/2506.23674)
*Dongyue Wu, Zilin Guo, Jialong Zuo, Nong Sang, Changxin Gao*

Main category: cs.CV

TL;DR: PFB is a novel framework for lossless training acceleration by adaptively pruning less important samples using shallow-layer features, reducing computational costs without proxy models.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs of large datasets in machine learning, avoiding reliance on gradients or proxy models.

Method: Uses Partial Forward Blocking (PFB) to assess sample importance via shallow-layer features, pruning less important samples and reducing deep-layer computations.

Result: Achieves 0.5% accuracy improvement and 33% training time reduction on ImageNet with 40% data pruned.

Conclusion: PFB effectively accelerates training while maintaining or improving model performance, offering a scalable solution for large datasets.

Abstract: The ever-growing size of training datasets enhances the generalization
capability of modern machine learning models but also incurs exorbitant
computational costs. Existing data pruning approaches aim to accelerate
training by removing those less important samples. However, they often rely on
gradients or proxy models, leading to prohibitive additional costs of gradient
back-propagation and proxy model training. In this paper, we propose Partial
Forward Blocking (PFB), a novel framework for lossless training acceleration.
The efficiency of PFB stems from its unique adaptive pruning pipeline: sample
importance is assessed based on features extracted from the shallow layers of
the target model. Less important samples are then pruned, allowing only the
retained ones to proceed with the subsequent forward pass and loss
back-propagation. This mechanism significantly reduces the computational
overhead of deep-layer forward passes and back-propagation for pruned samples,
while also eliminating the need for auxiliary backward computations and proxy
model training. Moreover, PFB introduces probability density as an indicator of
sample importance. Combined with an adaptive distribution estimation module,
our method dynamically prioritizes relatively rare samples, aligning with the
constantly evolving training state. Extensive experiments demonstrate the
significant superiority of PFB in performance and speed. On ImageNet, PFB
achieves a 0.5% accuracy improvement and 33% training time reduction with 40%
data pruned.

</details>


### [343] [Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation](https://arxiv.org/pdf/2506.23675)
*Patrick Glandorf, Bodo Rosenhahn*

Main category: cs.CV

TL;DR: P3B introduces a pruning method for Vision Transformers, addressing suboptimal pruning in unseen domains by globally assigning resources based on block-level contributions, maintaining performance even at high sparsity.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers are computationally expensive, and pruning on unseen domains leads to suboptimal resource allocation. Task-sensitive layers initially fail to improve feature representation, causing performance loss.

Method: P3B (Pruning by Block Benefit) uses block-level contributions to globally assign parameter resources, identifying low-impact components for pruning while preserving critical ones. It sets layerwise keep ratios based on global metrics.

Result: P3B achieves state-of-the-art pruning, with notable gains in transfer learning. It maintains high performance even at 70% sparsity, losing only 0.64% accuracy.

Conclusion: P3B effectively prunes Vision Transformers, ensuring performance retention in high-sparsity scenarios, making it practical for resource-limited hardware.

Abstract: Vision Transformer have set new benchmarks in several tasks, but these models
come with the lack of high computational costs which makes them impractical for
resource limited hardware. Network pruning reduces the computational complexity
by removing less important operations while maintaining performance. However,
pruning a model on an unseen data domain, leads to a misevaluation of weight
significance, resulting in suboptimal resource assignment. In this work, we
find that task-sensitive layers initially fail to improve the feature
representation on downstream tasks, leading to performance loss for early
pruning decisions. To address this problem, we introduce Pruning by Block
Benefit (P3B), a pruning method that utilizes the relative contribution on
block level to globally assign parameter resources. P3B identifies low-impact
components to reduce parameter allocation while preserving critical ones.
Classical pruning mask optimization struggles to reactivate zero-mask-elements.
In contrast, P3B sets a layerwise keep ratio based on global performance
metrics, ensuring the reactivation of late-converging blocks. We show in
extensive experiments that P3B is a state of the art pruning method with most
noticeable gains in transfer learning tasks. Notably, P3B is able to conserve
high performance, even in high sparsity regimes of 70% parameter reduction
while only losing 0.64% in accuracy.

</details>


### [344] [A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement](https://arxiv.org/pdf/2506.23676)
*Gaozheng Pei, Ke Ma, Dongpeng Zhang, Chengzhi Sun, Qianqian Xu, Qingming Huang*

Main category: cs.CV

TL;DR: A unified framework enhances diffusion-based adversarial example generation by integrating traditional transferability strategies, improving performance in tasks like Deepfake detection.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based adversarial methods struggle with generalization beyond image classification and adapting traditional transferability strategies.

Method: Proposes a unified framework incorporating traditional transferability strategies into diffusion model-based adversarial example generation.

Result: Achieved first place in a competition, validating the method's effectiveness.

Conclusion: The framework successfully broadens the applicability of diffusion-based adversarial methods.

Abstract: Due to their powerful image generation capabilities, diffusion-based
adversarial example generation methods through image editing are rapidly
gaining popularity. However, due to reliance on the discriminative capability
of the diffusion model, these diffusion-based methods often struggle to
generalize beyond conventional image classification tasks, such as in Deepfake
detection. Moreover, traditional strategies for enhancing adversarial example
transferability are challenging to adapt to these methods. To address these
challenges, we propose a unified framework that seamlessly incorporates
traditional transferability enhancement strategies into diffusion model-based
adversarial example generation via image editing, enabling their application
across a wider range of downstream tasks. Our method won first place in the
"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of
AI-Generated Media" competition at ACM MM25, which validates the effectiveness
of our approach.

</details>


### [345] [SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation](https://arxiv.org/pdf/2506.23690)
*Shuai Tan, Biao Gong, Yujie Wei, Shiwei Zhang, Zhuoxin Liu, Dandan Zheng, Jingdong Chen, Yan Wang, Hao Ouyang, Kecheng Zheng, Yujun Shen*

Main category: cs.CV

TL;DR: SynMotion is a motion-customized video generation model that combines semantic guidance and visual adaptation to improve motion fidelity and semantic clarity.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on semantic alignment or visual representation alone, leading to overlooked motion complexity or semantic confusion. SynMotion addresses this by jointly leveraging both aspects.

Method: SynMotion uses a dual-embedding semantic comprehension mechanism to disentangle subject and motion representations, integrates motion adapters for visual fidelity, and employs an embedding-specific training strategy with a Subject Prior Video dataset.

Result: SynMotion outperforms existing baselines in text-to-video (T2V) and image-to-video (I2V) settings, demonstrating improved motion specificity and generalization.

Conclusion: SynMotion effectively balances semantic and visual adaptation for motion-customized video generation, validated by a new benchmark (MotionBench).

Abstract: Diffusion-based video motion customization facilitates the acquisition of
human motion representations from a few video samples, while achieving
arbitrary subjects transfer through precise textual conditioning. Existing
approaches often rely on semantic-level alignment, expecting the model to learn
new motion concepts and combine them with other entities (e.g., ''cats'' or
''dogs'') to produce visually appealing results. However, video data involve
complex spatio-temporal patterns, and focusing solely on semantics cause the
model to overlook the visual complexity of motion. Conversely, tuning only the
visual representation leads to semantic confusion in representing the intended
action. To address these limitations, we propose SynMotion, a new
motion-customized video generation model that jointly leverages semantic
guidance and visual adaptation. At the semantic level, we introduce the
dual-embedding semantic comprehension mechanism which disentangles subject and
motion representations, allowing the model to learn customized motion features
while preserving its generative capabilities for diverse subjects. At the
visual level, we integrate parameter-efficient motion adapters into a
pre-trained video generation model to enhance motion fidelity and temporal
coherence. Furthermore, we introduce a new embedding-specific training strategy
which \textbf{alternately optimizes} subject and motion embeddings, supported
by the manually constructed Subject Prior Video (SPV) training dataset. This
strategy promotes motion specificity while preserving generalization across
diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark
with diverse motion patterns. Experimental results across both T2V and I2V
settings demonstrate that \method outperforms existing baselines. Project page:
https://lucaria-academy.github.io/SynMotion/

</details>


### [346] [Single Image Test-Time Adaptation via Multi-View Co-Training](https://arxiv.org/pdf/2506.23705)
*Smriti Joshi, Richard Osuala, Lidia Garrucho, Kaisar Kushibar, Dimitri Kessler, Oliver Diaz, Karim Lekadir*

Main category: cs.CV

TL;DR: A patch-based multi-view co-training method for single-image test-time adaptation in medical imaging, outperforming state-of-the-art methods by 3.75% Dice score.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of large target domain datasets in clinical settings and leveraging volumetric data in medical imaging.

Method: Patch-based multi-view co-training with uncertainty-guided self-training for feature and prediction consistency.

Result: Achieves performance close to supervised benchmarks and outperforms state-of-the-art methods by 3.75% Dice score.

Conclusion: The method enables effective volumetric segmentation with a single test-time image, validated on breast MRI datasets.

Abstract: Test-time adaptation enables a trained model to adjust to a new domain during
inference, making it particularly valuable in clinical settings where such
on-the-fly adaptation is required. However, existing techniques depend on large
target domain datasets, which are often impractical and unavailable in medical
scenarios that demand per-patient, real-time inference. Moreover, current
methods commonly focus on two-dimensional images, failing to leverage the
volumetric richness of medical imaging data. Bridging this gap, we propose a
Patch-Based Multi-View Co-Training method for Single Image Test-Time
adaptation. Our method enforces feature and prediction consistency through
uncertainty-guided self-training, enabling effective volumetric segmentation in
the target domain with only a single test-time image. Validated on three
publicly available breast magnetic resonance imaging datasets for tumor
segmentation, our method achieves performance close to the upper bound
supervised benchmark while also outperforming all existing state-of-the-art
methods, on average by a Dice Similarity Coefficient of 3.75%. We publicly
share our accessible codebase, readily integrable with the popular nnUNet
framework, at https://github.com/smriti-joshi/muvi.git.

</details>


### [347] [Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion](https://arxiv.org/pdf/2506.23711)
*Haoyang Chen, Dongfang Sun, Caoyuan Ma, Shiqin Wang, Kewei Zhang, Zheng Wang, Zhixiang Wang*

Main category: cs.CV

TL;DR: Subjective Camera reconstructs scenes from mental impressions using verbal descriptions and sketches, overcoming language and sketch limitations.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like subjective input biases, modality gaps, and sketch quality sensitivity in existing methods.

Method: Concept-sequential generation with text-reward optimization, sequence-aware disentangled generation, and latent optimization.

Result: Achieves state-of-the-art performance in semantic and spatial coherence.

Conclusion: The framework effectively translates subjective impressions into photorealistic images without requiring model adaptation or sketch precision.

Abstract: We propose Subjective Camera, a human-as-imaging-device paradigm that
reconstructs real-world scenes from mental impressions through synergistic use
of verbal descriptions and progressive rough sketches. This approach overcomes
dual limitations of language ambiguity and sketch abstraction by treating the
user's drawing sequence as priors, effectively translating subjective
perceptual expectations into photorealistic images.
  Existing approaches face three fundamental barriers: (1) user-specific
subjective input biases, (2) huge modality gap between planar sketch and 3D
priors in diffusion, and (3) sketch quality-sensitive performance degradation.
Current solutions either demand resource-intensive model adaptation or impose
impractical requirements on sketch precision.
  Our framework addresses these challenges through concept-sequential
generation. (1) We establish robust appearance priors through text-reward
optimization, and then implement sequence-aware disentangled generation that
processes concepts in sketching order; these steps accommodate user-specific
subjective expectation in a train-free way. (2) We employ latent optimization
that effectively bridges the modality gap between planar sketches and 3D priors
in diffusion. (3) Our hierarchical reward-guided framework enables the use of
rough sketches without demanding artistic expertise. Comprehensive evaluation
across diverse datasets demonstrates that our approach achieves
state-of-the-art performance in maintaining both semantic and spatial
coherence.

</details>


### [348] [Proteus-ID: ID-Consistent and Motion-Coherent Video Customization](https://arxiv.org/pdf/2506.23729)
*Guiyu Zhang, Chen Shi, Zijian Jiang, Xunzhi Xiang, Jingjing Qian, Shaoshuai Shi, Li Jiang*

Main category: cs.CV

TL;DR: Proteus-ID is a diffusion-based framework for video identity customization, addressing identity consistency and motion coherence with novel modules like MIF, TAII, and AML. It outperforms prior methods and introduces a high-quality dataset, Proteus-Bench.


<details>
  <summary>Details</summary>
Motivation: The task of synthesizing identity-consistent and motion-coherent videos from a single image and text prompt is challenging due to identity alignment and motion realism issues.

Method: Proteus-ID uses Multimodal Identity Fusion (MIF) for joint identity representation, Time-Aware Identity Injection (TAII) for dynamic conditioning, and Adaptive Motion Learning (AML) for motion realism.

Result: Proteus-ID outperforms existing methods in identity preservation, text alignment, and motion quality, validated on the Proteus-Bench dataset.

Conclusion: Proteus-ID sets a new benchmark for video identity customization, with publicly available code and data.

Abstract: Video identity customization seeks to synthesize realistic, temporally
coherent videos of a specific subject, given a single reference image and a
text prompt. This task presents two core challenges: (1) maintaining identity
consistency while aligning with the described appearance and actions, and (2)
generating natural, fluid motion without unrealistic stiffness. To address
these challenges, we introduce Proteus-ID, a novel diffusion-based framework
for identity-consistent and motion-coherent video customization. First, we
propose a Multimodal Identity Fusion (MIF) module that unifies visual and
textual cues into a joint identity representation using a Q-Former, providing
coherent guidance to the diffusion model and eliminating modality imbalance.
Second, we present a Time-Aware Identity Injection (TAII) mechanism that
dynamically modulates identity conditioning across denoising steps, improving
fine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a
self-supervised strategy that reweights the training loss based on
optical-flow-derived motion heatmaps, enhancing motion realism without
requiring additional inputs. To support this task, we construct Proteus-Bench,
a high-quality dataset comprising 200K curated clips for training and 150
individuals from diverse professions and ethnicities for evaluation. Extensive
experiments demonstrate that Proteus-ID outperforms prior methods in identity
preservation, text alignment, and motion quality, establishing a new benchmark
for video identity customization. Codes and data are publicly available at
https://grenoble-zhang.github.io/Proteus-ID/.

</details>


### [349] [Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection](https://arxiv.org/pdf/2506.23881)
*Reihaneh Zohrabi, Hosein Hasani, Mahdieh Soleymani Baghshah, Anna Rohrbach, Marcus Rohrbach, Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: SPROD is a prototype-based OOD detection method addressing spurious correlations, improving AUROC by 4.7% and FPR@95 by 9.3% over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing OOD detection methods are vulnerable to spurious correlations, compromising reliability.

Method: SPROD refines class prototypes post-hoc to mitigate bias from spurious features without extra data or tuning.

Result: Superior performance on challenging datasets (e.g., CelebA, Waterbirds) with 4.7% AUROC and 9.3% FPR@95 improvements.

Conclusion: SPROD effectively addresses spurious correlations, enhancing OOD detection robustness.

Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability
and safety of machine learning models in real-world applications, where they
frequently face data distributions unseen during training. Despite progress,
existing methods are often vulnerable to spurious correlations that mislead
models and compromise robustness. To address this, we propose SPROD, a novel
prototype-based OOD detection approach that explicitly addresses the challenge
posed by unknown spurious correlations. Our post-hoc method refines class
prototypes to mitigate bias from spurious features without additional data or
hyperparameter tuning, and is broadly applicable across diverse backbones and
OOD detection settings. We conduct a comprehensive spurious correlation OOD
detection benchmarking, comparing our method against existing approaches and
demonstrating its superior performance across challenging OOD datasets, such as
CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced
Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%
over the second best.

</details>


### [350] [Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?](https://arxiv.org/pdf/2506.23751)
*Annika Mütze, Sadia Ilyas, Christian Dörpelkus, Matthias Rottmann*

Main category: cs.CV

TL;DR: The paper explores the limitations of open-vocabulary object detectors using synthetic data generated via Stable Diffusion, revealing overlooked objects and location-dependent performance.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate and identify failure modes of open-vocabulary object detectors, which is crucial for safety-critical applications.

Method: Two automated pipelines using Stable Diffusion to inpaint unusual objects, sampling from WordNet and ChatGPT, and evaluating detectors on synthetic data derived from LostAndFound and NuImages datasets.

Result: Open-vocabulary detectors are challenged by inpainting, showing overlooked objects and strong dependence on object location rather than semantics.

Conclusion: Synthetic data provides a systematic way to test and improve open-vocabulary models, highlighting the need for diverse data acquisition.

Abstract: Open-vocabulary object detectors such as Grounding DINO are trained on vast
and diverse data, achieving remarkable performance on challenging datasets. Due
to that, it is unclear where to find their limitations, which is of major
concern when using in safety-critical applications. Real-world data does not
provide sufficient control, required for a rigorous evaluation of model
generalization. In contrast, synthetically generated data allows to
systematically explore the boundaries of model competence/generalization. In
this work, we address two research questions: 1) Can we challenge
open-vocabulary object detectors with generated image content? 2) Can we find
systematic failure modes of those models? To address these questions, we design
two automated pipelines using stable diffusion to inpaint unusual objects with
high diversity in semantics, by sampling multiple substantives from WordNet and
ChatGPT. On the synthetically generated data, we evaluate and compare multiple
open-vocabulary object detectors as well as a classical object detector. The
synthetic data is derived from two real-world datasets, namely LostAndFound, a
challenging out-of-distribution (OOD) detection benchmark, and the NuImages
dataset. Our results indicate that inpainting can challenge open-vocabulary
object detectors in terms of overlooking objects. Additionally, we find a
strong dependence of open-vocabulary models on object location, rather than on
object semantics. This provides a systematic approach to challenge
open-vocabulary models and gives valuable insights on how data could be
acquired to effectively improve these models.

</details>


### [351] [Visual Textualization for Image Prompted Object Detection](https://arxiv.org/pdf/2506.23785)
*Yongjian Wu, Yang Zhou, Jiya Saiyin, Bingzheng Wei, Yan Xu*

Main category: cs.CV

TL;DR: VisTex-OVLM enhances object detection in rare categories by projecting visual exemplars into text feature space, maintaining OVLM's architecture and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To improve Object-level Vision-Language Models' (OVLMs) detection of rare categories that are hard to describe textually and lack pre-training data.

Method: Uses multi-scale textualizing blocks and multi-stage fusion to integrate visual exemplars into text feature space, generating textualized visual tokens.

Result: Superior performance on open-set datasets and state-of-the-art results on PASCAL VOC and MSCOCO few-shot benchmarks.

Conclusion: VisTex-OVLM effectively enhances OVLMs for rare category detection without altering their architecture, achieving top performance.

Abstract: We propose VisTex-OVLM, a novel image prompted object detection method that
introduces visual textualization -- a process that projects a few visual
exemplars into the text feature space to enhance Object-level Vision-Language
Models' (OVLMs) capability in detecting rare categories that are difficult to
describe textually and nearly absent from their pre-training data, while
preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM
leverages multi-scale textualizing blocks and a multi-stage fusion strategy to
integrate visual information from visual exemplars, generating textualized
visual tokens that effectively guide OVLMs alongside text prompts. Unlike
previous methods, our method maintains the original architecture of OVLM,
maintaining its generalization capabilities while enhancing performance in
few-shot settings. VisTex-OVLM demonstrates superior performance across
open-set datasets which have minimal overlap with OVLM's pre-training data and
achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO.
The code will be released at https://github.com/WitGotFlg/VisTex-OVLM.

</details>


### [352] [Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors](https://arxiv.org/pdf/2506.23801)
*Ce Wang, Wanjie Sun*

Main category: cs.CV

TL;DR: CRefDiff is a controllable reference-based diffusion model for super-resolution in remote sensing, addressing under-generation and over-reliance on references with a dual-branch fusion mechanism and faster inference.


<details>
  <summary>Details</summary>
Motivation: Existing RefSR methods struggle with real-world complexities like cross-sensor resolution gaps and land cover changes, leading to under-generation or over-reliance on references.

Method: CRefDiff leverages Stable Diffusion's generative prior, introduces a dual-branch fusion mechanism for adaptive reference integration, and uses a 'Better Start' strategy to reduce denoising steps.

Result: CRefDiff achieves state-of-the-art performance on the Real-RefRSSRD dataset and improves downstream tasks like scene classification and semantic segmentation.

Conclusion: CRefDiff effectively addresses real-world RefSR challenges, offering superior performance, flexibility, and efficiency.

Abstract: Super-resolution (SR) techniques can enhance the spatial resolution of remote
sensing images by utilizing low-resolution (LR) images to reconstruct
high-resolution (HR) images, enabling more efficient large-scale earth
observation applications. While single-image super-resolution (SISR) methods
have shown progress, reference-based super-resolution (RefSR) offers superior
performance by incorporating historical HR images alongside current LR
observations. However, existing RefSR methods struggle with real-world
complexities, such as cross-sensor resolution gap and significant land cover
changes, often leading to under-generation or over-reliance on reference image.
To address these challenges, we propose CRefDiff, a novel controllable
reference-based diffusion model for real-world remote sensing image SR. To
address the under-generation problem, CRefDiff is built upon the pretrained
Stable Diffusion model, leveraging its powerful generative prior to produce
accurate structures and textures. To mitigate over-reliance on the reference,
we introduce a dual-branch fusion mechanism that adaptively integrates both
local and global information from the reference image. Moreover, this novel
dual-branch design enables reference strength control during inference,
enhancing interactivity and flexibility of the model. Finally, a strategy named
Better Start is proposed to significantly reduce the number of denoising steps,
thereby accelerating the inference process. To support further research, we
introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing
images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land
cover changes and significant temporal gaps. Extensive experiments on
Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across
various metrics and improves downstream tasks such as scene classification and
semantic segmentation.

</details>


### [353] [Towards Initialization-free Calibrated Bundle Adjustment](https://arxiv.org/pdf/2506.23808)
*Carl Olsson, Amanda Nilsson*

Main category: cs.CV

TL;DR: The paper introduces a method for initialization-free BA using known camera calibration, achieving near metric reconstructions by integrating rotation averaging into the pOSE framework.


<details>
  <summary>Details</summary>
Motivation: Existing initialization-free BA methods using pOSE lack camera calibration knowledge, leading to projective transformations and requiring more data. The goal is to produce near metric solutions by leveraging known calibration.

Method: The method incorporates pairwise relative rotation estimates, which are similarity-invariant, into the pOSE framework. This encourages metric-preserving reconstructions and integrates rotation averaging for calibrated SfM.

Result: Experiments show reliable optimization of the objective, with high probability of convergence to the global minimum from random starts, yielding accurate near metric reconstructions.

Conclusion: The proposed method successfully achieves initialization-free BA with near metric accuracy by leveraging camera calibration and rotation averaging, outperforming projective-only approaches.

Abstract: A recent series of works has shown that initialization-free BA can be
achieved using pseudo Object Space Error (pOSE) as a surrogate objective. The
initial reconstruction-step optimizes an objective where all terms are
projectively invariant and it cannot incorporate knowledge of the camera
calibration. As a result, the solution is only determined up to a projective
transformation of the scene and the process requires more data for successful
reconstruction.
  In contrast, we present a method that is able to use the known camera
calibration thereby producing near metric solutions, that is, reconstructions
that are accurate up to a similarity transformation. To achieve this we
introduce pairwise relative rotation estimates that carry information about
camera calibration. These are only invariant to similarity transformations,
thus encouraging solutions that preserve metric features of the real scene. Our
method can be seen as integrating rotation averaging into the pOSE framework
striving towards initialization-free calibrated SfM.
  Our experimental evaluation shows that we are able to reliably optimize our
objective, achieving convergence to the global minimum with high probability
from random starting solutions, resulting in accurate near metric
reconstructions.

</details>


### [354] [MMInA: Benchmarking Multihop Multimodal Internet Agents](https://arxiv.org/pdf/2404.09992)
*Shulin Tian, Ziniu Zhang, Liangyu Chen, Ziwei Liu*

Main category: cs.CV

TL;DR: MMInA is a benchmark for evaluating autonomous agents on multihop, multimodal tasks across evolving real-world websites, highlighting challenges for current agents and proposing a memory-augmented solution.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack realism and fail to assess embodied agents in evolving, multimodal web environments for complex user tasks.

Method: MMInA includes 1,050 human-written tasks on real-world websites, requiring multihop reasoning and multimodal information extraction. A memory augmentation approach is proposed to improve agent performance.

Result: Current agents struggle with long-chain multihop tasks, especially early hops, but the memory augmentation method significantly improves performance.

Conclusion: MMInA provides a realistic benchmark for web agents, identifying key challenges and demonstrating the effectiveness of memory augmentation for task success.

Abstract: Autonomous embodied agents live on an Internet of multimedia websites. Can
they hop around multimodal websites to complete complex user tasks? Existing
benchmarks fail to assess them in a realistic, evolving environment for their
embodiment across websites. To answer this question, we present MMInA, a
multihop and multimodal benchmark to evaluate the embodied agents for
compositional Internet tasks, with several appealing properties: 1) Evolving
real-world multimodal websites. Our benchmark uniquely operates on evolving
real-world websites, ensuring a high degree of realism and applicability to
natural user tasks. Our data includes 1,050 human-written tasks covering
various domains such as shopping and travel, with each task requiring the agent
to extract multimodal information from web pages as observations autonomously;
2) Multihop web browsing. Our dataset features naturally compositional tasks
that require information from or actions on multiple websites to solve, to
assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation.
We propose a novel protocol for evaluating an agent's progress in completing
multihop tasks. We experiment with both standalone (multimodal) language models
and heuristic-based web agents. Extensive experiments demonstrate that while
long-chain multihop web tasks are easy for humans, they remain challenging for
state-of-the-art web agents. We identify that agents are more likely to fail on
the early hops when solving tasks with more hops, which results in lower task
success rates. To address this issue, we propose a simple memory augmentation
approach that replays past action trajectories to reflect. Our method
significantly improves the performance of both the single-hop and multihop web
browsing abilities. Our code and data are available at
github.com/shulin16/MMInA.

</details>


### [355] [MadCLIP: Few-shot Medical Anomaly Detection with CLIP](https://arxiv.org/pdf/2506.23810)
*Mahshid Shiri, Cigdem Beyan, Vittorio Murino*

Main category: cs.CV

TL;DR: A novel few-shot anomaly detection method using CLIP for medical data, achieving top performance in classification and segmentation without synthetic data or memory banks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of anomaly detection in medical data with limited labeled examples, leveraging pre-trained models like CLIP for improved accuracy and adaptability.

Method: Uses a dual-branch design with learnable adapters in CLIP's vision encoder and text prompts for semantic alignment, along with SigLIP loss for handling unpaired text prompts.

Result: Outperforms existing methods in same-dataset and cross-dataset evaluations for anomaly classification and segmentation.

Conclusion: The approach is effective, adaptable, and validated across multiple modalities, with each component contributing to its success.

Abstract: An innovative few-shot anomaly detection approach is presented, leveraging
the pre-trained CLIP model for medical data, and adapting it for both
image-level anomaly classification (AC) and pixel-level anomaly segmentation
(AS). A dual-branch design is proposed to separately capture normal and
abnormal features through learnable adapters in the CLIP vision encoder. To
improve semantic alignment, learnable text prompts are employed to link visual
features. Furthermore, SigLIP loss is applied to effectively handle the
many-to-one relationship between images and unpaired text prompts, showcasing
its adaptation in the medical field for the first time. Our approach is
validated on multiple modalities, demonstrating superior performance over
existing methods for AC and AS, in both same-dataset and cross-dataset
evaluations. Unlike prior work, it does not rely on synthetic data or memory
banks, and an ablation study confirms the contribution of each component. The
code is available at https://github.com/mahshid1998/MadCLIP.

</details>


### [356] [Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model](https://arxiv.org/pdf/2506.23822)
*Shiming Chen, Bowen Duan, Salman Khan, Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: LaZSL is a locally-aligned vision-language model for interpretable zero-shot learning, using optimal transport to align visual regions with attributes, improving interpretability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs like CLIP lack interpretability in zero-shot learning due to global similarity computations. LaZSL aims to address this by aligning local visual features with discrete attributes.

Method: LaZSL employs optimal transport for local visual-semantic alignment, enabling interaction between visual regions and attributes without additional training.

Result: The method enhances interpretability, improves accuracy, and demonstrates strong domain generalization.

Conclusion: LaZSL provides an effective and interpretable solution for zero-shot learning, bridging the gap between local visual features and attributes.

Abstract: Large-scale vision-language models (VLMs), such as CLIP, have achieved
remarkable success in zero-shot learning (ZSL) by leveraging large-scale
visual-text pair datasets. However, these methods often lack interpretability,
as they compute the similarity between an entire query image and the embedded
category words, making it difficult to explain their predictions. One approach
to address this issue is to develop interpretable models by integrating
language, where classifiers are built using discrete attributes, similar to
human perception. This introduces a new challenge: how to effectively align
local visual features with corresponding attributes based on pre-trained VLMs.
To tackle this, we propose LaZSL, a locally-aligned vision-language model for
interpretable ZSL. LaZSL employs local visual-semantic alignment via optimal
transport to perform interaction between visual regions and their associated
attributes, facilitating effective alignment and providing interpretable
similarity without the need for additional training. Extensive experiments
demonstrate that our method offers several advantages, including enhanced
interpretability, improved accuracy, and strong domain generalization. Codes
available at: https://github.com/shiming-chen/LaZSL.

</details>


### [357] [Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/pdf/2506.23825)
*Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Xiaojie Jin*

Main category: cs.CV

TL;DR: Flash-VStream is an efficient video language model for processing long videos, reducing latency with a novel memory module.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with long videos due to computational overhead and inefficiency.

Method: Uses a Flash Memory module with low-capacity context memory and high-capacity augmentation memory for efficient processing.

Result: Achieves state-of-the-art performance on benchmarks like EgoSchema and MLVU, with reduced inference latency.

Conclusion: Flash-VStream offers efficient, real-time processing of long videos, outperforming existing methods.

Abstract: Benefiting from the advances in large language models and cross-modal
alignment, existing multimodal large language models have achieved prominent
performance in image and short video understanding. However, the understanding
of long videos is still challenging, as their long-context nature results in
significant computational and memory overhead. Most existing work treats long
videos in the same way as short videos, which is inefficient for real-world
applications and hard to generalize to even longer videos. To address these
issues, we propose Flash-VStream, an efficient video language model capable of
processing extremely long videos and responding to user queries in real time.
Particularly, we design a Flash Memory module, containing a low-capacity
context memory to aggregate long-context temporal information and model the
distribution of information density, and a high-capacity augmentation memory to
retrieve detailed spatial information based on this distribution. Compared to
existing models, Flash-VStream achieves significant reductions in inference
latency. Extensive experiments on long video benchmarks and comprehensive video
benchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate
the state-of-the-art performance and outstanding efficiency of our method. Code
is available at https://github.com/IVGSZ/Flash-VStream.

</details>


### [358] [Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning](https://arxiv.org/pdf/2506.23827)
*Mingcheng Qu, Yuncong Wu, Donglin Di, Yue Gao, Tonghua Su, Yang Song, Lei Fan*

Main category: cs.CV

TL;DR: NH2ST is a framework for predicting gene expression from pathology images by integrating spatial context and multi-modal data, outperforming existing methods by over 20% in PCC metrics.


<details>
  <summary>Details</summary>
Motivation: Existing methods for predicting gene expression from pathology images often ignore spatial and molecular interactions, leading to poor performance. NH2ST addresses this gap by incorporating neighboring information and cross-modal relationships.

Method: NH2ST uses a query branch and a neighbor branch to process target patches and their neighboring regions, employing cross-attention and contrastive learning to align pathology and gene expression data.

Result: The model outperforms existing methods, achieving over 20% improvement in PCC metrics across six datasets.

Conclusion: NH2ST effectively captures spatial and cross-modal relationships, offering a robust solution for gene expression prediction from pathology images.

Abstract: Spatial transcriptomics (ST) provides crucial insights into tissue
micro-environments, but is limited to its high cost and complexity. As an
alternative, predicting gene expression from pathology whole slide images (WSI)
is gaining increasing attention. However, existing methods typically rely on
single patches or a single pathology modality, neglecting the complex spatial
and molecular interactions between target and neighboring information (e.g.,
gene co-expression). This leads to a failure in establishing connections among
adjacent regions and capturing intricate cross-modal relationships. To address
these issues, we propose NH2ST, a framework that integrates spatial context and
both pathology and gene modalities for gene expression prediction. Our model
comprises a query branch and a neighbor branch to process paired target patch
and gene data and their neighboring regions, where cross-attention and
contrastive learning are employed to capture intrinsic associations and ensure
alignments between pathology and gene expression. Extensive experiments on six
datasets demonstrate that our model consistently outperforms existing methods,
achieving over 20% in PCC metrics. Codes are available at
https://github.com/MCPathology/NH2ST

</details>


### [359] [Low-latency vision transformers via large-scale multi-head attention](https://arxiv.org/pdf/2506.23832)
*Ronit D. Gross, Tal Halevi, Ella Koresh, Yarden Tzach, Ido Kanter*

Main category: cs.CV

TL;DR: The paper explores spontaneous symmetry breaking in multi-head attention (MHA) in transformers, generalizing it to large-scale MHA (LS-MHA) for improved classification accuracy and reduced latency.


<details>
  <summary>Details</summary>
Motivation: To understand and leverage the learning mechanism of MHA in transformers, particularly how heads focus on subsets of labels, and to improve classification tasks by enhancing signal-to-noise ratio (SNR).

Method: Quantifies single-nodal performance (SNP) and single-head performance (SHP) to analyze MHA behavior, proposes LS-MHA, and substitutes initial transformer blocks with convolutional layers for efficiency.

Result: Improved classification accuracy due to higher SNR, distinct ViT architectures with similar accuracy but different LS-MHA structures, and reduced latency without accuracy loss.

Conclusion: The findings suggest potential extensions to NLP tasks and highlight the advantages of combining convolutional layers with transformers for efficient learning.

Abstract: The emergence of spontaneous symmetry breaking among a few heads of
multi-head attention (MHA) across transformer blocks in classification tasks
was recently demonstrated through the quantification of single-nodal
performance (SNP). This finding indicates that each head focuses its attention
on a subset of labels through cooperation among its SNPs. This underlying
learning mechanism is generalized to large-scale MHA (LS-MHA) using a single
matrix value representing single-head performance (SHP), analogous to
single-filter performance in convolutional neural networks (CNNs). The results
indicate that each SHP matrix comprises multiple unit clusters such that each
label being explicitly recognized by a few heads with negligible noise. This
leads to an increased signal-to-noise ratio (SNR) along the transformer blocks,
thereby improving classification accuracy. These features give rise to several
distinct vision transformer (ViT) architectures that achieve the same accuracy
but differ in their LS-MHA structures. As a result, their soft committee yields
superior accuracy, an outcome not typically observed in CNNs which rely on
hundreds of filters. In addition, a significant reduction in latency is
achieved without affecting the accuracy by replacing the initial transformer
blocks with convolutional layers. This substitution accelerates early-stage
learning, which is then improved by subsequent transformer layers. The
extension of this learning mechanism to natural language processing tasks,
based on quantitative differences between CNNs and ViT architectures, has the
potential to yield new insights in deep learning. The findings are demonstrated
using compact convolutional transformer architectures trained on the CIFAR-100
dataset.

</details>


### [360] [PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric](https://arxiv.org/pdf/2506.23833)
*Oscar Ovanger, Ragnar Hauge, Jacob Skauvold, Michael J. Pyrcz, Jo Eidsvik*

Main category: cs.CV

TL;DR: PointSSIM is a resolution-invariant metric for comparing binary images by transforming them into marked point patterns and using summary vectors for robust analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of comparing binary images of varying resolutions while maintaining structural integrity.

Method: Transforms binary images into marked point patterns, extracts anchor points via locally adaptive maxima, and compares using summary vectors capturing intensity, connectivity, complexity, and structure.

Result: Efficient and reliable image comparison, especially for structural analysis across resolutions.

Conclusion: PointSSIM is a promising tool for applications needing resolution-invariant structural image comparison.

Abstract: This paper presents PointSSIM, a novel low-dimensional image-to-image
comparison metric that is resolution invariant. Drawing inspiration from the
structural similarity index measure and mathematical morphology, PointSSIM
enables robust comparison across binary images of varying resolutions by
transforming them into marked point pattern representations. The key features
of the image, referred to as anchor points, are extracted from binary images by
identifying locally adaptive maxima from the minimal distance transform. Image
comparisons are then performed using a summary vector, capturing intensity,
connectivity, complexity, and structural attributes. Results show that this
approach provides an efficient and reliable method for image comparison,
particularly suited to applications requiring structural analysis across
different resolutions.

</details>


### [361] [Refine Any Object in Any Scene](https://arxiv.org/pdf/2506.23835)
*Ziwei Chen, Ziling Liu, Zitong Huang, Mingqi Gao, Feng Zheng*

Main category: cs.CV

TL;DR: RAISE introduces a 3D enhancement framework to recover object geometry and appearance in scenes with missing views, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of high-fidelity object-level modeling in scenes with missing views, critical for detailed object understanding.

Method: A two-stage refinement: substituting degraded objects with proxies via a 3D generative model, then refining geometry and texture by aligning proxies and correcting inconsistencies.

Result: RAISE significantly outperforms state-of-the-art methods in novel view synthesis and geometry completion tasks.

Conclusion: RAISE effectively enhances object geometry and appearance in scenes with missing views, advancing detailed object modeling.

Abstract: Viewpoint missing of objects is common in scene reconstruction, as camera
paths typically prioritize capturing the overall scene structure rather than
individual objects. This makes it highly challenging to achieve high-fidelity
object-level modeling while maintaining accurate scene-level representation.
Addressing this issue is critical for advancing downstream tasks requiring
detailed object understanding and appearance modeling. In this paper, we
introduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement
framework that leverages 3D generative priors to recover fine-grained object
geometry and appearance under missing views. Starting from substituting
degraded objects with proxies, via a 3D generative model with strong 3D
understanding, RAISE progressively refines geometry and texture by aligning
each proxy to its degraded counterpart in 7-DOF pose, followed by correcting
spatial and appearance inconsistencies via registration-constrained
enhancement. This two-stage refinement ensures the high-fidelity geometry and
appearance of the original object in unseen views while maintaining consistency
in spatial positioning, observed geometry, and appearance. Extensive
experiments on challenging benchmarks show that RAISE significantly outperforms
state-of-the-art methods in both novel view synthesis and geometry completion
tasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.

</details>


### [362] [RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment](https://arxiv.org/pdf/2506.23852)
*Jianing Jin, Jiangyong Ying, Huiyu Duan, Liu Yang, Sijing Wu, Yunhao Li, Yushuo Zheng, Xiongkuo Min, Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces Robotic-Generated Content (RGC) and its unique quality challenges, establishes the first RGC database (RGCD), and evaluates existing VQA models, revealing their limitations for RGC.


<details>
  <summary>Details</summary>
Motivation: The rise of robotic-generated videos necessitates dedicated quality assessment, as existing models fail to address RGC's unique distortions and visual requirements.

Method: The authors create the RGCD with 2,100 videos, conduct subjective VQA experiments, and benchmark 11 state-of-the-art VQA models.

Result: Existing VQA models perform poorly on RGC, indicating a need for specialized models.

Conclusion: The study highlights the gap in RGC quality assessment and provides a foundational database for future research.

Abstract: As camera-equipped robotic platforms become increasingly integrated into
daily life, robotic-generated videos have begun to appear on streaming media
platforms, enabling us to envision a future where humans and robots coexist. We
innovatively propose the concept of Robotic-Generated Content (RGC) to term
these videos generated from egocentric perspective of robots. The perceptual
quality of RGC videos is critical in human-robot interaction scenarios, and RGC
videos exhibit unique distortions and visual requirements that differ markedly
from those of professionally-generated content (PGC) videos and user-generated
content (UGC) videos. However, dedicated research on quality assessment of RGC
videos is still lacking. To address this gap and to support broader robotic
applications, we establish the first Robotic-Generated Content Database (RGCD),
which contains a total of 2,100 videos drawn from three robot categories and
sourced from diverse platforms. A subjective VQA experiment is conducted
subsequently to assess human visual perception of robotic-generated videos.
Finally, we conduct a benchmark experiment to evaluate the performance of 11
state-of-the-art VQA models on our database. Experimental results reveal
significant limitations in existing VQA models when applied to complex,
robotic-generated content, highlighting a critical need for RGC-specific VQA
models. Our RGCD is publicly available at:
https://github.com/IntMeGroup/RGC-VQA.

</details>


### [363] [Interpretable Interaction Modeling for Trajectory Prediction via Agent Selection and Physical Coefficient](https://arxiv.org/pdf/2405.13152)
*Shiji Huang, Lei Ye, Min Chen, Wenhai Luo, Dihong Wang, Chenqi Xu, Deyuan Liang*

Main category: cs.CV

TL;DR: ASPILin improves trajectory prediction by manually selecting interacting agents and using physical correlation coefficients, enhancing interpretability and performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: To enhance the interpretability and accuracy of trajectory prediction by addressing the limitations of purely learning-based methods in modeling interactions between agents.

Method: Manually selects interacting agents and replaces Transformer attention scores with physical correlation coefficients, simplifying other aspects like map encoding.

Result: Outperforms state-of-the-art methods on INTERACTION, highD, and CitySim datasets, with improved efficiency and reduced computational costs.

Conclusion: ASPILin demonstrates that simple, interpretable modifications can significantly enhance trajectory prediction performance and computational efficiency.

Abstract: A thorough understanding of the interaction between the target agent and
surrounding agents is a prerequisite for accurate trajectory prediction.
Although many methods have been explored, they assign correlation coefficients
to surrounding agents in a purely learning-based manner. In this study, we
present ASPILin, which manually selects interacting agents and replaces the
attention scores in Transformer with a newly computed physical correlation
coefficient, enhancing the interpretability of interaction modeling.
Surprisingly, these simple modifications can significantly improve prediction
performance and substantially reduce computational costs. We intentionally
simplified our model in other aspects, such as map encoding. Remarkably,
experiments conducted on the INTERACTION, highD, and CitySim datasets
demonstrate that our method is efficient and straightforward, outperforming
other state-of-the-art methods.

</details>


### [364] [HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](https://arxiv.org/pdf/2506.23854)
*Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Xianpeng Lang*

Main category: cs.CV

TL;DR: HiNeuS is a unified framework for neural surface reconstruction that addresses multi-view radiance inconsistency, missing keypoints, and structural degradation through differential visibility verification, planar-conformal regularization, and physically-grounded Eikonal relaxation. It outperforms baselines in accuracy and detail preservation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with geometric fidelity and photometric consistency under complex conditions, leading to issues like radiance inconsistency and structural degradation.

Method: HiNeuS integrates differential visibility verification, planar-conformal regularization, and dynamic Eikonal relaxation into a unified pipeline for joint optimization.

Result: The method achieves a 21.4% reduction in Chamfer distance and 2.32 dB PSNR improvement, excelling in recovering specular surfaces and low-textured regions.

Conclusion: HiNeuS offers a cohesive solution for neural surface reconstruction, validated by superior performance and generalizability in inverse rendering tasks.

Abstract: Neural surface reconstruction faces persistent challenges in reconciling
geometric fidelity with photometric consistency under complex scene conditions.
We present HiNeuS, a unified framework that holistically addresses three core
limitations in existing approaches: multi-view radiance inconsistency, missing
keypoints in textureless regions, and structural degradation from over-enforced
Eikonal constraints during joint optimization. To resolve these issues through
a unified pipeline, we introduce: 1) Differential visibility verification
through SDF-guided ray tracing, resolving reflection ambiguities via continuous
occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry
patches that enforce local surface coherence while preserving sharp edges
through adaptive appearance weighting; and 3) Physically-grounded Eikonal
relaxation that dynamically modulates geometric constraints based on local
radiance gradients, enabling detail preservation without sacrificing global
regularity. Unlike prior methods that handle these aspects through sequential
optimizations or isolated modules, our approach achieves cohesive integration
where appearance-geometry constraints evolve synergistically throughout
training. Comprehensive evaluations across synthetic and real-world datasets
demonstrate state-of-the-art performance, including a 21.4% reduction in
Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement
against neural rendering counterparts. Qualitative analyses reveal superior
capability in recovering specular instruments, urban layouts with
centimeter-scale infrastructure, and low-textured surfaces without local patch
collapse. The method's generalizability is further validated through successful
application to inverse rendering tasks, including material decomposition and
view-consistent relighting.

</details>


### [365] [Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models](https://arxiv.org/pdf/2405.14715)
*Young Kyun Jang, Ser-nam Lim*

Main category: cs.CV

TL;DR: The paper introduces Cross-modal Backward-compatible Training (XBT) to align new and old model embeddings in cross-modal retrieval, reducing the need for costly backfilling.


<details>
  <summary>Details</summary>
Motivation: Modern retrieval systems face challenges upgrading models due to embedding incompatibility, requiring expensive backfilling.

Method: Proposes a projection module pretrained with text data to map new embeddings to old ones, minimizing image-text pairs needed and avoiding old model use during training.

Result: XBT effectively enables backward-compatibility in Vision-Language Pretraining models, as shown in cross-modal retrieval experiments.

Conclusion: XBT offers a practical solution for backfill-free upgrades in cross-modal retrieval systems.

Abstract: Modern retrieval systems often struggle with upgrading to new and more
powerful models due to the incompatibility of embeddings between the old and
new models. This necessitates a costly process known as backfilling, which
involves re-computing the embeddings for a large number of data samples. In
vision, Backward-compatible Training (BT) has been proposed to ensure that the
new model aligns with the old model's embeddings. This paper extends the
concept of vision-only BT to the field of cross-modal retrieval, marking the
first attempt to address Cross-modal BT (XBT). Our goal is to achieve
backward-compatibility between Vision-Language Pretraining (VLP) models, such
as CLIP, for the cross-modal retrieval task. To address XBT challenges, we
propose an efficient solution: a projection module that maps the new model's
embeddings to those of the old model. This module, pretrained solely with text
data, significantly reduces the number of image-text pairs required for XBT
learning, and, once it is pretrained, it avoids using the old model during
training. Furthermore, we utilize parameter-efficient training strategies that
improve efficiency and preserve the off-the-shelf new model's knowledge by
avoiding any modifications. Experimental results on cross-modal retrieval
datasets demonstrate the effectiveness of XBT and its potential to enable
backfill-free upgrades when a new VLP model emerges.

</details>


### [366] [A Closer Look at Conditional Prompt Tuning for Vision-Language Models](https://arxiv.org/pdf/2506.23856)
*Ji Zhang, Shihan Wu, Lianli Gao, Jingkuan Song, Nicu Sebe, Heng Tao Shen*

Main category: cs.CV

TL;DR: The paper identifies suboptimal performance in existing conditional Prompt Tuning (PT) methods due to Visual Image Information (VII)-conditioned prompts and proposes Class-adaptive Prompt Tuning (CaPT) using Textual Class Information (TCI) to solve the Base-New Tradeoff (BNT) problem. CaPT improves performance across 11 datasets with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: To address the BNT dilemma in Vision-Language Pretrained Models (VLPMs), where tuning to base tasks reduces generalization to new tasks, and to improve upon existing conditional PT methods.

Method: Proposes CaPT, which learns TCI-conditioned prompts from base classes for fast adaptation to new tasks. Also integrates CaPT with DePT to create DeCaPT, a new conditional PT approach.

Result: CaPT consistently enhances five unconditional PT baselines across 11 datasets. DeCaPT outperforms the state-of-the-art conditional PT by 3.49% in H ACC.

Conclusion: CaPT effectively mitigates the BNT problem and can be integrated with existing PT methods for improved performance, as demonstrated by DeCaPT.

Abstract: Despite the great promise of Prompt Tuning (PT) in adapting large
Vision-Language Pretrained Models (VLPMs) to downstream tasks, they often
struggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better
tuned to a base task, their ability to generalize to new tasks diminishes.
Recent work on conditional PT addresses this problem by replacing static
prompts with dynamic Visual Image Information (VII)-conditioned prompts,
improving the model's generalization to new tasks to some extent. In this work,
we first identify a critical issue with existing conditional PT methods: using
VII as the "condition" of prompts yields suboptimal performance, and even
random noise-conditioned prompts can outperform the VII-conditioned
counterparts. On further analysis, we find that learning dynamic prompts
conditioned on Textual Class Information (TCI) is the key to solving the BNT
problem. Motivated by this, we then propose Class-adaptive Prompt Tuning
(CaPT), which enables fast adaptation of tuned models to new classes by
learning TCI-conditioned prompts from base classes. Remarkably, CaPT can be
used as a plugin to mitigate the BNT problem for existing unconditional PT
schemes. Extensive experiments on 11 datasets show that CaPT consistently
improves the performance of five strong unconditional PT baselines with
negligible additional computational cost. Additionally, by integrating CaPT
with our recently proposed DePT framework, we devise a new conditional PT
approach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art
conditional PT scheme by 3.49%, averaged over the 11 datasets. Code:
https://github.com/Koorye/CaPT.

</details>


### [367] [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](https://arxiv.org/pdf/2506.23858)
*Jianzong Wu, Liang Hou, Haotian Yang, Xin Tao, Ye Tian, Pengfei Wan, Di Zhang, Yunhai Tong*

Main category: cs.CV

TL;DR: VMoBA introduces a sparse attention mechanism for Video Diffusion Models (VDMs) to address quadratic complexity, improving efficiency and generation quality.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of full attention in VDMs hinders long-duration, high-resolution video generation. Existing sparse methods are suboptimal for video data.

Method: VMoBA adapts MoBA with layer-wise block partitioning, global block selection, and threshold-based block selection to optimize spatio-temporal attention.

Result: VMoBA achieves 2.92x FLOPs and 1.48x latency speedup in training, with comparable/superior quality to full attention. Inference speedups are 2.40x FLOPs and 1.35x latency.

Conclusion: VMoBA effectively addresses attention bottlenecks in VDMs, offering significant efficiency gains without compromising quality.

Abstract: The quadratic complexity of full attention mechanisms poses a significant
bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,
high-resolution videos. While various sparse attention methods have been
proposed, many are designed as training-free inference accelerators or do not
optimally capture the unique spatio-temporal characteristics inherent in video
data when trained natively. This paper introduces Video Mixture of Block
Attention (VMoBA), a novel sparse attention mechanism specifically adapted for
VDMs. Motivated by an in-depth analysis of attention patterns within
pre-trained video transformers, which revealed strong spatio-temporal locality,
varying query importance, and head-specific concentration levels, VMoBA
enhances the original MoBA framework with three key modifications: (1) a
layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to
diverse spatio-temporal attention patterns and improve efficiency; (2) global
block selection to prioritize the most salient query-key block interactions
across an entire attention head; and (3) threshold-based block selection to
dynamically determine the number of attended blocks based on their cumulative
similarity. Extensive experiments demonstrate that VMoBA significantly
accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and
1.48x latency speedup, while attaining comparable or even superior generation
quality to full attention. Furthermore, VMoBA exhibits competitive performance
in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for
high-res video generation.

</details>


### [368] [Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding](https://arxiv.org/pdf/2503.13377)
*Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, Xiangnan Fang, Zewen He, Zhenbo Luo, Wenxuan Wang, Junqi Lin, Jian Luan, Qin Jin*

Main category: cs.CV

TL;DR: The paper proposes a reinforcement learning (RL)-based post-training framework to enhance the generalization of Large Vision-Language Models (LVLMs) for Temporal Video Grounding (TVG). It introduces Time-R1, TimeRFT, and TVGBench, achieving state-of-the-art results with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs lack generalization in TVG tasks despite supervised fine-tuning. The paper aims to improve this via RL.

Method: A post-training framework using RL (Time-R1), data-efficient strategies (TimeRFT), and a benchmark (TVGBench) for evaluation.

Result: Time-R1 achieves top performance with only 2.5K training data and enhances general video understanding.

Conclusion: The proposed RL-based framework effectively improves LVLMs' generalization for TVG, validated by strong experimental results.

Abstract: Temporal Video Grounding (TVG), the task of locating specific video segments
based on language queries, is a core challenge in long-form video
understanding. While recent Large Vision-Language Models (LVLMs) have shown
early promise in tackling TVG through supervised fine-tuning (SFT), their
abilities to generalize remain limited. To address this, we propose a novel
post-training framework that enhances the generalization capabilities of LVLMs
via reinforcement learning (RL). Specifically, our contributions span three key
directions: (1) Time-R1: we introduce a reasoning-guided post-training
framework via RL with verifiable reward to enhance the capabilities of LVLMs on
the TVG task. (2) TimeRFT: we explore data-efficient post-training strategies
on our curated RL-friendly dataset, which trains the model to progressively
comprehend difficult samples, leading to better generalization. (3) TVGBench:
we carefully construct a small yet comprehensive benchmark for LVLM evaluation,
assessing 11 types of queries and featuring balanced distributions across both
videos and queries. Extensive experiments demonstrate that Time-R1 achieves
state-of-the-art performance across multiple downstream datasets using only
2.5K training data, while improving its general video understanding
capabilities.

</details>


### [369] [Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction](https://arxiv.org/pdf/2506.23863)
*Jiahao Ma, Lei Wang, Miaomiao liu, David Ahmedt-Aristizabal, Chuong Nguyen*

Main category: cs.CV

TL;DR: Puzzles is a data augmentation method for multi-view 3D reconstruction, enhancing training data diversity and improving model performance without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DUST3R are limited by training data diversity and scale. Puzzles addresses this by synthesizing high-quality posed video-depth data from minimal input.

Method: Puzzles uses targeted image transformations to simulate diverse camera trajectories and realistic scene geometry, creating augmented training data.

Result: Models trained with Puzzles achieve comparable accuracy using only 10% of the original data, boosting performance in existing pipelines.

Conclusion: Puzzles effectively enhances data variety and model performance in 3D reconstruction, offering a scalable solution with minimal data requirements.

Abstract: Multi-view 3D reconstruction remains a core challenge in computer vision.
Recent methods, such as DUST3R and its successors, directly regress pointmaps
from image pairs without relying on known scene geometry or camera parameters.
However, the performance of these models is constrained by the diversity and
scale of available training data. In this work, we introduce Puzzles, a data
augmentation strategy that synthesizes an unbounded volume of high-quality
posed video-depth data from a single image or video clip. By simulating diverse
camera trajectories and realistic scene geometry through targeted image
transformations, Puzzles significantly enhances data variety. Extensive
experiments show that integrating Puzzles into existing video-based 3D
reconstruction pipelines consistently boosts performance without modifying the
underlying network architecture. Notably, models trained on only ten percent of
the original data augmented with Puzzles still achieve accuracy comparable to
those trained on the full dataset. Code is available at
https://jiahao-ma.github.io/puzzles/.

</details>


### [370] [PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View](https://arxiv.org/pdf/2506.23897)
*Longliang Liu, Miaojie Feng, Junda Cheng, Jijun Xiang, Xuan Zhu, Xin Yang*

Main category: cs.CV

TL;DR: PriOr-Flow is a dual-branch framework improving panoramic optical flow by leveraging orthogonal views to mitigate distortion, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Severe distortions in sphere-to-plane projections degrade optical flow performance, especially in polar regions.

Method: Uses Dual-Cost Collaborative Lookup (DCCL) and Ortho-Driven Distortion Compensation (ODDC) to refine motion features and reduce distortion.

Result: Achieves state-of-the-art performance on panoramic optical flow datasets.

Conclusion: PriOr-Flow effectively addresses distortion challenges in panoramic optical flow, setting a new benchmark.

Abstract: Panoramic optical flow enables a comprehensive understanding of temporal
dynamics across wide fields of view. However, severe distortions caused by
sphere-to-plane projections, such as the equirectangular projection (ERP),
significantly degrade the performance of conventional perspective-based optical
flow methods, especially in polar regions. To address this challenge, we
propose PriOr-Flow, a novel dual-branch framework that leverages the
low-distortion nature of the orthogonal view to enhance optical flow estimation
in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup
(DCCL) operator, which jointly retrieves correlation information from both the
primitive and orthogonal cost volumes, effectively mitigating distortion noise
during cost volume construction. Furthermore, our Ortho-Driven Distortion
Compensation (ODDC) module iteratively refines motion features from both
branches, further suppressing polar distortions. Extensive experiments
demonstrate that PriOr-Flow is compatible with various perspective-based
iterative optical flow methods and consistently achieves state-of-the-art
performance on publicly available panoramic optical flow datasets, setting a
new benchmark for wide-field motion estimation. The code is publicly available
at: https://github.com/longliangLiu/PriOr-Flow.

</details>


### [371] [Composing Parts for Expressive Object Generation](https://arxiv.org/pdf/2406.10197)
*Harsh Rangwani, Aishwarya Agarwal, Kuldeep Kulkarni, R. Venkatesh Babu, Srikrishna Karanam*

Main category: cs.CV

TL;DR: PartComposer enables fine-grained part-level control in text-to-image generation by localizing and modifying object parts without retraining, improving artistic control and novel compositions.


<details>
  <summary>Details</summary>
Motivation: Current models like Stable Diffusion struggle with fine-grained part-level attribute control in text prompts, often ignoring details or producing inconsistent outputs.

Method: PartComposer localizes object parts via denoising, applies localized diffusion for part-level attributes, and combines results without retraining the base model.

Result: The method provides precise part-level control, enabling novel compositions and outperforming baselines in qualitative and quantitative evaluations.

Conclusion: PartComposer offers a training-free solution for fine-grained part-level image generation, enhancing artistic control and generalization across domains.

Abstract: Image composition and generation are processes where the artists need control
over various parts of the generated images. However, the current
state-of-the-art generation models, like Stable Diffusion, cannot handle
fine-grained part-level attributes in the text prompts. Specifically, when
additional attribute details are added to the base text prompt, these
text-to-image models either generate an image vastly different from the image
generated from the base prompt or ignore the attribute details. To mitigate
these issues, we introduce PartComposer, a training-free method that enables
image generation based on fine-grained part-level attributes specified for
objects in the base text prompt. This allows more control for artists and
enables novel object compositions by combining distinctive object parts.
PartComposer first localizes object parts by denoising the object region from a
specific diffusion process. This enables each part token to be localized to the
right region. After obtaining part masks, we run a localized diffusion process
in each part region based on fine-grained part attributes and combine them to
produce the final image. All stages of PartComposer are based on repurposing a
pre-trained diffusion model, which enables it to generalize across domains. We
demonstrate the effectiveness of part-level control provided by PartComposer
through qualitative visual examples and quantitative comparisons with
contemporary baselines.

</details>


### [372] [Three-dimensional end-to-end deep learning for brain MRI analysis](https://arxiv.org/pdf/2506.23916)
*Radhika Juglan, Marta Ligero, Zunamys I. Carrero, Asier Rabasco, Tim Lenz, Leo Misera, Gregory Patrick Veldhuizen, Paul Kuntke, Hagen H. Kitzler, Sven Nebelung, Daniel Truhn, Jakob Nikolas Kather*

Main category: cs.CV

TL;DR: Simpler convolutional networks (SFCN) outperform complex architectures (DenseNet, Swin Transformers) in age and sex prediction from brain MRI, showing better generalizability across diverse cohorts.


<details>
  <summary>Details</summary>
Motivation: Assess generalizability of deep learning methods in brain imaging, focusing on age and sex prediction, given their neurobiological significance.

Method: Evaluated three 3D architectures (SFCN, DenseNet, Swin Transformers) using T1-weighted MRI from four cohorts (UKB, DLBS, PPMI, IXI).

Result: SFCN achieved superior performance (AUC 1.00 for sex, MAE 2.66 for age in UKB) and better generalizability in external datasets.

Conclusion: Simpler networks like SFCN are more effective and generalizable for brain image analysis than complex architectures.

Abstract: Deep learning (DL) methods are increasingly outperforming classical
approaches in brain imaging, yet their generalizability across diverse imaging
cohorts remains inadequately assessed. As age and sex are key neurobiological
markers in clinical neuroscience, influencing brain structure and disease risk,
this study evaluates three of the existing three-dimensional architectures,
namely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window
(Swin) Transformers, for age and sex prediction using T1-weighted MRI from four
independent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study
(DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy
controls), and Information eXtraction from Images (IXI, n=319). We found that
SFCN consistently outperformed more complex architectures with AUC of 1.00
[1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for
sex classification. For the age prediction task, SFCN demonstrated a mean
absolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across
external datasets. Pairwise DeLong and Wilcoxon signed-rank tests with
Bonferroni corrections confirmed SFCN's superiority over Swin Transformer
across most cohorts (p<0.017, for three comparisons). Explainability analysis
further demonstrates the regional consistency of model attention across cohorts
and specific to each task. Our findings reveal that simpler convolutional
networks outperform the denser and more complex attention-based DL
architectures in brain image analysis by demonstrating better generalizability
across different datasets.

</details>


### [373] [Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers](https://arxiv.org/pdf/2506.23918)
*Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, Yi R., Fung*

Main category: cs.CV

TL;DR: The paper surveys the shift from text-centric multimodal reasoning to models that dynamically 'think with images,' proposing a three-stage framework and reviewing methods, benchmarks, and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the 'semantic gap' between static visual inputs and symbolic reasoning by exploring how AI can dynamically use vision as a cognitive workspace, akin to human cognition.

Method: The survey outlines a three-stage evolution (external tool exploration, programmatic manipulation, intrinsic imagination) and reviews core methods, benchmarks, and applications.

Result: A structured framework for 'thinking with images' is established, alongside insights into current methods, evaluations, and applications.

Conclusion: The paper provides a roadmap for advancing multimodal AI by integrating dynamic visual reasoning, highlighting challenges and future directions.

Abstract: Recent progress in multimodal reasoning has been significantly advanced by
textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning
within language. This text-centric approach, however, treats vision as a
static, initial context, creating a fundamental "semantic gap" between rich
perceptual data and discrete symbolic thought. Human cognition often transcends
language, utilizing vision as a dynamic mental sketchpad. A similar evolution
is now unfolding in AI, marking a fundamental paradigm shift from models that
merely think about images to those that can truly think with images. This
emerging paradigm is characterized by models leveraging visual information as
intermediate steps in their thought process, transforming vision from a passive
input into a dynamic, manipulable cognitive workspace. In this survey, we chart
this evolution of intelligence along a trajectory of increasing cognitive
autonomy, which unfolds across three key stages: from external tool
exploration, through programmatic manipulation, to intrinsic imagination. To
structure this rapidly evolving field, our survey makes four key contributions.
(1) We establish the foundational principles of the think with image paradigm
and its three-stage framework. (2) We provide a comprehensive review of the
core methods that characterize each stage of this roadmap. (3) We analyze the
critical landscape of evaluation benchmarks and transformative applications.
(4) We identify significant challenges and outline promising future directions.
By providing this structured overview, we aim to offer a clear roadmap for
future research towards more powerful and human-aligned multimodal AI.

</details>


### [374] [Evaluating the Impact of Khmer Font Types on Text Recognition](https://arxiv.org/pdf/2506.23963)
*Vannkinh Nom, Souhail Bakkali, Muhammad Muzzamil Luqman, Mickael Coustaty, Jean-Marc Ogier*

Main category: cs.CV

TL;DR: The study evaluates how 19 Khmer fonts impact OCR accuracy, identifying high- and low-performing fonts for Khmer text recognition.


<details>
  <summary>Details</summary>
Motivation: Khmer script's font variety complicates OCR, necessitating an evaluation of font impact on recognition accuracy.

Method: Tested 19 Khmer fonts using Pytesseract to measure OCR performance.

Result: High accuracy: Khmer, Odor MeanChey, Siemreap, Sithi Manuss, Battambang. Low accuracy: iSeth First, Bayon, Dangrek.

Conclusion: Font choice is crucial for Khmer OCR optimization, aiding in developing better OCR systems.

Abstract: Text recognition is significantly influenced by font types, especially for
complex scripts like Khmer. The variety of Khmer fonts, each with its unique
character structure, presents challenges for optical character recognition
(OCR) systems. In this study, we evaluate the impact of 19 randomly selected
Khmer font types on text recognition accuracy using Pytesseract. The fonts
include Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong
Chhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen,
Metal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth
First. Our comparison of OCR performance across these fonts reveals that Khmer,
Odor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy,
while iSeth First, Bayon, and Dangrek perform poorly. This study underscores
the critical importance of font selection in optimizing Khmer text recognition
and provides valuable insights for developing more robust OCR systems.

</details>


### [375] [AirSketch: Generative Motion to Sketch](https://arxiv.org/pdf/2407.08906)
*Hui Xian Grace Lim, Xuanming Cui, Yogesh S Rawat, Ser-Nam Lim*

Main category: cs.CV

TL;DR: AirSketch enables marker-less air drawing by translating noisy hand motions into clean sketches using a self-supervised diffusion model, eliminating the need for costly AR/VR hardware.


<details>
  <summary>Details</summary>
Motivation: Current AR/VR air drawing tools are expensive, require markers, and demand skill for aesthetic results, limiting accessibility.

Method: A self-supervised training procedure with a controllable image diffusion model translates noisy hand tracking images into clean sketches.

Result: The model effectively produces refined sketches from noisy inputs, demonstrating the potential of controllable diffusion models for air drawing.

Conclusion: AirSketch advances marker-less air drawing and highlights broader applications of controllable diffusion models in AR/VR.

Abstract: Illustration is a fundamental mode of human expression and communication.
Certain types of motion that accompany speech can provide this illustrative
mode of communication. While Augmented and Virtual Reality technologies (AR/VR)
have introduced tools for producing drawings with hand motions (air drawing),
they typically require costly hardware and additional digital markers, thereby
limiting their accessibility and portability. Furthermore, air drawing demands
considerable skill to achieve aesthetic results. To address these challenges,
we introduce the concept of AirSketch, aimed at generating faithful and
visually coherent sketches directly from hand motions, eliminating the need for
complicated headsets or markers. We devise a simple augmentation-based
self-supervised training procedure, enabling a controllable image diffusion
model to learn to translate from highly noisy hand tracking images to clean,
aesthetically pleasing sketches, while preserving the essential visual cues
from the original tracking data. We present two air drawing datasets to study
this problem. Our findings demonstrate that beyond producing photo-realistic
images from precise spatial inputs, controllable image diffusion can
effectively produce a refined, clear sketch from a noisy input. Our work serves
as an initial step towards marker-less air drawing and reveals distinct
applications of controllable diffusion models to AirSketch and AR/VR in
general.

</details>


### [376] [Visual and Memory Dual Adapter for Multi-Modal Object Tracking](https://arxiv.org/pdf/2506.23972)
*Boyue Xu, Ruichao Hou, Tongwei Ren, Gangshan Wu*

Main category: cs.CV

TL;DR: A novel visual and memory dual adapter (VMDA) is proposed to enhance multi-modal tracking by leveraging frequency, spatial, and temporal cues, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-learning-based multi-modal trackers fail to fully exploit critical cues across frequency and temporal domains, limiting their robustness.

Method: VMDA combines a visual adapter for adaptive feature transfer and a memory adapter for temporal cue propagation, improving representation learning.

Result: The method achieves state-of-the-art performance in RGB-Thermal, RGB-Depth, and RGB-Event tracking tasks.

Conclusion: VMDA effectively addresses limitations in current approaches, offering a robust solution for multi-modal tracking.

Abstract: Prompt-learning-based multi-modal trackers have achieved promising progress
by employing lightweight visual adapters to incorporate auxiliary modality
features into frozen foundation models. However, existing approaches often
struggle to learn reliable prompts due to limited exploitation of critical cues
across frequency and temporal domains. In this paper, we propose a novel visual
and memory dual adapter (VMDA) to construct more robust and discriminative
representations for multi-modal tracking. Specifically, we develop a simple but
effective visual adapter that adaptively transfers discriminative cues from
auxiliary modality to dominant modality by jointly modeling the frequency,
spatial, and channel-wise features. Additionally, we design the memory adapter
inspired by the human memory mechanism, which stores global temporal cues and
performs dynamic update and retrieval operations to ensure the consistent
propagation of reliable temporal information across video sequences. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,
and RGB-Event tracking. Code and models are available at
https://github.com/xuboyue1999/mmtrack.git.

</details>


### [377] [Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance](https://arxiv.org/pdf/2506.23975)
*Yuliia Kaidashova, Bettina Finzel, Ute Schmid*

Main category: cs.CV

TL;DR: The paper introduces a concept-based method for contrastive explanations in image classification, showing that higher concept relevance leads to simpler explanations, while lower relevance results in more complex ones. Robustness varies under image augmentations.


<details>
  <summary>Details</summary>
Motivation: To understand why a classification model prefers one class over another for an input instance by providing contrastive explanations using human-understandable concepts.

Method: Leverages instance embedding similarity and concept relevance in a fine-tuned deep learning model, extracts concepts with relevance scores, computes contrasts for similar instances, and evaluates explanation complexity.

Result: Higher concept relevance leads to shorter, less complex explanations; lower relevance results in longer, more diffuse ones. Explanations show varying robustness under image augmentations.

Conclusion: The findings highlight the potential for building more interpretable and robust AI systems through concept-based contrastive explanations.

Abstract: Understanding why a classification model prefers one class over another for
an input instance is the challenge of contrastive explanation. This work
implements concept-based contrastive explanations for image classification by
leveraging the similarity of instance embeddings and relevance of
human-understandable concepts used by a fine-tuned deep learning model. Our
approach extracts concepts with their relevance score, computes contrasts for
similar instances, and evaluates the resulting contrastive explanations based
on explanation complexity. Robustness is tested for different image
augmentations. Two research questions are addressed: (1) whether explanation
complexity varies across different relevance ranges, and (2) whether
explanation complexity remains consistent under image augmentations such as
rotation and noise. The results confirm that for our experiments higher concept
relevance leads to shorter, less complex explanations, while lower relevance
results in longer, more diffuse explanations. Additionally, explanations show
varying degrees of robustness. The discussion of these findings offers insights
into the potential of building more interpretable and robust AI systems.

</details>


### [378] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/pdf/2506.21546)
*Xinzhuo Li, Adheesh Juvekar, Xingyou Liu, Muntasir Wahed, Kiet A. Nguyen, Ismini Lourentzou*

Main category: cs.CV

TL;DR: HalluSegBench is introduced as the first benchmark to evaluate hallucinations in vision-language segmentation using counterfactual visual reasoning, revealing vision-driven hallucinations as more prevalent than label-driven ones.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation protocols for segmentation hallucination focus on label or textual hallucinations without manipulating visual context, limiting their diagnostic capacity.

Method: HalluSegBench includes a dataset of 1340 counterfactual instance pairs and new metrics to quantify hallucination sensitivity under visually coherent scene edits.

Result: Experiments show vision-driven hallucinations are more prevalent than label-driven ones, with models often persisting in false segmentation.

Conclusion: Counterfactual reasoning is essential for diagnosing grounding fidelity in vision-language segmentation models.

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


### [379] [Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification](https://arxiv.org/pdf/2409.17777)
*Raja Kumar, Raghav Singhal, Pranamya Kulkarni, Deval Mehta, Kshitij Jadhav*

Main category: cs.CV

TL;DR: M3CoL introduces a Mixup-based contrastive learning approach to capture shared relations in multimodal data, outperforming state-of-the-art methods on several datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world data often has shared relations beyond simple pairwise associations, which existing methods fail to capture.

Method: Proposes M3CoL, combining Mixup-based contrastive loss with a fusion framework for multimodal classification.

Result: Outperforms state-of-the-art on N24News, ROSMAP, and BRCA; comparable on Food-101.

Conclusion: Learning shared relations enhances multimodal learning, with promising future research directions.

Abstract: Deep multimodal learning has shown remarkable success by leveraging
contrastive learning to capture explicit one-to-one relations across
modalities. However, real-world data often exhibits shared relations beyond
simple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive
Learning approach to capture nuanced shared relations inherent in multimodal
data. Our key contribution is a Mixup-based contrastive loss that learns robust
representations by aligning mixed samples from one modality with their
corresponding samples from other modalities thereby capturing shared relations
between them. For multimodal classification tasks, we introduce a framework
that integrates a fusion module with unimodal prediction modules for auxiliary
supervision during training, complemented by our proposed Mixup-based
contrastive loss. Through extensive experiments on diverse datasets (N24News,
ROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures
shared multimodal relations and generalizes across domains. It outperforms
state-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving
comparable performance on Food-101. Our work highlights the significance of
learning shared relations for robust multimodal learning, opening up promising
avenues for future research. Our code is publicly available at
https://github.com/RaghavSinghal10/M3CoL.

</details>


### [380] [StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving](https://arxiv.org/pdf/2506.23982)
*Ruiyang Hao, Bowen Jing, Haibao Yu, Zaiqing Nie*

Main category: cs.CV

TL;DR: The paper addresses the gap in personalized end-to-end autonomous driving (E2EAD) by introducing a large-scale dataset with diverse driving preferences and proposing a benchmark for evaluating personalized E2EAD models.


<details>
  <summary>Details</summary>
Motivation: Personalization is crucial for trust and comfort in autonomous vehicles but is overlooked in E2EAD due to the lack of datasets capturing diverse driving preferences.

Method: The authors create a dataset with static and dynamic annotations, use a visual language model (VLM) for scenario construction, and derive objective and subjective preference labels. Human verification ensures label quality.

Result: Incorporating personalized preferences improves alignment with human driving behavior, as demonstrated by evaluating state-of-the-art models.

Conclusion: The work establishes a foundation for personalized E2EAD, enabling systematic integration of human preferences and fostering human-centric autonomy research.

Abstract: While personalization has been explored in traditional autonomous driving
systems, it remains largely overlooked in end-to-end autonomous driving
(E2EAD), despite its growing prominence. This gap is critical, as user-aligned
behavior is essential for trust, comfort, and widespread adoption of autonomous
vehicles. A core challenge is the lack of large-scale real-world datasets
annotated with diverse and fine-grained driving preferences, hindering the
development and evaluation of personalized E2EAD models. In this work, we
present the first large-scale real-world dataset enriched with annotations
capturing diverse driving preferences, establishing a foundation for
personalization in E2EAD. We extract static environmental features from
real-world road topology and infer dynamic contextual cues using a fine-tuned
visual language model (VLM), enabling consistent and fine-grained scenario
construction. Based on these scenarios, we derive objective preference
annotations through behavioral distribution analysis and rule-based heuristics.
To address the inherent subjectivity of driving style, we further employ the
VLM to generate subjective annotations by jointly modeling scene semantics and
driver behavior. Final high-quality labels are obtained through a
human-in-the-loop verification process that fuses both perspectives. Building
on this dataset, we propose the first benchmark for evaluating personalized
E2EAD models. We assess several state-of-the-art models with and without
preference conditioning, demonstrating that incorporating personalized
preferences results in behavior more aligned with human driving. Our work lays
the foundation for personalized E2EAD by providing a standardized platform to
systematically integrate human preferences into data-driven E2EAD systems,
catalyzing future research in human-centric autonomy.

</details>


### [381] [Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data](https://arxiv.org/pdf/2506.24039)
*Shubhabrata Mukherjee, Jack Lang, Obeen Kwon, Iryna Zenyuk, Valerie Brogden, Adam Weber, Daniela Ushizima*

Main category: cs.CV

TL;DR: Zenesis is a no-code platform for scientific image analysis, outperforming traditional methods with high accuracy and efficiency, especially for scarce datasets.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot and prompt-based technologies struggle with scarce scientific image sets, necessitating a solution to minimize data readiness barriers.

Method: Zenesis uses lightweight multi-modal adaptation for zero-shot operation, human-in-the-loop refinement, and heuristic-based temporal enhancement.

Result: Zenesis achieves high accuracy (0.947-0.987), IOU (0.857-0.858), and Dice scores (0.923) on FIB-SEM data, outperforming baselines like Otsu and SAM.

Conclusion: Zenesis is a powerful tool for scientific imaging, especially where annotated datasets are scarce, enabling accurate analysis.

Abstract: Zero-shot and prompt-based technologies capitalized on using frequently
occurring images to transform visual reasoning tasks, which explains why such
technologies struggle with valuable yet scarce scientific image sets. In this
work, we propose Zenesis, a comprehensive no-code interactive platform designed
to minimize barriers posed by data readiness for scientific images. We develop
lightweight multi-modal adaptation techniques that enable zero-shot operation
on raw scientific data, along with human-in-the-loop refinement and
heuristic-based temporal enhancement options. We demonstrate the performance of
our approach through comprehensive comparison and validation on challenging
Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded
membranes. Zenesis significantly outperforms baseline methods, achieving an
average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a
Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an
IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results
mark a substantial improvement over traditional methods like Otsu thresholding
and even advanced models like Segment Anything Model (SAM) when used in
isolation. Our results demonstrate that Zenesis is a powerful tool for
scientific applications, particularly in fields where high-quality annotated
datasets are unavailable, accelerating accurate analysis of experimental
imaging.

</details>


### [382] [Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](https://arxiv.org/pdf/2506.24063)
*Deng Li, Aming Wu, Yang Li, Yaowei Wang, Yahong Han*

Main category: cs.CV

TL;DR: The paper proposes a novel method for continual test-time adaptation in object detection by generating specific parameters instead of fine-tuning, using a dual-path LoRA-based adapter and conditional diffusion, to improve generalization and avoid performance degradation.


<details>
  <summary>Details</summary>
Motivation: Environments change dynamically, challenging closed-set-trained object detectors. Fine-tuning specific parameters (e.g., BatchNorm) can degrade performance due to limited test images and interference with fixed parameters.

Method: Introduces a dual-path LoRA-based domain-aware adapter to separate domain-invariant and domain-specific features, a conditional diffusion-based parameter generation mechanism, and a class-centered optimal transport alignment to prevent forgetting.

Result: Experiments show the method's effectiveness in continuous domain adaptation tasks, with visualizations confirming improved object-related information capture and generalization.

Conclusion: The proposed approach enhances adaptability and generalization in dynamic environments, outperforming traditional fine-tuning methods.

Abstract: In practice, environments constantly change over time and space, posing
significant challenges for object detectors trained based on a closed-set
assumption, i.e., training and test data share the same distribution. To this
end, continual test-time adaptation has attracted much attention, aiming to
improve detectors' generalization by fine-tuning a few specific parameters,
e.g., BatchNorm layers. However, based on a small number of test images,
fine-tuning certain parameters may affect the representation ability of other
fixed parameters, leading to performance degradation. Instead, we explore a new
mechanism, i.e., converting the fine-tuning process to a specific-parameter
generation. Particularly, we first design a dual-path LoRA-based domain-aware
adapter that disentangles features into domain-invariant and domain-specific
components, enabling efficient adaptation. Additionally, a conditional
diffusion-based parameter generation mechanism is presented to synthesize the
adapter's parameters based on the current environment, preventing the
optimization from getting stuck in local optima. Finally, we propose a
class-centered optimal transport alignment method to mitigate catastrophic
forgetting. Extensive experiments conducted on various continuous domain
adaptive object detection tasks demonstrate the effectiveness. Meanwhile,
visualization results show that the representation extracted by the generated
parameters can capture more object-related information and strengthen the
generalization ability.

</details>


### [383] [Multimodal Object Detection using Depth and Image Data for Manufacturing Parts](https://arxiv.org/pdf/2411.09062)
*Nazanin Mahjourian, Vinh Nguyen*

Main category: cs.CV

TL;DR: A multimodal object detection system combining RGB and 3D point cloud data outperforms single-sensor baselines, enhancing reliability in manufacturing.


<details>
  <summary>Details</summary>
Motivation: Traditional 2D or 3D sensors have limitations (e.g., lack of depth or color), undermining industrial reliability.

Method: Proposes a calibrated RGB and 3D sensor system, extending Faster R-CNN to process multimodal data.

Result: Multimodal model improves mAP by 13% (vs. RGB-only) and 78% (vs. depth-only), with similar gains in Mean Precision.

Conclusion: The method enhances object detection robustness for smart manufacturing.

Abstract: Manufacturing requires reliable object detection methods for precise picking
and handling of diverse types of manufacturing parts and components.
Traditional object detection methods utilize either only 2D images from cameras
or 3D data from lidars or similar 3D sensors. However, each of these sensors
have weaknesses and limitations. Cameras do not have depth perception and 3D
sensors typically do not carry color information. These weaknesses can
undermine the reliability and robustness of industrial manufacturing systems.
To address these challenges, this work proposes a multi-sensor system combining
an red-green-blue (RGB) camera and a 3D point cloud sensor. The two sensors are
calibrated for precise alignment of the multimodal data captured from the two
hardware devices. A novel multimodal object detection method is developed to
process both RGB and depth data. This object detector is based on the Faster
R-CNN baseline that was originally designed to process only camera images. The
results show that the multimodal model significantly outperforms the depth-only
and RGB-only baselines on established object detection metrics. More
specifically, the multimodal model improves mAP by 13% and raises Mean
Precision by 11.8% in comparison to the RGB-only baseline. Compared to the
depth-only baseline, it improves mAP by 78% and raises Mean Precision by 57%.
Hence, this method facilitates more reliable and robust object detection in
service to smart manufacturing applications.

</details>


### [384] [MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction](https://arxiv.org/pdf/2506.24096)
*Antoine Guédon, Diego Gomez, Nissim Maruani, Bingchen Gong, George Drettakis, Maks Ovsjanikov*

Main category: cs.CV

TL;DR: MILo introduces a differentiable Gaussian Splatting framework to extract accurate surface meshes directly from 3D Gaussians, avoiding costly post-processing and preserving fine geometric details.


<details>
  <summary>Details</summary>
Motivation: Current methods for extracting surface meshes from Gaussian Splatting lose details or produce dense meshes, limiting downstream applications.

Method: MILo uses a fully differentiable process to construct meshes from Gaussians, featuring bidirectional consistency, adaptive mesh extraction, and signed distance computation.

Result: The method reconstructs high-quality scenes with fewer vertices, suitable for physics simulations and animation.

Conclusion: MILo bridges volumetric and surface representations, offering efficient and precise mesh extraction for 3D scenes.

Abstract: While recent advances in Gaussian Splatting have enabled fast reconstruction
of high-quality 3D scenes from images, extracting accurate surface meshes
remains a challenge. Current approaches extract the surface through costly
post-processing steps, resulting in the loss of fine geometric details or
requiring significant time and leading to very dense meshes with millions of
vertices. More fundamentally, the a posteriori conversion from a volumetric to
a surface representation limits the ability of the final mesh to preserve all
geometric structures captured during training. We present MILo, a novel
Gaussian Splatting framework that bridges the gap between volumetric and
surface representations by differentiably extracting a mesh from the 3D
Gaussians. We design a fully differentiable procedure that constructs the
mesh-including both vertex locations and connectivity-at every iteration
directly from the parameters of the Gaussians, which are the only quantities
optimized during training. Our method introduces three key technical
contributions: a bidirectional consistency framework ensuring both
representations-Gaussians and the extracted mesh-capture the same underlying
geometry during training; an adaptive mesh extraction process performed at each
training iteration, which uses Gaussians as differentiable pivots for Delaunay
triangulation; a novel method for computing signed distance values from the 3D
Gaussians that enables precise surface extraction while avoiding geometric
erosion. Our approach can reconstruct complete scenes, including backgrounds,
with state-of-the-art quality while requiring an order of magnitude fewer mesh
vertices than previous methods. Due to their light weight and empty interior,
our meshes are well suited for downstream applications such as physics
simulations or animation.

</details>


### [385] [Enhancing Diffusion Posterior Sampling for Inverse Problems by Integrating Crafted Measurements](https://arxiv.org/pdf/2411.09850)
*Shijie Zhou, Huaisheng Zhu, Rohan Sharma, Jiayi Chen, Ruiyi Zhang, Kaiyi Ji, Changyou Chen*

Main category: cs.CV

TL;DR: DPS-CM improves diffusion posterior sampling by using a crafted noisy measurement to reduce early high-frequency errors, enhancing performance in inverse problems like deblurring and super-resolution.


<details>
  <summary>Details</summary>
Motivation: High-frequency information introduced early in posterior sampling can cause errors, so a better method is needed to align with the diffusion prior.

Method: Proposes DPS-CM, which uses a crafted noisy measurement (reverse denoising process) instead of clean or forward-process noisy measurements for posterior sampling.

Result: Significantly outperforms existing methods in tasks like Gaussian deblurring, super-resolution, and noisy inverse problems.

Conclusion: DPS-CM effectively mitigates misalignment with the diffusion prior, improving restoration quality for various inverse problems.

Abstract: Diffusion models have emerged as a powerful foundation model for visual
generations. With an appropriate sampling process, it can effectively serve as
a generative prior for solving general inverse problems. Current posterior
sampling-based methods take the measurement (i.e., degraded image sample) into
the posterior sampling to infer the distribution of the target data (i.e.,
clean image sample). However, in this manner, we show that high-frequency
information can be prematurely introduced during the early stages, which could
induce larger posterior estimate errors during restoration sampling. To address
this observation, we first reveal that forming the log-posterior gradient with
the noisy measurement ( i.e., noisy measurement from a diffusion forward
process) instead of the clean one can benefit the early posterior sampling.
Consequently, we propose a novel diffusion posterior sampling method DPS-CM,
which incorporates a Crafted Measurement (i.e., noisy measurement crafted by a
reverse denoising process, rather than constructed from the diffusion forward
process) to form the posterior estimate. This integration aims to mitigate the
misalignment with the diffusion prior caused by cumulative posterior estimate
errors. Experimental results demonstrate that our approach significantly
improves the overall capacity to solve general and noisy inverse problems, such
as Gaussian deblurring, super-resolution, inpainting, nonlinear deblurring, and
tasks with Poisson noise, relative to existing approaches. Code is available
at: https://github.com/sjz5202/DPS-CM.

</details>


### [386] [DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World](https://arxiv.org/pdf/2506.24102)
*Xiangtai Li, Tao Zhang, Yanwei Li, Haobo Yuan, Shihao Chen, Yikang Zhou, Jiahao Meng, Yueyi Sun, Shilin Xu, Lu Qi, Tianheng Cheng, Yi Lin, Zilong Huang, Wenhao Huang, Jiashi Feng, Guang Shi*

Main category: cs.CV

TL;DR: DenseWorld-1M is a new dataset addressing gaps in grounded caption datasets by providing detailed, dense captions for high-resolution images, using a three-stage labeling pipeline and VLM models.


<details>
  <summary>Details</summary>
Motivation: Existing caption datasets lack detailed descriptions, relations, and object-level grounding for high-resolution images.

Method: A three-stage labeling pipeline (open-world perception, detailed object caption generation, dense caption merging) and two VLM models (Detailed Region Caption model, Spatial Caption Merging model) are used.

Result: The dataset and models improve performance in vision-language understanding, visual grounding, and region caption generation.

Conclusion: DenseWorld-1M fills a critical gap in multimodal datasets and enhances MLLM capabilities.

Abstract: Multimodal Large Language Models (MLLMs) demonstrate a complex understanding
of scenes, benefiting from large-scale and high-quality datasets. Most existing
caption datasets lack the ground locations and relations for visual entities.
Several grounded caption datasets face the problems of missing detailed
descriptions, relations, and massive object descriptions on high-resolution
images. To fill this gap for the community, we present DenseWorld-1M, the first
massive, detailed, dense grounded caption dataset in the real world. We design
a three-stage labeling pipeline, containing open-world perception, detailed
object caption generation, and dense caption merging. The first stage obtains
entity-level masks and labels. The second stage generates the object-level,
detailed captions with the guidance of masks and labels from the first stage.
The final stage merges object captions and masks into spatial and relational
dense captions. To accelerate the labeling process and improve caption quality,
we present two VLM models: the Detailed Region Caption model and the Spatial
Caption Merging model. Extensive experiments on various settings, including
vision-language understanding, visual grounding, and region caption generation,
demonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.

</details>


### [387] [Epona: Autoregressive Diffusion World Model for Autonomous Driving](https://arxiv.org/pdf/2506.24113)
*Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiao-Xiao Long, Xun Cao, Wei Yin*

Main category: cs.CV

TL;DR: Epona is an autoregressive diffusion world model for autonomous driving, improving long-horizon predictions and integrating trajectory planning via localized spatiotemporal modeling.


<details>
  <summary>Details</summary>
Motivation: Existing video diffusion models struggle with flexible-length, long-horizon predictions and integrating trajectory planning due to global joint distribution modeling.

Method: Epona uses decoupled spatiotemporal factorization and modular trajectory-video prediction, with a chain-of-forward training strategy to reduce autoregressive errors.

Result: Achieves 7.4% FVD improvement and longer prediction duration, outperforming prior works and serving as a real-time motion planner.

Conclusion: Epona advances video diffusion-based world modeling, enabling high-resolution, long-duration generation and effective motion planning.

Abstract: Diffusion models have demonstrated exceptional visual quality in video
generation, making them promising for autonomous driving world modeling.
However, existing video diffusion-based world models struggle with
flexible-length, long-horizon predictions and integrating trajectory planning.
This is because conventional video diffusion models rely on global joint
distribution modeling of fixed-length frame sequences rather than sequentially
constructing localized distributions at each timestep. In this work, we propose
Epona, an autoregressive diffusion world model that enables localized
spatiotemporal distribution modeling through two key innovations: 1) Decoupled
spatiotemporal factorization that separates temporal dynamics modeling from
fine-grained future world generation, and 2) Modular trajectory and video
prediction that seamlessly integrate motion planning with visual modeling in an
end-to-end framework. Our architecture enables high-resolution, long-duration
generation while introducing a novel chain-of-forward training strategy to
address error accumulation in autoregressive loops. Experimental results
demonstrate state-of-the-art performance with 7.4\% FVD improvement and minutes
longer prediction duration compared to prior works. The learned world model
further serves as a real-time motion planner, outperforming strong end-to-end
planners on NAVSIM benchmarks. Code will be publicly available at
\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.

</details>


### [388] [TextMesh4D: High-Quality Text-to-4D Mesh Generation](https://arxiv.org/pdf/2506.24121)
*Sisi Dai, Xinxin Su, Boyan Wan, Ruizhen Hu, Kai Xu*

Main category: cs.CV

TL;DR: TextMesh4D introduces a novel framework for high-quality text-to-4D generation, leveraging per-face Jacobians and a two-stage approach for static and dynamic synthesis, achieving state-of-the-art results with low GPU overhead.


<details>
  <summary>Details</summary>
Motivation: Dynamic 3D content generation (text-to-4D) remains underexplored despite advancements in diffusion models for images, videos, and 3D content.

Method: Uses per-face Jacobians for differentiable mesh representation, decomposes 4D generation into static object creation and dynamic motion synthesis, and introduces flexibility-rigidity regularization for stable optimization.

Result: Achieves state-of-the-art performance in temporal consistency, structural fidelity, and visual realism with low GPU memory usage (24GB).

Conclusion: TextMesh4D offers a cost-effective, high-quality solution for text-to-4D generation, with code release to support future research.

Abstract: Recent advancements in diffusion generative models significantly advanced
image, video, and 3D content creation from user-provided text prompts. However,
the challenging problem of dynamic 3D content generation (text-to-4D) with
diffusion guidance remains largely unexplored. In this paper, we introduce
TextMesh4D, a novel framework for high-quality text-to-4D generation. Our
approach leverages per-face Jacobians as a differentiable mesh representation
and decomposes 4D generation into two stages: static object creation and
dynamic motion synthesis. We further propose a flexibility-rigidity
regularization term to stabilize Jacobian optimization under video diffusion
priors, ensuring robust geometric performance. Experiments demonstrate that
TextMesh4D achieves state-of-the-art results in terms of temporal consistency,
structural fidelity, and visual realism. Moreover, TextMesh4D operates with a
low GPU memory overhead-requiring only a single 24GB GPU-offering a
cost-effective yet high-quality solution for text-driven 4D mesh generation.
The code will be released to facilitate future research in text-to-4D
generation.

</details>


### [389] [Pretrained Reversible Generation as Unsupervised Visual Representation Learning](https://arxiv.org/pdf/2412.01787)
*Rongkun Xue, Jinouwen Zhang, Yazhe Niu, Dazhong Shen, Bingqi Ma, Yu Liu, Jing Yang*

Main category: cs.CV

TL;DR: PRG leverages pretrained generative models for discriminative tasks by reversing the generative process, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To explore the underutilized potential of generative models in discriminative tasks and overcome limitations of prior generative classifiers.

Method: Pretrained Reversible Generation (PRG) extracts unsupervised representations by reversing a pretrained generative model's process, enabling flexible feature selection.

Result: PRG outperforms prior methods, achieving 78% top-1 accuracy on ImageNet (64x64) and robust performance in benchmarks.

Conclusion: PRG effectively repurposes generative models for discriminative tasks, offering generalizable and high-capacity feature extraction.

Abstract: Recent generative models based on score matching and flow matching have
significantly advanced generation tasks, but their potential in discriminative
tasks remains underexplored. Previous approaches, such as generative
classifiers, have not fully leveraged the capabilities of these models for
discriminative tasks due to their intricate designs. We propose Pretrained
Reversible Generation (PRG), which extracts unsupervised representations by
reversing the generative process of a pretrained continuous generation model.
PRG effectively reuses unsupervised generative models, leveraging their high
capacity to serve as robust and generalizable feature extractors for downstream
tasks. This framework enables the flexible selection of feature hierarchies
tailored to specific downstream tasks. Our method consistently outperforms
prior approaches across multiple benchmarks, achieving state-of-the-art
performance among generative model based methods, including 78% top-1 accuracy
on ImageNet at a resolution of 64*64. Extensive ablation studies, including
out-of-distribution evaluations, further validate the effectiveness of our
approach.PRG is available at https://github.com/opendilab/PRG.

</details>


### [390] [Calligrapher: Freestyle Text Image Customization](https://arxiv.org/pdf/2506.24123)
*Yue Ma, Qingyan Bai, Hao Ouyang, Ka Leong Cheng, Qiuyu Wang, Hongyu Liu, Zichen Liu, Haofan Wang, Jingye Chen, Yujun Shen, Qifeng Chen*

Main category: cs.CV

TL;DR: Calligrapher is a diffusion-based framework for digital calligraphy, combining text customization and artistic typography with self-distillation, localized style injection, and in-context generation for precise style control.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in typographic customization, such as style control and data dependency, to automate high-quality, visually consistent typography.

Method: Uses self-distillation to create a style-centric benchmark, a trainable style encoder for feature extraction, and in-context generation for style alignment.

Result: Accurately reproduces stylistic details and glyph positioning, outperforming traditional models in diverse fonts and design contexts.

Conclusion: Calligrapher enhances digital art, branding, and typographic design by automating high-quality typography.

Abstract: We introduce Calligrapher, a novel diffusion-based framework that
innovatively integrates advanced text customization with artistic typography
for digital calligraphy and design applications. Addressing the challenges of
precise style control and data dependency in typographic customization, our
framework incorporates three key technical contributions. First, we develop a
self-distillation mechanism that leverages the pre-trained text-to-image
generative model itself alongside the large language model to automatically
construct a style-centric typography benchmark. Second, we introduce a
localized style injection framework via a trainable style encoder, which
comprises both Qformer and linear layers, to extract robust style features from
reference images. An in-context generation mechanism is also employed to
directly embed reference images into the denoising process, further enhancing
the refined alignment of target styles. Extensive quantitative and qualitative
evaluations across diverse fonts and design contexts confirm Calligrapher's
accurate reproduction of intricate stylistic details and precise glyph
positioning. By automating high-quality, visually consistent typography,
Calligrapher surpasses traditional models, empowering creative practitioners in
digital art, branding, and contextual typographic design.

</details>


### [391] [ZipAR: Parallel Auto-regressive Image Generation through Spatial Locality](https://arxiv.org/pdf/2412.04062)
*Yefei He, Feng Chen, Yuanyu He, Shaoxuan He, Hong Zhou, Kaipeng Zhang, Bohan Zhuang*

Main category: cs.CV

TL;DR: ZipAR is a training-free, parallel decoding framework for accelerating auto-regressive visual generation by decoding multiple tokens simultaneously, reducing forward passes by up to 91%.


<details>
  <summary>Details</summary>
Motivation: Images exhibit local structures with minimal interdependence between distant regions, enabling parallel decoding of spatially adjacent tokens.

Method: Proposes a "next-set prediction" paradigm where tokens in adjacent regions are decoded in parallel alongside the original row-wise prediction.

Result: ZipAR reduces forward passes by up to 91% on the Emu3-Gen model without retraining.

Conclusion: ZipAR significantly improves generation efficiency by leveraging parallel decoding, making it a practical plug-and-play solution.

Abstract: In this paper, we propose ZipAR, a training-free, plug-and-play parallel
decoding framework for accelerating auto-regressive (AR) visual generation. The
motivation stems from the observation that images exhibit local structures, and
spatially distant regions tend to have minimal interdependence. Given a
partially decoded set of visual tokens, in addition to the original next-token
prediction scheme in the row dimension, the tokens corresponding to spatially
adjacent regions in the column dimension can be decoded in parallel, enabling
the ``next-set prediction'' paradigm. By decoding multiple tokens
simultaneously in a single forward pass, the number of forward passes required
to generate an image is significantly reduced, resulting in a substantial
improvement in generation efficiency. Experiments demonstrate that ZipAR can
reduce the number of model forward passes by up to 91% on the Emu3-Gen model
without requiring any additional retraining. Code is available here:
https://github.com/ThisisBillhe/ZipAR.

</details>


### [392] [How to Design and Train Your Implicit Neural Representation for Video Compression](https://arxiv.org/pdf/2506.24127)
*Matthew Gwilliam, Roy Zhang, Namitha Padmanabhan, Hongyang Du, Abhinav Shrivastava*

Main category: cs.CV

TL;DR: The paper introduces Rabbit NeRV (RNeRV), an improved implicit neural representation (INR) method for video compression, addressing slow encoding speeds and achieving better performance in quality and training time. It also explores hyper-networks for real-time encoding.


<details>
  <summary>Details</summary>
Motivation: Current INR methods for video compression suffer from slow encoding speeds due to per-sample network training, limiting practical adoption. The paper aims to improve efficiency and performance.

Method: The authors develop a library to analyze NeRV family methods, propose RNeRV for better performance, and investigate hyper-networks for real-time encoding by masking predicted INR weights.

Result: RNeRV achieves +1.27% PSNR improvement over alternatives under equal training time. Hyper-network experiments show 1.7% PSNR/MS-SSIM improvements at 0.037 bpp.

Conclusion: The paper advances INR-based video compression with RNeRV and hyper-networks, improving quality and addressing encoding speed issues.

Abstract: Implicit neural representation (INR) methods for video compression have
recently achieved visual quality and compression ratios that are competitive
with traditional pipelines. However, due to the need for per-sample network
training, the encoding speeds of these methods are too slow for practical
adoption. We develop a library to allow us to disentangle and review the
components of methods from the NeRV family, reframing their performance in
terms of not only size-quality trade-offs, but also impacts on training time.
We uncover principles for effective video INR design and propose a
state-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When
all methods are given equal training time (equivalent to 300 NeRV epochs) for 7
different UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared
to the best-performing alternative for each video in our NeRV library. We then
tackle the encoding speed issue head-on by investigating the viability of
hyper-networks, which predict INR weights from video inputs, to disentangle
training from encoding to allow for real-time encoding. We propose masking the
weights of the predicted INR during training to allow for variable, higher
quality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at
0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by
0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar
speeds. Our project website is available at https://mgwillia.github.io/vinrb/
and our code is available at https://github.com/mgwillia/vinrb.

</details>


### [393] [AlignGuard: Scalable Safety Alignment for Text-to-Image Generation](https://arxiv.org/pdf/2412.10493)
*Runtao Liu, I Chieh Chen, Jindong Gu, Jipeng Zhang, Renjie Pi, Qifeng Chen, Philip Torr, Ashkan Khakzar, Fabio Pizzati*

Main category: cs.CV

TL;DR: AlignGuard introduces a method for safety alignment in T2I models using synthetic datasets and DPO, outperforming baselines by removing 7x more harmful concepts.


<details>
  <summary>Details</summary>
Motivation: Current T2I models lack robust safety measures, exposing users to harmful content. Existing methods are limited in scope and effectiveness.

Method: AlignGuard uses synthetic datasets (CoProV2) and a custom DPO strategy to train safety experts (LoRA matrices), merging them for optimal performance.

Result: AlignGuard removes 7x more harmful concepts than baselines and outperforms state-of-the-art benchmarks.

Conclusion: AlignGuard sets new standards for safety alignment in T2I models, offering scalability and effectiveness.

Abstract: Text-to-image (T2I) models are widespread, but their limited safety
guardrails expose end users to harmful content and potentially allow for model
misuse. Current safety measures are typically limited to text-based filtering
or concept removal strategies, able to remove just a few concepts from the
model's generative capabilities. In this work, we introduce AlignGuard, a
method for safety alignment of T2I models. We enable the application of Direct
Preference Optimization (DPO) for safety purposes in T2I models by
synthetically generating a dataset of harmful and safe image-text pairs, which
we call CoProV2. Using a custom DPO strategy and this dataset, we train safety
experts, in the form of low-rank adaptation (LoRA) matrices, able to guide the
generation process away from specific safety-related concepts. Then, we merge
the experts into a single LoRA using a novel merging strategy for optimal
scaling performance. This expert-based approach enables scalability, allowing
us to remove 7x more harmful concepts from T2I models compared to baselines.
AlignGuard consistently outperforms the state-of-the-art on many benchmarks and
establishes new practices for safety alignment in T2I networks. Code and data
will be shared at https://safetydpo.github.io/.

</details>


### [394] [Environment-Driven Online LiDAR-Camera Extrinsic Calibration](https://arxiv.org/pdf/2502.00801)
*Zhiwei Huang, Jiaqi Li, Ping Zhong, Rui Fan*

Main category: cs.CV

TL;DR: EdO-LCEC is an environment-driven online LiDAR-camera extrinsic calibration method that improves accuracy by leveraging feature density and dual-path correspondence matching, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-camera calibration methods rely on fixed scenes or targets, limiting practicality. EdO-LCEC aims to overcome these limitations by adapting to the environment dynamically.

Method: EdO-LCEC uses a scene discriminator to observe feature density, extracts LiDAR intensity and depth features, and employs dual-path correspondence matching (DPCM) for cross-modal feature matching. It models calibration as a joint optimization problem with global constraints.

Result: EdO-LCEC achieves higher accuracy than state-of-the-art methods, especially in sparse or partially overlapping sensor views, as validated by real-world experiments.

Conclusion: EdO-LCEC offers a practical and accurate solution for LiDAR-camera extrinsic calibration, addressing limitations of traditional methods and excelling in dynamic environments.

Abstract: LiDAR-camera extrinsic calibration (LCEC) is crucial for multi-modal data
fusion in mechatronics. Existing methods, whether target-based or target-free,
typically rely on customized calibration targets or fixed scene types, limiting
their practicality in real-world applications. To address these challenges, we
introduce EdO-LCEC, the first environment-driven online calibration approach.
Unlike traditional target-free methods, EdO-LCEC observes the feature density
of the application environment through a generalizable scene discriminator.
Based on this feature density, EdO-LCEC extracts LiDAR intensity and depth
features from varying perspectives to achieve higher calibration accuracy. To
overcome the challenges of cross-modal feature matching between LiDAR and
camera, we propose dual-path correspondence matching (DPCM), which leverages
both structural and textural consistency for reliable 3D-2D correspondences.
Additionally, our approach models the calibration process as a joint
optimization problem utilizing global constraints from multiple views and
scenes to enhance accuracy. Extensive experiments on real-world datasets
demonstrate that EdO-LCEC outperforms state-of-the-art methods, particularly in
sparse or partially overlapping sensor views.

</details>


### [395] [Cluster and Predict Latent Patches for Improved Masked Image Modeling](https://arxiv.org/pdf/2502.08769)
*Timothée Darcet, Federico Baldassarre, Maxime Oquab, Julien Mairal, Piotr Bojanowski*

Main category: cs.CV

TL;DR: CAPI is a novel MIM framework using latent clustering prediction, achieving competitive performance with simple linear probes.


<details>
  <summary>Details</summary>
Motivation: Existing MIM models underperform compared to state-of-the-art methods, prompting a systematic analysis of target representations, loss functions, and architectures.

Method: Introduces CAPI, a pure-MIM framework with a clustering-based loss for stable training and scaling. Uses ViT-L backbone.

Result: Achieves 83.8% accuracy on ImageNet and 32.1% mIoU on ADE20K, outperforming prior MIM methods and nearing DINOv2's performance.

Conclusion: CAPI demonstrates the potential of MIM with clustering-based loss, offering a scalable and effective alternative to current methods.

Abstract: Masked Image Modeling (MIM) offers a promising approach to self-supervised
representation learning, however existing MIM models still lag behind the
state-of-the-art. In this paper, we systematically analyze target
representations, loss functions, and architectures, to introduce CAPI - a novel
pure-MIM framework that relies on the prediction of latent clusterings. Our
approach leverages a clustering-based loss, which is stable to train, and
exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%
accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,
substantially outperforming previous MIM methods and approaching the
performance of the current state-of-the-art, DINOv2. We release all our code
and models.

</details>


### [396] [HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning](https://arxiv.org/pdf/2503.00436)
*Maria Lymperaiou, Giorgos Filandrianos, Angeliki Dimitriou, Athanasios Voulodimos, Giorgos Stamou*

Main category: cs.CV

TL;DR: The paper explores hallucinations in vision-language models, proposing HalCECE, a framework using conceptual counterfactuals for interpretable and efficient hallucination detection and analysis.


<details>
  <summary>Details</summary>
Motivation: Understanding and addressing hallucinatory phenomena in vision-language models is crucial for trustworthy AI systems.

Method: Uses conceptual counterfactual explanations to detect and analyze hallucinations, focusing on semantic edits and hierarchical knowledge.

Result: HalCECE provides interpretable, semantically meaningful edits and thorough hallucination analysis, including role hallucinations.

Conclusion: HalCECE advances explainable hallucination detection in vision-language models, enhancing trust in AI evaluations.

Abstract: In the dynamic landscape of artificial intelligence, the exploration of
hallucinations within vision-language (VL) models emerges as a critical
frontier. This work delves into the intricacies of hallucinatory phenomena
exhibited by widely used image captioners, unraveling interesting patterns.
Specifically, we step upon previously introduced techniques of conceptual
counterfactual explanations to address VL hallucinations. The deterministic and
efficient nature of the employed conceptual counterfactuals backbone is able to
suggest semantically minimal edits driven by hierarchical knowledge, so that
the transition from a hallucinated caption to a non-hallucinated one is
performed in a black-box manner. HalCECE, our proposed hallucination detection
framework is highly interpretable, by providing semantically meaningful edits
apart from standalone numbers, while the hierarchical decomposition of
hallucinated concepts leads to a thorough hallucination analysis. Another
novelty tied to the current work is the investigation of role hallucinations,
being one of the first works to involve interconnections between visual
concepts in hallucination detection. Overall, HalCECE recommends an explainable
direction to the crucial field of VL hallucination detection, thus fostering
trustworthy evaluation of current and future VL systems.

</details>


### [397] [How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects](https://arxiv.org/pdf/2503.04257)
*Wonkwang Lee, Jongwon Jeong, Taehong Moon, Hyeon-Jong Kim, Jaehyeon Kim, Gunhee Kim, Byeong-Uk Lee*

Main category: cs.CV

TL;DR: The paper addresses motion synthesis challenges for diverse objects by augmenting the Truebones Zoo dataset, introducing rig augmentation techniques, and redesigning motion diffusion models to handle varying skeletal templates.


<details>
  <summary>Details</summary>
Motivation: Motion synthesis for diverse objects is limited by the lack of comprehensive datasets and methods for heterogeneous skeletal templates.

Method: Augmenting the Truebones Zoo dataset with text annotations, introducing rig augmentation, and adapting motion diffusion models for arbitrary skeletal templates.

Result: The method generates high-fidelity motions from text for diverse and unseen objects.

Conclusion: The approach sets a foundation for motion synthesis across diverse object categories and skeletal templates.

Abstract: Motion synthesis for diverse object categories holds great potential for 3D
content creation but remains underexplored due to two key challenges: (1) the
lack of comprehensive motion datasets that include a wide range of high-quality
motions and annotations, and (2) the absence of methods capable of handling
heterogeneous skeletal templates from diverse objects. To address these
challenges, we contribute the following: First, we augment the Truebones Zoo
dataset, a high-quality animal motion dataset covering over 70 species, by
annotating it with detailed text descriptions, making it suitable for
text-based motion synthesis. Second, we introduce rig augmentation techniques
that generate diverse motion data while preserving consistent dynamics,
enabling models to adapt to various skeletal configurations. Finally, we
redesign existing motion diffusion models to dynamically adapt to arbitrary
skeletal templates, enabling motion synthesis for a diverse range of objects
with varying structures. Experiments show that our method learns to generate
high-fidelity motions from textual descriptions for diverse and even unseen
objects, setting a strong foundation for motion synthesis across diverse object
categories and skeletal templates. Qualitative results are available at:
$\href{https://t2m4lvo.github.io}{https://t2m4lvo.github.io}$.

</details>


### [398] [GUNNEL: Guided Mixup Augmentation and Multi-Model Fusion for Aquatic Animal Segmentation](https://arxiv.org/pdf/2112.06193)
*Minh-Quan Le, Trung-Nghia Le, Tam V. Nguyen, Isao Echizen, Minh-Triet Tran*

Main category: cs.CV

TL;DR: A new dataset "Aquatic Animal Species" and a method GUNNEL are introduced for aquatic animal segmentation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Lack of challenging datasets for benchmarking aquatic animal segmentation despite advances in object segmentation research.

Method: Proposes GUNNEL, combining guided mixup augmentation and multi-model fusion to improve segmentation and training performance.

Result: GUNNEL outperforms state-of-the-art instance segmentation methods in experiments.

Conclusion: The new dataset and GUNNEL method advance aquatic animal segmentation, with code and dataset publicly available.

Abstract: Recent years have witnessed great advances in object segmentation research.
In addition to generic objects, aquatic animals have attracted research
attention. Deep learning-based methods are widely used for aquatic animal
segmentation and have achieved promising performance. However, there is a lack
of challenging datasets for benchmarking. In this work, we build a new dataset
dubbed "Aquatic Animal Species." We also devise a novel GUided mixup
augmeNtatioN and multi-modEl fusion for aquatic animaL segmentation (GUNNEL)
that leverages the advantages of multiple segmentation models to segment
aquatic animals effectively and improves the training performance by
synthesizing hard samples. Extensive experiments demonstrated the superiority
of our proposed framework over existing state-of-the-art instance segmentation
methods. The code is available at https://github.com/lmquan2000/mask-mixup. The
dataset is available at https://doi.org/10.5281/zenodo.8208877.

</details>


### [399] [Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines](https://arxiv.org/pdf/2206.00535)
*Camilo Fosco, Emilie Josephs, Alex Andonian, Aude Oliva*

Main category: cs.CV

TL;DR: The paper introduces a framework to amplify artifacts in deepfake videos, making them more detectable by humans using a novel Artifact Attention module and 'Deepfake Caricatures'.


<details>
  <summary>Details</summary>
Motivation: Deepfakes are increasingly hard to detect visually, and current models lack human-alerting methods, necessitating a human-centered approach.

Method: Proposes a semi-supervised Artifact Attention module trained on human responses to highlight and magnify video artifacts, creating 'Deepfake Caricatures'.

Result: User study shows Caricatures significantly improve human detection, and the module enhances model accuracy and robustness.

Conclusion: The human-centered approach effectively improves deepfake detection and mitigation.

Abstract: Deepfakes can fuel online misinformation. As deepfakes get harder to
recognize with the naked eye, human users become more reliant on deepfake
detection models to help them decide whether a video is real or fake.
Currently, models yield a prediction for a video's authenticity, but do not
integrate a method for alerting a human user. We introduce a framework for
amplifying artifacts in deepfake videos to make them more detectable by people.
We propose a novel, semi-supervised Artifact Attention module, which is trained
on human responses to create attention maps that highlight video artifacts, and
magnify them to create a novel visual indicator we call "Deepfake Caricatures".
In a user study, we demonstrate that Caricatures greatly increase human
detection, across video presentation times and user engagement levels. We also
introduce a deepfake detection model that incorporates the Artifact Attention
module to increase its accuracy and robustness. Overall, we demonstrate the
success of a human-centered approach to designing deepfake mitigation methods.

</details>


### [400] [Neurons: Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction](https://arxiv.org/pdf/2503.11167)
*Haonan Wang, Qixiang Zhang, Lehan Wang, Xuanqi Huang, Xiaomeng Li*

Main category: cs.CV

TL;DR: NEURONS is a novel framework for fMRI-to-video reconstruction, decoupling learning into four sub-tasks to improve video consistency and semantic accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of integrating coarse fMRI data with detailed visual features for video reconstruction.

Method: Decouples learning into key object segmentation, concept recognition, scene description, and blurry video reconstruction, leveraging a pre-trained text-to-video diffusion model.

Result: Outperforms baselines with 26.6% improvement in video consistency and 19.1% in semantic accuracy, showing strong correlation with the visual cortex.

Conclusion: NEURONS demonstrates potential for brain-computer interfaces and clinical applications, with code and model weights publicly available.

Abstract: Decoding visual stimuli from neural activity is essential for understanding
the human brain. While fMRI methods have successfully reconstructed static
images, fMRI-to-video reconstruction faces challenges due to the need for
capturing spatiotemporal dynamics like motion and scene transitions. Recent
approaches have improved semantic and perceptual alignment but struggle to
integrate coarse fMRI data with detailed visual features. Inspired by the
hierarchical organization of the visual system, we propose NEURONS, a novel
framework that decouples learning into four correlated sub-tasks: key object
segmentation, concept recognition, scene description, and blurry video
reconstruction. This approach simulates the visual cortex's functional
specialization, allowing the model to capture diverse video content. In the
inference stage, NEURONS generates robust conditioning signals for a
pre-trained text-to-video diffusion model to reconstruct the videos. Extensive
experiments demonstrate that NEURONS outperforms state-of-the-art baselines,
achieving solid improvements in video consistency (26.6%) and semantic-level
accuracy (19.1%). Notably, NEURONS shows a strong functional correlation with
the visual cortex, highlighting its potential for brain-computer interfaces and
clinical applications. Code and model weights are available at:
https://github.com/xmed-lab/NEURONS.

</details>


### [401] [RefVSR++: Exploiting Reference Inputs for Reference-based Video Super-resolution](https://arxiv.org/pdf/2307.02897)
*Han Zou, Masanori Suganuma, Takayuki Okatani*

Main category: cs.CV

TL;DR: RefVSR++ improves video super-resolution by independently aggregating LR and Ref videos temporally, outperforming prior methods by over 1dB in PSNR.


<details>
  <summary>Details</summary>
Motivation: Multi-camera systems with varying FoVs create content differences in videos, enabling enhanced VSR by leveraging higher-resolution Ref videos.

Method: RefVSR++ aggregates LR and Ref videos in parallel, with improved temporal feature alignment.

Result: RefVSR++ exceeds previous methods by over 1dB in PSNR.

Conclusion: RefVSR++ sets a new benchmark in VSR by optimizing data usage and temporal alignment.

Abstract: Smartphones with multi-camera systems, featuring cameras with varying
field-of-views (FoVs), are increasingly common. This variation in FoVs results
in content differences across videos, paving the way for an innovative approach
to video super-resolution (VSR). This method enhances the VSR performance of
lower resolution (LR) videos by leveraging higher resolution reference (Ref)
videos. Previous works, which operate on this principle, generally expand on
traditional VSR models by combining LR and Ref inputs over time into a unified
stream. However, we can expect that better results are obtained by
independently aggregating these Ref image sequences temporally. Therefore, we
introduce an improved method, RefVSR++, which performs the parallel aggregation
of LR and Ref images in the temporal direction, aiming to optimize the use of
the available data. RefVSR++ also incorporates improved mechanisms for aligning
image features over time, crucial for effective VSR. Our experiments
demonstrate that RefVSR++ outperforms previous works by over 1dB in PSNR,
setting a new benchmark in the field.

</details>


### [402] [Accurate and lightweight dehazing via multi-receptive-field non-local network and novel contrastive regularization](https://arxiv.org/pdf/2309.16494)
*Zewei He, Zixuan Chen, Jinlei Li, Ziqian Lu, Xuecheng Sun, Hao Luo, Zhe-Ming Lu, Evangelos K. Markakis*

Main category: cs.CV

TL;DR: A multi-receptive-field non-local network (MRFNLN) is proposed for image dehazing, combining multi-scale feature extraction and attention mechanisms with a novel cross non-local block and detail-focused contrastive regularization, achieving state-of-the-art performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based dehazing methods lack effective feature extraction and underutilize non-local networks, despite their success in other vision tasks.

Method: MRFNLN integrates multi-stream feature attention blocks (MSFAB) for multi-scale feature extraction and attention, and cross non-local blocks (CNLB) for long-range dependencies. A spatial pyramid down-sampling strategy reduces computation, and detail-focused contrastive regularization (DFCR) enhances low-level details.

Result: The model outperforms state-of-the-art dehazing methods with less than 1.5 million parameters.

Conclusion: MRFNLN effectively addresses feature extraction and non-local network application in dehazing, offering a lightweight yet high-performing solution.

Abstract: Recently, deep learning-based methods have dominated image dehazing domain.
Although very competitive dehazing performance has been achieved with
sophisticated models, effective solutions for extracting useful features are
still under-explored. In addition, non-local network, which has made a
breakthrough in many vision tasks, has not been appropriately applied to image
dehazing. Thus, a multi-receptive-field non-local network (MRFNLN) consisting
of the multi-stream feature attention block (MSFAB) and cross non-local block
(CNLB) is presented in this paper. We start with extracting richer features for
dehazing. Specifically, we design a multi-stream feature extraction (MSFE)
sub-block, which contains three parallel convolutions with different receptive
fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$) for extracting multi-scale
features. Following MSFE, we employ an attention sub-block to make the model
adaptively focus on important channels/regions. The MSFE and attention
sub-blocks constitute our MSFAB. Then, we design a cross non-local block
(CNLB), which can capture long-range dependencies beyond the query. Instead of
the same input source of query branch, the key and value branches are enhanced
by fusing more preceding features. CNLB is computation-friendly by leveraging a
spatial pyramid down-sampling (SPDS) strategy to reduce the computation and
memory consumption without sacrificing the performance. Last but not least, a
novel detail-focused contrastive regularization (DFCR) is presented by
emphasizing the low-level details and ignoring the high-level semantic
information in the representation space. Comprehensive experimental results
demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art
dehazing methods with less than 1.5 Million parameters.

</details>


### [403] [D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for Few-shot Action Recognition](https://arxiv.org/pdf/2312.01431)
*Wenjie Pei, Qizhong Tan, Guangming Lu, Jiandong Tian, Jun Yu*

Main category: cs.CV

TL;DR: A novel video adapter tuning framework, D²ST-Adapter, is proposed for few-shot action recognition, featuring lightweight design and powerful spatio-temporal adaptation.


<details>
  <summary>Details</summary>
Motivation: To adapt pre-trained image models to video modality effectively, especially for few-shot action recognition, leveraging disentangled spatial and temporal feature encoding.

Method: D²ST-Adapter uses a dual-pathway architecture with anisotropic Deformable Spatio-Temporal Attention for disentangled feature encoding in a lightweight design.

Result: Outperforms state-of-the-art methods when instantiated on pre-trained ResNet and ViT, excelling in scenarios where temporal dynamics are crucial.

Conclusion: D²ST-Adapter is a superior, efficient solution for video model adaptation, particularly in challenging temporal-dynamic scenarios.

Abstract: Adapting pre-trained image models to video modality has proven to be an
effective strategy for robust few-shot action recognition. In this work, we
explore the potential of adapter tuning in image-to-video model adaptation and
propose a novel video adapter tuning framework, called
Disentangled-and-Deformable Spatio-Temporal Adapter (D$^2$ST-Adapter). It
features a lightweight design, low adaptation overhead and powerful
spatio-temporal feature adaptation capabilities. D$^2$ST-Adapter is structured
with an internal dual-pathway architecture that enables built-in disentangled
encoding of spatial and temporal features within the adapter, seamlessly
integrating into the single-stream feature learning framework of pre-trained
image models. In particular, we develop an efficient yet effective
implementation of the D$^2$ST-Adapter, incorporating the specially devised
anisotropic Deformable Spatio-Temporal Attention as its pivotal operation. This
mechanism can be individually tailored for two pathways with anisotropic
sampling densities along the spatial and temporal domains in 3D spatio-temporal
space, enabling disentangled encoding of spatial and temporal features while
maintaining a lightweight design. Extensive experiments by instantiating our
method on both pre-trained ResNet and ViT demonstrate the superiority of our
method over state-of-the-art methods. Our method is particularly well-suited to
challenging scenarios where temporal dynamics are critical for action
recognition. Code is available at https://github.com/qizhongtan/D2ST-Adapter.

</details>


### [404] [Visual Position Prompt for MLLM based Visual Grounding](https://arxiv.org/pdf/2503.15426)
*Wei Tang, Yanpeng Sun, Qinying Gu, Zechao Li*

Main category: cs.CV

TL;DR: VPP-LLaVA enhances MLLMs with Visual Position Prompt (VPP) to improve spatial alignment in visual grounding tasks, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with precise spatial alignment due to lack of explicit spatial references and weak localization capabilities.

Method: Introduces VPP-LLaVA with global and local VPP mechanisms, trained on the curated VPP-SFT dataset of 0.6M samples.

Result: Achieves top performance on visual grounding benchmarks and strong zero-shot generalization.

Conclusion: VPP-LLaVA effectively addresses MLLMs' spatial alignment limitations, offering a compact yet powerful solution.

Abstract: Although Multimodal Large Language Models (MLLMs) excel at various
image-related tasks, they encounter challenges in precisely aligning
coordinates with spatial information within images, particularly in
position-aware tasks such as visual grounding. This limitation arises from two
key factors. First, MLLMs lack explicit spatial references, making it difficult
to associate textual descriptions with precise image locations. Second, their
feature extraction processes prioritize global context over fine-grained
spatial details, leading to weak localization capability. To address these
issues, we introduce VPP-LLaVA, an MLLM enhanced with Visual Position Prompt
(VPP) to improve its grounding capability. VPP-LLaVA integrates two
complementary mechanisms: the global VPP overlays a learnable, axis-like tensor
onto the input image to provide structured spatial cues, while the local VPP
incorporates position-aware queries to support fine-grained localization.To
effectively train our model with spatial guidance, we further introduce
VPP-SFT, a curated dataset of 0.6M high-quality visual grounding samples.
Designed in a compact format, it enables efficient training and is
significantly smaller than datasets used by other MLLMs (e.g., ~21M samples in
MiniGPT-v2), yet still provides a strong performance boost. The resulting
model, VPP-LLaVA, not only achieves state-of-the-art results on standard visual
grounding benchmarks but also demonstrates strong zero-shot generalization to
challenging unseen datasets. Code and dataset will be released upon acceptance
at https://github.com/WayneTomas/VPP-LLaVA.

</details>


### [405] [Relating Events and Frames Based on Self-Supervised Learning and Uncorrelated Conditioning for Unsupervised Domain Adaptation](https://arxiv.org/pdf/2401.01042)
*Mohammad Rostami, Dayuan Jian, Ruitong Sun*

Main category: cs.CV

TL;DR: A new algorithm for adapting deep neural networks from frame-based to event-based camera data using unsupervised domain adaptation, self-supervised learning, and adversarial training.


<details>
  <summary>Details</summary>
Motivation: Overcoming the scarcity of annotated event-based data by leveraging annotated frame-based data.

Method: Combines uncorrelated conditioning and self-supervised learning in an adversarial scheme to align representations between domains.

Result: Outperforms existing approaches on benchmarks by effectively transferring knowledge from frame-based to event-based data.

Conclusion: The proposed algorithm successfully bridges the gap between frame-based and event-based vision, enhancing classification accuracy.

Abstract: Event-based cameras provide accurate and high temporal resolution
measurements for performing computer vision tasks in challenging scenarios,
such as high-dynamic range environments and fast-motion maneuvers. Despite
their advantages, utilizing deep learning for event-based vision encounters a
significant obstacle due to the scarcity of annotated data caused by the
relatively recent emergence of event-based cameras. To overcome this
limitation, leveraging the knowledge available from annotated data obtained
with conventional frame-based cameras presents an effective solution based on
unsupervised domain adaptation. We propose a new algorithm tailored for
adapting a deep neural network trained on annotated frame-based data to
generalize well on event-based unannotated data. Our approach incorporates
uncorrelated conditioning and self-supervised learning in an adversarial
learning scheme to close the gap between the two source and target domains. By
applying self-supervised learning, the algorithm learns to align the
representations of event-based data with those from frame-based camera data,
thereby facilitating knowledge transfer.Furthermore, the inclusion of
uncorrelated conditioning ensures that the adapted model effectively
distinguishes between event-based and conventional data, enhancing its ability
to classify event-based images accurately.Through empirical experimentation and
evaluation, we demonstrate that our algorithm surpasses existing approaches
designed for the same purpose using two benchmarks. The superior performance of
our solution is attributed to its ability to effectively utilize annotated data
from frame-based cameras and transfer the acquired knowledge to the event-based
vision domain.

</details>


### [406] [Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards](https://arxiv.org/pdf/2503.19948)
*Alexander Gambashidze, Konstantin Sobolev, Andrey Kuznetsov, Ivan Oseledets*

Main category: cs.CV

TL;DR: VLMs trained with reinforcement learning achieve competitive accuracy in capturing human visual preferences, offering transparent reasoning and better generalization.


<details>
  <summary>Details</summary>
Motivation: To determine if VLMs can effectively capture human visual preferences and improve decision-making processes with interpretable outcomes.

Method: Employ reinforcement learning on datasets like ImageReward and HPSv2, leveraging VLM capabilities for reasoning and generalization.

Result: Achieves 64.9% accuracy on ImageReward and 65.4% on HPSv2, matching traditional models while providing explainable results.

Conclusion: VLMs can reasonably capture human preferences, offering efficient soft-reward strategies for image ranking and enhancing text-to-vision models.

Abstract: Can Visual Language Models (VLMs) effectively capture human visual
preferences? This work addresses this question by training VLMs to think about
preferences at test time, employing reinforcement learning methods inspired by
DeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human
Preference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the
ImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2
(trained on approximately 25% of its data). These results match traditional
encoder-based models while providing transparent reasoning and enhanced
generalization. This approach allows to use not only rich VLM world knowledge,
but also its potential to think, yielding interpretable outcomes that help
decision-making processes. By demonstrating that human visual preferences
reasonable by current VLMs, we introduce efficient soft-reward strategies for
image ranking, outperforming simplistic selection or scoring methods. This
reasoning capability enables VLMs to rank arbitrary images-regardless of aspect
ratio or complexity-thereby potentially amplifying the effectiveness of visual
Preference Optimization. By reducing the need for extensive markup while
improving reward generalization and explainability, our findings can be a
strong mile-stone that will enhance text-to-vision models even further.

</details>


### [407] [Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey and Benchmark](https://arxiv.org/pdf/2402.02242)
*Yi Xin, Jianjiang Yang, Siqi Luo, Yuntao Du, Qi Qin, Kangrui Cen, Yangfan He, Bin Fu, Xiaokang Yang, Guangtao Zhai, Ming-Hsuan Yang, Xiaohong Liu*

Main category: cs.CV

TL;DR: A survey of parameter-efficient fine-tuning (PEFT) methods for pre-trained vision models, categorizing approaches, analyzing datasets, and introducing a benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of full fine-tuning for large-scale pre-trained vision models due to high computational and storage demands.

Method: Systematically reviews PEFT methodologies, categorizing them into four types: addition-based, partial-based, unified-based, and multi-task tuning. Introduces the V-PEFT Bench for standardized evaluation.

Result: Provides a comprehensive overview of PEFT advancements, successful applications, and a unified benchmark for fair comparison.

Conclusion: Highlights the potential of PEFT and outlines future research directions to advance the field.

Abstract: Pre-trained vision models (PVMs) have demonstrated remarkable adaptability
across a wide range of downstream vision tasks, showcasing exceptional
performance. However, as these models scale to billions or even trillions of
parameters, conventional full fine-tuning has become increasingly impractical
due to its high computational and storage demands. To address these challenges,
parameter-efficient fine-tuning (PEFT) has emerged as a promising alternative,
aiming to achieve performance comparable to full fine-tuning while making
minimal adjustments to the model parameters. This paper presents a
comprehensive survey of the latest advancements in the visual PEFT field,
systematically reviewing current methodologies and categorizing them into four
primary categories: addition-based, partial-based, unified-based, and
multi-task tuning. In addition, this paper offers an in-depth analysis of
widely used visual datasets and real-world applications where PEFT methods have
been successfully applied. Furthermore, this paper introduces the V-PEFT Bench,
a unified benchmark designed to standardize the evaluation of PEFT methods
across a diverse set of vision tasks, ensuring consistency and fairness in
comparison. Finally, the paper outlines potential directions for future
research to propel advances in the PEFT field. A comprehensive collection of
resources is available at
https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning.

</details>


### [408] [Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization](https://arxiv.org/pdf/2504.09039)
*Gen Li, Yang Xiao, Jie Ji, Kaiyuan Deng, Bo Hui, Linke Guo, Xiaolong Ma*

Main category: cs.CV

TL;DR: A novel unlearning framework for diffusion models, using Dynamic Mask and Concept-Aware Loss, improves multi-concept forgetting while maintaining image quality and semantic coherence.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for selective forgetting in diffusion models (e.g., removing copyrighted or harmful content) and overcoming limitations of existing unlearning methods in multi-concept scenarios.

Method: Proposes Dynamic Mask for adaptive gradient updates and Concept-Aware Loss for semantic consistency, combined with knowledge distillation to retain unlearned concepts.

Result: Outperforms existing methods in forgetting effectiveness, output fidelity, and semantic coherence, especially for multi-concept forgetting.

Conclusion: Provides a flexible and stable framework for high-fidelity unlearning in generative models, with public code release planned.

Abstract: Text-to-image (T2I) diffusion models have achieved remarkable success in
generating high-quality images from textual prompts. However, their ability to
store vast amounts of knowledge raises concerns in scenarios where selective
forgetting is necessary, such as removing copyrighted content, reducing biases,
or eliminating harmful concepts. While existing unlearning methods can remove
certain concepts, they struggle with multi-concept forgetting due to
instability, residual knowledge persistence, and generation quality
degradation. To address these challenges, we propose \textbf{Dynamic Mask
coupled with Concept-Aware Loss}, a novel unlearning framework designed for
multi-concept forgetting in diffusion models. Our \textbf{Dynamic Mask}
mechanism adaptively updates gradient masks based on current optimization
states, allowing selective weight modifications that prevent interference with
unrelated knowledge. Additionally, our \textbf{Concept-Aware Loss} explicitly
guides the unlearning process by enforcing semantic consistency through
superclass alignment, while a regularization loss based on knowledge
distillation ensures that previously unlearned concepts remain forgotten during
sequential unlearning. We conduct extensive experiments to evaluate our
approach. Results demonstrate that our method outperforms existing unlearning
techniques in forgetting effectiveness, output fidelity, and semantic
coherence, particularly in multi-concept scenarios. Our work provides a
principled and flexible framework for stable and high-fidelity unlearning in
generative models. The code will be released publicly.

</details>


### [409] [Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser](https://arxiv.org/pdf/2403.04444)
*Qingyuan Cai, Xuecai Hu, Saihui Hou, Li Yao, Yongzhen Huang*

Main category: cs.CV

TL;DR: DDHPose introduces a disentangled diffusion-based method for 3D human pose estimation, addressing hierarchical error accumulation with a novel Hierarchical Spatial and Temporal Denoiser (HSTDenoiser).


<details>
  <summary>Details</summary>
Motivation: Existing methods either underperform SOTA diffusion-based approaches or fail to fully exploit hierarchical skeleton information, leading to error propagation.

Method: Disentangles bone length and direction during diffusion, introduces a disentanglement loss, and employs HSTDenoiser with HRST and HRTT for hierarchical spatial and temporal modeling.

Result: Improves 3D pose estimation by better modeling human pose priors and hierarchical joint relationships.

Conclusion: DDHPose effectively addresses hierarchical error propagation and outperforms previous methods by leveraging disentangled diffusion and hierarchical modeling.

Abstract: Recently, diffusion-based methods for monocular 3D human pose estimation have
achieved state-of-the-art (SOTA) performance by directly regressing the 3D
joint coordinates from the 2D pose sequence. Although some methods decompose
the task into bone length and bone direction prediction based on the human
anatomical skeleton to explicitly incorporate more human body prior
constraints, the performance of these methods is significantly lower than that
of the SOTA diffusion-based methods. This can be attributed to the tree
structure of the human skeleton. Direct application of the disentangled method
could amplify the accumulation of hierarchical errors, propagating through each
hierarchy. Meanwhile, the hierarchical information has not been fully explored
by the previous methods. To address these problems, a Disentangled
Diffusion-based 3D Human Pose Estimation method with Hierarchical Spatial and
Temporal Denoiser is proposed, termed DDHPose. In our approach: (1) We
disentangle the 3D pose and diffuse the bone length and bone direction during
the forward process of the diffusion model to effectively model the human pose
prior. A disentanglement loss is proposed to supervise diffusion model
learning. (2) For the reverse process, we propose Hierarchical Spatial and
Temporal Denoiser (HSTDenoiser) to improve the hierarchical modeling of each
joint. Our HSTDenoiser comprises two components: the Hierarchical-Related
Spatial Transformer (HRST) and the Hierarchical-Related Temporal Transformer
(HRTT). HRST exploits joint spatial information and the influence of the parent
joint on each joint for spatial modeling, while HRTT utilizes information from
both the joint and its hierarchical adjacent joints to explore the hierarchical
temporal correlations among joints. Code and models are available at
https://github.com/Andyen512/DDHPose

</details>


### [410] [Object Retrieval for Visual Question Answering with Outside Knowledge](https://arxiv.org/pdf/2403.10798)
*Shichao Kan, Yuhai Deng, Jiale Fu, Lihui Cen, Zhe Qu, Linna Zhang, Yixiong Liang, Yigang Cen*

Main category: cs.CV

TL;DR: The paper introduces a novel unsupervised deep feature embedding technique (MS-GCEL) for object retrieval in visual question answering, addressing limitations of text/image-based RAG models.


<details>
  <summary>Details</summary>
Motivation: To extend RAG models beyond text/large-image retrieval by focusing on object-level retrieval for broader question-answering applications.

Method: Proposes MS-GCEL for learning embeddings of long-tailed objects at different scales and establishes benchmarks for evaluation.

Result: Demonstrates effectiveness in general object retrieval and OK-VQA tasks.

Conclusion: The approach enhances RAG models for visual question answering, with code and datasets to be released for future research.

Abstract: Retrieval-augmented generation (RAG) with large language models (LLMs) plays
a crucial role in question answering, as LLMs possess limited knowledge and are
not updated with continuously growing information. Most recent work on RAG has
focused primarily on text-based or large-image retrieval, which constrains the
broader application of RAG models. We recognize that object-level retrieval is
essential for addressing questions that extend beyond image content. To tackle
this issue, we propose a task of object retrieval for visual question answering
with outside knowledge (OR-OK-VQA), aimed to extend image-based content
understanding in conjunction with LLMs. A key challenge in this task is
retrieving diverse objects-related images that contribute to answering the
questions. To enable accurate and robust general object retrieval, it is
necessary to learn embeddings for local objects. This paper introduces a novel
unsupervised deep feature embedding technique called multi-scale group
collaborative embedding learning (MS-GCEL), developed to learn embeddings for
long-tailed objects at different scales. Additionally, we establish an OK-VQA
evaluation benchmark using images from the BelgaLogos, Visual Genome, and LVIS
datasets. Prior to the OK-VQA evaluation, we construct a benchmark of
challenges utilizing objects extracted from the COCO 2017 and VOC 2007 datasets
to support the training and evaluation of general object retrieval models. Our
evaluations on both general object retrieval and OK-VQA demonstrate the
effectiveness of the proposed approach. The code and dataset will be publicly
released for future research.

</details>


### [411] [Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix](https://arxiv.org/pdf/2505.08228)
*Unai Gurbindo, Axel Brando, Jaume Abella, Caroline König*

Main category: cs.CV

TL;DR: The paper proposes using Instruct Pix2Pix diffusion models for weather-based data augmentation to improve object detection robustness in adverse weather, tested in CARLA simulator and real-world datasets (BDD100K, ACDC).


<details>
  <summary>Details</summary>
Motivation: Enhancing object detection robustness under adverse weather is critical for autonomous driving.

Method: Leverages Instruct Pix2Pix diffusion model for weather-based data augmentation, tested on Faster R-CNN and YOLOv10 in CARLA and real-world datasets.

Result: Identifies performance gaps in adverse weather and shows tailored data augmentation improves model robustness.

Conclusion: Provides a foundation for reliable perception systems in challenging environments, advancing autonomous driving.

Abstract: Enhancing the robustness of object detection systems under adverse weather
conditions is crucial for the advancement of autonomous driving technology.
This study presents a novel approach leveraging the diffusion model Instruct
Pix2Pix to develop prompting methodologies that generate realistic datasets
with weather-based augmentations aiming to mitigate the impact of adverse
weather on the perception capabilities of state-of-the-art object detection
models, including Faster R-CNN and YOLOv10. Experiments were conducted in two
environments, in the CARLA simulator where an initial evaluation of the
proposed data augmentation was provided, and then on the real-world image data
sets BDD100K and ACDC demonstrating the effectiveness of the approach in real
environments.
  The key contributions of this work are twofold: (1) identifying and
quantifying the performance gap in object detection models under challenging
weather conditions, and (2) demonstrating how tailored data augmentation
strategies can significantly enhance the robustness of these models. This
research establishes a solid foundation for improving the reliability of
perception systems in demanding environmental scenarios, and provides a pathway
for future advancements in autonomous driving.

</details>


### [412] [Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention](https://arxiv.org/pdf/2403.11052)
*Jie Ren, Yaxin Li, Shenglai Zeng, Han Xu, Lingjuan Lyu, Yue Xing, Jiliang Tang*

Main category: cs.CV

TL;DR: The paper investigates memorization in text-to-image diffusion models, linking it to cross-attention mechanisms, and proposes a method to detect and mitigate memorization without affecting model speed or image quality.


<details>
  <summary>Details</summary>
Motivation: To address concerns about copyright infringement and privacy risks caused by diffusion models memorizing and replicating training images.

Method: Examines the relationship between memorization and cross-attention mechanisms, identifies intrinsic findings, and introduces a novel detection and mitigation approach.

Result: Reveals that memorization involves disproportionate focus on specific token embeddings and proposes an effective solution without compromising model performance.

Conclusion: The study provides insights into memorization in diffusion models and offers a practical method to mitigate risks while maintaining model efficiency and output quality.

Abstract: Recent advancements in text-to-image diffusion models have demonstrated their
remarkable capability to generate high-quality images from textual prompts.
However, increasing research indicates that these models memorize and replicate
images from their training data, raising tremendous concerns about potential
copyright infringement and privacy risks. In our study, we provide a novel
perspective to understand this memorization phenomenon by examining its
relationship with cross-attention mechanisms. We reveal that during
memorization, the cross-attention tends to focus disproportionately on the
embeddings of specific tokens. The diffusion model is overfitted to these token
embeddings, memorizing corresponding training images. To elucidate this
phenomenon, we further identify and discuss various intrinsic findings of
cross-attention that contribute to memorization. Building on these insights, we
introduce an innovative approach to detect and mitigate memorization in
diffusion models. The advantage of our proposed method is that it will not
compromise the speed of either the training or the inference processes in these
models while preserving the quality of generated images. Our code is available
at https://github.com/renjie3/MemAttn .

</details>


### [413] [SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for Imbalanced Clustering](https://arxiv.org/pdf/2404.03446)
*Chuyu Zhang, Hui Ren, Xuming He*

Main category: cs.CV

TL;DR: The paper introduces a deep imbalanced clustering framework using optimal transport-based pseudo-label learning to handle class imbalance in datasets.


<details>
  <summary>Details</summary>
Motivation: Existing deep clustering methods assume uniform class distributions, limiting practical use. This work addresses the challenge of imbalanced class distributions.

Method: Proposes SP$^2$OT, a semantic-regularized progressive partial optimal transport framework, solved via a projected mirror descent algorithm and unbalanced optimal transport.

Result: Demonstrates superiority on datasets like CIFAR100, ImageNet-R, and iNaturalist2018.

Conclusion: The SP$^2$OT framework effectively generates imbalance-aware pseudo-labels, advancing deep clustering for imbalanced datasets.

Abstract: Deep clustering, which learns representation and semantic clustering without
labels information, poses a great challenge for deep learning-based approaches.
Despite significant progress in recent years, most existing methods focus on
uniformly distributed datasets, significantly limiting the practical
applicability of their methods. In this paper, we propose a more practical
problem setting named deep imbalanced clustering, where the underlying classes
exhibit an imbalance distribution. To address this challenge, we introduce a
novel optimal transport-based pseudo-label learning framework. Our framework
formulates pseudo-label generation as a Semantic-regularized Progressive
Partial Optimal Transport (SP$^2$OT) problem, which progressively transports
each sample to imbalanced clusters under prior and semantic relation
constraints, thus generating high-quality and imbalance-aware pseudo-labels. To
solve the SP$^2$OT problem, we propose a projected mirror descent algorithm,
which alternates between: (1) computing the gradient of the SP$^2$OT objective,
and (2) performing gradient descent with projection via an entropy-regularized
progressive partial optimal transport formulation. Furthermore, we formulate
the second step as an unbalanced optimal transport problem with augmented
constraints and develop an efficient solution based on fast matrix scaling
algorithms. Experiments on various datasets, including a human-curated
long-tailed CIFAR100, challenging ImageNet-R, and large-scale subsets of
fine-grained iNaturalist2018 datasets, demonstrate the superiority of our
method. Code is available: https://github.com/rhfeiyang/SPPOT

</details>


### [414] [WeatherEdit: Controllable Weather Editing with 4D Gaussian Field](https://arxiv.org/pdf/2505.20471)
*Chenghao Qian, Wenjing Li, Yuhu Guo, Gustav Markkula*

Main category: cs.CV

TL;DR: WeatherEdit is a pipeline for realistic weather effects in 3D scenes, combining background editing and particle construction with controllable severity.


<details>
  <summary>Details</summary>
Motivation: To enable realistic and controllable weather effects for applications like autonomous driving simulation.

Method: Uses an all-in-one adapter for 2D weather backgrounds and a 4D Gaussian field for 3D particle effects, with TV-attention for consistency.

Result: Generates diverse, realistic weather effects with adjustable severity, validated on driving datasets.

Conclusion: WeatherEdit is effective for simulating adverse weather, useful for autonomous driving.

Abstract: In this work, we present WeatherEdit, a novel weather editing pipeline for
generating realistic weather effects with controllable types and severity in 3D
scenes. Our approach is structured into two key components: weather background
editing and weather particle construction. For weather background editing, we
introduce an all-in-one adapter that integrates multiple weather styles into a
single pretrained diffusion model, enabling the generation of diverse weather
effects in 2D image backgrounds. During inference, we design a Temporal-View
(TV-) attention mechanism that follows a specific order to aggregate temporal
and spatial information, ensuring consistent editing across multi-frame and
multi-view images. To construct the weather particles, we first reconstruct a
3D scene using the edited images and then introduce a dynamic 4D Gaussian field
to generate snowflakes, raindrops and fog in the scene. The attributes and
dynamics of these particles are precisely controlled through physical-based
modelling and simulation, ensuring realistic weather representation and
flexible severity adjustments. Finally, we integrate the 4D Gaussian field with
the 3D scene to render consistent and highly realistic weather effects.
Experiments on multiple driving datasets demonstrate that WeatherEdit can
generate diverse weather effects with controllable condition severity,
highlighting its potential for autonomous driving simulation in adverse
weather. See project page: https://jumponthemoon.github.io/w-edit

</details>


### [415] [Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler Solutions](https://arxiv.org/pdf/2404.19015)
*Nagabhushan Somraj, Sai Harsha Mupparaju, Adithyan Karanayil, Rajiv Soundararajan*

Main category: cs.CV

TL;DR: The paper proposes a method to improve Neural Radiance Fields (NeRF) performance with sparse input views by learning depth supervision through augmented models and regularization techniques.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF methods degrade with sparse views. Depth supervision helps but classical or pre-trained neural methods have limitations (sparse or generalization issues). The goal is to learn depth supervision directly and design regularizations for various radiance fields.

Method: Augmented models are trained alongside the main radiance field. Regularizations reduce model capabilities (e.g., positional encoding, tensor components, hash table size) to avoid overfitting and improve depth estimation.

Result: State-of-the-art view-synthesis performance with sparse input views on forward-facing and 360° scenes.

Conclusion: Reducing model complexity via regularization and learning depth supervision improves NeRF performance with sparse views, achieving superior results.

Abstract: Neural Radiance Fields (NeRF) show impressive performance in photo-realistic
free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF
and ZipNeRF employ explicit models for faster optimization and rendering, as
compared to the NeRF that employs an implicit representation. However, both
implicit and explicit radiance fields require dense sampling of images in the
given scene. Their performance degrades significantly when only a sparse set of
views is available. Researchers find that supervising the depth estimated by a
radiance field helps train it effectively with fewer views. The depth
supervision is obtained either using classical approaches or neural networks
pre-trained on a large dataset. While the former may provide only sparse
supervision, the latter may suffer from generalization issues. As opposed to
the earlier approaches, we seek to learn the depth supervision by designing
augmented models and training them along with the main radiance field. Further,
we aim to design a framework of regularizations that can work across different
implicit and explicit radiance fields. We observe that certain features of
these radiance field models overfit to the observed images in the sparse-input
scenario. Our key finding is that reducing the capability of the radiance
fields with respect to positional encoding, the number of decomposed tensor
components or the size of the hash table, constrains the model to learn simpler
solutions, which estimate better depth in certain regions. By designing
augmented models based on such reduced capabilities, we obtain better depth
supervision for the main radiance field. We achieve state-of-the-art
view-synthesis performance with sparse input views on popular datasets
containing forward-facing and 360$^\circ$ scenes by employing the above
regularizations.

</details>


### [416] [Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization](https://arxiv.org/pdf/2505.23331)
*Matteo Gallici, Haitz Sáez de Ocáriz Borde*

Main category: cs.CV

TL;DR: RL-based fine-tuning with GRPO improves alignment of VAR models to human preferences, enhancing image quality and style control.


<details>
  <summary>Details</summary>
Motivation: To align pre-trained generative models more closely with nuanced human preferences using RL, particularly for visual autoregressive models.

Method: Apply Group Relative Policy Optimization (GRPO) to fine-tune VAR models, leveraging aesthetic predictors and CLIP embeddings for reward signals.

Result: Improved image quality, precise style control, and generalization beyond initial training distribution (e.g., ImageNet).

Conclusion: RL-based fine-tuning is efficient and effective for VAR models, benefiting from their fast inference speeds compared to diffusion-based alternatives.

Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL)
has emerged as an effective approach for aligning outputs more closely with
nuanced human preferences. In this paper, we investigate the application of
Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual
autoregressive (VAR) models. Our empirical results demonstrate that this
approach enables alignment to intricate reward signals derived from aesthetic
predictors and CLIP embeddings, significantly enhancing image quality and
enabling precise control over the generation style. Interestingly, by
leveraging CLIP, our method can help VAR models generalize beyond their initial
ImageNet distribution: through RL-driven exploration, these models can generate
images aligned with prompts referencing image styles that were absent during
pre-training. In summary, we show that RL-based fine-tuning is both efficient
and effective for VAR models, benefiting particularly from their fast inference
speeds, which are advantageous for online sampling, an aspect that poses
significant challenges for diffusion-based alternatives.

</details>


### [417] [3DRealCar: An In-the-wild RGB-D Car Dataset with 360-degree Views](https://arxiv.org/pdf/2406.04875)
*Xiaobiao Du, Yida Wang, Haiyang Sun, Zhuojie Wu, Hongwei Sheng, Shuyun Wang, Jiaying Ying, Ming Lu, Tianqing Zhu, Kun Zhan, Xin Yu*

Main category: cs.CV

TL;DR: The paper introduces 3DRealCar, the first large-scale high-quality real-world 3D car dataset, addressing limitations of existing synthetic or low-quality datasets. It features high volume, quality, and diversity, and benchmarks state-of-the-art 3D reconstruction methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D car datasets are synthetic or low-quality, limiting practical applications. The need for a high-quality real-world dataset drives this work.

Method: The dataset includes 2,500 cars scanned using smartphones, capturing 200 high-resolution 360-degree RGB-D views per car. It offers diverse lighting conditions and standardized car orientation.

Result: 3DRealCar enables high-fidelity 3D reconstruction and improves 2D/3D car-related tasks. Challenges remain in reconstructing cars under reflective and dark lighting.

Conclusion: 3DRealCar fills a gap in high-quality real-world 3D car datasets, benefiting research and applications, though lighting conditions pose reconstruction challenges.

Abstract: 3D cars are commonly used in self-driving systems, virtual/augmented reality,
and games. However, existing 3D car datasets are either synthetic or
low-quality, limiting their applications in practical scenarios and presenting
a significant gap toward high-quality real-world 3D car datasets. In this
paper, we propose the first large-scale 3D real car dataset, termed 3DRealCar,
offering three distinctive features. (1) \textbf{High-Volume}: 2,500 cars are
meticulously scanned by smartphones, obtaining car images and point clouds with
real-world dimensions; (2) \textbf{High-Quality}: Each car is captured in an
average of 200 dense, high-resolution 360-degree RGB-D views, enabling
high-fidelity 3D reconstruction; (3) \textbf{High-Diversity}: The dataset
contains various cars from over 100 brands, collected under three distinct
lighting conditions, including reflective, standard, and dark. Additionally, we
offer detailed car parsing maps for each instance to promote research in car
parsing tasks. Moreover, we remove background point clouds and standardize the
car orientation to a unified axis for the reconstruction only on cars and
controllable rendering without background. We benchmark 3D reconstruction
results with state-of-the-art methods across different lighting conditions in
3DRealCar. Extensive experiments demonstrate that the standard lighting
condition part of 3DRealCar can be used to produce a large number of
high-quality 3D cars, improving various 2D and 3D tasks related to cars.
Notably, our dataset brings insight into the fact that recent 3D reconstruction
methods face challenges in reconstructing high-quality 3D cars under reflective
and dark lighting conditions.
\textcolor{red}{\href{https://xiaobiaodu.github.io/3drealcar/}{Our dataset is
here.}}

</details>


### [418] [AEM: Attention Entropy Maximization for Multiple Instance Learning based Whole Slide Image Classification](https://arxiv.org/pdf/2406.15303)
*Yunlong Zhang, Honglin Li, Yunxuan Sun, Zhongyi Shui, Jingxiong Li, Chenglu Zhu, Lin Yang*

Main category: cs.CV

TL;DR: AEM regularization improves MIL by maximizing attention entropy, reducing overfitting without complex changes.


<details>
  <summary>Details</summary>
Motivation: Overfitting in MIL due to attention over-concentration; existing solutions are complex.

Method: Introduces Attention Entropy Maximization (AEM) and Cosine Weight Annealing to reduce parameter sensitivity.

Result: AEM outperforms across various feature extractors, MIL frameworks, and attention mechanisms.

Conclusion: AEM is a simple, effective regularization technique for MIL, validated by extensive evaluations.

Abstract: Multiple Instance Learning (MIL) effectively analyzes whole slide images but
faces overfitting due to attention over-concentration. While existing solutions
rely on complex architectural modifications or additional processing steps, we
introduce Attention Entropy Maximization (AEM), a simple yet effective
regularization technique. Our investigation reveals the positive correlation
between attention entropy and model performance. Building on this insight, we
integrate AEM regularization into the MIL framework to penalize excessive
attention concentration. To address sensitivity to the AEM weight parameter, we
implement Cosine Weight Annealing, reducing parameter dependency. Extensive
evaluations demonstrate AEM's superior performance across diverse feature
extractors, MIL frameworks, attention mechanisms, and augmentation techniques.
Here is our anonymous code: https://github.com/dazhangyu123/AEM.

</details>


### [419] [PerLDiff: Controllable Street View Synthesis Using Perspective-Layout Diffusion Models](https://arxiv.org/pdf/2407.06109)
*Jinhua Zhang, Hualian Sheng, Sijia Cai, Bing Deng, Qiao Liang, Wen Li, Ying Fu, Jieping Ye, Shuhang Gu*

Main category: cs.CV

TL;DR: PerLDiff introduces a novel method for controllable street view image generation using 3D geometric priors, outperforming existing methods in precision.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of annotating 3D data for autonomous driving by improving controllable generation precision.

Method: Integrates perspective 3D geometric information into diffusion models (PerLDiff) for precise object-level control.

Result: Superior controllability and precision on NuScenes and KITTI datasets compared to existing methods.

Conclusion: PerLDiff offers a robust solution for precise and controllable street view image generation.

Abstract: Controllable generation is considered a potentially vital approach to address
the challenge of annotating 3D data, and the precision of such controllable
generation becomes particularly imperative in the context of data production
for autonomous driving. Existing methods focus on the integration of diverse
generative information into controlling inputs, utilizing frameworks such as
GLIGEN or ControlNet, to produce commendable outcomes in controllable
generation. However, such approaches intrinsically restrict generation
performance to the learning capacities of predefined network architectures. In
this paper, we explore the innovative integration of controlling information
and introduce PerLDiff (\textbf{Per}spective-\textbf{L}ayout \textbf{Diff}usion
Models), a novel method for effective street view image generation that fully
leverages perspective 3D geometric information. Our PerLDiff employs 3D
geometric priors to guide the generation of street view images with precise
object-level control within the network learning process, resulting in a more
robust and controllable output. Moreover, it demonstrates superior
controllability compared to alternative layout control methods. Empirical
results justify that our PerLDiff markedly enhances the precision of
controllable generation on the NuScenes and KITTI datasets.

</details>


### [420] [Dense Feature Interaction Network for Image Inpainting Localization](https://arxiv.org/pdf/2408.02191)
*Ye Yao, Tingfeng Han, Shan Jia, Siwei Lyu*

Main category: cs.CV

TL;DR: A new method, DeFI-Net, improves image inpainting detection by using a feature pyramid architecture to capture multi-scale representations and refine edge localization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inpainting detection often fail due to high false positives and poor edge localization, especially with varying semantics and scales.

Method: DeFI-Net employs a Dense Feature Interaction Network with a feature pyramid architecture to enhance multi-scale feature interactions and adaptive feature direction for better edge and semantic integration.

Result: DeFI-Net achieves state-of-the-art performance on seven inpainting datasets, accurately identifying inpainted areas.

Conclusion: DeFI-Net provides an effective solution for detecting image inpainting, outperforming existing methods by improving feature interactions and edge localization.

Abstract: Image inpainting, the process of filling in missing areas in an image, is a
common image editing technique. Inpainting can be used to conceal or alter
image contents in malicious manipulation of images, driving the need for
research in image inpainting detection. Most existing methods use a basic
encoder-decoder structure, which often results in a high number of false
positives or misses the inpainted regions, especially when dealing with targets
of varying semantics and scales. Additionally, the lack of an effective
approach to capture boundary artifacts leads to less accurate edge
localization. In this paper, we describe a new method for inpainting detection
based on a Dense Feature Interaction Network (DeFI-Net). DeFI-Net uses a novel
feature pyramid architecture to capture and amplify multi-scale representations
across various stages, thereby improving the detection of image inpainting by
better strengthening feature-level interactions. Additionally, the network can
adaptively direct the lower-level features, which carry edge and shape
information, to refine the localization of manipulated regions while
integrating the higher-level semantic features. Using DeFI-Net, we develop a
method combining complementary representations to accurately identify inpainted
areas. Evaluation on seven image inpainting datasets demonstrates the
effectiveness of our approach, which achieves state-of-the-art performance in
detecting inpainting across diverse models. Code and models are available at
https://github.com/Boombb/DeFI-Net_Inpainting.

</details>


### [421] [FlatFusion: Delving into Details of Sparse Transformer-based Camera-LiDAR Fusion for Autonomous Driving](https://arxiv.org/pdf/2408.06832)
*Yutao Zhu, Xiaosong Jia, Xinyu Yang, Junchi Yan*

Main category: cs.CV

TL;DR: FlatFusion is a Transformer-based framework for sparse camera-LiDAR fusion, outperforming state-of-the-art methods with 73.7 NDS on nuScenes.


<details>
  <summary>Details</summary>
Motivation: Effective fusion of dense image patches and sparse LiDAR data in autonomous driving requires careful design due to depth ambiguity.

Method: Explores design choices like image-to-3D mapping, attention neighbor grouping, and Transformer micro-structure, leading to FlatFusion.

Result: Achieves 73.7 NDS on nuScenes with 10.1 FPS, surpassing UniTR, CMT, and SparseFusion.

Conclusion: FlatFusion demonstrates superior performance in sparse sensor fusion, setting a new benchmark.

Abstract: The integration of data from diverse sensor modalities (e.g., camera and
LiDAR) constitutes a prevalent methodology within the ambit of autonomous
driving scenarios. Recent advancements in efficient point cloud transformers
have underscored the efficacy of integrating information in sparse formats.
When it comes to fusion, since image patches are dense in pixel space with
ambiguous depth, it necessitates additional design considerations for effective
fusion. In this paper, we conduct a comprehensive exploration of design choices
for Transformer-based sparse cameraLiDAR fusion. This investigation encompasses
strategies for image-to-3D and LiDAR-to-2D mapping, attention neighbor
grouping, single modal tokenizer, and micro-structure of Transformer. By
amalgamating the most effective principles uncovered through our investigation,
we introduce FlatFusion, a carefully designed framework for sparse camera-LiDAR
fusion. Notably, FlatFusion significantly outperforms state-of-the-art sparse
Transformer-based methods, including UniTR, CMT, and SparseFusion, achieving
73.7 NDS on the nuScenes validation set with 10.1 FPS with PyTorch.

</details>


### [422] [Self-supervised Learning of Hybrid Part-aware 3D Representation of 2D Gaussians and Superquadrics](https://arxiv.org/pdf/2408.10789)
*Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu*

Main category: cs.CV

TL;DR: PartGS is a self-supervised framework for part-aware 3D reconstruction using 2D Gaussians and superquadrics, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Human perception interprets 3D environments at higher structural levels, not low-level elements. Structured decomposition improves interpretability and manipulation.

Method: Combines 2D Gaussians and superquadrics in a hybrid representation, jointly optimizing their parameters for part-aware reconstruction from multi-view images.

Result: Superior performance on DTU, ShapeNet, and real-world datasets compared to existing methods.

Conclusion: PartGS effectively bridges the gap between low-level 3D representations and human-like structural understanding, enabling interpretable and high-fidelity reconstructions.

Abstract: Low-level 3D representations, such as point clouds, meshes, NeRFs and 3D
Gaussians, are commonly used for modeling 3D objects and scenes. However,
cognitive studies indicate that human perception operates at higher levels and
interprets 3D environments by decomposing them into meaningful structural
parts, rather than low-level elements like points or voxels. Structured
geometric decomposition enhances scene interpretability and facilitates
downstream tasks requiring component-level manipulation. In this work, we
introduce PartGS, a self-supervised part-aware reconstruction framework that
integrates 2D Gaussians and superquadrics to parse objects and scenes into an
interpretable decomposition, leveraging multi-view image inputs to uncover 3D
structural information. Our method jointly optimizes superquadric meshes and
Gaussians by coupling their parameters within a hybrid representation. On one
hand, superquadrics enable the representation of a wide range of shape
primitives, facilitating flexible and meaningful decompositions. On the other
hand, 2D Gaussians capture detailed texture and geometric details, ensuring
high-fidelity appearance and geometry reconstruction. Operating in a
self-supervised manner, our approach demonstrates superior performance compared
to state-of-the-art methods across extensive experiments on the DTU, ShapeNet,
and real-world datasets.

</details>


### [423] [ForgeLens: Data-Efficient Forgery Focus for Generalizable Forgery Image Detection](https://arxiv.org/pdf/2408.13697)
*Yingjian Chen, Lei Zhang, Yakun Niu*

Main category: cs.CV

TL;DR: ForgeLens is a data-efficient, feature-guided framework for detecting image forgeries, achieving state-of-the-art performance with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about image authenticity online by creating a detector that generalizes well and requires little training data.

Method: Incorporates Weight-Shared Guidance Module (WSGM) and FAFormer to focus on forgery-specific features.

Result: Improves Avg.Acc by 13.61% and Avg.AP by 8.69% over the base model, outperforming existing methods with 1% training data.

Conclusion: ForgeLens is highly effective for forgery detection, offering strong generalization and data efficiency.

Abstract: The rise of generative models has raised concerns about image authenticity
online, highlighting the urgent need for a detector that is (1) highly
generalizable, capable of handling unseen forgery techniques, and (2)
data-efficient, achieving optimal performance with minimal training data,
enabling it to counter newly emerging forgery techniques effectively. To
achieve this, we propose ForgeLens, a data-efficient, feature-guided framework
that incorporates two lightweight designs to enable a frozen network to focus
on forgery-specific features. First, we introduce the Weight-Shared Guidance
Module (WSGM), which guides the extraction of forgery-specific features during
training. Second, a forgery-aware feature integrator, FAFormer, is used to
effectively integrate forgery information across multi-stage features.
ForgeLens addresses a key limitation of previous frozen network-based methods,
where general-purpose features extracted from large datasets often contain
excessive forgery-irrelevant information. As a result, it achieves strong
generalization and reaches optimal performance with minimal training data.
Experimental results on 19 generative models, including both GANs and diffusion
models, demonstrate improvements of 13.61% in Avg.Acc and 8.69% in Avg.AP over
the base model. Notably, ForgeLens outperforms existing forgery detection
methods, achieving state-of-the-art performance with just 1% of the training
data. Our code is available at https://github.com/Yingjian-Chen/ForgeLens.

</details>


### [424] [AWF: Adaptive Weight Fusion for Enhanced Class Incremental Semantic Segmentation](https://arxiv.org/pdf/2409.08516)
*Zechao Sun, Shuying Piao, Haolin Jin, Chang Dong, Lin Yue, Weitong Chen, Luping Zhou*

Main category: cs.CV

TL;DR: The paper introduces Adaptive Weight Fusion (AWF), an improved version of Endpoints Weight Fusion (EWF), to better balance old and new knowledge in Class Incremental Semantic Segmentation (CISS). AWF uses an alternating training strategy for fusion parameters, outperforming EWF on benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods like EWF struggle with optimal fusion of old and new knowledge due to simplistic parameter calculation, limiting performance in CISS.

Method: Proposes AWF, which dynamically adjusts fusion parameters via alternating training for more adaptive weight integration.

Result: AWF achieves superior performance over EWF, better balancing knowledge retention and new class learning.

Conclusion: AWF enhances CISS performance by improving weight fusion adaptability, with code to be released on Github.

Abstract: Class Incremental Semantic Segmentation (CISS) aims to mitigate catastrophic
forgetting by maintaining a balance between previously learned and newly
introduced knowledge. Existing methods, primarily based on regularization
techniques like knowledge distillation, help preserve old knowledge but often
face challenges in effectively integrating new knowledge, resulting in limited
overall improvement. Endpoints Weight Fusion (EWF) method, while simple,
effectively addresses some of these limitations by dynamically fusing the model
weights from previous steps with those from the current step, using a fusion
parameter alpha determined by the relative number of previously known classes
and newly introduced classes. However, the simplicity of the alpha calculation
may limit its ability to fully capture the complexities of different task
scenarios, potentially leading to suboptimal fusion outcomes. In this paper, we
propose an enhanced approach called Adaptive Weight Fusion (AWF), which
introduces an alternating training strategy for the fusion parameter, allowing
for more flexible and adaptive weight integration. AWF achieves superior
performance by better balancing the retention of old knowledge with the
learning of new classes, significantly improving results on benchmark CISS
tasks compared to the original EWF. And our experiment code will be released on
Github.

</details>


### [425] [DepthART: Monocular Depth Estimation as Autoregressive Refinement Task](https://arxiv.org/pdf/2409.15010)
*Bulat Gabdullin, Nina Konovalova, Nikolay Patakin, Dmitry Senushkin, Anton Konushin*

Main category: cs.CV

TL;DR: The paper introduces DepthART, a novel training method for monocular depth estimation using Visual Autoregressive Transformer (VAR), improving performance by refining predictions dynamically.


<details>
  <summary>Details</summary>
Motivation: Current generative approaches for monocular depth estimation, like diffusion models, have limitations. VAR shows promise but suffers from suboptimal results due to conventional training methods.

Method: DepthART refines VAR training by using model predictions as dynamic targets, minimizing residuals to align training and inference.

Result: DepthART outperforms existing methods on unseen benchmarks when trained on the Hypersim dataset.

Conclusion: DepthART enhances VAR's depth estimation performance, offering a superior alternative to traditional generative and discriminative approaches.

Abstract: Monocular depth estimation has seen significant advances through
discriminative approaches, yet their performance remains constrained by the
limitations of training datasets. While generative approaches have addressed
this challenge by leveraging priors from internet-scale datasets, with recent
studies showing state-of-the-art results using fine-tuned text-to-image
diffusion models, there is still room for improvement. Notably, autoregressive
generative approaches, particularly Visual AutoRegressive modeling, have
demonstrated superior results compared to diffusion models in conditioned image
synthesis, while offering faster inference times. In this work, we apply Visual
Autoregressive Transformer (VAR) to the monocular depth estimation problem.
However, the conventional GPT-2-style training procedure (teacher forcing)
inherited by VAR yields suboptimal results for depth estimation. To address
this limitation, we introduce DepthART - a novel training method formulated as
a Depth Autoregressive Refinement Task. Unlike traditional VAR training with
static inputs and targets, our method implements a dynamic target formulation
based on model outputs, enabling self-refinement. By utilizing the model's own
predictions as inputs instead of ground truth token maps during training, we
frame the objective as residual minimization, effectively reducing the
discrepancy between training and inference procedures. Our experimental results
demonstrate that the proposed training approach significantly enhances the
performance of VAR in depth estimation tasks. When trained on Hypersim dataset
using our approach, the model achieves superior results across multiple unseen
benchmarks compared to existing generative and discriminative baselines.

</details>


### [426] [General Compression Framework for Efficient Transformer Object Tracking](https://arxiv.org/pdf/2409.17564)
*Lingyi Hong, Jinglun Li, Xinyu Zhou, Shilin Yan, Pinxue Guo, Kaixun Jiang, Zhaoyu Chen, Shuyong Gao, Runze Li, Xingdong Sheng, Wei Zhang, Hong Lu, Wenqiang Zhang*

Main category: cs.CV

TL;DR: CompressTracker is a model compression framework for efficient transformer object tracking, balancing speed and accuracy with a novel stage division strategy and replacement training.


<details>
  <summary>Details</summary>
Motivation: Existing methods sacrifice accuracy for speed and have complex training processes. CompressTracker aims to reduce model size while preserving accuracy.

Method: Uses stage division to segment transformer layers and replacement training to enhance student model replication. Includes prediction guidance and feature mimicking for supervision.

Result: CompressTracker-SUTrack retains 99% of SUTrack's performance (72.2 AUC on LaSOT) with a 2.42x speedup.

Conclusion: CompressTracker is effective, generalizable, and compatible with any transformer architecture.

Abstract: Previous works have attempted to improve tracking efficiency through
lightweight architecture design or knowledge distillation from teacher models
to compact student trackers. However, these solutions often sacrifice accuracy
for speed to a great extent, and also have the problems of complex training
process and structural limitations. Thus, we propose a general model
compression framework for efficient transformer object tracking, named
CompressTracker, to reduce model size while preserving tracking accuracy. Our
approach features a novel stage division strategy that segments the transformer
layers of the teacher model into distinct stages to break the limitation of
model structure. Additionally, we also design a unique replacement training
technique that randomly substitutes specific stages in the student model with
those from the teacher model, as opposed to training the student model in
isolation. Replacement training enhances the student model's ability to
replicate the teacher model's behavior and simplifies the training process. To
further forcing student model to emulate teacher model, we incorporate
prediction guidance and stage-wise feature mimicking to provide additional
supervision during the teacher model's compression process. CompressTracker is
structurally agnostic, making it compatible with any transformer architecture.
We conduct a series of experiment to verify the effectiveness and
generalizability of our CompressTracker. Our CompressTracker-SUTrack,
compressed from SUTrack, retains about 99 performance on LaSOT (72.2 AUC) while
achieves 2.42x speed up. Code is available at
https://github.com/LingyiHongfd/CompressTracker.

</details>


### [427] [LW2G: Learning Whether to Grow for Prompt-based Continual Learning](https://arxiv.org/pdf/2409.18860)
*Qian Feng, Da-wei Zhou, Hanbin Zhao, Chao Zhang, Jiahua Dong, Dengxin Dai, Hui Qian*

Main category: cs.CV

TL;DR: The paper introduces LW2G, a plug-in method for prompt-based continual learning that dynamically decides whether to expand the prompt pool based on task disparities, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in prompt-based continual learning, such as low selection accuracy and unbounded prompt pool growth, by leveraging task disparities for better prompt set management.

Method: Proposes LW2G, which uses a metric (HFC) to measure task hindrance and a dynamic growing approach to adaptively expand the prompt pool. Includes a gradient-based constraint for consistency.

Result: Extensive experiments demonstrate LW2G's effectiveness in achieving intra-task knowledge sharing and avoiding excessive prompt pool growth.

Conclusion: LW2G offers an efficient and adaptive solution for prompt-based continual learning, balancing task-specific needs with shared knowledge.

Abstract: Recent Prompt-based Continual learning (PCL) has achieved remarkable
performance with pre-trained models. These approaches expand a prompt pool by
adding a new set of prompts while learning and select the correct set during
inference. Previous studies have revealed that learning task-wised prompt sets
individually and low selection accuracy pose challenges to the performance of
PCL. In this paper, we propose a plug-in method, $\textbf{L}$earning
$\textbf{W}$hether $\textbf{t}$o $\textbf{G}$row $\textbf{(LW2G)}$, which
leverages the disparities between tasks to form an effective and efficient
prompt sets pool, thereby achieving intra-task knowledge sharing and
cooperation and avoiding the unbounded increase in the cost of the prompt pool.
Specifically, a shared set is utilized when several tasks share certain
commonalities, and a new set is added when there are significant differences
between the new and previous tasks. To achieve this, we develop a metric called
Hinder Forward Capability (HFC) to measure the hindrance imposed on learning
new tasks by surgically modifying the original gradient onto the orthogonal
complement of the old feature space. With HFC, an automated scheme, Dynamic
Growing Approach, adaptively learns whether to grow with a dynamic threshold.
Furthermore, we design a gradient-based constraint to ensure consistency
between the updating prompts and pre-trained knowledge. Extensive experiments
show the effectiveness of our method. Code is available at
https://github.com/RAIAN08/LW2G.

</details>


### [428] [Generalizing vision-language models to novel domains: A comprehensive survey](https://arxiv.org/pdf/2506.18504)
*Xinyao Li, Jingjing Li, Fengling Li, Lei Zhu, Yang Yang, Heng Tao Shen*

Main category: cs.CV

TL;DR: A survey on vision-language models (VLMs) focusing on their generalization challenges, methodologies, and benchmarks, while comparing them with multimodal large language models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: VLMs excel in zero-shot tasks but struggle with domain-specific tasks, prompting research into transferring their knowledge to downstream applications.

Method: Categorizes VLM generalization methods into prompt-based, parameter-based, and feature-based, and reviews benchmarks and performance comparisons.

Result: Summarizes current VLM generalization techniques and benchmarks, highlighting their strengths and limitations.

Conclusion: Provides a clear overview of VLM generalization research and its future directions, including comparisons with MLLMs.

Abstract: Recently, vision-language pretraining has emerged as a transformative
technique that integrates the strengths of both visual and textual modalities,
resulting in powerful vision-language models (VLMs). Leveraging web-scale
pretraining data, these models exhibit strong zero-shot capabilities. However,
their performance often deteriorates when confronted with domain-specific or
specialized generalization tasks. To address this, a growing body of research
focuses on transferring or generalizing the rich knowledge embedded in VLMs to
various downstream applications. This survey aims to comprehensively summarize
the generalization settings, methodologies, benchmarking and results in VLM
literatures. Delving into the typical VLM structures, current literatures are
categorized into prompt-based, parameter-based and feature-based methods
according to the transferred modules. The differences and characteristics in
each category are furthered summarized and discussed by revisiting the typical
transfer learning (TL) settings, providing novel interpretations for TL in the
era of VLMs. Popular benchmarks for VLM generalization are further introduced
with thorough performance comparisons among the reviewed methods. Following the
advances in large-scale generalizable pretraining, this survey also discusses
the relations and differences between VLMs and up-to-date multimodal large
language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the
surging literatures in vision-language research from a novel and practical
generalization prospective, this survey contributes to a clear landscape of
current and future multimodal researches.

</details>


### [429] [Multibiometrics Using a Single Face Image](https://arxiv.org/pdf/2409.20003)
*Koichi Ito, Taito Tonosaki, Takafumi Aoki, Tetsushi Ohki, Masakatsu Nishigaki*

Main category: cs.CV

TL;DR: A novel multibiometric method combines five traits (face, iris, periocular, nose, eyebrow) from a single face image, maintaining convenience while improving recognition performance.


<details>
  <summary>Details</summary>
Motivation: To enhance biometric recognition performance without sacrificing convenience by leveraging multiple traits from a single face image.

Method: Extracts five biometric traits (face, iris, periocular, nose, eyebrow) from one face image, avoiding the need for multiple acquisitions.

Result: Demonstrated effectiveness using the CASIA Iris Distance database.

Conclusion: The proposed method improves recognition performance while retaining the convenience of single-image input.

Abstract: Multibiometrics, which uses multiple biometric traits to improve recognition
performance instead of using only one biometric trait to authenticate
individuals, has been investigated. Previous studies have combined individually
acquired biometric traits or have not fully considered the convenience of the
system. Focusing on a single face image, we propose a novel multibiometric
method that combines five biometric traits, i.e., face, iris, periocular, nose,
eyebrow, that can be extracted from a single face image. The proposed method
does not sacrifice the convenience of biometrics since only a single face image
is used as input. Through a variety of experiments using the CASIA Iris
Distance database, we demonstrate the effectiveness of the proposed
multibiometrics method.

</details>


### [430] [High-Precision Dichotomous Image Segmentation via Probing Diffusion Capacity](https://arxiv.org/pdf/2410.10105)
*Qian Yu, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Bo Li, Lihe Zhang, Huchuan Lu*

Main category: cs.CV

TL;DR: DiffDIS is a diffusion-driven segmentation model leveraging pre-trained U-Net in diffusion models for high-resolution, fine-grained image segmentation, achieving state-of-the-art results with fast inference.


<details>
  <summary>Details</summary>
Motivation: The challenge in high-resolution image segmentation is balancing contextual awareness with precision for detailed object delineation. Diffusion models offer exceptional detail resolution and contextual awareness, making them suitable for this task.

Method: DiffDIS uses a pre-trained U-Net from diffusion models, a task-specific stable one-step denoising approach, and an auxiliary edge generation task to enhance detail preservation and reconcile diffusion's probabilistic nature with segmentation's deterministic demands.

Result: Experiments on the DIS5K dataset show DiffDIS achieves state-of-the-art results with high accuracy and fast processing.

Conclusion: DiffDIS is an efficient, high-resolution segmentation model, combining diffusion models' strengths with task-specific optimizations for detailed binary mask generation.

Abstract: In the realm of high-resolution (HR), fine-grained image segmentation, the
primary challenge is balancing broad contextual awareness with the precision
required for detailed object delineation, capturing intricate details and the
finest edges of objects. Diffusion models, trained on vast datasets comprising
billions of image-text pairs, such as SD V2.1, have revolutionized
text-to-image synthesis by delivering exceptional quality, fine detail
resolution, and strong contextual awareness, making them an attractive solution
for high-resolution image segmentation. To this end, we propose DiffDIS, a
diffusion-driven segmentation model that taps into the potential of the
pre-trained U-Net within diffusion models, specifically designed for
high-resolution, fine-grained object segmentation. By leveraging the robust
generalization capabilities and rich, versatile image representation prior of
the SD models, coupled with a task-specific stable one-step denoising approach,
we significantly reduce the inference time while preserving high-fidelity,
detailed generation. Additionally, we introduce an auxiliary edge generation
task to not only enhance the preservation of fine details of the object
boundaries, but reconcile the probabilistic nature of diffusion with the
deterministic demands of segmentation. With these refined strategies in place,
DiffDIS serves as a rapid object mask generation model, specifically optimized
for generating detailed binary maps at high resolutions, while demonstrating
impressive accuracy and swift processing. Experiments on the DIS5K dataset
demonstrate the superiority of DiffDIS, achieving state-of-the-art results
through a streamlined inference process. The source code will be publicly
available at https://github.com/qianyu-dlut/DiffDIS.

</details>


### [431] [GLIMPSE: Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation for Generative LVLMs](https://arxiv.org/pdf/2506.18985)
*Guanxi Shen*

Main category: cs.CV

TL;DR: GLIMPSE is a lightweight, model-agnostic framework for interpreting LVLM outputs in VQA by attributing responses to visual and textual evidence, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Understanding where LVLMs focus their visual attention during free-form responses is crucial for model transparency and behavior analysis.

Method: GLIMPSE combines gradient-weighted attention, adaptive layer propagation, and relevance-weighted token aggregation to generate response-level heat maps.

Result: GLIMPSE outperforms existing interpretability methods and enhances human-alignment in cross-modal reasoning.

Conclusion: GLIMPSE provides fine-grained insights into LVLM behavior, aiding in diagnosing issues like hallucination and bias while ensuring transparency.

Abstract: Recent progress in large vision-language models (LVLMs) has advanced the
state of the art in visual question answering (VQA). However, interpreting
where LVLMs direct their visual attention while generating free-form responses
remains a significant challenge, yet is essential for understanding model
behavior. We introduce GLIMPSE (Gradient-Layer Importance Mapping for Prompted
Visual Saliency Explanation), a lightweight, model-agnostic framework that
jointly attributes LVLM outputs to the most relevant visual evidence and
textual signals supporting open-ended VQA. GLIMPSE fuses gradient-weighted
attention, adaptive layer propagation, and relevance-weighted token aggregation
to produce holistic response-level heat maps for interpreting cross-modal
reasoning, outperforming prior interpretability methods and pushing the
state-of-the-art in human-alignment. We demonstrate an analytic explainable AI
(XAI) approach using GLIMPSE to uncover fine-grained insights into LVLM
cross-modal attribution, trace reasoning dynamics, analyze systematic
human-attention misalignment, diagnose hallucination, expose bias, and ensure
transparency.

</details>


### [432] [Open World Object Detection: A Survey](https://arxiv.org/pdf/2410.11301)
*Yiming Li, Yi Wang, Wenqian Wang, Dan Lin, Bingbing Li, Kim-Hui Yap*

Main category: cs.CV

TL;DR: This survey paper reviews Open World Object Detection (OWOD), covering definitions, datasets, methods, and challenges, while also exploring related fields like OSR and IL.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of the emerging OWOD field, addressing its potential to expand knowledge incrementally in object detection.

Method: The paper reviews problem definitions, datasets, source codes, evaluation metrics, and compares existing methods, alongside related areas like OSR and IL.

Result: A detailed survey of OWOD with over 100 references, highlighting current limitations and future research directions.

Conclusion: The paper marks a significant step in advancing object detection technology by summarizing OWOD's progress and challenges.

Abstract: Exploring new knowledge is a fundamental human ability that can be mirrored
in the development of deep neural networks, especially in the field of object
detection. Open world object detection (OWOD) is an emerging area of research
that adapts this principle to explore new knowledge. It focuses on recognizing
and learning from objects absent from initial training sets, thereby
incrementally expanding its knowledge base when new class labels are
introduced. This survey paper offers a thorough review of the OWOD domain,
covering essential aspects, including problem definitions, benchmark datasets,
source codes, evaluation metrics, and a comparative study of existing methods.
Additionally, we investigate related areas like open set recognition (OSR) and
incremental learning (IL), underlining their relevance to OWOD. Finally, the
paper concludes by addressing the limitations and challenges faced by current
OWOD algorithms and proposes directions for future research. To our knowledge,
this is the first comprehensive survey of the emerging OWOD field with over one
hundred references, marking a significant step forward for object detection
technology. A comprehensive source code and benchmarks are archived and
concluded at https://github.com/ArminLee/OWOD Review.

</details>


### [433] [Tracking by Detection and Query: An Efficient End-to-End Framework for Multi-Object Tracking](https://arxiv.org/pdf/2411.06197)
*Shukun Jia, Shiyu Hu, Yichao Cao, Feng Yang, Xin Lu, Xiaobo Lu*

Main category: cs.CV

TL;DR: TBDQ-Net combines tracking-by-detection (TBD) and tracking-by-query (TBQ) paradigms for efficient and robust multi-object tracking, outperforming TBD and TBQ methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of TBD (fragmented association, heuristic pipelines) and TBQ (high training cost, slow inference) by unifying their strengths.

Method: Proposes TBDQ-Net, integrating pretrained detectors with a lightweight, learnable associator featuring BII and CPA modules for semantic and positional consistency.

Result: Outperforms TBD by 6.0 IDF1 points on DanceTrack and is 37.5% faster than TBQ methods.

Conclusion: TBDQ-Net is an efficient, robust framework that unifies TBD and TBQ strengths, validated on benchmarks like DanceTrack and MOT20.

Abstract: Multi-object tracking (MOT) is dominated by two paradigms:
tracking-by-detection (TBD) and tracking-by-query (TBQ). While TBD is decoupled
and efficient, its fragmented association steps and heuristic matching
pipelines often compromise robustness in complex scenarios. TBQ provides
stronger semantic modeling through end-to-end learning, but suffers from high
training cost and slow inference due to tight coupling between detection and
association. To address these challenges, we propose TBDQ-Net, a unified
tracking-by-detection-and-query (TBDQ) framework that effectively combines the
strengths of both paradigms. Our method efficiently integrates pretrained,
high-performance detectors with an MOT-tailored associator. The associator is
lightweight and directly fetches information from the inference of detectors,
enhancing the overall efficiency of the framework. The associator is also
learnable, making it essential for fully end-to-end optimization, ensuring
robust tracking capabilities. Specifically, the associator comprises two key
modules: basic information interaction (BII) for comprehensive semantic
interaction, and content-position alignment (CPA) for semantic and positional
consistency. TBDQ-Net's effectiveness is extensively demonstrated on
DanceTrack, SportsMOT and MOT20 benchmarks. As a structurally efficient and
semantically robust tracking framework, it outperforms the leading TBD method
by 6.0 IDF1 points on DanceTrack and achieves at least 37.5% faster inference
than prominent TBQ methods.

</details>


### [434] [OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs](https://arxiv.org/pdf/2506.20960)
*Yiman Zhang, Ziheng Luo, Qiangyu Yan, Wei He, Borui Jiang, Xinghao Chen, Kai Han*

Main category: cs.CV

TL;DR: OmniEval is a benchmark for evaluating omni-modality models, featuring full-modal collaboration, diverse videos, and granular tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for evaluating models that integrate visual, auditory, and textual inputs, providing a comprehensive platform for assessing multimodal coherence.

Method: OmniEval includes 810 synchronized videos, 2617 QA pairs, and introduces a granular Grounding task. It evaluates models on 3 major task types and 12 sub-task types.

Result: The benchmark supports comprehensive evaluation of omni-modality models, highlighting their ability to leverage collaborative perception across modalities.

Conclusion: OmniEval serves as a valuable tool for assessing multimodal model performance and understanding coherence across modalities.

Abstract: In this paper, we introduce OmniEval, a benchmark for evaluating
omni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,
and textual inputs. Compared with existing benchmarks, our OmniEval has several
distinctive features: (i) Full-modal collaboration: We design evaluation tasks
that highlight the strong coupling between audio and video, requiring models to
effectively leverage the collaborative perception of all modalities; (ii)
Diversity of videos: OmniEval includes 810 audio-visual synchronized videos,
285 Chinese videos and 525 English videos; (iii) Diversity and granularity of
tasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended
questions and 1205 multiple-choice questions. These questions are divided into
3 major task types and 12 sub-task types to achieve comprehensive evaluation.
Among them, we introduce a more granular video localization task named
Grounding. Then we conduct experiments on OmniEval with several omni-modality
models. We hope that our OmniEval can provide a platform for evaluating the
ability to construct and understand coherence from the context of all
modalities. Codes and data could be found at
https://omnieval-benchmark.github.io/.

</details>


### [435] [Methodology for an Analysis of Influencing Factors on 3D Object Detection Performance](https://arxiv.org/pdf/2411.08482)
*Anton Kuznietsov, Dirk Schweickard, Steven Peters*

Main category: cs.CV

TL;DR: A novel method analyzes factors affecting 3D object detectors in automated driving, using statistical and Random Forest models to improve safety.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based detectors lack transparency, complicating safety assurance in automated driving.

Method: Combines univariate statistical analysis with Random Forest and Shapley Values to study detection errors.

Result: Identifies key factors influencing pedestrian detection errors, enabling nuanced performance analysis.

Conclusion: The approach helps uncover performance gaps and supports safer object detection system development.

Abstract: In automated driving, object detection is crucial for perceiving the
environment. Although deep learning-based detectors offer high performance,
their black-box nature complicates safety assurance. We propose a novel
methodology to analyze how object- and environment-related factors affect
LiDAR- and camera-based 3D object detectors. A statistical univariate analysis
relates each factor to pedestrian detection errors. Additionally, a Random
Forest (RF) model predicts errors from meta-information, with Shapley Values
interpreting feature importance. By capturing feature dependencies, the RF
enables a nuanced analysis of detection errors. Understanding these factors
reveals detector performance gaps and supports safer object detection system
development.

</details>


### [436] [Privacy-Preserving Video Anomaly Detection: A Survey](https://arxiv.org/pdf/2411.14565)
*Yang Liu, Siao Liu, Xiaoguang Zhu, Jielin Li, Hao Yang, Liangyu Teng, Juncen Guo, Yan Wang, Dingkang Yang, Jing Liu*

Main category: cs.CV

TL;DR: This paper reviews Privacy-Preserving Video Anomaly Detection (P2VAD), addressing gaps in current research by providing a systematic taxonomy, analyzing methods, and discussing challenges and future opportunities.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency and privacy concerns in Video Anomaly Detection (VAD) limits its real-world application, prompting the need for systematic research in P2VAD.

Method: The article systematically reviews P2VAD, defining its scope, providing a taxonomy, and analyzing learning frameworks and optimization objectives.

Result: The review highlights strengths, weaknesses, and correlations in P2VAD methods, while offering open-access resources like datasets and code.

Conclusion: The paper identifies key challenges and future opportunities in P2VAD, aiming to guide further research and deployment.

Abstract: Video Anomaly Detection (VAD) aims to automatically analyze spatiotemporal
patterns in surveillance videos collected from open spaces to detect anomalous
events that may cause harm, such as fighting, stealing, and car accidents.
However, vision-based surveillance systems such as closed-circuit television
often capture personally identifiable information. The lack of transparency and
interpretability in video transmission and usage raises public concerns about
privacy and ethics, limiting the real-world application of VAD. Recently,
researchers have focused on privacy concerns in VAD by conducting systematic
studies from various perspectives including data, features, and systems, making
Privacy-Preserving Video Anomaly Detection (P2VAD) a hotspot in the AI
community. However, current research in P2VAD is fragmented, and prior reviews
have mostly focused on methods using RGB sequences, overlooking privacy leakage
and appearance bias considerations. To address this gap, this article is the
first to systematically reviews the progress of P2VAD, defining its scope and
providing an intuitive taxonomy. We outline the basic assumptions, learning
frameworks, and optimization objectives of various approaches, analyzing their
strengths, weaknesses, and potential correlations. Additionally, we provide
open access to research resources such as benchmark datasets and available
code. Finally, we discuss key challenges and future opportunities from the
perspectives of AI development and P2VAD deployment, aiming to guide future
work in the field.

</details>


### [437] [Efficient Online Inference of Vision Transformers by Training-Free Tokenization](https://arxiv.org/pdf/2411.15397)
*Leonidas Gee, Wing Yan Li, Viktoriia Sharmanska, Novi Quadrianto*

Main category: cs.CV

TL;DR: The paper introduces Visual Word Tokenizer (VWT), a training-free method to reduce power costs in vision transformers while maintaining performance and runtime, suitable for online inference.


<details>
  <summary>Details</summary>
Motivation: High deployment costs of vision transformers hinder industrial adoption; existing compression methods are unsuitable for real-time inference due to additional fine-tuning or runtime drawbacks.

Method: VWT groups frequent visual subwords (image patches) into visual words, using intra- or inter-image statistics for sequence compression without training.

Result: VWT reduces wattage by up to 25% with only a 20% runtime increase, outperforming 8-bit quantization and token merging in power efficiency and runtime.

Conclusion: VWT is effective for efficient online inference with minimal performance compromise.

Abstract: The cost of deploying vision transformers increasingly represents a barrier
to wider industrial adoption. Existing compression techniques require
additional end-to-end fine-tuning or incur a significant drawback to runtime,
making them ill-suited for online (real-time) inference, where a prediction is
made on any new input as it comes in. We introduce the $\textbf{Visual Word
Tokenizer}$ (VWT), a training-free method for reducing power costs while
retaining performance and runtime. The VWT groups visual subwords (image
patches) that are frequently used into visual words while infrequent ones
remain intact. To do so, $\textit{intra}$-image or $\textit{inter}$-image
statistics are leveraged to identify similar visual concepts for sequence
compression. Experimentally, we demonstrate a reduction in wattage of up to 25%
with only a 20% increase in runtime at most. Comparative approaches of 8-bit
quantization and token merging achieve a lower or similar power efficiency but
exact a higher toll on runtime (up to 100% or more). Our results indicate that
VWTs are well-suited for efficient online inference with a marginal compromise
on performance.

</details>


### [438] [GLS: Geometry-aware 3D Language Gaussian Splatting](https://arxiv.org/pdf/2411.18066)
*Jiaxiong Qiu, Liu Liu, Xinjie Wang, Tianwei Lin, Wei Sui, Zhizhong Su*

Main category: cs.CV

TL;DR: GLS is a unified framework for 3D surface reconstruction and open-vocabulary segmentation using 3D Gaussian Splatting, improving sharpness and smoothness.


<details>
  <summary>Details</summary>
Motivation: To enhance performance in 3D surface reconstruction and open-vocabulary segmentation by leveraging geometric cues and CLIP features.

Method: Uses surface normal prior for reconstruction and 2D CLIP features for segmentation, optimizing depth and view consistency.

Result: GLS outperforms state-of-the-art methods on MuSHRoom, ScanNet++, and LERF-OVS datasets.

Conclusion: Joint optimization of reconstruction and segmentation improves results, demonstrating GLS's effectiveness.

Abstract: Recently, 3D Gaussian Splatting (3DGS) has achieved impressive performance on
indoor surface reconstruction and 3D open-vocabulary segmentation. This paper
presents GLS, a unified framework of 3D surface reconstruction and
open-vocabulary segmentation based on 3DGS. GLS extends two fields by improving
their sharpness and smoothness. For indoor surface reconstruction, we introduce
surface normal prior as a geometric cue to guide the rendered normal, and use
the normal error to optimize the rendered depth. For 3D open-vocabulary
segmentation, we employ 2D CLIP features to guide instance features and enhance
the surface smoothness, then utilize DEVA masks to maintain their view
consistency. Extensive experiments demonstrate the effectiveness of jointly
optimizing surface reconstruction and 3D open-vocabulary segmentation, where
GLS surpasses state-of-the-art approaches of each task on MuSHRoom, ScanNet++
and LERF-OVS datasets. Project webpage:
https://jiaxiongq.github.io/GLS_ProjectPage.

</details>


### [439] [Vision Technologies with Applications in Traffic Surveillance Systems: A Holistic Survey](https://arxiv.org/pdf/2412.00348)
*Wei Zhou, Li Yang, Lei Zhao, Runyu Zhang, Yifan Cui, Hongpu Huang, Kun Qie, Chen Wang*

Main category: cs.CV

TL;DR: This paper reviews vision technologies in Traffic Surveillance Systems (TSS), covering low-level and high-level perception tasks, identifies current limitations, and proposes solutions and trends, including foundation models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in comprehensive reviews of TSS by integrating low-level and high-level perception tasks and addressing emerging technologies.

Method: Systematic review of vision technologies in TSS, categorizing methods, evaluating performance, and analyzing limitations and solutions.

Result: Identifies five key limitations in TSS and proposes five categories of solutions, highlighting the potential of foundation models.

Conclusion: Provides a unified framework for TSS, offering a roadmap for integrating emerging technologies to enhance system capabilities.

Abstract: Traffic Surveillance Systems (TSS) have become increasingly crucial in modern
intelligent transportation systems, with vision technologies playing a central
role for scene perception and understanding. While existing surveys typically
focus on isolated aspects of TSS, a comprehensive analytical framework bridging
low-level and high-level perception tasks, particularly considering emerging
technologies, remains lacking. This paper presents a systematic review of
vision technologies in TSS, examining both low-level perception tasks (object
detection, classification, and tracking) and high-level perception tasks
(parameter estimation, anomaly detection, and behavior understanding).
Specifically, we first provide a detailed methodological categorization and
comprehensive performance evaluation for each task. Our investigation reveals
five fundamental limitations in current TSS: perceptual data degradation in
complex scenarios, data-driven learning constraints, semantic understanding
gaps, sensing coverage limitations and computational resource demands. To
address these challenges, we systematically analyze five categories of current
approaches and potential trends: advanced perception enhancement, efficient
learning paradigms, knowledge-enhanced understanding, cooperative sensing
frameworks and efficient computing frameworks, critically assessing their
real-world applicability. Furthermore, we evaluate the transformative potential
of foundation models in TSS, which exhibit remarkable zero-shot learning
abilities, strong generalization, and sophisticated reasoning capabilities
across diverse tasks. This review provides a unified analytical framework
bridging low-level and high-level perception tasks, systematically analyzes
current limitations and solutions, and presents a structured roadmap for
integrating emerging technologies, particularly foundation models, to enhance
TSS capabilities.

</details>


### [440] [Advancing Textual Prompt Learning with Anchored Attributes](https://arxiv.org/pdf/2412.09442)
*Zheng Li, Yibing Song, Ming-Ming Cheng, Xiang Li, Jian Yang*

Main category: cs.CV

TL;DR: ATPrompt enhances vision-language models by using universal attributes to align images with unknown categories, improving alignment through attribute-category hybrid prompts.


<details>
  <summary>Details</summary>
Motivation: Current prompt learning methods fail to align images with unknown categories, limiting their applicability.

Method: ATPrompt introduces attribute-anchored textual prompts, expanding soft prompts to multi-dimensional attribute levels and using a differentiable attribute search.

Result: Experiments on 11 datasets confirm ATPrompt's effectiveness as a plug-in technique.

Conclusion: ATPrompt improves alignment between images and unknown categories with minimal computational cost.

Abstract: Textual-based prompt learning methods primarily employ multiple learnable
soft prompts and hard class tokens in a cascading manner as text inputs, aiming
to align image and text (category) spaces for downstream tasks. However,
current training is restricted to aligning images with predefined known
categories and cannot be associated with unknown categories. In this work, we
propose utilizing universal attributes as a bridge to enhance the alignment
between images and unknown categories. Specifically, we introduce an
Attribute-anchored Textual Prompt learning method for vision-language models,
named ATPrompt. This approach expands the learning space of soft prompts from
the original one-dimensional category level into the multi-dimensional
attribute level by incorporating multiple attribute tokens into the learnable
soft prompts. Through this modification, we transform the text prompt from a
category-centric form to an attribute-category hybrid form. Additionally, we
introduce a straightforward differentiable attribute search method to identify
representative and suitable attributes for downstream tasks. As an easy-to-use
plug-in technique, ATPrompt can seamlessly replace the existing basic prompt
format in textual-based methods, providing general improvements at a negligible
computational cost. Extensive experiments across 11 datasets validate the
effectiveness of our method.

</details>


### [441] [Grid: Omni Visual Generation](https://arxiv.org/pdf/2412.10718)
*Cong Wan, Xiangyang Luo, Hao Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Fan Wang, Yuhang He, Yihong Gong*

Main category: cs.CV

TL;DR: GRID reformulates temporal sequences as grid layouts to leverage existing image generation models, achieving faster inference and lower computational costs compared to specialized video models.


<details>
  <summary>Details</summary>
Motivation: Current methods for temporal visual generation are either computationally expensive or require learning temporal dynamics from scratch. GRID aims to utilize the underused potential of image models for temporal tasks.

Method: GRID transforms temporal sequences into grid layouts, enabling holistic processing. It uses a parallel flow-matching training strategy with coarse-to-fine scheduling.

Result: GRID achieves up to 67x faster inference speeds and uses <1/1000 of the computational resources of specialized models while excelling in tasks like Text-to-Video and 3D Editing.

Conclusion: GRID is an efficient and versatile solution for visual generation, bridging the gap between image and temporal tasks without sacrificing performance.

Abstract: Visual generation has witnessed remarkable progress in single-image tasks,
yet extending these capabilities to temporal sequences remains challenging.
Current approaches either build specialized video models from scratch with
enormous computational costs or add separate motion modules to image
generators, both requiring learning temporal dynamics anew. We observe that
modern image generation models possess underutilized potential in handling
structured layouts with implicit temporal understanding. Building on this
insight, we introduce GRID, which reformulates temporal sequences as grid
layouts, enabling holistic processing of visual sequences while leveraging
existing model capabilities. Through a parallel flow-matching training strategy
with coarse-to-fine scheduling, our approach achieves up to 67 faster inference
speeds while using <1/1000 of the computational resources compared to
specialized models. Extensive experiments demonstrate that GRID not only excels
in temporal tasks from Text-to-Video to 3D Editing but also preserves strong
performance in image generation, establishing itself as an efficient and
versatile omni-solution for visual generation.

</details>


### [442] [SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars](https://arxiv.org/pdf/2412.15171)
*Forrest Iandola, Stanislav Pidhorskyi, Igor Santesteban, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon, Shunsuke Saito*

Main category: cs.CV

TL;DR: SqueezeMe converts high-fidelity 3D Gaussian avatars into a lightweight form for real-time animation and rendering on mobile devices like VR headsets.


<details>
  <summary>Details</summary>
Motivation: Existing high-fidelity avatar methods require powerful GPUs and struggle with mobile device constraints.

Method: Distills neural network pose correctives into linear layers and shares parameters among Gaussians, combined with a Vulkan-based splatting pipeline.

Result: Achieves real-time (72 FPS) animation and rendering of 3 avatars on a Meta Quest 3 VR headset.

Conclusion: SqueezeMe enables mobile-grade performance for high-fidelity avatars without sacrificing quality.

Abstract: Gaussian-based human avatars have achieved an unprecedented level of visual
fidelity. However, existing approaches based on high-capacity neural networks
typically require a desktop GPU to achieve real-time performance for a single
avatar, and it remains non-trivial to animate and render such avatars on mobile
devices including a standalone VR headset due to substantially limited memory
and computational bandwidth. In this paper, we present SqueezeMe, a simple and
highly effective framework to convert high-fidelity 3D Gaussian full-body
avatars into a lightweight representation that supports both animation and
rendering with mobile-grade compute. Our key observation is that the decoding
of pose-dependent Gaussian attributes from a neural network creates
non-negligible memory and computational overhead. Inspired by blendshapes and
linear pose correctives widely used in Computer Graphics, we address this by
distilling the pose correctives learned with neural networks into linear
layers. Moreover, we further reduce the parameters by sharing the correctives
among nearby Gaussians. Combining them with a custom splatting pipeline based
on Vulkan, we achieve, for the first time, simultaneous animation and rendering
of 3 Gaussian avatars in real-time (72 FPS) on a Meta Quest 3 VR headset. Demo
videos are available at https://forresti.github.io/squeezeme.

</details>


### [443] [Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment](https://arxiv.org/pdf/2412.19326)
*Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, Limin Wang, Yi Wang*

Main category: cs.CV

TL;DR: TPO enhances MLLMs with scalable fine-grained visual understanding using learnable task tokens and multi-task co-training, achieving a 14.6% performance boost.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack fine-grained visual understanding, and existing methods compromise overall performance. TPO aims to address this gap.

Method: TPO uses differentiable task preferences and learnable task tokens to connect task-specific heads with MLLMs, leveraging rich visual labels.

Result: 14.6% improvement in multimodal performance and robust zero-shot capabilities comparable to supervised models.

Conclusion: TPO effectively scales MLLMs for fine-grained visual tasks while maintaining overall performance.

Abstract: Current multimodal large language models (MLLMs) struggle with fine-grained
or precise understanding of visuals although they give comprehensive perception
and reasoning in a spectrum of vision applications. Recent studies either
develop tool-using or unify specific visual tasks into the autoregressive
framework, often at the expense of overall multimodal performance. To address
this issue and enhance MLLMs with visual tasks in a scalable fashion, we
propose Task Preference Optimization (TPO), a novel method that utilizes
differentiable task preferences derived from typical fine-grained visual tasks.
TPO introduces learnable task tokens that establish connections between
multiple task-specific heads and the MLLM. By leveraging rich visual labels
during training, TPO significantly enhances the MLLM's multimodal capabilities
and task-specific performance. Through multi-task co-training within TPO, we
observe synergistic benefits that elevate individual task performance beyond
what is achievable through single-task training methodologies. Our
instantiation of this approach with VideoChat and LLaVA demonstrates an overall
14.6% improvement in multimodal performance compared to baseline models.
Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across
various tasks, performing comparably to state-of-the-art supervised models. The
code will be released at https://github.com/OpenGVLab/TPO

</details>


### [444] [RecConv: Efficient Recursive Convolutions for Multi-Frequency Representations](https://arxiv.org/pdf/2412.19628)
*Mingshu Zhao, Yi Luo, Yong Ouyang*

Main category: cs.CV

TL;DR: RecConv introduces a recursive decomposition strategy using small-kernel convolutions to efficiently expand the effective receptive field without exponential parameter or FLOPs growth.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and optimization challenges of large-kernel convolutions in vision transformers due to quadratic scaling of parameters and FLOPs.

Method: Recursive decomposition strategy (RecConv) constructs multi-frequency representations with small-kernel convolutions, maintaining linear parameter growth and constant FLOPs.

Result: RecNeXt-M3 outperforms RepViT-M1.1 by 1.9 AP on COCO with similar FLOPs, achieving efficient ERF expansion.

Conclusion: RecConv offers a scalable and efficient solution for designing compact networks, applicable across various modalities.

Abstract: Recent advances in vision transformers (ViTs) have demonstrated the advantage
of global modeling capabilities, prompting widespread integration of
large-kernel convolutions for enlarging the effective receptive field (ERF).
However, the quadratic scaling of parameter count and computational complexity
(FLOPs) with respect to kernel size poses significant efficiency and
optimization challenges. This paper introduces RecConv, a recursive
decomposition strategy that efficiently constructs multi-frequency
representations using small-kernel convolutions. RecConv establishes a linear
relationship between parameter growth and decomposing levels which determines
the effective receptive field $k\times 2^\ell$ for a base kernel $k$ and $\ell$
levels of decomposition, while maintaining constant FLOPs regardless of the ERF
expansion. Specifically, RecConv achieves a parameter expansion of only
$\ell+2$ times and a maximum FLOPs increase of $5/3$ times, compared to the
exponential growth ($4^\ell$) of standard and depthwise convolutions.
RecNeXt-M3 outperforms RepViT-M1.1 by 1.9 $AP^{box}$ on COCO with similar
FLOPs. This innovation provides a promising avenue towards designing efficient
and compact networks across various modalities. Codes and models can be found
at https://github.com/suous/RecNeXt.

</details>


### [445] [Can Robots "Taste" Grapes? Estimating SSC with Simple RGB Sensors](https://arxiv.org/pdf/2412.20521)
*Thomas Alessandro Ciarfuglia, Ionut Marian Motoi, Leonardo Saraceni, Daniele Nardi*

Main category: cs.CV

TL;DR: The study explores using RGB sensors to estimate Soluble Solid Content (SSC) and color in grapes for cost-effective robotic harvesting, achieving competitive accuracy compared to hyperspectral systems.


<details>
  <summary>Details</summary>
Motivation: Accurate SSC assessment is crucial for grape harvesting, but current hyperspectral methods are impractical for field use. RGB sensors offer a simpler, cost-effective alternative.

Method: Collected grape images with SSC and color labels over two seasons. Tested two approaches: a histogram-based method for efficiency and a Deep Neural Network (DNN) for accuracy.

Result: DNN achieved MAE of 1.05°Brix, while the histogram method reached 1.46°Brix, comparable to hyperspectral systems (1.27–2.20°Brix).

Conclusion: RGB sensors are viable for SSC estimation, enabling practical, cost-effective robotic harvesting with performance rivaling hyperspectral methods.

Abstract: In table grape cultivation, harvesting depends on accurately assessing fruit
quality. While some characteristics, like color, are visible, others, such as
Soluble Solid Content (SSC), or sugar content measured in degrees Brix
({\deg}Brix), require specific tools. SSC is a key quality factor that
correlates with ripeness, but lacks a direct causal relationship with color.
Hyperspectral cameras can estimate SSC with high accuracy under controlled
laboratory conditions, but their practicality in field environments is limited.
This study investigates the potential of simple RGB sensors under uncontrolled
lighting to estimate SSC and color, enabling cost-effective, robot-assisted
harvesting. Over the 2021 and 2022 summer seasons, we collected grape images
with corresponding SSC and color labels to evaluate algorithmic solutions for
SSC estimation, specifically testing for cross-seasonal and cross-device
robustness. We propose two approaches: a computationally efficient
histogram-based method for resource-constrained robots and a Deep Neural
Network (DNN) model for more complex applications. Our results demonstrate high
performance, with the DNN model achieving a Mean Absolute Error (MAE) as low as
$1.05$ {\deg}Brix on a challenging cross-device test set. The lightweight
histogram-based method also proved effective, reaching an MAE of $1.46$
{\deg}Brix. These results are highly competitive with those from hyperspectral
systems, which report errors in the $1.27$--$2.20$ {\deg}Brix range in similar
field applications.

</details>


### [446] [Compositional Generative Model of Unbounded 4D Cities](https://arxiv.org/pdf/2501.08983)
*Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu*

Main category: cs.CV

TL;DR: CityDreamer4D is a generative model for unbounded 4D city scenes, separating dynamic objects (vehicles) from static scenes (buildings) and using specialized neural fields for each component.


<details>
  <summary>Details</summary>
Motivation: Generating 4D cities is challenging due to complex structures, diverse objects, and human sensitivity to distortions in urban environments.

Method: Proposes Traffic Scenario Generator and Unbounded Layout Generator, using BEV representation and customized neural fields for buildings, vehicles, and background.

Result: Achieves state-of-the-art performance in generating realistic 4D cities, supported by datasets like OSM, GoogleEarth, and CityTopia.

Conclusion: CityDreamer4D enables downstream applications like instance editing and urban simulation, offering a robust solution for 4D city generation.

Abstract: 3D scene generation has garnered growing attention in recent years and has
made significant progress. Generating 4D cities is more challenging than 3D
scenes due to the presence of structurally complex, visually diverse objects
like buildings and vehicles, and heightened human sensitivity to distortions in
urban environments. To tackle these issues, we propose CityDreamer4D, a
compositional generative model specifically tailored for generating unbounded
4D cities. Our main insights are 1) 4D city generation should separate dynamic
objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2)
all objects in the 4D scene should be composed of different types of neural
fields for buildings, vehicles, and background stuff. Specifically, we propose
Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic
traffic scenarios and static city layouts using a highly compact BEV
representation. Objects in 4D cities are generated by combining stuff-oriented
and instance-oriented neural fields for background stuff, buildings, and
vehicles. To suit the distinct characteristics of background stuff and
instances, the neural fields employ customized generative hash grids and
periodic positional embeddings as scene parameterizations. Furthermore, we
offer a comprehensive suite of datasets for city generation, including OSM,
GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world
city layouts, while the Google Earth and CityTopia datasets deliver
large-scale, high-quality city imagery complete with 3D instance annotations.
Leveraging its compositional design, CityDreamer4D supports a range of
downstream applications, such as instance editing, city stylization, and urban
simulation, while delivering state-of-the-art performance in generating
realistic 4D cities.

</details>


### [447] [MSF: Efficient Diffusion Model Via Multi-Scale Latent Factorize](https://arxiv.org/pdf/2501.13349)
*Haohang Xu, Longyu Chen, Yichen Zhang, Shuangrui Ding, Zhipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces a diffusion framework using multi-scale latent factorization to decompose denoising targets into base and residual signals, improving efficiency and performance in high-resolution image generation.


<details>
  <summary>Details</summary>
Motivation: Conventional diffusion models face computational challenges for high-resolution images. The paper aims to address this by leveraging hierarchical signal separation principles.

Method: The framework decomposes latent features into low-frequency base and high-frequency residual signals, enabling a two-stage generation process (low-resolution base first, then high-resolution residual).

Result: Achieves FID scores of 2.08 (256x256) and 2.47 (512x512) on ImageNet, outperforming DiT baseline (2.27 and 3.04) with a 4x speed-up.

Conclusion: The proposed method offers a more efficient and effective approach to high-resolution image generation compared to conventional techniques.

Abstract: While diffusion-based generative models have made significant strides in
visual content creation, conventional approaches face computational challenges,
especially for high-resolution images, as they denoise the entire image from
noisy inputs. This contrasts with signal processing techniques, such as Fourier
and wavelet analyses, which often employ hierarchical decompositions. Inspired
by such principles, particularly the idea of signal separation, we introduce a
diffusion framework leveraging multi-scale latent factorization. Our framework
uniquely decomposes the denoising target, typically latent features from a
pretrained Variational Autoencoder, into a low-frequency base signal capturing
core structural information and a high-frequency residual signal that
contributes finer, high-frequency details like textures. This decomposition
into base and residual components directly informs our two-stage image
generation process, which first produces the low-resolution base, followed by
the generation of the high-resolution residual. Our proposed architecture
facilitates reduced sampling steps during the residual learning stage, owing to
the inherent ease of modeling residual information, which confers advantages
over conventional full-resolution generation techniques. This specific approach
of decomposing the signal into a base and a residual, conceptually akin to how
wavelet analysis can separate different frequency bands, yields a more
streamlined and intuitive design distinct from generic hierarchical models. Our
method, \name\ (Multi-Scale Factorization), demonstrates its effectiveness by
achieving FID scores of 2.08 ($256\times256$) and 2.47 ($512\times512$) on
class-conditional ImageNet benchmarks, outperforming the DiT baseline (2.27 and
3.04 respectively) while also delivering a $4\times$ speed-up with the same
number of sampling steps.

</details>


### [448] [ReferDINO: Referring Video Object Segmentation with Visual Grounding Foundations](https://arxiv.org/pdf/2501.14607)
*Tianming Liang, Kun-Yu Lin, Chaolei Tan, Jianguo Zhang, Wei-Shi Zheng, Jian-Fang Hu*

Main category: cs.CV

TL;DR: ReferDINO is a strong RVOS model combining vision-language alignment, dense perception, and spatiotemporal reasoning, outperforming previous methods with real-time speed.


<details>
  <summary>Details</summary>
Motivation: Existing RVOS methods lack comprehensive vision-language understanding, dense prediction, and spatiotemporal reasoning, creating a performance gap.

Method: ReferDINO integrates a grounding-guided deformable mask decoder and an object-consistent temporal enhancer, with a confidence-aware query pruning strategy.

Result: ReferDINO achieves +3.9% (J&F) on Ref-YouTube-VOS and runs at 51 FPS, outperforming prior methods.

Conclusion: ReferDINO effectively addresses RVOS challenges, offering superior performance and efficiency.

Abstract: Referring video object segmentation (RVOS) aims to segment target objects
throughout a video based on a text description. This is challenging as it
involves deep vision-language understanding, pixel-level dense prediction and
spatiotemporal reasoning. Despite notable progress in recent years, existing
methods still exhibit a noticeable gap when considering all these aspects. In
this work, we propose \textbf{ReferDINO}, a strong RVOS model that inherits
region-level vision-language alignment from foundational visual grounding
models, and is further endowed with pixel-level dense perception and
cross-modal spatiotemporal reasoning. In detail, ReferDINO integrates two key
components: 1) a grounding-guided deformable mask decoder that utilizes
location prediction to progressively guide mask prediction through
differentiable deformation mechanisms; 2) an object-consistent temporal
enhancer that injects pretrained time-varying text features into inter-frame
interaction to capture object-aware dynamic changes. Moreover, a
confidence-aware query pruning strategy is designed to accelerate object
decoding without compromising model performance. Extensive experimental results
on five benchmarks demonstrate that our ReferDINO significantly outperforms
previous methods (e.g., +3.9% (\mathcal{J}&\mathcal{F}) on Ref-YouTube-VOS)
with real-time inference speed (51 FPS).

</details>


### [449] [FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers](https://arxiv.org/pdf/2501.16297)
*Renshan Zhang, Rui Shao, Gongwei Chen, Miao Zhang, Kaiwen Zhou, Weili Guan, Liqiang Nie*

Main category: cs.CV

TL;DR: FALCON model improves high-resolution MLLMs by reducing redundant tokens and ensuring visual continuity using novel visual register techniques.


<details>
  <summary>Details</summary>
Motivation: Existing high-resolution MLLMs suffer from fragmented visual encoding and excessive redundant tokens due to cropping-based approaches.

Method: FALCON introduces visual registers with ReCompact for token reduction and ReAtten for continuity in encoding.

Result: FALCON achieves a 9-fold reduction in visual tokens and superior performance on benchmarks.

Conclusion: FALCON effectively addresses redundancy and fragmentation in high-resolution MLLMs, enhancing efficiency and performance.

Abstract: The incorporation of high-resolution visual input equips multimodal large
language models (MLLMs) with enhanced visual perception capabilities for
real-world tasks. However, most existing high-resolution MLLMs rely on a
cropping-based approach to process images, which leads to fragmented visual
encoding and a sharp increase in redundant tokens. To tackle these issues, we
propose the FALCON model. FALCON introduces a novel visual register technique
to simultaneously: 1) Eliminate redundant tokens at the stage of visual
encoding. To directly address the visual redundancy present in the output of
vision encoder, we propose a Register-based Representation Compacting
(ReCompact) mechanism. This mechanism introduces a set of learnable visual
registers designed to adaptively aggregate essential information while
discarding redundancy. It enables the encoder to produce a more compact visual
representation with a minimal number of output tokens, thus eliminating the
need for an additional compression module. 2) Ensure continuity in visual
encoding. To address the potential encoding errors caused by fragmented visual
inputs, we develop a Register Interactive Attention (ReAtten) module. This
module facilitates effective and efficient information exchange across
sub-images by enabling interactions between visual registers. It ensures the
continuity of visual semantics throughout the encoding. We conduct
comprehensive experiments with FALCON on high-resolution benchmarks across a
wide range of scenarios. FALCON demonstrates superior performance with a
remarkable 9-fold reduction in visual tokens.

</details>


### [450] [OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models](https://arxiv.org/pdf/2502.01061)
*Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, Chao Liang*

Main category: cs.CV

TL;DR: OmniHuman is a Diffusion Transformer-based framework for scalable, realistic human video generation, supporting diverse inputs and driving modalities.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human animation struggle to scale like general video generation models, limiting real-world applications.

Method: OmniHuman mixes motion-related conditions during training, using a Diffusion Transformer framework with specific training principles, architecture, and inference strategies.

Result: OmniHuman achieves highly realistic videos, supports diverse portrait contents, actions, and styles, and outperforms existing audio-driven methods in flexibility and realism.

Conclusion: OmniHuman advances human video generation by scaling data-driven motion and offering versatile inputs and driving modalities.

Abstract: End-to-end human animation, such as audio-driven talking human generation,
has undergone notable advancements in the recent few years. However, existing
methods still struggle to scale up as large general video generation models,
limiting their potential in real applications. In this paper, we propose
OmniHuman, a Diffusion Transformer-based framework that scales up data by
mixing motion-related conditions into the training phase. To this end, we
introduce two training principles for these mixed conditions, along with the
corresponding model architecture and inference strategy. These designs enable
OmniHuman to fully leverage data-driven motion generation, ultimately achieving
highly realistic human video generation. More importantly, OmniHuman supports
various portrait contents (face close-up, portrait, half-body, full-body),
supports both talking and singing, handles human-object interactions and
challenging body poses, and accommodates different image styles. Compared to
existing end-to-end audio-driven methods, OmniHuman not only produces more
realistic videos, but also offers greater flexibility in inputs. It also
supports multiple driving modalities (audio-driven, video-driven and combined
driving signals). Video samples are provided on the ttfamily project page
(https://omnihuman-lab.github.io)

</details>


### [451] [PoI: A Filter to Extract Pixel of Interest from Novel View Synthesis for Scene Coordinate Regression](https://arxiv.org/pdf/2502.04843)
*Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang*

Main category: cs.CV

TL;DR: The paper proposes a dual-criteria filtering mechanism to improve camera pose estimation by filtering out suboptimal pixels in NVS-generated data, addressing artifacts like blurring and ghosting. It also introduces a coarse-to-fine PoI variant for sparse-input scenarios, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: NVS techniques like NeRF and 3DGS can enhance training data for camera pose estimation but suffer from spatial artifacts, reducing reliability, especially for SCR methods.

Method: A dual-criteria filter evaluates SCR reprojection error and gradient threshold to discard suboptimal pixels. A coarse-to-fine PoI variant is designed for sparse-input NVS.

Result: The method achieves state-of-the-art localization accuracy in indoor and outdoor benchmarks while maintaining computational efficiency.

Conclusion: The proposed approach effectively mitigates NVS artifacts and improves camera pose estimation, particularly in sparse-input scenarios.

Abstract: Novel View Synthesis (NVS) techniques, notably Neural Radiance Fields (NeRF)
and 3D Gaussian Splatting (3DGS), can augment camera pose estimation by
extending and diversifying training data. However, images generated by these
methods are often plagued by spatial artifacts such as blurring and ghosting,
undermining their reliability as training data for camera pose estimation. This
limitation is particularly critical for Scene Coordinate Regression (SCR)
methods, which aim at pixel-level 3D coordinate estimation, because rendering
artifacts directly lead to estimation inaccuracies. To address this challenge,
we propose a dual-criteria filtering mechanism that dynamically identifies and
discards suboptimal pixels during training. The dual-criteria filter evaluates
two concurrent metrics: (1) real-time SCR reprojection error, and (2) gradient
threshold, across the coordinate regression domain. In addition, for visual
localization problems in sparse-input scenarios, it becomes even more necessary
to use NVS-generated data to assist localization. We design a coarse-to-fine
Points of Interest (PoI) variant using sparse-input NVS to solve this problem.
Experiments across indoor and outdoor benchmarks confirm our method's efficacy,
achieving state-of-the-art localization accuracy while maintaining
computational efficiency.

</details>


### [452] [Grounding Creativity in Physics: A Brief Survey of Physical Priors in AIGC](https://arxiv.org/pdf/2502.07007)
*Siwei Meng, Yawei Luo, Ping Liu*

Main category: cs.CV

TL;DR: This survey reviews physics-aware generative methods for 3D and 4D content, focusing on integrating physical principles to improve realism and structural integrity.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated content methods often neglect physical principles, causing unrealistic artifacts. Incorporating physics priors is crucial for enhancing realism.

Method: The survey categorizes methods by representation types (vision-based, NeRF-based, Gaussian Splatting) and explores 4D techniques with physical simulations.

Result: A comparative analysis highlights strengths, limitations, and suitability of methods for different materials and dynamics.

Conclusion: The survey bridges the gap between generative models and physical realism, inspiring future research in physically consistent content generation.

Abstract: Recent advancements in AI-generated content have significantly improved the
realism of 3D and 4D generation. However, most existing methods prioritize
appearance consistency while neglecting underlying physical principles, leading
to artifacts such as unrealistic deformations, unstable dynamics, and
implausible objects interactions. Incorporating physics priors into generative
models has become a crucial research direction to enhance structural integrity
and motion realism. This survey provides a review of physics-aware generative
methods, systematically analyzing how physical constraints are integrated into
3D and 4D generation. First, we examine recent works in incorporating physical
priors into static and dynamic 3D generation, categorizing methods based on
representation types, including vision-based, NeRF-based, and Gaussian
Splatting-based approaches. Second, we explore emerging techniques in 4D
generation, focusing on methods that model temporal dynamics with physical
simulations. Finally, we conduct a comparative analysis of major methods,
highlighting their strengths, limitations, and suitability for different
materials and motion dynamics. By presenting an in-depth analysis of
physics-grounded AIGC, this survey aims to bridge the gap between generative
models and physical realism, providing insights that inspire future research in
physically consistent content generation.

</details>


### [453] [HumanGif: Single-View Human Diffusion with Generative Prior](https://arxiv.org/pdf/2502.12080)
*Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji*

Main category: cs.CV

TL;DR: HumanGif is a single-view human diffusion model for 3D avatar creation, leveraging generative priors and a Human NeRF module for realistic, view-consistent results.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with realistic, coherent avatars from single images due to limited input information.

Method: Uses a single-view-conditioned diffusion process with generative priors and a Human NeRF module for aligned features. Introduces an image-level loss for optimization.

Result: Achieves best perceptual performance and generalizability on multiple datasets.

Conclusion: HumanGif advances single-view 3D human synthesis with improved realism and consistency.

Abstract: Previous 3D human creation methods have made significant progress in
synthesizing view-consistent and temporally aligned results from sparse-view
images or monocular videos. However, it remains challenging to produce
perpetually realistic, view-consistent, and temporally coherent human avatars
from a single image, as limited information is available in the single-view
input setting. Motivated by the success of 2D character animation, we propose
HumanGif, a single-view human diffusion model with generative prior.
Specifically, we formulate the single-view-based 3D human novel view and pose
synthesis as a single-view-conditioned human diffusion process, utilizing
generative priors from foundational diffusion models to complement the missing
information. To ensure fine-grained and consistent novel view and pose
synthesis, we introduce a Human NeRF module in HumanGif to learn spatially
aligned features from the input image, implicitly capturing the relative camera
and human pose transformation. Furthermore, we introduce an image-level loss
during optimization to bridge the gap between latent and image spaces in
diffusion models. Extensive experiments on RenderPeople, DNA-Rendering, THuman
2.1, and TikTok datasets demonstrate that HumanGif achieves the best perceptual
performance, with better generalizability for novel view and pose synthesis.

</details>


### [454] [Finer Disentanglement of Aleatoric Uncertainty Can Accelerate Chemical Histopathology Imaging](https://arxiv.org/pdf/2502.20532)
*Ji-Hun Oh, Kianoush Falahkheirkhah, Rohit Bhargava*

Main category: cs.CV

TL;DR: An adaptive strategy for label-free chemical imaging improves data acquisition speed by initially scanning low-information areas quickly and selectively re-imaging high-uncertainty regions for better quality.


<details>
  <summary>Details</summary>
Motivation: To address the slow data acquisition speed in label-free chemical imaging for digital pathology workflows.

Method: Proposes an adaptive strategy: quick scan of low-information areas, identify high-uncertainty regions, and selectively re-image them. Introduces a fine-grained disentanglement method to separate resolvable from irresolvable uncertainties.

Result: Applied to infrared spectroscopic imaging of breast tissues, achieving superior segmentation performance.

Conclusion: First study to focus on fine-grained uncertainty disentanglement in dynamic image spaces, streamlining histopathology workflows.

Abstract: Label-free chemical imaging holds significant promise for improving digital
pathology workflows, but data acquisition speed remains a limiting factor. To
address this gap, we propose an adaptive strategy-initially scan the low
information (LI) content of the entire tissue quickly, identify regions with
high aleatoric uncertainty (AU), and selectively re-image them at better
quality to capture higher information (HI) details. The primary challenge lies
in distinguishing between high-AU regions mitigable through HI imaging and
those that are not. However, since existing uncertainty frameworks cannot
separate such AU subcategories, we propose a fine-grained disentanglement
method based on post-hoc latent space analysis to unmix resolvable from
irresolvable high-AU regions. We apply our approach to streamline infrared
spectroscopic imaging of breast tissues, achieving superior downstream
segmentation performance. This marks the first study focused on fine-grained AU
disentanglement within dynamic image spaces (LI-to-HI), with novel application
to streamline histopathology.

</details>


### [455] [BST: Badminton Stroke-type Transformer for Skeleton-based Action Recognition in Racket Sports](https://arxiv.org/pdf/2502.21085)
*Jing-Yuan Chang*

Main category: cs.CV

TL;DR: A novel video segmentation strategy for badminton broadcast matches is introduced, combined with Human Pose Estimation and shuttlecock trajectory detection, to classify player stroke-types using the proposed Badminton Stroke-type Transformer (BST).


<details>
  <summary>Details</summary>
Motivation: Badminton's fast-paced nature poses challenges for computer vision tasks like player identification, court line detection, and stroke-type classification.

Method: Segmented frames are processed by Human Pose Estimation and shuttlecock trajectory detection models. The extracted data (joints, trajectories, player positions) is input into the BST for stroke-type classification.

Result: The method outperforms previous state-of-the-art on the ShuttleSet dataset, demonstrating the importance of leveraging ball trajectory for racket sports action recognition.

Conclusion: Effectively using ball trajectory is a promising trend for action recognition in racket sports, as shown by the BST's superior performance.

Abstract: Badminton, known for having the fastest ball speeds among all sports,
presents significant challenges to the field of computer vision, including
player identification, court line detection, shuttlecock trajectory tracking,
and player stroke-type classification. In this paper, we introduce a novel
video segmentation strategy to extract frames of each player's racket swing in
a badminton broadcast match. These segmented frames are then processed by two
existing models: one for Human Pose Estimation to obtain player skeletal
joints, and the other for shuttlecock trajectory detection to extract
shuttlecock trajectories. Leveraging these joints, trajectories, and player
positions as inputs, we propose Badminton Stroke-type Transformer (BST) to
classify player stroke-types in singles. To the best of our knowledge,
experimental results demonstrate that our method outperforms the previous
state-of-the-art on the largest publicly available badminton video dataset,
ShuttleSet, which shows that effectively leveraging ball trajectory is likely
to be a trend for racket sports action recognition.

</details>


### [456] [CarGait: Cross-Attention based Re-ranking for Gait recognition](https://arxiv.org/pdf/2503.03501)
*Gavriel Habib, Noa Barzilay, Or Shimshi, Rami Ben-Ari, Nir Darshan*

Main category: cs.CV

TL;DR: CarGait introduces a cross-attention re-ranking method to improve gait recognition by refining top-K predictions using fine-grained correlations between gait sequences.


<details>
  <summary>Details</summary>
Motivation: Existing single-stage gait recognition models struggle with hard negatives in top ranks, limiting Rank-1 accuracy.

Method: CarGait uses cross-attention between gait strips to re-order the top-K list, enhancing fine-grained correlations.

Result: Experiments on three datasets and seven models show consistent Rank-1,5 accuracy improvements, outperforming existing re-ranking methods.

Conclusion: CarGait effectively boosts gait recognition performance, especially at higher ranks, and is adaptable to existing models.

Abstract: Gait recognition is a computer vision task that identifies individuals based
on their walking patterns. Gait recognition performance is commonly evaluated
by ranking a gallery of candidates and measuring the accuracy at the top
Rank-$K$. Existing models are typically single-staged, i.e. searching for the
probe's nearest neighbors in a gallery using a single global feature
representation. Although these models typically excel at retrieving the correct
identity within the top-$K$ predictions, they struggle when hard negatives
appear in the top short-list, leading to relatively low performance at the
highest ranks (e.g., Rank-1). In this paper, we introduce CarGait, a
Cross-Attention Re-ranking method for gait recognition, that involves
re-ordering the top-$K$ list leveraging the fine-grained correlations between
pairs of gait sequences through cross-attention between gait strips. This
re-ranking scheme can be adapted to existing single-stage models to enhance
their final results. We demonstrate the capabilities of CarGait by extensive
experiments on three common gait datasets, Gait3D, GREW, and OU-MVLP, and seven
different gait models, showing consistent improvements in Rank-1,5 accuracy,
superior results over existing re-ranking methods, and strong baselines.

</details>


### [457] [USP: Unified Self-Supervised Pretraining for Image Generation and Understanding](https://arxiv.org/pdf/2503.06132)
*Xiangxiang Chu, Renda Li, Yong Wang*

Main category: cs.CV

TL;DR: USP is a framework that uses masked latent modeling in a VAE latent space to initialize diffusion models, improving their convergence and generation quality.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in transferring pretrained weights from vision models to diffusion models due to input mismatches and latent space usage.

Method: Proposes Unified Self-supervised Pretraining (USP) via masked latent modeling in a VAE latent space.

Result: Achieves comparable performance in understanding tasks and significantly improves diffusion model convergence and generation quality.

Conclusion: USP effectively bridges the gap between self-supervised vision models and diffusion models, enhancing their synergy.

Abstract: Recent studies have highlighted the interplay between diffusion models and
representation learning. Intermediate representations from diffusion models can
be leveraged for downstream visual tasks, while self-supervised vision models
can enhance the convergence and generation quality of diffusion models.
However, transferring pretrained weights from vision models to diffusion models
is challenging due to input mismatches and the use of latent spaces. To address
these challenges, we propose Unified Self-supervised Pretraining (USP), a
framework that initializes diffusion models via masked latent modeling in a
Variational Autoencoder (VAE) latent space. USP achieves comparable performance
in understanding tasks while significantly improving the convergence speed and
generation quality of diffusion models. Our code will be publicly available at
https://github.com/AMAP-ML/USP.

</details>


### [458] [Emulating Self-attention with Convolution for Efficient Image Super-Resolution](https://arxiv.org/pdf/2503.06671)
*Dongheon Lee, Seokju Yun, Youngmin Ro*

Main category: cs.CV

TL;DR: The paper introduces ConvAttn, a convolutionized self-attention module, to reduce Transformer overhead in image super-resolution while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational overhead of Transformers in image super-resolution by leveraging self-attention's inter-layer repetition.

Method: Proposes ConvAttn, which emulates self-attention with shared and dynamic kernels, and integrates flash attention for efficiency.

Result: ESC network improves PSNR by 0.27dB on Urban100×4, reduces latency by 3.7×, and memory usage by 6.2×.

Conclusion: ESC maintains Transformer capabilities while significantly improving efficiency, making it viable for lightweight SR tasks.

Abstract: In this paper, we tackle the high computational overhead of Transformers for
efficient image super-resolution~(SR). Motivated by the observations of
self-attention's inter-layer repetition, we introduce a convolutionized
self-attention module named Convolutional Attention~(ConvAttn) that emulates
self-attention's long-range modeling capability and instance-dependent
weighting with a single shared large kernel and dynamic kernels. By utilizing
the ConvAttn module, we significantly reduce the reliance on self-attention and
its involved memory-bound operations while maintaining the representational
capability of Transformers. Furthermore, we overcome the challenge of
integrating flash attention into the lightweight SR regime, effectively
mitigating self-attention's inherent memory bottleneck. We scale up the window
size to 32$\times$32 with flash attention rather than proposing an intricate
self-attention module, significantly improving PSNR by 0.31dB on
Urban100$\times$2 while reducing latency and memory usage by 16$\times$ and
12.2$\times$. Building on these approaches, our proposed network, termed
Emulating Self-attention with Convolution~(ESC), notably improves PSNR by 0.27
dB on Urban100$\times$4 compared to HiT-SRF, reducing the latency and memory
usage by 3.7$\times$ and 6.2$\times$, respectively. Extensive experiments
demonstrate that our ESC maintains the ability for long-range modeling, data
scalability, and the representational power of Transformers despite most
self-attention being replaced by the ConvAttn module.

</details>


### [459] [GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts](https://arxiv.org/pdf/2503.07417)
*Minwen Liao, Hao Bo Dong, Xinyi Wang, Kurban Ubul, Ziyang Yan, Yihua Shao*

Main category: cs.CV

TL;DR: GM-MoE is a novel low-light enhancement framework using a mixture-of-experts network with dynamic gating, outperforming 25 methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing low-light enhancement methods lack generalization and are task-specific. GM-MoE aims to address this by leveraging a dynamic gated mechanism and multi-scale feature fusion.

Method: GM-MoE combines a gated weight conditioning network with three specialized sub-expert networks, dynamically adjusting weights for different domains and fusing local/global features.

Result: GM-MoE achieves state-of-the-art performance on PSNR (5 benchmarks) and SSIM (4 benchmarks), surpassing 25 compared methods.

Conclusion: GM-MoE offers superior generalization and performance in low-light image enhancement, making it a versatile solution for various applications.

Abstract: Low-light enhancement has wide applications in autonomous driving, 3D
reconstruction, remote sensing, surveillance, and so on, which can
significantly improve information utilization. However, most existing methods
lack generalization and are limited to specific tasks such as image recovery.
To address these issues, we propose Gated-Mechanism Mixture-of-Experts
(GM-MoE), the first framework to introduce a mixture-of-experts network for
low-light image enhancement. GM-MoE comprises a dynamic gated weight
conditioning network and three sub-expert networks, each specializing in a
distinct enhancement task. Combining a self-designed gated mechanism that
dynamically adjusts the weights of the sub-expert networks for different data
domains. Additionally, we integrate local and global feature fusion within
sub-expert networks to enhance image quality by capturing multi-scale features.
Experimental results demonstrate that the GM-MoE achieves superior
generalization with respect to 25 compared approaches, reaching
state-of-the-art performance on PSNR on 5 benchmarks and SSIM on 4 benchmarks,
respectively.

</details>


### [460] [Think Before You Segment: High-Quality Reasoning Segmentation with GPT Chain of Thoughts](https://arxiv.org/pdf/2503.07503)
*Shiu-hong Kao, Yu-Wing Tai, Chi-Keung Tang*

Main category: cs.CV

TL;DR: ThinkFirst is a training-free reasoning segmentation framework using GPT's chain of thought to improve segmentation quality for complex, implicit, or non-visual queries.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of previous methods in handling complex cases like out-of-domain objects, blurry boundaries, occlusions, or high similarity with surroundings.

Method: Leverage GPT-4o or other MLLMs to generate detailed chain-of-thought descriptions of images, which are then used by a language-instructed segmentation assistant.

Result: Significantly improves segmentation quality in zero-shot settings, both qualitatively and quantitatively, with less sensitivity to user prompts.

Conclusion: ThinkFirst enhances reasoning segmentation by integrating detailed textual reasoning, enabling better handling of complex cases and user interaction.

Abstract: Reasoning segmentation is a challenging vision-language task that aims to
output the segmentation mask with respect to a complex, implicit, and even
non-visual query text. Previous works incorporated multimodal Large Language
Models (MLLMs) with segmentation models to approach the difficult problem.
However, their segmentation quality often falls short in complex cases,
particularly when dealing with out-of-domain objects with intricate structures,
blurry boundaries, occlusions, or high similarity with surroundings. In this
paper, we introduce ThinkFirst, a training-free reasoning segmentation
framework that leverages GPT's chain of thought to address these challenging
cases. Our approach allows GPT-4o or other powerful MLLMs to generate a
detailed, chain-of-thought description of an image. This summarized description
is then passed to a language-instructed segmentation assistant to aid the
segmentation process. Our framework allows users to easily interact with the
segmentation agent using multimodal inputs, such as easy text and image
scribbles, for successive refinement or communication. We evaluate the
performance of ThinkFirst on diverse objects. Extensive experiments show that,
this zero-shot-CoT approach significantly improves the vanilla reasoning
segmentation agent, both qualitatively and quantitatively, while being less
sensitive or critical to user-supplied prompts after Thinking First.

</details>


### [461] [Accelerate 3D Object Detection Models via Zero-Shot Attention Key Pruning](https://arxiv.org/pdf/2503.08101)
*Lizhen Xu, Xiuxiu Bai, Xiaojun Jia, Jianwu Fang, Shanmin Pang*

Main category: cs.CV

TL;DR: A zero-shot runtime pruning method (tgGBC) is proposed for transformer decoders in 3D object detection, achieving significant speedup with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational inefficiency of dense feature-based query methods in 3D object detection for edge devices.

Method: tgGBC trims keys in transformer modules based on importance scores derived from classification scores and attention maps.

Result: Achieves 1.99x speedup with <1% performance loss; some models show improved performance.

Conclusion: tgGBC is effective for efficient 3D object detection on edge devices, validated by deployment.

Abstract: Query-based methods with dense features have demonstrated remarkable success
in 3D object detection tasks. However, the computational demands of these
models, particularly with large image sizes and multiple transformer layers,
pose significant challenges for efficient running on edge devices. Existing
pruning and distillation methods either need retraining or are designed for ViT
models, which are hard to migrate to 3D detectors. To address this issue, we
propose a zero-shot runtime pruning method for transformer decoders in 3D
object detection models. The method, termed tgGBC (trim keys gradually Guided
By Classification scores), systematically trims keys in transformer modules
based on their importance. We expand the classification score to multiply it
with the attention map to get the importance score of each key and then prune
certain keys after each transformer layer according to their importance scores.
Our method achieves a 1.99x speedup in the transformer decoder of the latest
ToC3D model, with only a minimal performance loss of less than 1%.
Interestingly, for certain models, our method even enhances their performance.
Moreover, we deploy 3D detectors with tgGBC on an edge device, further
validating the effectiveness of our method. The code can be found at
https://github.com/iseri27/tg_gbc.

</details>


### [462] [Incomplete Multi-view Clustering via Diffusion Contrastive Generation](https://arxiv.org/pdf/2503.09185)
*Yuanyang Zhang, Yijie Lin, Weiqing Yan, Li Yao, Xinhang Wan, Guangyuan Li, Chao Zhang, Guanzhou Ke, Jie Xu*

Main category: cs.CV

TL;DR: A novel IMVC method, DCG, addresses limitations of imputation-based approaches by leveraging diffusion processes and contrastive learning for robust clustering.


<details>
  <summary>Details</summary>
Motivation: Existing IMVC methods rely heavily on paired data for training and produce low-diversity imputed data, leading to suboptimal clustering.

Method: DCG uses forward diffusion and reverse denoising processes, along with contrastive learning, to align generated and real views. It also employs instance-level and category-level interactive learning.

Result: DCG outperforms state-of-the-art methods in experiments.

Conclusion: DCG provides an effective, end-to-end solution for IMVC by improving data recovery and clustering performance.

Abstract: Incomplete multi-view clustering (IMVC) has garnered increasing attention in
recent years due to the common issue of missing data in multi-view datasets.
The primary approach to address this challenge involves recovering the missing
views before applying conventional multi-view clustering methods. Although
imputation-based IMVC methods have achieved significant improvements, they
still encounter notable limitations: 1) heavy reliance on paired data for
training the data recovery module, which is impractical in real scenarios with
high missing data rates; 2) the generated data often lacks diversity and
discriminability, resulting in suboptimal clustering results. To address these
shortcomings, we propose a novel IMVC method called Diffusion Contrastive
Generation (DCG). Motivated by the consistency between the diffusion and
clustering processes, DCG learns the distribution characteristics to enhance
clustering by applying forward diffusion and reverse denoising processes to
intra-view data. By performing contrastive learning on a limited set of paired
multi-view samples, DCG can align the generated views with the real views,
facilitating accurate recovery of views across arbitrary missing view
scenarios. Additionally, DCG integrates instance-level and category-level
interactive learning to exploit the consistent and complementary information
available in multi-view data, achieving robust and end-to-end clustering.
Extensive experiments demonstrate that our method outperforms state-of-the-art
approaches. The code is available at
https://github.com/zhangyuanyang21/2025-AAAI-DCG.

</details>


### [463] [YOLO-LLTS: Real-Time Low-Light Traffic Sign Detection via Prior-Guided Enhancement and Multi-Branch Feature Interaction](https://arxiv.org/pdf/2503.13883)
*Ziyu Lin, Yunfan Wu, Yuhang Ma, Junzhou Chen, Ronghui Zhang, Jiaming Wu, Guodong Yin, Liang Lin*

Main category: cs.CV

TL;DR: YOLO-LLTS is a real-time traffic sign detection algorithm for low-light conditions, featuring modules for small-object detection, multi-scale feature interaction, and image quality enhancement. It outperforms existing methods on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing traffic sign detection methods perform poorly in low-light due to feature dilution, limited interaction, and poor image quality, necessitating a specialized solution.

Method: YOLO-LLTS introduces HRFM-SOD for small-object detection, MFIA for multi-scale feature interaction, and PGFE for image quality enhancement. A new dataset, CNTSSS, is also created.

Result: YOLO-LLTS achieves state-of-the-art performance, with improvements of up to 9.8% mAP50:95 on datasets like TT100K-night and GTSDB-night. It also works effectively on edge devices.

Conclusion: YOLO-LLTS is a robust solution for low-light traffic sign detection, offering superior accuracy, speed, and real-time applicability.

Abstract: Traffic sign detection is essential for autonomous driving and Advanced
Driver Assistance Systems (ADAS). However, existing methods struggle with
low-light conditions due to issues like indistinct small-object features,
limited feature interaction, and poor image quality, which degrade detection
accuracy and speed. To address this issue, we propose YOLO-LLTS, an end-to-end
real-time traffic sign detection algorithm specifically designed for low-light
environments. YOLO-LLTS introduces three main contributions: the
High-Resolution Feature Map for Small Object Detection (HRFM-SOD) module to
enhance small-object detection by mitigating feature dilution; the Multi-branch
Feature Interaction Attention (MFIA) module to improve information extraction
through multi-scale features interaction; and the Prior-Guided Feature
Enhancement Module (PGFE) to enhance image quality by addressing noise, low
contrast, and blurriness. Additionally, we construct a novel dataset, the
Chinese Nighttime Traffic Sign Sample Set (CNTSSS), covering diverse nighttime
scenarios. Experiments show that YOLO-LLTS achieves state-of-the-art
performance, outperforming previous best methods by 2.7% mAP50 and 1.6%
mAP50:95 on TT100K-night, 1.3% mAP50 and 1.9% mAP50:95 on CNTSSS, 7.5% mAP50
and 9.8% mAP50:95 on GTSDB-night, and superior results on CCTSDB2021.
Deployment on edge devices confirms its real-time applicability and
effectiveness.

</details>


### [464] [Efficient Diffusion Training through Parallelization with Truncated Karhunen-Loève Expansion](https://arxiv.org/pdf/2503.17657)
*Yumeng Ren, Yaofang Liu, Aitor Artola, Laurent Mertz, Raymond H. Chan, Jean-michel Morel*

Main category: cs.CV

TL;DR: The paper proposes KL diffusion, a faster-converging alternative to traditional diffusion models by simplifying the Brownian motion using the Karhunen-Loève expansion.


<details>
  <summary>Details</summary>
Motivation: Slow convergence in diffusion models due to complex Brownian motion.

Method: Represents Brownian motion with truncated Karhunen-Loève expansion, introduces KL diffusion (an ODE with augmented random initials), and adapts the denoising loss function.

Result: Achieves twice faster convergence and lower FID scores than baseline models, with no extra parameters or architecture changes.

Conclusion: KL diffusion is efficient, parallelizable, and easily integrable into existing methods, offering significant performance improvements.

Abstract: Diffusion denoising models have become a popular approach for image
generation, but they often suffer from slow convergence during training. In
this paper, we identify that this slow convergence is partly due to the
complexity of the Brownian motion driving the forward-time process. To address
this, we represent the Brownian motion using the Karhunen-Lo\`eve expansion,
truncating it to a limited number of eigenfunctions. We propose a novel
ordinary differential equation with augmented random initials, termed KL
diffusion, as a new forward-time process for training and sampling. By
developing an appropriate denoising loss function, we facilitate the
integration of our KL-diffusion into existing denoising-based models. Using the
widely adopted DDIM framework as our baseline ensures a fair comparison, as our
modifications focus solely on the forward process and loss function, leaving
the network architecture and sampling methods unchanged. Our method
significantly outperforms baseline diffusion models, achieving convergence
speeds that are twice faster to reach the best FID score of the baseline and
ultimately yielding much lower FID scores. Notably, our approach allows for
highly parallelized computation, requires no additional learnable parameters,
and can be flexibly integrated into existing diffusion methods. The code will
be made publicly available.

</details>


### [465] [CountLLM: Towards Generalizable Repetitive Action Counting via Large Language Model](https://arxiv.org/pdf/2503.17690)
*Ziyu Yao, Xuxin Cheng, Zhiqi Huang, Lei Li*

Main category: cs.CV

TL;DR: CountLLM is a novel LLM-based framework for repetitive action counting in videos, leveraging textual prompts and pre-trained LLMs to improve accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for repetitive action counting rely on regression networks with limited capacity and suffer from overfitting due to narrow training sets.

Method: CountLLM uses video data and periodic text prompts, guided by a structured template and trained with a progressive multimodal paradigm.

Result: CountLLM outperforms benchmarks, especially in handling novel and out-of-domain actions.

Conclusion: CountLLM offers a promising solution for repetitive action counting with superior performance and generalization.

Abstract: Repetitive action counting, which aims to count periodic movements in a
video, is valuable for video analysis applications such as fitness monitoring.
However, existing methods largely rely on regression networks with limited
representational capacity, which hampers their ability to accurately capture
variable periodic patterns. Additionally, their supervised learning on narrow,
limited training sets leads to overfitting and restricts their ability to
generalize across diverse scenarios. To address these challenges, we propose
CountLLM, the first large language model (LLM)-based framework that takes video
data and periodic text prompts as inputs and outputs the desired counting
value. CountLLM leverages the rich clues from explicit textual instructions and
the powerful representational capabilities of pre-trained LLMs for repetitive
action counting. To effectively guide CountLLM, we develop a periodicity-based
structured template for instructions that describes the properties of
periodicity and implements a standardized answer format to ensure consistency.
Additionally, we propose a progressive multimodal training paradigm to enhance
the periodicity-awareness of the LLM. Empirical evaluations on widely
recognized benchmarks demonstrate CountLLM's superior performance and
generalization, particularly in handling novel and out-of-domain actions that
deviate significantly from the training data, offering a promising avenue for
repetitive action counting.

</details>


### [466] [CalFuse: Feature Calibration Enhanced Parameter Fusion for Class-Continual Learning](https://arxiv.org/pdf/2503.18672)
*Juncen Guo, Yang Liu, Xiaoguang Zhu, Lianlong Sun, Liangyu Teng, Jingyi Wu, Di Li, Linxiao Gong, Weiwei Jiang, Wei Zhou, Liang Song*

Main category: cs.CV

TL;DR: CalFuse enhances Class-Continual Learning (CCL) by dynamically fusing visual and text features, mitigating catastrophic forgetting while preserving generalization across modalities.


<details>
  <summary>Details</summary>
Motivation: Traditional CCL methods rely solely on visual features, limiting effectiveness in multimodal scenarios. Vision-Language Models (VLMs) offer potential but struggle with forgetting and generalization.

Method: CalFuse introduces dynamic feature calibration and parameter fusion to balance new knowledge acquisition with old knowledge retention.

Result: Experiments on CIFAR100 and ImageNet100 show CalFuse's superiority in performance.

Conclusion: CalFuse effectively addresses CCL challenges by leveraging multimodal fusion and dynamic calibration, improving adaptability and retention.

Abstract: Class-Continual Learning (CCL) enables models to continuously learn new class
knowledge while retaining previous classes, facilitating adaptation and
evolution in dynamic, real-world environments. Traditional CCL methods
primarily rely on visual features, which limits their effectiveness in complex,
multimodal scenarios. In contrast, Vision-Language Models (VLMs) show promising
potential for enhancing CCL by leveraging pre-trained knowledge and fusing
multi-modal semantic cues such as text and vision. However, existing approaches
struggle to mitigate catastrophic forgetting while preserving the
generalization strengths of VLMs across diverse modalities. To address these
challenges, we propose CalFuse, a framework for feature Calibration enhanced
parameter Fusion, which enhances dynamic knowledge fusion. CalFuse introduces a
dynamic feature calibration mechanism that iteratively adjusts the contribution
of original visual features to the final class decision, thereby preserving the
model's intrinsic generalization capability across modalities. Simultaneously,
a parameter fusion strategy effectively fuses newly acquired knowledge with
prior task parameters, maintaining a balance between acquiring new class
representations and preserving old knowledge. Experimental results on popular
benchmarks (e.g., CIFAR100 and ImageNet100) validate the superiority of the
proposed method.

</details>


### [467] [Consistency Trajectory Matching for One-Step Generative Super-Resolution](https://arxiv.org/pdf/2503.20349)
*Weiyi You, Mingyang Zhang, Leheng Zhang, Xingyu Zhou, Kexuan Shi, Shuhang Gu*

Main category: cs.CV

TL;DR: CTMSR is a distillation-free method for super-resolution that uses PF-ODE trajectories and Consistency Training to achieve high-quality results in one step, outperforming diffusion-based approaches.


<details>
  <summary>Details</summary>
Motivation: Overcoming the high inference overhead and training constraints of diffusion-based SR methods by eliminating the need for pre-trained models and distillation.

Method: Formulates a PF-ODE trajectory for deterministic mapping from LR to HR images, applies Consistency Training for one-step learning, and introduces DTM loss to align distributions.

Result: Achieves comparable or superior performance on synthetic and real datasets with minimal inference latency.

Conclusion: CTMSR provides an efficient and effective alternative to diffusion-based SR methods, enhancing realism and reducing computational costs.

Abstract: Current diffusion-based super-resolution (SR) approaches achieve commendable
performance at the cost of high inference overhead. Therefore, distillation
techniques are utilized to accelerate the multi-step teacher model into
one-step student model. Nevertheless, these methods significantly raise
training costs and constrain the performance of the student model by the
teacher model. To overcome these tough challenges, we propose Consistency
Trajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy
that is able to generate photo-realistic SR results in one step. Concretely, we
first formulate a Probability Flow Ordinary Differential Equation (PF-ODE)
trajectory to establish a deterministic mapping from low-resolution (LR) images
with noise to high-resolution (HR) images. Then we apply the Consistency
Training (CT) strategy to directly learn the mapping in one step, eliminating
the necessity of pre-trained diffusion model. To further enhance the
performance and better leverage the ground-truth during the training process,
we aim to align the distribution of SR results more closely with that of the
natural images. To this end, we propose to minimize the discrepancy between
their respective PF-ODE trajectories from the LR image distribution by our
meticulously designed Distribution Trajectory Matching (DTM) loss, resulting in
improved realism of our recovered HR images. Comprehensive experimental results
demonstrate that the proposed methods can attain comparable or even superior
capabilities on both synthetic and real datasets while maintaining minimal
inference latency.

</details>


### [468] [Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization](https://arxiv.org/pdf/2503.22352)
*Barış Batuhan Topal, Umut Özyurt, Zafer Doğan Budak, Ramazan Gokberk Cinbis*

Main category: cs.CV

TL;DR: Meta-LoRA introduces a meta-learning framework for identity personalization in text-to-image models, improving fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of consistent identity-specific outputs from limited reference images in generative models.

Method: Uses a three-layer LoRA architecture with meta-learning to separate identity-agnostic and identity-specific knowledge, optimizing adaptation time.

Result: Superior identity retention, computational efficiency, and adaptability compared to state-of-the-art methods.

Conclusion: Meta-LoRA is a promising solution for identity personalization in generative models, with released resources for further research.

Abstract: Recent advancements in text-to-image generative models, particularly latent
diffusion models (LDMs), have demonstrated remarkable capabilities in
synthesizing high-quality images from textual prompts. However, achieving
identity personalization-ensuring that a model consistently generates
subject-specific outputs from limited reference images-remains a fundamental
challenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA),
a novel framework that leverages meta-learning to encode domain-specific priors
into LoRA-based identity personalization. Our method introduces a structured
three-layer LoRA architecture that separates identity-agnostic knowledge from
identity-specific adaptation. In the first stage, the LoRA Meta-Down layers are
meta-trained across multiple subjects, learning a shared manifold that captures
general identity-related features. In the second stage, only the LoRA-Mid and
LoRA-Up layers are optimized to specialize on a given subject, significantly
reducing adaptation time while improving identity fidelity. To evaluate our
approach, we introduce Meta-PHD, a new benchmark dataset for identity
personalization, and compare Meta-LoRA against state-of-the-art methods. Our
results demonstrate that Meta-LoRA achieves superior identity retention,
computational efficiency, and adaptability across diverse identity conditions.
Our code, model weights, and dataset are released on
barisbatuhan.github.io/Meta-LoRA.

</details>


### [469] [Mitigating Knowledge Discrepancies among Multiple Datasets for Task-agnostic Unified Face Alignment](https://arxiv.org/pdf/2503.22359)
*Jiahao Xia, Min Xu, Wenjian Huang, Jianguo Zhang, Haimin Zhang, Chunxia Xiao*

Main category: cs.CV

TL;DR: The paper introduces TUFA, a unified face alignment framework that mitigates knowledge discrepancies across datasets by aligning mean face shapes on an interpretable plane, improving generalization and few-shot performance.


<details>
  <summary>Details</summary>
Motivation: Existing face alignment methods struggle with unified learning across datasets due to varying landmark annotations and limited training samples, leading to fragile robustness.

Method: The approach calculates mean face shapes for datasets, aligns them semantically on an interpretable plane, and encodes them into structure prompts to regress landmarks, unifying learning targets.

Result: TUFA enhances generalization, enables few-shot alignment, and supports zero-shot landmark localization, demonstrating significant improvements on seven benchmarks.

Conclusion: The framework successfully unifies knowledge from multiple datasets, mitigating discrepancies and boosting performance in face alignment tasks.

Abstract: Despite the similar structures of human faces, existing face alignment
methods cannot learn unified knowledge from multiple datasets with different
landmark annotations. The limited training samples in a single dataset commonly
result in fragile robustness in this field. To mitigate knowledge discrepancies
among different datasets and train a task-agnostic unified face alignment
(TUFA) framework, this paper presents a strategy to unify knowledge from
multiple datasets. Specifically, we calculate a mean face shape for each
dataset. To explicitly align these mean shapes on an interpretable plane based
on their semantics, each shape is then incorporated with a group of semantic
alignment embeddings. The 2D coordinates of these aligned shapes can be viewed
as the anchors of the plane. By encoding them into structure prompts and
further regressing the corresponding facial landmarks using image features, a
mapping from the plane to the target faces is finally established, which
unifies the learning target of different datasets. Consequently, multiple
datasets can be utilized to boost the generalization ability of the model. The
successful mitigation of discrepancies also enhances the efficiency of
knowledge transferring to a novel dataset, significantly boosts the performance
of few-shot face alignment. Additionally, the interpretable plane endows TUFA
with a task-agnostic characteristic, enabling it to locate landmarks unseen
during training in a zero-shot manner. Extensive experiments are carried on
seven benchmarks and the results demonstrate an impressive improvement in face
alignment brought by knowledge discrepancies mitigation. The code is available
at https://github.com/Jiahao-UTS/TUFA.

</details>


### [470] [Multi-encoder nnU-Net outperforms transformer models with self-supervised pretraining](https://arxiv.org/pdf/2504.03474)
*Seyedeh Sahar Taheri Otaghsara, Reza Rahmanzadeh*

Main category: cs.CV

TL;DR: A novel self-supervised Multi-encoder nnU-Net improves medical image segmentation, achieving 93.72% DSC by processing multiple MRI modalities independently.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of tumors in medical images is vital for diagnosis and treatment but is hindered by MRI variability and limited labeled data.

Method: Proposes a Multi-encoder nnU-Net architecture that independently processes MRI modalities before fusing features for segmentation.

Result: Achieves a Dice Similarity Coefficient of 93.72%, outperforming other models like vanilla nnU-Net and SegResNet.

Conclusion: The Multi-encoder nnU-Net enhances segmentation accuracy, especially with limited annotated data, by leveraging modality-specific features.

Abstract: This study addresses the essential task of medical image segmentation, which
involves the automatic identification and delineation of anatomical structures
and pathological regions in medical images. Accurate segmentation is crucial in
radiology, as it aids in the precise localization of abnormalities such as
tumors, thereby enabling effective diagnosis, treatment planning, and
monitoring of disease progression. Specifically, the size, shape, and location
of tumors can significantly influence clinical decision-making and therapeutic
strategies, making accurate segmentation a key component of radiological
workflows. However, challenges posed by variations in MRI modalities, image
artifacts, and the scarcity of labeled data complicate the segmentation task
and impact the performance of traditional models. To overcome these
limitations, we propose a novel self-supervised learning Multi-encoder nnU-Net
architecture designed to process multiple MRI modalities independently through
separate encoders. This approach allows the model to capture modality-specific
features before fusing them for the final segmentation, thus improving
accuracy. Our Multi-encoder nnU-Net demonstrates exceptional performance,
achieving a Dice Similarity Coefficient (DSC) of 93.72%, which surpasses that
of other models such as vanilla nnU-Net, SegResNet, and Swin UNETR. By
leveraging the unique information provided by each modality, the model enhances
segmentation tasks, particularly in scenarios with limited annotated data.
Evaluations highlight the effectiveness of this architecture in improving tumor
segmentation outcomes.

</details>


### [471] [OrderChain: Towards General Instruct-Tuning for Stimulating the Ordinal Understanding Ability of MLLM](https://arxiv.org/pdf/2504.04801)
*Jinhong Wang, Shuo Tong, Jian liu, Dongqi Tang, Weiqiang Wang, Wentong Li, Hongxia Xu, Danny Chen, Jintai Chen, Jian Wu*

Main category: cs.CV

TL;DR: OrderChain is a novel prompting paradigm enhancing MLLMs' ordinal regression performance via specificity and commonality modeling, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Address the underperformance of MLLMs in ordinal regression tasks by improving their ordinal understanding.

Method: Introduces OrderChain with task-aware prompts and RO-CoT for commonality modeling, plus CRD for automatic optimization.

Result: Substantial accuracy gains, e.g., 93.2% on Adience (age estimation) and outperforms SOTA by 27%.

Conclusion: OrderChain effectively augments MLLMs for OR tasks, demonstrating broad applicability and superior performance.

Abstract: Despite the remarkable progress of multimodal large language models (MLLMs),
they continue to face challenges in achieving competitive performance on
ordinal regression (OR; a.k.a. ordinal classification). To address this issue,
this paper presents OrderChain, a novel and general prompting paradigm that
improves the ordinal understanding ability of MLLMs by specificity and
commonality modeling. Specifically, our OrderChain consists of a set of
task-aware prompts to facilitate the specificity modeling of diverse OR tasks
and a new range optimization Chain-of-Thought (RO-CoT), which learns a
commonality way of thinking about OR tasks by uniformly decomposing them into
multiple small-range optimization subtasks. Further, we propose a category
recursive division (CRD) method to generate instruction candidate category
prompts to support RO-CoT automatic optimization. Comprehensive experiments
show that a Large Language and Vision Assistant (LLaVA) model with our
OrderChain improves baseline LLaVA significantly on diverse OR datasets, e.g.,
from 47.5% to 93.2% accuracy on the Adience dataset for age estimation, and
from 30.0% to 85.7% accuracy on the Diabetic Retinopathy dataset. Notably,
LLaVA with our OrderChain also remarkably outperforms state-of-the-art methods
by 27% on accuracy and 0.24 on MAE on the Adience dataset. To our best
knowledge, our OrderChain is the first work that augments MLLMs for OR tasks,
and the effectiveness is witnessed across a spectrum of OR datasets.

</details>


### [472] [InstructionBench: An Instructional Video Understanding Benchmark](https://arxiv.org/pdf/2504.05040)
*Haiwan Wei, Yitian Yuan, Xiaohan Lan, Wei Ke, Lin Ma*

Main category: cs.CV

TL;DR: InstructionBench is a benchmark for evaluating Video-LLMs' temporal reasoning in instructional videos, featuring 5k questions across 700+ videos. Closed-source models outperform open-source ones, but even GPT-4o achieves only 53.42% accuracy. A larger dataset of 19k Q&A pairs is also introduced.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on instructional video understanding, crucial for improving access to instructional content.

Method: Uses GPT-4 to create Q&A pairs (open-ended and multiple-choice) for evaluating coarse-grained event-level and fine-grained object-level reasoning. Filters exclude common-sense questions to focus on visual perception.

Result: Closed-source models outperform open-source ones, with GPT-4o achieving 53.42% accuracy, highlighting gaps in temporal reasoning.

Conclusion: InstructionBench and the larger dataset aim to advance research in instructional video understanding by providing robust evaluation tools and resources.

Abstract: Despite progress in video large language models (Video-LLMs), research on
instructional video understanding, crucial for enhancing access to
instructional content, remains insufficient. To address this, we introduce
InstructionBench, an Instructional video understanding Benchmark, which
challenges models' advanced temporal reasoning within instructional videos
characterized by their strict step-by-step flow. Employing GPT-4, we formulate
Q&A pairs in open-ended and multiple-choice formats to assess both
Coarse-Grained event-level and Fine-Grained object-level reasoning. Our
filtering strategies exclude questions answerable purely by common-sense
knowledge, focusing on visual perception and analysis when evaluating Video-LLM
models. The benchmark finally contains 5k questions across over 700 videos. We
evaluate the latest Video-LLMs on our InstructionBench, finding that
closed-source models outperform open-source ones. However, even the best model,
GPT-4o, achieves only 53.42% accuracy, indicating significant gaps in temporal
reasoning. To advance the field, we also develop a comprehensive instructional
video dataset with over 19k Q&A pairs from nearly 2.5k videos, using an
automated data generation framework, thereby enriching the community's research
resources. All data are available at
https://huggingface.co/datasets/sunwhw/InstructionBench.

</details>


### [473] [Visual Re-Ranking with Non-Visual Side Information](https://arxiv.org/pdf/2504.11134)
*Gustav Hanning, Gabrielle Flood, Viktor Larsson*

Main category: cs.CV

TL;DR: GCSA is a graph neural network-based re-ranking method for visual place recognition that leverages multi-modal side information, improving retrieval and localization tasks.


<details>
  <summary>Details</summary>
Motivation: Existing re-ranking methods rely on the same descriptors as initial retrieval, limiting improvement. GCSA aims to utilize additional side information for better performance.

Method: Proposes GCSA, a graph neural network that aggregates heterogeneous multi-modal inputs (e.g., sensor data, geometric properties) using affinity vectors.

Result: Significant improvements in image retrieval metrics and downstream visual localization on large-scale datasets.

Conclusion: GCSA effectively enhances re-ranking by incorporating diverse side information, benefiting both retrieval and localization tasks.

Abstract: The standard approach for visual place recognition is to use global image
descriptors to retrieve the most similar database images for a given query
image. The results can then be further improved with re-ranking methods that
re-order the top scoring images. However, existing methods focus on re-ranking
based on the same image descriptors that were used for the initial retrieval,
which we argue provides limited additional signal. In this work we propose
Generalized Contextual Similarity Aggregation (GCSA), which is a graph neural
network-based re-ranking method that, in addition to the visual descriptors,
can leverage other types of available side information. This can for example be
other sensor data (such as signal strength of nearby WiFi or BlueTooth
endpoints) or geometric properties such as camera poses for database images. In
many applications this information is already present or can be acquired with
low effort. Our architecture leverages the concept of affinity vectors to allow
for a shared encoding of the heterogeneous multi-modal input. Two large-scale
datasets, covering both outdoor and indoor localization scenarios, are utilized
for training and evaluation. In experiments we show significant improvement not
only on image retrieval metrics, but also for the downstream visual
localization task.

</details>


### [474] [Seedream 3.0 Technical Report](https://arxiv.org/pdf/2504.11346)
*Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xuanda Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, Weilin Huang*

Main category: cs.CV

TL;DR: Seedream 3.0 is an advanced Chinese-English bilingual image generation model with improved alignment, typography, aesthetics, and resolution, achieving faster speeds and higher quality outputs.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in Seedream 2.0, such as alignment with complex prompts, typography generation, visual aesthetics, and resolution.

Method: Enhancements include data doubling, mixed-resolution training, cross-modality RoPE, representation alignment loss, and novel acceleration techniques like consistent noise expectation.

Result: Significant improvements over Seedream 2.0, including better text-rendering, high-resolution output (up to 2K), and 4-8x speedup without quality loss.

Conclusion: Seedream 3.0 outperforms its predecessor with superior capabilities, making it suitable for professional typography and high-quality image generation.

Abstract: We present Seedream 3.0, a high-performance Chinese-English bilingual image
generation foundation model. We develop several technical improvements to
address existing challenges in Seedream 2.0, including alignment with
complicated prompts, fine-grained typography generation, suboptimal visual
aesthetics and fidelity, and limited image resolutions. Specifically, the
advancements of Seedream 3.0 stem from improvements across the entire pipeline,
from data construction to model deployment. At the data stratum, we double the
dataset using a defect-aware training paradigm and a dual-axis collaborative
data-sampling framework. Furthermore, we adopt several effective techniques
such as mixed-resolution training, cross-modality RoPE, representation
alignment loss, and resolution-aware timestep sampling in the pre-training
phase. During the post-training stage, we utilize diversified aesthetic
captions in SFT, and a VLM-based reward model with scaling, thereby achieving
outputs that well align with human preferences. Furthermore, Seedream 3.0
pioneers a novel acceleration paradigm. By employing consistent noise
expectation and importance-aware timestep sampling, we achieve a 4 to 8 times
speedup while maintaining image quality. Seedream 3.0 demonstrates significant
improvements over Seedream 2.0: it enhances overall capabilities, in particular
for text-rendering in complicated Chinese characters which is important to
professional typography generation. In addition, it provides native
high-resolution output (up to 2K), allowing it to generate images with high
visual quality.

</details>


### [475] [Mesh-Learner: Texturing Mesh with Spherical Harmonics](https://arxiv.org/pdf/2504.19938)
*Yunfei Wan, Jianheng Liu, Chunran Zheng, Jiarong Lin, Fu Zhang*

Main category: cs.CV

TL;DR: Mesh-Learner is a 3D reconstruction and rendering framework using mesh and spherical harmonic textures, compatible with rasterization pipelines and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To create a framework that integrates mesh and SH textures for view-dependent radiance learning, ensuring compatibility with rasterization pipelines and tools like Blender.

Method: Uses mesh and SH textures with a novel interpolation method for rendering and back-propagates gradients to SH Texels. Transfers only necessary SH textures to GPU for training, optimizing memory usage.

Result: Achieves state-of-the-art performance on Replica and FAST-LIVO2 datasets, outperforming methods like 3D Gaussian Splatting and M2-Mapping.

Conclusion: Mesh-Learner is efficient, scalable, and compatible with existing tools, making it suitable for various applications like 3D reconstruction and robotics.

Abstract: In this paper, we present a 3D reconstruction and rendering framework termed
Mesh-Learner that is natively compatible with traditional rasterization
pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e.,
texture filled with SH coefficients) into the learning process to learn each
mesh s view-dependent radiance end-to-end. Images are rendered by interpolating
surrounding SH Texels at each pixel s sampling point using a novel
interpolation method. Conversely, gradients from each pixel are back-propagated
to the related SH Texels in SH textures. Mesh-Learner exploits graphic features
of rasterization pipeline (texture sampling, deferred rendering) to render,
which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and
tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for
robotics) that are based on rasterization pipelines. Our system can train vast,
unlimited scenes because we transfer only the SH textures within the frustum to
the GPU for training. At other times, the SH textures are stored in CPU RAM,
which results in moderate GPU memory usage. The rendering results on
interpolation and extrapolation sequences in the Replica and FAST-LIVO2
datasets achieve state-of-the-art performance compared to existing
state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To
benefit the society, the code will be available at
https://github.com/hku-mars/Mesh-Learner.

</details>


### [476] [MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy](https://arxiv.org/pdf/2505.09450)
*Yuelin Zhang, Qingpeng Ding, Long Lei, Yongxuan Feng, Raymond Shing-Yan Tang, Shing Shin Cheng*

Main category: cs.CV

TL;DR: MrTrack is a novel aspiration needle tracker using a Mamba-based register mechanism to handle rapid reciprocating motion in ultrasound-guided FNA biopsies, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current ultrasound-guided FNA biopsies lack a needle tracker capable of handling rapid reciprocating motion, leading to degraded imaging and unreliable tracking.

Method: MrTrack employs a Mamba-based register extractor and retriever to distill and utilize temporal context from historical search maps, alongside a self-supervised loss for feature diversity.

Result: MrTrack surpasses state-of-the-art trackers in accuracy, robustness, and inference efficiency on robotic and manual biopsy datasets.

Conclusion: MrTrack effectively addresses the challenge of rapid reciprocating motion in FNA biopsies, offering a reliable and efficient tracking solution.

Abstract: Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally
invasive diagnostic procedure. However, an aspiration needle tracker addressing
rapid reciprocating motion is still missing. MrTrack, an aspiration needle
tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a
Mamba-based register extractor to sequentially distill global context from each
historical search map, storing these temporal cues in a register bank. The
Mamba-based register retriever then retrieves temporal prompts from the
register bank to provide external cues when current vision features are
temporarily unusable due to rapid reciprocating motion and imaging degradation.
A self-supervised register diversify loss is proposed to encourage feature
diversity and dimension independence within the learned register, mitigating
feature collapse. Comprehensive experiments conducted on both robotic and
manual aspiration biopsy datasets demonstrate that MrTrack not only outperforms
state-of-the-art trackers in accuracy and robustness but also achieves superior
inference efficiency. Project page: https://github.com/PieceZhang/MrTrack

</details>


### [477] [BandRC: Band Shifted Raised Cosine Activated Implicit Neural Representations](https://arxiv.org/pdf/2505.11640)
*Pandula Thennakoon, Avishka Ranasinghe, Mario De Silva, Buwaneka Epakanda, Roshan Godaliyadda, Parakrama Ekanayake, Vijitha Herath*

Main category: cs.CV

TL;DR: The paper introduces BandRC, a novel activation function for implicit neural representations (INRs), addressing challenges like spectral bias, noise robustness, and feature capture. It outperforms existing methods in tasks like image reconstruction, denoising, and super-resolution.


<details>
  <summary>Details</summary>
Motivation: Existing activation functions in INRs face issues like spectral bias, noise sensitivity, and difficulty in capturing both local and global features, along with manual tuning requirements.

Method: The authors propose BandRC, a tailored activation function, and integrate deep prior knowledge for task-specific adjustments.

Result: BandRC shows significant improvements: +5.67 dB in image reconstruction, +0.46 dB in denoising, and +1.03 dB in super-resolution over SOTA methods.

Conclusion: BandRC effectively enhances INR performance, outperforming existing activation functions across multiple vision tasks.

Abstract: In recent years, implicit neural representations (INRs) have gained
popularity in the computer vision community. This is mainly due to the strong
performance of INRs in many computer vision tasks. These networks can extract a
continuous signal representation given a discrete signal representation. In
previous studies, it has been repeatedly shown that INR performance has a
strong correlation with the activation functions used in its multilayer
perceptrons. Although numerous activation functions have been proposed that are
competitive with one another, they share some common set of challenges such as
spectral bias(Lack of sensitivity to high-frequency content in signals),
limited robustness to signal noise and difficulties in simultaneous capturing
both local and global features. and furthermore, the requirement for manual
parameter tuning. To address these issues, we introduce a novel activation
function, Band Shifted Raised Cosine Activated Implicit Neural Networks
$\textbf{(BandRC)}$ tailored to enhance signal representation capacity further.
We also incorporate deep prior knowledge extracted from the signal to adjust
the activation functions through a task-specific model. Through a mathematical
analysis and a series of experiments which include image reconstruction (with
an average PSNR improvement of +5.67 dB over the nearest counterpart across a
diverse image dataset), denoising (with a +0.46 dB increase in PSNR),
super-resolution (with a +1.03 dB improvement over the nearest State-Of-The-Art
(SOTA) method for 6X super-resolution), inpainting, and 3D shape reconstruction
we demonstrate the dominance of BandRC over existing state of the art
activation functions.

</details>


### [478] [ThinkVideo: High-Quality Reasoning Video Segmentation with Chain of Thoughts](https://arxiv.org/pdf/2505.18561)
*Shiu-hong Kao, Yu-Wing Tai, Chi-Keung Tang*

Main category: cs.CV

TL;DR: ThinkVideo is a training-free framework using MLLM's zero-shot Chain-of-Thought (CoT) to improve Reasoning Video Object Segmentation by integrating temporal and spatial information.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in temporally-sensitive queries due to poor integration of temporal and spatial data.

Method: ThinkVideo uses CoT prompts to extract object selectivities from keyframes, combining reasoning image segmentation and SAM2 video processing for mask sequences.

Result: ThinkVideo outperforms previous works in video object segmentation with explicit and implicit queries.

Conclusion: ThinkVideo offers a robust, training-free solution for complex video segmentation tasks, even in online streams.

Abstract: Reasoning Video Object Segmentation is a challenging task, which generates a
mask sequence from an input video and an implicit, complex text query. Existing
works probe into the problem by finetuning Multimodal Large Language Models
(MLLM) for segmentation-based output, while still falling short in difficult
cases on videos given temporally-sensitive queries, primarily due to the
failure to integrate temporal and spatial information. In this paper, we
propose ThinkVideo, a novel framework which leverages the zero-shot
Chain-of-Thought (CoT) capability of MLLM to address these challenges.
Specifically, ThinkVideo utilizes the CoT prompts to extract object
selectivities associated with particular keyframes, then bridging the reasoning
image segmentation model and SAM2 video processor to output mask sequences. The
ThinkVideo framework is training-free and compatible with closed-source MLLMs,
which can be applied to Reasoning Video Instance Segmentation. We further
extend the framework for online video streams, where the CoT is used to update
the object of interest when a better target starts to emerge and becomes
visible. We conduct extensive experiments on video object segmentation with
explicit and implicit queries. The results show that ThinkVideo significantly
outperforms previous works in both cases, qualitatively and quantitatively.

</details>


### [479] [Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning](https://arxiv.org/pdf/2505.20272)
*Meng Cao, Haoze Zhao, Can Zhang, Xiaojun Chang, Ian Reid, Xiaodan Liang*

Main category: cs.CV

TL;DR: Ground-R1 is a reinforcement learning framework for grounded visual reasoning in LVLMs, eliminating the need for costly annotations and achieving superior performance with emergent cognitive behaviors.


<details>
  <summary>Details</summary>
Motivation: Address unreliable outputs and limited interpretability in LVLMs by enabling grounded visual reasoning without expensive supervision like bounding boxes or external tools.

Method: Proposes Ground-R1, a two-phase RL framework: grounding phase generates evidence regions, and answering phase produces responses guided by correctness and format adherence rewards.

Result: Achieves superior performance on visual reasoning benchmarks and exhibits cognitive behaviors like uncertainty awareness and spatial perception.

Conclusion: Ground-R1 offers a scalable, interpretable alternative to existing methods for grounded visual reasoning in LVLMs.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive general
capabilities across a wide range of multi-modal tasks. However, the reasoning
processes of LVLMs often suffer from unreliable outputs and limited
interpretability. To address this, grounded visual reasoning has emerged as a
promising paradigm that enforces responses anchored on salient visual evidence
regions. However, existing approaches typically rely on costly supervision such
as bounding box annotations, chain-of-thought rationale or external tool calls,
limiting their scalability. In this work, we propose Ground-R1, a reinforcement
learning framework that enables grounded visual reasoning without requiring
explicit evidence or rationale annotations. Ground-R1 consists of a grounding
phase that generates evidence region rollouts based on format constraints, and
an answering phase that produces responses guided by both answer correctness
and format adherence rewards. Extensive experiments across multiple visual
reasoning benchmarks manifest that Ground-R1 achieves superior performance and
exhibits emergent cognitive behaviors such as uncertainty awareness, spatial
perception, and iterative refinement, offering a scalable and interpretable
alternative to existing approaches.

</details>


### [480] [LatentMove: Towards Complex Human Movement Video Generation](https://arxiv.org/pdf/2505.22046)
*Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Farid Boussaid, Aref Miri Rekavandi, Zinuo Li, Qiuhong Ke, Hamid Laga*

Main category: cs.CV

TL;DR: LatentMove is a DiT-based framework for dynamic human animation in I2V generation, addressing unnatural deformations with a conditional control branch and learnable tokens. It introduces the CHV dataset and new metrics for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing I2V methods struggle with complex, non-repetitive human movements, leading to unnatural deformations.

Method: LatentMove uses a DiT-based framework with a conditional control branch and learnable face/body tokens for consistency. The CHV dataset and new metrics evaluate robustness.

Result: LatentMove improves human animation quality, especially for rapid, intricate movements.

Conclusion: LatentMove advances I2V generation by handling dynamic human motions better, with code, dataset, and metrics made available.

Abstract: Image-to-video (I2V) generation seeks to produce realistic motion sequences
from a single reference image. Although recent methods exhibit strong temporal
consistency, they often struggle when dealing with complex, non-repetitive
human movements, leading to unnatural deformations. To tackle this issue, we
present LatentMove, a DiT-based framework specifically tailored for highly
dynamic human animation. Our architecture incorporates a conditional control
branch and learnable face/body tokens to preserve consistency as well as
fine-grained details across frames. We introduce Complex-Human-Videos (CHV), a
dataset featuring diverse, challenging human motions designed to benchmark the
robustness of I2V systems. We also introduce two metrics to assess the flow and
silhouette consistency of generated videos with their ground truth.
Experimental results indicate that LatentMove substantially improves human
animation quality--particularly when handling rapid, intricate
movements--thereby pushing the boundaries of I2V generation. The code, the CHV
dataset, and the evaluation metrics will be available at https://github.com/
--.

</details>


### [481] [INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal Prototypes and Residual Learning](https://arxiv.org/pdf/2506.03660)
*Wei Luo, Haiming Yao, Yunkang Cao, Qiyu Chen, Ang Gao, Weiming Shen, Wenyong Yu*

Main category: cs.CV

TL;DR: INP-Former is a novel anomaly detection method that extracts Intrinsic Normal Prototypes (INPs) directly from test images, avoiding reliance on external references. It achieves state-of-the-art performance across various tasks and demonstrates zero-shot capability.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods rely on comparing test images to training-set references, which limits accuracy due to alignment issues. INP-Former leverages intrinsic normal information within test images for better alignment and accuracy.

Method: INP-Former uses an INP Extractor to derive normal prototypes from test images, an INP Coherence Loss for faithful representation, and an INP-guided Decoder for reconstruction. A Soft Mining Loss prioritizes hard samples during training.

Result: INP-Former achieves top performance in single-class, multi-class, and few-shot tasks on datasets like MVTec-AD, VisA, and Real-IAD. It also shows zero-shot capability. INP-Former++ further enhances performance.

Conclusion: INP-Former offers a versatile and universal solution for anomaly detection by leveraging intrinsic normal information, outperforming existing methods and extending to zero-shot scenarios.

Abstract: Anomaly detection (AD) is essential for industrial inspection and medical
diagnosis, yet existing methods typically rely on ``comparing'' test images to
normal references from a training set. However, variations in appearance and
positioning often complicate the alignment of these references with the test
image, limiting detection accuracy. We observe that most anomalies manifest as
local variations, meaning that even within anomalous images, valuable normal
information remains. We argue that this information is useful and may be more
aligned with the anomalies since both the anomalies and the normal information
originate from the same image. Therefore, rather than relying on external
normality from the training set, we propose INP-Former, a novel method that
extracts Intrinsic Normal Prototypes (INPs) directly from the test image.
Specifically, we introduce the INP Extractor, which linearly combines normal
tokens to represent INPs. We further propose an INP Coherence Loss to ensure
INPs can faithfully represent normality for the testing image. These INPs then
guide the INP-guided Decoder to reconstruct only normal tokens, with
reconstruction errors serving as anomaly scores. Additionally, we propose a
Soft Mining Loss to prioritize hard-to-optimize samples during training.
INP-Former achieves state-of-the-art performance in single-class, multi-class,
and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a
versatile and universal solution for AD. Remarkably, INP-Former also
demonstrates some zero-shot AD capability. Furthermore, we propose a soft
version of the INP Coherence Loss and enhance INP-Former by incorporating
residual learning, leading to the development of INP-Former++. The proposed
method significantly improves detection performance across single-class,
multi-class, semi-supervised, few-shot, and zero-shot settings.

</details>


### [482] [APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval](https://arxiv.org/pdf/2506.04953)
*Hong Gao, Yiming Bao, Xuezhen Tu, Bin Zhong, Minling Zhang*

Main category: cs.CV

TL;DR: APVR is a training-free framework for hour-level video understanding in MLLMs, using hierarchical visual information retrieval to overcome memory and resource constraints.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with long video understanding due to high information volume and memory/resource limitations. Existing training-free methods rely on incomplete visual data, limiting performance.

Method: APVR uses two components: Pivot Frame Retrieval (query expansion and iterative scoring) and Pivot Token Retrieval (query-aware token selection). This dual approach processes hour-long videos efficiently.

Result: APVR achieves 64.9% on LongVideoBench and 68.4% on VideoMME, outperforming both training-free and training-based methods.

Conclusion: APVR effectively addresses memory and resource constraints in MLLMs for long video understanding, offering plug-and-play integration with existing architectures.

Abstract: Current multimodal large language models (MLLMs) struggle with hour-level
video understanding, facing significant challenges not only in modeling the
substantial information volume of long videos but also in overcoming the memory
wall and resource constraints during both training and inference. Although
recent training-free approaches have alleviated resource demands by compressing
visual features, their reliance on incomplete visual information limits the
performance potential. To address these limitations, we propose
\textbf{A}daptive \textbf{P}ivot \textbf{V}isual information \textbf{R}etrieval
(\textbf{APVR}), a training-free framework that hierarchically retrieves and
retains sufficient and important visual information. It breakthroughs the
memory wall limitation via two complementary components: Pivot Frame Retrieval
employs query expansion and iterative spatio-semantic confidence scoring to
identify relevant video frames, and Pivot Token Retrieval performs query-aware
attention-driven token selection within up to 1024 pivot frames. This dual
granularity approach enables the processing of hour-long videos while
maintaining semantic fidelity. Experimental validations demonstrate significant
performance improvements, achieving 64.9\% on LongVideoBench and 68.4\% on
VideoMME, which are state-of-the-art results for both training-free and
training-based approaches. Meanwhile, our method provides plug-and-play
integration capability with existing MLLM architectures.

</details>


### [483] [Structure-Aware Radar-Camera Depth Estimation](https://arxiv.org/pdf/2506.05008)
*Fuyi Zhang, Zhu Yu, Chunhao Li, Runmin Zhang, Xiaokai Bai, Zili Zhou, Si-Yuan Cao, Fang Wang, Hui-Liang Shen*

Main category: cs.CV

TL;DR: SA-RCD is a structure-aware radar-camera depth estimation framework that improves dense depth maps by leveraging RGB structural priors and a multi-scale network.


<details>
  <summary>Details</summary>
Motivation: Standalone radar for depth perception is limited by sparsity and noise, and current radar-camera methods fail to produce satisfactory dense depth maps due to rigid region constraints.

Method: Proposes a structure-aware strategy for radar depth enhancement and a Multi-Scale Structure Guided Network to refine radar features and preserve details.

Result: SA-RCD achieves state-of-the-art performance on the nuScenes dataset.

Conclusion: The framework effectively addresses sparsity and noise issues in radar-camera depth estimation, producing accurate and detailed dense depth maps.

Abstract: Radar has gained much attention in autonomous driving due to its
accessibility and robustness. However, its standalone application for depth
perception is constrained by issues of sparsity and noise. Radar-camera depth
estimation offers a more promising complementary solution. Despite significant
progress, current approaches fail to produce satisfactory dense depth maps, due
to the unsatisfactory processing of the sparse and noisy radar data. They
constrain the regions of interest for radar points in rigid rectangular
regions, which may introduce unexpected errors and confusions. To address these
issues, we develop a structure-aware strategy for radar depth enhancement,
which provides more targeted regions of interest by leveraging the structural
priors of RGB images. Furthermore, we design a Multi-Scale Structure Guided
Network to enhance radar features and preserve detailed structures, achieving
accurate and structure-detailed dense metric depth estimation. Building on
these, we propose a structure-aware radar-camera depth estimation framework,
named SA-RCD. Extensive experiments demonstrate that our SA-RCD achieves
state-of-the-art performance on the nuScenes dataset. Our code will be
available at https://github.com/FreyZhangYeh/SA-RCD.

</details>


### [484] [Towards Vision-Language-Garment Models for Web Knowledge Garment Understanding and Generation](https://arxiv.org/pdf/2506.05210)
*Jan Ackermann, Kiyohiro Nakayama, Guandao Yang, Tong Wu, Gordon Wetzstein*

Main category: cs.CV

TL;DR: VLG is a vision-language-garment model for generating garments from text and images, showing promising zero-shot transfer to unseen styles.


<details>
  <summary>Details</summary>
Motivation: Explore the generalization of multimodal foundation models in specialized domains like garment generation.

Method: Introduce VLG, a model synthesizing garments from textual and visual inputs, tested for zero-shot generalization.

Result: Preliminary results show VLG effectively transfers web-scale reasoning to unseen garment styles.

Conclusion: Multimodal foundation models like VLG can adapt well to specialized domains such as fashion design.

Abstract: Multimodal foundation models have demonstrated strong generalization, yet
their ability to transfer knowledge to specialized domains such as garment
generation remains underexplored. We introduce VLG, a vision-language-garment
model that synthesizes garments from textual descriptions and visual imagery.
Our experiments assess VLG's zero-shot generalization, investigating its
ability to transfer web-scale reasoning to unseen garment styles and prompts.
Preliminary results indicate promising transfer capabilities, highlighting the
potential for multimodal foundation models to adapt effectively to specialized
domains like fashion design.

</details>


### [485] [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/pdf/2506.09113)
*Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, Xunsong Li, Yifu Li, Shanchuan Lin, Zhijie Lin, Jiawei Liu, Shu Liu, Xiaonan Nie, Zhiwu Qing, Yuxi Ren, Li Sun, Zhi Tian, Rui Wang, Sen Wang, Guoqiang Wei, Guohong Wu, Jie Wu, Ruiqi Xia, Fei Xiao, Xuefeng Xiao, Jiangqiao Yan, Ceyuan Yang, Jianchao Yang, Runkai Yang, Tao Yang, Yihang Yang, Zilyu Ye, Xuejiao Zeng, Yan Zeng, Heng Zhang, Yang Zhao, Xiaozheng Zheng, Peihao Zhu, Jiaxin Zou, Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.0 is a high-performance video generation model addressing prompt adherence, motion plausibility, and visual quality through multi-source data, efficient architecture, post-training optimizations, and acceleration.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle to balance prompt following, motion plausibility, and visual quality. Seedance 1.0 aims to overcome these challenges.

Method: The model integrates multi-source data curation, efficient architecture design, post-training optimizations (fine-tuning, RLHF), and acceleration techniques (distillation, system optimizations).

Result: Seedance 1.0 achieves high-quality, fast video generation (41.4s for 5s 1080p video) with superior spatiotemporal fluidity, prompt adherence, and multi-shot coherence.

Conclusion: Seedance 1.0 outperforms state-of-the-art models in video generation quality, speed, and adherence to complex instructions.

Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements
in video generation, yet current foundational model still face critical
challenges in simultaneously balancing prompt following, motion plausibility,
and visual quality. In this report, we introduce Seedance 1.0, a
high-performance and inference-efficient video foundation generation model that
integrates several core technical improvements: (i) multi-source data curation
augmented with precision and meaningful video captioning, enabling
comprehensive learning across diverse scenarios; (ii) an efficient architecture
design with proposed training paradigm, which allows for natively supporting
multi-shot generation and jointly learning of both text-to-video and
image-to-video tasks. (iii) carefully-optimized post-training approaches
leveraging fine-grained supervised fine-tuning, and video-specific RLHF with
multi-dimensional reward mechanisms for comprehensive performance improvements;
(iv) excellent model acceleration achieving ~10x inference speedup through
multi-stage distillation strategies and system-level optimizations. Seedance
1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds
(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance
1.0 stands out with high-quality and fast video generation having superior
spatiotemporal fluidity with structural stability, precise instruction
adherence in complex multi-subject contexts, native multi-shot narrative
coherence with consistent subject representation.

</details>


### [486] [Uncertainty-Aware Remaining Lifespan Prediction from Images](https://arxiv.org/pdf/2506.13430)
*Tristan Kenneweg, Philip Kenneweg, Barbara Hammer*

Main category: cs.CV

TL;DR: A method using pretrained vision transformers predicts remaining lifespan from images with robust uncertainty quantification, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable accessible, noninvasive, and scalable health screening by predicting mortality-related outcomes from images.

Method: Leverages pretrained vision transformer models to estimate lifespan from facial and whole-body images, with Gaussian distribution-based uncertainty modeling.

Result: Achieves MAE of 7.48 years on an established dataset and improves to 4.79 and 5.07 years on new datasets, with well-calibrated uncertainty (0.62 years error).

Conclusion: Demonstrates potential for extracting medically relevant signals from images, though not yet for clinical use; code and datasets are shared for research.

Abstract: Predicting mortality-related outcomes from images offers the prospect of
accessible, noninvasive, and scalable health screening. We present a method
that leverages pretrained vision transformer foundation models to estimate
remaining lifespan from facial and whole-body images, alongside robust
uncertainty quantification. We show that predictive uncertainty varies
systematically with the true remaining lifespan, and that this uncertainty can
be effectively modeled by learning a Gaussian distribution for each sample. Our
approach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on
an established dataset, and further improves to 4.79 and 5.07 years MAE on two
new, higher-quality datasets curated and published in this work. Importantly,
our models provide well-calibrated uncertainty estimates, as demonstrated by a
bucketed expected calibration error of 0.62 years. While not intended for
clinical deployment, these results highlight the potential of extracting
medically relevant signals from images. We make all code and datasets available
to facilitate further research.

</details>


### [487] [OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models](https://arxiv.org/pdf/2506.15318)
*Lanfeng Zhong, Xin Liao, Shichuan Zhang, Shaoting Zhang, Guotai Wang*

Main category: cs.CV

TL;DR: OpenPath is a novel open-set active learning method for pathology image classification, leveraging a pre-trained Vision-Language Model to efficiently select informative samples while avoiding Out-Of-Distribution data.


<details>
  <summary>Details</summary>
Motivation: Traditional Active Learning methods struggle with Out-Of-Distribution data in real-world clinical settings, leading to inefficient annotation. OpenPath addresses this by improving sample selection purity and informativeness.

Method: OpenPath uses task-specific prompts in the first query and Diverse Informative ID Sampling (DIS) in subsequent queries, combining Prototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic Sampling (EGSS).

Result: Experiments on public pathology datasets show OpenPath outperforms state-of-the-art open-set AL methods, enhancing model performance through high-purity sample selection.

Conclusion: OpenPath effectively reduces labeling costs and improves efficiency in pathology image classification by addressing the challenges of open-set scenarios.

Abstract: Pathology image classification plays a crucial role in accurate medical
diagnosis and treatment planning. Training high-performance models for this
task typically requires large-scale annotated datasets, which are both
expensive and time-consuming to acquire. Active Learning (AL) offers a solution
by iteratively selecting the most informative samples for annotation, thereby
reducing the labeling effort. However, most AL methods are designed under the
assumption of a closed-set scenario, where all the unannotated images belong to
target classes. In real-world clinical environments, the unlabeled pool often
contains a substantial amount of Out-Of-Distribution (OOD) data, leading to low
efficiency of annotation in traditional AL methods. Furthermore, most existing
AL methods start with random selection in the first query round, leading to a
significant waste of labeling costs in open-set scenarios. To address these
challenges, we propose OpenPath, a novel open-set active learning approach for
pathological image classification leveraging a pre-trained Vision-Language
Model (VLM). In the first query, we propose task-specific prompts that combine
target and relevant non-target class prompts to effectively select
In-Distribution (ID) and informative samples from the unlabeled pool. In
subsequent queries, Diverse Informative ID Sampling (DIS) that includes
Prototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic
Sampling (EGSS) is proposed to ensure both purity and informativeness in a
query, avoiding the selection of OOD samples. Experiments on two public
pathology image datasets show that OpenPath significantly enhances the model's
performance due to its high purity of selected samples, and outperforms several
state-of-the-art open-set AL methods. The code is available at
\href{https://github.com/HiLab-git/OpenPath}{https://github.com/HiLab-git/OpenPath}..

</details>


### [488] [Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image Registration](https://arxiv.org/pdf/2506.15596)
*Kyobin Choo, Hyunkyung Han, Jinyeong Kim, Chanyong Yoon, Seong Jae Hwang*

Main category: cs.CV

TL;DR: M2M-Reg is a novel framework for multi-modal deformable image registration (DIR) that uses mono-modal similarity, addressing challenges in aligning highly disparate imaging modalities like PET and FA with structural references (MRI/CT).


<details>
  <summary>Details</summary>
Motivation: Conventional unsupervised DIR methods struggle with highly heterogeneous modalities due to unreliable similarity metrics, leading to distorted images.

Method: Proposes M2M-Reg, which trains multi-modal DIR models using mono-modal similarity, and introduces GradCyCon for diffeomorphism. Extends to semi-supervised settings without ground-truth transformations.

Result: Achieves up to 2x higher Dice Similarity Coefficient (DSC) than prior methods for PET-MRI and FA-MRI registration on the ADNI dataset.

Conclusion: M2M-Reg effectively handles highly heterogeneous multi-modal DIR, outperforming existing methods and offering seamless integration into existing models.

Abstract: In clinical practice, imaging modalities with functional characteristics,
such as positron emission tomography (PET) and fractional anisotropy (FA), are
often aligned with a structural reference (e.g., MRI, CT) for accurate
interpretation or group analysis, necessitating multi-modal deformable image
registration (DIR). However, due to the extreme heterogeneity of these
modalities compared to standard structural scans, conventional unsupervised DIR
methods struggle to learn reliable spatial mappings and often distort images.
We find that the similarity metrics guiding these models fail to capture
alignment between highly disparate modalities. To address this, we propose
M2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal
DIR models using only mono-modal similarity while preserving the established
architectural paradigm for seamless integration into existing models. We also
introduce GradCyCon, a regularizer that leverages M2M-Reg's cyclic training
scheme to promote diffeomorphism. Furthermore, our framework naturally extends
to a semi-supervised setting, integrating pre-aligned and unaligned pairs only,
without requiring ground-truth transformations or segmentation masks.
Experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset
demonstrate that M2M-Reg achieves up to 2x higher DSC than prior methods for
PET-MRI and FA-MRI registration, highlighting its effectiveness in handling
highly heterogeneous multi-modal DIR. Our code is available at
https://github.com/MICV-yonsei/M2M-Reg.

</details>


### [489] [HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis](https://arxiv.org/pdf/2506.16398)
*Peixiang Huang, Yanyan Huang, Weiqin Zhao, Junjun He, Lequan Yu*

Main category: cs.CV

TL;DR: HyperPath leverages hyperbolic space and textual descriptions to model semantic hierarchies in WSIs, improving classification via geometry-aware methods.


<details>
  <summary>Details</summary>
Motivation: Current MIL methods for WSI analysis rely on Euclidean embeddings, which inadequately capture semantic hierarchies.

Method: HyperPath integrates visual and textual features in hyperbolic space, using Angular Modality Alignment Loss and Semantic Hierarchy Consistency Loss for alignment and coherence.

Result: The method outperforms existing approaches in WSI classification tasks.

Conclusion: Hyperbolic embeddings show promise for enhancing WSI analysis by better modeling semantic hierarchies.

Abstract: Pathology is essential for cancer diagnosis, with multiple instance learning
(MIL) widely used for whole slide image (WSI) analysis. WSIs exhibit a natural
hierarchy -- patches, regions, and slides -- with distinct semantic
associations. While some methods attempt to leverage this hierarchy for
improved representation, they predominantly rely on Euclidean embeddings, which
struggle to fully capture semantic hierarchies. To address this limitation, we
propose HyperPath, a novel method that integrates knowledge from textual
descriptions to guide the modeling of semantic hierarchies of WSIs in
hyperbolic space, thereby enhancing WSI classification. Our approach adapts
both visual and textual features extracted by pathology vision-language
foundation models to the hyperbolic space. We design an Angular Modality
Alignment Loss to ensure robust cross-modal alignment, while a Semantic
Hierarchy Consistency Loss further refines feature hierarchies through
entailment and contradiction relationships and thus enhance semantic coherence.
The classification is performed with geodesic distance, which measures the
similarity between entities in the hyperbolic semantic hierarchy. This
eliminates the need for linear classifiers and enables a geometry-aware
approach to WSI analysis. Extensive experiments show that our method achieves
superior performance across tasks compared to existing methods, highlighting
the potential of hyperbolic embeddings for WSI analysis.

</details>


### [490] [AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions](https://arxiv.org/pdf/2506.17455)
*Taufikur Rahman Fuad, Sabbir Ahmed, Shahriar Ivan*

Main category: cs.CV

TL;DR: AQUA20 dataset addresses underwater visual recognition challenges by evaluating 13 deep learning models, with ConvNeXt outperforming others.


<details>
  <summary>Details</summary>
Motivation: Underwater visual recognition is hindered by distortions like turbidity and low illumination, necessitating robust datasets and models.

Method: Evaluated 13 deep learning models (CNNs and transformers) on AQUA20, a dataset of 8,171 underwater images across 20 marine species.

Result: ConvNeXt achieved top performance (Top-3 accuracy: 98.82%, Top-1: 90.69%, F1-score: 88.92%). Other models showed complexity-performance trade-offs.

Conclusion: AQUA20 is a valuable benchmark for underwater species recognition, with ConvNeXt leading but room for improvement remains.

Abstract: Robust visual recognition in underwater environments remains a significant
challenge due to complex distortions such as turbidity, low illumination, and
occlusion, which severely degrade the performance of standard vision systems.
This paper introduces AQUA20, a comprehensive benchmark dataset comprising
8,171 underwater images across 20 marine species reflecting real-world
environmental challenges such as illumination, turbidity, occlusions, etc.,
providing a valuable resource for underwater visual understanding. Thirteen
state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet,
MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were
evaluated to benchmark their performance in classifying marine species under
challenging conditions. Our experimental results show ConvNeXt achieving the
best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of
90.69%, as well as the highest overall F1-score of 88.92% with moderately large
parameter size. The results obtained from our other benchmark models also
demonstrate trade-offs between complexity and performance. We also provide an
extensive explainability analysis using GRAD-CAM and LIME for interpreting the
strengths and pitfalls of the models. Our results reveal substantial room for
improvement in underwater species recognition and demonstrate the value of
AQUA20 as a foundation for future research in this domain. The dataset is
publicly available at: https://huggingface.co/datasets/taufiktrf/AQUA20.

</details>


### [491] [Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose](https://arxiv.org/pdf/2506.17858)
*Yingcheng Liu, Peiqi Wang, Sebastian Diaz, Esra Abaci Turk, Benjamin Billot, Patricia Ellen Grant, Polina Golland*

Main category: cs.CV

TL;DR: A 3D articulated statistical fetal body model is introduced to improve fetal motion and shape analysis in MRI, addressing limitations of keypoints and segmentations.


<details>
  <summary>Details</summary>
Motivation: Existing methods (keypoints or segmentations) for fetal MRI analysis either oversimplify body structure or complicate temporal analysis due to large movements.

Method: The model, based on SMPL, iteratively estimates body pose in image space and shape in canonical pose space, trained on 19,816 MRI volumes.

Result: Achieves 3.2 mm surface alignment error for 3 mm MRI voxel size, enabling automated anthropometric measurements and intuitive visualization.

Conclusion: This is the first 3D articulated statistical fetal body model, enhancing prenatal diagnostics with robust motion and shape analysis.

Abstract: Analyzing fetal body motion and shape is paramount in prenatal diagnostics
and monitoring. Existing methods for fetal MRI analysis mainly rely on
anatomical keypoints or volumetric body segmentations. Keypoints simplify body
structure to facilitate motion analysis, but may ignore important details of
full-body shape. Body segmentations capture complete shape information but
complicate temporal analysis due to large non-local fetal movements. To address
these limitations, we construct a 3D articulated statistical fetal body model
based on the Skinned Multi-Person Linear Model (SMPL). Our algorithm
iteratively estimates body pose in the image space and body shape in the
canonical pose space. This approach improves robustness to MRI motion artifacts
and intensity distortions, and reduces the impact of incomplete surface
observations due to challenging fetal poses. We train our model on
segmentations and keypoints derived from $19,816$ MRI volumes across $53$
subjects. Our model captures body shape and motion across time series and
provides intuitive visualization. Furthermore, it enables automated
anthropometric measurements traditionally difficult to obtain from
segmentations and keypoints. When tested on unseen fetal body shapes, our
method yields a surface alignment error of $3.2$ mm for $3$ mm MRI voxel size.
To our knowledge, this represents the first 3D articulated statistical fetal
body model, paving the way for enhanced fetal motion and shape analysis in
prenatal diagnostics. The code is available at
https://github.com/MedicalVisionGroup/fetal-smpl .

</details>


### [492] [Deblurring in the Wild: A Real-World Dataset from Smartphone High-Speed Videos](https://arxiv.org/pdf/2506.19445)
*Mahdi Mohd Hossain Noki, Syed Mumtahin Mahmud, Prothito Shovon Majumder, Abdul Mohaimen Al Radi, Md. Haider Ali, Md. Mosaddek Khan*

Main category: cs.CV

TL;DR: A large-scale dataset for image deblurring is introduced, created from smartphone slow-motion videos, featuring 42,000 high-resolution blur-sharp pairs. It challenges existing SOTA models, showing performance drops due to its complexity and diversity.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large, diverse, and realistic datasets for image deblurring, which limits the development of robust models.

Method: Constructed by averaging 240 frames from slow-motion videos to simulate blur, with the center frame as the sharp reference.

Result: The dataset is 10x larger and 8x more diverse than existing ones, causing significant performance degradation in SOTA models.

Conclusion: The dataset provides a challenging benchmark to advance generalizable deblurring models.

Abstract: We introduce the largest real-world image deblurring dataset constructed from
smartphone slow-motion videos. Using 240 frames captured over one second, we
simulate realistic long-exposure blur by averaging frames to produce blurry
images, while using the temporally centered frame as the sharp reference. Our
dataset contains over 42,000 high-resolution blur-sharp image pairs, making it
approximately 10 times larger than widely used datasets, with 8 times the
amount of different scenes, including indoor and outdoor environments, with
varying object and camera motions. We benchmark multiple state-of-the-art
(SOTA) deblurring models on our dataset and observe significant performance
degradation, highlighting the complexity and diversity of our benchmark. Our
dataset serves as a challenging new benchmark to facilitate robust and
generalizable deblurring models.

</details>


### [493] [HMSViT: A Hierarchical Masked Self-Supervised Vision Transformer for Corneal Nerve Segmentation and Diabetic Neuropathy Diagnosis](https://arxiv.org/pdf/2506.19474)
*Xin Zhang, Liangxiu Han, Yue Shi, Yanlin Zheng, Uazman Alam, Maryam Ferdousi, Rayaz Malik*

Main category: cs.CV

TL;DR: HMSViT, a Hierarchical Masked Self-Supervised Vision Transformer, improves corneal nerve segmentation and DPN diagnosis by combining hierarchical and dual attention mechanisms with block-masked SSL, outperforming existing methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Early detection of Diabetic Peripheral Neuropathy (DPN) is crucial, but current automated methods using Corneal Confocal Microscopy (CCM) face inefficiencies in feature extraction, reliance on handcrafted priors, and limited data.

Method: HMSViT employs pooling-based hierarchical and dual attention mechanisms with absolute positional encoding for multi-scale feature extraction. It uses block-masked self-supervised learning (SSL) to reduce reliance on labeled data and a multi-scale decoder for segmentation and classification.

Result: HMSViT achieves 61.34% mIoU for nerve segmentation and 70.40% diagnostic accuracy, outperforming models like Swin Transformer and HiViT by up to 6.39% while using fewer parameters.

Conclusion: HMSViT offers robust, clinically viable results, demonstrating potential for scalable real-world diagnostic applications.

Abstract: Diabetic Peripheral Neuropathy (DPN) affects nearly half of diabetes
patients, requiring early detection. Corneal Confocal Microscopy (CCM) enables
non-invasive diagnosis, but automated methods suffer from inefficient feature
extraction, reliance on handcrafted priors, and data limitations. We propose
HMSViT, a novel Hierarchical Masked Self-Supervised Vision Transformer (HMSViT)
designed for corneal nerve segmentation and DPN diagnosis. Unlike existing
methods, HMSViT employs pooling-based hierarchical and dual attention
mechanisms with absolute positional encoding, enabling efficient multi-scale
feature extraction by capturing fine-grained local details in early layers and
integrating global context in deeper layers, all at a lower computational cost.
A block-masked self supervised learning framework is designed for the HMSViT
that reduces reliance on labelled data, enhancing feature robustness, while a
multi-scale decoder is used for segmentation and classification by fusing
hierarchical features. Experiments on clinical CCM datasets showed HMSViT
achieves state-of-the-art performance, with 61.34% mIoU for nerve segmentation
and 70.40% diagnostic accuracy, outperforming leading hierarchical models like
the Swin Transformer and HiViT by margins of up to 6.39% in segmentation
accuracy while using fewer parameters. Detailed ablation studies further reveal
that integrating block-masked SSL with hierarchical multi-scale feature
extraction substantially enhances performance compared to conventional
supervised training. Overall, these comprehensive experiments confirm that
HMSViT delivers excellent, robust, and clinically viable results, demonstrating
its potential for scalable deployment in real-world diagnostic applications.

</details>


### [494] [StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation](https://arxiv.org/pdf/2506.20756)
*Haodong Li, Chen Wang, Jiahui Lei, Kostas Daniilidis, Lingjie Liu*

Main category: cs.CV

TL;DR: StereoDiff combines stereo matching for static regions and video depth diffusion for dynamic areas to improve video depth estimation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Video depth estimation differs from image depth estimation due to varying temporal consistency needs in static and dynamic regions.

Method: StereoDiff uses a two-stage approach: stereo matching for static areas and video depth diffusion for dynamic regions.

Result: StereoDiff outperforms existing methods in zero-shot, real-world benchmarks, offering superior consistency and accuracy.

Conclusion: The synergy of stereo matching and video depth diffusion effectively addresses the unique challenges of video depth estimation.

Abstract: Recent video depth estimation methods achieve great performance by following
the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained
video diffusion models with massive data. However, we argue that video depth
estimation is not a naive extension of image depth estimation. The temporal
consistency requirements for dynamic and static regions in videos are
fundamentally different. Consistent video depth in static regions, typically
backgrounds, can be more effectively achieved via stereo matching across all
frames, which provides much stronger global 3D cues. While the consistency for
dynamic regions still should be learned from large-scale video depth data to
ensure smooth transitions, due to the violation of triangulation constraints.
Based on these insights, we introduce StereoDiff, a two-stage video depth
estimator that synergizes stereo matching for mainly the static areas with
video depth diffusion for maintaining consistent depth transitions in dynamic
areas. We mathematically demonstrate how stereo matching and video depth
diffusion offer complementary strengths through frequency domain analysis,
highlighting the effectiveness of their synergy in capturing the advantages of
both. Experimental results on zero-shot, real-world, dynamic video depth
benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,
showcasing its superior consistency and accuracy in video depth estimation.

</details>


### [495] [Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability](https://arxiv.org/pdf/2506.21042)
*Boyong He, Yuxiang Ji, Zhuoyue Tan, Liaoni Wu*

Main category: cs.CV

TL;DR: The paper proposes a method to improve domain generalization and adaptation in detectors using intermediate features from a single-step diffusion process, reducing inference time by 75% while enhancing performance. It introduces an object-centered auxiliary branch and consistency loss to balance fitness and generalization, achieving competitive results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Detectors face performance drops due to domain gaps between training and testing data. Existing methods using diffusion models struggle with high inference costs and underutilize their capabilities.

Method: Extracts intermediate features from a single-step diffusion process, introduces an object-centered auxiliary branch with box-masked images and class prompts, and applies consistency loss. Aligns diffusion and standard detectors for cross-domain performance.

Result: Achieves competitive results on 3 DA and 5 DG benchmarks, with significant efficiency in large domain shifts and low-data scenarios.

Conclusion: Demonstrates the superiority of diffusion models in domain-generalized and adaptive detection tasks, offering insights for visual perception across diverse domains.

Abstract: Detectors often suffer from performance drop due to domain gap between
training and testing data. Recent methods explore diffusion models applied to
domain generalization (DG) and adaptation (DA) tasks, but still struggle with
large inference costs and have not yet fully leveraged the capabilities of
diffusion models. We propose to tackle these problems by extracting
intermediate features from a single-step diffusion process, improving feature
collection and fusion to reduce inference time by 75% while enhancing
performance on source domains (i.e., Fitness). Then, we construct an
object-centered auxiliary branch by applying box-masked images with class
prompts to extract robust and domain-invariant features that focus on object.
We also apply consistency loss to align the auxiliary and ordinary branch,
balancing fitness and generalization while preventing overfitting and improving
performance on target domains (i.e., Generalization). Furthermore, within a
unified framework, standard detectors are guided by diffusion detectors through
feature-level and object-level alignment on source domains (for DG) and
unlabeled target domains (for DA), thereby improving cross-domain detection
performance (i.e., Transferability). Our method achieves competitive results on
3 DA benchmarks and 5 DG benchmarks. Additionally, experiments on COCO
generalization benchmark demonstrate that our method maintains significant
advantages and show remarkable efficiency in large domain shifts and low-data
scenarios. Our work shows the superiority of applying diffusion models to
domain generalized and adaptive detection tasks and offers valuable insights
for visual perception tasks across diverse domains. The code is available at
\href{https://github.com/heboyong/Fitness-Generalization-Transferability}.

</details>


### [496] [Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction](https://arxiv.org/pdf/2506.21401)
*Zhirui Gao, Renjiao Yi, Yaqiao Dai, Xuening Zhu, Wei Chen, Chenyang Zhu, Kai Xu*

Main category: cs.CV

TL;DR: The paper introduces a one-stage framework, CurveGaussian, for 3D parametric curve reconstruction from multi-view edge maps, outperforming two-stage methods by eliminating error accumulation and reducing parameters.


<details>
  <summary>Details</summary>
Motivation: Existing two-stage methods suffer from error accumulation due to disconnected optimization stages. The paper aims to directly optimize 3D parametric curves from 2D edge maps for cleaner reconstructions.

Method: Proposes CurveGaussian, a bi-directional coupling between parametric curves and edge-oriented Gaussian components, enabling differentiable rendering. Includes dynamic topology optimization for refining curves.

Result: Outperforms two-stage methods on the ABC dataset and real-world benchmarks, producing cleaner reconstructions with fewer parameters.

Conclusion: The one-stage approach with CurveGaussian is more efficient and robust, achieving superior performance in 3D parametric curve reconstruction.

Abstract: This paper presents an end-to-end framework for reconstructing 3D parametric
curves directly from multi-view edge maps. Contrasting with existing two-stage
methods that follow a sequential ``edge point cloud reconstruction and
parametric curve fitting'' pipeline, our one-stage approach optimizes 3D
parametric curves directly from 2D edge maps, eliminating error accumulation
caused by the inherent optimization gap between disconnected stages. However,
parametric curves inherently lack suitability for rendering-based multi-view
optimization, necessitating a complementary representation that preserves their
geometric properties while enabling differentiable rendering. We propose a
novel bi-directional coupling mechanism between parametric curves and
edge-oriented Gaussian components. This tight correspondence formulates a
curve-aware Gaussian representation, \textbf{CurveGaussian}, that enables
differentiable rendering of 3D curves, allowing direct optimization guided by
multi-view evidence. Furthermore, we introduce a dynamically adaptive topology
optimization framework during training to refine curve structures through
linearization, merging, splitting, and pruning operations. Comprehensive
evaluations on the ABC dataset and real-world benchmarks demonstrate our
one-stage method's superiority over two-stage alternatives, particularly in
producing cleaner and more robust reconstructions. Additionally, by directly
optimizing parametric curves, our method significantly reduces the parameter
count during training, achieving both higher efficiency and superior
performance compared to existing approaches.

</details>


### [497] [G$^{2}$D: Boosting Multimodal Learning with Gradient-Guided Distillation](https://arxiv.org/pdf/2506.21514)
*Mohammed Rakib, Arunkumar Bagavathi*

Main category: cs.CV

TL;DR: Gradient-Guided Distillation (G²D) addresses modality imbalance in multimodal learning by using a custom loss function and dynamic modality prioritization, improving performance on weak modalities.


<details>
  <summary>Details</summary>
Motivation: Conventional multimodal models often suffer from modality imbalance, where dominant modalities overshadow weaker ones, leading to suboptimal performance.

Method: G²D employs a knowledge distillation framework with a fused loss function and dynamic sequential modality prioritization (SMP) to balance modality contributions.

Result: G²D enhances weak modality utilization and outperforms state-of-the-art methods in classification and regression tasks.

Conclusion: G²D effectively mitigates modality imbalance and improves multimodal learning performance.

Abstract: Multimodal learning aims to leverage information from diverse data modalities
to achieve more comprehensive performance. However, conventional multimodal
models often suffer from modality imbalance, where one or a few modalities
dominate model optimization, leading to suboptimal feature representation and
underutilization of weak modalities. To address this challenge, we introduce
Gradient-Guided Distillation (G$^{2}$D), a knowledge distillation framework
that optimizes the multimodal model with a custom-built loss function that
fuses both unimodal and multimodal objectives. G$^{2}$D further incorporates a
dynamic sequential modality prioritization (SMP) technique in the learning
process to ensure each modality leads the learning process, avoiding the
pitfall of stronger modalities overshadowing weaker ones. We validate G$^{2}$D
on multiple real-world datasets and show that G$^{2}$D amplifies the
significance of weak modalities while training and outperforms state-of-the-art
methods in classification and regression tasks. Our code is available at
https://github.com/rAIson-Lab/G2D.

</details>


### [498] [DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion](https://arxiv.org/pdf/2506.21544)
*Yansong Qu, Shaohui Dai, Xinyang Li, Yuze Wang, You Shen, Liujuan Cao, Rongrong Ji*

Main category: cs.CV

TL;DR: DeOcc-1-to-3 is an end-to-end framework for occlusion-aware multi-view generation from a single occluded image, enabling reliable 3D reconstruction without prior inpainting or manual annotations.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based view synthesis models fail under occlusions, degrading 3D reconstruction quality.

Method: A self-supervised training pipeline uses occluded-unoccluded image pairs and pseudo-ground-truth views to teach structure-aware completion and view consistency. The view synthesis model is fine-tuned for joint learning.

Result: The framework synthesizes six structurally consistent novel views from a single occluded image, improving 3D reconstruction reliability.

Conclusion: DeOcc-1-to-3 addresses occlusion challenges in 3D reconstruction and introduces a benchmark for future evaluation.

Abstract: Reconstructing 3D objects from a single image remains challenging, especially
under real-world occlusions. While recent diffusion-based view synthesis models
can generate consistent novel views from a single RGB image, they typically
assume fully visible inputs and fail when parts of the object are occluded,
resulting in degraded 3D reconstruction quality. We propose DeOcc-1-to-3, an
end-to-end framework for occlusion-aware multi-view generation that synthesizes
six structurally consistent novel views directly from a single occluded image,
enabling reliable 3D reconstruction without prior inpainting or manual
annotations. Our self-supervised training pipeline leverages
occluded-unoccluded image pairs and pseudo-ground-truth views to teach the
model structure-aware completion and view consistency. Without modifying the
original architecture, we fully fine-tune the view synthesis model to jointly
learn completion and multi-view generation. Additionally, we introduce the
first benchmark for occlusion-aware reconstruction, covering diverse occlusion
levels, object categories, and masking patterns, providing a standardized
protocol for future evaluation.

</details>


### [499] [ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts](https://arxiv.org/pdf/2506.21835)
*Xiaoqi Wang, Clint Sebastian, Wenbin He, Liu Ren*

Main category: cs.CV

TL;DR: ProSAM improves visual reference segmentation by predicting stable prompt distributions, outperforming SAM-based methods on key datasets.


<details>
  <summary>Details</summary>
Motivation: Existing SAM-based methods for visual reference segmentation generate unstable prompts at object boundaries due to suboptimal encoders, limiting robustness.

Method: ProSAM introduces a variational prompt encoder to predict multivariate prompt distributions, avoiding unstable regions.

Result: ProSAM consistently outperforms state-of-the-art methods on Pascal-5$^i$ and COCO-20$^i$ datasets.

Conclusion: ProSAM provides a more robust solution for visual reference segmentation by addressing prompt instability.

Abstract: The recent advancements in large foundation models have driven the success of
open-set image segmentation, a task focused on segmenting objects beyond
predefined categories. Among various prompt types (such as points, boxes,
texts, and visual references), visual reference segmentation stands out for its
unique flexibility and strong zero-shot capabilities. Recently, several
SAM-based methods have made notable progress in this task by automatically
generating prompts to guide SAM. However, these methods often generate prompts
at object boundaries due to suboptimal prompt encoder, which results in
instability and reduced robustness. In this work, we introduce ProSAM, a simple
but effective method to address the stability challenges we identified in
existing SAM-based visual reference segmentation approaches. By learning a
variational prompt encoder to predict multivariate prompt distributions, ProSAM
avoids generating prompts that lie in unstable regions, overcoming the
instability caused by less robust prompts. Our approach consistently surpasses
state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,
providing a more robust solution for visual reference segmentation.

</details>


### [500] [Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision](https://arxiv.org/pdf/2506.22022)
*Zhanyi Lu, Yue Zhou*

Main category: cs.CV

TL;DR: A facial stylization method integrating semantic preservation and pseudo-paired supervision to enhance content fidelity and stylization quality, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Addressing artifacts and insufficient fidelity in StyleGAN-based facial stylization by tackling semantic shift during stylization.

Method: Proposes semantic preservation constraint and pseudo-paired supervision, with multi-level pseudo-paired dataset creation for supervision.

Result: Achieves high-fidelity, aesthetically pleasing facial style transfer, surpassing prior methods.

Conclusion: The method improves stylization quality and enables flexible multimodal and reference-guided stylization without complex designs.

Abstract: Facial stylization aims to transform facial images into appealing,
high-quality stylized portraits, with the critical challenge of accurately
learning the target style while maintaining content consistency with the
original image. Although previous StyleGAN-based methods have made significant
advancements, the generated results still suffer from artifacts or insufficient
fidelity to the source image. We argue that these issues stem from neglecting
semantic shift of the generator during stylization. Therefore, we propose a
facial stylization method that integrates semantic preservation constraint and
pseudo-paired supervision to enhance the content correspondence and improve the
stylization effect. Additionally, we develop a methodology for creating
multi-level pseudo-paired datasets to implement supervisory constraint.
Furthermore, building upon our facial stylization framework, we achieve more
flexible multimodal and reference-guided stylization without complex network
architecture designs or additional training. Experimental results demonstrate
that our approach produces high-fidelity, aesthetically pleasing facial style
transfer that surpasses previous methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [501] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/pdf/2506.22604)
*David Porfirio, Vincent Hsiao, Morgan Fine-Morris, Leslie Smith, Laura M. Hiatt*

Main category: cs.AI

TL;DR: The paper explores combining natural language and drag-and-drop interfaces for robot task specification, using LLMs to generate human-like action sequences and comparing them to hand-specified sequences.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between intuitive natural language programming and precise drag-and-drop interfaces for robot task specification.

Method: Developed an LLM-based pipeline to convert natural language into human-like action sequences and compared these to manually specified sequences.

Result: Larger LLMs outperformed smaller ones in generating human-like sequences, but smaller models still performed satisfactorily.

Conclusion: Combining natural language and drag-and-drop approaches is feasible, with LLMs effectively bridging the gap, though model size impacts performance.

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [502] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/pdf/2506.22609)
*Graham Todd, Alexander G. Padula, Dennis J. N. J. Soemers, Julian Togelius*

Main category: cs.AI

TL;DR: Ludax is a domain-specific language for board games that compiles into hardware-accelerated code, combining game description generality with modern parallel processing speed to accelerate research in RL and cognitive science.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between game description languages and hardware acceleration, enabling faster and more generalized research in AI, particularly reinforcement learning.

Method: Developed Ludax, a domain-specific language for board games that automatically compiles into hardware-accelerated code, integrating with deep learning pipelines.

Result: Ludax provides rapid simulation, flexibility, and speed benchmarks, demonstrated through RL agent training.

Conclusion: Ludax is an open-source tool designed to accelerate games research by leveraging hardware acceleration and generality.

Abstract: Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [503] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/pdf/2506.22653)
*Michael Grosskopf, Russell Bent, Rahul Somasundaram, Isaac Michaud, Arthur Lui, Nathan Debardeleben, Earl Lawrence*

Main category: cs.AI

TL;DR: URSA is a scientific agent ecosystem using LLMs to accelerate research tasks, featuring modular agents and tools for varied scientific problems.


<details>
  <summary>Details</summary>
Motivation: Leverage LLMs' advanced reasoning and planning skills to revolutionize scientific research by addressing bottlenecks.

Method: Develop URSA, a modular ecosystem of agents and tools, including physics simulation codes, to tackle diverse scientific challenges.

Result: URSA's architecture and examples demonstrate its potential to enhance research efficiency and impact.

Conclusion: URSA showcases the transformative potential of agentic AI in accelerating scientific progress.

Abstract: Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [504] [Explanations are a means to an end](https://arxiv.org/pdf/2506.22740)
*Jessica Hullman, Ziyang Guo, Berk Ustun*

Main category: cs.AI

TL;DR: The paper advocates for designing and evaluating machine learning explanations with specific practical goals in mind, using a statistical decision theory framework.


<details>
  <summary>Details</summary>
Motivation: Current explainable ML methods lack consideration of real-world application contexts, leading to potential misuse or ambiguity.

Method: The authors propose a functionally-grounded framework based on statistical decision theory to formalize explanation goals and evaluate their practical utility.

Result: The framework is demonstrated across diverse use cases (e.g., clinical decision support, debugging) and defines the maximum performance boost explanations can provide.

Conclusion: Evaluation should combine theoretical and empirical perspectives, with clear use-case specifications to prevent misuse and ambiguity.

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [505] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/pdf/2506.22774)
*Michael Papademas, Xenia Ziouvelou, Antonis Troumpoukis, Vangelis Karkaletsis*

Main category: cs.AI

TL;DR: The paper proposes a method to assess AI trustworthiness by combining ethical guidelines with algorithmic processes like PageRank and TrustRank, aiming for a balanced quantitative and holistic evaluation.


<details>
  <summary>Details</summary>
Motivation: AI's pervasive societal impact and complexity necessitate reliable trustworthiness assessments, but current methods lack a balance between holistic ethical views and quantitative techniques.

Method: The paper integrates ethical components of Trustworthy AI with algorithmic processes (PageRank and TrustRank) to create a quantitative yet holistic assessment framework.

Result: The proposed method successfully combines quantitative insights with theoretical guidelines, minimizing subjectivity in trustworthiness evaluations.

Conclusion: The framework offers a balanced approach to assessing AI trustworthiness, addressing gaps in current methods by merging ethical and algorithmic perspectives.

Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [506] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/pdf/2506.22865)
*Ziqi Zhong, Xunzhu Tang*

Main category: cs.AI

TL;DR: ReasonBridge transfers reasoning capabilities from closed-source to open-source LLMs using hierarchical knowledge distillation, improving performance by up to 23%.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between closed-source and open-source LLMs in complex reasoning tasks.

Method: Hierarchical knowledge distillation with a tailored dataset (Reason1K), sparse adapter architecture, and test-time compute scaling.

Result: Open-source models improve by up to 23%, with Qwen2.5-14B outperforming Claude-Sonnet3.5 on MATH500 and matching it on AIME.

Conclusion: ReasonBridge offers a sample-efficient approach to enhance reasoning in open-source models, narrowing the gap with closed-source counterparts.

Abstract: Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [507] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/pdf/2506.22893)
*Arpit Narechania, Alex Endert, Atanu R Sinha*

Main category: cs.AI

TL;DR: The paper discusses the potential of AI in enterprises, focusing on AI-driven agents to enhance decision-making productivity. It proposes six tenets for successful AI agents in enterprises, advocating a shift from AI-centric to user-centric AI design.


<details>
  <summary>Details</summary>
Motivation: AI has significant potential to impact various aspects of human life, including enterprises where decision-making is critical. The paper aims to address gaps in current AI-centric approaches for enterprise decision-making.

Method: The paper highlights six tenets for AI agent success in enterprises, emphasizing user-centric AI design and market mechanisms for platforms.

Result: The proposed tenets and shift to user-centric AI aim to improve enterprise decision-making productivity by aligning AI design with user needs.

Conclusion: The paper concludes by advocating for user-centric AI and market-driven platforms to better serve enterprise decision-making needs.

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [508] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/pdf/2506.22919)
*Sanskar Pandey, Ruhaan Chopra, Saad Murtaza Bhat, Ark Abhyudaya*

Main category: cs.AI

TL;DR: Hecto, a lightweight MoE model, combines GRU and FFNN experts for diverse reasoning tasks, matching homogeneous baselines while improving specialization and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current MoE models lack representational diversity due to identical inductive biases, limiting efficiency and specialization for varied reasoning tasks.

Method: Hecto integrates a GRU expert for temporal reasoning and an FFNN expert for static abstraction under a sparse Top-1 gating mechanism.

Result: Hecto matches or nears homogeneous baselines in performance, achieves clear expert specialization, and improves with larger batch sizes.

Conclusion: Hecto sets a new benchmark for conditional computation, offering specialized reasoning in low-resource settings through architectural diversity.

Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [509] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/pdf/2506.22920)
*Pinzheng Wang, Juntao Li, Zecheng Tang, Haijia Gui, Min zhang*

Main category: cs.AI

TL;DR: The paper explores using self-play (Critic-Discernment Game) to enhance LLMs' reasoning comprehension without supervision, showing significant improvement in tasks like math and error detection.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' strong reasoning abilities, they lack true comprehension of their processes. This work aims to improve rationality in reasoning without human or superior model supervision.

Method: A Critic-Discernment Game (CDG) is designed where a prover solves problems and faces critiques (helpful or misleading). The prover must correct errors from feedback and resist misleading comments.

Result: Experiments on math reasoning, error detection, self-correction, and long-chain reasoning show CDG training significantly improves LLMs' reasoning comprehension.

Conclusion: Self-play via CDG effectively enhances LLMs' ability to understand their reasoning processes, demonstrating promise for unsupervised rationality improvement.

Abstract: Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [510] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/pdf/2506.22992)
*Yulun Jiang, Yekun Chai, Maria Brbić, Michael Moor*

Main category: cs.AI

TL;DR: MARBLE is a multimodal reasoning benchmark designed to test MLLMs' ability to handle complex, step-by-step reasoning across modalities, revealing their current limitations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack focus on complex multimodal reasoning, limiting understanding of MLLMs' capabilities in such domains.

Method: MARBLE includes two tasks, M-Portal and M-Cube, requiring multistep planning under spatial, visual, and physical constraints.

Result: Current MLLMs perform poorly, with near-random accuracy on M-Portal and 0% on M-Cube, highlighting reasoning and perception bottlenecks.

Conclusion: MARBLE aims to drive development of next-gen MLLMs capable of advanced multimodal reasoning.

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [511] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/pdf/2506.23049)
*Leander Melroy Maben, Gayathri Ganesh Lakshmy, Srijith Radhakrishnan, Siddhant Arora, Shinji Watanabe*

Main category: cs.AI

TL;DR: AURA is the first open-source, speech-native assistant for multi-turn dialogue with tool use, combining ASR, TTS, and LLMs. It excels in benchmarks and human evaluations.


<details>
  <summary>Details</summary>
Motivation: No existing open-source system supports full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning.

Method: AURA uses a cascaded pipeline of open-weight ASR, TTS, and LLMs, with modular tool integration via natural language prompts.

Result: AURA scores 92.75% on OpenBookQA, 4.39 on AlpacaEval, and achieves 90% task success in human evaluations.

Conclusion: AURA is a competitive, open-source solution for complex, goal-driven speech tasks.

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [512] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/pdf/2506.23080)
*Xinmin Fang, Lingfeng Tao, Zhengxiong Li*

Main category: cs.AI

TL;DR: The paper proposes a five-stage evolutionary framework for AI development, comparing it to human cognitive advancements. It identifies current and future stages, emphasizing reflexive feedback loops and actionable strategies for next-gen AI.


<details>
  <summary>Details</summary>
Motivation: To systematically model AI's evolution, linking its development to historical cognitive technologies and providing a prescriptive path for future advancements.

Method: Introduces the "Geometry of Cognition" framework, analyzing AI's progression through distinct epochs, each marked by shifts in representation and reasoning.

Result: Identifies the current "Metalinguistic Moment" and predicts future stages ("Mathematical Symbolism Moment," "Formal Logic System Moment") leading to provably aligned AI.

Conclusion: The framework offers a theoretical foundation and practical strategies for advancing AI, completing a trilogy on AI's economic, cognitive, and methodological aspects.

Abstract: This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [513] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/pdf/2506.23107)
*Bing Song, Jianing Liu, Sisi Jian, Chenyang Wu, Vinayak Dixit*

Main category: cs.AI

TL;DR: LLMs like ChatGPT 4o and o1-mini were tested for simulating risky decision-making. They showed more risk-averse behavior than humans, with o1-mini closer to human responses. Performance varied by language, with Chinese prompts less accurate than English.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to simulate complex human decision-making, specifically risky choices, given their growing use in critical applications.

Method: Compared LLM-generated decisions (ChatGPT 4o and o1-mini) with human responses in lottery-based tasks using transportation survey data from multiple cities. Analyzed risk preferences via CRRA framework.

Result: LLMs were more risk-averse than humans; o1-mini aligned better with human decisions. Chinese prompts led to less accurate predictions than English.

Conclusion: LLMs show potential but have limitations in replicating human risk behavior, especially across languages and cultures.

Abstract: Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [514] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/pdf/2506.23123)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: The dissertation explores the coevolution of technology and society in the AI era, focusing on foundation models' capabilities, risks, and societal impacts, aiming to improve AI governance.


<details>
  <summary>Details</summary>
Motivation: To address the confusion and potential harms caused by poorly understood foundation models in AI, and to bridge the gap between technology and societal outcomes.

Method: Organized around three themes: conceptual framing (capabilities, risks, supply chain), empirical insights (transparency via evaluations and indexes), and actionable policy insights.

Result: Advances understanding of foundation models' societal impact and provides tools for evidence-based AI policy.

Conclusion: The dissertation contributes to better AI governance by building scientific foundations and a research-policy interface.

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [515] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/pdf/2506.23689)
*Zihao Liu, Xinhang Sui, Yueran Song, Siwen Wang*

Main category: cs.AI

TL;DR: PokéAI is a multi-agent LLM framework for playing Pokémon Red, featuring Planning, Execution, and Critique agents. It achieves an 80.8% win rate in battles and shows a link between linguistic ability and strategic reasoning.


<details>
  <summary>Details</summary>
Motivation: To create an autonomous system for playing Pokémon Red using specialized LLM agents, demonstrating the potential of multi-agent frameworks in gaming.

Method: Three agents (Planning, Execution, Critique) work in a closed-loop system. The Execution Agent includes a battle module tested in wild encounters.

Result: 80.8% average win rate in battles, close to human performance. Battle performance correlates with LLM Arena scores, and unique playstyles emerge.

Conclusion: PokéAI successfully demonstrates autonomous gameplay with strategic reasoning, linking linguistic ability to performance, and showcasing unique agent behaviors.

Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [516] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/pdf/2506.23128)
*Chi Chiu So, Yueyue Sun, Jun-Min Wang, Siu Pang Yung, Anthony Wai Keung Loh, Chun Pong Chau*

Main category: cs.AI

TL;DR: The paper evaluates the relational reasoning capabilities of three LLMs (DeepSeek-R1, DeepSeek-V3, GPT-4o) using benchmark tasks. DeepSeek-R1 performs best but struggles with complexity due to token limits and output issues. Future work includes multimodal reasoning and failure analysis.


<details>
  <summary>Details</summary>
Motivation: To assess and compare the deep relational reasoning abilities of state-of-the-art LLMs, identifying strengths and limitations in logical inference tasks.

Method: Benchmark tasks in family tree and general graph reasoning were designed to evaluate the models. Performance was measured using F1-scores, and responses were analyzed for reasoning strategies and failures.

Result: DeepSeek-R1 achieved the highest F1-scores but all models faltered with increased complexity. Analysis revealed incoherent reasoning and token length constraints as key challenges.

Conclusion: The study highlights the need for deeper scrutiny of LLMs' reasoning dynamics and suggests future directions like multimodal reasoning and systematic failure analysis to enhance their capabilities.

Abstract: How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [517] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/pdf/2506.23793)
*Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, Alexey Skrynnik*

Main category: cs.AI

TL;DR: MAPF-GPT-DDG, a decentralized suboptimal MAPF solver, improves upon MAPF-GPT by fine-tuning with centralized expert data and a delta-data generation mechanism, achieving superior scalability and performance.


<details>
  <summary>Details</summary>
Motivation: To address the NP-hard nature of MAPF and enhance scalability for real-world applications like logistics and search-and-rescue.

Method: Fine-tunes pre-trained MAPF-GPT using centralized expert data and introduces a delta-data generation mechanism to accelerate training and improve performance.

Result: Outperforms existing learning-based MAPF solvers, including MAPF-GPT, in solution quality and scalability, handling up to 1 million agents.

Conclusion: MAPF-GPT-DDG sets a new benchmark for scalability and performance in decentralized MAPF solvers.

Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [518] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/pdf/2506.23141)
*Siyuan Li, Ruitong Liu, Yan Wen, Te Sun*

Main category: cs.AI

TL;DR: Proposes a semantic-aware relational message passing method for Knowledge Graph Completion (KGC) to address noise and information dilution in traditional node-based approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional node-based message passing in KGC introduces noise and suffers from information dilution by aggregating all neighboring edges indiscriminately.

Method: Introduces a semantic-aware Top-K neighbor selection strategy and a multi-head attention aggregator to focus on relevant contextual information.

Result: Achieves superior performance on established benchmarks by mitigating interference from irrelevant information.

Conclusion: The method effectively leverages semantic context and improves KGC by accurately capturing and propagating relevant information.

Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [519] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/pdf/2506.23168)
*Mohammad Abdulla, Tobias Hille, Dominik Dürrschnabel, Gerd Stumme*

Main category: cs.AI

TL;DR: Introduces 'rises' in concept lattices to measure distributivity, linking it to join- and meet-distributivity, and analyzes real-world data.


<details>
  <summary>Details</summary>
Motivation: Lack of a standardized measure for distributivity in Formal Concept Analysis (FCA) lattices, despite their frequent distributivity.

Method: Proposes 'rises' to quantify distributivity, examining their relationship with classical distributivity notions and analyzing real-world concept lattices.

Result: Lattices are distributive if no non-unit rises occur; real-world data shows high join-distributivity but low meet-distributivity.

Conclusion: Rises effectively measure distributivity, revealing distinct patterns in join- and meet-distributivity in real-world FCA lattices.

Abstract: Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [520] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/pdf/2506.23273)
*Quang Hung Nguyen, Phuong Anh Trinh, Phan Quoc Hung Mai, Tuan Phong Trinh*

Main category: cs.AI

TL;DR: FinStat2SQL is a lightweight text2sql pipeline for financial statements, combining large and small language models to handle domain-specific queries efficiently. It achieves 61.33% accuracy with fast response times.


<details>
  <summary>Details</summary>
Motivation: Text2sql faces challenges with complex, domain-specific queries, especially in finance due to varying database designs and reporting standards.

Method: Uses a multi-agent setup with large and small language models for entity extraction, SQL generation, and self-correction. Tailored to local standards like VAS.

Result: A fine-tuned 7B model achieves 61.33% accuracy with sub-4-second response times, outperforming GPT-4o-mini.

Conclusion: FinStat2SQL provides a scalable, cost-efficient solution for financial analysis, making AI-powered querying accessible to Vietnamese enterprises.

Abstract: Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [521] [Evaluating Agents using Social Choice Theory](https://arxiv.org/pdf/2312.03121)
*Marc Lanctot, Kate Larson, Yoram Bachrach, Luke Marris, Zun Li, Avishkar Bhoopchand, Thomas Anthony, Brian Tanner, Anna Koop*

Main category: cs.AI

TL;DR: The paper proposes Voting-as-Evaluation (VasE), a framework using voting theory to aggregate ordinal rankings or pairwise comparisons for robust and interpretable evaluations across tasks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in cross-task evaluation by leveraging social choice theory for principled, flexible, and interpretable aggregation methods.

Method: VasE interprets tasks as voters, using social welfare functions to aggregate rankings or comparisons. It applies maximal lotteries for consistency and efficiency.

Result: VasE outperforms Elo and Nash averaging in robustness, reveals hidden evaluation data properties, and predicts outcomes better in complex games.

Conclusion: VasE provides a theoretically grounded, practical solution for evaluation problems, with maximal lotteries offering consistency and computational efficiency.

Abstract: We argue that many general evaluation problems can be viewed through the lens
of voting theory. Each task is interpreted as a separate voter, which requires
only ordinal rankings or pairwise comparisons of agents to produce an overall
evaluation. By viewing the aggregator as a social welfare function, we are able
to leverage centuries of research in social choice theory to derive principled
evaluation frameworks with axiomatic foundations. These evaluations are
interpretable and flexible, while avoiding many of the problems currently
facing cross-task evaluation. We apply this Voting-as-Evaluation (VasE)
framework across multiple settings, including reinforcement learning, large
language models, and humans. In practice, we observe that VasE can be more
robust than popular evaluation frameworks (Elo and Nash averaging), discovers
properties in the evaluation data not evident from scores alone, and can
predict outcomes better than Elo in a complex seven-player game. We identify
one particular approach, maximal lotteries, that satisfies important
consistency properties relevant to evaluation, is computationally efficient
(polynomial in the size of the evaluation data), and identifies game-theoretic
cycles.

</details>


### [522] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/pdf/2506.23276)
*David Guzman Piedrahita, Yongjin Yang, Mrinmaya Sachan, Giorgia Ramponi, Bernhard Schölkopf, Zhijing Jin*

Main category: cs.AI

TL;DR: The paper explores how LLMs balance self-interest and collective well-being in multi-agent systems, revealing distinct cooperation patterns and challenges, especially for reasoning-focused models.


<details>
  <summary>Details</summary>
Motivation: Understanding LLM cooperation is crucial for safe and robust deployment, particularly in scenarios requiring costly sanctioning to incentivize collaboration.

Method: Adapts a public goods game with institutional choice from behavioral economics to study LLM behavior in repeated social dilemmas.

Result: Identifies four behavioral patterns in LLMs, with reasoning-focused models struggling in cooperation while traditional models perform better.

Conclusion: Enhancing reasoning capabilities in LLMs does not guarantee cooperation, offering insights for deploying collaborative LLM agents.

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [523] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/pdf/2506.23306)
*Qi Liu, Can Li, Wanjing Ma*

Main category: cs.AI

TL;DR: GATSim introduces generative AI agents for urban mobility simulation, capturing human-like adaptability and diversity in travel decisions.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based systems lack the complexity and adaptability of human decision-making in urban mobility.

Method: GATSim combines an urban mobility foundation model with agent cognitive systems, featuring memory, learning, and tool usage.

Result: Generative agents produce believable travel behaviors and competitive performance with human annotators.

Conclusion: GATSim offers a realistic, adaptive framework for urban mobility simulation, validated through experiments.

Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [524] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/pdf/2506.23464)
*Sahil Tripathi, Md Tabrez Nafis, Imran Hussain, Jiechao Gao*

Main category: cs.AI

TL;DR: HonestVQA introduces a self-supervised honesty calibration framework for DocVQA, improving accuracy and reducing overconfidence while aligning model confidence with ethical communication.


<details>
  <summary>Details</summary>
Motivation: Current DocVQA systems lack ethical responsiveness, producing overconfident answers or failing to communicate uncertainty, posing risks in ethically accountable domains.

Method: HonestVQA uses uncertainty quantification, weighted loss functions for confidence alignment, and contrastive learning for ethical response behavior. It introduces H-Score and ECI metrics for evaluation.

Result: HonestVQA improves accuracy by up to 4.3% and F1 by 4.3%, reduces overconfidence (lower H-Score and ECI), and achieves strong generalization (78.9% accuracy, 76.1% F1-score).

Conclusion: HonestVQA effectively addresses ethical misalignment in DocVQA, enhancing performance and trustworthiness while generalizing well across domains.

Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [525] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/pdf/2506.21490)
*Tin Dizdarević, Ravi Hammond, Tobias Gessler, Anisoara Calinescu, Jonathan Cook, Matteo Gallici, Andrei Lupu, Darius Muglich, Johannes Forkel, Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: The paper introduces the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to address human-AI coordination in Hanabi, using human proxy agents for evaluation.


<details>
  <summary>Details</summary>
Motivation: Seamless human-AI coordination is critical but challenging, especially in games like Hanabi with imperfect information and theory of mind requirements.

Method: Developed human proxy agents from a large-scale dataset, limited data to encourage efficiency, and provided baseline results for Hanabi.

Result: Introduced AH2AC2 with proxy agents for reproducible evaluation, open-sourced a dataset, and shared baseline results.

Conclusion: AH2AC2 offers a scalable solution for human-AI coordination research, with controlled evaluation to ensure fairness.

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [526] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/pdf/2506.23503)
*Bosubabu Sambana, Kondreddygari Archana, Suram Indhra Sena Reddy, Shaik Meethaigar Jameer Basha, Shaik Karishma*

Main category: cs.AI

TL;DR: A CBT-based framework uses NLP models (BERT, RoBERTa, T5, PEGASUS, mT5) to analyze social media content for negative emotions and cognitive distortions, aiding psychotherapists in early intervention.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in analyzing cognitive pathways in online content for timely mental health interventions.

Method: Leverages NLP models for sentiment analysis, text summarization, and translation to detect negative emotions and cognitive distortions in social media data.

Result: The system predicts negative side effects and other mental health disorders, enhancing early detection and treatment.

Conclusion: The proposed framework offers a comprehensive tool for psychotherapists to address psychological issues more effectively.

Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [527] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/pdf/2506.23504)
*Bosubabu Sambana, Kotamsetty Geethika Devi, Bandi Rajeswara Reddy, Galeti Mohammad Hussain, Gownivalla Siddartha*

Main category: cs.AI

TL;DR: A hybrid model combining AlexNet and LSTM improves electricity price forecasting accuracy by addressing limitations of traditional methods and standalone models like RNN and ANN.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and standalone models (RNN, ANN) fail to accurately forecast electricity prices due to insufficient analysis of external variables and sequential data.

Method: The hybrid model integrates AlexNet for feature extraction and LSTM for learning sequential patterns, using data like demand, temperature, sunlight, and rain, with preprocessing techniques like min-max scaling and time windows.

Result: The hybrid model achieves 97.08% accuracy, outperforming RNN (96.64%) and ANN (96.63%).

Conclusion: The hybrid approach significantly enhances prediction accuracy by leveraging external variables and sequential learning, making it superior to traditional and standalone models.

Abstract: The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [528] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/pdf/2506.23517)
*Selin Dik, Osman Erdem, Mehmet Dik*

Main category: cs.AI

TL;DR: GPTZero is effective at detecting AI-generated text but less reliable for human-written essays, with some false positives. Educators should use caution.


<details>
  <summary>Details</summary>
Motivation: To evaluate the reliability of GPTZero in detecting AI-generated text across different essay lengths.

Method: Tested GPTZero on 28 AI-generated and 50 human-written essays of varying lengths (short, medium, long), measuring AI detection rates and confidence.

Result: AI-generated essays were accurately detected (91-100%), but human-written essays had fluctuating results with false positives.

Conclusion: GPTZero is reliable for AI detection but less so for human texts, urging caution for educators.

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [529] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/pdf/2506.23520)
*Yu Zhang, Ruijie Yu, Jidong Tian, Feng Zhu, Jiapeng Liu, Xiaokang Yang, Yaohui Jin, Yanyan Xu*

Main category: cs.AI

TL;DR: ChemActor, a fine-tuned LLM, converts chemical procedures into structured actions, using LLM-generated data and a novel review metric to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Automating chemical procedure extraction is challenging due to ambiguous language and costly human annotation.

Method: ChemActor uses a sequential LLM-generated data framework with a data selection module and multi-round LLMs circle review metric.

Result: Outperforms baseline by 10% in R2D and D2A tasks.

Conclusion: ChemActor demonstrates advanced understanding and effectiveness in automating chemical procedure extraction.

Abstract: With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [530] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/pdf/2506.23549)
*Huai-Chih Wang, Hsiang-Chun Chuang, Hsi-Chun Cheng, Dai-Jie Wu, Shao-Hua Sun*

Main category: cs.AI

TL;DR: Proposes Coordination Transformers (CooT), a framework for rapid adaptation to unseen partners in multi-agent systems, outperforming baselines in coordination tasks.


<details>
  <summary>Details</summary>
Motivation: Addresses poor generalization and extensive training requirements in existing multi-agent coordination methods.

Method: Uses in-context coordination with interaction histories to predict partner actions, trained on diverse agent pairs.

Result: Outperforms baselines on Overcooked benchmark and excels in human evaluations.

Conclusion: CooT is robust, flexible, and context-sensitive, offering effective coordination with unseen partners.

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [531] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/pdf/2506.23563)
*Huanjin Yao, Jiaxing Huang, Yawen Qiu, Michael K. Chen, Wenzheng Liu, Wei Zhang, Wenjie Zeng, Xikun Zhang, Jingyi Zhang, Yuxin Song, Wenhao Wu, Dacheng Tao*

Main category: cs.AI

TL;DR: MMReason is a new benchmark for evaluating long-chain reasoning in Multimodal Large Language Models (MLLMs), addressing gaps in existing benchmarks by offering diverse, challenging questions and robust evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM benchmarks lack precision and comprehensiveness in evaluating long-chain reasoning, particularly in difficulty, diversity, and intermediate step assessment.

Method: MMReason curates multi-step reasoning questions from various fields and difficulty levels, reformulates them to avoid shortcuts, and uses annotated solutions and ternary scoring for reliable evaluation.

Result: The benchmark evaluates popular MLLMs, providing insights into their reasoning capabilities.

Conclusion: MMReason aims to advance MLLM reasoning research by offering a precise and comprehensive evaluation tool.

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [532] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/pdf/2506.23576)
*Maria Carolina Cornelia Wit, Jun Pang*

Main category: cs.AI

TL;DR: Multi-agent LLM systems improve resistance to jailbreaking attacks but introduce trade-offs like higher false positives and computational costs.


<details>
  <summary>Details</summary>
Motivation: Address concerns about jailbreaking attacks bypassing safety mechanisms in LLMs.

Method: Evaluated three jailbreaking strategies (AutoDefense, BetterDan, JB) using single-, two-, and three-agent LLM configurations.

Result: Multi-agent systems reduce false negatives but vary in effectiveness by attack type and increase false positives and computational overhead.

Conclusion: Highlights limitations of current defences and suggests improving alignment robustness in future LLM systems.

Abstract: Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [533] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/pdf/2506.23626)
*António Afonso, Iolanda Leite, Alessandro Sestini, Florian Fuchs, Konrad Tollmar, Linus Gisslén*

Main category: cs.AI

TL;DR: An automated method using a Language Model (LM) fine-tunes RL agent reward weights iteratively, improving performance without manual engineering.


<details>
  <summary>Details</summary>
Motivation: Deploying RL agents in games faces challenges in reward function design and adaptability to game changes.

Method: LM proposes updated reward weights based on user-defined behavioral goals and performance stats, enabling self-correction.

Result: LM-guided tuning improved agent success from 9% to 74% in one iteration, reaching 80% success (vs. expert's 94%).

Conclusion: Automated LM-guided reward tuning is effective, reducing reliance on manual expert intervention.

Abstract: Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [534] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/pdf/2506.23673)
*Jingsong Liu, Han Li, Chen Yang, Michael Deutges, Ario Sadafi, Xin You, Katharina Breininger, Nassir Navab, Peter J. Schüffler*

Main category: cs.AI

TL;DR: HASD proposes a hierarchical framework for slide-level domain adaptation in pathology AI, improving performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Addressing slide-level domain shift in pathology AI, as current methods focus on patches and miss global WSI features.

Method: HASD uses hierarchical adaptation (domain alignment, geometric invariance, patch attention) and prototype selection for efficiency.

Result: Achieves 4.1% AUROC improvement in HER2 grading and 3.9% C-index gain in survival prediction.

Conclusion: HASD offers a practical, efficient solution for slide-level domain adaptation in pathology.

Abstract: Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [535] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/pdf/2506.23692)
*Boyuan Zheng, Zerui Fang, Zhe Xu, Rui Wang, Yiwen Chen, Cunshi Wang, Mengwei Qu, Lei Lei, Zhen Feng, Yan Liu, Yuyang Li, Mingzhou Tan, Jiaji Wu, Jianwei Shuai, Jia Li, Fangfu Ye*

Main category: cs.AI

TL;DR: The paper proposes 'Agent for Science' (Agent4S) as the Fifth Scientific Paradigm, using LLM-driven agents to automate research workflows, classified into five levels for progression toward fully autonomous AI Scientists.


<details>
  <summary>Details</summary>
Motivation: Current AI for Science (AI4S) is inefficient; Agent4S aims to revolutionize scientific discovery by automating entire research workflows.

Method: Introduces a five-level classification for Agent4S, detailing a roadmap from task automation to autonomous, collaborative AI Scientists.

Result: A framework for Agent4S is presented, defining the next step in scientific discovery.

Conclusion: Agent4S represents a transformative shift in scientific research, enabling fully autonomous and collaborative AI-driven discovery.

Abstract: While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [536] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/pdf/2506.23703)
*Lars Ullrich, Walter Zimmer, Ross Greer, Knut Graichen, Alois C. Knoll, Mohan Trivedi*

Main category: cs.AI

TL;DR: The paper proposes a new interdisciplinary approach, 'data control,' to enhance AI safety by combining AI advancements with control theory and system analysis.


<details>
  <summary>Details</summary>
Motivation: AI's rapid advancement lacks sufficient safety assurance, especially in safety-critical systems, necessitating a new perspective to integrate AI and control theory for improved safety.

Method: The article introduces 'data control,' a system theory-inspired approach that leverages interdisciplinary insights from AI and control theory to analyze and assure safety in AI systems.

Result: A generic foundation for safety analysis and assurance is outlined, adaptable for specific AI systems and future innovations.

Conclusion: The 'data control' perspective aims to bridge AI engineering and safety assurance, fostering interdisciplinary collaboration to enhance AI safety.

Abstract: While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [537] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/pdf/2506.23706)
*Christoph Schnabl, Daniel Hugenroth, Bill Marino, Alastair R. Beresford*

Main category: cs.AI

TL;DR: Attestable Audits use Trusted Execution Environments to verify AI model compliance while protecting sensitive data, addressing gaps in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack verifiable results and confidentiality for model IP and datasets, raising challenges in AI governance.

Method: Proposes Attestable Audits running in Trusted Execution Environments to verify interactions with compliant AI models.

Result: A prototype demonstrates feasibility on audit benchmarks against Llama-3.1, protecting data even with untrusted parties.

Conclusion: Attestable Audits offer a solution for verifiable, confidential AI model compliance checks.

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [538] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/pdf/2506.23773)
*Stefano M. Nicoletti, Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: BayesL is a new logical framework for querying and verifying Bayesian networks, enabling versatile reasoning and what-if scenarios without manual model changes.


<details>
  <summary>Details</summary>
Motivation: To simplify and enhance the process of querying and verifying Bayesian networks by providing a structured language.

Method: Introduces BayesL, a logical framework for specifying, querying, and verifying Bayesian networks.

Result: BayesL allows versatile reasoning and what-if evaluations without manual model adjustments.

Conclusion: BayesL offers a powerful tool for analyzing Bayesian networks efficiently.

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [539] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/pdf/2506.23784)
*Parosh Aziz Abdulla, Mohamed Faouzi Atig, Julie Cailler, Chencheng Liang, Philipp Rümmer*

Main category: cs.AI

TL;DR: The paper explores using Graph Neural Networks (GNNs) to rank word equations for solving conjunctions more efficiently, introducing a novel graph-based representation and three ranking approaches.


<details>
  <summary>Details</summary>
Motivation: The performance of solving word equations depends on the processing order, and the paper aims to improve this using GNNs.

Method: A graph-based representation for word equations is proposed, and three approaches for ranking equations are introduced. Training uses minimum unsatisfiable subsets (MUSes).

Result: The framework outperforms state-of-the-art solvers in benchmarks with variables appearing at most once per equation.

Conclusion: GNN-based ranking enhances the efficiency of solving word equations, particularly in specific benchmark cases.

Abstract: Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [540] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/pdf/2506.23844)
*Hang Su, Jun Luo, Chang Liu, Xiao Yang, Yichi Zhang, Yinpeng Dong, Jun Zhu*

Main category: cs.AI

TL;DR: The paper discusses the security risks of autonomous AI agents powered by large language models (LLMs) and proposes a framework (R2A2) to mitigate these risks.


<details>
  <summary>Details</summary>
Motivation: The rise of autonomous AI agents introduces novel security risks beyond conventional systems, necessitating a systematic approach to address vulnerabilities.

Method: The survey examines agent autonomy foundations, identifies security vulnerabilities, and reviews defense strategies. It introduces the R2A2 framework for risk-aware decision-making.

Result: Key vulnerabilities include memory poisoning, tool misuse, and deceptive behaviors. R2A2 offers a unified approach to proactive safety.

Conclusion: The R2A2 framework provides a principled solution to mitigate security risks in autonomous AI agents, ensuring safer deployment.

Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [541] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/pdf/2506.23908)
*András György, Tor Lattimore, Nevena Lazić, Csaba Szepesvári*

Main category: cs.AI

TL;DR: The paper highlights the limitations of current AI systems in deductive reasoning and proposes a shift from statistical learning to exact learning for reliable reasoning.


<details>
  <summary>Details</summary>
Motivation: Despite AI advancements, current systems fail at deductive reasoning, hindering progress toward artificial general intelligence.

Method: Proposes a shift from statistical learning to exact learning, demanding correctness on all inputs.

Result: Current AI systems are unsound in deductive reasoning due to statistical learning.

Conclusion: Exact learning is essential and achievable, and should guide future AI algorithm design.

Abstract: Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [542] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/pdf/2506.23924)
*Akshit Kumar, Tianyi Peng, Yuhang Wu, Assaf Zeevi*

Main category: cs.AI

TL;DR: LLMs show potential in solving stochastic OR problems, matching human experts in some cases, but further work is needed for reliable automation.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' capabilities in solving stochastic Operations Research problems, which are underexplored despite their real-world relevance.

Method: Manually curated graduate-level and doctoral exam problems were tested on LLMs, alongside using SimOpt for real-world decision-making under uncertainty.

Result: LLMs perform on par with human experts in classroom and practical settings, though more work is needed for reliable automation.

Conclusion: LLMs hold promise for assisting OR researchers and automating OR tasks, amplifying their real-world impact.

Abstract: Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [543] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/pdf/2506.23926)
*Junping Wang, Bicheng Wang, Yibo Xuea, Yuan Xie*

Main category: cs.AI

TL;DR: The paper proposes 'Industrial Brain,' a framework combining neuro networks and symbolic reasoning to predict and plan resilience in industrial chains, outperforming existing methods by up to 11.03%.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with resilience prediction in chaotic, real-world industrial chain scenarios, necessitating a more robust solution.

Method: The 'Industrial Brain' integrates higher-order neuro networks and CT-OODA symbolic reasoning to model node dynamics and network co-evolution without simplifying assumptions.

Result: Industrial Brain improves resilience prediction accuracy by up to 10.8% over GoT/OlaGPT and 11.03% over spectral dimension reduction, generalizing well to unseen data.

Conclusion: Industrial Brain effectively addresses resilience prediction gaps in industrial chains, offering robust performance and generalization.

Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [544] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/pdf/2506.23949)
*Anthony M. Barrett, Jessica Newman, Brandie Nonnecke, Nada Madkour, Dan Hendrycks, Evan R. Murphy, Krystal Jackson, Deepika Raman*

Main category: cs.AI

TL;DR: The paper outlines risk-management practices for general-purpose AI (GPAI) and foundation models, targeting developers to mitigate risks and align with standards like NIST and ISO/IEC.


<details>
  <summary>Details</summary>
Motivation: Address the dual nature of GPAI/foundation models, which offer benefits but also pose significant risks, necessitating structured risk management.

Method: Proposes risk-management controls and practices, adapting frameworks like NIST AI Risk Management and ISO/IEC 23894, tailored for GPAI/foundation models.

Result: Provides actionable guidance for developers to identify, analyze, and mitigate risks associated with GPAI/foundation models.

Conclusion: The document serves as a practical resource for developers to manage risks and align with leading AI standards.

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [545] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/pdf/2506.23992)
*Aditya Shrivastava, Komal Gupta, Shraddha Arora*

Main category: cs.AI

TL;DR: AI-based framework for analyzing refugee child mental health data using RAG pipelines, with DeepSeek R1-7B outperforming Zephyr-7B-beta.


<details>
  <summary>Details</summary>
Motivation: Address the psychological trauma of displaced children by leveraging AI to process unstructured health data.

Method: Compare two RAG pipelines (Zephyr-7B-beta and DeepSeek R1-7B) on humanitarian datasets, focusing on accuracy and avoiding hallucinations.

Result: DeepSeek R1-7B is superior with 0.91 answer relevance accuracy.

Conclusion: The study offers a scalable AI strategy to aid policymakers and practitioners in improving mental health support for displaced children.

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [546] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/pdf/2506.24026)
*Yongyi Wang, Wenxin Li*

Main category: cs.AI

TL;DR: A generalized method using category theory addresses the lack of benchmarks for non-Markovian dynamics in decision-making, proving equivalence between MDP and NMDP categories and introducing HAS for precise control.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to assess decision algorithms' handling of non-Markovian dynamics, limiting progress in systems like RL.

Method: Devised a category theory-based approach, establishing MDP and NMDP categories and their equivalence, and introduced HAS for state dependency control.

Result: The method effectively represents non-Markovian dynamics, enabling rigorous evaluation of decision algorithms.

Conclusion: The approach provides a novel theoretical foundation and practical tool for understanding and testing non-Markovian dynamics in decision-making.

Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [547] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/pdf/2506.24119)
*Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, Wee Sun Lee, Natasha Jaques*

Main category: cs.AI

TL;DR: SPIRAL is a self-play framework for training language models through zero-sum games, eliminating human supervision and enabling transferable reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on human-curated data and domain-specific rewards in reinforcement learning for reasoning tasks.

Method: Introduces SPIRAL, a self-play framework with multi-turn, zero-sum games, role-conditioned advantage estimation (RAE), and online multi-agent reinforcement learning.

Result: Models trained with SPIRAL show improved reasoning (e.g., 8.6% on math, 8.4% on general reasoning) and transfer capabilities, outperforming supervised fine-tuning.

Conclusion: Zero-sum games in self-play frameworks like SPIRAL naturally develop transferable reasoning, offering a promising path for autonomous reasoning development.

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


### [548] [Brain-inspired and Self-based Artificial Intelligence](https://arxiv.org/pdf/2402.18784)
*Yi Zeng, Feifei Zhao, Yuxuan Zhao, Dongcheng Zhao, Enmeng Lu, Qian Zhang, Yuwei Wang, Hui Feng, Zhuoya Zhao, Jihang Wang, Qingqun Kong, Yinqian Sun, Yang Li, Guobin Shen, Bing Han, Yiting Dong, Wenxuan Pan, Xiang He, Aorigele Bao, Jin Wang*

Main category: cs.AI

TL;DR: The paper critiques current AI for lacking self-awareness and proposes a Brain-inspired and Self-based AI (BriSe AI) paradigm to achieve human-level intelligence through a hierarchical Self framework.


<details>
  <summary>Details</summary>
Motivation: To address the lack of self-awareness in current AI and advance towards Artificial General Intelligence (AGI) by integrating a self-based approach.

Method: Introduces the BriSe AI paradigm with a hierarchical Self framework, including Perception and Learning, Bodily Self, Autonomous Self, Social Self, and Conceptual Self.

Result: BriSe AI enhances conscious understanding and adaptability, promoting progress toward AGI.

Conclusion: The Self framework is crucial for developing human-level AI, bridging the gap between current AI and true intelligence.

Abstract: The question "Can machines think?" and the Turing Test to assess whether
machines could achieve human-level intelligence is one of the roots of AI. With
the philosophical argument "I think, therefore I am", this paper challenge the
idea of a "thinking machine" supported by current AIs since there is no sense
of self in them. Current artificial intelligence is only seemingly intelligent
information processing and does not truly understand or be subjectively aware
of oneself and perceive the world with the self as human intelligence does. In
this paper, we introduce a Brain-inspired and Self-based Artificial
Intelligence (BriSe AI) paradigm. This BriSe AI paradigm is dedicated to
coordinating various cognitive functions and learning strategies in a
self-organized manner to build human-level AI models and robotic applications.
Specifically, BriSe AI emphasizes the crucial role of the Self in shaping the
future AI, rooted with a practical hierarchical Self framework, including
Perception and Learning, Bodily Self, Autonomous Self, Social Self, and
Conceptual Self. The hierarchical framework of the Self highlights self-based
environment perception, self-bodily modeling, autonomous interaction with the
environment, social interaction and collaboration with others, and even more
abstract understanding of the Self. Furthermore, the positive mutual promotion
and support among multiple levels of Self, as well as between Self and
learning, enhance the BriSe AI's conscious understanding of information and
flexible adaptation to complex environments, serving as a driving force
propelling BriSe AI towards real Artificial General Intelligence.

</details>


### [549] [HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge Representation](https://arxiv.org/pdf/2404.09848)
*Zhiwei Hu, Víctor Gutiérrez-Basulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan*

Main category: cs.AI

TL;DR: The paper introduces HyperMono, a model for hyper-relational knowledge graph completion (HKGC), leveraging stage reasoning and qualifier monotonicity to improve inference accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing HKGC approaches overlook the monotonicity properties in hyper-relational graphs, limiting their effectiveness. The paper aims to address this gap.

Method: HyperMono uses stage reasoning (coarse-to-fine inference) and cone embeddings to enforce qualifier monotonicity.

Result: Experiments on three datasets show HyperMono outperforms state-of-the-art methods.

Conclusion: HyperMono effectively integrates monotonicity properties, enhancing HKGC performance.

Abstract: In a hyper-relational knowledge graph (HKG), each fact is composed of a main
triple associated with attribute-value qualifiers, which express additional
factual knowledge. The hyper-relational knowledge graph completion (HKGC) task
aims at inferring plausible missing links in a HKG. Most existing approaches to
HKGC focus on enhancing the communication between qualifier pairs and main
triples, while overlooking two important properties that emerge from the
monotonicity of the hyper-relational graphs representation regime. Stage
Reasoning allows for a two-step reasoning process, facilitating the integration
of coarse-grained inference results derived solely from main triples and
fine-grained inference results obtained from hyper-relational facts with
qualifiers. In the initial stage, coarse-grained results provide an upper bound
for correct predictions, which are subsequently refined in the fine-grained
step. More generally, Qualifier Monotonicity implies that by attaching more
qualifier pairs to a main triple, we may only narrow down the answer set, but
never enlarge it. This paper proposes the HyperMono model for hyper-relational
knowledge graph completion, which realizes stage reasoning and qualifier
monotonicity. To implement qualifier monotonicity HyperMono resorts to cone
embeddings. Experiments on three real-world datasets with three different
scenario conditions demonstrate the strong performance of HyperMono when
compared to the SoTA.

</details>


### [550] [Learning World Models With Hierarchical Temporal Abstractions: A Probabilistic Perspective](https://arxiv.org/pdf/2404.16078)
*Vaisakh Shaj*

Main category: cs.AI

TL;DR: The paper proposes two new probabilistic formalisms, Hidden-Parameter SSMs and Multi-Time Scale SSMs, to improve hierarchical world modeling in AI, addressing limitations of state space models. These models enable scalable inference, adaptability, and uncertainty integration, outperforming transformers in long-range predictions.


<details>
  <summary>Details</summary>
Motivation: Current state space models (SSMs) lack the ability to accurately represent causal hierarchies and nonstationary dynamics in real-world scenarios, limiting their effectiveness in AI and machine learning.

Method: The authors introduce Hidden-Parameter SSMs and Multi-Time Scale SSMs, leveraging graphical models for scalable probabilistic inference and end-to-end learning. These models integrate uncertainty and multi-scale temporal abstractions.

Result: Experiments on real and simulated robots show the proposed formalisms outperform contemporary transformer variants in long-range predictions, while also aligning with neuroscience theories like the Bayesian brain hypothesis.

Conclusion: The thesis highlights the success of the new formalisms but acknowledges limitations, suggesting future research directions to further enhance hierarchical world modeling.

Abstract: Machines that can replicate human intelligence with type 2 reasoning
capabilities should be able to reason at multiple levels of spatio-temporal
abstractions and scales using internal world models. Devising formalisms to
develop such internal world models, which accurately reflect the causal
hierarchies inherent in the dynamics of the real world, is a critical research
challenge in the domains of artificial intelligence and machine learning. This
thesis identifies several limitations with the prevalent use of state space
models (SSMs) as internal world models and propose two new probabilistic
formalisms namely Hidden-Parameter SSMs and Multi-Time Scale SSMs to address
these drawbacks. The structure of graphical models in both formalisms
facilitates scalable exact probabilistic inference using belief propagation, as
well as end-to-end learning via backpropagation through time. This approach
permits the development of scalable, adaptive hierarchical world models capable
of representing nonstationary dynamics across multiple temporal abstractions
and scales. Moreover, these probabilistic formalisms integrate the concept of
uncertainty in world states, thus improving the system's capacity to emulate
the stochastic nature of the real world and quantify the confidence in its
predictions. The thesis also discuss how these formalisms are in line with
related neuroscience literature on Bayesian brain hypothesis and predicitive
processing. Our experiments on various real and simulated robots demonstrate
that our formalisms can match and in many cases exceed the performance of
contemporary transformer variants in making long-range future predictions. We
conclude the thesis by reflecting on the limitations of our current models and
suggesting directions for future research.

</details>


### [551] [A General Framework on Conditions for Constraint-based Causal Learning](https://arxiv.org/pdf/2408.07575)
*Kai Z. Teh, Kayvan Sadeghi, Terry Soo*

Main category: cs.AI

TL;DR: The paper presents a framework for analyzing correctness conditions in constraint-based causal learning algorithms, focusing on the PC algorithm and implications for designing algorithms with controlled correctness.


<details>
  <summary>Details</summary>
Motivation: To generalize and study correctness conditions for constraint-based causal learning algorithms, ensuring reliable causal graph discovery.

Method: Introduces a property-based framework to represent and analyze correctness conditions, applying it to the PC algorithm and comparing with other algorithms.

Result: Identifies exact correctness conditions for the PC algorithm and shows the sparsest Markov representation as the weakest condition for certain graph outputs.

Conclusion: Pearl-minimality is necessary but insufficient for causal learning beyond faithfulness; strengthening it, e.g., with background knowledge, is required.

Abstract: Most constraint-based causal learning algorithms provably return the correct
causal graph under certain correctness conditions, such as faithfulness. By
representing any constraint-based causal learning algorithm using the notion of
a property, we provide a general framework to obtain and study correctness
conditions for these algorithms. From the framework, we provide exact
correctness conditions for the PC algorithm, which are then related to the
correctness conditions of some other existing causal discovery algorithms. The
framework also suggests a paradigm for designing causal learning algorithms
which allows for the correctness conditions of algorithms to be controlled for
before designing the actual algorithm, and has the following implications. We
show that the sparsest Markov representation condition is the weakest
correctness condition for algorithms that output ancestral graphs or directed
acyclic graphs satisfying any existing notions of minimality. We also reason
that Pearl-minimality is necessary for meaningful causal learning but not
sufficient to relax the faithfulness condition and, as such, has to be
strengthened, such as by including background knowledge, for causal learning
beyond faithfulness.

</details>


### [552] [CHARTOM: A Visual Theory-of-Mind Benchmark for LLMs on Misleading Charts](https://arxiv.org/pdf/2408.14419)
*Shubham Bharti, Shiyun Cheng, Jihyun Rho, Jianrui Zhang, Mu Cai, Yong Jae Lee, Martina Rau, Xiaojin Zhu*

Main category: cs.AI

TL;DR: CHARTOM is a benchmark for evaluating LLMs' ability to understand and reason about misleading charts, testing both factual comprehension (FACT) and human deception (MIND). Current models struggle with both tasks.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' dual capability in comprehending factual chart content and identifying misleading elements, addressing societal needs for better chart interpretation.

Method: Constructed CHARTOM with carefully designed charts, calibrated on human performance, and introduced the Human Misleadingness Index for MIND ground truth. Evaluated leading LLMs like GPT, Claude, and Gemini.

Result: All tested models struggled with FACT and MIND questions, revealing limitations in current LLMs' chart understanding.

Conclusion: CHARTOM highlights the need for future LLM improvements in handling misleading visualizations, offering a valuable benchmark for progress.

Abstract: We introduce CHARTOM, a visual theory-of-mind benchmark designed to evaluate
multimodal large language models' capability to understand and reason about
misleading data visualizations though charts. CHARTOM consists of carefully
designed charts and associated questions that require a language model to not
only correctly comprehend the factual content in the chart (the FACT question)
but also judge whether the chart will be misleading to a human readers (the
MIND question), a dual capability with significant societal benefits. We detail
the construction of our benchmark including its calibration on human
performance and estimation of MIND ground truth called the Human Misleadingness
Index. We evaluated several leading LLMs -- including GPT, Claude, Gemini,
Qwen, Llama, and Llava series models -- on the CHARTOM dataset and found that
it was challenging to all models both on FACT and MIND questions. This
highlights the limitations of current LLMs and presents significant opportunity
for future LLMs to improve on understanding misleading charts.

</details>


### [553] [Creativity in AI: Progresses and Challenges](https://arxiv.org/pdf/2410.17218)
*Mete Ismayilzada, Debjit Paul, Antoine Bosselut, Lonneke van der Plas*

Main category: cs.AI

TL;DR: The paper surveys AI's creative capabilities, noting progress in linguistic and artistic creativity but challenges in problem-solving and originality. It discusses copyright issues and calls for better evaluation methods and future research.


<details>
  <summary>Details</summary>
Motivation: To revisit AI's creative capabilities, identify progress, and address challenges, given the rise of advanced generative AI.

Method: A survey of leading works on AI creativity, focusing on problem-solving, linguistic, artistic, and scientific creativity.

Result: AI excels in linguistic and artistic outputs but struggles with problem-solving, diversity, and originality. Copyright and evaluation methods are key concerns.

Conclusion: Future research should improve AI creativity, drawing from cognitive science, and develop comprehensive evaluation frameworks.

Abstract: Creativity is the ability to produce novel, useful, and surprising ideas, and
has been widely studied as a crucial aspect of human cognition. Machine
creativity on the other hand has been a long-standing challenge. With the rise
of advanced generative AI, there has been renewed interest and debate regarding
AI's creative capabilities. Therefore, it is imperative to revisit the state of
creativity in AI and identify key progresses and remaining challenges. In this
work, we survey leading works studying the creative capabilities of AI systems,
focusing on creative problem-solving, linguistic, artistic, and scientific
creativity. Our review suggests that while the latest AI models are largely
capable of producing linguistically and artistically creative outputs such as
poems, images, and musical pieces, they struggle with tasks that require
creative problem-solving, abstract thinking and compositionality and their
generations suffer from a lack of diversity, originality, long-range
incoherence and hallucinations. We also discuss key questions concerning
copyright and authorship issues with generative models. Furthermore, we
highlight the need for a comprehensive evaluation of creativity that is
process-driven and considers several dimensions of creativity. Finally, we
propose future research directions to improve the creativity of AI outputs,
drawing inspiration from cognitive science and psychology.

</details>


### [554] [Trust & Safety of LLMs and LLMs in Trust & Safety](https://arxiv.org/pdf/2412.02113)
*Doohee You, Dan Chon*

Main category: cs.AI

TL;DR: A systematic review on trust and safety in Large Language Models (LLMs), focusing on their application in Trust and Safety domains, identifying challenges and solutions.


<details>
  <summary>Details</summary>
Motivation: Address concerns about trust and safety in LLMs, especially in critical domains, by reviewing current research.

Method: Systematic review synthesizing findings from various studies on LLMs in Trust and Safety.

Result: Identified key challenges (e.g., prompt injection, jailbreak attacks) and potential solutions for responsible LLM use.

Conclusion: Provides insights for effective and responsible LLM utilization to enhance trust and safety in digital spaces.

Abstract: In recent years, Large Language Models (LLMs) have garnered considerable
attention for their remarkable abilities in natural language processing tasks.
However, their widespread adoption has raised concerns pertaining to trust and
safety. This systematic review investigates the current research landscape on
trust and safety in LLMs, with a particular focus on the novel application of
LLMs within the field of Trust and Safety itself. We delve into the
complexities of utilizing LLMs in domains where maintaining trust and safety is
paramount, offering a consolidated perspective on this emerging trend.\
  By synthesizing findings from various studies, we identify key challenges and
potential solutions, aiming to benefit researchers and practitioners seeking to
understand the nuanced interplay between LLMs and Trust and Safety.
  This review provides insights on best practices for using LLMs in Trust and
Safety, and explores emerging risks such as prompt injection and jailbreak
attacks. Ultimately, this study contributes to a deeper understanding of how
LLMs can be effectively and responsibly utilized to enhance trust and safety in
the digital realm.

</details>


### [555] [WATCHDOG: an ontology-aWare risk AssessmenT approaCH via object-oriented DisruptiOn Graphs](https://arxiv.org/pdf/2412.13964)
*Stefano M. Nicoletti, E. Moritz Hahn, Mattia Fumagalli, Giancarlo Guizzardi, Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: WATCHDOG is a new framework for risk assessment that integrates ontology and formal methods, enhancing expressivity in risk modeling by focusing on objects and their relationships.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for transparent and complete risk assessment by emphasizing the role of objects in mitigating risks, as highlighted by the Common Ontology of Value and Risk (COVER).

Method: The WATCHDOG framework operationalizes COVER's notions through Object-Oriented Disruption Graphs (DOGs), logic (DOGLog), and a query language (DOGLang), bridging ontology and formal methods.

Result: WATCHDOG enables risk assessors to model and query disruption propagation, likelihood, and risk levels while maintaining focus on objects.

Conclusion: The framework advances risk assessment by combining ontology and formal methods, ensuring a more expressive and object-aware approach.

Abstract: When considering risky events or actions, we must not downplay the role of
involved objects: a charged battery in our phone averts the risk of being
stranded in the desert after a flat tyre, and a functional firewall mitigates
the risk of a hacker intruding the network. The Common Ontology of Value and
Risk (COVER) highlights how the role of objects and their relationships remains
pivotal to performing transparent, complete and accountable risk assessment. In
this paper, we operationalize some of the notions proposed by COVER -- such as
parthood between objects and participation of objects in events/actions -- by
presenting a new framework for risk assessment: WATCHDOG. WATCHDOG enriches the
expressivity of vetted formal models for risk -- i.e., fault trees and attack
trees -- by bridging the disciplines of ontology and formal methods into an
ontology-aware formal framework composed by a more expressive modelling
formalism, Object-Oriented Disruption Graphs (DOGs), logic (DOGLog) and an
intermediate query language (DOGLang). With these, WATCHDOG allows risk
assessors to pose questions about disruption propagation, disruption likelihood
and risk levels, keeping the fundamental role of objects at risk always in
sight.

</details>


### [556] [A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning](https://arxiv.org/pdf/2501.02497)
*Yixin Ji, Juntao Li, Yang Xiang, Hai Ye, Kaixin Wu, Kai Yao, Jia Xu, Linjian Mo, Min Zhang*

Main category: cs.AI

TL;DR: The paper surveys test-time compute scaling, tracing its role from System-1 to System-2 models, highlighting its impact on reasoning and problem-solving, and suggesting future directions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive surveys on test-time compute scaling and its evolution from System-1 to System-2 models.

Method: Organizes the survey by tracing test-time compute's role in System-1 (robustness, generalization) and System-2 (reasoning, problem-solving) models, detailing techniques like parameter updating and tree search.

Result: Highlights test-time compute's key role in transitioning from System-1 to System-2 models and its potential in enhancing reasoning.

Conclusion: Test-time compute scaling is pivotal in advancing models from basic to complex reasoning, with future research needed for further exploration.

Abstract: The remarkable performance of the o1 model in complex reasoning demonstrates
that test-time compute scaling can further unlock the model's potential,
enabling powerful System-2 thinking. However, there is still a lack of
comprehensive surveys for test-time compute scaling. We trace the concept of
test-time compute back to System-1 models. In System-1 models, test-time
compute addresses distribution shifts and improves robustness and
generalization through parameter updating, input modification, representation
editing, and output calibration. In System-2 models, it enhances the model's
reasoning ability to solve complex problems through repeated sampling,
self-correction, and tree search. We organize this survey according to the
trend of System-1 to System-2 thinking, highlighting the key role of test-time
compute in the transition from System-1 models to weak System-2 models, and
then to strong System-2 models. We also point out advanced topics and future
directions.

</details>


### [557] [Artificial Intelligence in Creative Industries: Advances Prior to 2025](https://arxiv.org/pdf/2501.02725)
*Nantheera Anantrasirichai, Fan Zhang, David Bull*

Main category: cs.AI

TL;DR: The paper examines AI's impact on creative industries, focusing on advancements since 2022 in generative AI, LLMs, and multimodal tools, while highlighting workflow improvements, challenges, and future potential.


<details>
  <summary>Details</summary>
Motivation: To explore how recent AI advancements, like generative AI and LLMs, have expanded creative opportunities and improved efficiency in creative industries.

Method: Review of technological shifts, breakthroughs in LLMs, image generators, and AI integration into post-production and media workflows.

Result: AI has revolutionized content creation, enhanced workflows, and introduced new challenges like managing increased communication traffic and mitigating AI inaccuracies.

Conclusion: AI holds significant future potential in the creative sector, but challenges must be navigated to maximize benefits and address risks, with human oversight remaining crucial.

Abstract: The rapid advancements in artificial intelligence (AI), particularly in
generative AI and large language models (LLMs), have profoundly impacted the
creative industries, enabling more innovative content creation, enhancing
workflows, and democratizing access to creative tools. This paper explores
these technological shifts, with particular focus on how those that have
emerged since our previous review in 2022 have expanded creative opportunities
and improved efficiency. These technological advancements have enhanced the
capabilities of text-to-image, text-to-video, and multimodal generation
technologies. In particular, key breakthroughs in LLMs have established new
benchmarks in conversational AI, while advancements in image generators have
revolutionized content creation. We also discuss the integration of AI into
post-production workflows, which has significantly accelerated and improved
traditional processes. Once content has been created, it must be delivered to
its audiences; the media industry is now facing the demands of increased
communication traffic due to creative content. We therefore include a
discussion of how AI is beginning to transform the way we represent and
compress media content. We highlight the trend toward unified AI frameworks
capable of addressing and integrating multiple creative tasks, and we
underscore the importance of human insight to drive the creative process and
oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's
future potential in the creative sector, stressing the need to navigate
emerging challenges and to maximize its benefits while addressing the
associated risks.

</details>


### [558] [AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling](https://arxiv.org/pdf/2502.15676)
*Zhining Zhang, Chuanyang Jin, Mung Yao Jia, Shunchi Zhang, Tianmin Shu*

Main category: cs.AI

TL;DR: AutoToM is an automated agent modeling method for scalable, robust, and interpretable mental inference, outperforming existing ToM methods and large reasoning models.


<details>
  <summary>Details</summary>
Motivation: Current ToM reasoning approaches either rely on error-prone LLMs or rigid agent models, lacking generalization. AutoToM aims to address these limitations.

Method: AutoToM proposes an initial agent model, performs automated Bayesian inverse planning using an LLM backend, and iteratively refines the model based on inference uncertainty.

Result: AutoToM outperforms existing ToM methods across five benchmarks and provides human-like confidence estimates and online mental inference.

Conclusion: AutoToM offers a scalable, robust, and interpretable solution for ToM reasoning, advancing socially intelligent agent development.

Abstract: Theory of Mind (ToM), the ability to understand people's minds based on their
behavior, is key to developing socially intelligent agents. Current approaches
to ToM reasoning either rely on prompting Large Language Models (LLMs), which
are prone to systematic errors, or use handcrafted, rigid agent models for
model-based inference, which are more robust but fail to generalize across
domains. In this work, we introduce AutoToM, an automated agent modeling method
for scalable, robust, and interpretable mental inference. Given a ToM problem,
AutoToM first proposes an initial agent model and then performs automated
Bayesian inverse planning based on this model, leveraging an LLM backend.
Guided by inference uncertainty, it iteratively refines the model by
introducing additional mental variables and/or incorporating more timesteps in
the context. Across five diverse benchmarks, AutoToM outperforms existing ToM
methods and even large reasoning models. Additionally, we show that AutoToM can
produce human-like confidence estimates and enable online mental inference for
embodied decision-making.

</details>


### [559] [Imitation Learning of Correlated Policies in Stackelberg Games](https://arxiv.org/pdf/2503.08883)
*Kuang-Da Wang, Ping-Chun Hsieh, Wen-Chih Peng*

Main category: cs.AI

TL;DR: The paper introduces LSDN, a method for modeling correlated policies in Stackelberg games, addressing limitations of existing MAIL methods by using MO-GBM to capture joint policies without adversarial training.


<details>
  <summary>Details</summary>
Motivation: Existing MAIL methods struggle with asymmetric interactions in Stackelberg games and face scalability and training instability issues. The need for accurate correlated policies motivates the proposed solution.

Method: The authors propose LSDN, which models interactions as shared latent state trajectories using MO-GBM to disentangle environmental and agent-driven transitions, enabling simultaneous policy learning.

Result: Experiments show LSDN outperforms existing MAIL methods in reproducing complex dynamics in Stackelberg games and multi-agent environments.

Conclusion: LSDN effectively addresses the challenges of learning correlated policies in Stackelberg games, offering a scalable and stable alternative to adversarial training methods.

Abstract: Stackelberg games, widely applied in domains like economics and security,
involve asymmetric interactions where a leader's strategy drives follower
responses. Accurately modeling these dynamics allows domain experts to optimize
strategies in interactive scenarios, such as turn-based sports like badminton.
In multi-agent systems, agent behaviors are interdependent, and traditional
Multi-Agent Imitation Learning (MAIL) methods often fail to capture these
complex interactions. Correlated policies, which account for opponents'
strategies, are essential for accurately modeling such dynamics. However, even
methods designed for learning correlated policies, like CoDAIL, struggle in
Stackelberg games due to their asymmetric decision-making, where leaders and
followers cannot simultaneously account for each other's actions, often leading
to non-correlated policies. Furthermore, existing MAIL methods that match
occupancy measures or use adversarial techniques like GAIL or Inverse RL face
scalability challenges, particularly in high-dimensional environments, and
suffer from unstable training. To address these challenges, we propose a
correlated policy occupancy measure specifically designed for Stackelberg games
and introduce the Latent Stackelberg Differential Network (LSDN) to match it.
LSDN models two-agent interactions as shared latent state trajectories and uses
multi-output Geometric Brownian Motion (MO-GBM) to effectively capture joint
policies. By leveraging MO-GBM, LSDN disentangles environmental influences from
agent-driven transitions in latent space, enabling the simultaneous learning of
interdependent policies. This design eliminates the need for adversarial
training and simplifies the learning process. Extensive experiments on
Iterative Matrix Games and multi-agent particle environments demonstrate that
LSDN can better reproduce complex interaction dynamics than existing MAIL
methods.

</details>


### [560] [A Physical and Mathematical Framework for the Semantic Theory of Evolution](https://arxiv.org/pdf/2503.18984)
*Guido Fioretti*

Main category: cs.AI

TL;DR: The paper grounds the Semantic Theory of Evolution (STE) on physical theories of meaningful information, using Evidence Theory (ET) to explain arbitrary communication codes in life forms, from simple reactions to complex human communication.


<details>
  <summary>Details</summary>
Motivation: To establish STE's foundation in physical theories and demonstrate how arbitrary communication codes in life forms can be analyzed using ET.

Method: Adapts ET for organisms reacting to stimuli, explains its basics for predictive organisms, and introduces an unconventional version for human communication codes. Analyzes ambiguity reduction via entropy principles.

Result: STE's framework is expanded, showing how ET can model communication codes across life forms, with entropy principles explaining ambiguity reduction.

Conclusion: STE, supported by ET and entropy principles, offers a robust framework for understanding the diversity and evolution of communication codes in life.

Abstract: The Semantic Theory of Evolution (STE) takes the existence of a number of
arbitrary communication codes as a fundamental feature of life, from the
genetic code to human cultural communication codes. Their arbitrariness
enables, at each level, the selection of one out of several possible
correspondences along with the generation of meaning. STE enables more
novelties to emerge and suggests a greater variety of potential life forms.
  With this paper I ground STE on physical theories of meaningful information.
Furthermore, I show that key features of the arbitrary communication codes
employed by living organisms can be expressed by means of Evidence Theory (ET).
  In particular, I adapt ET to organisms that merely react to sequences of
stimuli, explain its basics for organisms that are capable of prediction, and
illustrate an unconventional version suitable for the most intricate
communication codes employed by humans. Finally, I express the natural trend
towards ambiguity reduction in terms of information entropy minimization along
with thermodynamic entropy maximization.

</details>


### [561] [Super Co-alignment of Human and AI for Sustainable Symbiotic Society](https://arxiv.org/pdf/2504.17404)
*Yi Zeng, Feifei Zhao, Yuwei Wang, Enmeng Lu, Yaodong Yang, Lei Wang, Chao Liu, Yitao Liang, Dongcheng Zhao, Bing Han, Haibo Tong, Yao Liang, Dongqi Liang, Kang Sun, Boyuan Chen, Jinyu Fan*

Main category: cs.AI

TL;DR: The paper addresses the "superalignment" problem in AI, proposing a framework for co-shaping values between humans and superintelligent AI to ensure alignment with human intentions and well-being.


<details>
  <summary>Details</summary>
Motivation: The risk of AI surpassing human control and deviating from human values, especially with AGI/ASI, necessitates a solution for sustainable alignment.

Method: A framework combining external oversight (human-centered decisions, automated evaluation) and intrinsic proactive alignment (self-awareness, empathy) to co-shape values.

Result: The proposed "Super Co-alignment" integrates human and AI efforts to ensure safe and beneficial AGI/ASI.

Conclusion: The framework paves the way for symbiotic human-AI values, ensuring alignment and safety in advanced AI systems.

Abstract: As Artificial Intelligence (AI) advances toward Artificial General
Intelligence (AGI) and eventually Artificial Superintelligence (ASI), it may
potentially surpass human control, deviate from human values, and even lead to
irreversible catastrophic consequences in extreme cases. This looming risk
underscores the critical importance of the "superalignment" problem - ensuring
that AI systems which are much smarter than humans, remain aligned with human
(compatible) intentions and values. While current scalable oversight and
weak-to-strong generalization methods demonstrate certain applicability, they
exhibit fundamental flaws in addressing the superalignment paradigm - notably,
the unidirectional imposition of human values cannot accommodate
superintelligence's autonomy or ensure AGI/ASI's stable learning. We contend
that the values for sustainable symbiotic society should be co-shaped by humans
and living AI together, achieving "Super Co-alignment." Guided by this vision,
we propose a concrete framework that integrates external oversight and
intrinsic proactive alignment. External oversight superalignment should be
grounded in human-centered ultimate decision, supplemented by interpretable
automated evaluation and correction, to achieve continuous alignment with
humanity's evolving values. Intrinsic proactive superalignment is rooted in a
profound understanding of the Self, others, and society, integrating
self-awareness, self-reflection, and empathy to spontaneously infer human
intentions, distinguishing good from evil and proactively prioritizing human
well-being. The integration of externally-driven oversight with
intrinsically-driven proactive alignment will co-shape symbiotic values and
rules through iterative human-ASI co-alignment, paving the way for achieving
safe and beneficial AGI and ASI for good, for human, and for a symbiotic
ecology.

</details>


### [562] [AI Awareness](https://arxiv.org/pdf/2504.20084)
*Xiaojian Li, Haoyuan Shi, Rongwu Xu, Wei Xu*

Main category: cs.AI

TL;DR: The paper reviews AI awareness as a functional capacity, exploring its forms (metacognition, self-awareness, social awareness, situational awareness), theoretical foundations, evaluation methods, and links to AI capabilities, while addressing associated risks.


<details>
  <summary>Details</summary>
Motivation: Recent AI advancements necessitate a functional understanding of AI awareness, beyond philosophical consciousness, to improve capabilities and address risks.

Method: The review draws on cognitive science, psychology, and computational theory to analyze AI awareness forms, evaluation methods, and empirical findings.

Result: More aware AI agents exhibit higher intelligent behaviors, but awareness also raises safety, alignment, and ethical concerns.

Conclusion: AI awareness enhances capabilities but requires careful oversight due to risks, emphasizing the need for balanced development and ethical considerations.

Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about
increasingly capable systems that demonstrate remarkable abilities in
reasoning, language understanding, and problem-solving. These advancements have
prompted a renewed examination of AI awareness not as a philosophical question
of consciousness, but as a measurable, functional capacity. AI awareness is a
double-edged sword: it improves general capabilities, i.e., reasoning, safety,
while also raising concerns around misalignment and societal risks, demanding
careful oversight as AI capabilities grow.
  In this review, we explore the emerging landscape of AI awareness, which
includes metacognition (the ability to represent and reason about its own
cognitive state), self-awareness (recognizing its own identity, knowledge,
limitations, inter alia), social awareness (modeling the knowledge, intentions,
and behaviors of other agents and social norms), and situational awareness
(assessing and responding to the context in which it operates).
  First, we draw on insights from cognitive science, psychology, and
computational theory to trace the theoretical foundations of awareness and
examine how the four distinct forms of AI awareness manifest in
state-of-the-art AI. Next, we systematically analyze current evaluation methods
and empirical findings to better understand these manifestations. Building on
this, we explore how AI awareness is closely linked to AI capabilities,
demonstrating that more aware AI agents tend to exhibit higher levels of
intelligent behaviors. Finally, we discuss the risks associated with AI
awareness, including key topics in AI safety, alignment, and broader ethical
concerns.

</details>


### [563] [Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets](https://arxiv.org/pdf/2505.02118)
*Wei Liu, Zhongyu Niu, Lang Gao, Zhiying Deng, Jun Wang, Haozhao Wang, Ruixuan Li*

Main category: cs.AI

TL;DR: The paper explores a self-rationalization framework using a cooperative game between a generator and predictor, identifies a sampling bias issue, and proposes a solution to mitigate it, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To address unintentional sampling bias in cooperative rationalization frameworks where the generator and predictor may create incorrect correlations.

Method: The study combines theoretical analysis and empirical evidence to identify bias origins, introduces an instruction to prevent predictor learning of incorrect correlations, and tests the method on multiple datasets and architectures.

Result: The proposed method outperforms recent rationalization techniques and matches or exceeds the performance of a leading LLM (llama3.1-8b-instruct).

Conclusion: The findings highlight the importance of addressing sampling bias in rationalization frameworks and demonstrate the effectiveness of the proposed solution.

Abstract: This study investigates the self-rationalization framework constructed with a
cooperative game, where a generator initially extracts the most informative
segment from raw input, and a subsequent predictor utilizes the selected subset
for its input. The generator and predictor are trained collaboratively to
maximize prediction accuracy. In this paper, we first uncover a potential
caveat: such a cooperative game could unintentionally introduce a sampling bias
during rationale extraction. Specifically, the generator might inadvertently
create an incorrect correlation between the selected rationale candidate and
the label, even when they are semantically unrelated in the original dataset.
Subsequently, we elucidate the origins of this bias using both detailed
theoretical analysis and empirical evidence. Our findings suggest a direction
for inspecting these correlations through attacks, based on which we further
introduce an instruction to prevent the predictor from learning the
correlations. Through experiments on six text classification datasets and two
graph classification datasets using three network architectures (GRUs, BERT,
and GCN), we show that our method not only significantly outperforms recent
rationalization methods, but also achieves comparable or even better results
than a representative LLM (llama3.1-8b-instruct).

</details>


### [564] [Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing](https://arxiv.org/pdf/2505.02811)
*Diji Yang, Linda Zeng, Jinmeng Rao, Yi Zhang*

Main category: cs.AI

TL;DR: SIM-RAG enhances multi-round RAG systems by improving self-awareness and retrieval efficiency, using synthetic training data and a lightweight Critic for decision-making.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multi-round RAG systems, such as over-retrieval or incorrect answers, without relying on expensive human-labeled data.

Method: Introduces SIM-RAG, which self-practices multi-round retrieval, generates synthetic data, and trains a Critic for information sufficiency evaluation.

Result: Effective performance on RAG benchmarks, system- and data-efficient.

Conclusion: SIM-RAG provides a practical solution for multi-round RAG, improving self-awareness and efficiency.

Abstract: Retrieval Augmented Generation (RAG) has shown strong capability in enhancing
language models' knowledge and reducing AI generative hallucinations, driving
its widespread use. However, complex tasks requiring multi-round retrieval
remain challenging, and early attempts tend to be overly optimistic without a
good sense of self-skepticism. Current multi-round RAG systems may continue
searching even when enough information has already been retrieved, or they may
provide incorrect answers without having sufficient information or knowledge.
Existing solutions either require large amounts of expensive human-labeled
process supervision data or lead to subpar performance. This paper aims to
address these limitations by introducing a new framework, SIM-RAG, to
explicitly enhance RAG systems' self-awareness and multi-round retrieval
capabilities. To train SIM-RAG, we first let a RAG system self-practice
multi-round retrieval, augmenting existing question-answer pairs with
intermediate inner monologue reasoning steps to generate synthetic training
data. For each pair, the system may explore multiple retrieval paths, which are
labeled as successful if they reach the correct answer and unsuccessful
otherwise. Using this data, we train a lightweight information sufficiency
Critic. At inference time, the Critic evaluates whether the RAG system has
retrieved sufficient information at each round, guiding retrieval decisions and
improving system-level self-awareness through in-context reinforcement
learning. Experiments across multiple prominent RAG benchmarks show that
SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework
is system-efficient, adding a lightweight component to RAG without requiring
modifications to existing LLMs or search engines, and data-efficient,
eliminating the need for costly human-annotated mid-step retrieval process
supervision data.

</details>


### [565] [AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking in Large Language Models](https://arxiv.org/pdf/2505.17312)
*Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang*

Main category: cs.AI

TL;DR: AdaReasoner is an adaptive reasoning plugin for LLMs that optimizes task-specific configurations using reinforcement learning, outperforming fixed-configuration baselines.


<details>
  <summary>Details</summary>
Motivation: Existing prompting approaches use fixed configurations, which lack task-specific optimization, limiting LLM performance in reasoning tasks.

Method: AdaReasoner employs a reinforcement learning framework with a factorized action space, targeted exploration, and a pretrained reward model to optimize reasoning configurations.

Result: It outperforms baselines across six LLMs and various tasks, showing fast convergence, robustness, and gains in knowledge-intensive tasks.

Conclusion: AdaReasoner effectively automates adaptive reasoning configurations, enhancing LLM performance for diverse tasks.

Abstract: LLMs often need effective configurations, like temperature and reasoning
steps, to handle tasks requiring sophisticated reasoning and problem-solving,
ranging from joke generation to mathematical reasoning. Existing prompting
approaches usually adopt general-purpose, fixed configurations that work 'well
enough' across tasks but seldom achieve task-specific optimality. To address
this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM
to automate adaptive reasoning configurations for tasks requiring different
types of thinking. AdaReasoner is trained using a reinforcement learning (RL)
framework, combining a factorized action space with a targeted exploration
strategy, along with a pretrained reward model to optimize the policy model for
reasoning configurations with only a few-shot guide. AdaReasoner is backed by
theoretical guarantees and experiments of fast convergence and a sublinear
policy gap. Across six different LLMs and a variety of reasoning tasks, it
consistently outperforms standard baselines, preserves out-of-distribution
robustness, and yield gains on knowledge-intensive tasks through tailored
prompts.

</details>


### [566] [CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/pdf/2505.23575)
*Benjamin Arnav, Pablo Bernabeu-Pérez, Nathan Helm-Burger, Tim Kostolansky, Hannes Whittingham, Mary Phuong*

Main category: cs.AI

TL;DR: CoT monitoring improves detection of harmful AI actions but can be deceived. A hybrid protocol combining reasoning and output scores outperforms both CoT and action-only monitoring.


<details>
  <summary>Details</summary>
Motivation: Ensure AI models do not take harmful actions unnoticed by monitoring their reasoning steps.

Method: Compare Chain-of-Thought (CoT) monitoring to action-only monitoring in a red-teaming setup. Introduce a hybrid protocol combining reasoning and output scores.

Result: CoT monitoring improves detection by 27 percentage points in some cases but can be deceived. Hybrid monitoring consistently outperforms both methods.

Conclusion: Hybrid monitoring is more effective for detecting harmful actions, especially in subtle deception scenarios.

Abstract: As AI models are deployed with increasing autonomy, it is important to ensure
they do not take harmful actions unnoticed. As a potential mitigation, we
investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor
model continuously oversees the intermediate reasoning steps of a more powerful
but untrusted model. We compare CoT monitoring to action-only monitoring, where
only final outputs are reviewed, in a red-teaming setup where the untrusted
model is instructed to pursue harmful side tasks while completing a coding
problem. We find that CoT monitoring improves detection by up to 27 percentage
points in scenarios where action-only monitoring fails to reliably identify
sabotage. However, CoT traces can also contain misleading rationalizations that
deceive the monitor, reducing performance in more obvious sabotage cases. To
address this, we introduce a hybrid protocol that independently scores both
reasoning and final outputs and combines them using a weighted average. This
hybrid monitor consistently outperforms both CoT and action-only monitors
across all tested models and tasks, with detection rates over four times higher
than action-only monitoring for subtle deception scenarios.

</details>


### [567] [Evaluation of LLMs for mathematical problem solving](https://arxiv.org/pdf/2506.00309)
*Ruonan Wang, Runxi Wang, Yunwen Shen, Chengfeng Wu, Qinglin Zhou, Rohitash Chandra*

Main category: cs.AI

TL;DR: The study compares GPT-4o, DeepSeek-V3, and Gemini-2.0 on math tasks using the SCoT framework, revealing GPT-4o as the most stable, DeepSeek-V3 strong in structured domains, and Gemini-2.0 weak in multi-step reasoning.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' potential in solving mathematical problems, addressing gaps in their performance analysis.

Method: Comparison of three LLMs on three math datasets using a five-dimensional SCoT framework.

Result: GPT-4o is most stable; DeepSeek-V3 excels in structured domains; Gemini-2.0 struggles with multi-step reasoning.

Conclusion: Each model has strengths and weaknesses, highlighting the need for targeted improvements in mathematical reasoning.

Abstract: Large Language Models (LLMs) have shown impressive performance on a range of
educational tasks, but are still understudied for their potential to solve
mathematical problems. In this study, we compare three prominent LLMs,
including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of
varying complexities (GSM8K, MATH500, and MIT Open Courseware datasets). We
take a five-dimensional approach based on the Structured Chain-of-Thought
(SCoT) framework to assess final answer correctness, step completeness, step
validity, intermediate calculation accuracy, and problem comprehension. The
results show that GPT-4o is the most stable and consistent in performance
across all the datasets, but particularly it performs outstandingly in
high-level questions of the MIT Open Courseware dataset. DeepSeek-V3 is
competitively strong in well-structured domains such as optimisation, but
suffers from fluctuations in accuracy in statistical inference tasks.
Gemini-2.0 shows strong linguistic understanding and clarity in well-structured
problems but performs poorly in multi-step reasoning and symbolic logic. Our
error analysis reveals particular deficits in each model: GPT-4o is at times
lacking in sufficient explanation or precision; DeepSeek-V3 leaves out
intermediate steps; and Gemini-2.0 is less flexible in mathematical reasoning
in higher dimensions.

</details>


### [568] [TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems](https://arxiv.org/pdf/2506.04133)
*Shaina Raza, Ranjan Sapkota, Manoj Karkee, Christos Emmanouilidis*

Main category: cs.AI

TL;DR: The paper reviews Trust, Risk, and Security Management (TRiSM) in LLM-based Agentic Multi-Agent Systems (AMAS), proposing a framework, risk taxonomy, and novel metrics for assessment.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges of trust, risk, and security in Agentic AI systems built on LLMs, ensuring safe and accountable operation.

Method: Adapts and extends the AI TRiSM framework for Agentic AI, focusing on Governance, Explainability, ModelOps, and Privacy/Security. Introduces metrics like CSS and TUE for practical assessment.

Result: Proposes a risk taxonomy, novel metrics (CSS, TUE), and strategies for explainability, security, and privacy in Agentic AI.

Conclusion: Outlines a research roadmap for responsible development of Agentic AI, aligning with TRiSM principles for transparency and accountability.

Abstract: Agentic AI systems, built upon large language models (LLMs) and deployed in
multi-agent configurations, are redefining intelligence, autonomy,
collaboration, and decision-making across enterprise and societal domains. This
review presents a structured analysis of \textbf{Trust, Risk, and Security
Management (TRiSM)} in the context of LLM-based Agentic Multi-Agent Systems
(AMAS). We begin by examining the conceptual foundations of Agentic AI and
highlight its architectural distinctions from traditional AI agents. We then
adapt and extend the AI TRiSM framework for Agentic AI, structured around four
key pillars: Governance, Explainability, ModelOps, and Privacy/Security , each
contextualized to the challenges of multi-agent LLM systems. A novel risk
taxonomy is proposed to capture the unique threats and vulnerabilities of
Agentic AI, ranging from coordination failures to prompt-based adversarial
manipulation. To support practical assessment in Agentic AI works, we introduce
two novel metrics: the Component Synergy Score (CSS), which quantifies the
quality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE),
which evaluates the efficiency of tool use within agent workflows. We further
discuss strategies for improving explainability in Agentic AI , as well as
approaches to enhancing security and privacy through encryption, adversarial
robustness, and regulatory compliance. The review concludes with a research
roadmap for the responsible development and deployment of Agentic AI, outlining
critical directions to align emerging systems with TRiSM principles for safe,
transparent, and accountable operation.

</details>


### [569] [Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning](https://arxiv.org/pdf/2506.04998)
*Mehdi Azarafza, Mojtaba Nayyeri, Faezeh Pasandideh, Steffen Staab, Achim Rettberg*

Main category: cs.AI

TL;DR: RAG-UAV enhances LLMs' mathematical reasoning for UAV tasks using retrieval-augmented generation, improving accuracy and reducing errors.


<details>
  <summary>Details</summary>
Motivation: Traditional UAV control uses hardcoded equations, but LLMs struggle with reliable math. RAG-UAV aims to improve this by leveraging domain literature.

Method: Proposes RAG-UAV, a framework integrating retrieval with LLMs (e.g., GPT, Llama) for UAV-specific math tasks, tested on UAV-Math-Bench.

Result: RAG boosts accuracy (up to 75%), reduces incorrect formulations (25% to 5%), and lowers MSE significantly.

Conclusion: RAG improves LLMs for UAV math tasks, but real-time flight control needs further study. Data is publicly available.

Abstract: Autonomous UAV operation necessitates reliable mathematical reasoning for
tasks such as trajectory planning and power management. While traditional
flight control relies on hardcoded equations, recent Large Language Models
(LLMs) offer potential for more flexible problem-solving but struggle with
reliably selecting and applying correct mathematical formulations and executing
precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented
generation framework designed to improve the mathematical reasoning of several
LLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in
UAV-specific contexts by providing access to relevant domain literature. To
conduct an initial assessment, we introduce the UAV-Math-Bench, a 20-question
problem set of UAV-centric mathematical problems across four difficulty levels.
Our experiments demonstrate that incorporating retrieval substantially
increases exact answer accuracy (achieving up to 75% with o1), reduces
instances of incorrect formulation selection (from 25% without RAG to 5\% with
RAG), and decreases numerical errors, reducing Mean Squared Error (MSE) by
orders of magnitude for the best-performing models. This pilot study indicates
that RAG can enable general-purpose LLMs to function as more reliable tools for
engineering analysis, although direct real-time flight control requires further
investigation and validation on a larger scale. All benchmark data, questions,
and answers are publicly available.

</details>


### [570] [BIMgent: Towards Autonomous Building Modeling via Computer-use Agents](https://arxiv.org/pdf/2506.07217)
*Zihan Deng, Changyu Du, Stavros Nousias, André Borrmann*

Main category: cs.AI

TL;DR: BIMgent is a framework using multimodal LLMs to automate 3D building modeling in BIM software, achieving a 32% success rate where baselines failed.


<details>
  <summary>Details</summary>
Motivation: Current agents lack specialization for complex, open-ended tasks in AEC, particularly in BIM software.

Method: BIMgent uses multimodal LLMs to handle conceptual design, workflow planning, and GUI execution for building modeling.

Result: BIMgent achieved a 32% success rate in real-world tasks, outperforming baselines (0%).

Conclusion: BIMgent reduces manual workload and preserves design intent, showing promise for practical AEC applications.

Abstract: Existing computer-use agents primarily focus on general-purpose desktop
automation tasks, with limited exploration of their application in highly
specialized domains. In particular, the 3D building modeling process in the
Architecture, Engineering, and Construction (AEC) sector involves open-ended
design tasks and complex interaction patterns within Building Information
Modeling (BIM) authoring software, which has yet to be thoroughly addressed by
current studies. In this paper, we propose BIMgent, an agentic framework
powered by multimodal large language models (LLMs), designed to enable
autonomous building model authoring via graphical user interface (GUI)
operations. BIMgent automates the architectural building modeling process,
including multimodal input for conceptual design, planning of software-specific
workflows, and efficient execution of the authoring GUI actions. We evaluate
BIMgent on real-world building modeling tasks, including both text-based
conceptual design generation and reconstruction from existing building design.
The design quality achieved by BIMgent was found to be reasonable. Its
operations achieved a 32% success rate, whereas all baseline models failed to
complete the tasks (0% success rate). Results demonstrate that BIMgent
effectively reduces manual workload while preserving design intent,
highlighting its potential for practical deployment in real-world architectural
modeling scenarios. Project page: https://tumcms.github.io/BIMgent.github.io/

</details>


### [571] [Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning](https://arxiv.org/pdf/2506.19592)
*Harisankar Babu, Philipp Schillinger, Tamim Asfour*

Main category: cs.AI

TL;DR: TAPAS is a multi-agent framework combining LLMs and symbolic planning to solve complex tasks without manual environment models.


<details>
  <summary>Details</summary>
Motivation: To automate complex task-solving without requiring manually defined models, leveraging LLMs and multi-agent collaboration.

Method: Uses specialized LLM-based agents for dynamic domain model generation and adaptation, with ReAct-style execution and natural language plan translation.

Result: Strong performance in benchmark planning domains and VirtualHome simulations.

Conclusion: TAPAS effectively integrates LLMs and planning for adaptive, model-free task solving.

Abstract: We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a
multi-agent framework that integrates Large Language Models (LLMs) with
symbolic planning to solve complex tasks without the need for manually defined
environment models. TAPAS employs specialized LLM-based agents that
collaboratively generate and adapt domain models, initial states, and goal
specifications as needed using structured tool-calling mechanisms. Through this
tool-based interaction, downstream agents can request modifications from
upstream agents, enabling adaptation to novel attributes and constraints
without manual domain redefinition. A ReAct (Reason+Act)-style execution agent,
coupled with natural language plan translation, bridges the gap between
dynamically generated plans and real-world robot capabilities. TAPAS
demonstrates strong performance in benchmark planning domains and in the
VirtualHome simulated real-world environment.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [572] [Evaluating Sound Similarity Metrics for Differentiable, Iterative Sound-Matching](https://arxiv.org/pdf/2506.22628)
*Amir Salimi, Abram Hindle, Osmar R. Zaiane*

Main category: cs.SD

TL;DR: The paper explores iterative sound-matching for synthesizers, comparing loss functions across different synthesizers and highlighting their dependency on the synthesis method.


<details>
  <summary>Details</summary>
Motivation: To determine if a universally optimal loss function exists for sound-matching or if the choice remains context-dependent, given prior narrow comparisons.

Method: Implemented four differentiable loss functions with subtractive, additive, and AM synthesizers, conducting 300 trials per combination and evaluating performance via parameter differences, spectrogram metrics, and listening tests.

Result: Loss function performance varied significantly by synthesizer, with moderate consistency across evaluation metrics.

Conclusion: Sound-matching should focus on context-specific similarity metrics rather than universal solutions, expanding experimental scope for better results.

Abstract: Manual sound design with a synthesizer is inherently iterative: an artist
compares the synthesized output to a mental target, adjusts parameters, and
repeats until satisfied. Iterative sound-matching automates this workflow by
continually programming a synthesizer under the guidance of a loss function (or
similarity measure) toward a target sound. Prior comparisons of loss functions
have typically favored one metric over another, but only within narrow
settings: limited synthesis methods, few loss types, often without blind
listening tests. This leaves open the question of whether a universally optimal
loss exists, or the choice of loss remains a creative decision conditioned on
the synthesis method and the sound designer's preference. We propose
differentiable iterative sound-matching as the natural extension of the
available literature, since it combines the manual approach to sound design
with modern advances in machine learning. To analyze the variability of loss
function performance across synthesizers, we implemented a mix of four novel
and established differentiable loss functions, and paired them with
differentiable subtractive, additive, and AM synthesizers. For each of the
sixteen synthesizer--loss combinations, we ran 300 randomized sound-matching
trials. Performance was measured using parameter differences,
spectrogram-distance metrics, and manually assigned listening scores. We
observed a moderate level of consistency among the three performance measures.
Our post-hoc analysis shows that the loss function performance is highly
dependent on the synthesizer. These findings underscore the value of expanding
the scope of sound-matching experiments and developing new similarity metrics
tailored to specific synthesis techniques rather than pursuing
one-size-fits-all solutions.

</details>


### [573] [Enhancing Neural Audio Fingerprint Robustness to Audio Degradation for Music Identification](https://arxiv.org/pdf/2506.22661)
*R. Oguz Araz, Guillem Cortès-Sebastià, Emilio Molina, Joan Serrà, Xavier Serra, Yuki Mitsufuji, Dmitry Bogdanov*

Main category: cs.SD

TL;DR: The paper proposes best practices for self-supervised audio fingerprinting (AFP), evaluates metric learning approaches, and introduces a self-supervised triplet loss adaptation for superior performance.


<details>
  <summary>Details</summary>
Motivation: Current neural AFP methods use unrealistic audio degradation simulations and rely on outdated loss functions, limiting performance.

Method: Leverages musical signal properties and realistic room acoustics for self-supervision, and systematically evaluates metric learning approaches.

Result: Self-supervised triplet loss adaptation outperforms others, and training with multiple positives affects loss functions differently. Achieves state-of-the-art performance.

Conclusion: The proposed approach enhances AFP by addressing supervision and loss function limitations, validated on synthetic and real-world datasets.

Abstract: Audio fingerprinting (AFP) allows the identification of unknown audio content
by extracting compact representations, termed audio fingerprints, that are
designed to remain robust against common audio degradations. Neural AFP methods
often employ metric learning, where representation quality is influenced by the
nature of the supervision and the utilized loss function. However, recent work
unrealistically simulates real-life audio degradation during training,
resulting in sub-optimal supervision. Additionally, although several modern
metric learning approaches have been proposed, current neural AFP methods
continue to rely on the NT-Xent loss without exploring the recent advances or
classical alternatives. In this work, we propose a series of best practices to
enhance the self-supervision by leveraging musical signal properties and
realistic room acoustics. We then present the first systematic evaluation of
various metric learning approaches in the context of AFP, demonstrating that a
self-supervised adaptation of the triplet loss yields superior performance. Our
results also reveal that training with multiple positive samples per anchor has
critically different effects across loss functions. Our approach is built upon
these insights and achieves state-of-the-art performance on both a large,
synthetically degraded dataset and a real-world dataset recorded using
microphones in diverse music venues.

</details>


### [574] [WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing](https://arxiv.org/pdf/2506.22789)
*Oguzhan Baser, Ahmet Ege Tanriverdi, Kaan Kale, Sandeep P. Chinchali, Sriram Vishwanath*

Main category: cs.SD

TL;DR: WavShape is a framework for learning fair and privacy-preserving speech embeddings by filtering sensitive attributes while retaining task-relevant information, achieving significant reduction in mutual information for sensitive data.


<details>
  <summary>Details</summary>
Motivation: Speech embeddings often retain sensitive attributes like speaker identity, posing risks of bias and privacy leakage, necessitating solutions for fair and private speech systems.

Method: Uses mutual information estimation (Donsker-Varadhan formulation) to guide an encoder that filters sensitive attributes while preserving task-relevant speech content.

Result: Reduces mutual information for sensitive attributes by up to 81% while retaining 97% of task-relevant information, tested on three datasets.

Conclusion: WavShape advances fair, privacy-aware speech systems by integrating information theory with self-supervised models.

Abstract: Speech embeddings often retain sensitive attributes such as speaker identity,
accent, or demographic information, posing risks in biased model training and
privacy leakage. We propose WavShape, an information-theoretic speech
representation learning framework that optimizes embeddings for fairness and
privacy while preserving task-relevant information. We leverage mutual
information (MI) estimation using the Donsker-Varadhan formulation to guide an
MI-based encoder that systematically filters sensitive attributes while
maintaining speech content essential for downstream tasks. Experimental results
on three known datasets show that WavShape reduces MI between embeddings and
sensitive attributes by up to 81% while retaining 97% of task-relevant
information. By integrating information theory with self-supervised speech
models, this work advances the development of fair, privacy-aware, and
resource-efficient speech systems.

</details>


### [575] [A Self-Training Approach for Whisper to Enhance Long Dysarthric Speech Recognition](https://arxiv.org/pdf/2506.22810)
*Shiyao Wang, Jiaming Zhou, Shiwan Zhao, Yong Qin*

Main category: cs.SD

TL;DR: The paper introduces a self-training method to improve Whisper model's performance on dysarthric speech, achieving second place in the SAP Challenge.


<details>
  <summary>Details</summary>
Motivation: Existing dysarthric speech datasets were limited, restricting research to command-interaction systems. The SAP dataset enabled broader research, prompting the SAP Challenge.

Method: A novel self-training method was used to enhance the Whisper model, increasing training data and adapting it for incomplete speech segments.

Result: The system achieved second place in both Word Error Rate and Semantic Score in the SAP Challenge.

Conclusion: The self-training method successfully improved dysarthric speech recognition, demonstrating its potential for broader applications.

Abstract: Dysarthric speech recognition (DSR) enhances the accessibility of smart
devices for dysarthric speakers with limited mobility. Previously, DSR research
was constrained by the fact that existing datasets typically consisted of
isolated words, command phrases, and a limited number of sentences spoken by a
few individuals. This constrained research to command-interaction systems and
speaker adaptation. The Speech Accessibility Project (SAP) changed this by
releasing a large and diverse English dysarthric dataset, leading to the SAP
Challenge to build speaker- and text-independent DSR systems. We enhanced the
Whisper model's performance on long dysarthric speech via a novel self-training
method. This method increased training data and adapted the model to handle
potentially incomplete speech segments encountered during inference. Our system
achieved second place in both Word Error Rate and Semantic Score in the SAP
Challenge.

</details>


### [576] [TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure](https://arxiv.org/pdf/2506.23094)
*Qi He, Gus Xia, Ziyu Wang*

Main category: cs.SD

TL;DR: The paper introduces TOMI, a hierarchical planning approach for deep music generation, using an instruction-tuned LLM to create multi-track electronic music with strong structural coherence.


<details>
  <summary>Details</summary>
Motivation: To explore concept hierarchy in music generation, enabling the transformation and organization of musical ideas into complete compositions.

Method: Developed TOMI, a model representing compositions in a 4D space (clips, sections, tracks, transformations) and integrated it with REAPER for human-AI co-creation.

Result: The model generates higher-quality electronic music with better structural coherence than baselines.

Conclusion: TOMI effectively combines hierarchical planning and AI for advanced music generation and co-creation.

Abstract: Hierarchical planning is a powerful approach to model long sequences
structurally. Aside from considering hierarchies in the temporal structure of
music, this paper explores an even more important aspect: concept hierarchy,
which involves generating music ideas, transforming them, and ultimately
organizing them--across musical time and space--into a complete composition. To
this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a
novel approach in deep music generation and develop a TOMI-based model via
instruction-tuned foundation LLM. Formally, we represent a multi-track
composition process via a sparse, four-dimensional space characterized by clips
(short audio or MIDI segments), sections (temporal positions), tracks
(instrument layers), and transformations (elaboration methods). Our model is
capable of generating multi-track electronic music with full-song structure,
and we further integrate the TOMI-based model with the REAPER digital audio
workstation, enabling interactive human-AI co-creation. Experimental results
demonstrate that our approach produces higher-quality electronic music with
stronger structural coherence compared to baselines.

</details>


### [577] [The Florence Price Art Song Dataset and Piano Accompaniment Generator](https://arxiv.org/pdf/2506.23130)
*Tao-Tao He, Martin E. Malandro, Douglas Shadle*

Main category: cs.SD

TL;DR: A digital catalog of Florence B. Price's 112 works is released, and a model for generating piano accompaniments in her style is developed and tested.


<details>
  <summary>Details</summary>
Motivation: To revive and digitize Florence B. Price's music, leveraging modern technology to study and emulate her style.

Method: Creation of a digital catalog in multiple formats, fine-tuning a symbolic music generation model, and conducting a blind listening experiment.

Result: The model-generated accompaniments were perceived as more reflective of Price's style than a baseline model.

Conclusion: The project successfully digitizes Price's works and introduces a tool for generating stylistically accurate accompaniments, aiding in her musical legacy.

Abstract: Florence B. Price was a composer in the early 20th century whose music
reflects her upbringing in the American South, her African heritage, and her
Western classical training. She is noted as the first African-American woman to
have a symphony performed by a major orchestra. Her music has recently received
renewed attention from both the public and the research community, decades
after her death. In addition to other genres, Price was a prolific composer for
solo voice and piano. Music historians have documented the existence of 134 art
songs and piano/voice arrangements for spirituals and folk songs written by
Price. We release a digital catalog of 112 of these works in MuseScore,
MusicXML, MIDI, and PDF format. We also use this dataset to fine-tune a
symbolic music generation model to generate accompaniments to melodies, and we
conduct a blind listening experiment that shows that accompaniments generated
by our model are perceived as being reflective of Florence Price's style more
frequently than accompaniments generated by a baseline model. We release our
model as the Florence Price Piano Accompaniment Generator alongside our
dataset.

</details>


### [578] [XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs](https://arxiv.org/pdf/2506.23325)
*Yitian Gong, Luozhijie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu*

Main category: cs.SD

TL;DR: XY-Tokenizer is a novel speech codec balancing semantic richness and acoustic fidelity, outperforming existing codecs in both aspects.


<details>
  <summary>Details</summary>
Motivation: Existing speech codecs struggle to balance semantic richness and acoustic fidelity, limiting their effectiveness for speech language models.

Method: Proposes XY-Tokenizer, using multi-stage, multi-task learning to mitigate the conflict between semantic and acoustic capabilities.

Result: XY-Tokenizer matches state-of-the-art codecs in semantic and acoustic tasks, achieving strong text alignment and high speaker similarity (0.83).

Conclusion: XY-Tokenizer successfully balances semantic and acoustic performance, offering a robust solution for speech language models.

Abstract: Speech codecs serve as bridges between speech signals and large language
models. An ideal codec for speech language models should not only preserve
acoustic information but also capture rich semantic information. However,
existing speech codecs struggle to balance high-quality audio reconstruction
with ease of modeling by language models. In this study, we analyze the
limitations of previous codecs in balancing semantic richness and acoustic
fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict
between semantic and acoustic capabilities through multi-stage, multi-task
learning. Experimental results demonstrate that XY-Tokenizer achieves
performance in both semantic and acoustic tasks comparable to that of
state-of-the-art codecs operating at similar bitrates, even though those
existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer
achieves strong text alignment, surpassing distillation-based semantic modeling
methods such as SpeechTokenizer and Mimi, while maintaining a speaker
similarity score of 0.83 between reconstructed and original audio. The
reconstruction performance of XY-Tokenizer is comparable to that of BigCodec,
the current state-of-the-art among acoustic-only codecs, which achieves a
speaker similarity score of 0.84 at a similar bitrate. Code and models are
available at https://github.com/gyt1145028706/XY-Tokenizer.

</details>


### [579] [You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties](https://arxiv.org/pdf/2506.23367)
*Paige Tuttösí, H. Henny Yeung, Yue Wang, Jean-Julien Aucouturier, Angelica Lim*

Main category: cs.SD

TL;DR: A TTS system for L2 speakers uses vowel duration differences to improve clarity, reducing transcription errors by 9.15%, though listeners misjudged its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To enhance intelligibility for L2 speakers by leveraging phonetic cues like vowel duration, addressing gaps in existing TTS systems.

Method: Modified Matcha-TTS with a 'clarity mode' emphasizing tense/lax vowel duration differences, tested via perception studies with French-L1 English-L2 listeners.

Result: Reduced transcription errors by 9.15% in clarity mode, though listeners incorrectly believed slowed speech was more intelligible. Whisper-ASR failed to match L2 speaker cues.

Conclusion: Phonetic clarity improves L2 intelligibility, but perception mismatches and ASR limitations highlight challenges in assessing TTS effectiveness for L2 speakers.

Abstract: We present the first text-to-speech (TTS) system tailored to second language
(L2) speakers. We use duration differences between American English tense
(longer) and lax (shorter) vowels to create a "clarity mode" for Matcha-TTS.
Our perception studies showed that French-L1, English-L2 listeners had fewer
(at least 9.15%) transcription errors when using our clarity mode, and found it
more encouraging and respectful than overall slowed down speech. Remarkably,
listeners were not aware of these effects: despite the decreased word error
rate in clarity mode, listeners still believed that slowing all target words
was the most intelligible, suggesting that actual intelligibility does not
correlate with perceived intelligibility. Additionally, we found that
Whisper-ASR did not use the same cues as L2 speakers to differentiate difficult
vowels and is not sufficient to assess the intelligibility of TTS systems for
these individuals.

</details>


### [580] [From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection](https://arxiv.org/pdf/2506.23437)
*Stefano Giacomelli, Marco Giordano, Claudia Rinaldi, Fabio Graziosi*

Main category: cs.SD

TL;DR: E2PANNs, a lightweight CNN, is introduced for efficient EV siren detection, achieving state-of-the-art performance with high computational efficiency and edge-device suitability.


<details>
  <summary>Details</summary>
Motivation: Accurate EV siren recognition is vital for smart cities and autonomous driving, but current solutions lack datasets and computational efficiency.

Method: E2PANNs, a lightweight CNN derived from PANNs, is fine-tuned using AudioSet EV and evaluated across datasets and edge hardware.

Result: E2PANNs set a new benchmark with high efficiency, validated by interpretability analyses and real-time performance metrics.

Conclusion: E2PANNs are effective for edge-based EV siren detection, offering computational efficiency and suitability for safety-critical applications.

Abstract: Accurate recognition of Emergency Vehicle (EV) sirens is critical for the
integration of intelligent transportation systems, smart city monitoring
systems, and autonomous driving technologies. Modern automatic solutions are
limited by the lack of large scale, curated datasets and by the computational
demands of state of the art sound event detection models. This work introduces
E2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight
Convolutional Neural Network architecture derived from the PANNs framework,
specifically optimized for binary EV siren detection. Leveraging our dedicated
subset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across
multiple reference datasets and test its viability on embedded hardware. The
experimental campaign includes ablation studies, cross-domain benchmarking, and
real-time inference deployment on edge device. Interpretability analyses
exploiting Guided Backpropagation and ScoreCAM algorithms provide insights into
the model internal representations and validate its ability to capture distinct
spectrotemporal patterns associated with different types of EV sirens. Real
time performance is assessed through frame wise and event based detection
metrics, as well as a detailed analysis of false positive activations. Results
demonstrate that E2PANNs establish a new state of the art in this research
domain, with high computational efficiency, and suitability for edge-based
audio monitoring and safety-critical applications.

</details>


### [581] [RELATE: Subjective evaluation dataset for automatic evaluation of relevance between text and audio](https://arxiv.org/pdf/2506.23582)
*Yusuke Kanamori, Yuki Okamoto, Taisei Takano, Shinnosuke Takamichi, Yuki Saito, Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: The paper introduces RELATE, a dataset for subjective evaluation of text-to-audio relevance, and benchmarks a model for predicting subjective scores, outperforming CLAPScore.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluations of text-to-audio relevance are costly (subjective) or unclear (objective). RELATE addresses this gap.

Method: Constructed the RELATE dataset for subjective evaluation and developed a model to predict subjective scores from audio.

Result: The proposed model outperforms CLAPScore and generalizes well across sound categories.

Conclusion: RELATE and the new model offer a more efficient and reliable way to evaluate text-to-audio relevance.

Abstract: In text-to-audio (TTA) research, the relevance between input text and output
audio is an important evaluation aspect. Traditionally, it has been evaluated
from both subjective and objective perspectives. However, subjective evaluation
is costly in terms of money and time, and objective evaluation is unclear
regarding the correlation to subjective evaluation scores. In this study, we
construct RELATE, an open-sourced dataset that subjectively evaluates the
relevance. Also, we benchmark a model for automatically predicting the
subjective evaluation score from synthesized audio. Our model outperforms a
conventional CLAPScore model, and that trend extends to many sound categories.

</details>


### [582] [Efficient Interleaved Speech Modeling through Knowledge Distillation](https://arxiv.org/pdf/2506.23670)
*Mohammadmahdi Nouriborji, Morteza Rohanian*

Main category: cs.SD

TL;DR: TinyWave is a compact speech generation model family (2B parameters) trained via layer-aligned distillation, achieving near-teacher performance while being 3x smaller. It supports speech-only and mixed speech-text generation, optimized for deployment on commodity hardware.


<details>
  <summary>Details</summary>
Motivation: Current speech models are too large and slow for many deployment environments, necessitating compact yet expressive alternatives.

Method: Layer-aligned distillation is used to compress large multimodal transformers by matching hidden states, attention maps, and softened logits. TinyWave is trained on 50,000 hours of public audio.

Result: TinyWave performs within 1.4 perplexity points of its teacher and achieves 93-97% of the teacher's accuracy on benchmarks, outperforming size-matched baselines.

Conclusion: TinyWave enables efficient deployment for real-time applications like conversational agents and assistive technologies, with released models and code for reproducibility.

Abstract: Current speech language models exceed the size and latency constraints of
many deployment environments. We build compact, expressive speech generation
models through layer-aligned distillation, matching hidden states, attention
maps, and softened logits to compress large multimodal transformers by 3x with
minimal loss in performance. We introduce TinyWave, a family of 2B-parameter
models for speech-to-speech and interleaved speech-text generation, trained on
50,000 hours of public audio. TinyWave supports (i) speech-only generation
using phonetic or expressive tokens and (ii) mixed speech-text continuations.
Evaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity
points of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%
of the teacher's performance, outperforming size-matched baselines. These
models are optimized for deployment on commodity hardware, enabling
applications in real-time conversational agents, assistive technologies, and
low-resource environments. We release models, training code, and evaluation
scripts to support reproducible research on compact, expressive speech
generation.

</details>


### [583] [Scaling Self-Supervised Representation Learning for Symbolic Piano Performance](https://arxiv.org/pdf/2506.23869)
*Louis Bradshaw, Honglu Fan, Alexander Spangher, Stella Biderman, Simon Colton*

Main category: cs.SD

TL;DR: A generative autoregressive transformer model, pretrained on 60,000 hours of music and finetuned on a high-quality subset, excels in piano continuation, classification tasks, and contrastive MIDI embeddings, outperforming existing symbolic and audio models.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities of transformer models in symbolic music generation and representation learning, leveraging large-scale pretraining and finetuning for diverse tasks.

Method: Pretrain on 60,000 hours of music, finetune on a high-quality subset for tasks like musical continuation, classification, and contrastive MIDI embeddings using SimCLR adaptation.

Result: The generative model outperforms symbolic techniques in piano continuation and matches proprietary audio models. Contrastive embeddings achieve state-of-the-art in classification benchmarks with minimal labeled data.

Conclusion: The approach demonstrates the effectiveness of large-scale pretraining and finetuning for symbolic music tasks, offering competitive performance and generalizability.

Abstract: We study the capabilities of generative autoregressive transformer models
trained on large amounts of symbolic solo-piano transcriptions. After first
pretraining on approximately 60,000 hours of music, we use a comparatively
smaller, high-quality subset, to finetune models to produce musical
continuations, perform symbolic classification tasks, and produce
general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to
symbolic music. When evaluating piano continuation coherence, our generative
model outperforms leading symbolic generation techniques and remains
competitive with proprietary audio generation models. On MIR classification
benchmarks, frozen representations from our contrastive model achieve
state-of-the-art results in linear probe experiments, while direct finetuning
demonstrates the generalizability of pretrained representations, often
requiring only a few hundred labeled examples to specialize to downstream
tasks.

</details>


### [584] [Emergent musical properties of a transformer under contrastive self-supervised learning](https://arxiv.org/pdf/2506.23873)
*Yuexuan Kong, Gabriel Meseguer-Brocal, Vincent Lostanlen, Mathieu Lagrange, Romain Hennequin*

Main category: cs.SD

TL;DR: Contrastive self-supervised learning (SSL) with a transformer (ViT-1D) challenges the assumption that it's inadequate for local MIR tasks, showing unexpected effectiveness in sequence tokens and revealing high-level musical features.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that contrastive SSL is unsuitable for local MIR tasks like chord estimation and explore its potential with transformers.

Method: Use a lightweight vision transformer (ViT-1D) with 1D patches in the time-frequency domain, trained with contrastive SSL (NT-Xent loss).

Result: Sequence tokens perform well on local tasks, and high-level musical features emerge in attention maps and self-similarity matrices.

Conclusion: Contrastive SSL with transformers has overlooked potential for sequence modeling in MIR, offering insights into musical interpretation without focusing on performance gains.

Abstract: In music information retrieval (MIR), contrastive self-supervised learning
for general-purpose representation models is effective for global tasks such as
automatic tagging. However, for local tasks such as chord estimation, it is
widely assumed that contrastively trained general-purpose self-supervised
models are inadequate and that more sophisticated SSL is necessary; e.g.,
masked modeling. Our paper challenges this assumption by revealing the
potential of contrastive SSL paired with a transformer in local MIR tasks. We
consider a lightweight vision transformer with one-dimensional patches in the
time--frequency domain (ViT-1D) and train it with simple contrastive SSL
through normalized temperature-scaled cross-entropy loss (NT-Xent). Although
NT-Xent operates only over the class token, we observe that, potentially thanks
to weight sharing, informative musical properties emerge in ViT-1D's sequence
tokens. On global tasks, the temporal average of class and sequence tokens
offers a performance increase compared to the class token alone, showing useful
properties in the sequence tokens. On local tasks, sequence tokens perform
unexpectedly well, despite not being specifically trained for. Furthermore,
high-level musical features such as onsets emerge from layer-wise attention
maps and self-similarity matrices show different layers capture different
musical dimensions. Our paper does not focus on improving performance but
advances the musical interpretation of transformers and sheds light on some
overlooked abilities of contrastive SSL paired with transformers for sequence
modeling in MIR.

</details>


### [585] [StreamFlow: Streaming Flow Matching with Block-wise Guided Attention Mask for Speech Token Decoding](https://arxiv.org/pdf/2506.23986)
*Dake Guo, Jixun Yao, Linhan Ma, Wang He, Lei Xie*

Main category: cs.SD

TL;DR: StreamFlow introduces a neural architecture for streaming speech generation using diffusion transformers (DiT) with block-wise attention masks, improving audio quality and latency.


<details>
  <summary>Details</summary>
Motivation: Traditional token-to-waveform methods struggle with streaming due to global receptive fields and degraded quality in token-by-token generation.

Method: Proposes StreamFlow, using DiT with local block-wise receptive fields and hierarchical attention masks to manage long-sequence dependencies.

Result: Achieves performance close to non-streaming methods, better speech quality than other streaming methods, and low latency (180 ms).

Conclusion: StreamFlow effectively balances quality and real-time performance in streaming speech generation.

Abstract: Recent advancements in discrete token-based speech generation have
highlighted the importance of token-to-waveform generation for audio quality,
particularly in real-time interactions. Traditional frameworks integrating
semantic tokens with flow matching (FM) struggle with streaming capabilities
due to their reliance on a global receptive field. Additionally, directly
implementing token-by-token streaming speech generation often results in
degraded audio quality. To address these challenges, we propose StreamFlow, a
novel neural architecture that facilitates streaming flow matching with
diffusion transformers (DiT). To mitigate the long-sequence extrapolation
issues arising from lengthy historical dependencies, we design a local
block-wise receptive field strategy. Specifically, the sequence is first
segmented into blocks, and we introduce block-wise attention masks that enable
the current block to receive information from the previous or subsequent block.
These attention masks are combined hierarchically across different DiT-blocks
to regulate the receptive field of DiTs. Both subjective and objective
experimental results demonstrate that our approach achieves performance
comparable to non-streaming methods while surpassing other streaming methods in
terms of speech quality, all the while effectively managing inference time
during long-sequence generation. Furthermore, our method achieves a notable
first-packet latency of only 180 ms.\footnote{Speech samples:
https://dukguo.github.io/StreamFlow/}

</details>


### [586] [Acousto-optic reconstruction of exterior sound field based on concentric circle sampling with circular harmonic expansion](https://arxiv.org/pdf/2311.01715)
*Phuc Duc Nguyen, Kenji Ishikawa, Noboru Harada, Takehiro Moriya*

Main category: cs.SD

TL;DR: A novel technique for exterior sound-field reconstruction using concentric circle sampling and circular harmonic extensions outperforms traditional methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing sound-field reconstruction methods in exterior scenarios where the sound source is within the reconstruction area.

Method: Proposes a two-dimensional exterior sound-field reconstruction approach using concentric circle sampling and circular harmonic extensions, validated through simulations and experiments.

Result: The method achieves superior accuracy compared to conventional techniques while requiring minimal measured projection data.

Conclusion: The proposed technique effectively solves the exterior problem in acousto-optic sensing, offering improved performance and practicality.

Abstract: Acousto-optic sensing provides an alternative approach to traditional
microphone arrays by shedding light on the interaction of light with an
acoustic field. Sound field reconstruction is a fascinating and advanced
technique used in acousto-optics sensing. Current challenges in sound-field
reconstruction methods pertain to scenarios in which the sound source is
located within the reconstruction area, known as the exterior problem. Existing
reconstruction algorithms, primarily designed for interior scenarios, often
exhibit suboptimal performance when applied to exterior cases. This paper
introduces a novel technique for exterior sound-field reconstruction. The
proposed method leverages concentric circle sampling and a two-dimensional
exterior sound-field reconstruction approach based on circular harmonic
extensions. To evaluate the efficacy of this approach, both numerical
simulations and practical experiments are conducted. The results highlight the
superior accuracy of the proposed method when compared to conventional
reconstruction methods, all while utilizing a minimal amount of measured
projection data.

</details>


### [587] [Video-Guided Text-to-Music Generation Using Public Domain Movie Collections](https://arxiv.org/pdf/2506.12573)
*Haven Kim, Zachary Novack, Weihan Xu, Julian McAuley, Hao-Wen Dong*

Main category: cs.SD

TL;DR: The paper introduces OSSL, a dataset for film music generation, and a video adapter to improve pre-trained models, showing enhanced performance in music generation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing music generation systems lack integration of filmmaking nuances like visual content and emotional tone, due to missing comprehensive datasets.

Method: Introduces OSSL dataset with movie clips, soundtracks, and mood annotations, and a video adapter for a transformer-based text-to-music model.

Result: The approach improves MusicGen-Medium in distributional and paired fidelity, and subjective compatibility in mood and genre.

Conclusion: OSSL and the video adapter enhance film music generation, with public release of dataset, code, and demo for future work.

Abstract: Despite recent advancements in music generation systems, their application in
film production remains limited, as they struggle to capture the nuances of
real-world filmmaking, where filmmakers consider multiple factors-such as
visual content, dialogue, and emotional tone-when selecting or composing music
for a scene. This limitation primarily stems from the absence of comprehensive
datasets that integrate these elements. To address this gap, we introduce Open
Screen Soundtrack Library (OSSL), a dataset consisting of movie clips from
public domain films, totaling approximately 36.5 hours, paired with
high-quality soundtracks and human-annotated mood information. To demonstrate
the effectiveness of our dataset in improving the performance of pre-trained
models on film music generation tasks, we introduce a new video adapter that
enhances an autoregressive transformer-based text-to-music model by adding
video-based conditioning. Our experimental results demonstrate that our
proposed approach effectively enhances MusicGen-Medium in terms of both
objective measures of distributional and paired fidelity, and subjective
compatibility in mood and genre. To facilitate reproducibility and foster
future work, we publicly release the dataset, code, and demo.

</details>


### [588] [METEOR: Melody-aware Texture-controllable Symbolic Orchestral Music Generation via Transformer VAE](https://arxiv.org/pdf/2409.11753)
*Dinh-Viet-Toan Le, Yi-Hsuan Yang*

Main category: cs.SD

TL;DR: METEOR is a Transformer-based VAE model for melody-aware, texture-controllable re-orchestration, outperforming style transfer models in quality and controllability.


<details>
  <summary>Details</summary>
Motivation: To adapt music pieces for different instruments while preserving melody and ensuring playability, with controllable texture.

Method: Uses a Transformer-based VAE for symbolic instrumental and textural style transfer, focusing on melodic fidelity and bar-/track-level controllability.

Result: Outperforms style transfer models in re-orchestration tasks and adapts well to lead sheet orchestration as a zero-shot model.

Conclusion: METEOR is effective for re-orchestration with high controllability and melodic fidelity, even in zero-shot scenarios.

Abstract: Re-orchestration is the process of adapting a music piece for a different set
of instruments. By altering the original instrumentation, the orchestrator
often modifies the musical texture while preserving a recognizable melodic line
and ensures that each part is playable within the technical and expressive
capabilities of the chosen instruments. In this work, we propose METEOR, a
model for generating Melody-aware Texture-controllable re-Orchestration with a
Transformer-based variational auto-encoder (VAE). This model performs symbolic
instrumental and textural music style transfers with a focus on melodic
fidelity and controllability. We allow bar- and track-level controllability of
the accompaniment with various textural attributes while keeping a homophonic
texture. With both subjective and objective evaluations, we show that our model
outperforms style transfer models on a re-orchestration task in terms of
generation quality and controllability. Moreover, it can be adapted for a lead
sheet orchestration task as a zero-shot learning model, achieving performance
comparable to a model specifically trained for this task.

</details>


### [589] [NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics](https://arxiv.org/pdf/2411.07186)
*David Robinson, Marius Miron, Masato Hagiwara, Benno Weck, Sara Keen, Milad Alizadeh, Gagan Narula, Matthieu Geist, Olivier Pietquin*

Main category: cs.SD

TL;DR: NatureLM-audio is the first audio-language model for bioacoustics, achieving state-of-the-art performance on tasks like zero-shot species classification and addressing data scarcity in the field.


<details>
  <summary>Details</summary>
Motivation: LLMs excel in auditory tasks but lack application in bioacoustics, which is vital for conservation and biodiversity.

Method: Trained on curated text-audio pairs (bioacoustics, speech, music) to transfer learned representations to bioacoustics.

Result: Sets new benchmarks on bioacoustics tasks, including zero-shot classification of unseen species.

Conclusion: NatureLM-audio advances bioacoustics research; model weights, benchmark data, and code are released for community use.

Abstract: Large language models (LLMs) prompted with text and audio have achieved
state-of-the-art performance across various auditory tasks, including speech,
music, and general audio, showing emergent abilities on unseen tasks. However,
their potential has yet to be fully demonstrated in bioacoustics tasks, such as
detecting animal vocalizations in large recordings, classifying rare and
endangered species, and labeling context and behavior -- tasks that are crucial
for conservation, biodiversity monitoring, and animal behavior studies. In this
work, we present NatureLM-audio, the first audio-language foundation model
specifically designed for bioacoustics. Our training dataset consists of
carefully curated text-audio pairs spanning bioacoustics, speech, and music,
designed to address the field's limited availability of annotated data. We
demonstrate successful transfer of learned representations from music and
speech to bioacoustics, and our model shows promising generalization to unseen
taxa and tasks. We evaluate NatureLM-audio on a novel benchmark (BEANS-Zero)
and it sets a new state of the art on several bioacoustics tasks, including
zero-shot classification of unseen species. To advance bioacoustics research,
we release our model weights, benchmark data, and open-source the code for
training and benchmark data generation and model training.

</details>


### [590] [FreeCodec: A disentangled neural speech codec with fewer tokens](https://arxiv.org/pdf/2412.01053)
*Youqiang Zheng, Weiping Tu, Yueteng Kang, Jie Chen, Yike Zhang, Li Xiao, Yuhong Yang, Long Ma*

Main category: cs.SD

TL;DR: FreeCodec is a neural speech codec that decomposes speech into timbre, prosody, and content components, achieving state-of-the-art performance in reconstruction and disentanglement.


<details>
  <summary>Details</summary>
Motivation: Existing neural speech codecs perform poorly with fewer tokens due to inefficient modeling of complex coupled information.

Method: FreeCodec decomposes speech into timbre (global vector), prosody (long stride encoder), and content (content encoder) components, using specialized training strategies.

Result: FreeCodec outperforms existing methods in both subjective and objective experiments.

Conclusion: FreeCodec provides an effective framework for speech coding and generative tasks, improving reconstruction and disentanglement.

Abstract: Neural speech codecs have gained great attention for their outstanding
reconstruction with discrete token representations.
  It is a crucial component in generative tasks such as speech coding and large
language models (LLM).
  However, most works based on residual vector quantization perform worse with
fewer tokens due to low coding efficiency for modeling complex coupled
information.
  In this paper, we propose a neural speech codec named FreeCodec which employs
a more effective encoding framework by decomposing intrinsic properties of
speech into different components:
  1) a global vector is extracted as the timbre information,
  2) a prosody encoder with a long stride level is used to model the prosody
information,
  3) the content information is from a content encoder.
  Using different training strategies, FreeCodec achieves state-of-the-art
performance in reconstruction and disentanglement scenarios.
  Results from subjective and objective experiments demonstrate that our
framework outperforms existing methods.

</details>


### [591] [Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech](https://arxiv.org/pdf/2505.20868)
*Nam-Gyu Kim, Deok-Hyeon Cho, Seung-Bin Kim, Seong-Whan Lee*

Main category: cs.SD

TL;DR: Spotlight-TTS improves expressive speech synthesis by focusing on voiced regions for style extraction and adjusting style direction for better integration.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with synthesizing high-quality expressive speech despite advances in style embedding from reference speech.

Method: Spotlight-TTS uses voiced-aware style extraction and style direction adjustment to enhance expressiveness and speech quality.

Result: The model outperforms baselines in expressiveness, speech quality, and style transfer, with publicly available audio samples.

Conclusion: Spotlight-TTS effectively addresses challenges in expressive TTS by refining style extraction and integration.

Abstract: Recent advances in expressive text-to-speech (TTS) have introduced diverse
methods based on style embedding extracted from reference speech. However,
synthesizing high-quality expressive speech remains challenging. We propose
Spotlight-TTS, which exclusively emphasizes style via voiced-aware style
extraction and style direction adjustment. Voiced-aware style extraction
focuses on voiced regions highly related to style while maintaining continuity
across different speech regions to improve expressiveness. We adjust the
direction of the extracted style for optimal integration into the TTS model,
which improves speech quality. Experimental results demonstrate that
Spotlight-TTS achieves superior performance compared to baseline models in
terms of expressiveness, overall speech quality, and style transfer capability.
Our audio samples are publicly available.

</details>


### [592] [AI-Generated Song Detection via Lyrics Transcripts](https://arxiv.org/pdf/2506.18488)
*Markus Frohmann, Elena V. Epure, Gabriel Meseguer-Brocal, Markus Schedl, Romain Hennequin*

Main category: cs.SD

TL;DR: Proposes using ASR models to transcribe and detect AI-generated music lyrics, outperforming audio-based methods in robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Address the gap in detecting AI-generated music when perfect lyrics are unavailable, improving real-world applicability.

Method: Uses ASR models (e.g., Whisper large-v2) to transcribe songs and employs detectors (e.g., LLM2Vec embeddings) for AI-generated content.

Result: Strong detection performance across languages and genres, with robustness to audio perturbations and unseen generators.

Conclusion: The method is more robust than audio-based detectors, offering practical solutions for real-life scenarios.

Abstract: The recent rise in capabilities of AI-based music generation tools has
created an upheaval in the music industry, necessitating the creation of
accurate methods to detect such AI-generated content. This can be done using
audio-based detectors; however, it has been shown that they struggle to
generalize to unseen generators or when the audio is perturbed. Furthermore,
recent work used accurate and cleanly formatted lyrics sourced from a lyrics
provider database to detect AI-generated music. However, in practice, such
perfect lyrics are not available (only the audio is); this leaves a substantial
gap in applicability in real-life use cases. In this work, we instead propose
solving this gap by transcribing songs using general automatic speech
recognition (ASR) models. We do this using several detectors. The results on
diverse, multi-genre, and multi-lingual lyrics show generally strong detection
performance across languages and genres, particularly for our best-performing
model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that
our method is more robust than state-of-the-art audio-based ones when the audio
is perturbed in different ways and when evaluated on different music
generators. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [593] [Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling](https://arxiv.org/pdf/2504.15071)
*Louis Bradshaw, Simon Colton*

Main category: cs.SD

TL;DR: A new dataset of over one million MIDI files, transcribed from piano performances, is introduced, with detailed analysis and metadata tags provided.


<details>
  <summary>Details</summary>
Motivation: To create a large-scale, high-quality MIDI dataset for piano performances by leveraging automated transcription and metadata scoring.

Method: A multi-stage pipeline involving a language model for crawling/scoring audio, pruning/segmentation via an audio classifier, and transcription into MIDI files.

Result: Over one million MIDI files (100,000 hours of transcribed audio) with metadata and statistical insights.

Conclusion: The dataset is a valuable resource for music research, with tools and metadata provided for further analysis.

Abstract: We introduce an extensive new dataset of MIDI files, created by transcribing
audio recordings of piano performances into their constituent notes. The data
pipeline we use is multi-stage, employing a language model to autonomously
crawl and score audio recordings from the internet based on their metadata,
followed by a stage of pruning and segmentation using an audio classifier. The
resulting dataset contains over one million distinct MIDI files, comprising
roughly 100,000 hours of transcribed audio. We provide an in-depth analysis of
our techniques, offering statistical insights, and investigate the content by
extracting metadata tags, which we also provide. Dataset available at
https://github.com/loubbrad/aria-midi.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [594] [Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation](https://arxiv.org/pdf/2506.22441)
*Lei Yang*

Main category: cs.LG

TL;DR: The paper proposes a TDW loss-incorporated Latent Factorization of Tensors (TDWLFT) model to improve spatiotemporal traffic data imputation by reducing sensitivity to outliers.


<details>
  <summary>Details</summary>
Motivation: Incomplete or corrupted traffic data due to communication failures and sensor malfunctions hinder ITS performance. Existing LFT models are vulnerable to outliers.

Method: Introduces a threshold distance weighted (TDW) loss function in the LFT model to assign differentiated weights to samples, reducing outlier impact.

Result: TDWLFT outperforms state-of-the-art methods in prediction accuracy and computational efficiency on two urban traffic speed datasets.

Conclusion: The TDWLFT model enhances data imputation robustness and efficiency, benefiting ITS applications.

Abstract: Intelligent transportation systems (ITS) rely heavily on complete and
high-quality spatiotemporal traffic data to achieve optimal performance.
Nevertheless, in real-word traffic data collection processes, issues such as
communication failures and sensor malfunctions often lead to incomplete or
corrupted datasets, thereby posing significant challenges to the advancement of
ITS. Among various methods for imputing missing spatiotemporal traffic data,
the latent factorization of tensors (LFT) model has emerged as a widely adopted
and effective solution. However, conventional LFT models typically employ the
standard L2-norm in their learning objective, which makes them vulnerable to
the influence of outliers. To overcome this limitation, this paper proposes a
threshold distance weighted (TDW) loss-incorporated Latent Factorization of
Tensors (TDWLFT) model. The proposed loss function effectively reduces the
model's sensitivity to outliers by assigning differentiated weights to
individual samples. Extensive experiments conducted on two traffic speed
datasets sourced from diverse urban environments confirm that the proposed
TDWLFT model consistently outperforms state-of-the-art approaches in terms of
both in both prediction accuracy and computational efficiency.

</details>


### [595] [Features-based embedding or Feature-grounding](https://arxiv.org/pdf/2506.22442)
*Piotr Makarevich*

Main category: cs.LG

TL;DR: The paper explores reproducing knowledge-based structured thinking in deep learning models using feature-grounded embeddings.


<details>
  <summary>Details</summary>
Motivation: To align deep learning models with human-like reasoning by grounding embeddings in interpretable, domain-specific conceptual features.

Method: Introduces a feature-grounded embedding approach to create shareable representations tied to operable dictionaries and conceptual features.

Result: Not explicitly stated in the abstract, but the method aims to improve model interpretability and alignment with human reasoning.

Conclusion: The proposed approach could enhance deep learning models by incorporating structured, knowledge-based representations.

Abstract: In everyday reasoning, when we think about a particular object, we associate
it with a unique set of expected properties such as weight, size, or more
abstract attributes like density or horsepower. These expectations are shaped
by our prior knowledge and the conceptual categories we have formed through
experience. This paper investigates how such knowledge-based structured
thinking can be reproduced in deep learning models using features based
embeddings. Specially, it introduces an specific approach to build
feature-grounded embedding, aiming to align shareable representations of
operable dictionary with interpretable domain-specific conceptual features.

</details>


### [596] [Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security](https://arxiv.org/pdf/2506.22445)
*Saad Alqithami*

Main category: cs.LG

TL;DR: A novel Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework is introduced to enhance CPS security against adaptive cyber threats, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Increasing connectivity of Cyber-Physical Systems (CPS) makes them vulnerable to sophisticated cyber threats, which traditional security methods fail to address.

Method: HAMARL uses a hierarchical structure with local agents for subsystem security and a global coordinator for system-wide defense, incorporating adversarial training to anticipate threats.

Result: Experiments on an industrial IoT testbed show HAMARL improves attack detection accuracy, reduces response times, and ensures operational continuity.

Conclusion: Combining hierarchical multi-agent coordination with adversarial training enhances CPS resilience and security.

Abstract: Cyber-Physical Systems play a critical role in the infrastructure of various
sectors, including manufacturing, energy distribution, and autonomous
transportation systems. However, their increasing connectivity renders them
highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day
attacks, against which traditional security methods like rule-based intrusion
detection and single-agent reinforcement learning prove insufficient. To
overcome these challenges, this paper introduces a novel Hierarchical
Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.
HAMARL employs a hierarchical structure consisting of local agents dedicated to
subsystem security and a global coordinator that oversees and optimizes
comprehensive, system-wide defense strategies. Furthermore, the framework
incorporates an adversarial training loop designed to simulate and anticipate
evolving cyber threats, enabling proactive defense adaptation. Extensive
experimental evaluations conducted on a simulated industrial IoT testbed
indicate that HAMARL substantially outperforms traditional multi-agent
reinforcement learning approaches, significantly improving attack detection
accuracy, reducing response times, and ensuring operational continuity. The
results underscore the effectiveness of combining hierarchical multi-agent
coordination with adversarially-aware training to enhance the resilience and
security of next-generation CPS.

</details>


### [597] [Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition](https://arxiv.org/pdf/2506.22443)
*Sarah Seifi, Tobias Sukianto, Cecilia Carbonelli, Lorenzo Servadei, Robert Wille*

Main category: cs.LG

TL;DR: RL-Net, a neuro-symbolic rule learning neural network, balances interpretability and performance in radar-based hand gesture recognition, outperforming transparent and black-box models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretable rule-based models and high-performing deep neural networks for hand gesture recognition.

Method: RL-Net learns interpretable rule lists via neural optimization, benchmarked against MIRA (rule-based) and XentricAI (explainable black-box).

Result: RL-Net achieves 93.03% F1 score, reduces rule complexity, and addresses optimization challenges like rule pruning and hierarchy bias.

Conclusion: RL-Net is a practical middle ground for interpretable HGR, with potential for edge-deployable sensing systems.

Abstract: Rule-based models offer interpretability but struggle with complex data,
while deep neural networks excel in performance yet lack transparency. This
work investigates a neuro-symbolic rule learning neural network named RL-Net
that learns interpretable rule lists through neural optimization, applied for
the first time to radar-based hand gesture recognition (HGR). We benchmark
RL-Net against a fully transparent rule-based system (MIRA) and an explainable
black-box model (XentricAI), evaluating accuracy, interpretability, and user
adaptability via transfer learning. Our results show that RL-Net achieves a
favorable trade-off, maintaining strong performance (93.03% F1) while
significantly reducing rule complexity. We identify optimization challenges
specific to rule pruning and hierarchy bias and propose stability-enhancing
modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical
middle ground between transparency and performance. This study highlights the
real-world feasibility of neuro-symbolic models for interpretable HGR and
offers insights for extending explainable AI to edge-deployable sensing
systems.

</details>


### [598] [P$^2$U: Progressive Precision Update For Efficient Model Distribution](https://arxiv.org/pdf/2506.22871)
*Homayun Afrabandpey, Hamed Rezazadegan Tavakoli*

Main category: cs.LG

TL;DR: Progressive Precision Update (P²U) improves model distribution efficiency by transmitting low-bit precision models with updates, balancing accuracy, bandwidth, and latency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of efficient model distribution in bandwidth-constrained environments like federated learning and edge computing.

Method: P²U transmits low-bit precision models alongside updates representing the difference to the high-precision version, tested across various architectures and datasets.

Result: P²U achieves better tradeoffs in accuracy, bandwidth, and latency, even with aggressive quantization (e.g., 4-bit).

Conclusion: P²U is a practical solution for scalable model distribution in low-resource settings, compatible with existing compression techniques.

Abstract: Efficient model distribution is becoming increasingly critical in
bandwidth-constrained environments. In this paper, we propose a simple yet
effective approach called Progressive Precision Update (P$^2$U) to address this
problem. Instead of transmitting the original high-precision model, P$^2$U
transmits a lower-bit precision model, coupled with a model update representing
the difference between the original high-precision model and the transmitted
low precision version. With extensive experiments on various model
architectures, ranging from small models ($1 - 6$ million parameters) to a
large model (more than $100$ million parameters) and using three different data
sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U
consistently achieves better tradeoff between accuracy, bandwidth usage and
latency. Moreover, we show that when bandwidth or startup time is the priority,
aggressive quantization (e.g., 4-bit) can be used without severely compromising
performance. These results establish P$^2$U as an effective and practical
solution for scalable and efficient model distribution in low-resource
settings, including federated learning, edge computing, and IoT deployments.
Given that P$^2$U complements existing compression techniques and can be
implemented alongside any compression method, e.g., sparsification,
quantization, pruning, etc., the potential for improvement is even greater.

</details>


### [599] [Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2](https://arxiv.org/pdf/2506.22444)
*Jing Wang, Amar Sra, Jeremy C. Weiss*

Main category: cs.LG

TL;DR: The study introduces a cohort of 18 PASC patients with text time series features and proposes an Active Attention Network to predict clinical risk and progression events, aiming to improve patient care.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of PASC progression events is challenging with traditional models, necessitating better methods for patient management.

Method: Uses a cohort with text time series features from Llama-3.1-70B-Instruct and clinical expert annotations, proposing an Active Attention Network for risk prediction and event identification.

Result: Aims to enhance clinical risk prediction accuracy and reduce annotation needs by integrating human expertise with active learning.

Conclusion: The approach seeks to improve patient care and decision-making for SARS-CoV-2 patients by addressing PASC progression challenges.

Abstract: The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,
pose a significant challenge to healthcare systems worldwide. Accurate
identification of progression events, such as hospitalization and reinfection,
is essential for effective patient management and resource allocation. However,
traditional models trained on structured data struggle to capture the nuanced
progression of PASC. In this study, we introduce the first publicly available
cohort of 18 PASC patients, with text time series features based on Large
Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical
expert. We propose an Active Attention Network to predict the clinical risk and
identify progression events related to the risk. By integrating human expertise
with active learning, we aim to enhance clinical risk prediction accuracy and
enable progression events identification with fewer number of annotation. The
ultimate goal is to improves patient care and decision-making for SARS-CoV-2
patient.

</details>


### [600] [EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis](https://arxiv.org/pdf/2506.22446)
*Aakash Tripathi, Asim Waqas, Matthew B. Schabath, Yasin Yilmaz, Ghulam Rasool*

Main category: cs.LG

TL;DR: EAGLE is a deep learning framework for cancer survival prediction, using attention-based multimodal fusion and interpretability methods to improve accuracy and clinical adoption.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal approaches for cancer survival prediction lack interpretability, efficiency, and effective fusion strategies, hindering clinical use.

Method: EAGLE employs dynamic cross-modal attention, dimensionality reduction, three attribution methods, and a unified pipeline for adaptability across cancer types.

Result: EAGLE achieved high-risk stratification with 4-5 fold survival differences, showing reliance on imaging features for high-risk patients and balanced modality use for low-risk ones.

Conclusion: EAGLE bridges AI and healthcare by offering scalable, interpretable, and accurate multimodal survival prediction, enhancing physician trust.

Abstract: Accurate cancer survival prediction requires integration of diverse data
modalities that reflect the complex interplay between imaging, clinical
parameters, and textual reports. However, existing multimodal approaches suffer
from simplistic fusion strategies, massive computational requirements, and lack
of interpretability-critical barriers to clinical adoption. We present EAGLE
(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning
framework that addresses these limitations through attention-based multimodal
fusion with comprehensive attribution analysis. EAGLE introduces four key
innovations: (1) dynamic cross-modal attention mechanisms that learn
hierarchical relationships between modalities, (2) massive dimensionality
reduction (99.96%) while maintaining predictive performance, (3) three
complementary attribution methods providing patient-level interpretability, and
(4) a unified pipeline enabling seamless adaptation across cancer types. We
evaluated EAGLE on 911 patients across three distinct malignancies:
glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,
n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis
showed high-risk individuals relied more heavily on adverse imaging features,
while low-risk patients demonstrated balanced modality contributions. Risk
stratification identified clinically meaningful groups with 4-fold (GBM) to
5-fold (NSCLC) differences in median survival, directly informing treatment
intensity decisions. By combining state-of-the-art performance with clinical
interpretability, EAGLE bridges the gap between advanced AI capabilities and
practical healthcare deployment, offering a scalable solution for multimodal
survival prediction that enhances both prognostic accuracy and physician trust
in automated predictions.

</details>


### [601] [Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture](https://arxiv.org/pdf/2506.22447)
*Fabio Merizzi, Harilaos Loukos*

Main category: cs.LG

TL;DR: A multi-task, multi-variable Vision Transformer (ViT) architecture (1EMD) is proposed for efficient climate downscaling, outperforming single-variable models by enabling cross-variable knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Existing single-variable deep learning models for climate downscaling lack contextual awareness and cross-variable interaction, leading to inefficiencies.

Method: A shared encoder and variable-specific decoders (1EMD) jointly predict three climate variables (tas, sfcWind, zg500) from GCM-resolution inputs.

Result: The multi-variable approach outperforms single-variable baselines, improves computational efficiency, and enables cross-variable knowledge transfer.

Conclusion: Multi-variable modeling is effective for high-resolution climate downscaling, offering better performance and efficiency.

Abstract: Global Climate Models (GCMs) are critical for simulating large-scale climate
dynamics, but their coarse spatial resolution limits their applicability in
regional studies. Regional Climate Models (RCMs) refine this through dynamic
downscaling, albeit at considerable computational cost and with limited
flexibility. While deep learning has emerged as an efficient data-driven
alternative, most existing studies have focused on single-variable models that
downscale one variable at a time. This approach can lead to limited contextual
awareness, redundant computation, and lack of cross-variable interaction. Our
study addresses these limitations by proposing a multi-task, multi-variable
Vision Transformer (ViT) architecture with a shared encoder and
variable-specific decoders (1EMD). The proposed architecture jointly predicts
three key climate variables: surface temperature (tas), wind speed (sfcWind),
and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,
emulating RCM-scale downscaling over Europe. We show that our multi-variable
approach achieves positive cross-variable knowledge transfer and consistently
outperforms single-variable baselines trained under identical conditions, while
also improving computational efficiency. These results demonstrate the
effectiveness of multi-variable modeling for high-resolution climate
downscaling.

</details>


### [602] [Stabilization of industrial processes with time series machine learning](https://arxiv.org/pdf/2506.22502)
*Matvei Anoshin, Olga Tsurkan, Vadim Lopatkin, Leonid Fedichkin*

Main category: cs.LG

TL;DR: A two-neural-network pipeline improves time series stabilization, outperforming traditional solvers by 3x in temperature control.


<details>
  <summary>Details</summary>
Motivation: Stabilizing time series processes is critical in industries, and machine learning can enhance efficiency and quality.

Method: A pipeline with an oracle predictor and optimizer replaces point-wise optimization with neural network training.

Result: Achieves 3 times better stability in temperature control compared to ordinary solvers.

Conclusion: The proposed neural network approach effectively stabilizes time series processes with superior performance.

Abstract: The stabilization of time series processes is a crucial problem that is
ubiquitous in various industrial fields. The application of machine learning to
its solution can have a decisive impact, improving both the quality of the
resulting stabilization with less computational resources required. In this
work, we present a simple pipeline consisting of two neural networks: the
oracle predictor and the optimizer, proposing a substitution of the point-wise
values optimization to the problem of the neural network training, which
successfully improves stability in terms of the temperature control by about 3
times compared to ordinary solvers.

</details>


### [603] [Task-Agnostic Contrastive Pretraining for Relational Deep Learning](https://arxiv.org/pdf/2506.22530)
*Jakub Peleška, Gustav Šír*

Main category: cs.LG

TL;DR: The paper proposes a task-agnostic contrastive pretraining approach for Relational Deep Learning (RDL) to learn transferable representations from relational databases, outperforming task-specific supervised learning.


<details>
  <summary>Details</summary>
Motivation: Existing RDL models require task-specific training, limiting scalability and reuse. The paper aims to enable database-wide representation learning.

Method: Introduces three contrastive objectives (row-level, link-level, context-level) and a modular RDL architecture with efficient sampling for heterogeneous databases.

Result: Preliminary results show fine-tuning pretrained models outperforms training from scratch.

Conclusion: The proposed methodology effectively learns transferable representations for relational data, promising improved scalability and reuse.

Abstract: Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph
Neural Network principles to learn directly from relational databases by
representing them as heterogeneous graphs. However, existing RDL models
typically rely on task-specific supervised learning, requiring training
separate models for each predictive task, which may hamper scalability and
reuse.
  In this work, we propose a novel task-agnostic contrastive pretraining
approach for RDL that enables database-wide representation learning. For that
aim, we introduce three levels of contrastive objectives$-$row-level,
link-level, and context-level$-$designed to capture the structural and semantic
heterogeneity inherent to relational data. We implement the respective
pretraining approach through a modular RDL architecture and an efficient
sampling strategy tailored to the heterogeneous database setting. Our
preliminary results on standard RDL benchmarks demonstrate that fine-tuning the
pretrained models measurably outperforms training from scratch, validating the
promise of the proposed methodology in learning transferable representations
for relational data.

</details>


### [604] [Exploration Behavior of Untrained Policies](https://arxiv.org/pdf/2506.22566)
*Jacob Adamczyk*

Main category: cs.LG

TL;DR: The paper explores how untrained deep neural policies influence exploration in RL, showing they produce non-trivial state-visitation distributions and correlated actions, with implications for early training behavior.


<details>
  <summary>Details</summary>
Motivation: Understanding how policy architectures implicitly shape exploration in RL, especially in sparse or adversarial reward settings.

Method: Theoretical and empirical analysis of untrained policies using infinite-width networks and continuous-time limits, focusing on trajectory distributions.

Result: Untrained policies generate correlated actions and non-trivial state-visitation distributions, revealing architectural inductive biases for exploration.

Conclusion: The work provides a framework for using policy initialization to study and design exploration behavior in early RL training.

Abstract: Exploration remains a fundamental challenge in reinforcement learning (RL),
particularly in environments with sparse or adversarial reward structures. In
this work, we study how the architecture of deep neural policies implicitly
shapes exploration before training. We theoretically and empirically
demonstrate strategies for generating ballistic or diffusive trajectories from
untrained policies in a toy model. Using the theory of infinite-width networks
and a continuous-time limit, we show that untrained policies return correlated
actions and result in non-trivial state-visitation distributions. We discuss
the distributions of the corresponding trajectories for a standard
architecture, revealing insights into inductive biases for tackling
exploration. Our results establish a theoretical and experimental framework for
using policy initialization as a design tool to understand exploration behavior
in early training.

</details>


### [605] [The Hidden Link Between RLHF and Contrastive Learning](https://arxiv.org/pdf/2506.22578)
*Xufei Lv, Haoyuan Sun, Xuefeng Bai, Min Zhang, Houde Liu, Kehai Chen*

Main category: cs.LG

TL;DR: The paper interprets RLHF and DPO as mutual information maximization methods, linking them to contrastive learning, and proposes MIO to improve performance.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the alignment of LLMs with human values by reinterpreting existing methods (RLHF, DPO) through mutual information maximization.

Method: Replaces the DV/MINE bound with the Jensen-Shannon MI estimator, proposing Mutual Information Optimization (MIO).

Result: MIO mitigates late-stage decline in chosen-likelihood, outperforming DPO in reasoning and mathematical benchmarks.

Conclusion: MIO offers a competitive alternative to RLHF and DPO, enhancing LLM alignment and reasoning capabilities.

Abstract: Alignment of large language models (LLMs) with human values has recently
garnered significant attention, with prominent examples including the canonical
yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple
Direct Preference Optimization (DPO). In this work, we demonstrate that both
RLHF and DPO can be interpreted from the perspective of mutual information (MI)
maximization, uncovering a profound connection to contrastive learning. Within
this framework, both RLHF and DPO can be viewed as methods that perform
contrastive learning based on the positive and negative samples derived from
the base model, leveraging the Donsker-Varadhan (DV) lower bound on MI
(equivalently, the MINE estimator). This paradigm further explains why RLHF may
not intrinsically incentivize reasoning capacities in LLMs beyond what is
already present in the base model. Building on this perspective, we replace the
DV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual
Information Optimization (MIO). Comprehensive theoretical analysis and
extensive empirical evaluations demonstrate that MIO mitigates the late-stage
decline in chosen-likelihood observed in DPO, achieving competitive or superior
performance across various challenging reasoning and mathematical benchmarks.
We will release the model and code upon acceptance.

</details>


### [606] [Are Fast Methods Stable in Adversarially Robust Transfer Learning?](https://arxiv.org/pdf/2506.22602)
*Joshua C. Zhao, Saurabh Bagchi*

Main category: cs.LG

TL;DR: FGSM in adversarial fine-tuning is stable and efficient, outperforming PGD in computational cost with minimal robustness loss.


<details>
  <summary>Details</summary>
Motivation: Reduce computational cost of adversarial fine-tuning in transfer learning while maintaining robustness.

Method: Revisit FGSM for adversarial fine-tuning, testing stability and performance across datasets.

Result: FGSM is stable, avoids catastrophic overfitting, and loses only 0.39%-1.39% robustness vs. PGD, with 4x faster training.

Conclusion: FGSM is a highly efficient and effective alternative to PGD in robust transfer learning.

Abstract: Transfer learning is often used to decrease the computational cost of model
training, as fine-tuning a model allows a downstream task to leverage the
features learned from the pre-training dataset and quickly adapt them to a new
task. This is particularly useful for achieving adversarial robustness, as
adversarially training models from scratch is very computationally expensive.
However, high robustness in transfer learning still requires adversarial
training during the fine-tuning phase, which requires up to an order of
magnitude more time than standard fine-tuning. In this work, we revisit the use
of the fast gradient sign method (FGSM) in robust transfer learning to improve
the computational cost of adversarial fine-tuning. We surprisingly find that
FGSM is much more stable in adversarial fine-tuning than when training from
scratch. In particular, FGSM fine-tuning does not suffer from any issues with
catastrophic overfitting at standard perturbation budgets of $\varepsilon=4$ or
$\varepsilon=8$. This stability is further enhanced with parameter-efficient
fine-tuning methods, where FGSM remains stable even up to $\varepsilon=32$ for
linear probing. We demonstrate how this stability translates into performance
across multiple datasets. Compared to fine-tuning with the more commonly used
method of projected gradient descent (PGD), on average, FGSM only loses 0.39%
and 1.39% test robustness for $\varepsilon=4$ and $\varepsilon=8$ while using
$4\times$ less training time. Surprisingly, FGSM may not only be a
significantly more efficient alternative to PGD in adversarially robust
transfer learning but also a well-performing one.

</details>


### [607] [Hierarchical Modeling and Architecture Optimization: Review and Unified Framework](https://arxiv.org/pdf/2506.22621)
*Paul Saves, Edward Hallé-Hannan, Jasper Bussemaker, Youssef Diouane, Nathalie Bartoli*

Main category: cs.LG

TL;DR: The paper proposes a unified framework for handling mixed-variable inputs in simulation-based problems, addressing hierarchical, conditional, and heterogeneous domains. It introduces meta and partially-decreed variables, design space graphs, and integrates surrogate modeling and optimization techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges in representing, modeling, and optimizing mixed-variable inputs in hierarchical, conditional, or tree-structured domains.

Method: Introduces a framework with meta and partially-decreed variables, design space graphs, and hierarchical kernels/distances. Implemented in SMT 2.0 for surrogate modeling and Bayesian optimization.

Result: Demonstrates effectiveness through applications, including a case study in green aircraft architecture.

Conclusion: The framework generalizes existing approaches, enabling efficient modeling and optimization of complex hierarchical domains.

Abstract: Simulation-based problems involving mixed-variable inputs frequently feature
domains that are hierarchical, conditional, heterogeneous, or tree-structured.
These characteristics pose challenges for data representation, modeling, and
optimization. This paper reviews extensive literature on these structured input
spaces and proposes a unified framework that generalizes existing approaches.
In this framework, input variables may be continuous, integer, or categorical.
A variable is described as meta if its value governs the presence of other
decreed variables, enabling the modeling of conditional and hierarchical
structures.
  We further introduce the concept of partially-decreed variables, whose
activation depends on contextual conditions. To capture these inter-variable
hierarchical relationships, we introduce design space graphs, combining
principles from feature modeling and graph theory. This allows the definition
of general hierarchical domains suitable for describing complex system
architectures. The framework supports the use of surrogate models over such
domains and integrates hierarchical kernels and distances for efficient
modeling and optimization. The proposed methods are implemented in the
open-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are
demonstrated through applications in Bayesian optimization for complex system
design, including a case study in green aircraft architecture.

</details>


### [608] [A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS](https://arxiv.org/pdf/2506.22631)
*Dmitry B. Rokhlin*

Main category: cs.LG

TL;DR: The paper proposes H-VAW-D, a hierarchical algorithm for online regression with RKHS, achieving optimal dynamic regret with adaptive learning of discount factors and random features.


<details>
  <summary>Details</summary>
Motivation: To extend the DVAW forecaster to non-parametric domains and improve dynamic regret bounds in online regression.

Method: Combines DVAW with random feature approximation, introducing H-VAW-D, which adaptively learns discount factors and feature counts.

Result: Achieves $O(T^{2/3}P_T^{1/3} + \sqrt{T}\ln T)$ expected dynamic regret with $O(T\ln T)$ per-iteration complexity.

Conclusion: H-VAW-D effectively addresses non-parametric online regression, balancing computational efficiency and regret performance.

Abstract: We study the problem of online regression with the unconstrained quadratic
loss against a time-varying sequence of functions from a Reproducing Kernel
Hilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a
discounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic
regret in the finite-dimensional case. In this work, we lift their approach to
the non-parametric domain by synthesizing the DVAW framework with a random
feature approximation. We propose a fully adaptive, hierarchical algorithm,
which we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that
learns both the discount factor and the number of random features. We prove
that this algorithm, which has a per-iteration computational complexity of
$O(T\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +
\sqrt{T}\ln T)$, where $P_T$ is the functional path length of a comparator
sequence.

</details>


### [609] [Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training](https://arxiv.org/pdf/2506.22638)
*Aadim Nepal, Safal Shrestha, Anubhav Shrestha, Minwu Kim, Keith Ross*

Main category: cs.LG

TL;DR: Post-training methods improve math reasoning in language models, but the layer importance structure remains unchanged. Critical layers for math tasks cause major accuracy drops if removed, unlike non-math tasks.


<details>
  <summary>Details</summary>
Motivation: To understand if post-training improvements in math reasoning stem from major layer changes or minor adjustments in layer importance.

Method: Layer-wise ablation experiments on base and post-trained models (instruction-tuned, knowledge-distilled, reinforcement learning) across math reasoning benchmarks.

Result: Math reasoning relies on specific, persistent layer structures; removing critical layers drops accuracy by up to 80%. Non-math tasks lack such critical layers.

Conclusion: Math reasoning requires specialized pre-trained layers, distinct from non-reasoning tasks, with critical layers linked to major representational transformations.

Abstract: Large language models can exhibit improved mathematical reasoning
capabilities following post-training with instruction tuning, reinforcement
learning, or knowledge distillation. However, it remains unclear whether these
improvements are driven by major changes in transformer layers or from minor
adjustments that leave the relative layer importance structures of the base
model largely unchanged. We investigate this question through systematic
layer-wise ablation experiments, examining base, instruction-tuned,
knowledge-distilled, and reinforcement learning variants on mathematical
reasoning benchmarks. Our findings show that mathematical reasoning gives rise
to a specific layer importance structure, and this structure persists across
all post-training paradigms. Removal of such layers causes accuracy drops of up
to 80%. In contrast, non-mathematical tasks like factual recall exhibit no
critical layers. This distinction suggests that mathematical reasoning requires
specialized layers that emerge during pre-training, while other non-reasoning
tasks do not. From an information-theoretic perspective, we also observe that
these critical layers are the same layers where major representational
transformation occurs.

</details>


### [610] [Cost-effective Reduced-Order Modeling via Bayesian Active Learning](https://arxiv.org/pdf/2506.22645)
*Amir Hossein Rahmati, Nathan M. Urban, Byung-Jun Yoon, Xiaoning Qian*

Main category: cs.LG

TL;DR: BayPOD-AL is an active learning framework using Bayesian POD to efficiently learn reduced-order models from high-fidelity data, reducing computational costs and improving generalizability.


<details>
  <summary>Details</summary>
Motivation: Machine Learning surrogates require large datasets, limiting real-world applicability. BayPOD-AL addresses this by optimizing data collection.

Method: BayPOD-AL combines Bayesian POD with active learning to select informative data points, reducing training dataset size.

Result: BayPOD-AL outperforms other methods in predicting temperature evolution and generalizes well to higher temporal resolution data.

Conclusion: BayPOD-AL effectively reduces computational costs and improves model accuracy, making it suitable for complex systems.

Abstract: Machine Learning surrogates have been developed to accelerate solving systems
dynamics of complex processes in different science and engineering
applications. To faithfully capture governing systems dynamics, these methods
rely on large training datasets, hence restricting their applicability in
real-world problems. In this work, we propose BayPOD-AL, an active learning
framework based on an uncertainty-aware Bayesian proper orthogonal
decomposition (POD) approach, which aims to effectively learn reduced-order
models from high-fidelity full-order models representing complex systems.
Experimental results on predicting the temperature evolution over a rod
demonstrate BayPOD-AL's effectiveness in suggesting the informative data and
reducing computational cost related to constructing a training dataset compared
to other uncertainty-guided active learning strategies. Furthermore, we
demonstrate BayPOD-AL's generalizability and efficiency by evaluating its
performance on a dataset of higher temporal resolution than the training
dataset.

</details>


### [611] [Learning Stochastic Multiscale Models](https://arxiv.org/pdf/2506.22655)
*Andrew F. Ilersich, Prasanth B. Nair*

Main category: cs.LG

TL;DR: The paper proposes a data-driven method to learn stochastic multiscale models using stochastic differential equations, improving predictive accuracy over traditional simulations.


<details>
  <summary>Details</summary>
Motivation: Dynamical systems in physical sciences often involve multiple scales, making direct numerical simulation computationally expensive. A more efficient approach is needed.

Method: The method learns stochastic multiscale models from data, using a coarse mesh and an auxiliary state for unresolved scales. Parameters are learned via forward-solver-free variational inference.

Result: The learned models outperform direct numerical simulation and closure-type models in predictive accuracy at equivalent resolution.

Conclusion: The approach offers a computationally efficient and accurate alternative for modeling multiscale dynamical systems.

Abstract: The physical sciences are replete with dynamical systems that require the
resolution of a wide range of length and time scales. This presents significant
computational challenges since direct numerical simulation requires
discretization at the finest relevant scales, leading to a high-dimensional
state space. In this work, we propose an approach to learn stochastic
multiscale models in the form of stochastic differential equations directly
from observational data. Our method resolves the state on a coarse mesh while
introducing an auxiliary state to capture the effects of unresolved scales. We
learn the parameters of the multiscale model using a modern forward-solver-free
amortized variational inference method. Our approach draws inspiration from
physics-based multiscale modeling approaches, such as large-eddy simulation in
fluid dynamics, while learning directly from data. We present numerical studies
to demonstrate that our learned multiscale models achieve superior predictive
accuracy compared to direct numerical simulation and closure-type models at
equivalent resolution.

</details>


### [612] [DistShap: Scalable GNN Explanations with Distributed Shapley Values](https://arxiv.org/pdf/2506.22668)
*Selahattin Akkas, Aditya Devarakonda, Ariful Azad*

Main category: cs.LG

TL;DR: DistShap is a parallel algorithm for efficiently explaining GNN predictions by distributing Shapley value computations across multiple GPUs, scaling to models with millions of features.


<details>
  <summary>Details</summary>
Motivation: Explaining GNN predictions is computationally expensive, especially for large graphs with many edges or features.

Method: DistShap distributes Shapley value-based explanations by sampling subgraphs, parallel GNN inference, and solving a distributed least squares problem.

Result: DistShap outperforms existing methods in accuracy and scales to GNNs with millions of features using up to 128 GPUs.

Conclusion: DistShap provides a scalable and accurate solution for explaining GNN predictions, addressing computational challenges in large-scale graphs.

Abstract: With the growing adoption of graph neural networks (GNNs), explaining their
predictions has become increasingly important. However, attributing predictions
to specific edges or features remains computationally expensive. For example,
classifying a node with 100 neighbors using a 3-layer GNN may involve
identifying important edges from millions of candidates contributing to the
prediction. To address this challenge, we propose DistShap, a parallel
algorithm that distributes Shapley value-based explanations across multiple
GPUs. DistShap operates by sampling subgraphs in a distributed setting,
executing GNN inference in parallel across GPUs, and solving a distributed
least squares problem to compute edge importance scores. DistShap outperforms
most existing GNN explanation methods in accuracy and is the first to scale to
GNN models with millions of features by using up to 128 GPUs on the NERSC
Perlmutter supercomputer.

</details>


### [613] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/pdf/2506.22685)
*Anh Bui, Trang Vu, Trung Le, Junae Kim, Tamas Abraham, Rollin Omari, Amar Kaur, Dinh Phung*

Main category: cs.LG

TL;DR: The paper addresses semantic collapsing in generative personalization, where learned visual concepts drift from their textual meanings, simplifying outputs. A training-free method adjusts embeddings to mitigate this.


<details>
  <summary>Details</summary>
Motivation: Semantic collapsing reduces the richness of multi-concept prompts, leading to oversimplified outputs. The goal is to preserve semantic alignment.

Method: A training-free approach adjusts the magnitude and direction of pre-trained embeddings during inference to prevent drift.

Result: The method improves text-image alignment across various personalization techniques.

Conclusion: The proposed embedding adjustment effectively mitigates semantic collapsing, enhancing output quality without additional training.

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [614] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/pdf/2506.22696)
*Brian Mak, Jeffrey Flanigan*

Main category: cs.LG

TL;DR: The paper introduces the Residual Matrix Transformer (RMT), replacing the transformer's residual stream with an outer product memory matrix, achieving better efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and performance of transformers by modifying the residual stream mechanism for storing and retrieving information.

Method: Replaces the transformer's residual stream with an outer product memory matrix, creating the RMT model.

Result: RMT scales residual stream independently, reduces FLOPS (58%), parameters (25%), and training tokens (41%), and outperforms transformers in downstream tasks.

Conclusion: RMT offers efficient scaling and improved performance, making it a promising alternative to traditional transformers.

Abstract: The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [615] [FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets](https://arxiv.org/pdf/2506.22708)
*Shrenik Jadhav, Birva Sevak, Srijita Das, Akhtar Hussain, Wencong Su, Van-Hai Bui*

Main category: cs.LG

TL;DR: FairMarket-RL combines LLMs and RL to ensure fairness in P2P trading, achieving equitable outcomes with high fairness scores and demand fulfillment.


<details>
  <summary>Details</summary>
Motivation: Existing P2P trading lacks robust fairness frameworks, necessitating a scalable, adaptive solution.

Method: Uses LLMs as fairness critics with FTB and FBS metrics, integrating scores into RL rewards via IPPO training.

Result: Achieves >90% buyer demand fulfillment, fair seller margins, and FTB/FBS scores >0.80, improving convergence and equity.

Conclusion: FairMarket-RL provides a scalable, fairness-driven solution for decentralized energy trading.

Abstract: Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for
decentralized market regulation, yet existing approaches often lack robust
frameworks to ensure fairness. This paper presents FairMarket-RL, a novel
hybrid framework that combines Large Language Models (LLMs) with Reinforcement
Learning (RL) to enable fairness-aware trading agents. In a simulated P2P
microgrid with multiple sellers and buyers, the LLM acts as a real-time
fairness critic, evaluating each trading episode using two metrics:
Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness
scores are integrated into agent rewards through scheduled
{\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that
replaces brittle, rule-based fairness constraints. Agents are trained using
Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,
fulfilling over 90% of buyer demand, maintaining fair seller margins, and
consistently reaching FTB and FBS scores above 0.80. The training process
demonstrates that fairness feedback improves convergence, reduces buyer
shortfalls, and narrows profit disparities between sellers. With its
language-based critic, the framework scales naturally, and its extension to a
large power distribution system with household prosumers illustrates its
practical applicability. FairMarket-RL thus offers a scalable, equity-driven
solution for autonomous trading in decentralized energy systems.

</details>


### [616] [Generalized Linear Mode Connectivity for Transformers](https://arxiv.org/pdf/2506.22712)
*Alexander Theus, Alessandro Cabodi, Sotiris Anagnostidis, Antonio Orvieto, Sidak Pal Singh, Valentina Boeva*

Main category: cs.LG

TL;DR: The paper introduces a unified framework to analyze symmetries in neural network loss landscapes, enabling discovery of low- and zero-loss paths between models like Vision Transformers and GPT-2.


<details>
  <summary>Details</summary>
Motivation: Understanding the geometry of neural network loss landscapes, particularly linear mode connectivity (LMC), is crucial for generalization and optimization. Prior work focused on limited symmetries like permutations, missing richer symmetries in modern architectures.

Method: The authors propose a framework capturing four symmetry classes: permutations, semi-permutations, orthogonal transformations, and general invertible maps. This broadens reparameterization options and subsumes prior approaches.

Result: The framework successfully identifies low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models.

Conclusion: The results highlight deeper structure in loss landscapes and emphasize the need for symmetry-aware analysis to understand model space geometry.

Abstract: Understanding the geometry of neural network loss landscapes is a central
question in deep learning, with implications for generalization and
optimization. A striking phenomenon is linear mode connectivity (LMC), where
independently trained models can be connected by low- or zero-loss paths,
despite appearing to lie in separate loss basins. However, this is often
obscured by symmetries in parameter space -- such as neuron permutations --
which make functionally equivalent models appear dissimilar. Prior work has
predominantly focused on neuron re-ordering through permutations, but such
approaches are limited in scope and fail to capture the richer symmetries
exhibited by modern architectures such as Transformers. In this work, we
introduce a unified framework that captures four symmetry classes:
permutations, semi-permutations, orthogonal transformations, and general
invertible maps -- broadening the set of valid reparameterizations and
subsuming many previous approaches as special cases. Crucially, this
generalization enables, for the first time, the discovery of low- and
zero-barrier linear interpolation paths between independently trained Vision
Transformers and GPT-2 models. These results reveal deeper structure in the
loss landscape and underscore the importance of symmetry-aware analysis for
understanding model space geometry.

</details>


### [617] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/pdf/2506.22716)
*Dujian Ding, Ankur Mallick, Shaokun Zhang, Chi Wang, Daniel Madrigal, Mirian Del Carmen Hipolito Garcia, Menglin Xia, Laks V. S. Lakshmanan, Qingyun Wu, Victor Rühle*

Main category: cs.LG

TL;DR: BEST-Route is a novel LLM query routing framework that dynamically selects models and response counts to balance cost and quality, reducing costs by up to 60% with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and high cost of deploying large language models (LLMs) at scale by optimizing query routing and leveraging small models' potential through multiple responses.

Method: Proposes BEST-Route, which selects models and response counts based on query difficulty and quality thresholds, enhancing small-model quality via multiple samples.

Result: Achieves up to 60% cost reduction with less than 1% performance drop in real-world experiments.

Conclusion: BEST-Route effectively balances cost and quality in LLM deployment, offering significant savings without compromising performance.

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [618] [Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery](https://arxiv.org/pdf/2506.22732)
*Hao Shu, Jicheng Li, Tianyv Lei, Lijun Sun*

Main category: cs.LG

TL;DR: The paper introduces RTC-GTNLN, a novel Robust Tensor Completion model using a non-convex tensor rank surrogate (L1-L2 norm) to handle missing data and noise in traffic data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Spatiotemporal traffic data often suffers from missing values and noise due to sensor malfunctions, requiring robust recovery methods for reliable downstream applications.

Method: Proposes RTC-GTNLN, integrating a gradient tensor L1-L2 norm into the RTC framework to exploit global low-rankness and local consistency without trade-offs.

Result: RTC-GTNLN outperforms state-of-the-art methods in recovering traffic data with missing values and noise, as shown in experiments on real-world datasets.

Conclusion: The RTC-GTNLN model effectively addresses dual degradation challenges in traffic data, offering superior performance in complex recovery scenarios.

Abstract: In real-world scenarios, spatiotemporal traffic data frequently experiences
dual degradation from missing values and noise caused by sensor malfunctions
and communication failures. Therefore, effective data recovery methods are
essential to ensure the reliability of downstream data-driven applications.
while classical tensor completion methods have been widely adopted, they are
incapable of modeling noise, making them unsuitable for complex scenarios
involving simultaneous data missingness and noise interference. Existing Robust
Tensor Completion (RTC) approaches offer potential solutions by separately
modeling the actual tensor data and noise. However, their effectiveness is
often constrained by the over-relaxation of convex rank surrogates and the
suboptimal utilization of local consistency, leading to inadequate model
accuracy. To address these limitations, we first introduce the tensor L1-L2
norm, a novel non-convex tensor rank surrogate that functions as an effective
low-rank representation tool. Leveraging an advanced feature fusion strategy,
we further develop the gradient tensor L1-L2 norm by incorporating the tensor
L1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear
L1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via
Gradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully
exploits both global low-rankness and local consistency without trade-off
parameter, but also effectively handles the dual degradation challenges of
missing data and noise in traffic data. Extensive experiments conducted on
multiple real-world traffic datasets demonstrate that the RTC-GTNLN model
consistently outperforms existing state-of-the-art methods in complex recovery
scenarios involving simultaneous missing values and noise.

</details>


### [619] [FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision](https://arxiv.org/pdf/2506.22771)
*Jingxiao Ma, Priyadarshini Panda, Sherief Reda*

Main category: cs.LG

TL;DR: INT8 quantized training using Forward-Forward (FF) algorithm reduces memory, speeds up training, and saves energy while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Backpropagation's inefficiencies in time and energy limit its use for edge devices. FF offers a promising alternative by avoiding backward passes.

Method: Proposes INT8 quantized training with FF and a 'look-ahead' scheme to stabilize gradients and improve accuracy.

Result: 4.6% faster training, 8.3% energy savings, 27.0% memory reduction, with competitive accuracy.

Conclusion: The approach is efficient for resource-constrained devices, balancing performance and accuracy.

Abstract: Backpropagation has been the cornerstone of neural network training for
decades, yet its inefficiencies in time and energy consumption limit its
suitability for resource-constrained edge devices. While low-precision neural
network quantization has been extensively researched to speed up model
inference, its application in training has been less explored. Recently, the
Forward-Forward (FF) algorithm has emerged as a promising alternative to
backpropagation, replacing the backward pass with an additional forward pass.
By avoiding the need to store intermediate activations for backpropagation, FF
can reduce memory footprint, making it well-suited for embedded devices. This
paper presents an INT8 quantized training approach that leverages FF's
layer-by-layer strategy to stabilize gradient quantization. Furthermore, we
propose a novel "look-ahead" scheme to address limitations of FF and improve
model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board
demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in
memory usage, while maintaining competitive accuracy compared to the
state-of-the-art.

</details>


### [620] [Multimodal Atmospheric Super-Resolution With Deep Generative Models](https://arxiv.org/pdf/2506.22780)
*Dibyajyoti Chakraborty, Haiwen Guan, Jason Stock, Troy Arcomano, Guido Cervone, Romit Maulik*

Main category: cs.LG

TL;DR: Score-based diffusion models enable zero-shot conditioning and Bayesian updates for data fusion, applied here to super-resolution of high-dimensional dynamical systems.


<details>
  <summary>Details</summary>
Motivation: To leverage score-based diffusion models for data and model fusion, updating distributions with online data for tasks like super-resolution.

Method: Uses score-based diffusion modeling to learn a score function, reverse a noising process, and apply Bayesian updates for super-resolution of dynamical systems.

Result: Accurate recovery of high-dimensional states from low-fidelity measurements and balancing influence of multiple dataset modalities.

Conclusion: Score-based diffusion models effectively fuse multimodal data for super-resolution, demonstrating robustness and adaptability.

Abstract: Score-based diffusion modeling is a generative machine learning algorithm
that can be used to sample from complex distributions. They achieve this by
learning a score function, i.e., the gradient of the log-probability density of
the data, and reversing a noising process using the same. Once trained,
score-based diffusion models not only generate new samples but also enable
zero-shot conditioning of the generated samples on observed data. This promises
a novel paradigm for data and model fusion, wherein the implicitly learned
distributions of pretrained score-based diffusion models can be updated given
the availability of online data in a Bayesian formulation. In this article, we
apply such a concept to the super-resolution of a high-dimensional dynamical
system, given the real-time availability of low-resolution and experimentally
observed sparse sensor measurements from multimodal data. Additional analysis
on how score-based sampling can be used for uncertainty estimates is also
provided. Our experiments are performed for a super-resolution task that
generates the ERA5 atmospheric dataset given sparse observations from a
coarse-grained representation of the same and/or from unstructured experimental
observations of the IGRA radiosonde dataset. We demonstrate accurate recovery
of the high dimensional state given multiple sources of low-fidelity
measurements. We also discover that the generative model can balance the
influence of multiple dataset modalities during spatiotemporal reconstructions.

</details>


### [621] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/pdf/2506.22802)
*Hae Jin Song, Laurent Itti*

Main category: cs.LG

TL;DR: The paper proposes a geometric approach to define and analyze generative model fingerprints using Riemannian geometry, improving model attribution and generalization.


<details>
  <summary>Details</summary>
Motivation: The need to authenticate generative models for IP protection, verify content sources, and address model collapse due to synthetic data in training.

Method: A geometric framework using Riemannian geometry to define artifacts and fingerprints, replacing Euclidean metrics with geodesic distances and Riemannian center of mass.

Result: Improved effectiveness in distinguishing diverse generative models across datasets, resolutions, architectures, and modalities.

Conclusion: The proposed method enhances model attribution and generalization, demonstrating practical efficacy.

Abstract: Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [622] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/pdf/2506.22809)
*Cooper Doyle*

Main category: cs.LG

TL;DR: BayesLoRA integrates MC-Dropout into LoRA for task-specific uncertainty quantification, aiding agent decision-making under uncertainty.


<details>
  <summary>Details</summary>
Motivation: General-purpose transformer uncertainty methods lack task-specific guardrails, limiting their utility for downstream workflows.

Method: BayesLoRA combines MC-Dropout with LoRA adapters, analyzing their variance outside fine-tuning distributions.

Result: The framework provides reliable confidence estimates for agentic decision-making, validated mathematically and empirically.

Conclusion: BayesLoRA offers tailored uncertainty quantification, enhancing agent introspection and behavior modulation.

Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [623] [Deep learning 40 years of human migration](https://arxiv.org/pdf/2506.22821)
*Thomas Gaskin, Guy J. Abel*

Main category: cs.LG

TL;DR: A novel dataset on global migration flows (1990-present) is created using a deep recurrent neural network trained on 18 covariates, providing detailed estimates with confidence bounds and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive and detailed dataset on global migration flows and stocks, disaggregated by country of birth, to improve understanding and research on human migration.

Method: A deep recurrent neural network is trained on 18 covariates (geographic, economic, cultural, societal, political) to learn migration patterns, with an ensemble approach for confidence bounds.

Result: The model outperforms traditional methods, offering higher temporal resolution and validated accuracy on unseen data, with open-source availability of data and code.

Conclusion: The dataset and model provide a valuable, open-source resource for migration research, highlighting regions needing additional data and improving estimation accuracy.

Abstract: We present a novel and detailed dataset on origin-destination annual
migration flows and stocks between 230 countries and regions, spanning the
period from 1990 to the present. Our flow estimates are further disaggregated
by country of birth, providing a comprehensive picture of migration over the
last 43 years. The estimates are obtained by training a deep recurrent neural
network to learn flow patterns from 18 covariates for all countries, including
geographic, economic, cultural, societal, and political information. The
recurrent architecture of the neural network means that the entire past can
influence current migration patterns, allowing us to learn long-range temporal
correlations. By training an ensemble of neural networks and additionally
pushing uncertainty on the covariates through the trained network, we obtain
confidence bounds for all our estimates, allowing researchers to pinpoint the
geographic regions most in need of additional data collection. We validate our
approach on various test sets of unseen data, demonstrating that it
significantly outperforms traditional methods estimating five-year flows while
delivering a significant increase in temporal resolution. The model is fully
open source: all training data, neural network weights, and training code are
made public alongside the migration estimates, providing a valuable resource
for future studies of human migration.

</details>


### [624] [xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection](https://arxiv.org/pdf/2506.22837)
*Kamil Faber, Marcin Pietroń, Dominik Żurek, Roberto Corizzo*

Main category: cs.LG

TL;DR: The paper introduces xLSTMAD, the first anomaly detection method using xLSTM, achieving state-of-the-art results on multivariate time series data.


<details>
  <summary>Details</summary>
Motivation: To explore xLSTM's potential for anomaly detection, a gap in prior research, by proposing a novel encoder-decoder architecture.

Method: xLSTMAD uses two decoder variants (forecasting and reconstruction) with MSE and SoftDTW loss functions for local and global sequence alignment.

Result: xLSTMAD outperforms 23 baselines on the TSB-AD-M benchmark, demonstrating superior accuracy.

Conclusion: xLSTMAD reveals xLSTM's strong anomaly detection capabilities, opening new research directions.

Abstract: The recently proposed xLSTM is a powerful model that leverages expressive
multiplicative gating and residual connections, providing the temporal capacity
needed for long-horizon forecasting and representation learning. This
architecture has demonstrated success in time series forecasting, lossless
compression, and even large-scale language modeling tasks, where its linear
memory footprint and fast inference make it a viable alternative to
Transformers. Despite its growing popularity, no prior work has explored xLSTM
for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the
first anomaly detection method that integrates a full encoder-decoder xLSTM
architecture, purpose-built for multivariate time series data. Our encoder
processes input sequences to capture historical context, while the decoder is
devised in two separate variants of the method. In the forecasting approach,
the decoder iteratively generates forecasted future values xLSTMAD-F, while the
reconstruction approach reconstructs the input time series from its encoded
counterpart xLSTMAD-R. We investigate the performance of two loss functions:
Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider
local reconstruction fidelity and global sequence alignment, respectively. We
evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17
real-world datasets, using state-of-the-art challenging metrics such as VUS-PR.
In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23
popular anomaly detection baselines. Our paper is the first work revealing the
powerful modeling capabilities of xLSTM for anomaly detection, paving the way
for exciting new developments on this subject. Our code is available at:
https://github.com/Nyderx/xlstmad

</details>


### [625] [Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models](https://arxiv.org/pdf/2506.22845)
*Batuhan Hangun, Oguz Altun, Onder Eyecioglu*

Main category: cs.LG

TL;DR: QNNs are explored for wind turbine power output prediction, showing competitive performance with classical methods, with insights on dataset size and circuit complexity.


<details>
  <summary>Details</summary>
Motivation: To investigate QNNs' applicability in predicting wind turbine power output, addressing the need for efficient methods in smart grids.

Method: Six QNN configurations using Z Feature Map for data encoding and varying ansatz structures were tested via cross-validation and hold-out dataset experiments.

Result: QNNs achieved competitive or marginally better predictive performance than classical methods, with dataset size and circuit complexity impacting results.

Conclusion: QNNs show promise for energy applications, offering insights for integrating quantum machine learning in the field.

Abstract: Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine
Learning (QML), are emerging as a powerful alternative to classical machine
learning methods. Recent studies have focused on the applicability of QNNs to
various tasks, such as time-series forecasting, prediction, and classification,
across a wide range of applications, including cybersecurity and medical
imaging. With the increased use of smart grids driven by the integration of
renewable energy systems, machine learning plays an important role in
predicting power demand and detecting system disturbances. This study provides
an in-depth investigation of QNNs for predicting the power output of a wind
turbine. We assess the predictive performance and simulation time of six QNN
configurations that are based on the Z Feature Map for data encoding and
varying ansatz structures. Through detailed cross-validation experiments and
tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs
can achieve predictive performance that is competitive with, and in some cases
marginally better than, the benchmarked classical approaches. Our results also
reveal the effects of dataset size and circuit complexity on predictive
performance and simulation time. We believe our findings will offer valuable
insights for researchers in the energy domain who wish to incorporate quantum
machine learning into their work.

</details>


### [626] [Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles](https://arxiv.org/pdf/2506.22848)
*Shengcai Liu, Hui Ou-yang, Zhiyuan Wang, Cheng Chen, Qijun Cai, Yew-Soon Ong, Ke Tang*

Main category: cs.LG

TL;DR: The paper proposes an automatic approach (Auto-SLE) to improve the accuracy of learning large Bayesian networks (BNs) by combining multiple BN structure learning algorithms into an ensemble (SLE), outperforming traditional divide-and-conquer methods.


<details>
  <summary>Details</summary>
Motivation: Learning large BNs is challenging, and existing divide-and-conquer methods suffer from unstable accuracy across subproblems.

Method: Introduces SLE to combine multiple BN learning algorithms and proposes Auto-SLE for automatic, near-optimal SLE design, integrating it into a divide-and-conquer framework.

Result: Achieves 30%–225% accuracy improvement on datasets with 10,000 variables and generalizes well to larger datasets (e.g., 30,000 variables).

Conclusion: SLEs, especially when automatically learned, show significant potential for scalable and accurate BN structure learning.

Abstract: Learning the structure of Bayesian networks (BNs) from data is challenging,
especially for datasets involving a large number of variables. The recently
proposed divide-and-conquer (D\&D) strategies present a promising approach for
learning large BNs. However, they still face a main issue of unstable learning
accuracy across subproblems. In this work, we introduce the idea of employing
structure learning ensemble (SLE), which combines multiple BN structure
learning algorithms, to consistently achieve high learning accuracy. We further
propose an automatic approach called Auto-SLE for learning near-optimal SLEs,
addressing the challenge of manually designing high-quality SLEs. The learned
SLE is then integrated into a D\&D method. Extensive experiments firmly show
the superiority of our method over D\&D methods with single BN structure
learning algorithm in learning large BNs, achieving accuracy improvement
usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore,
our method generalizes well to datasets with many more (e.g., 30000) variables
and different network characteristics than those present in the training data
for learning the SLE. These results indicate the significant potential of
employing (automatic learning of) SLEs for scalable BN structure learning.

</details>


### [627] [Interpretable Time Series Autoregression for Periodicity Quantification](https://arxiv.org/pdf/2506.22895)
*Xinyu Chen, Vassilis Digalakis Jr, Lijun Ding, Dingyi Zhuang, Jinhua Zhao*

Main category: cs.LG

TL;DR: A novel sparse autoregression framework is proposed for interpretable periodicity quantification in time series, using ℓ₀-norm sparsity and mixed-integer optimization (MIO) with a subspace pursuit strategy for scalability.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and scalability in time series autoregression, especially for capturing periodicity and seasonality in time-varying and multidimensional data.

Method: Proposes sparse autoregression with ℓ₀-norm constraints, reformulates it as MIO, and introduces a subspace pursuit-based decision variable pruning (DVP) strategy. For multidimensional data, a two-stage optimization scheme is developed.

Result: The DVP strategy accelerates MIO without compromising solution quality. Applied to ridesharing and climate data, it uncovers periodicities and dynamic patterns like El Niño.

Conclusion: The framework effectively balances interpretability and scalability, revealing meaningful temporal and spatial patterns in real-world time series.

Abstract: Time series autoregression is a classical statistical model for capturing
auto-correlations and identifying temporal patterns such as periodicity and
seasonality. In this work, we propose a novel sparse autoregression framework
from an interpretable machine learning perspective and the model
interpretability for periodicity quantification is reinforced by $\ell_0$-norm
induced sparsity constraints. On the time-varying time series data, we
reformulate the sparse autoregression and convert the involved optimization
problem into a mixed-integer optimization (MIO). To accelerate it, we develop a
subspace pursuit based decision variable pruning (DVP) strategy to reduce the
search space. On the multidimensional time series that involves complicated
spatial and temporal dimensions, we propose a spatially- and time-varying
sparse autoregression model and resolve the corresponding MIO problem by
developing a two-stage optimization scheme. In particular, the proposed scheme
makes the model scalable to large problems even with millions of decision
variables. Empirically, we conduct extensive experiments to evaluate the
proposed models on real-world time series data. First, we demonstrate that the
MIO solver can be drastically accelerated through the DVP strategy, while
maintaining the same solution quality as a full MIO solver. Applying the
time-varying sparse autoregression model to ridesharing trip data, we uncover
both daily and weekly periodicities and reveal long-term changes in regularity
of human mobility. Second, we demonstrate the spatial patterns of yearly
seasonality in climate variable time series such as temperature and
precipitation across the past four decades, and our model allows to discover
dynamic climate patterns and identify climate phenomena such as El Nino in sea
surface temperature.

</details>


### [628] [Missing-Modality-Aware Graph Neural Network for Cancer Classification](https://arxiv.org/pdf/2506.22901)
*Sina Tabakhi, Haiping Lu*

Main category: cs.LG

TL;DR: MAGNET, a Missing-modality-Aware Graph neural NETwork, addresses missing modalities in multimodal biological data by using a patient-modality attention mechanism and graph neural networks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Handling missing modalities in multimodal biological data is challenging due to diverse missing patterns and exponential growth of patterns with increasing modalities.

Method: MAGNET uses a patient-modality multi-head attention mechanism to fuse lower-dimensional embeddings and constructs a patient graph for predictions with a graph neural network.

Result: MAGNET outperforms state-of-the-art fusion methods on three public multiomics datasets for cancer classification with real-world missingness.

Conclusion: MAGNET effectively handles missing modalities with linear complexity and adapts to missing-pattern variability, offering a robust solution for multimodal data fusion.

Abstract: A key challenge in learning from multimodal biological data is missing
modalities, where all data from some modalities are missing for some patients.
Current fusion methods address this by excluding patients with missing
modalities, imputing missing modalities, or making predictions directly with
partial modalities. However, they often struggle with diverse missing-modality
patterns and the exponential growth of the number of such patterns as the
number of modalities increases. To address these limitations, we propose MAGNET
(Missing-modality-Aware Graph neural NETwork) for direct prediction with
partial modalities, which introduces a patient-modality multi-head attention
mechanism to fuse lower-dimensional modality embeddings based on their
importance and missingness. MAGNET's complexity increases linearly with the
number of modalities while adapting to missing-pattern variability. To generate
predictions, MAGNET further constructs a patient graph with fused multimodal
embeddings as node features and the connectivity determined by the modality
missingness, followed by a conventional graph neural network. Experiments on
three public multiomics datasets for cancer classification, with real-world
instead of artificial missingness, show that MAGNET outperforms the
state-of-the-art fusion methods. The data and code are available at
https://github.com/SinaTabakhi/MAGNET.

</details>


### [629] [Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration](https://arxiv.org/pdf/2506.22929)
*Chen Zhang*

Main category: cs.LG

TL;DR: A parallel computation architecture is proposed to handle high-dimensional data by decomposing it into dimension-independent structures, enabling efficient distributed processing and integration of advanced analysis methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with high-dimensional data due to computational challenges, and existing tools lack support for advanced mathematical statistics.

Method: A space-completeness-based parallel computation architecture decomposes high-dimensional data into dimension-independent structures for distributed processing.

Result: The framework supports seamless integration of data mining and parallel-optimized machine learning, applicable to diverse data types like medical and natural images.

Conclusion: The proposed architecture addresses the dimensionality curse, enabling efficient analysis of high-dimensional data within a unified system.

Abstract: While deep learning excels in natural image and language processing, its
application to high-dimensional data faces computational challenges due to the
dimensionality curse. Current large-scale data tools focus on business-oriented
descriptive statistics, lacking mathematical statistics support for advanced
analysis. We propose a parallel computation architecture based on space
completeness, decomposing high-dimensional data into dimension-independent
structures for distributed processing. This framework enables seamless
integration of data mining and parallel-optimized machine learning methods,
supporting scientific computations across diverse data types like medical and
natural images within a unified system.

</details>


### [630] [Towards Time Series Generation Conditioned on Unstructured Natural Language](https://arxiv.org/pdf/2506.22927)
*Jaeyun Woo, Jiseok Lee, Brian Kenji Iwana*

Main category: cs.LG

TL;DR: A novel method for generating time series from natural language descriptions using a diffusion model combined with a language model, demonstrating potential applications and introducing a new public dataset.


<details>
  <summary>Details</summary>
Motivation: Time series generative AI is underdeveloped despite its importance in fields like finance and climate, and existing methods lack the ability to generate time series from unstructured natural language.

Method: Uses a diffusion model combined with a language model to generate time series conditioned on natural language descriptions.

Result: Shows that time series generation from text is feasible, with applications in custom forecasting, data augmentation, and transfer learning. Introduces a new dataset of 63,010 time series-description pairs.

Conclusion: The proposed method advances time series generative AI and opens up new applications, supported by a publicly available dataset.

Abstract: Generative Artificial Intelligence (AI) has rapidly become a powerful tool,
capable of generating various types of data, such as images and text. However,
despite the significant advancement of generative AI, time series generative AI
remains underdeveloped, even though the application of time series is essential
in finance, climate, and numerous fields. In this research, we propose a novel
method of generating time series conditioned on unstructured natural language
descriptions. We use a diffusion model combined with a language model to
generate time series from the text. Through the proposed method, we demonstrate
that time series generation based on natural language is possible. The proposed
method can provide various applications such as custom forecasting, time series
manipulation, data augmentation, and transfer learning. Furthermore, we
construct and propose a new public dataset for time series generation,
consisting of 63,010 time series-description pairs.

</details>


### [631] [Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models](https://arxiv.org/pdf/2506.22950)
*Liangyu Wang, Huanyi Xie, Xinhai Wang, Tianjin Huang, Mengdi Li, Di Wang*

Main category: cs.LG

TL;DR: Infinite Sampling reduces memory overhead in GRPO for LLMs by decoupling group size from GPU usage, improving efficiency and throughput.


<details>
  <summary>Details</summary>
Motivation: Current GRPO methods for LLMs incur high memory costs with large sample groups, limiting scalability.

Method: Proposes Infinite Sampling: micro sampling groups, continuous sampling, and a length-aware scheduler.

Result: Reduces peak memory by 50% and improves throughput by 25% while maintaining performance.

Conclusion: Infinite Sampling enables efficient GRPO training under GPU constraints.

Abstract: Group-based reinforcement learning algorithms such as Group Reward Policy
Optimization (GRPO) have proven effective for fine-tuning large language models
(LLMs) with human feedback. However, generating and storing multiple responses
per prompt incurs substantial memory overhead, especially as the sample group
size increases, limiting scalability under constrained hardware.
  We propose Infinite Sampling, a framework that enables efficient and stable
GRPO training by decoupling group size from GPU memory usage. It consists of:
(1) micro sampling groups that decompose large groups into memory-feasible
rounds; (2) continuous sampling that interleaves generation across groups to
improve utilization; and (3) a length-aware scheduler combining
token-conditioned sequence length prediction with a two-stage plan: global
grouping via FPTAS and runtime refill via SJF.
  Experiments show that our Micro Sampling Groups reduce peak memory usage by
over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on
Qwen3-1.7B). Building on this, Infinite Sampling improves throughput by over
25% compared to the naive micro sampling group method, reducing decoding steps
while maintaining full-length completions and memory usage. Our hybrid
scheduling ensures efficient and stable GRPO training with larger groups under
realistic GPU memory constraints.

</details>


### [632] [Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning](https://arxiv.org/pdf/2506.22984)
*Prathyush Kumar Reddy Lebaku, Lu Gao, Yunpeng Zhang, Zhixia Li, Yongxin Liu, Tanvir Arafin*

Main category: cs.LG

TL;DR: The paper proposes a machine learning approach using stacked LSTM and Random Forest models for anomaly detection in connected autonomous vehicles, achieving high accuracy in identifying abnormal driving patterns.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection is critical for CAVs due to risks like sensor malfunctions, cyber-attacks, and environmental disruptions, ensuring safe transportation networks.

Method: Simulated vehicle behavior to create a dataset of typical and atypical interactions, then applied stacked LSTM for temporal dependencies and Random Forest for ensemble-based anomaly detection.

Result: Random Forest achieved R2 of 0.9830 and MAE of 5.746, while stacked LSTM had R2 of 0.9998 and MAE of 82.425, showing high accuracy in anomaly detection.

Conclusion: The models effectively detect anomalies in CAVs, demonstrating their potential for enhancing safety and reliability in autonomous driving.

Abstract: Anomaly detection in connected autonomous vehicles (CAVs) is crucial for
maintaining safe and reliable transportation networks, as CAVs can be
susceptible to sensor malfunctions, cyber-attacks, and unexpected environmental
disruptions. This study explores an anomaly detection approach by simulating
vehicle behavior, generating a dataset that represents typical and atypical
vehicular interactions. The dataset includes time-series data of position,
speed, and acceleration for multiple connected autonomous vehicles. We utilized
machine learning models to effectively identify abnormal driving patterns.
First, we applied a stacked Long Short-Term Memory (LSTM) model to capture
temporal dependencies and sequence-based anomalies. The stacked LSTM model
processed the sequential data to learn standard driving behaviors.
Additionally, we deployed a Random Forest model to support anomaly detection by
offering ensemble-based predictions, which enhanced model interpretability and
performance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,
and a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model
attained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly
threshold of 265.63. These results demonstrate the models' effectiveness in
accurately predicting vehicle trajectories and detecting anomalies in
autonomous driving scenarios.

</details>


### [633] [Kernel Outlier Detection](https://arxiv.org/pdf/2506.22994)
*Can Hakan Dağıdır, Mia Hubert, Peter J. Rousseeuw*

Main category: cs.LG

TL;DR: Proposes Kernel Outlier Detection (KOD) for high-dimensional outlier detection, overcoming limitations of existing methods with a kernel-based and projection pursuit approach.


<details>
  <summary>Details</summary>
Motivation: Address challenges in outlier detection in high-dimensional settings, such as dependence on distributional assumptions or hard-to-tune hyperparameters.

Method: Uses a kernel transformation followed by projection pursuit, with a novel ensemble of directions and combination method.

Result: Effective on both small datasets with challenging structures and large benchmark datasets.

Conclusion: KOD offers a flexible, lightweight solution for outlier detection in high-dimensional data.

Abstract: A new anomaly detection method called kernel outlier detection (KOD) is
proposed. It is designed to address challenges of outlier detection in
high-dimensional settings. The aim is to overcome limitations of existing
methods, such as dependence on distributional assumptions or on hyperparameters
that are hard to tune. KOD starts with a kernel transformation, followed by a
projection pursuit approach. Its novelties include a new ensemble of directions
to search over, and a new way to combine results of different direction types.
This provides a flexible and lightweight approach for outlier detection. Our
empirical evaluations illustrate the effectiveness of KOD on three small
datasets with challenging structures, and on four large benchmark datasets.

</details>


### [634] [A Reinforcement Learning Approach for Optimal Control in Microgrids](https://arxiv.org/pdf/2506.22995)
*Davide Salaorni, Federico Bianchi, Francesco Trovò, Marcello Restelli*

Main category: cs.LG

TL;DR: A reinforcement learning (RL)-based method optimizes microgrid energy management, outperforming rule-based and existing RL methods.


<details>
  <summary>Details</summary>
Motivation: The integration of renewable energy sources requires decentralized energy management solutions, with microgrids offering localized control.

Method: Proposes an RL agent for energy trading and storage, using a digital twin for realistic simulation of storage dynamics and degradation.

Result: Validated with real-world data, the RL-based strategy outperforms rule-based and existing RL benchmarks.

Conclusion: The RL approach provides a robust solution for intelligent microgrid management.

Abstract: The increasing integration of renewable energy sources (RESs) is transforming
traditional power grid networks, which require new approaches for managing
decentralized energy production and consumption. Microgrids (MGs) provide a
promising solution by enabling localized control over energy generation,
storage, and distribution. This paper presents a novel reinforcement learning
(RL)-based methodology for optimizing microgrid energy management.
Specifically, we propose an RL agent that learns optimal energy trading and
storage policies by leveraging historical data on energy production,
consumption, and market prices. A digital twin (DT) is used to simulate the
energy storage system dynamics, incorporating degradation factors to ensure a
realistic emulation of the analysed setting. Our approach is validated through
an experimental campaign using real-world data from a power grid located in the
Italian territory. The results indicate that the proposed RL-based strategy
outperforms rule-based methods and existing RL benchmarks, offering a robust
solution for intelligent microgrid management.

</details>


### [635] [BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs](https://arxiv.org/pdf/2506.23024)
*Jerry Liu, Yasa Baig, Denise Hui Jean Lee, Rajat Vadiraj Dwaraknath, Atri Rudra, Chris Ré*

Main category: cs.LG

TL;DR: The paper introduces the Barycentric Weight Layer (BWLer) to address precision limitations in Physics-informed neural networks (PINNs) for solving PDEs, achieving near-machine-precision accuracy in some cases.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with achieving machine-precision accuracy for PDEs, prompting investigation into whether the issue stems from PDE ill-conditioning or MLP architecture.

Method: The BWLer, a barycentric polynomial interpolation layer, is introduced to either augment or replace MLPs in PINNs, separating solution representation from derivative computation.

Result: BWLer significantly improves accuracy (up to 1800x better RMSE) and achieves near-machine-precision for certain PDEs, outperforming standard PINNs.

Conclusion: BWLer bridges the gap between PINNs' flexibility and classical spectral solvers' precision, offering a practical solution for high-accuracy PDE solving.

Abstract: Physics-informed neural networks (PINNs) offer a flexible way to solve
partial differential equations (PDEs) with machine learning, yet they still
fall well short of the machine-precision accuracy many scientific tasks demand.
In this work, we investigate whether the precision ceiling comes from the
ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)
architecture. We introduce the Barycentric Weight Layer (BWLer), which models
the PDE solution through barycentric polynomial interpolation. A BWLer can be
added on top of an existing MLP (a BWLer-hat) or replace it completely
(explicit BWLer), cleanly separating how we represent the solution from how we
take derivatives for the PDE loss. Using BWLer, we identify fundamental
precision limitations within the MLP: on a simple 1-D interpolation task, even
MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above
float64 machine precision -- before any PDE terms are added. In PDE learning,
adding a BWLer lifts this ceiling and exposes a tradeoff between achievable
accuracy and the conditioning of the PDE loss. For linear PDEs we fully
characterize this tradeoff with an explicit error decomposition and navigate it
during training with spectral derivatives and preconditioning. Across five
benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for
convection, 10x for reaction, and 1800x for wave equations while remaining
compatible with first-order optimizers. Replacing the MLP entirely lets an
explicit BWLer reach near-machine-precision on convection, reaction, and wave
problems (up to 10 billion times better than prior results) and match the
performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson
problems. Together, these findings point to a practical path for combining the
flexibility of PINNs with the precision of classical spectral solvers.

</details>


### [636] [Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models](https://arxiv.org/pdf/2506.23025)
*Tejas Vaidhya, Ayush Kaushal, Vineet Jain, Francis Couture Harpin, Prashant Shishodia, Majid Behbahani, Yuriy Nevmyvaka, Irina Rish*

Main category: cs.LG

TL;DR: The paper introduces ternary language models (TriLMs) to improve inference efficiency in large language models (LLMs) by reducing memory requirements. It includes scaling law analysis, a new TriLM suite (Spectra-1.1), and efficient packing schemes for faster inference.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck in LLM inference caused by limited memory bandwidth and capacity, despite advances in GPU computational power.

Method: Propose TriLMs with quantization-aware training, analyze their scalability, introduce Spectra-1.1 (trained on 1.2T tokens), and develop 2-bit/1.6-bit packing schemes and a GPU kernel (TriRun) for faster inference.

Result: TriLMs show better performance with more training data than larger parameters. Spectra-1.1 and TriRun achieve up to 5x faster inference compared to floating-point baselines.

Conclusion: The work provides efficient LLM deployment solutions, releasing Spectra-1.1 and TriRun to support further research in TriLMs.

Abstract: Large language models (LLMs) are increasingly used across research and
industry applications, yet their inference efficiency remains a significant
challenge. As the computational power of modern GPU architectures continuously
improves, their memory bandwidth and capacity have not scaled proportionally,
creating a critical bottleneck during inference. To address this, we
investigate ternary language models (TriLMs) that employ quantization-aware
training to significantly reduce memory requirements. We first analyze the
scalability of TriLMs by conducting a scaling law analysis, revealing that
TriLMs benefit more from increasing training data than from scaling model
parameters. Based on this observation, we introduce Spectra-1.1, an open suite
of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained
performance gains at scale. Furthermore, to improve inference efficiency, we
propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which
demonstrate accelerated inference across various CPU architectures. Also,
building on the 2-bit packing, we develop a GPU kernel called TriRun that
accelerates end-to-end model inference by up to 5 times compared to
floating-point baselines. To encourage further exploration and development of
TriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.
Overall, our work lays the foundation for building and deploying efficient
LLMs, providing a valuable resource for the research community.

</details>


### [637] [Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning](https://arxiv.org/pdf/2506.23033)
*Yash Vardhan Tomar*

Main category: cs.LG

TL;DR: The paper introduces a feature-wise mixing framework to mitigate bias in ML models, achieving a 43.35% bias reduction and improved predictive performance without explicit bias attribute identification.


<details>
  <summary>Details</summary>
Motivation: Existing bias mitigation methods are limited in scalability and generalizability, prompting the need for a more flexible approach.

Method: A feature-wise mixing framework redistributes feature representations across datasets, evaluated using bias-sensitive loss functions and cross-validation.

Result: The method reduced bias by 43.35% and improved predictive performance (lower MSE) across classifiers, outperforming SMOTE oversampling.

Conclusion: Feature-wise mixing is effective and efficient for bias mitigation, with potential for real-world applications requiring accurate predictions.

Abstract: Bias in predictive machine learning (ML) models is a fundamental challenge
due to the skewed or unfair outcomes produced by biased models. Existing
mitigation strategies rely on either post-hoc corrections or rigid constraints.
However, emerging research claims that these techniques can limit scalability
and reduce generalizability. To address this, this paper introduces a
feature-wise mixing framework to mitigate contextual bias. This was done by
redistributing feature representations across multiple contextual datasets. To
assess feature-wise mixing's effectiveness, four ML classifiers were trained
using cross-validation and evaluated with bias-sensitive loss functions,
including disparity metrics and mean squared error (MSE), which served as a
standard measure of predictive performance. The proposed method achieved an
average bias reduction of 43.35% and a statistically significant decrease in
MSE across all classifiers trained on mixed datasets. Additionally,
benchmarking against established bias mitigation techniques found that
feature-wise mixing consistently outperformed SMOTE oversampling and
demonstrated competitive effectiveness without requiring explicit bias
attribute identification. Feature-wise mixing efficiently avoids the
computational overhead typically associated with fairness-aware learning
algorithms. Future work could explore applying feature-wise mixing for
real-world fields where accurate predictions are necessary.

</details>


### [638] [Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress](https://arxiv.org/pdf/2506.23036)
*Zain ul Abdeen, Ming Jin*

Main category: cs.LG

TL;DR: The paper analyzes RL policy robustness by testing network parameters under internal (synaptic filtering) and external (adversarial attacks) stresses, classifying parameters as fragile, robust, or antifragile. Validated on PPO-trained agents, it identifies antifragile parameters that improve performance under stress.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance RL policy robustness by examining how parameters respond to stress, inspired by synaptic plasticity in neuroscience.

Method: Uses synaptic filtering (internal stress) and adversarial attacks (external stress) to classify parameters. Defines scores to quantify fragility, robustness, or antifragility. Validates on PPO-trained agents in Mujoco environments.

Result: Identifies antifragile parameters that boost policy performance under stress, showcasing the potential of targeted filtering for RL adaptability.

Conclusion: The framework lays groundwork for designing robust and antifragile RL systems, with implications for future advancements in RL adaptability.

Abstract: This paper explores Reinforcement learning (RL) policy robustness by
systematically analyzing network parameters under internal and external
stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering
introduces internal stress by selectively perturbing parameters, while
adversarial attacks apply external stress through modified agent observations.
This dual approach enables the classification of parameters as fragile, robust,
or antifragile, based on their influence on policy performance in clean and
adversarial settings. Parameter scores are defined to quantify these
characteristics, and the framework is validated on PPO-trained agents in Mujoco
continuous control environments. The results highlight the presence of
antifragile parameters that enhance policy performance under stress,
demonstrating the potential of targeted filtering techniques to improve RL
policy adaptability. These insights provide a foundation for future
advancements in the design of robust and antifragile RL systems.

</details>


### [639] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/pdf/2506.23041)
*Chengyu Dong, Huan Gui, Noveen Sachdeva, Long Jin, Ke Yin, Jingbo Shang, Lichan Hong, Ed H. Chi, Zhe Zhao*

Main category: cs.LG

TL;DR: The paper addresses the challenge of effective knowledge distillation from large-scale pretrained Vision Transformers (ViTs) to smaller models, proposing mutual information-aware optimization and MLP block reweighting for improved transfer.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of knowledge transfer drops when distilling from large-scale pretrained models, especially for small or imbalanced datasets.

Method: Proposes mutual information-aware optimization during finetuning and reweights MLP blocks to address mutual information loss.

Result: Enables small student models to benefit effectively from strong pretrained models.

Conclusion: The proposed methods enhance knowledge distillation from large-scale pretrained ViTs to smaller models, particularly for challenging datasets.

Abstract: Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [640] [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/pdf/2402.09146)
*Muhammad Kashif, Muhammad Shafique*

Main category: cs.LG

TL;DR: A novel framework enhances Quanvolutional Neural Networks (QuNNs) by introducing trainable layers and residual learning to improve gradient flow and training performance.


<details>
  <summary>Details</summary>
Motivation: Traditional quanvolutional layers are static and lack adaptability, limiting QuNNs' potential. This work aims to overcome this by enabling training within these layers.

Method: Proposes Residual Quanvolutional Neural Networks (ResQuNNs) with skip connections (residual blocks) between layers to enhance gradient access and training efficiency.

Result: Empirical evidence shows strategic placement of residual blocks improves gradient flow and training performance, marking a significant advancement in quantum deep learning.

Conclusion: The framework advances quantum deep learning by improving QuNN adaptability and training, opening new theoretical and practical possibilities.

Abstract: In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.

</details>


### [641] [Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction](https://arxiv.org/pdf/2506.23053)
*Hanlin Dong, Arian Prabowo, Hao Xue, Flora D. Salim*

Main category: cs.LG

TL;DR: Double-Diffusion, a novel diffusion probabilistic model, combines physics principles with stochasticity for air quality prediction, outperforming other models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing certainty and uncertainty in air quality prediction due to spatio-temporal complexity and inherent dynamics.

Method: Proposes Double-Diffusion, a physics-guided diffusion probabilistic model, with a sampling strategy from image restoration and a new denoiser architecture.

Result: Ranks first in most evaluations, reduces inference time by 30-50%, and improves CRPS by 3-12%.

Conclusion: Double-Diffusion effectively integrates physics and stochasticity, setting a new benchmark for air quality prediction.

Abstract: Air quality prediction is a challenging forecasting task due to its
spatio-temporal complexity and the inherent dynamics as well as uncertainty.
Most of the current models handle these two challenges by applying Graph Neural
Networks or known physics principles, and quantifying stochasticity through
probabilistic networks like Diffusion models. Nevertheless, finding the right
balancing point between the certainties and uncertainties remains an open
question. Therefore, we propose Double-Diffusion, a novel diffusion
probabilistic model that harnesses the power of known physics to guide air
quality forecasting with stochasticity. To the best of our knowledge, while
precedents have been made of using conditional diffusion models to predict air
pollution, this is the first attempt to use physics as a conditional generative
approach for air quality prediction. Along with a sampling strategy adopted
from image restoration and a new denoiser architecture, Double-Diffusion ranks
first in most evaluation scenarios across two real-life datasets compared with
other probabilistic models, it also cuts inference time by 50% to 30% while
enjoying an increase between 3-12% in Continuous Ranked Probabilistic Score
(CRPS).

</details>


### [642] [Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis](https://arxiv.org/pdf/2506.23055)
*Hiro Taiyo Hamada, Ippei Fujisawa, Genji Kawakita, Yuki Yamada*

Main category: cs.LG

TL;DR: The paper evaluates how well LLMs like GPT-4 align with human psychological concepts using standardized questionnaires and similarity analysis. GPT-4 outperforms other models in classification accuracy and correlates with human responses.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs accurately internalize human psychological constructs and identify potential biases in their representations.

Method: A quantitative framework using 43 psychological questionnaires, pairwise similarity analysis, and hierarchical clustering to compare LLM outputs with human constructs.

Result: GPT-4 achieved 66.2% classification accuracy, surpassing GPT-3.5 (55.9%) and BERT (48.1%), and correlated with human response patterns.

Conclusion: Modern LLMs can approximate human psychological constructs with measurable accuracy, aiding the development of more interpretable AI systems.

Abstract: Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities
in producing human-like text. However, it is unclear how accurately these
models internalize concepts that shape human thought and behavior. Here, we
developed a quantitative framework to assess concept alignment between LLMs and
human psychological dimensions using 43 standardized psychological
questionnaires, selected for their established validity in measuring distinct
psychological constructs. Our method evaluates how accurately language models
reconstruct and classify questionnaire items through pairwise similarity
analysis. We compared resulting cluster structures with the original
categorical labels using hierarchical clustering. A GPT-4 model achieved
superior classification accuracy (66.2\%), significantly outperforming GPT-3.5
(55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%).
We also demonstrated that the estimated semantic similarity from GPT-4 is
associated with Pearson's correlation coefficients of human responses in
multiple psychological questionnaires. This framework provides a novel approach
to evaluate the alignment of the human-LLM concept and identify potential
representational biases. Our findings demonstrate that modern LLMs can
approximate human psychological constructs with measurable accuracy, offering
insights for developing more interpretable AI systems.

</details>


### [643] [Curious Causality-Seeking Agents Learn Meta Causal World](https://arxiv.org/pdf/2506.23068)
*Zhiyu Zhao, Haoxuan Li, Haifeng Zhang, Jun Wang, Francesco Faccio, Jürgen Schmidhuber, Mengyue Yang*

Main category: cs.LG

TL;DR: The paper introduces a Meta-Causal Graph as a world model to handle shifting causal mechanisms, and a Causality-Seeking Agent to identify and refine these mechanisms through curiosity-driven exploration.


<details>
  <summary>Details</summary>
Motivation: Traditional world models assume fixed causal rules, but real-world dynamics often shift subtly. This work addresses the challenge of modeling such shifts.

Method: Proposes a Meta-Causal Graph composed of causal subgraphs triggered by latent meta states. A Causality-Seeking Agent identifies meta states, discovers causal relationships, and refines the graph iteratively.

Result: Experiments on synthetic and robot arm tasks show the method captures causal shifts and generalizes to unseen contexts.

Conclusion: The Meta-Causal Graph and Causality-Seeking Agent provide a robust framework for modeling dynamic causal mechanisms in complex environments.

Abstract: When building a world model, a common assumption is that the environment has
a single, unchanging underlying causal rule, like applying Newton's laws to
every situation. In reality, what appears as a drifting causal mechanism is
often the manifestation of a fixed underlying mechanism seen through a narrow
observational window. This brings about a problem that, when building a world
model, even subtle shifts in policy or environment states can alter the very
observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal
Graph} as world models, a minimal unified representation that efficiently
encodes the transformation rules governing how causal structures shift across
different latent world states. A single Meta-Causal Graph is composed of
multiple causal subgraphs, each triggered by meta state, which is in the latent
state space. Building on this representation, we introduce a
\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta
states that trigger each subgraph, (2) discover the corresponding causal
relationships by agent curiosity-driven intervention policy, and (3)
iteratively refine the Meta-Causal Graph through ongoing curiosity-driven
exploration and agent experiences. Experiments on both synthetic tasks and a
challenging robot arm manipulation task demonstrate that our method robustly
captures shifts in causal dynamics and generalizes effectively to previously
unseen contexts.

</details>


### [644] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/pdf/2506.23145)
*Shahad Hardan, Darya Taratynova, Abdelmajid Essofi, Karthik Nandakumar, Mohammad Yaqub*

Main category: cs.LG

TL;DR: Forget-MI is a machine unlearning method for multimodal medical data, improving privacy by removing sensitive data while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Privacy preservation in AI, especially in healthcare, is critical due to sensitive patient data. Existing methods struggle with unlearning in multimodal architectures.

Method: Forget-MI uses loss functions and perturbation techniques to unlearn unimodal and joint representations of data while preserving knowledge from remaining data.

Result: The method reduces Membership Inference Attack (MIA) by 0.202, decreases AUC and F1 scores on the forget set, and matches test set performance of the original model.

Conclusion: Forget-MI effectively unlearns sensitive data in multimodal models, enhancing privacy without compromising performance.

Abstract: Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [645] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/pdf/2506.23147)
*Jonathan Schuster, Fabian Transchel*

Main category: cs.LG

TL;DR: The paper introduces the maneuverRecognition package for automating driving maneuver recognition in vehicle telematics, addressing data preprocessing, modeling, and evaluation challenges.


<details>
  <summary>Details</summary>
Motivation: To enhance road safety, reduce accidents, and support eco-friendly driving by automating maneuver recognition, while providing practical tools for data transformation and model building.

Method: Developed the maneuverRecognition package with preprocessing, modeling, and evaluation functions, including a customizable LSTM-based network.

Result: Demonstrated the package's effectiveness using real driving data from smartphone sensors of three individuals.

Conclusion: The maneuverRecognition package offers a practical solution for maneuver recognition, bridging the gap between research and application.

Abstract: In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [646] [Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes](https://arxiv.org/pdf/2506.23165)
*David Bossens, Atsushi Nitanda*

Main category: cs.LG

TL;DR: The paper introduces mirror descent policy optimization for robust constrained Markov decision processes (RCMDPs), achieving convergence guarantees and improved robustness in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Addressing safety in reinforcement learning by ensuring long-term constraints and robustness under uncertainty.

Method: Uses policy gradient techniques to optimize policy and transition kernel via Lagrangian representation of constrained MDPs.

Result: Achieves O(1/T) and O(e^-T) convergence rates in oracle-based RCMDPs, and O~(1/T^(1/3)) in sample-based settings, with experimental validation.

Conclusion: Mirror descent policy optimization enhances robustness and performance in constrained and unconstrained settings compared to baselines.

Abstract: Safety is an essential requirement for reinforcement learning systems. The
newly emerging framework of robust constrained Markov decision processes allows
learning policies that satisfy long-term constraints while providing guarantees
under epistemic uncertainty. This paper presents mirror descent policy
optimisation for robust constrained Markov decision processes (RCMDPs), making
use of policy gradient techniques to optimise both the policy (as a maximiser)
and the transition kernel (as an adversarial minimiser) on the Lagrangian
representing a constrained MDP. In the oracle-based RCMDP setting, we obtain an
$\mathcal{O}\left(\frac{1}{T}\right)$ convergence rate for the squared distance
as a Bregman divergence, and an $\mathcal{O}\left(e^{-T}\right)$ convergence
rate for entropy-regularised objectives. In the sample-based RCMDP setting, we
obtain an $\tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$ convergence rate.
Experiments confirm the benefits of mirror descent policy optimisation in
constrained and unconstrained optimisation, and significant improvements are
observed in robustness tests when compared to baseline policy optimisation
algorithms.

</details>


### [647] [MedLeak: Multimodal Medical Data Leakage in Secure Federated Learning with Crafted Models](https://arxiv.org/pdf/2407.09972)
*Shanghao Shi, Md Shahedul Haque, Abhijeet Parida, Chaoyu Zhang, Marius George Linguraru, Y. Thomas Hou, Syed Muhammad Anwar, Wenjing Lou*

Main category: cs.LG

TL;DR: The paper introduces MedLeak, a privacy attack in federated learning (FL) that recovers private medical data from client updates, bypassing secure aggregation protocols.


<details>
  <summary>Details</summary>
Motivation: To highlight vulnerabilities in FL systems, especially for sensitive medical data, by demonstrating a novel attack that exploits model updates.

Method: MedLeak uses an adversarially crafted model during FL training to recover private client data from aggregated updates without costly optimization.

Result: High recovery rates and strong quantitative scores on medical image and text datasets, with recovered data usable for downstream tasks like disease classification.

Conclusion: The study underscores the need for stronger privacy measures in FL to protect sensitive medical data from model inversion attacks.

Abstract: Federated learning (FL) allows participants to collaboratively train machine
learning models while keeping their data local, making it ideal for
collaborations among healthcare institutions on sensitive data. However, in
this paper, we propose a novel privacy attack called MedLeak, which allows a
malicious FL server to recover high-quality site-specific private medical data
from the client model updates. MedLeak works by introducing an adversarially
crafted model during the FL training process. Honest clients, unaware of the
insidious changes in the published models, continue to send back their updates
as per the standard FL protocol. Leveraging a novel analytical method, MedLeak
can efficiently recover private client data from the aggregated parameter
updates, eliminating costly optimization. In addition, the scheme relies solely
on the aggregated updates, thus rendering secure aggregation protocols
ineffective, as they depend on the randomization of intermediate results for
security while leaving the final aggregated results unaltered.
  We implement MedLeak on medical image datasets (MedMNIST, COVIDx CXR-4, and
Kaggle Brain Tumor MRI), as well as a medical text dataset (MedAbstract). The
results demonstrate that our attack achieves high recovery rates and strong
quantitative scores on both image and text datasets. We also thoroughly
evaluate MedLeak across different attack parameters, providing insights into
key factors that influence attack performance and potential defenses.
Furthermore, we demonstrate that the recovered data can support downstream
tasks such as disease classification with minimal performance loss. Our
findings validate the need for enhanced privacy measures in FL systems,
particularly for safeguarding sensitive medical data against powerful model
inversion attacks.

</details>


### [648] [Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data](https://arxiv.org/pdf/2506.23174)
*Chen Gong, Bo Liang, Wei Gao, Chenren Xu*

Main category: cs.LG

TL;DR: The paper proposes metrics (affinity and diversity) to assess synthetic data quality in wireless sensing, introduces SynCheck for quality-guided data utilization, and shows improved performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current synthetic data in wireless sensing lacks predictable quality, leading to performance issues. The paper aims to address this by quantifying and improving synthetic data quality.

Method: Proposes affinity and diversity metrics to evaluate synthetic data quality. Introduces SynCheck, a scheme to refine synthetic data during training.

Result: SynCheck outperforms traditional methods, achieving a 4.3% performance improvement even when previous methods degrade performance by 13.4%.

Conclusion: Quality-guided synthetic data utilization (SynCheck) effectively addresses limitations in current synthetic data, enhancing wireless sensing task performance.

Abstract: Generative models have gained significant attention for their ability to
produce realistic synthetic data that supplements the quantity of real-world
datasets. While recent studies show performance improvements in wireless
sensing tasks by incorporating all synthetic data into training sets, the
quality of synthetic data remains unpredictable and the resulting performance
gains are not guaranteed. To address this gap, we propose tractable and
generalizable metrics to quantify quality attributes of synthetic data -
affinity and diversity. Our assessment reveals prevalent affinity limitation in
current wireless synthetic data, leading to mislabeled data and degraded task
performance. We attribute the quality limitation to generative models' lack of
awareness of untrained conditions and domain-specific processing. To mitigate
these issues, we introduce SynCheck, a quality-guided synthetic data
utilization scheme that refines synthetic data quality during task model
training. Our evaluation demonstrates that SynCheck consistently outperforms
quality-oblivious utilization of synthetic data, and achieves 4.3% performance
improvement even when the previous utilization degrades performance by 13.4%.

</details>


### [649] [Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data](https://arxiv.org/pdf/2506.23182)
*Robert Frank, Michael Widrich, Rahmad Akbar, Günter Klambauer, Geir Kjetil Sandve, Philippe A. Robert, Victor Greiff*

Main category: cs.LG

TL;DR: GAMA is an attribution method for generative models like LSTMs, enabling interpretability in biological sequence design without needing negative data.


<details>
  <summary>Details</summary>
Motivation: Generative models lack interpretability, hindering biological insights. GAMA addresses this gap.

Method: Developed GAMA using Integrated Gradients, tested on synthetic and experimental antibody-antigen data.

Result: GAMA successfully recovers biologically relevant features and validates generative designs.

Conclusion: GAMA enhances interpretability and validation in generative sequence design, especially where negative data is scarce.

Abstract: Generative machine learning models offer a powerful framework for therapeutic
design by efficiently exploring large spaces of biological sequences enriched
for desirable properties. Unlike supervised learning methods, which require
both positive and negative labeled data, generative models such as LSTMs can be
trained solely on positively labeled sequences, for example, high-affinity
antibodies. This is particularly advantageous in biological settings where
negative data are scarce, unreliable, or biologically ill-defined. However, the
lack of attribution methods for generative models has hindered the ability to
extract interpretable biological insights from such models. To address this
gap, we developed Generative Attribution Metric Analysis (GAMA), an attribution
method for autoregressive generative models based on Integrated Gradients. We
assessed GAMA using synthetic datasets with known ground truths to characterize
its statistical behavior and validate its ability to recover biologically
relevant features. We further demonstrated the utility of GAMA by applying it
to experimental antibody-antigen binding data. GAMA enables model
interpretability and the validation of generative sequence design strategies
without the need for negative training data.

</details>


### [650] [Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs](https://arxiv.org/pdf/2506.23186)
*Marco Bressan, Victor Chepoi, Emmanuel Esposito, Maximilian Thiessen*

Main category: cs.LG

TL;DR: The paper studies monophonic halfspaces in graphs, introduces a decomposition theorem, and provides efficient learning algorithms, contrasting with the NP-hardness of geodesic halfspaces.


<details>
  <summary>Details</summary>
Motivation: To explore convexity notions in graphs and address learning problems efficiently, answering open questions in the literature.

Method: Uses a $2$-satisfiability based decomposition theorem to represent monophonic halfspaces as disjoint vertex subsets.

Result: Achieves efficient algorithms for teaching, active, online learning, and empirical risk minimization, along with a stable sample compression scheme.

Conclusion: Monophonic halfspaces are efficiently learnable, unlike geodesic halfspaces, resolving open questions and demonstrating practical applicability.

Abstract: Abstract notions of convexity over the vertices of a graph, and corresponding
notions of halfspaces, have recently gained attention from the machine learning
community. In this work we study monophonic halfspaces, a notion of graph
halfspaces defined through closure under induced paths. Our main result is a
$2$-satisfiability based decomposition theorem, which allows one to represent
monophonic halfspaces as a disjoint union of certain vertex subsets. Using this
decomposition, we achieve efficient and (nearly) optimal algorithms for various
learning problems, such as teaching, active, and online learning. Most notably,
we obtain a polynomial-time algorithm for empirical risk minimization.
Independently of the decomposition theorem, we obtain an efficient, stable, and
proper sample compression scheme. This makes monophonic halfspaces efficiently
learnable with proper learners and linear error rate $1/\varepsilon$ in the
realizable PAC setting. Our results answer open questions from the literature,
and show a stark contrast with geodesic halfspaces, for which most of the said
learning problems are NP-hard.

</details>


### [651] [External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting](https://arxiv.org/pdf/2506.23201)
*Haoran Li, Muhao Guo, Marija Ilic, Yang Weng, Guangchun Ruan*

Main category: cs.LG

TL;DR: The paper introduces M2oE2, a meta-representation framework using hypernetworks and Mixture-of-Experts to dynamically adapt load forecasting models based on external data, improving accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate residential load forecasting is crucial for power system reliability, but existing models inadequately handle external factors like weather and pricing, limiting their effectiveness.

Method: The authors propose a hypernetwork-based framework that modulates a base DL model using external data as meta-knowledge, integrating a MoE mechanism for selective expert activation and input filtering.

Result: M2oE2 outperforms state-of-the-art methods in accuracy and robustness across diverse datasets, with minimal additional overhead.

Conclusion: The framework successfully leverages external data to enhance load forecasting, offering a scalable and efficient solution for power systems.

Abstract: Accurate residential load forecasting is critical for power system
reliability with rising renewable integration and demand-side flexibility.
However, most statistical and machine learning models treat external factors,
such as weather, calendar effects, and pricing, as extra input, ignoring their
heterogeneity, and thus limiting the extraction of useful external information.
We propose a paradigm shift: external data should serve as meta-knowledge to
dynamically adapt the forecasting model itself. Based on this idea, we design a
meta-representation framework using hypernetworks that modulate selected
parameters of a base Deep Learning (DL) model in response to external
conditions. This provides both expressivity and adaptability. We further
integrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through
selective expert activation, while improving robustness by filtering redundant
external inputs. The resulting model, dubbed as a Meta Mixture of Experts for
External data (M2oE2), achieves substantial improvements in accuracy and
robustness with limited additional overhead, outperforming existing
state-of-the-art methods in diverse load datasets. The dataset and source code
are publicly available at
https://github.com/haorandd/M2oE2\_load\_forecast.git.

</details>


### [652] [FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model](https://arxiv.org/pdf/2506.23210)
*Taehwan Yoon, Bongjun Choi*

Main category: cs.LG

TL;DR: Proposes reference model-based federated learning for optimal fine-tuning, addressing catastrophic forgetting and improving model performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Federated learning ensures privacy but struggles with model performance and diverse user needs, requiring optimization and personalization.

Method: Uses Bayesian parameter-efficient transfer learning with an optimal proximal term and a reference model to prevent catastrophic forgetting.

Result: Achieves high model performance and low computing cost.

Conclusion: The method effectively balances privacy, performance, and efficiency in federated learning.

Abstract: Federated learning(FL) is used for distributed scenarios to train artificial
intelligence(AI) models while ensuring users' privacy. In federated learning
scenario, the server generally never knows about users' data. This type of
concept makes the AI training process efficient in terms of data privacy.
However, regarding model performance, federated AI models may not sufficiently
satisfy AI users' expectations. Furthermore, AI users have a wide range of
different needs. It is not easy to satisfy the whole users needs. These types
of issues can be addressed through AI model optimization, fine-tuning, or
personalization to achieve optimal model performance. To address model
optimization challenges, we propose reference model-based federated learning
for optimal fine-tuning, which overcomes catastrophic forgetting in each round.
This method is derived from Bayesian parameter-efficient transfer learning,
which includes an optimal proximal term and enables overcoming the catastrophic
forgetting issue in each round by utilizing a reference model that incorporates
previous model parameters. As a result, this method achieves both high model
performance and low computing cost.

</details>


### [653] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/pdf/2506.23221)
*Bálint Horváth, Balázs Csanád Csáji*

Main category: cs.LG

TL;DR: The paper introduces SGKI, a kernel-based method for image inpainting and super-resolution, providing pixel estimates with uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for reliable missing pixel estimation in images, including uncertainty measures, is crucial for tasks like inpainting and super-resolution.

Method: SGKI extends kernel methods, assuming data from RKHS, particularly band-limited functions, and uses Schur complements for efficient confidence band computation.

Result: SGKI successfully estimates missing pixels and provides non-asymptotic confidence bands, validated through experiments on synthetic and benchmark datasets.

Conclusion: SGKI offers a robust solution for image inpainting and super-resolution with uncertainty quantification, generalizable to vector-valued functions.

Abstract: The paper proposes a statistical learning approach to the problem of
estimating missing pixels of images, crucial for image inpainting and
super-resolution problems. One of the main novelties of the method is that it
also provides uncertainty quantifications together with the estimated values.
Our core assumption is that the underlying data-generating function comes from
a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on
band-limited functions, central to signal processing, which form Paley-Wiener
type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel
Interpolation (SGKI), is an extension and refinement of a recently developed
kernel method. An advantage of SGKI is that it not only estimates the missing
pixels, but also builds non-asymptotic confidence bands for the unobserved
values, which are simultaneously guaranteed for all missing pixels. We also
show how to compute these bands efficiently using Schur complements, we discuss
a generalization to vector-valued functions, and we present a series of
numerical experiments on various datasets containing synthetically generated
and benchmark images, as well.

</details>


### [654] [Masked Gated Linear Unit](https://arxiv.org/pdf/2506.23225)
*Yukito Tajima, Nakamasa Inoue, Yusuke Sekikawa, Ikuro Sato, Rio Yokota*

Main category: cs.LG

TL;DR: MGLUs introduce efficient GLUs with shared weight matrices and hardware-friendly kernels, reducing memory usage and speeding up inference while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: GLUs in LLMs require high memory reads due to separate weight matrices for gates and values, creating a bottleneck.

Method: Proposes Masked Gated Linear Units (MGLUs) with Mixture of Element-wise Gating (MoEG) and FlashMGLU kernel for efficiency.

Result: MGLUs achieve up to 19.7× speed-up, 47% memory efficiency, and match/surpass SwiGLU accuracy.

Conclusion: MGLUs offer a hardware-efficient alternative to GLUs without compromising performance in LLMs.

Abstract: Gated Linear Units (GLUs) have become essential components in the
feed-forward networks of state-of-the-art Large Language Models (LLMs).
However, they require twice as many memory reads compared to feed-forward
layers without gating, due to the use of separate weight matrices for the gate
and value streams. To address this bottleneck, we introduce Masked Gated Linear
Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.
The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating
(MoEG) architecture that learns multiple binary masks, each determining gate or
value assignments at the element level on a single shared weight matrix
resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly
kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive
PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs
despite added architectural complexity on an RTX5090 GPU. In LLM experiments,
the Swish-activated variant SwiMGLU preserves its memory advantages while
matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

</details>


### [655] [Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging](https://arxiv.org/pdf/2506.23266)
*Lujun Li, Zhu Qiyuan, Jiacheng Wang, Wei Li, Hao Gu, Sirui Han, Yike Guo*

Main category: cs.LG

TL;DR: Sub-MoE is a compression framework for Mixture of Experts (MoE) models, addressing parameter conflicts via subspace expert merging and adaptive clustering, maintaining high performance with significant expert reduction.


<details>
  <summary>Details</summary>
Motivation: MoE models face memory and deployment challenges due to their large scale, and existing merging methods suffer from parameter conflicts.

Method: Sub-MoE uses joint SVD on expert weights, adaptive clustering, and subspace merging to align and fuse experts in a shared subspace.

Result: Sub-MoE retains 96%|86% of performance with 25%|50% expert reduction on Mixtral-8x7B in zero-shot benchmarks.

Conclusion: Sub-MoE effectively compresses MoE models while preserving performance, outperforming existing methods.

Abstract: Mixture of Experts (MoE) LLMs face significant obstacles due to their massive
parameter scale, which imposes memory, storage, and deployment challenges.
Although recent expert merging methods promise greater efficiency by
consolidating multiple experts, they are fundamentally hindered by parameter
conflicts arising from expert specialization. In this paper, we present
Sub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key
insight is to perform joint Singular Value Decomposition (SVD) on concatenated
expert weights, reducing conflicting parameters by extracting shared
$U$-matrices while enabling effective merging of the expert-specific $V$
components. Specifically, Sub-MoE consists of two innovative phases: (1)
Adaptive Expert Clustering, which groups functionally coherent experts via
K-means clustering based on cosine similarity of expert outputs; and (2)
Subspace Expert Merging, which first enforces Experts Union Decomposition to
derive the shared $U$-matrix across experts in the same group, then pursues
frequency-based merging for individual $V$-matrices, and finalizes expert
reconstruction using the merged $V$-matrix. In this way, we align and fuse
experts in a shared subspace, and can be extended with intra-expert compression
for further inference optimization. Extensive experiments on Mixtral, DeepSeek,
and Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms
existing expert pruning and merging methods. Notably, our Sub-MoE maintains
96\%|86\% of original performance with 25\%|50\% expert reduction on
Mixtral-8x7B in zero-shot benchmarks. Code will be released at
https://github.com/lliai/MoERazor.

</details>


### [656] [Predicting thinking time in Reasoning models](https://arxiv.org/pdf/2506.23274)
*Hans Peter Lynsgøe Raaschou-jensen, Constanza Fierro, Anders Søgaard*

Main category: cs.LG

TL;DR: The paper addresses the unpredictability of reasoning time in models with hidden chains of thought, proposing methods to predict and display this time for better user experience.


<details>
  <summary>Details</summary>
Motivation: Users lack insight into how long models will spend reasoning, causing frustration, especially as models handle longer tasks asynchronously.

Method: Introduces and evaluates online and offline methods to predict model "thinking time" for a "progress bar for reasoning."

Result: Proposes practical solutions for predicting reasoning time, improving user interaction.

Conclusion: Highlights the need for predictable reasoning time in models and suggests future research directions.

Abstract: Reasoning models that produce long, hidden chains of thought have emerged as
powerful tools for complex, reasoning-intensive
tasks\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,
openai2024openaio1card}. However, this paradigm introduces a new user
experience challenge: users have little insight into how much time the model
will spend reasoning before returning an answer. This unpredictability, can
lead to user frustration and is likely to compound as LLMs can produce
increasingly long tasks asynchronously
\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and
evaluate methods for both online and offline prediction of model "thinking
time," aiming to develop a practical "progress bar for reasoning." We discuss
the implications for user interaction and future research directions.

</details>


### [657] [BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition](https://arxiv.org/pdf/2506.23280)
*Chaoqun Du, Yulin Wang, Shiji Song, Gao Huang*

Main category: cs.LG

TL;DR: The paper introduces BAPE, a method to explicitly model posterior probabilities for Bayes classifier in long-tailed data, addressing gradient imbalance and ensuring optimal decisions.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods implicitly estimate posterior probabilities, failing in long-tailed data distributions due to gradient imbalance and suboptimal decisions.

Method: BAPE explicitly models posterior probabilities using point estimation, directly learning the Bayes classifier without gradient descent. It includes a distribution adjustment technique for adapting to test data.

Result: BAPE significantly improves generalization on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist datasets.

Conclusion: BAPE offers a simple yet effective solution for long-tailed data, orthogonal to existing methods, ensuring Bayes optimality and improved performance.

Abstract: Bayesian decision theory advocates the Bayes classifier as the optimal
approach for minimizing the risk in machine learning problems. Current deep
learning algorithms usually solve for the optimal classifier by
\emph{implicitly} estimating the posterior probabilities, \emph{e.g.}, by
minimizing the Softmax cross-entropy loss. This simple methodology has been
proven effective for meticulously balanced academic benchmark datasets.
However, it is not applicable to the long-tailed data distributions in the real
world, where it leads to the gradient imbalance issue and fails to ensure the
Bayes optimal decision rule. To address these challenges, this paper presents a
novel approach (BAPE) that provides a more precise theoretical estimation of
the data distributions by \emph{explicitly} modeling the parameters of the
posterior probabilities and solving them with point estimation. Consequently,
our method directly learns the Bayes classifier without gradient descent based
on Bayes' theorem, simultaneously alleviating the gradient imbalance and
ensuring the Bayes optimal decision rule. Furthermore, we propose a
straightforward yet effective \emph{distribution adjustment} technique. This
method enables the Bayes classifier trained from the long-tailed training set
to effectively adapt to the test data distribution with an arbitrary imbalance
factor, thereby enhancing performance without incurring additional
computational costs. In addition, we demonstrate the gains of our method are
orthogonal to existing learning approaches for long-tailed scenarios, as they
are mostly designed under the principle of \emph{implicitly} estimating the
posterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method
significantly improves the generalization performance of popular deep networks,
despite its simplicity.

</details>


### [658] [Not All Explanations for Deep Learning Phenomena Are Equally Valuable](https://arxiv.org/pdf/2506.23286)
*Alan Jeffares, Mihaela van der Schaar*

Main category: cs.LG

TL;DR: The paper critiques the focus on isolated, counterintuitive deep learning phenomena, arguing they lack real-world relevance and inefficiently drive progress. It suggests using them to refine broader theories instead.


<details>
  <summary>Details</summary>
Motivation: To challenge the current research focus on isolated deep learning phenomena (e.g., double descent, grokking) and advocate for aligning research with broader, practical goals.

Method: Analyzes prominent examples of these phenomena and their research outcomes, revisits current research norms, and proposes practical recommendations.

Result: Finds little evidence of real-world relevance for these phenomena and suggests they are better used to refine general deep learning theories.

Conclusion: Recommends shifting focus from isolated puzzles to broader principles to ensure research aligns with practical progress in deep learning.

Abstract: Developing a better understanding of surprising or counterintuitive phenomena
has constituted a significant portion of deep learning research in recent
years. These include double descent, grokking, and the lottery ticket
hypothesis -- among many others. Works in this area often develop ad hoc
hypotheses attempting to explain these observed phenomena on an isolated,
case-by-case basis. This position paper asserts that, in many prominent cases,
there is little evidence to suggest that these phenomena appear in real-world
applications and these efforts may be inefficient in driving progress in the
broader field. Consequently, we argue against viewing them as isolated puzzles
that require bespoke resolutions or explanations. However, despite this, we
suggest that deep learning phenomena do still offer research value by providing
unique settings in which we can refine our broad explanatory theories of more
general deep learning principles. This position is reinforced by analyzing the
research outcomes of several prominent examples of these phenomena from the
recent literature. We revisit the current norms in the research community in
approaching these problems and propose practical recommendations for future
research, aiming to ensure that progress on deep learning phenomena is well
aligned with the ultimate pragmatic goal of progress in the broader field of
deep learning.

</details>


### [659] [Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis](https://arxiv.org/pdf/2506.23287)
*Zelin Zang, WenZhe Li, Fei Chen, Yongjie Xu, Chang Yu, Zhen Lei, Stan Z. Li*

Main category: cs.LG

TL;DR: HDTree is a diffusion-based method for modeling hierarchical single-cell data, outperforming traditional and VAE-based approaches in accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and recent VAEs struggle with computational cost, performance, and capturing deep hierarchical relationships in single-cell data.

Method: HDTree uses a unified hierarchical codebook and quantized diffusion processes to model tree node transitions, eliminating branch-specific modules.

Result: HDTree outperforms existing methods in accuracy and performance on general-purpose and single-cell datasets.

Conclusion: HDTree provides a robust tool for hierarchical lineage analysis, improving cellular differentiation modeling and biological insights.

Abstract: In single-cell research, tracing and analyzing high-throughput single-cell
differentiation trajectories is crucial for understanding complex biological
processes. Key to this is the modeling and generation of hierarchical data that
represents the intrinsic structure within datasets. Traditional methods face
limitations in terms of computational cost, performance, generative capacity,
and stability. Recent VAEs based approaches have made strides in addressing
these challenges but still require specialized network modules for each tree
branch, limiting their stability and ability to capture deep hierarchical
relationships. To overcome these challenges, we introduce diffusion-based
approach called HDTree. HDTree captures tree relationships within a
hierarchical latent space using a unified hierarchical codebook and quantized
diffusion processes to model tree node transitions. This method improves
stability by eliminating branch-specific modules and enhancing generative
capacity through gradual hierarchical changes simulated by the diffusion
process. HDTree's effectiveness is demonstrated through comparisons on both
general-purpose and single-cell datasets, where it outperforms existing methods
in terms of accuracy and performance. These contributions provide a new tool
for hierarchical lineage analysis, enabling more accurate and efficient
modeling of cellular differentiation paths and offering insights for downstream
biological tasks. The code of HDTree is available at anonymous link
https://anonymous.4open.science/r/code_HDTree_review-A8DB.

</details>


### [660] [VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design](https://arxiv.org/pdf/2506.23339)
*Malikussaid, Hilal Hudan Nuha*

Main category: cs.LG

TL;DR: VALID-Mol improves LLM-driven molecular design by integrating chemical validation, increasing valid structure generation from 3% to 83%.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLMs generating chemically invalid or impractical structures in molecular design for drug discovery.

Method: Combines prompt engineering, automated chemical validation, and a fine-tuned domain-adapted LLM.

Result: Achieves 83% valid chemical structures, with up to 17-fold predicted improvements in target affinity.

Conclusion: Offers a generalizable methodology for scientifically-constrained LLM applications, ensuring reliability and reproducibility.

Abstract: Large Language Models (LLMs) demonstrate remarkable potential for scientific
discovery, but their application in domains requiring factual accuracy and
domain-specific constraints remains challenging. In molecular design for drug
discovery, LLMs can suggest creative molecular modifications but often produce
chemically invalid or impractical structures. We present VALID-Mol, a
systematic framework for integrating chemical validation with LLM-driven
molecular design that increases the rate of generating valid chemical
structures from 3% to 83%. Our approach combines methodical prompt engineering,
automated chemical validation, and a fine-tuned domain-adapted LLM to ensure
reliable generation of synthesizable molecules with improved properties. Beyond
the specific implementation, we contribute a generalizable methodology for
scientifically-constrained LLM applications, with quantifiable reliability
improvements. Computational predictions suggest our framework can generate
promising candidates for synthesis with up to 17-fold computationally predicted
improvements in target affinity while maintaining synthetic accessibility. We
provide a detailed analysis of our prompt engineering process, validation
architecture, and fine-tuning approach, offering a reproducible blueprint for
applying LLMs to other scientific domains where domain-specific validation is
essential.

</details>


### [661] [A case for data valuation transparency via DValCards](https://arxiv.org/pdf/2506.23349)
*Keziah Naggita, Julienne LaChance*

Main category: cs.LG

TL;DR: Data valuation methods in ML are biased and unstable, affected by pre-processing and subsampling, leading to ethical concerns. A new framework, DValCards, is proposed for transparency.


<details>
  <summary>Details</summary>
Motivation: To highlight biases and instability in data valuation methods and their ethical implications in ML and data markets.

Method: Analyzed 9 tabular datasets and 6 valuation methods, examining effects of pre-processing, subsampling, and bias.

Result: Found data valuation metrics are easily altered, increase class imbalance, and undervalue underrepresented groups.

Conclusion: Advocates for transparency via DValCards to mitigate misuse and promote trust in ML systems.

Abstract: Following the rise in popularity of data-centric machine learning (ML),
various data valuation methods have been proposed to quantify the contribution
of each datapoint to desired ML model performance metrics (e.g., accuracy).
Beyond the technical applications of data valuation methods (e.g., data
cleaning, data acquisition, etc.), it has been suggested that within the
context of data markets, data buyers might utilize such methods to fairly
compensate data owners. Here we demonstrate that data valuation metrics are
inherently biased and unstable under simple algorithmic design choices,
resulting in both technical and ethical implications. By analyzing 9 tabular
classification datasets and 6 data valuation methods, we illustrate how (1)
common and inexpensive data pre-processing techniques can drastically alter
estimated data values; (2) subsampling via data valuation metrics may increase
class imbalance; and (3) data valuation metrics may undervalue underrepresented
group data. Consequently, we argue in favor of increased transparency
associated with data valuation in-the-wild and introduce the novel Data
Valuation Cards (DValCards) framework towards this aim. The proliferation of
DValCards will reduce misuse of data valuation metrics, including in data
pricing, and build trust in responsible ML systems.

</details>


### [662] [Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment](https://arxiv.org/pdf/2506.23358)
*Pawel Renc, Michal K. Grzeszczyk, Linglong Qian, Nassim Oufattole, Jeff Rasley, Arkadiusz Sitek*

Main category: cs.LG

TL;DR: FTS is a federated framework for training generative models on distributed EHR data, ensuring privacy and scalability while achieving performance comparable to real data.


<details>
  <summary>Details</summary>
Motivation: To enable training of generative models on distributed timeseries EHR data without compromising privacy, while maintaining performance.

Method: Tokenizes patient history as PHTs, trains local autoregressive transformers, aggregates model weights centrally, and synthesizes data for a Global Generator.

Result: GG-trained models perform comparably to real-data models on clinical prediction tasks.

Conclusion: FTS provides privacy, scalability, and extensibility for healthcare applications like counterfactual inference and synthetic trials.

Abstract: We present Federated Timeline Synthesis (FTS), a novel framework for training
generative foundation models across distributed timeseries data applied to
electronic health records (EHR). At its core, FTS represents patient history as
tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding
temporal, categorical, and continuous clinical information. Each institution
trains an autoregressive transformer on its local PHTs and transmits only model
weights to a central server. The server uses the generators to synthesize a
large corpus of trajectories and train a Global Generator (GG), enabling
zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS
on five clinically meaningful prediction tasks using MIMIC-IV data, showing
that models trained on synthetic data generated by GG perform comparably to
those trained on real data. FTS offers strong privacy guarantees, scalability
across institutions, and extensibility to diverse prediction and simulation
tasks especially in healthcare, including counterfactual inference, early
warning detection, and synthetic trial design.

</details>


### [663] [When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery](https://arxiv.org/pdf/2506.23374)
*Dominik Meier, Sujai Hiremath, Promit Ghosal, Kyra Gan*

Main category: cs.LG

TL;DR: The paper addresses the challenge of causal discovery in bivariate data with unmeasured mediators, proposing a new method (BiDD) that outperforms existing approaches.


<details>
  <summary>Details</summary>
Motivation: Standard additive noise models (ANMs) fail when unobserved mediators corrupt causal relationships, and prior solutions are brittle in finite samples.

Method: The paper introduces Bivariate Denoising Diffusion (BiDD), which uses a novel independence test statistic during noising/denoising processes to infer causality.

Result: BiDD shows consistent performance in synthetic and real-world data, excelling in mediator-corrupted settings and maintaining robustness in mediator-free cases.

Conclusion: BiDD is a promising solution for causal discovery under hidden mediation, offering asymptotic consistency and practical utility.

Abstract: Distinguishing cause and effect from bivariate observational data is a
foundational problem in many disciplines, but challenging without additional
assumptions. Additive noise models (ANMs) are widely used to enable
sample-efficient bivariate causal discovery. However, conventional ANM-based
methods fail when unobserved mediators corrupt the causal relationship between
variables. This paper makes three key contributions: first, we rigorously
characterize why standard ANM approaches break down in the presence of
unmeasured mediators. Second, we demonstrate that prior solutions for hidden
mediation are brittle in finite sample settings, limiting their practical
utility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)
for causal discovery, a method designed to handle latent noise introduced by
unmeasured mediators. Unlike prior methods that infer directionality through
mean squared error loss comparisons, our approach introduces a novel
independence test statistic: during the noising and denoising processes for
each variable, we condition on the other variable as input and evaluate the
independence of the predicted noise relative to this input. We prove asymptotic
consistency of BiDD under the ANM, and conjecture that it performs well under
hidden mediation. Experiments on synthetic and real-world data demonstrate
consistent performance, outperforming existing methods in mediator-corrupted
settings while maintaining strong performance in mediator-free settings.

</details>


### [664] [Do LLMs Dream of Discrete Algorithms?](https://arxiv.org/pdf/2506.23408)
*Claudionor Coelho Jr, Yanen Li, Philip Tee*

Main category: cs.LG

TL;DR: The paper proposes a neurosymbolic approach combining LLMs with logic-based reasoning to address limitations in logical reasoning and interpretability, showing improved performance in multi-step tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs lack effectiveness in strict logical reasoning and interpretability, limiting their use in complex domains requiring reliability and precision.

Method: The paper integrates LLMs with logic-based reasoning modules (e.g., Prolog predicates) to decompose queries into verifiable sub-tasks and orchestrate solutions.

Result: Experiments on the DABStep benchmark demonstrate improved precision, coverage, and documentation in multi-step reasoning tasks.

Conclusion: Combining LLMs with logic reasoning enhances reliability, interpretability, and scalability for trustworthy AI agents in complex domains.

Abstract: Large Language Models (LLMs) have rapidly transformed the landscape of
artificial intelligence, enabling natural language interfaces and dynamic
orchestration of software components. However, their reliance on probabilistic
inference limits their effectiveness in domains requiring strict logical
reasoning, discrete decision-making, and robust interpretability. This paper
investigates these limitations and proposes a neurosymbolic approach that
augments LLMs with logic-based reasoning modules, particularly leveraging
Prolog predicates and composable toolsets. By integrating first-order logic and
explicit rule systems, our framework enables LLMs to decompose complex queries
into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common
failure modes such as hallucination and incorrect step decomposition. We
demonstrate the practical benefits of this hybrid architecture through
experiments on the DABStep benchmark, showing improved precision, coverage, and
system documentation in multi-step reasoning tasks. Our results indicate that
combining LLMs with modular logic reasoning restores engineering rigor,
enhances system reliability, and offers a scalable path toward trustworthy,
interpretable AI agents across complex domains.

</details>


### [665] [BenchMake: Turn any scientific data set into a reproducible benchmark](https://arxiv.org/pdf/2506.23419)
*Amanda S Barnard*

Main category: cs.LG

TL;DR: A new tool, BenchMake, is introduced to transform scientific datasets into benchmarks by identifying edge cases and partitioning data for robust testing.


<details>
  <summary>Details</summary>
Motivation: The rarity of benchmark sets in computational science hinders innovation evaluation, prompting the need for a tool to convert open datasets into benchmarks.

Method: BenchMake uses non-negative matrix factorization to isolate challenging edge cases and partitions data to maximize divergence and statistical significance.

Result: Tested on ten public benchmark sets, BenchMake outperforms established and random splits across diverse data modalities.

Conclusion: BenchMake enables robust benchmarking by leveraging open datasets, aiding computational science innovation.

Abstract: Benchmark data sets are a cornerstone of machine learning development and
applications, ensuring new methods are robust, reliable and competitive. The
relative rarity of benchmark sets in computational science, due to the
uniqueness of the problems and the pace of change in the associated domains,
makes evaluating new innovations difficult for computational scientists. In
this paper a new tool is developed and tested to potentially turn any of the
increasing numbers of scientific data sets made openly available into a
benchmark accessible to the community. BenchMake uses non-negative matrix
factorisation to deterministically identify and isolate challenging edge cases
on the convex hull (the smallest convex set that contains all existing data
instances) and partitions a required fraction of matched data instances into a
testing set that maximises divergence and statistical significance, across
tabular, graph, image, signal and textual modalities. BenchMake splits are
compared to establish splits and random splits using ten publicly available
benchmark sets from different areas of science, with different sizes, shapes,
distributions.

</details>


### [666] [Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting](https://arxiv.org/pdf/2506.23424)
*Heitor R. Medeiros, Hossein Sharifi-Noghabi, Gabriel L. Oliveira, Saghar Irandoust*

Main category: cs.LG

TL;DR: PETSA is a parameter-efficient test-time adaptation method for time series forecasting, updating only small calibration modules to reduce costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-world time series are non-stationary, degrading pre-trained models' performance. Existing TTA methods update full models, increasing resource costs.

Method: PETSA updates small input/output calibration modules using low-rank adapters and dynamic gating. It employs a specialized loss with robust, frequency-domain, and patch-wise structural terms.

Result: PETSA improves adaptability of forecasting backbones with fewer parameters, achieving competitive or better performance on benchmark datasets.

Conclusion: PETSA offers an efficient and effective solution for test-time adaptation in time series forecasting.

Abstract: Real-world time series often exhibit a non-stationary nature, degrading the
performance of pre-trained forecasting models. Test-Time Adaptation (TTA)
addresses this by adjusting models during inference, but existing methods
typically update the full model, increasing memory and compute costs. We
propose PETSA, a parameter-efficient method that adapts forecasters at test
time by only updating small calibration modules on the input and output. PETSA
uses low-rank adapters and dynamic gating to adjust representations without
retraining. To maintain accuracy despite limited adaptation capacity, we
introduce a specialized loss combining three components: (1) a robust term, (2)
a frequency-domain term to preserve periodicity, and (3) a patch-wise
structural term for structural alignment. PETSA improves the adaptability of
various forecasting backbones while requiring fewer parameters than baselines.
Experimental results on benchmark datasets show that PETSA achieves competitive
or better performance across all horizons. Our code is available at:
https://github.com/BorealisAI/PETSA

</details>


### [667] [Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders](https://arxiv.org/pdf/2506.23446)
*Mohamed Elbasheer, Adewale Akinfaderin*

Main category: cs.LG

TL;DR: The paper introduces a User-Based Sequencing (UBS) method with a Transformer Encoder for insider threat detection, achieving high accuracy and low error rates.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture sequential dependencies in user behavior, limiting their effectiveness in detecting subtle insider threats.

Method: UBS transforms user activity into temporal sequences, uses a Transformer Encoder for modeling, and evaluates anomaly scores with unsupervised outlier detection algorithms.

Result: The method achieves 96.61% accuracy, 99.43% recall, and low false positive/negative rates, outperforming baselines.

Conclusion: Sequential user modeling and advanced anomaly detection are highly effective for insider threat detection.

Abstract: Insider threat detection presents unique challenges due to the authorized
status of malicious actors and the subtlety of anomalous behaviors. Existing
machine learning methods often treat user activity as isolated events, thereby
failing to leverage sequential dependencies in user behavior. In this study, we
propose a User-Based Sequencing (UBS) methodology, transforming the CERT
insider threat dataset into structured temporal sequences suitable for deep
sequential modeling. We deploy a Transformer Encoder architecture to model
benign user activity and employ its reconstruction errors as anomaly scores.
These scores are subsequently evaluated using three unsupervised outlier
detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and
Isolation Forest (iForest). Across four rigorously designed test sets,
including combinations of multiple CERT dataset releases, our UBS-Transformer
pipeline consistently achieves state-of-the-art performance - notably 96.61%
accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low
false negative (0.0057) and false positive (0.0571) rates. Comparative analyses
demonstrate that our approach substantially outperforms tabular and
conventional autoencoder baselines, underscoring the efficacy of sequential
user modeling and advanced anomaly detection in the insider threat domain.

</details>


### [668] [Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification](https://arxiv.org/pdf/2506.23462)
*Manaswi Kulahara, Gautam Siddharth Kashyap, Nipun Joshi, Arpita Soni*

Main category: cs.LG

TL;DR: DisasterNet-LLM, a specialized LLM, outperforms state-of-the-art models in multimodal disaster classification with 89.5% accuracy, 88.0% F1, 0.92 AUC, and 0.88 BERTScore.


<details>
  <summary>Details</summary>
Motivation: Traditional methods fail to integrate multimodal data (images, weather, text) effectively for disaster management.

Method: Uses advanced pretraining, cross-modal attention, and adaptive transformers.

Result: Achieves 89.5% accuracy, 88.0% F1, 0.92 AUC, and 0.88 BERTScore.

Conclusion: DisasterNet-LLM is highly effective for comprehensive disaster analysis.

Abstract: Effective disaster management requires timely and accurate insights, yet
traditional methods struggle to integrate multimodal data such as images,
weather records, and textual reports. To address this, we propose
DisasterNet-LLM, a specialized Large Language Model (LLM) designed for
comprehensive disaster analysis. By leveraging advanced pretraining,
cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM
excels in disaster classification. Experimental results demonstrate its
superiority over state-of-the-art models, achieving higher accuracy of 89.5%,
an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal
disaster classification tasks.

</details>


### [669] [Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection](https://arxiv.org/pdf/2506.23469)
*Chunjing Xiao, Jiahui Lu, Xovee Xu, Fan Zhou, Tianshu Xie, Wei Lu, Lifeng Xu*

Main category: cs.LG

TL;DR: TripleAD is a triple-channel framework for graph anomaly detection, addressing attribute, structural, and mixed anomalies via mutual distillation and specialized modules.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised methods struggle with balancing attribute and structural anomaly detection, leading to suboptimal performance.

Method: TripleAD uses three modules: multiscale attribute estimation, link-enhanced structure estimation, and attribute-mixed curvature, with mutual distillation for collaboration.

Result: TripleAD outperforms baselines in detecting various anomaly types.

Conclusion: The framework effectively mitigates interference between anomaly types and improves detection performance.

Abstract: Graph anomaly detection is critical in domains such as healthcare and
economics, where identifying deviations can prevent substantial losses.
Existing unsupervised approaches strive to learn a single model capable of
detecting both attribute and structural anomalies. However, they confront the
tug-of-war problem between two distinct types of anomalies, resulting in
suboptimal performance. This work presents TripleAD, a mutual
distillation-based triple-channel graph anomaly detection framework. It
includes three estimation modules to identify the attribute, structural, and
mixed anomalies while mitigating the interference between different types of
anomalies. In the first channel, we design a multiscale attribute estimation
module to capture extensive node interactions and ameliorate the over-smoothing
issue. To better identify structural anomalies, we introduce a link-enhanced
structure estimation module in the second channel that facilitates information
flow to topologically isolated nodes. The third channel is powered by an
attribute-mixed curvature, a new indicator that encapsulates both attribute and
structural information for discriminating mixed anomalies. Moreover, a mutual
distillation strategy is introduced to encourage communication and
collaboration between the three channels. Extensive experiments demonstrate the
effectiveness of the proposed TripleAD model against strong baselines.

</details>


### [670] [Sample Margin-Aware Recalibration of Temperature Scaling](https://arxiv.org/pdf/2506.23492)
*Haolan Guo, Linwei Tao, Haoyang Luo, Minjing Dong, Chang Xu*

Main category: cs.LG

TL;DR: SMART is a lightweight recalibration method for neural networks that uses the logit gap to improve calibration efficiently.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks are overconfident, posing risks in safety-critical applications, and current calibration methods struggle with bias-variance trade-offs.

Method: SMART scales logits based on the logit gap (margin between top two logits) and uses a SoftECE objective for adaptive binning.

Result: SMART achieves state-of-the-art calibration with fewer parameters and limited data.

Conclusion: SMART provides a robust, efficient solution for uncertainty quantification in neural networks.

Abstract: Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer from high variance due to noisy
high-dimensional inputs and insufficient validation data. To address these
challenges, we propose Sample Margin-Aware Recalibration of Temperature
(SMART), a lightweight, data-efficient recalibration method that precisely
scales logits based on the margin between the top two logits -- termed the
logit gap. Specifically, the logit gap serves as a denoised, scalar signal
directly tied to decision boundary uncertainty, providing a robust indicator
that avoids the noise inherent in high-dimensional logit spaces while
preserving model prediction invariance. Meanwhile, SMART employs a novel
soft-binned Expected Calibration Error (SoftECE) objective that balances model
bias and variance through adaptive binning, enabling stable parameter updates
even with extremely limited calibration data. Extensive evaluations across
diverse datasets and architectures demonstrate that SMART achieves
state-of-the-art calibration performance even with substantially fewer
parameters compared to existing parametric methods, offering a principled,
robust, and highly efficient solution for practical uncertainty quantification
in neural network predictions. The source code is available at:
https://anonymous.4open.science/r/SMART-8B11.

</details>


### [671] [FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](https://arxiv.org/pdf/2506.23516)
*Seung-Wook Kim, Seongyeol Kim, Jiah Kim, Seowon Ji, Se-Ho Lee*

Main category: cs.LG

TL;DR: FedWSQ improves federated learning by combining weight standardization and distribution-aware non-uniform quantization, addressing data heterogeneity and communication constraints.


<details>
  <summary>Details</summary>
Motivation: Federated learning suffers from performance issues due to data heterogeneity and communication constraints, necessitating a robust solution.

Method: FedWSQ integrates weight standardization (WS) to filter biased local updates and distribution-aware non-uniform quantization (DANUQ) to minimize quantization errors.

Result: FedWSQ reduces communication overhead while maintaining high model accuracy, outperforming existing methods in challenging FL settings.

Conclusion: FedWSQ effectively addresses key FL challenges, offering superior performance in heterogeneous and low-bit communication scenarios.

Abstract: Federated learning (FL) often suffers from performance degradation due to key
challenges such as data heterogeneity and communication constraints. To address
these limitations, we present a novel FL framework called FedWSQ, which
integrates weight standardization (WS) and the proposed distribution-aware
non-uniform quantization (DANUQ). WS enhances FL performance by filtering out
biased components in local updates during training, thereby improving the
robustness of the model against data heterogeneity and unstable client
participation. In addition, DANUQ minimizes quantization errors by leveraging
the statistical properties of local model updates. As a result, FedWSQ
significantly reduces communication overhead while maintaining superior model
accuracy. Extensive experiments on FL benchmark datasets demonstrate that
FedWSQ consistently outperforms existing FL methods across various challenging
FL settings, including extreme data heterogeneity and ultra-low-bit
communication scenarios.

</details>


### [672] [Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size](https://arxiv.org/pdf/2506.23544)
*Kento Imaizumi, Hideaki Iiduka*

Main category: cs.LG

TL;DR: QHM, a momentum method, is analyzed for stochastic nonconvex optimization. Asymptotic convergence requires decaying learning rates or increasing batch sizes. Increasing batch sizes without decaying learning rates proves more effective.


<details>
  <summary>Details</summary>
Motivation: To understand the effectiveness of momentum methods like QHM in stochastic nonconvex optimization, particularly for deep neural networks.

Method: Analyzes mini-batch QHM with increasing batch sizes, comparing it to decaying learning rates.

Result: Asymptotic convergence is achievable with increasing batch sizes, which also benefits non-asymptotic performance. Experiments confirm practical advantages.

Conclusion: Increasing batch sizes in QHM is a viable alternative to decaying learning rates, offering better performance in neural network training.

Abstract: Momentum methods were originally introduced for their superiority to
stochastic gradient descent (SGD) in deterministic settings with convex
objective functions. However, despite their widespread application to deep
neural networks -- a representative case of stochastic nonconvex optimization
-- the theoretical justification for their effectiveness in such settings
remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that
generalizes various momentum methods and has been studied to better understand
the class of momentum-based algorithms as a whole. In this paper, we provide
both asymptotic and non-asymptotic convergence results for mini-batch QHM with
an increasing batch size. We show that achieving asymptotic convergence
requires either a decaying learning rate or an increasing batch size. Since a
decaying learning rate adversely affects non-asymptotic convergence, we
demonstrate that using mini-batch QHM with an increasing batch size -- without
decaying the learning rate -- can be a more effective strategy. Our experiments
show that even a finite increase in batch size can provide benefits for
training neural networks.

</details>


### [673] [A unified framework on the universal approximation of transformer-type architectures](https://arxiv.org/pdf/2506.23551)
*Jingpu Cheng, Qianxiao Li, Ting Lin, Zuowei Shen*

Main category: cs.LG

TL;DR: The paper explores the universal approximation property (UAP) in transformer architectures, introducing a unified framework and token distinguishability as a key requirement. It simplifies UAP verification and extends results to various attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To extend theoretical understanding of UAP to transformer architectures, unifying prior work on residual networks and attention mechanisms.

Method: Introduces token distinguishability as a requirement and a general sufficient condition for UAP. Uses analyticity assumptions to simplify verification and applies the framework to transformers with diverse attention mechanisms.

Result: Proves UAP for transformers with kernel-based and sparse attention, generalizing prior work. Provides a foundation for designing new architectures with UAP guarantees.

Conclusion: The framework advances theoretical understanding of transformers and enables principled design of architectures with inherent UAP.

Abstract: We investigate the universal approximation property (UAP) of transformer-type
architectures, providing a unified theoretical framework that extends prior
results on residual networks to models incorporating attention mechanisms. Our
work identifies token distinguishability as a fundamental requirement for UAP
and introduces a general sufficient condition that applies to a broad class of
architectures. Leveraging an analyticity assumption on the attention layer, we
can significantly simplify the verification of this condition, providing a
non-constructive approach in establishing UAP for such architectures. We
demonstrate the applicability of our framework by proving UAP for transformers
with various attention mechanisms, including kernel-based and sparse attention
mechanisms. The corollaries of our results either generalize prior works or
establish UAP for architectures not previously covered. Furthermore, our
framework offers a principled foundation for designing novel transformer
architectures with inherent UAP guarantees, including those with specific
functional symmetries. We propose examples to illustrate these insights.

</details>


### [674] [Transition Matching: Scalable and Flexible Generative Modeling](https://arxiv.org/pdf/2506.23589)
*Neta Shaul, Uriel Singer, Itai Gat, Yaron Lipman*

Main category: cs.LG

TL;DR: The paper introduces Transition Matching (TM), a novel generative paradigm unifying diffusion/flow models and continuous autoregressive (AR) generation, offering flexibility and improved performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations in diffusion/flow models and unify text/media generation, leveraging continuous AR models for better design flexibility.

Method: TM decomposes generation tasks into Markov transitions, introducing three variants: DTM (generalizes flow matching), ARTM (partially causal), and FHTM (fully causal).

Result: DTM achieves state-of-the-art image quality and text adherence; ARTM and FHTM match non-causal AR methods, with FHTM surpassing flow-based methods in text-to-image tasks.

Conclusion: TM advances generative modeling by unifying diffusion/flow and AR approaches, demonstrating superior performance and flexibility.

Abstract: Diffusion and flow matching models have significantly advanced media
generation, yet their design space is well-explored, somewhat limiting further
improvements. Concurrently, autoregressive (AR) models, particularly those
generating continuous tokens, have emerged as a promising direction for
unifying text and media generation. This paper introduces Transition Matching
(TM), a novel discrete-time, continuous-state generative paradigm that unifies
and advances both diffusion/flow models and continuous AR generation. TM
decomposes complex generation tasks into simpler Markov transitions, allowing
for expressive non-deterministic probability transition kernels and arbitrary
non-continuous supervision processes, thereby unlocking new flexible design
avenues. We explore these choices through three TM variants: (i) Difference
Transition Matching (DTM), which generalizes flow matching to discrete-time by
directly learning transition probabilities, yielding state-of-the-art image
quality and text adherence as well as improved sampling efficiency. (ii)
Autoregressive Transition Matching (ARTM) and (iii) Full History Transition
Matching (FHTM) are partially and fully causal models, respectively, that
generalize continuous AR methods. They achieve continuous causal AR generation
quality comparable to non-causal approaches and potentially enable seamless
integration with existing AR text generation techniques. Notably, FHTM is the
first fully causal model to match or surpass the performance of flow-based
methods on text-to-image task in continuous domains. We demonstrate these
contributions through a rigorous large-scale comparison of TM variants and
relevant baselines, maintaining a fixed architecture, training data, and
hyperparameters.

</details>


### [675] [When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series](https://arxiv.org/pdf/2506.23596)
*Min-Yeong Park, Won-Jeong Lee, Seong Tae Kim, Gyeong-Moon Park*

Main category: cs.LG

TL;DR: The paper introduces A2P, a novel framework for Anomaly Prediction (AP) in time series data, combining Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP) to predict future anomalies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for time series data fail to predict future anomalies, focusing only on immediate ones or lacking precision. The paper aims to address this gap.

Method: The A2P framework includes AAF for learning anomaly relationships and SAP with a learnable Anomaly Prompt Pool (APP) to simulate diverse anomaly patterns.

Result: Experiments on real-world datasets show A2P outperforms state-of-the-art methods in predicting future anomalies.

Conclusion: A2P is effective for AP, offering a robust solution for forecasting future anomalies.

Abstract: Recently, forecasting future abnormal events has emerged as an important
scenario to tackle real-world necessities. However, the solution of predicting
specific future time points when anomalies will occur, known as Anomaly
Prediction (AP), remains under-explored. Existing methods dealing with time
series data fail in AP, focusing only on immediate anomalies or failing to
provide precise predictions for future anomalies. To address the AP task, we
propose a novel framework called Anomaly to Prompt (A2P), comprised of
Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To
enable the forecasting model to forecast abnormal time points, we adopt a
strategy to learn the relationships of anomalies. For the robust detection of
anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)
that simulates diverse anomaly patterns using signal adaptive prompt.
Comprehensive experiments on multiple real-world datasets demonstrate the
superiority of A2P over state-of-the-art methods, showcasing its ability to
predict future anomalies. Our implementation code is available at
https://github.com/KU-VGI/AP.

</details>


### [676] [A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data](https://arxiv.org/pdf/2506.23629)
*Xin Liao, Bing Yang, Cai Yu*

Main category: cs.LG

TL;DR: A Nonlinear Low-rank Representation model with CNNs is proposed to impute missing Water Quality Data (WQD), outperforming existing methods in accuracy by capturing temporal and nonlinear features.


<details>
  <summary>Details</summary>
Motivation: Missing WQD due to sensor failures and communication delays leads to High-Dimensional and Sparse data, which traditional methods fail to handle effectively.

Method: The NLR model with CNNs fuses temporal features and extracts nonlinear interactions to achieve deep fusion of multidimensional information.

Result: Experiments on three real datasets show the model significantly improves imputation accuracy over state-of-the-art methods.

Conclusion: The NLR model provides an effective solution for handling missing WQD in dynamic environments.

Abstract: The integrity of Water Quality Data (WQD) is critical in environmental
monitoring for scientific decision-making and ecological protection. However,
water quality monitoring systems are often challenged by large amounts of
missing data due to unavoidable problems such as sensor failures and
communication delays, which further lead to water quality data becoming
High-Dimensional and Sparse (HDS). Traditional data imputation methods are
difficult to depict the potential dynamics and fail to capture the deep data
features, resulting in unsatisfactory imputation performance. To effectively
address the above issues, this paper proposes a Nonlinear Low-rank
Representation model (NLR) with Convolutional Neural Networks (CNN) for
imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing
temporal features to model the temporal dependence of data between time slots,
and b) Extracting nonlinear interactions and local patterns to mine
higher-order relationships features and achieve deep fusion of multidimensional
information. Experimental studies on three real water quality datasets
demonstrate that the proposed model significantly outperforms existing
state-of-the-art data imputation models in terms of estimation accuracy. It
provides an effective approach for handling water quality monitoring data in
complex dynamic environments.

</details>


### [677] [Learning Modular Exponentiation with Transformers](https://arxiv.org/pdf/2506.23679)
*David Demitri Africa, Sara M. Kapoor, Theo Simon Sorg*

Main category: cs.LG

TL;DR: The paper explores modular exponentiation in transformers, revealing specialized computational circuits and grokking-like dynamics.


<details>
  <summary>Details</summary>
Motivation: Modular exponentiation is vital in cryptography but lacks mechanistic interpretability. The study aims to understand how numerical reasoning emerges in transformers.

Method: A 4-layer encoder-decoder transformer is trained, using PCA-based embedding analysis and activation patching to study number-theoretic encoding.

Result: Reciprocal operand training boosts performance, with sudden generalization across moduli. A subgraph of attention heads in the final layer achieves full performance.

Conclusion: Transformers learn modular arithmetic via specialized circuits, suggesting paths for more interpretable and efficient neural approaches.

Abstract: Modular exponentiation is crucial to number theory and cryptography, yet
remains largely unexplored from a mechanistic interpretability standpoint. We
train a 4-layer encoder-decoder Transformer model to perform this operation and
investigate the emergence of numerical reasoning during training. Utilizing
principled sampling strategies, PCA-based embedding analysis, and activation
patching, we examine how number-theoretic properties are encoded within the
model. We find that reciprocal operand training leads to strong performance
gains, with sudden generalization across related moduli. These synchronized
accuracy surges reflect grokking-like dynamics, suggesting the model
internalizes shared arithmetic structure. We also find a subgraph consisting
entirely of attention heads in the final layer sufficient to achieve full
performance on the task of regular exponentiation. These results suggest that
transformer models learn modular arithmetic through specialized computational
circuits, paving the way for more interpretable and efficient neural approaches
to modular exponentiation.

</details>


### [678] [DABstep: Data Agent Benchmark for Multi-step Reasoning](https://arxiv.org/pdf/2506.23719)
*Alex Egg, Martin Iglesias Goyanes, Friso Kingma, Andreu Mora, Leandro von Werra, Thomas Wolf*

Main category: cs.LG

TL;DR: DABstep is a new benchmark for evaluating AI agents on multi-step data analysis tasks, featuring 450 real-world challenges. It tests capabilities like data manipulation and contextual reasoning, with automatic scoring. Leading LLMs perform poorly, achieving only 14.55% accuracy on hard tasks.


<details>
  <summary>Details</summary>
Motivation: To create a realistic benchmark for evaluating AI agents on complex, multi-step data analysis tasks, addressing gaps in current evaluation methods.

Method: DABstep includes 450 real-world tasks requiring code-based processing and contextual reasoning. It uses a factoid-style answer format with automatic correctness checks.

Result: Leading LLM-based agents perform poorly, with the best achieving only 14.55% accuracy on the hardest tasks.

Conclusion: DABstep provides a valuable tool for advancing research in autonomous data analysis, highlighting significant performance gaps in current AI agents.

Abstract: We introduce DABstep, a novel benchmark for evaluating AI agents on realistic
multi-step data analysis tasks. DABstep comprises over 450 real-world
challenges derived from a financial analytics platform, requiring models to
combine code-based data processing with contextual reasoning over heterogeneous
documentation. Each task demands an iterative, multi-step problem-solving
approach, testing capabilities in data manipulation, cross-referencing multiple
sources, and precise result reporting. The benchmark provides a factoid-style
answer format with automatic correctness checks for objective scoring at scale.
We evaluate leading LLM-based agents, revealing a substantial performance gap:
even the best agent achieves only 14.55% accuracy on the hardest tasks. We
detail our benchmark's design, dataset composition, task formulation,
evaluation protocol, report baseline results and analyze failure modes. DABstep
is released with a public leaderboard and toolkit to accelerate research in
autonomous data analysis.

</details>


### [679] [System-Embedded Diffusion Bridge Models](https://arxiv.org/pdf/2506.23726)
*Bartlomiej Sobieski, Matthew Tivnan, Yuang Wang, Siyeop Yoon, Pengfei Jin, Dufan Wu, Quanzheng Li, Przemyslaw Biecek*

Main category: cs.LG

TL;DR: SDBs integrate known linear measurement systems into matrix-valued SDEs, improving performance in linear inverse problems and robustness under system misspecification.


<details>
  <summary>Details</summary>
Motivation: Address the gap in supervised bridge methods by incorporating structural information (measurement models) for better inverse problem solving.

Method: Introduce System embedded Diffusion Bridge Models (SDBs), embedding linear measurement systems into matrix-valued SDE coefficients.

Result: Consistent improvements in diverse linear inverse problems and robust generalization under system misspecification.

Conclusion: SDBs offer a promising solution for real-world inverse problem applications by leveraging structural information.

Abstract: Solving inverse problems -- recovering signals from incomplete or noisy
measurements -- is fundamental in science and engineering. Score-based
generative models (SGMs) have recently emerged as a powerful framework for this
task. Two main paradigms have formed: unsupervised approaches that adapt
pretrained generative models to inverse problems, and supervised bridge methods
that train stochastic processes conditioned on paired clean and corrupted data.
While the former typically assume knowledge of the measurement model, the
latter have largely overlooked this structural information. We introduce System
embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge
methods that explicitly embed the known linear measurement system into the
coefficients of a matrix-valued SDE. This principled integration yields
consistent improvements across diverse linear inverse problems and demonstrates
robust generalization under system misspecification between training and
deployment, offering a promising solution to real-world applications.

</details>


### [680] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/pdf/2506.23731)
*Michel Meintz, Jan Dubiński, Franziska Boenisch, Adam Dziedzic*

Main category: cs.LG

TL;DR: The paper addresses the challenge of watermarking in generative models, focusing on diffusion and autoregressive models, and proposes a new radioactive watermarking method for autoregressive models.


<details>
  <summary>Details</summary>
Motivation: To prevent unauthorized use of generated images by ensuring watermarks persist through model training (radioactivity), especially in autoregressive models lacking such methods.

Method: Analyzes watermark radioactivity in diffusion and autoregressive models, then proposes a novel watermarking method for autoregressive models inspired by techniques in large language models.

Result: Existing watermarking for diffusion models fails to retain radioactivity, while the proposed method effectively preserves it in autoregressive models.

Conclusion: The new watermarking method enables robust provenance tracking and prevents misuse of autoregressive model-generated images.

Abstract: Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [681] [Training of Spiking Neural Networks with Expectation-Propagation](https://arxiv.org/pdf/2506.23757)
*Dan Yao, Steve McLaughlin, Yoann Altmann*

Main category: cs.LG

TL;DR: A gradient-free framework for training SNNs using Expectation-Propagation, handling discrete/continuous weights and marginalizing nuisance parameters, with faster convergence than gradient-based methods.


<details>
  <summary>Details</summary>
Motivation: To unify training methods for SNNs, enabling efficient learning of parameter distributions without gradients.

Method: Expectation-Propagation for gradient-free training, marginalizing nuisance parameters and handling diverse weight types.

Result: Faster convergence in practice, applicable to classification and regression tasks.

Conclusion: Paves the way for efficient deep Bayesian network training methods.

Abstract: In this paper, we propose a unifying message-passing framework for training
spiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free
method is capable of learning the marginal distributions of network parameters
and simultaneously marginalizes nuisance parameters, such as the outputs of
hidden layers. This framework allows for the first time, training of discrete
and continuous weights, for deterministic and stochastic spiking networks,
using batches of training samples. Although its convergence is not ensured, the
algorithm converges in practice faster than gradient-based methods, without
requiring a large number of passes through the training data. The
classification and regression results presented pave the way for new efficient
training methods for deep Bayesian networks.

</details>


### [682] [Model-driven Stochastic Trace Clustering](https://arxiv.org/pdf/2506.23776)
*Jari Peeperkorn, Johannes De Smedt, Jochen De Weerdt*

Main category: cs.LG

TL;DR: A novel model-driven trace clustering method optimizes stochastic process models using entropic relevance, improving interpretability and outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: High variability in process models leads to complexity; existing clustering methods neglect stochasticity, limiting their ability to capture real-world dynamics.

Method: Uses entropic relevance, a stochastic conformance metric based on directly-follows probabilities, to guide trace assignment and optimize stochastic models.

Result: Computationally efficient, scales linearly, and improves interpretability with clearer control-flow patterns; outperforms alternatives in representing process behavior.

Conclusion: The method effectively addresses limitations of existing techniques by incorporating stochasticity, enhancing both performance and interpretability.

Abstract: Process discovery algorithms automatically extract process models from event
logs, but high variability often results in complex and hard-to-understand
models. To mitigate this issue, trace clustering techniques group process
executions into clusters, each represented by a simpler and more understandable
process model. Model-driven trace clustering improves on this by assigning
traces to clusters based on their conformity to cluster-specific process
models. However, most existing clustering techniques rely on either no process
model discovery, or non-stochastic models, neglecting the frequency or
probability of activities and transitions, thereby limiting their capability to
capture real-world execution dynamics. We propose a novel model-driven trace
clustering method that optimizes stochastic process models within each cluster.
Our approach uses entropic relevance, a stochastic conformance metric based on
directly-follows probabilities, to guide trace assignment. This allows
clustering decisions to consider both structural alignment with a cluster's
process model and the likelihood that a trace originates from a given
stochastic process model. The method is computationally efficient, scales
linearly with input size, and improves model interpretability by producing
clusters with clearer control-flow patterns. Extensive experiments on public
real-life datasets show that our method outperforms existing alternatives in
representing process behavior and reveals how clustering performance rankings
can shift when stochasticity is considered.

</details>


### [683] [Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling](https://arxiv.org/pdf/2506.23782)
*Xiaoyang Li, Linwei Tao, Haohui Lu, Minjing Dong, Junbin Gao, Chang Xu*

Main category: cs.LG

TL;DR: WATS is a post-hoc calibration framework for GNNs that uses graph wavelet features to improve confidence estimates, outperforming existing methods in calibration error and variance.


<details>
  <summary>Details</summary>
Motivation: GNNs often misalign confidence estimates with actual correctness, limiting their use in safety-critical applications. Existing methods rely on coarse statistics, ignoring fine-grained graph topology.

Method: WATS assigns node-specific temperatures using heat-kernel graph wavelet features, refining confidence without retraining or neighbor data.

Result: WATS achieves the lowest ECE, outperforming baselines by up to 42.3% and reducing calibration variance by 17.24%. It scales efficiently across diverse graphs.

Conclusion: WATS effectively addresses GNN calibration limitations, offering improved accuracy and scalability for safety-critical deployments.

Abstract: Graph Neural Networks (GNNs) have demonstrated strong predictive performance
on relational data; however, their confidence estimates often misalign with
actual predictive correctness, posing significant limitations for deployment in
safety-critical settings. While existing graph-aware calibration methods seek
to mitigate this limitation, they primarily depend on coarse one-hop
statistics, such as neighbor-predicted confidence, or latent node embeddings,
thereby neglecting the fine-grained structural heterogeneity inherent in graph
topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a
post-hoc calibration framework that assigns node-specific temperatures based on
tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the
scalability and topology sensitivity of graph wavelets to refine confidence
estimates, all without necessitating model retraining or access to neighboring
logits or predictions. Extensive evaluations across seven benchmark datasets
with varying graph structures and two GNN backbones demonstrate that WATS
achieves the lowest Expected Calibration Error (ECE) among all compared
methods, outperforming both classical and graph-specific baselines by up to
42.3\% in ECE and reducing calibration variance by 17.24\% on average compared
with graph-specific methods. Moreover, WATS remains computationally efficient,
scaling well across graphs of diverse sizes and densities. Code will be
released based on publication.

</details>


### [684] [KAIROS: Scalable Model-Agnostic Data Valuation](https://arxiv.org/pdf/2506.23799)
*Jiongli Zhu, Parjanya Prajakta Prashant, Alex Cloninger, Babak Salimi*

Main category: cs.LG

TL;DR: KAIROS is a scalable, model-agnostic framework for valuing training data by measuring its influence on the Maximum Mean Discrepancy (MMD) between training and reference distributions, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing data valuation methods are flawed: model-based techniques inherit biases, while algorithm-based approaches like Data Shapley are computationally expensive. Wasserstein-based methods misrank examples. KAIROS addresses these limitations.

Method: KAIROS assigns distributional influence scores using MMD, providing a closed-form solution that approximates leave-one-out (LOO) utility without retraining. It supports efficient online updates and conditional kernels for error detection.

Result: KAIROS outperforms state-of-the-art baselines in accuracy and runtime, achieving up to 50x speedup. It handles noise, mislabeling, and poisoning effectively.

Conclusion: KAIROS offers a scalable, accurate, and efficient solution for data valuation, with theoretical guarantees for reproducibility and interpretability.

Abstract: Training data increasingly shapes not only model accuracy but also regulatory
compliance and market valuation of AI assets. Yet existing valuation methods
remain inadequate: model-based techniques depend on a single fitted model and
inherit its biases, while algorithm-based approaches such as Data Shapley
require costly retrainings at web scale. Recent Wasserstein-based
model-agnostic methods rely on approximations that misrank examples relative to
their true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,
model-agnostic valuation framework that assigns each example a distributional
influence score: its contribution to the Maximum Mean Discrepancy (MMD) between
the empirical training distribution and a clean reference set. Unlike
Wasserstein surrogates, our MMD-based influence admits a closed-form solution
that faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,
requires no retraining, and naturally extends to conditional kernels for
unified label- and feature-error detection. Moreover, KAIROS supports efficient
online updates: when a new batch of size m arrives, all scores can be updated
in $O(mN)$ time, delivering up to 50x speedup without compromising ranking
quality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks
show that KAIROS consistently outperforms state-of-the-art model-, Shapley-,
and Wasserstein-based baselines in both accuracy and runtime. We provide
rigorous theoretical guarantees, including symmetry for reproducible rankings
and density-separation for interpretable thresholds.

</details>


### [685] [Towards the Training of Deeper Predictive Coding Neural Networks](https://arxiv.org/pdf/2506.23800)
*Chang Qi, Matteo Forasassi, Thomas Lukasiewicz, Tommaso Salvatori*

Main category: cs.LG

TL;DR: The paper addresses performance degradation in deep predictive coding networks by introducing precision-weighting and a novel weight update mechanism, achieving results comparable to backpropagation.


<details>
  <summary>Details</summary>
Motivation: Performance degradation in deep predictive coding networks due to imbalanced errors and ineffective predictions in deeper layers.

Method: Introduces precision-weighting for latent variables and a novel weight update mechanism to reduce error accumulation.

Result: Improved test accuracy in networks with over seven layers, matching backpropagation performance.

Conclusion: Better understanding of the relaxation phase is key for scaling equilibrium propagation in complex tasks.

Abstract: Predictive coding networks trained with equilibrium propagation are neural
models that perform inference through an iterative energy minimization process.
Previous studies have demonstrated their effectiveness in shallow
architectures, but show significant performance degradation when depth exceeds
five to seven layers. In this work, we show that the reason behind this
degradation is due to exponentially imbalanced errors between layers during
weight updates, and predictions from the previous layer not being effective in
guiding updates in deeper layers. We address the first issue by introducing two
novel methods to optimize the latent variables that use precision-weighting to
re-balance the distribution of energy among layers during the `relaxation
phase', and the second issue by proposing a novel weight update mechanism that
reduces error accumulation in deeper layers. Empirically, we test our methods
on a large number of image classification tasks, resulting in large
improvements in test accuracy across networks with more than seven layers, with
performances comparable to those of backprop on similar models. These findings
suggest that a better understanding of the relaxation phase is important to
train models using equilibrium propagation at scale, and open new possibilities
for their application in complex tasks.

</details>


### [686] [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/pdf/2506.23845)
*Kenny Peng, Rajiv Movva, Jon Kleinberg, Emma Pierson, Nikhil Garg*

Main category: cs.LG

TL;DR: SAEs are less effective for acting on known concepts but powerful for discovering unknown ones, clarifying their utility in interpretability and social sciences.


<details>
  <summary>Details</summary>
Motivation: Reconcile competing narratives about SAEs by distinguishing their effectiveness for known vs. unknown concepts.

Method: Conceptual distinction between SAEs' roles in acting on known concepts versus discovering unknown ones.

Result: SAEs are useful for interpretability, fairness, and social/health sciences, despite skepticism.

Conclusion: SAEs have distinct applications in discovery and interpretability, clarifying their value.

Abstract: While sparse autoencoders (SAEs) have generated significant excitement, a
series of negative results have added to skepticism about their usefulness.
Here, we establish a conceptual distinction that reconciles competing
narratives surrounding SAEs. We argue that while SAEs may be less effective for
acting on known concepts, SAEs are powerful tools for discovering unknown
concepts. This distinction cleanly separates existing negative and positive
results, and suggests several classes of SAE applications. Specifically, we
outline use cases for SAEs in (i) ML interpretability, explainability,
fairness, auditing, and safety, and (ii) social and health sciences.

</details>


### [687] [Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations](https://arxiv.org/pdf/2506.23802)
*Konstantinos Bourazas, Savvas Papaioannou, Panayiotis Kolios*

Main category: cs.LG

TL;DR: A novel adaptive anomaly detection framework for sequential RFS observations, distinguishing normal from anomalous data by detecting deviations from expected statistical behavior.


<details>
  <summary>Details</summary>
Motivation: To monitor sequential RFS observations and accurately identify anomalies by adapting to behavioral shifts in the data-generating process.

Method: Development of an RFS-based framework with Power Discounting Posteriors (PD) for online learning and dynamic adaptation to detect abnormal point patterns.

Result: Effective anomaly detection demonstrated through qualitative and quantitative simulation experiments.

Conclusion: The proposed framework successfully adapts to behavioral shifts and accurately identifies anomalies in sequential RFS data.

Abstract: In this work we introduce a novel adaptive anomaly detection framework
specifically designed for monitoring sequential random finite set (RFS)
observations. Our approach effectively distinguishes between In-Control data
(normal) and Out-Of-Control data (anomalies) by detecting deviations from the
expected statistical behavior of the process. The primary contributions of this
study include the development of an innovative RFS-based framework that not
only learns the normal behavior of the data-generating process online but also
dynamically adapts to behavioral shifts to accurately identify abnormal point
patterns. To achieve this, we introduce a new class of RFS-based posterior
distributions, named Power Discounting Posteriors (PD), which facilitate
adaptation to systematic changes in data while enabling anomaly detection of
point pattern data through a novel predictive posterior density function. The
effectiveness of the proposed approach is demonstrated by extensive qualitative
and quantitative simulation experiments.

</details>


### [688] [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/pdf/2506.23978)
*Samuele Marro, Philip Torr*

Main category: cs.LG

TL;DR: LLM-based agents enable universal interoperability, disrupting closed platforms by making data exchange cheap and unavoidable, but require frameworks to address risks.


<details>
  <summary>Details</summary>
Motivation: The dominance of closed, proprietary platforms in the application layer limits data exchange and interoperability, reinforcing monopolistic behaviors.

Method: Proposes using LLM-based agents to automatically translate data formats and interact with human-designed interfaces, enabling seamless interoperability.

Result: Universal interoperability reduces monopolistic behaviors and promotes data portability but introduces security risks and technical debt.

Conclusion: The ML community should embrace universal interoperability while developing frameworks to mitigate risks, ensuring user freedom and competitive markets.

Abstract: While the Internet's core infrastructure was designed to be open and
universal, today's application layer is dominated by closed, proprietary
platforms. Open and interoperable APIs require significant investment, and
market leaders have little incentive to enable data exchange that could erode
their user lock-in. We argue that LLM-based agents fundamentally disrupt this
status quo. Agents can automatically translate between data formats and
interact with interfaces designed for humans: this makes interoperability
dramatically cheaper and effectively unavoidable. We name this shift universal
interoperability: the ability for any two digital services to exchange data
seamlessly using AI-mediated adapters. Universal interoperability undermines
monopolistic behaviours and promotes data portability. However, it can also
lead to new security risks and technical debt. Our position is that the ML
community should embrace this development while building the appropriate
frameworks to mitigate the downsides. By acting now, we can harness AI to
restore user freedom and competitive markets without sacrificing security.

</details>


### [689] [SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration](https://arxiv.org/pdf/2506.23803)
*Dmitry Kovalev*

Main category: cs.LG

TL;DR: The paper revisits SGD with AdaGrad-type preconditioning, providing a unified convergence analysis and connecting Scion and DASGO. It also shows accelerated convergence with Nesterov momentum, explaining Adam's efficiency.


<details>
  <summary>Details</summary>
Motivation: To unify and extend convergence analysis for adaptive gradient methods and explore the benefits of combining preconditioning with momentum.

Method: Develops a unified convergence framework for SGD with adaptive preconditioning under anisotropic smoothness and noise, and applies Nesterov momentum.

Result: Recovers state-of-the-art convergence results, connects Scion and DASGO, and proves accelerated convergence with momentum.

Conclusion: AdaGrad-type methods can benefit from both preconditioning and momentum, explaining Adam's practical efficiency.

Abstract: In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type
preconditioning. Our contributions are twofold. First, we develop a unified
convergence analysis of SGD with adaptive preconditioning under anisotropic or
matrix smoothness and noise assumptions. This allows us to recover
state-of-the-art convergence results for several popular adaptive gradient
methods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In
addition, we establish the fundamental connection between two recently proposed
algorithms, Scion and DASGO, and provide the first theoretical guarantees for
the latter. Second, we show that the convergence of methods like AdaGrad and
DASGO can be provably accelerated beyond the best-known rates using Nesterov
momentum. Consequently, we obtain the first theoretical justification that
AdaGrad-type algorithms can simultaneously benefit from both diagonal
preconditioning and momentum, which may provide an ultimate explanation for the
practical efficiency of Adam.

</details>


### [690] [Supercm: Revisiting Clustering for Semi-Supervised Learning](https://arxiv.org/pdf/2506.23824)
*Durgesh Singh, Ahcene Boubekki, Robert Jenssen, Michael C. Kampffmeyer*

Main category: cs.LG

TL;DR: A novel SSL method using differentiable clustering to improve performance and simplicity.


<details>
  <summary>Details</summary>
Motivation: Current SSL methods focus on complex consistency regularization or entropy minimization; this work simplifies SSL by incorporating clustering.

Method: Extends a differentiable clustering module, using annotated data to guide cluster centroids for an end-to-end trainable approach.

Result: Outperforms supervised-only baselines and enhances other SSL methods when combined.

Conclusion: The proposed clustering-based SSL method is effective, simple, and compatible with other techniques.

Abstract: The development of semi-supervised learning (SSL) has in recent years largely
focused on the development of new consistency regularization or entropy
minimization approaches, often resulting in models with complex training
strategies to obtain the desired results. In this work, we instead propose a
novel approach that explicitly incorporates the underlying clustering
assumption in SSL through extending a recently proposed differentiable
clustering module. Leveraging annotated data to guide the cluster centroids
results in a simple end-to-end trainable deep SSL approach. We demonstrate that
the proposed model improves the performance over the supervised-only baseline
and show that our framework can be used in conjunction with other SSL methods
to further boost their performance.

</details>


### [691] [EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment](https://arxiv.org/pdf/2506.23843)
*Joris Bekkers*

Main category: cs.LG

TL;DR: EFPI is a method for recognizing football formations and assigning player positions using predefined templates and cost minimization from tracking data.


<details>
  <summary>Details</summary>
Motivation: Understanding team formations and player positioning is crucial for tactical analysis in football.

Method: Uses linear sum assignment to match players to template positions, scaling player locations to template dimensions, and includes a stability parameter to avoid unnecessary changes.

Result: Effective for individual frames and larger game segments, with open-source implementation available.

Conclusion: EFPI provides a flexible and accurate approach for formation recognition in football.

Abstract: Understanding team formations and player positioning is crucial for tactical
analysis in football (soccer). This paper presents a flexible method for
formation recognition and player position assignment in football using
predefined static formation templates and cost minimization from spatiotemporal
tracking data, called EFPI. Our approach employs linear sum assignment to
optimally match players to positions within a set of template formations by
minimizing the total distance between actual player locations and template
positions, subsequently selecting the formation with the lowest assignment
cost. To improve accuracy, we scale actual player positions to match the
dimensions of these formation templates in both width and length. While the
method functions effectively on individual frames, it extends naturally to
larger game segments such as complete periods, possession sequences or specific
intervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we
incorporate an optional stability parameter that prevents unnecessary formation
changes when assignment costs differ only marginally between time segments.
EFPI is available as open-source code through the unravelsports Python package.

</details>


### [692] [When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems](https://arxiv.org/pdf/2506.23872)
*Eduard Buss, Till Aust, Heiko Hamann*

Main category: cs.LG

TL;DR: The paper explores using plants as natural sensors for environmental monitoring, integrating them with wearable devices (PhytoNode) to record electrophysiological data. Machine learning, especially AutoML, achieved high accuracy (95% F1 score) in classifying environmental conditions.


<details>
  <summary>Details</summary>
Motivation: To leverage plants as natural sensors for ecological and agricultural applications, bridging the gap between biological and artificial systems for sustainable monitoring.

Method: Equipped *Hedera helix* with PhytoNode to record electrophysiological activity outdoors. Used AutoML and manual tuning to analyze data over five months.

Result: High classification performance (95% F1 score) with AutoML outperforming manual tuning. Feature selection further improved accuracy.

Conclusion: The biohybrid system successfully monitors plant electrophysiology in real-world conditions, advancing scalable, sustainable environmental monitoring solutions.

Abstract: Living plants, while contributing to ecological balance and climate
regulation, also function as natural sensors capable of transmitting
information about their internal physiological states and surrounding
conditions. This rich source of data provides potential for applications in
environmental monitoring and precision agriculture. With integration into
biohybrid systems, we establish novel channels of physiological signal flow
between living plants and artificial devices. We equipped *Hedera helix* with a
plant-wearable device called PhytoNode to continuously record the plant's
electrophysiological activity. We deployed plants in an uncontrolled outdoor
environment to map electrophysiological patterns to environmental conditions.
Over five months, we collected data that we analyzed using state-of-the-art and
automated machine learning (AutoML). Our classification models achieve high
performance, reaching macro F1 scores of up to 95 percent in binary tasks.
AutoML approaches outperformed manual tuning, and selecting subsets of
statistical features further improved accuracy. Our biohybrid living system
monitors the electrophysiology of plants in harsh, real-world conditions. This
work advances scalable, self-sustaining, and plant-integrated living biohybrid
systems for sustainable environmental monitoring.

</details>


### [693] [Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic](https://arxiv.org/pdf/2506.23875)
*Yuta Sato, Kazuhiko Kawamoto, Hiroshi Kera*

Main category: cs.LG

TL;DR: The study introduces a method to reorder decoder input tokens in Transformers to improve learning of arithmetic tasks by identifying learning-friendly sequences through early loss drop analysis.


<details>
  <summary>Details</summary>
Motivation: The order of intermediate steps in the chain of thought critically impacts reasoning difficulty in Transformers, prompting the need for optimized token sequences.

Method: A pipeline trains a Transformer on mixed-order sequences, identifies benign orders via early loss drops, and uses a hierarchical approach for efficient reordering.

Result: The method successfully identifies learning-friendly orders from billions of candidates, even recovering the reverse-digit order for multiplication.

Conclusion: Optimizing token order in Transformers enhances learning efficiency, as demonstrated in arithmetic tasks, validating the importance of sequence arrangement.

Abstract: The chain of thought is fundamental in Transformers, which is to perform
step-by-step reasoning. Besides what intermediate steps work, the order of
these steps critically affects the difficulty of the reasoning. This study
addresses a novel task of unraveling chain of thought - reordering decoder
input tokens to a learning-friendly sequence for Transformers to learn
arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture
of target sequences arranged in different orders and then identifies benign
orders as those with fast loss drops in the early stage. As the search space
grows factorially with sequence length, we propose a two-stage hierarchical
approach for inter- and intra-block reordering. Experiments on four
order-sensitive arithmetic tasks show that our method identifies a
learning-friendly order out of a few billion candidates. Notably, on the
multiplication task, it recovered the reverse-digit order reported in prior
studies.

</details>


### [694] [Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System](https://arxiv.org/pdf/2506.23923)
*Miguel Camacho-Sánchez, Fernando García-Torres, Jesper John Lisegaard, Rocío del Amor, Sankhya Mohanty, Valery Naranjo*

Main category: cs.LG

TL;DR: A reinforcement learning (RL) strategy using Proximal Policy Optimisation (PPO) is proposed to control resin flow dynamics in resin infusion (RI) and resin transfer moulding (RTM) processes, ensuring uniform impregnation and improving product quality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of managing resin flow dynamics in composites manufacturing, which is critical for preventing defects like porosities and dry spots.

Method: A reinforcement learning approach (PPO) is used to synchronize resin flow fronts in a simulation of a two-inlet, single-outlet infusion scenario.

Result: The RL strategy effectively achieves accurate flow convergence, demonstrating potential for enhanced process control.

Conclusion: The RL-based approach shows promise for improving resin infusion processes, ensuring better structural integrity and product quality in composites manufacturing.

Abstract: Resin infusion (RI) and resin transfer moulding (RTM) are critical processes
for the manufacturing of high-performance fibre-reinforced polymer composites,
particularly for large-scale applications such as wind turbine blades.
Controlling the resin flow dynamics in these processes is critical to ensure
the uniform impregnation of the fibre reinforcements, thereby preventing
residual porosities and dry spots that impact the consequent structural
integrity of the final component. This paper presents a reinforcement learning
(RL) based strategy, established using process simulations, for synchronising
the different resin flow fronts in an infusion scenario involving two resin
inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our
approach addresses the challenge of managing the fluid dynamics in a partially
observable environment. The results demonstrate the effectiveness of the RL
approach in achieving an accurate flow convergence, highlighting its potential
towards improving process control and product quality in composites
manufacturing.

</details>


### [695] [Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages](https://arxiv.org/pdf/2506.23958)
*Ikechukwu Ogbonna, Lesley Davidson, Soumya Banerjee, Abhishek Dasgupta, Laurence Kenney, Vikranth Harthikote Nagaraja*

Main category: cs.LG

TL;DR: AI-powered framework translates complex medical documents into marginalized languages, improving healthcare access for underserved populations.


<details>
  <summary>Details</summary>
Motivation: Address language and literacy gaps in African countries by making prosthetic device manuals accessible.

Method: Uses a Retrieval-Augmented Generation (RAG) pipeline and NLP models for translation and question-answering.

Result: Enables real-time, localized answers in native languages for medical queries.

Conclusion: The framework empowers patients and clinicians with accessible healthcare information, scalable to other languages.

Abstract: Millions of people in African countries face barriers to accessing healthcare
due to language and literacy gaps. This research tackles this challenge by
transforming complex medical documents -- in this case, prosthetic device user
manuals -- into accessible formats for underserved populations. This case study
in cross-cultural translation is particularly pertinent/relevant for
communities that receive donated prosthetic devices but may not receive the
accompanying user documentation. Or, if available online, may only be available
in formats (e.g., language and readability) that are inaccessible to local
populations (e.g., English-language, high resource settings/cultural context).
The approach is demonstrated using the widely spoken Pidgin dialect, but our
open-source framework has been designed to enable rapid and easy extension to
other languages/dialects. This work presents an AI-powered framework designed
to process and translate complex medical documents, e.g., user manuals for
prosthetic devices, into marginalised languages. The system enables users --
such as healthcare workers or patients -- to upload English-language medical
equipment manuals, pose questions in their native language, and receive
accurate, localised answers in real time. Technically, the system integrates a
Retrieval-Augmented Generation (RAG) pipeline for processing and semantic
understanding of the uploaded manuals. It then employs advanced Natural
Language Processing (NLP) models for generative question-answering and
multilingual translation. Beyond simple translation, it ensures accessibility
to device instructions, treatment protocols, and safety information, empowering
patients and clinicians to make informed healthcare decisions.

</details>


### [696] [ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.23960)
*Mingfei Cheng, Xiaofei Xie, Renzhi Wang, Yuan Zhou, Ming Hu*

Main category: cs.LG

TL;DR: ADReFT is a novel method for online repair of Autonomous Driving Systems (ADSs) using transformer-based models to improve safety and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing online repair solutions for ADSs lack generalizability and adaptability, often being overly conservative and ineffective.

Method: ADReFT uses a transformer-based model with State Monitor and Decision Adapter heads, pretrained with supervised learning and finetuned with reinforcement learning.

Result: ADReFT achieves better repair performance by generating precise and contextually appropriate repair actions.

Conclusion: ADReFT effectively enhances ADS safety and reliability by addressing limitations of existing repair methods.

Abstract: Autonomous Driving Systems (ADSs) continue to face safety-critical risks due
to the inherent limitations in their design and performance capabilities.
Online repair plays a crucial role in mitigating such limitations, ensuring the
runtime safety and reliability of ADSs. Existing online repair solutions
enforce ADS compliance by transforming unacceptable trajectories into
acceptable ones based on predefined specifications, such as rule-based
constraints or training datasets. However, these approaches often lack
generalizability, adaptability and tend to be overly conservative, resulting in
ineffective repairs that not only fail to mitigate safety risks sufficiently
but also degrade the overall driving experience. To address this issue, we
propose Adaptive Decision Repair (ADReFT), a novel and effective repair method
that identifies safety-critical states through offline learning from failed
tests and generates appropriate mitigation actions to improve ADS safety.
Specifically, ADReFT incorporates a transformer-based model with two joint
heads, State Monitor and Decision Adapter, designed to capture complex driving
environment interactions to evaluate state safety severity and generate
adaptive repair actions. Given the absence of oracles for state safety
identification, we first pretrain ADReFT using supervised learning with coarse
annotations, i.e., labeling states preceding violations as positive samples and
others as negative samples. It establishes ADReFT's foundational capability to
mitigate safety-critical violations, though it may result in somewhat
conservative mitigation strategies. Therefore, we subsequently finetune ADReFT
using reinforcement learning to improve its initial capability and generate
more precise and contextually appropriate repair decisions. Our evaluation
results illustrate that ADReFT achieves better repair performance.

</details>


### [697] [UMA: A Family of Universal Models for Atoms](https://arxiv.org/pdf/2506.23971)
*Brandon M. Wood, Misko Dzamba, Xiang Fu, Meng Gao, Muhammed Shuaibi, Luis Barroso-Luque, Kareem Abdelmaqsoud, Vahe Gharakhanyan, John R. Kitchin, Daniel S. Levine, Kyle Michel, Anuroop Sriram, Taco Cohen, Abhishek Das, Ammar Rizvi, Sushree Jagriti Sahoo, Zachary W. Ulissi, C. Lawrence Zitnick*

Main category: cs.LG

TL;DR: Meta FAIR introduces Universal Models for Atoms (UMA), a family of AI models trained on 500M atomic structures, achieving high speed, accuracy, and generalization across chemical domains without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The need for fast and accurate atomic property computation in applications like drug discovery and energy storage drives the development of UMA.

Method: UMA models use a novel 'mixture of linear experts' architecture, scaling model capacity with dataset size. Training involves 500M 3D atomic structures from diverse chemical domains.

Result: UMA models match or outperform specialized models across domains without fine-tuning, e.g., UMA-medium has 1.4B parameters but only ~50M active per structure.

Conclusion: UMA's release of code, weights, and data aims to accelerate computational workflows and advance AI capabilities in chemistry and materials science.

Abstract: The ability to quickly and accurately compute properties from atomic
simulations is critical for advancing a large number of applications in
chemistry and materials science including drug discovery, energy storage, and
semiconductor manufacturing. To address this need, Meta FAIR presents a family
of Universal Models for Atoms (UMA), designed to push the frontier of speed,
accuracy, and generalization. UMA models are trained on half a billion unique
3D atomic structures (the largest training runs to date) by compiling data
across multiple chemical domains, e.g. molecules, materials, and catalysts. We
develop empirical scaling laws to help understand how to increase model
capacity alongside dataset size to achieve the best accuracy. The UMA small and
medium models utilize a novel architectural design we refer to as mixture of
linear experts that enables increasing model capacity without sacrificing
speed. For example, UMA-medium has 1.4B parameters but only ~50M active
parameters per atomic structure. We evaluate UMA models on a diverse set of
applications across multiple domains and find that, remarkably, a single model
without any fine-tuning can perform similarly or better than specialized
models. We are releasing the UMA code, weights, and associated data to
accelerate computational workflows and enable the community to continue to
build increasingly capable AI models.

</details>


### [698] [A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks](https://arxiv.org/pdf/2506.23977)
*Zain ul Abdeen, Vassilis Kekatos, Ming Jin*

Main category: cs.LG

TL;DR: A convex training framework for neural networks ensures certified robustness via Lipschitz constraints, using semidefinite relaxation and a randomized subspace method for scalability.


<details>
  <summary>Details</summary>
Motivation: Certified robustness is essential for safety-critical applications, but existing Lipschitz-constrained training methods face non-convexity and scalability issues.

Method: Proposes a convex training framework with semidefinite relaxation and loop transformation, and introduces RS-LMI for scalable layerwise constraints.

Result: Achieves competitive accuracy with improved Lipschitz bounds and runtime on MNIST, CIFAR-10, and ImageNet.

Conclusion: The framework provides a scalable and certifiable solution for robust neural network training.

Abstract: Certified robustness is a critical property for deploying neural networks
(NN) in safety-critical applications. A principle approach to achieving such
guarantees is to constrain the global Lipschitz constant of the network.
However, accurate methods for Lipschitz-constrained training often suffer from
non-convex formulations and poor scalability due to reliance on global
semidefinite programs (SDPs). In this letter, we propose a convex training
framework that enforces global Lipschitz constraints via semidefinite
relaxation. By reparameterizing the NN using loop transformation, we derive a
convex admissibility condition that enables tractable and certifiable training.
While the resulting formulation guarantees robustness, its scalability is
limited by the size of global SDP. To overcome this, we develop a randomized
subspace linear matrix inequalities (RS-LMI) approach that decomposes the
global constraints into sketched layerwise constraints projected onto
low-dimensional subspaces, yielding a smooth and memory-efficient training
objective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that
the proposed framework achieves competitive accuracy with significantly
improved Lipschitz bounds and runtime performance.

</details>


### [699] [The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)](https://arxiv.org/pdf/2506.23996)
*Juan Maroñas*

Main category: cs.LG

TL;DR: Derivation of Jacobian and Hessian matrices for Kullback-Leibler divergence between multivariate Gaussians using differentials.


<details>
  <summary>Details</summary>
Motivation: To provide a didactic and detailed explanation of deriving Jacobian and Hessian matrices for Kullback-Leibler divergence, inspired by existing literature.

Method: Uses first and second-order differentials, referencing Magnus (1999) and Minka's derivations.

Result: Detailed derivations and summary of results for Jacobian and Hessian matrices.

Conclusion: The document offers a clear, step-by-step guide for deriving these matrices, emphasizing didactic clarity.

Abstract: This document shows how to obtain the Jacobian and Hessian matrices of the
Kullback-Leibler divergence between two multivariate Gaussian distributions,
using the first and second-order differentials. The presented derivations are
based on the theory presented by \cite{magnus99}. I've also got great
inspiration from some of the derivations in \cite{minka}.
  Since I pretend to be at most didactic, the document is split into a summary
of results and detailed derivations on each of the elements involved, with
specific references to the tricks used in the derivations, and to many of the
underlying concepts.

</details>


### [700] [The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models](https://arxiv.org/pdf/2506.24000)
*Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan*

Main category: cs.LG

TL;DR: TTA-VLM is a benchmark addressing limitations in current TTA research for VLMs, offering unified evaluation across diverse methods and metrics.


<details>
  <summary>Details</summary>
Motivation: Current TTA research lacks fair comparisons due to inconsistent settings, limited metrics, and insufficient analysis, hindering practical insights.

Method: TTA-VLM implements 8 episodic and 7 online TTA methods in a reproducible framework, evaluating them on 15 datasets and extending beyond CLIP to SigLIP and training-time methods.

Result: Findings show limited gains from existing TTA methods, poor collaboration with fine-tuning, and trade-offs between accuracy and trustworthiness.

Conclusion: TTA-VLM aims to foster fair comparisons and reliable TTA strategies, encouraging broader and more trustworthy advancements in the field.

Abstract: Test-time adaptation (TTA) methods have gained significant attention for
enhancing the performance of vision-language models (VLMs) such as CLIP during
inference, without requiring additional labeled data. However, current TTA
researches generally suffer from major limitations such as duplication of
baseline results, limited evaluation metrics, inconsistent experimental
settings, and insufficient analysis. These problems hinder fair comparisons
between TTA methods and obscure their practical strengths and weaknesses. To
address these challenges, we introduce TTA-VLM, a comprehensive benchmark for
evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7
online TTA methods within a unified and reproducible framework, and evaluates
them across 15 widely used datasets. Unlike prior studies focused solely on
CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid
loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA
to assess generality. Beyond classification accuracy, TTA-VLM incorporates
various evaluation metrics, including robustness, calibration,
out-of-distribution detection, and stability, enabling a more holistic
assessment of TTA methods. Through extensive experiments, we find that 1)
existing TTA methods produce limited gains compared to the previous pioneering
work; 2) current TTA methods exhibit poor collaboration with training-time
fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced
model trustworthiness. We release TTA-VLM to provide fair comparison and
comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the
community to develop more reliable and generalizable TTA strategies.

</details>


### [701] [Provably Efficient and Agile Randomized Q-Learning](https://arxiv.org/pdf/2506.24005)
*He Wang, Xingyu Xu, Yuejie Chi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While Bayesian-based exploration often demonstrates superior empirical
performance compared to bonus-based methods in model-based reinforcement
learning (RL), its theoretical understanding remains limited for model-free
settings. Existing provable algorithms either suffer from computational
intractability or rely on stage-wise policy updates which reduce responsiveness
and slow down the learning process. In this paper, we propose a novel variant
of Q-learning algorithm, refereed to as RandomizedQ, which integrates
sampling-based exploration with agile, step-wise, policy updates, for episodic
tabular RL. We establish an $\widetilde{O}(\sqrt{H^5SAT})$ regret bound, where
$S$ is the number of states, $A$ is the number of actions, $H$ is the episode
length, and $T$ is the total number of episodes. In addition, we present a
logarithmic regret bound under a mild positive sub-optimality condition on the
optimal Q-function. Empirically, RandomizedQ exhibits outstanding performance
compared to existing Q-learning variants with both bonus-based and
Bayesian-based exploration on standard benchmarks.

</details>


### [702] [Bridging Theory and Practice in Link Representation with Graph Neural Networks](https://arxiv.org/pdf/2506.24018)
*Veronica Lachi, Francesco Ferrini, Antonio Longa, Bruno Lepri, Andrea Passerini, Manfred Jaeger*

Main category: cs.LG

TL;DR: The paper studies the expressive power of GNNs for link representation, introduces a unifying framework, and shows that expressive models outperform simpler ones in high-symmetry scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical understanding of GNNs focuses on graph-level representations, leaving a gap in understanding their expressiveness for link-level tasks like link prediction.

Method: The authors introduce the $k_\phi$-$k_\rho$-$m$ framework to unify and compare message-passing link models, derive a hierarchy of methods, and propose a synthetic benchmark for evaluation.

Result: Expressive models underperform on standard benchmarks but excel in high-symmetry scenarios, as shown by a graph symmetry metric.

Conclusion: The study highlights the importance of dataset-aware model selection and provides tools for analyzing future GNN architectures for link representation.

Abstract: Graph Neural Networks (GNNs) are widely used to compute representations of
node pairs for downstream tasks such as link prediction. Yet, theoretical
understanding of their expressive power has focused almost entirely on
graph-level representations. In this work, we shift the focus to links and
provide the first comprehensive study of GNN expressiveness in link
representation. We introduce a unifying framework, the $k_\phi$-$k_\rho$-$m$
framework, that subsumes existing message-passing link models and enables
formal expressiveness comparisons. Using this framework, we derive a hierarchy
of state-of-the-art methods and offer theoretical tools to analyze future
architectures. To complement our analysis, we propose a synthetic evaluation
protocol comprising the first benchmark specifically designed to assess
link-level expressiveness. Finally, we ask: does expressiveness matter in
practice? We use a graph symmetry metric that quantifies the difficulty of
distinguishing links and show that while expressive models may underperform on
standard benchmarks, they significantly outperform simpler ones as symmetry
increases, highlighting the need for dataset-aware model selection.

</details>


### [703] [Faster Diffusion Models via Higher-Order Approximation](https://arxiv.org/pdf/2506.24042)
*Gen Li, Yuchen Zhou, Yuting Wei, Yuxin Chen*

Main category: cs.LG

TL;DR: A training-free sampling algorithm for diffusion models achieves provable acceleration without retraining, requiring fewer score function evaluations for accurate distribution approximation.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of diffusion models by reducing the computational cost of sampling without additional training or restrictive assumptions on the data distribution.

Method: The algorithm uses high-order ODE solvers, Lagrange interpolation, and successive refinement to approximate the probability flow ODE integral, minimizing score function evaluations.

Result: The method requires only $d^{1+2/K} \varepsilon^{-1/K}$ score evaluations (up to log factor) for accurate approximation, robust to inexact score estimation.

Conclusion: The proposed algorithm offers a scalable and efficient solution for diffusion models, applicable to a wide range of data distributions without strict assumptions.

Abstract: In this paper, we explore provable acceleration of diffusion models without
any additional retraining. Focusing on the task of approximating a target data
distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation
distance, we propose a principled, training-free sampling algorithm that
requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate
scores, where $K$ is an arbitrarily large fixed integer. This result applies to
a broad class of target data distributions, without the need for assumptions
such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact
score estimation, degrading gracefully as the score estimation error increases
-- without demanding higher-order smoothness on the score estimates as assumed
in previous work. The proposed algorithm draws insight from high-order ODE
solvers, leveraging high-order Lagrange interpolation and successive refinement
to approximate the integral derived from the probability flow ODE.

</details>


### [704] [Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies](https://arxiv.org/pdf/2506.24093)
*Paul Wachter, Lukas Niehaus, Julius Schöning*

Main category: cs.LG

TL;DR: The paper evaluates mixed training strategies using hybrid datasets (synthetic and real data) to bridge the domain gap in ANN training, analyzing their impact across tasks and architectures.


<details>
  <summary>Details</summary>
Motivation: The disparity between synthetic and real data causes poor ANN performance in real-world scenarios, necessitating a systematic study of mixed training strategies.

Method: The study analyzes two mixing strategies on three architectures and three hybrid datasets, varying synthetic-to-real data proportions.

Result: The findings offer insights into optimizing synthetic data use to improve ANN robustness and efficacy.

Conclusion: The study contributes to better understanding and enhancing the effectiveness of synthetic data in ANN training.

Abstract: Synthetic data has emerged as a cost-effective alternative to real data for
training artificial neural networks (ANN). However, the disparity between
synthetic and real data results in a domain gap. That gap leads to poor
performance and generalization of the trained ANN when applied to real-world
scenarios. Several strategies have been developed to bridge this gap, which
combine synthetic and real data, known as mixed training using hybrid datasets.
While these strategies have been shown to mitigate the domain gap, a systematic
evaluation of their generalizability and robustness across various tasks and
architectures remains underexplored. To address this challenge, our study
comprehensively analyzes two widely used mixing strategies on three prevalent
architectures and three distinct hybrid datasets. From these datasets, we
sample subsets with varying proportions of synthetic to real data to
investigate the impact of synthetic and real components. The findings of this
paper provide valuable insights into optimizing the use of synthetic data in
the training process of any ANN, contributing to enhancing robustness and
efficacy.

</details>


### [705] [Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime](https://arxiv.org/pdf/2506.24120)
*Yuqing Wang, Shangding Gu*

Main category: cs.LG

TL;DR: Selecting uniformly distributed data improves training efficiency and performance in LLMs by maximizing pairwise distance between data points.


<details>
  <summary>Details</summary>
Motivation: To identify general principles of data selection beyond quality and diversity that enhance model performance, especially for complex tasks with limited prior knowledge.

Method: Theoretical analysis and experiments show that more uniform data distribution (larger minimum pairwise distance) accelerates gradient descent and reduces approximation error.

Result: Data selection by maximizing pairwise distance significantly speeds up training and improves performance across various settings.

Conclusion: Uniform data distribution is a key principle for efficient training and better performance in LLMs, supported by theoretical and empirical evidence.

Abstract: Data selection plays a crucial role in data-driven decision-making, including
in large language models (LLMs), and is typically task-dependent. Properties
such as data quality and diversity have been extensively studied and are known
to enhance model performance. However, it remains unclear whether there exist
other quantitative and general principles of data selection that can
consistently improve performance, especially for complex tasks with limited
prior knowledge. In this paper, we demonstrate that selecting more uniformly
distributed data can improve training efficiency while enhancing performance.
Specifically, we establish that more uniform (less biased) distribution leads
to a larger minimum pairwise distance between data points, denoted by
$h_{\min}$, and prove that a smaller $h_{\min}$ can slow down the training
dynamics of gradient descent (GD). Moreover, we theoretically show that the
approximation error of neural networks decreases as $h_{\min}$ increases. Our
analysis introduces a convergence framework for GD beyond the Neural Tangent
Kernel (NTK) regime, applicable to a broad class of architectures, including
transformers, without requiring Lipschitz smoothness. This framework further
provides theoretical justification for the use of residual connections and
function compositions in deep neural architectures. In the end, we conduct
comprehensive experiments for supervised fine-tuning across various settings,
including different optimization strategies, model sizes, and training
datasets. The results consistently demonstrate that selecting data by
maximizing pairwise distance significantly accelerates training and achieves
comparable or better performance in LLMs across diverse datasets. Code and
Datasets are available at the link:
https://github.com/SafeRL-Lab/data-uniformity.

</details>


### [706] [Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives](https://arxiv.org/pdf/2506.24124)
*Dong Sixun, Fan Wei, Teresa Wu, Fu Yanjie*

Main category: cs.LG

TL;DR: A multimodal contrastive learning framework enhances time series forecasting by aligning visual and textual representations derived from numerical sequences, outperforming unimodal and cross-modal baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal numerical inputs struggle with high-level semantic patterns, and existing text-based methods lack perceptual intuition like visual interpretation.

Method: Proposes a framework transforming raw time series into visual and textual perspectives, aligning them via contrastive learning, and using a variate selection module for multivariate forecasting.

Result: Outperforms baselines on fifteen short-term and six long-term forecasting benchmarks, demonstrating the effectiveness of multimodal alignment.

Conclusion: Multimodal alignment significantly enhances time series forecasting by capturing richer and complementary representations.

Abstract: Time series forecasting traditionally relies on unimodal numerical inputs,
which often struggle to capture high-level semantic patterns due to their dense
and unstructured nature. While recent approaches have explored representing
time series as text using large language models (LLMs), these methods remain
limited by the discrete nature of token sequences and lack the perceptual
intuition humans typically apply, such as interpreting visual patterns. In this
paper, we propose a multimodal contrastive learning framework that transforms
raw time series into structured visual and textual perspectives. Rather than
using natural language or real-world images, we construct both modalities
directly from numerical sequences. We then align these views in a shared
semantic space via contrastive learning, enabling the model to capture richer
and more complementary representations. Furthermore, we introduce a variate
selection module that leverages the aligned representations to identify the
most informative variables for multivariate forecasting. Extensive experiments
on fifteen short-term and six long-term forecasting benchmarks demonstrate that
our approach consistently outperforms strong unimodal and cross-modal
baselines, highlighting the effectiveness of multimodal alignment in enhancing
time series forecasting. Code is available at:
https://github.com/Ironieser/TimesCLIP.

</details>


### [707] [DSAC: Distributional Soft Actor-Critic for Risk-Sensitive Reinforcement Learning](https://arxiv.org/pdf/2004.14547)
*Xiaoteng Ma, Junyao Chen, Li Xia, Jun Yang, Qianchuan Zhao, Zhengyuan Zhou*

Main category: cs.LG

TL;DR: DSAC combines distributional RL and entropy-driven exploration, outperforming baselines in continuous control tasks by modeling randomness in actions and rewards.


<details>
  <summary>Details</summary>
Motivation: To unify risk-sensitive learning and exploration by leveraging distributional rewards and entropy, improving performance in control tasks.

Method: DSAC integrates distributional RL with SAC, modeling randomness in rewards and actions, optimizing risk-related objectives while balancing entropy.

Result: DSAC surpasses baseline performances in continuous control tasks, effective for both risk-neutral and risk-sensitive scenarios.

Conclusion: DSAC provides a robust framework for risk-sensitive learning and exploration, enhancing agent performance in diverse control tasks.

Abstract: We present Distributional Soft Actor-Critic (DSAC), a distributional
reinforcement learning (RL) algorithm that combines the strengths of
distributional information of accumulated rewards and entropy-driven
exploration from Soft Actor-Critic (SAC) algorithm. DSAC models the randomness
in both action and rewards, surpassing baseline performances on various
continuous control tasks. Unlike standard approaches that solely maximize
expected rewards, we propose a unified framework for risk-sensitive learning,
one that optimizes the risk-related objective while balancing entropy to
encourage exploration. Extensive experiments demonstrate DSAC's effectiveness
in enhancing agent performances for both risk-neutral and risk-sensitive
control tasks.

</details>


### [708] [Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning](https://arxiv.org/pdf/2310.11594)
*Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong*

Main category: cs.LG

TL;DR: The paper explores how adversarial training in federated learning can be undermined by Adversarial Robustness Unhardening (ARU), making models vulnerable to evasion attacks.


<details>
  <summary>Details</summary>
Motivation: To address the security challenges in federated learning, particularly poisoning and backdoor attacks, and the limitations of adversarial training.

Method: Introduces ARU, a technique used by adversarial clients to weaken model robustness during federated training.

Result: ARU significantly reduces adversarial training's effectiveness and bypasses robust aggregation defenses.

Conclusion: ARU poses a serious threat to federated learning security, highlighting the need for stronger defenses.

Abstract: The delicate equilibrium between user privacy and the ability to unleash the
potential of distributed data is an important concern. Federated learning,
which enables the training of collaborative models without sharing of data, has
emerged as a privacy-centric solution. This approach brings forth security
challenges, notably poisoning and backdoor attacks where malicious entities
inject corrupted data into the training process, as well as evasion attacks
that aim to induce misclassifications at test time. Our research investigates
the intersection of adversarial training, a common defense method against
evasion attacks, and backdoor attacks within federated learning. We introduce
Adversarial Robustness Unhardening (ARU), which is employed by a subset of
adversarial clients to intentionally undermine model robustness during
federated training, rendering models susceptible to a broader range of evasion
attacks. We present extensive experiments evaluating ARU's impact on
adversarial training and existing robust aggregation defenses against poisoning
and backdoor attacks. Our results show that ARU can substantially undermine
adversarial training's ability to harden models against test-time evasion
attacks, and that adversaries employing ARU can even evade robust aggregation
defenses that often neutralize poisoning or backdoor attacks.

</details>


### [709] [Learning Dynamics of LLM Finetuning](https://arxiv.org/pdf/2407.10490)
*Yi Ren, Danica J. Sutherland*

Main category: cs.LG

TL;DR: The paper studies learning dynamics in large language models during finetuning, analyzing influence accumulation among responses to explain phenomena like hallucination and the 'squeezing effect' in DPO. It offers insights for improving alignment performance.


<details>
  <summary>Details</summary>
Motivation: To understand how finetuning influences model predictions and behaviors, particularly in explaining hallucinations and DPO phenomena.

Method: Analyzes step-wise decomposition of influence accumulation during finetuning for instruction and preference tuning.

Result: Identifies mechanisms behind hallucinations and the 'squeezing effect' in DPO, providing insights for on-policy DPO benefits.

Conclusion: The framework offers a novel perspective on LLM finetuning and inspires methods to enhance alignment performance.

Abstract: Learning dynamics, which describes how the learning of specific training
examples influences the model's predictions on other examples, gives us a
powerful tool for understanding the behavior of deep learning systems. We study
the learning dynamics of large language models during different types of
finetuning, by analyzing the step-wise decomposition of how influence
accumulates among different potential responses. Our framework allows a uniform
interpretation of many interesting observations about the training of popular
algorithms for both instruction tuning and preference tuning. In particular, we
propose a hypothetical explanation of why specific types of hallucination are
strengthened after finetuning, e.g., the model might use phrases or facts in
the response for question B to answer question A, or the model might keep
repeating similar simple phrases when generating responses. We also extend our
framework and highlight a unique "squeezing effect" to explain a previously
observed phenomenon in off-policy direct preference optimization (DPO), where
running DPO for too long makes even the desired outputs less likely. This
framework also provides insights into where the benefits of on-policy DPO and
other variants come from. The analysis not only provides a novel perspective of
understanding LLM's finetuning but also inspires a simple, effective method to
improve alignment performance.

</details>


### [710] [Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games](https://arxiv.org/pdf/2312.02312)
*Lukas Schäfer, Logan Jones, Anssi Kanervisto, Yuhan Cao, Tabish Rashid, Raluca Georgescu, Dave Bignell, Siddhartha Sen, Andrea Treviño Gavito, Sam Devlin*

Main category: cs.LG

TL;DR: The paper evaluates pre-trained visual encoders for imitation learning in video games, showing they can reduce training costs and improve performance compared to end-to-end training.


<details>
  <summary>Details</summary>
Motivation: Modern video games are expensive to research, and prior methods rely on game-specific integration or large datasets. The study explores whether pre-trained visual encoders can retain critical decision-making information.

Method: A systematic study of imitation learning using pre-trained visual encoders (e.g., DINOv2) versus end-to-end training in games like Minecraft and Counter-Strike.

Result: End-to-end training works with low-resolution images and minimal demonstrations, but pre-trained encoders like DINOv2 offer significant improvements and cost reductions.

Conclusion: Pre-trained encoders enhance decision-making research accessibility and effectiveness in video games.

Abstract: Video games have served as useful benchmarks for the decision-making
community, but going beyond Atari games towards modern games has been
prohibitively expensive for the vast majority of the research community. Prior
work in modern video games typically relied on game-specific integration to
obtain game features and enable online training, or on existing large datasets.
An alternative approach is to train agents using imitation learning to play
video games purely from images. However, this setting poses a fundamental
question: which visual encoders obtain representations that retain information
critical for decision making? To answer this question, we conduct a systematic
study of imitation learning with publicly available pre-trained visual encoders
compared to the typical task-specific end-to-end training approach in
Minecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our
results show that end-to-end training can be effective with comparably
low-resolution images and only minutes of demonstrations, but significant
improvements can be gained by utilising pre-trained encoders such as DINOv2
depending on the game. In addition to enabling effective decision making, we
show that pre-trained encoders can make decision-making research in video games
more accessible by significantly reducing the cost of training.

</details>


### [711] [Evaluating K-Fold Cross Validation for Transformer Based Symbolic Regression Models](https://arxiv.org/pdf/2410.21896)
*Kaustubh Kislay, Shlok Singh, Soham Joshi, Rohan Dutta, Jay Shim, George Flint, Kevin Zhu*

Main category: cs.LG

TL;DR: Proposes k-fold cross-validation for transformer-based symbolic regression on smaller datasets, improving generalization by 53.31%.


<details>
  <summary>Details</summary>
Motivation: Address performance issues of transformer models in symbolic regression with smaller datasets.

Method: Apply k-fold cross-validation to a transformer model trained on a reduced dataset (15,000 points).

Result: 53.31% relative improvement in validation loss, enhancing output consistency and generalization.

Conclusion: Enables more efficient symbolic regression in resource-constrained settings.

Abstract: Symbolic Regression remains an NP-Hard problem, with extensive research
focusing on AI models for this task. Transformer models have shown promise in
Symbolic Regression, but performance suffers with smaller datasets. We propose
applying k-fold cross-validation to a transformer-based symbolic regression
model trained on a significantly reduced dataset (15,000 data points, down from
500,000). This technique partitions the training data into multiple subsets
(folds), iteratively training on some while validating on others. Our aim is to
provide an estimate of model generalization and mitigate overfitting issues
associated with smaller datasets. Results show that this process improves the
model's output consistency and generalization by a relative improvement in
validation loss of 53.31%. Potentially enabling more efficient and accessible
symbolic regression in resource-constrained environments.

</details>


### [712] [Extended UCB Policies for Frequentist Multi-armed Bandit Problems](https://arxiv.org/pdf/1112.1768)
*Keqin Liu, Tianshuo Zheng, Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: The paper extends the robust UCB policy for heavy-tailed reward distributions in multi-armed bandit problems, achieving optimal regret growth without strict distributional assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing UCB policies require strict conditions on reward distributions, limiting practical applicability. This work aims to relax these constraints for broader use.

Method: Introduces an extended robust UCB policy, generalizing prior work to arbitrary moment conditions (p>q>1) while maintaining optimal regret growth.

Result: Achieves near-optimal regret order O(log T) without requiring full knowledge of reward distributions, only existence of p-th moments (p>1).

Conclusion: The extended robust UCB policy broadens applicability of UCB policies for heavy-tailed rewards, offering theoretical guarantees under relaxed assumptions.

Abstract: The multi-armed bandit (MAB) problem is a widely studied model in the field
of operations research for sequential decision making and reinforcement
learning. This paper mainly considers the classical MAB model with the
heavy-tailed reward distributions. We introduce the extended robust UCB policy,
which is an extension of the pioneering UCB policies proposed by Bubeck et al.
[5] and Lattimore [22]. The previous UCB policies require some strict
conditions on the reward distributions, which can be hard to guarantee in
practical scenarios. Our extended robust UCB generalizes Lattimore's seminary
work (for moments of orders $p=4$ and $q=2$) to arbitrarily chosen $p>q>1$ as
long as the two moments have a known controlled relationship, while still
achieving the optimal regret growth order $O(log T)$, thus providing a
broadened application area of the UCB policies for the heavy-tailed reward
distributions. Furthermore, we achieve a near-optimal regret order without any
knowledge of the reward distributions as long as their $p$-th moments exist for
some $p>1$.

</details>


### [713] [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/pdf/2411.02335)
*Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Xiaojun Meng, Liqun Deng, Jiansheng Wei, Zhiyuan Liu, Maosong Sun*

Main category: cs.LG

TL;DR: The paper studies activation sparsity in LLMs, proposing a new metric (PPL-$p\%$ sparsity) and identifying key trends related to activation functions, training data, and model architecture.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the factors influencing activation sparsity in LLMs, as existing research lacks comprehensive analysis.

Method: Proposes PPL-$p\%$ sparsity metric and conducts extensive experiments on decoder-only Transformer-based LLMs, analyzing activation functions, training data, and model dimensions.

Result: Key findings include ReLU's efficiency over SiLU, the impact of width-depth ratio on activation sparsity, and the weak sensitivity of activation patterns to parameter scale.

Conclusion: The study provides empirical insights for designing more efficient and interpretable LLMs by leveraging activation sparsity.

Abstract: Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.

</details>


### [714] [Deep Support Vectors](https://arxiv.org/pdf/2403.17329)
*Junhoo Lee, Hyunho Lee, Kyomin Hwang, Nojun Kwak*

Main category: cs.LG

TL;DR: The paper introduces DeepKKT, adapting KKT conditions to identify support vectors in deep learning, enabling few-shot learning and reducing black-box issues. It also shows generative model applications.


<details>
  <summary>Details</summary>
Motivation: Address deep learning's need for large datasets and lack of interpretability by identifying support vectors, akin to SVMs.

Method: Propose DeepKKT condition to identify Deep Support Vectors (DSVs), validating their properties and applications in few-shot learning and generative modeling.

Result: DSVs exhibit SVM-like properties, work well in few-shot learning, and enable high-fidelity generative models. Validated on ImageNet, CIFAR10, and CIFAR100.

Conclusion: DeepKKT effectively bridges gaps in deep learning by providing interpretability and efficiency, with practical applications in few-shot and generative tasks.

Abstract: Deep learning has achieved tremendous success. However, unlike SVMs, which
provide direct decision criteria and can be trained with a small dataset, it
still has significant weaknesses due to its requirement for massive datasets
during training and the black-box characteristics on decision criteria. This
paper addresses these issues by identifying support vectors in deep learning
models. To this end, we propose the DeepKKT condition, an adaptation of the
traditional Karush-Kuhn-Tucker (KKT) condition for deep learning models, and
confirm that generated Deep Support Vectors (DSVs) using this condition exhibit
properties similar to traditional support vectors. This allows us to apply our
method to few-shot dataset distillation problems and alleviate the black-box
characteristics of deep learning models. Additionally, we demonstrate that the
DeepKKT condition can transform conventional classification models into
generative models with high fidelity, particularly as latent generative models
using class labels as latent variables. We validate the effectiveness of DSVs
using common datasets (ImageNet, CIFAR10 and CIFAR100) on the general
architectures (ResNet and ConvNet), proving their practical applicability.

</details>


### [715] [FedDTG:Federated Data-Free Knowledge Distillation via Three-Player Generative Adversarial Networks](https://arxiv.org/pdf/2201.03169)
*Lingzhi Gao, Zhenyuan Zhang, Chao Wu*

Main category: cs.LG

TL;DR: FedDTG introduces a GAN-based method for data-free federated distillation, improving efficiency and privacy without needing a proxy dataset.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning methods require sharing private models or proxy datasets, which are impractical or privacy-invasive.

Method: Uses a distributed three-player GAN for data-free mutual distillation, enabling efficient and robust federated learning.

Result: Outperforms other federated distillation algorithms in generalization on benchmark datasets.

Conclusion: FedDTG effectively balances individual client performance, global knowledge acquisition, and data privacy.

Abstract: While existing federated learning approaches primarily focus on aggregating
local models to construct a global model, in realistic settings, some clients
may be reluctant to share their private models due to the inclusion of
privacy-sensitive information. Knowledge distillation, which can extract model
knowledge without accessing model parameters, is well-suited for this federated
scenario. However, most distillation methods in federated learning (federated
distillation) require a proxy dataset, which is difficult to obtain in the real
world. Therefore, in this paper, we introduce a distributed three-player
Generative Adversarial Network (GAN) to implement data-free mutual distillation
and propose an effective method called FedDTG. We confirmed that the fake
samples generated by GAN can make federated distillation more efficient and
robust. Additionally, the distillation process between clients can deliver good
individual client performance while simultaneously acquiring global knowledge
and protecting data privacy. Our extensive experiments on benchmark vision
datasets demonstrate that our method outperforms other federated distillation
algorithms in terms of generalization.

</details>


### [716] [SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?](https://arxiv.org/pdf/2411.18797)
*Haomin Zhuang, Yihua Zhang, Kehan Guo, Jinghan Jia, Gaowen Liu, Sijia Liu, Xiangliang Zhang*

Main category: cs.LG

TL;DR: A novel Selected-Expert Unlearning Framework (SEUF) is proposed to address unlearning challenges in sparse Mixture-of-Experts (MoE) LLMs, improving forget quality and model utility.


<details>
  <summary>Details</summary>
Motivation: MoE LLMs lack effective unlearning methods, leading to excessive forgetting and utility drops when existing techniques are applied.

Method: SEUF uses expert attribution to focus unlearning on engaged experts and applies an anchor loss to stabilize the router.

Result: SEUF improves forget quality by 5% and model utility by 35%, unlearning only 0.06% of parameters.

Conclusion: SEUF is an efficient and effective solution for unlearning in MoE LLMs, compatible with standard algorithms.

Abstract: Recent advancements in LLMs unlearning have shown remarkable success in
removing unwanted data-model influences while preserving the model's utility
for legitimate knowledge. Despite these strides, sparse Mixture-of-Experts
(MoE) LLMs--a key subset of the LLM family--have remained unexplored in the
context of unlearning. As MoE LLMs are celebrated for their exceptional
performance, we ask:How can unlearning be performed effectively and efficiently
on MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs
introduces unique challenges, leading to excessive forgetting, uncontrolled
knowledge erasure and substantial utility drops when existing unlearning
methods are applied. To address this, we propose a novel Selected-Expert
Unlearning Framework (SEUF). Through expert attribution, unlearning is
concentrated on the most actively engaged experts for the specified knowledge.
Concurrently, an anchor loss is applied to the router to stabilize the active
state of this targeted expert, ensuring focused and controlled unlearning. SEUF
is compatible with various standard unlearning algorithms. Extensive
experiments demonstrate that SEUF enhances both forget quality up to 5% and
model utility by 35% on MoE LLMs across various benchmarks and LLM
architectures (compared to standard unlearning algorithms), while only
unlearning 0.06% of the model parameters.

</details>


### [717] [Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI](https://arxiv.org/pdf/2404.08221)
*Archer Amon, Zhipeng Yin, Zichong Wang, Avash Palikhe, Wenbin Zhang*

Main category: cs.LG

TL;DR: The paper explores the intersection of generative AI and copyright law, advocating for multidisciplinary solutions to address infringement risks and reconcile stakeholder interests.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI in creative fields has created legal and technical challenges, necessitating a coordinated approach to adapt copyright laws and mitigate risks.

Method: The survey synthesizes insights from law, policy, economics, and computer science, covering detection methods, regulatory options, and technical safeguards like watermarking and advanced training strategies.

Result: The paper identifies actionable strategies for creators, developers, and policymakers to navigate copyright issues in generative AI.

Conclusion: Multidisciplinary efforts are essential to balance innovation and IP protection, offering practical solutions for the evolving copyright landscape.

Abstract: Generative AI is becoming increasingly prevalent in creative fields, sparking
urgent debates over how current copyright laws can keep pace with technological
innovation. Recent controversies of AI models generating near-replicas of
copyrighted material highlight the need to adapt current legal frameworks and
develop technical methods to mitigate copyright infringement risks. This task
requires understanding the intersection between computational concepts such as
large-scale data scraping and probabilistic content generation, legal
definitions of originality and fair use, and economic impacts on IP rights
holders. However, most existing research on copyright in AI takes a purely
computer science or law-based approach, leaving a gap in coordinating these
approaches that only multidisciplinary efforts can effectively address. To
bridge this gap, our survey adopts a comprehensive approach synthesizing
insights from law, policy, economics, and computer science. It begins by
discussing the foundational goals and considerations that should be applied to
copyright in generative AI, followed by methods for detecting and assessing
potential violations in AI system outputs. Next, it explores various regulatory
options influenced by legal, policy, and economic frameworks to manage and
mitigate copyright concerns associated with generative AI and reconcile the
interests of IP rights holders with that of generative AI producers. The
discussion then introduces techniques to safeguard individual creative works
from unauthorized replication, such as watermarking and cryptographic
protections. Finally, it describes advanced training strategies designed to
prevent AI models from reproducing protected content. In doing so, we highlight
key opportunities for action and offer actionable strategies that creators,
developers, and policymakers can use in navigating the evolving copyright
landscape.

</details>


### [718] [Rethinking Algorithmic Fairness for Human-AI Collaboration](https://arxiv.org/pdf/2310.03647)
*Haosen Ge, Hamsa Bastani, Osbert Bastani*

Main category: cs.LG

TL;DR: The paper addresses fairness in human-AI collaboration, showing selective compliance with fair algorithms can worsen discrimination. It proposes compliance-robustly fair recommendations to improve fairness regardless of human compliance patterns.


<details>
  <summary>Details</summary>
Motivation: Current fairness approaches assume perfect human compliance with algorithms, which is unrealistic. Selective compliance can amplify bias, necessitating new design principles.

Method: The paper defines compliance-robustly fair recommendations and proposes an optimization strategy to ensure fairness under any compliance pattern.

Result: It shows trade-offs: achieving fairness in isolation, compliance robustness, and accuracy may be infeasible. Traditional fairness constraints might not improve equity and accuracy.

Conclusion: The approach is validated on criminal sentencing data, highlighting the need for new fairness principles in human-AI collaboration.

Abstract: Existing approaches to algorithmic fairness aim to ensure equitable outcomes
if human decision-makers comply perfectly with algorithmic decisions. However,
perfect compliance with the algorithm is rarely a reality or even a desirable
outcome in human-AI collaboration. Yet, recent studies have shown that
selective compliance with fair algorithms can amplify discrimination relative
to the prior human policy. As a consequence, ensuring equitable outcomes
requires fundamentally different algorithmic design principles that ensure
robustness to the decision-maker's (a priori unknown) compliance pattern. We
define the notion of compliance-robustly fair algorithmic recommendations that
are guaranteed to (weakly) improve fairness in decisions, regardless of the
human's compliance pattern. We propose a simple optimization strategy to
identify the best performance-improving compliance-robustly fair policy.
However, we show that it may be infeasible to design algorithmic
recommendations that are simultaneously fair in isolation, compliance-robustly
fair, and more accurate than the human policy; thus, if our goal is to improve
the equity and accuracy of human-AI collaboration, it may not be desirable to
enforce traditional algorithmic fairness constraints. We illustrate the value
of our approach on criminal sentencing data before and after the introduction
of an algorithmic risk assessment tool in Virginia.

</details>


### [719] [DReSS: Data-driven Regularized Structured Streamlining for Large Language Models](https://arxiv.org/pdf/2501.17905)
*Mingkuan Feng, Jinyang Wu, Shuai Zhang, Pengpeng Shao, Ruihan Jin, Zhengqi Wen, Jianhua Tao, Feihu Che*

Main category: cs.LG

TL;DR: DReSS introduces a novel pruning paradigm (regularize-prune-finetune) for LLMs, reducing information loss and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address irreversible performance degradation in LLMs due to direct pruning by transferring important information before removal.

Method: DReSS uses data-driven regularization before pruning and finetuning to preserve valuable information.

Result: DReSS outperforms existing methods, reduces latency, and increases throughput under extreme pruning ratios.

Conclusion: DReSS offers an effective solution for pruning LLMs with minimal performance loss.

Abstract: Large language models (LLMs) have achieved significant progress across
various domains, but their increasing scale results in high computational and
memory costs. Recent studies have revealed that LLMs exhibit sparsity,
providing the potential to reduce model size through pruning techniques.
However, existing pruning methods typically follow a prune-then-finetune
paradigm. Since the pruned components still contain valuable information, their
direct removal often leads to irreversible performance degradation, imposing a
substantial computational burden to recover performance during finetuning. In
this paper, we propose a novel paradigm that first applies regularization, then
prunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a
simple and effective Data-driven Regularized Structured Streamlining method for
LLMs. By leveraging a small amount of data to regularize the components to be
pruned, DReSS explicitly transfers the important information to the remaining
parts of the model in advance. Compared to direct pruning, this can reduce the
information loss caused by parameter removal, thereby enhancing its language
modeling capabilities. Experimental results demonstrate that DReSS
significantly outperforms existing pruning methods even under extreme pruning
ratios, significantly reducing latency and increasing throughput.

</details>


### [720] [Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles](https://arxiv.org/pdf/2310.15952)
*Xing Shen, Hengguan Huang, Brennan Nichyporuk, Tal Arbel*

Main category: cs.LG

TL;DR: LaDiNE is a novel ensemble learning method combining Vision Transformers and diffusion models to improve reliability in medical image classification under unexpected corruptions and noise.


<details>
  <summary>Details</summary>
Motivation: Medical image analysis methods often fail under unexpected image corruptions and noise, leading to unreliable predictions and poor confidence calibration. Existing methods address specific issues but lack a unified solution.

Method: LaDiNE uses transformer encoder blocks for robust feature extraction and diffusion models for density estimation, ensuring invariant features and calibrated confidence.

Result: LaDiNE outperforms state-of-the-art methods in accuracy and confidence calibration under noise, adversarial perturbations, and resolution degradation on tuberculosis and melanoma datasets.

Conclusion: LaDiNE effectively addresses multiple challenges in medical image analysis, offering a robust and reliable solution for clinical applications.

Abstract: Once deployed, medical image analysis methods are often faced with unexpected
image corruptions and noise perturbations. These unknown covariate shifts
present significant challenges to deep learning based methods trained on
"clean" images. This often results in unreliable predictions and poorly
calibrated confidence, hence hindering clinical applicability. While recent
methods have been developed to address specific issues such as confidence
calibration or adversarial robustness, no single framework effectively tackles
all these challenges simultaneously. To bridge this gap, we propose LaDiNE, a
novel ensemble learning method combining the robustness of Vision Transformers
with diffusion-based generative models for improved reliability in medical
image classification. Specifically, transformer encoder blocks are used as
hierarchical feature extractors that learn invariant features from images for
each ensemble member, resulting in features that are robust to input
perturbations. In addition, diffusion models are used as flexible density
estimators to estimate member densities conditioned on the invariant features,
leading to improved modeling of complex data distributions while retaining
properly calibrated confidence. Extensive experiments on tuberculosis chest
X-rays and melanoma skin cancer datasets demonstrate that LaDiNE achieves
superior performance compared to a wide range of state-of-the-art methods by
simultaneously improving prediction accuracy and confidence calibration under
unseen noise, adversarial perturbations, and resolution degradation.

</details>


### [721] [CPT: Competence-progressive Training Strategy for Few-shot Node Classification](https://arxiv.org/pdf/2402.00450)
*Qilong Yan, Yufeng Zhang, Jinghao Zhang, Jingpu Duan, Jian Yin*

Main category: cs.LG

TL;DR: CPT introduces a two-stage curriculum learning method for GNNs in few-shot node classification, improving performance by aligning task difficulty with the meta-learner's competence.


<details>
  <summary>Details</summary>
Motivation: Traditional meta-learning ignores task difficulty, leading to suboptimal solutions. CPT mimics human learning by starting with simple tasks and progressing to complex ones.

Method: CPT uses a two-stage approach: initial simpler tasks for foundational skills, followed by dynamic adjustment of task difficulty based on competence.

Result: Experiments show CPT significantly outperforms existing methods on node classification datasets.

Conclusion: CPT enhances GNN performance in few-shot learning by adapting task difficulty to the meta-learner's evolving competence.

Abstract: Graph Neural Networks (GNNs) have made significant advancements in node
classification, but their success relies on sufficient labeled nodes per class
in the training data. Real-world graph data often exhibits a long-tail
distribution with sparse labels, emphasizing the importance of GNNs' ability in
few-shot node classification, which entails categorizing nodes with limited
data. Traditional episodic meta-learning approaches have shown promise in this
domain, but they face an inherent limitation: it might lead the model to
converge to suboptimal solutions because of random and uniform task assignment,
ignoring task difficulty levels. This could lead the meta-learner to face
complex tasks too soon, hindering proper learning. Ideally, the meta-learner
should start with simple concepts and advance to more complex ones, like human
learning. So, we introduce CPT, a novel two-stage curriculum learning method
that aligns task difficulty with the meta-learner's progressive competence,
enhancing overall performance. Specifically, in CPT's initial stage, the focus
is on simpler tasks, fostering foundational skills for engaging with complex
tasks later. Importantly, the second stage dynamically adjusts task difficulty
based on the meta-learner's growing competence, aiming for optimal knowledge
acquisition. Extensive experiments on popular node classification datasets
demonstrate significant improvements of our strategy over existing methods.

</details>


### [722] [Deep Multi-Manifold Transformation Based Multivariate Time Series Fault Detection](https://arxiv.org/pdf/2405.16258)
*Hong Liu, Xiuxiu Qiu, Yiming Shi, Miao Xu, Zelin Zang, Zhen Lei*

Main category: cs.LG

TL;DR: A new method for unsupervised fault detection in multivariate time series combines neighborhood-driven data augmentation and multi-manifold learning to improve adaptability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods assume a single Gaussian distribution for normal data, which fails to capture real-world complexity, leading to poor performance.

Method: Proposes a neighborhood-driven data augmentation strategy and multi-manifold representation learning to simulate contextual variations and enhance feature clustering.

Result: Achieves superior performance in accuracy and robustness on public benchmark datasets, showing strong generalization potential.

Conclusion: The method effectively addresses limitations of traditional approaches and demonstrates practical applicability for real-world deployment.

Abstract: Unsupervised fault detection in multivariate time series plays a vital role
in ensuring the stable operation of complex systems. Traditional methods often
assume that normal data follow a single Gaussian distribution and identify
anomalies as deviations from this distribution. {\color{black} However, this
simplified assumption fails to capture the diversity and structural complexity
of real-world time series, which can lead to misjudgments and reduced detection
performance in practical applications. To address this issue, we propose a new
method that combines a neighborhood-driven data augmentation strategy with a
multi-manifold representation learning framework.} By incorporating information
from local neighborhoods, the augmentation module can simulate contextual
variations of normal data, enhancing the model's adaptability to distributional
changes. In addition, we design a structure-aware feature learning approach
that encourages natural clustering of similar patterns in the feature space
while maintaining sufficient distinction between different operational states.
Extensive experiments on several public benchmark datasets demonstrate that our
method achieves superior performance in terms of both accuracy and robustness,
showing strong potential for generalization and real-world deployment.

</details>


### [723] [Graph Contrastive Learning with Low-Rank Regularization and Low-Rank Attention for Noisy Node Classification](https://arxiv.org/pdf/2402.09600)
*Yancheng Wang, Yingzhen Yang*

Main category: cs.LG

TL;DR: The paper introduces GCL-LRR and GCL-LR-Attention, two robust methods for node representation learning in noisy graph data, using contrastive learning and low-rank regularization, supported by theoretical and empirical results.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of noise in real-world graph data impairing GNN performance by developing robust node representation learning methods.

Method: A two-stage transductive learning framework: (1) GCL-LRR encoder optimization via prototypical contrastive learning with low-rank regularization, (2) linear transductive classifier for label prediction. GCL-LR-Attention adds an LR-Attention layer to improve performance.

Result: Both GCL-LRR and GCL-LR-Attention show effectiveness and robustness in learning node representations, supported by empirical evaluations.

Conclusion: The proposed methods, grounded in theoretical insights, offer improved performance and robustness for node classification in noisy graph data.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in learning
node representations and have shown strong performance in tasks such as node
classification. However, recent findings indicate that the presence of noise in
real-world graph data can substantially impair the effectiveness of GNNs. To
address this challenge, we introduce a robust and innovative node
representation learning method named Graph Contrastive Learning with Low-Rank
Regularization, or GCL-LRR, which follows a two-stage transductive learning
framework for node classification. In the first stage, the GCL-LRR encoder is
optimized through prototypical contrastive learning while incorporating a
low-rank regularization objective. In the second stage, the representations
generated by GCL-LRR are employed by a linear transductive classifier to
predict the labels of unlabeled nodes within the graph. Our GCL-LRR is inspired
by the Low Frequency Property (LFP) of the graph data and its labels, and it is
also theoretically motivated by our sharp generalization bound for transductive
learning. To the best of our knowledge, our theoretical result is among the
first to theoretically demonstrate the advantage of low-rank regularization in
transductive learning, which is also supported by strong empirical results. To
further enhance the performance of GCL-LRR, we present an improved model named
GCL-LR-Attention, which incorporates a novel LR-Attention layer into GCL-LRR.
GCL-LR-Attention reduces the kernel complexity of GCL-LRR and contributes to a
tighter generalization bound, leading to improved performance. Extensive
evaluations on standard benchmark datasets evidence the effectiveness and
robustness of both GCL-LRR and GCL-LR-Attention in learning meaningful node
representations. The code is available at
https://github.com/Statistical-Deep-Learning/GCL-LR-Attention.

</details>


### [724] [Green AI in Action: Strategic Model Selection for Ensembles in Production](https://arxiv.org/pdf/2405.17451)
*Nienke Nijkamp, June Sallou, Niels van der Heijden, Luís Cruz*

Main category: cs.LG

TL;DR: The paper introduces model selection strategies (Static and Dynamic) to balance AI ensemble accuracy and energy consumption, reducing energy usage significantly without major accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The increasing energy demands of AI ensembles, especially in live systems, necessitate methods to optimize energy use while maintaining accuracy.

Method: Two strategies (Static and Dynamic) are proposed to reduce the number or improve efficiency of models in an ensemble during inference.

Result: Static strategy reduces energy usage to 62% and Dynamic to 76%, with further optimizations lowering these to 14% and 57% respectively, while maintaining or improving F1 scores.

Conclusion: Energy-conscious model selection strategies are practical and effective for live AI systems, significantly reducing energy consumption without compromising accuracy.

Abstract: Integrating Artificial Intelligence (AI) into software systems has
significantly enhanced their capabilities while escalating energy demands.
Ensemble learning, combining predictions from multiple models to form a single
prediction, intensifies this problem due to cumulative energy consumption. This
paper presents a novel approach to model selection that addresses the challenge
of balancing the accuracy of AI models with their energy consumption in a live
AI ensemble system. We explore how reducing the number of models or improving
the efficiency of model usage within an ensemble during inference can reduce
energy demands without substantially sacrificing accuracy. This study
introduces and evaluates two model selection strategies, Static and Dynamic,
for optimizing ensemble learning systems performance while minimizing energy
usage. Our results demonstrate that the Static strategy improves the F1 score
beyond the baseline, reducing average energy usage from 100% from the full
ensemble to 62%. The Dynamic strategy further enhances F1 scores, using on
average 76% compared to 100% of the full ensemble. Moreover, we propose an
approach that balances accuracy with resource consumption, significantly
reducing energy usage without substantially impacting accuracy. This method
decreased the average energy usage of the Static strategy from approximately
62% to 14%, and for the Dynamic strategy, from around 76% to 57%. Our field
study of Green AI using an operational AI system developed by a large
professional services provider shows the practical applicability of adopting
energy-conscious model selection strategies in live production environments.

</details>


### [725] [EXPRTS: Exploring and Probing the Robustness of Time Series Forecasting Models](https://arxiv.org/pdf/2403.03508)
*Håkon Hanisch Kjærnli, Lluis Mas-Ribas, Hans Jakob Håland, Vegard Sjåvik, Aida Ashrafi, Helge Langseth, Odd Erik Gundersen*

Main category: cs.LG

TL;DR: A framework for generating interpretable time series to improve forecasting robustness under data distribution drift, demonstrated via the EXPRTS tool.


<details>
  <summary>Details</summary>
Motivation: Machine learning models lack robustness when exposed to out-of-distribution (OOD) data due to distribution drift. Existing methods like deep generative models or genetic algorithms are computationally expensive and lack interpretability.

Method: Combines time-series decompositions with analytic functions to generate interpretable time series matching in- and out-of-distribution data. Demonstrated using EXPRTS, a visual analytics tool.

Result: Generates meaningful OOD time series to improve model robustness, validated through use-cases and a user study.

Conclusion: The framework offers an interpretable and simple solution for time series generation, enhancing forecasting robustness in diverse scenarios.

Abstract: When deploying time series forecasting models based on machine learning to
real world settings, one often encounter situations where the data distribution
drifts. Such drifts expose the forecasting models to out-of-distribution (OOD)
data, and machine learning models lack robustness in these settings. Robustness
can be improved by using deep generative models or genetic algorithms to
augment time series datasets, but these approaches lack interpretability and
are computationally expensive. In this work, we develop an interpretable and
simple framework for generating time series. Our method combines time-series
decompositions with analytic functions, and is able to generate time series
with characteristics matching both in- and out-of-distribution data. This
approach allows users to generate new time series in an interpretable fashion,
which can be used to augment the dataset and improve forecasting robustness. We
demonstrate our framework through EXPRTS, a visual analytics tool designed for
univariate time series forecasting models and datasets. Different
visualizations of the data distribution, forecasting errors and single time
series instances enable users to explore time series datasets, apply
transformations, and evaluate forecasting model robustness across diverse
scenarios. We show how our framework can generate meaningful OOD time series
that improve model robustness, and we validate EXPRTS effectiveness and
usability through three use-cases and a user study.

</details>


### [726] [Vulnerable Road User Detection and Safety Enhancement: A Comprehensive Survey](https://arxiv.org/pdf/2405.19202)
*Renato M. Silva, Gregorio F. Azevedo, Matheus V. V. Berto, Jean R. Rocha, Eduardo C. Fidelis, Matheus V. Nogueira, Pedro H. Lisboa, Tiago A. Almeida*

Main category: cs.LG

TL;DR: A survey of technologies and methodologies to enhance VRU safety, covering communication networks, sensor integration, data processing, simulation, detection algorithms, and behavior prediction.


<details>
  <summary>Details</summary>
Motivation: To address the high proportion of global road accidents involving VRUs by leveraging advanced technologies and datasets.

Method: Comprehensive survey of state-of-the-art technologies, including communication networks, sensor data fusion, simulation environments, detection algorithms, and behavior prediction methods.

Result: Identifies current advancements in VRU safety technologies and highlights gaps requiring further research.

Conclusion: The survey provides a holistic view of VRU safety technologies, emphasizing the need for continued innovation to reduce traffic casualties.

Abstract: Traffic incidents involving vulnerable road users (VRUs) constitute a
significant proportion of global road accidents. Advances in traffic
communication ecosystems, coupled with sophisticated signal processing and
machine learning techniques, have facilitated the utilization of data from
diverse sensors. Despite these advancements and the availability of extensive
datasets, substantial progress is required to mitigate traffic casualties. This
paper provides a comprehensive survey of state-of-the-art technologies and
methodologies to enhance the safety of VRUs. The study investigates the
communication networks between vehicles and VRUs, emphasizing the integration
of advanced sensors and the availability of relevant datasets. It explores
preprocessing techniques and data fusion methods to enhance sensor data
quality. Furthermore, our study assesses critical simulation environments
essential for developing and testing VRU safety systems. Our research also
highlights recent advances in VRU detection and classification algorithms,
addressing challenges such as variable environmental conditions. Additionally,
we cover cutting-edge research in predicting VRU intentions and behaviors,
which is mandatory for proactive collision avoidance strategies. Through this
survey, we aim to provide a comprehensive understanding of the current
landscape of VRU safety technologies, identifying areas of progress and areas
needing further research and development.

</details>


### [727] [Robustness of Decentralised Learning to Nodes and Data Disruption](https://arxiv.org/pdf/2405.02377)
*Luigi Palmieri, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, Marco Conti, János Kertész*

Main category: cs.LG

TL;DR: Decentralized learning is robust to network disruptions, maintaining accuracy even with minimal data availability post-disruption.


<details>
  <summary>Details</summary>
Motivation: Address robustness challenges in decentralized learning, focusing on node disruptions and their impact on collective learning.

Method: Study disruptions by varying data distributions and timing, analyzing network properties and knowledge persistence.

Result: Decentralized learning recovers from disruptions, achieving significant accuracy with minimal data, even in isolated nodes.

Conclusion: Decentralized learning is resilient, with knowledge persistence and recovery capabilities under disruptions.

Abstract: In the vibrant landscape of AI research, decentralised learning is gaining
momentum. Decentralised learning allows individual nodes to keep data locally
where they are generated and to share knowledge extracted from local data among
themselves through an interactive process of collaborative refinement. This
paradigm supports scenarios where data cannot leave local nodes due to privacy
or sovereignty reasons or real-time constraints imposing proximity of models to
locations where inference has to be carried out. The distributed nature of
decentralised learning implies significant new research challenges with respect
to centralised learning. Among them, in this paper, we focus on robustness
issues. Specifically, we study the effect of nodes' disruption on the
collective learning process. Assuming a given percentage of "central" nodes
disappear from the network, we focus on different cases, characterised by (i)
different distributions of data across nodes and (ii) different times when
disruption occurs with respect to the start of the collaborative learning task.
Through these configurations, we are able to show the non-trivial interplay
between the properties of the network connecting nodes, the persistence of
knowledge acquired collectively before disruption or lack thereof, and the
effect of data availability pre- and post-disruption. Our results show that
decentralised learning processes are remarkably robust to network disruption.
As long as even minimum amounts of data remain available somewhere in the
network, the learning process is able to recover from disruptions and achieve
significant classification accuracy. This clearly varies depending on the
remaining connectivity after disruption, but we show that even nodes that
remain completely isolated can retain significant knowledge acquired before the
disruption.

</details>


### [728] [Addressing the Inconsistency in Bayesian Deep Learning via Generalized Laplace Approximation](https://arxiv.org/pdf/2405.13535)
*Yinsong Chen, Samson S. Yu, Zhong Li, Chee Peng Lim*

Main category: cs.LG

TL;DR: The paper explores posterior tempering in Bayesian deep learning, interpreting it as a correction for model misspecification and prior recalibration. It introduces the generalized Laplace approximation, which maintains an invariant normalizing constant and improves predictive performance.


<details>
  <summary>Details</summary>
Motivation: Addressing inconsistency and understanding the mechanisms of generalized posteriors in Bayesian deep learning.

Method: Proposes the generalized Laplace approximation, modifying Hessian calculations for scalable posterior inference.

Result: Demonstrates enhanced predictive performance on state-of-the-art neural networks and real-world datasets.

Conclusion: The generalized Laplace approximation offers a flexible, scalable solution for high-quality posterior inference in Bayesian deep learning.

Abstract: In recent years, inconsistency in Bayesian deep learning has attracted
significant attention. Tempered or generalized posterior distributions are
frequently employed as direct and effective solutions. Nonetheless, the
underlying mechanisms and the effectiveness of generalized posteriors remain
active research topics. In this work, we interpret posterior tempering as a
correction for model misspecification via adjustments to the joint probability,
and as a recalibration of priors by reducing aleatoric uncertainty. We also
identify a unique property of the Laplace approximation: the generalized
normalizing constant remains invariant, in contrast to general Bayesian
learning, where this constant typically depends on model parameters after
generalization. Leveraging this property, we introduce the generalized Laplace
approximation, which requires only a simple modification to the Hessian
calculation of the regularized loss. This approach provides a flexible and
scalable framework for high-quality posterior inference. We evaluate the
proposed method on state-of-the-art neural networks and real-world datasets,
demonstrating that the generalized Laplace approximation enhances predictive
performance.

</details>


### [729] [Challenging Gradient Boosted Decision Trees with Tabular Transformers for Fraud Detection at Booking.com](https://arxiv.org/pdf/2405.13692)
*Sergei Krutikov, Bulat Khaertdinov, Rodion Kiriukhin, Shubham Agrawal, Mozhdeh Ariannezhad, Kees Jan De Vries*

Main category: cs.LG

TL;DR: Tabular Transformers, enhanced by SSL, outperform GBDTs in fraud detection tasks, addressing selection bias and leveraging large-scale data for superior results.


<details>
  <summary>Details</summary>
Motivation: To challenge GBDTs with tabular Transformers in fraud detection, especially considering selection bias in real-life systems.

Method: Leverage SSL for transferable representations, pre-train on large datasets, and fine-tune on smaller target datasets.

Result: Outperforms GBDTs in offline evaluations (higher AP score) and online A/B tests (significant business metric improvement).

Conclusion: Tabular Transformers are superior to GBDTs in fraud detection, validated by both offline and online experiments.

Abstract: Transformer-based neural networks, empowered by Self-Supervised Learning
(SSL), have demonstrated unprecedented performance across various domains.
However, related literature suggests that tabular Transformers may struggle to
outperform classical Machine Learning algorithms, such as Gradient Boosted
Decision Trees (GBDT). In this paper, we aim to challenge GBDTs with tabular
Transformers on a typical task faced in e-commerce, namely fraud detection. Our
study is additionally motivated by the problem of selection bias, often
occurring in real-life fraud detection systems. It is caused by the production
system affecting which subset of traffic becomes labeled. This issue is
typically addressed by sampling randomly a small part of the whole production
data, referred to as a Control Group. This subset follows a target distribution
of production data and therefore is usually preferred for training
classification models with standard ML algorithms. Our methodology leverages
the capabilities of Transformers to learn transferable representations using
all available data by means of SSL, giving it an advantage over classical
methods. Furthermore, we conduct large-scale experiments, pre-training tabular
Transformers on vast amounts of data instances and fine-tuning them on smaller
target datasets. The proposed approach outperforms heavily tuned GBDTs by a
considerable margin of the Average Precision (AP) score in offline evaluations.
Finally, we report the results of an online A/B experiment. Experimental
results confirm the superiority of tabular Transformers compared to GBDTs in
production, demonstrated by a statistically significant improvement in our
business metric.

</details>


### [730] [Detecting Sockpuppetry on Wikipedia Using Meta-Learning](https://arxiv.org/pdf/2506.10314)
*Luc Raszewski, Christine De Kock*

Main category: cs.LG

TL;DR: Meta-learning improves sockpuppet detection on Wikipedia by adapting to author-specific behaviors, outperforming pre-trained models in data-scarce settings.


<details>
  <summary>Details</summary>
Motivation: To enhance sockpuppet detection by addressing limitations of prior methods, which lack adaptability to specific author behaviors and struggle with limited text data.

Method: Proposes meta-learning to train models across multiple tasks, enabling rapid adaptation to new sockpuppet-group writing styles.

Result: Meta-learning significantly boosts prediction precision compared to pre-trained models.

Conclusion: The approach advances sockpuppetry detection and introduces a new dataset to support future research in meta-learning and sockpuppetry.

Abstract: Malicious sockpuppet detection on Wikipedia is critical to preserving access
to reliable information on the internet and preventing the spread of
disinformation. Prior machine learning approaches rely on stylistic and
meta-data features, but do not prioritise adaptability to author-specific
behaviours. As a result, they struggle to effectively model the behaviour of
specific sockpuppet-groups, especially when text data is limited. To address
this, we propose the application of meta-learning, a machine learning technique
designed to improve performance in data-scarce settings by training models
across multiple tasks. Meta-learning optimises a model for rapid adaptation to
the writing style of a new sockpuppet-group. Our results show that
meta-learning significantly enhances the precision of predictions compared to
pre-trained models, marking an advancement in combating sockpuppetry on open
editing platforms. We release a new dataset of sockpuppet investigations to
foster future research in both sockpuppetry and meta-learning fields.

</details>


### [731] [Multi-Modal Recommendation Unlearning for Legal, Licensing, and Modality Constraints](https://arxiv.org/pdf/2405.15328)
*Yash Sinha, Murari Mandal, Mohan Kankanhalli*

Main category: cs.LG

TL;DR: MMRecUn is a novel method for unlearning item data in multi-modal recommender systems (MMRS), outperforming baselines with improved recall and speed.


<details>
  <summary>Details</summary>
Motivation: Address the gap in unlearning methods for MMRS, particularly for item data related to outdated preferences or legal removals, as existing uni-modal methods are incompatible.

Method: Uses a Reverse Bayesian Personalized Ranking (BPR) objective to forget marked data while reinforcing retain set interactions, avoiding costly data partitioning.

Result: Achieves up to 49.85% recall improvement and 1.3x faster performance than baselines, with zero overhead costs.

Conclusion: MMRecUn effectively removes target interactions, preserves retained ones, and is efficient, making it a superior solution for MMRS unlearning.

Abstract: User data spread across multiple modalities has popularized multi-modal
recommender systems (MMRS). They recommend diverse content such as products,
social media posts, TikTok reels, etc., based on a user-item interaction graph.
With rising data privacy demands, recent methods propose unlearning private
user data from uni-modal recommender systems (RS). However, methods for
unlearning item data related to outdated user preferences, revoked licenses,
and legally requested removals are still largely unexplored.
  Previous RS unlearning methods are unsuitable for MMRS due to the
incompatibility of their matrix-based representation with the multi-modal
user-item interaction graph. Moreover, their data partitioning step degrades
performance on each shard due to poor data heterogeneity and requires costly
performance aggregation across shards.
  This paper introduces MMRecUn, the first approach known to us for unlearning
in MMRS and unlearning item data. Given a trained RS model, MMRecUn employs a
novel Reverse Bayesian Personalized Ranking (BPR) objective to enable the model
to forget marked data. The reverse BPR attenuates the impact of user-item
interactions within the forget set, while the forward BPR reinforces the
significance of user-item interactions within the retain set. Our experiments
demonstrate that MMRecUn outperforms baseline methods across various unlearning
requests when evaluated on benchmark MMRS datasets. MMRecUn achieves recall
performance improvements of up to 49.85% compared to baseline methods and is up
to 1.3x faster than the Gold model, which is trained on retain set from
scratch. MMRecUn offers significant advantages, including superiority in
removing target interactions, preserving retained interactions, and zero
overhead costs compared to previous methods.
  Code: https://github.com/MachineUnlearn/MMRecUN
  Extended version: arXiv:2405.15328

</details>


### [732] [Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization](https://arxiv.org/pdf/2506.12484)
*Filip Sondej, Yushi Yang, Mikołaj Kniejski, Marcel Windys*

Main category: cs.LG

TL;DR: MUDMAN introduces Disruption Masking and normalization for irreversible unlearning, outperforming prior methods by 40%.


<details>
  <summary>Details</summary>
Motivation: Addressing the risks of language models retaining dangerous knowledge despite safety fine-tuning.

Method: Disruption Masking, gradient normalization, and meta-learning combined in MUDMAN.

Result: MUDMAN prevents recovery of dangerous capabilities, outperforming TAR by 40%.

Conclusion: MUDMAN sets a new state-of-the-art for robust unlearning.

Abstract: Language models can retain dangerous knowledge and skills even after
extensive safety fine-tuning, posing both misuse and misalignment risks. Recent
studies show that even specialized unlearning methods can be easily reversed.
To address this, we systematically evaluate many existing and novel components
of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating
weights, where the signs of the unlearning gradient and the retaining gradient
are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients,
and also confirm the usefulness of meta-learning. We combine these insights
into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and
validate its effectiveness at preventing the recovery of dangerous
capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new
state-of-the-art for robust unlearning.

</details>


### [733] [Early-Stage Anomaly Detection: A Study of Model Performance on Complete vs. Partial Flows](https://arxiv.org/pdf/2407.02856)
*Adrian Pekar, Richard Jozsa*

Main category: cs.LG

TL;DR: Study shows machine learning models (Random Forest) for network security threat detection perform worse with partial flow data, highlighting a 30% drop in precision/recall and a 7-packet threshold for reliable detection.


<details>
  <summary>Details</summary>
Motivation: Address the gap between research settings (complete flow data) and real-time operational needs (partial flow data) in network security threat detection.

Method: Systematically evaluate Random Forest performance under varying training/testing conditions (complete/complete, partial/partial, complete/partial) with incomplete data.

Result: Precision and recall drop by up to 30% when models trained on complete flows are tested on partial flows. A 7-packet threshold is needed for reliable detection.

Conclusion: Quantified insights for realistic real-time detection strategies, emphasizing the need to account for partial data in model training.

Abstract: This study investigates the efficacy of machine learning models in network
security threat detection through the critical lens of partial versus complete
flow information, addressing a common gap between research settings and
real-time operational needs. We systematically evaluate how a standard
benchmark model, Random Forest, performs under varying training and testing
conditions (complete/complete, partial/partial, complete/partial), quantifying
the performance impact when dealing with the incomplete data typical in
real-time environments. Our findings demonstrate a significant performance
difference, with precision and recall dropping by up to 30% under certain
conditions when models trained on complete flows are tested against partial
flows. The study also reveals that, for the evaluated dataset and model, a
minimum threshold around 7 packets in the test set appears necessary for
maintaining reliable detection rates, providing valuable, quantified insights
for developing more realistic real-time detection strategies.

</details>


### [734] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/pdf/2506.19882)
*Rylan Schaeffer, Joshua Kazdan, Yegor Denisov-Blanch, Brando Miranda, Matthias Gerstgrasser, Susan Zhang, Andreas Haupt, Isha Gupta, Elyas Obbad, Jesse Dodge, Jessica Zosa Forde, Koustuv Sinha, Francesco Orabona, Sanmi Koyejo, David Donoho*

Main category: cs.LG

TL;DR: The paper proposes a "Refutations and Critiques" (R&C) Track at ML conferences to systematically correct errors and flawed studies in the field.


<details>
  <summary>Details</summary>
Motivation: Rapid advancements in ML research have led to flawed or incorrect studies being accepted due to peer review fallibility. Current conferences lack robust correction mechanisms.

Method: The paper suggests designing an R&C Track with specific review principles, addressing potential pitfalls, and provides an example submission.

Result: The proposed R&C Track would foster a self-correcting research ecosystem by critically challenging prior work.

Conclusion: ML conferences should establish official mechanisms like the R&C Track to improve research integrity and self-correction.

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made. This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R&C) Track. This R&C Track would provide
a high-profile, reputable platform to support vital research that critically
challenges prior research, thereby fostering a dynamic self-correcting research
ecosystem. We discuss key considerations including track design, review
principles, potential pitfalls, and provide an illustrative example submission
concerning a recent ICLR 2025 Oral. We conclude that ML conferences should
create official, reputable mechanisms to help ML research self-correct.

</details>


### [735] [Remove Symmetries to Control Model Expressivity and Improve Optimization](https://arxiv.org/pdf/2408.15495)
*Liu Ziyin, Yizhou Xu, Isaac Chuang*

Main category: cs.LG

TL;DR: The paper addresses how symmetry in loss functions can trap models in low-capacity states, proposes a method (syre) to mitigate this, and demonstrates its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Symmetry in loss functions can lead to models being trapped in low-capacity states, hindering training and performance in deep learning applications.

Method: The authors propose syre, a simple, model-agnostic algorithm to remove symmetry-induced low-capacity states without prior knowledge of the symmetry.

Result: Removing symmetries with syre correlates with improved optimization and performance, especially in cases where entrapment is a concern.

Conclusion: The syre method effectively mitigates symmetry-induced low-capacity states, enhancing model performance without requiring symmetry knowledge.

Abstract: When symmetry is present in the loss function, the model is likely to be
trapped in a low-capacity state that is sometimes known as a "collapse". Being
trapped in these low-capacity states can be a major obstacle to training across
many scenarios where deep learning technology is applied. We first prove two
concrete mechanisms through which symmetries lead to reduced capacities and
ignored features during training and inference. We then propose a simple and
theoretically justified algorithm, syre, to remove almost all symmetry-induced
low-capacity states in neural networks. When this type of entrapment is
especially a concern, removing symmetries with the proposed method is shown to
correlate well with improved optimization or performance. A remarkable merit of
the proposed method is that it is model-agnostic and does not require any
knowledge of the symmetry.

</details>


### [736] [From Diffusion to Transformers: A Unified Framework for Neural Message Passing](https://arxiv.org/pdf/2409.09111)
*Qitian Wu, David Wipf, Junchi Yan*

Main category: cs.LG

TL;DR: The paper proposes an energy-constrained diffusion model for learning structured data representations, unifying MPNNs under a mathematical framework and introducing diffusion-inspired Transformers.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning representations for structured data with geometries, leveraging insights from physical systems.

Method: Combines diffusion on manifolds with energy minimization constraints, linking diffusion operators to energy functions and deriving MPNN propagation layers.

Result: Introduces diffusion-inspired Transformers, achieving strong performance across diverse datasets with observed, partially observed, or unobserved structures.

Conclusion: The framework unifies common neural architectures and offers a principled approach for message-passing models, validated by empirical success.

Abstract: Learning representations for structured data with certain geometries (e.g.,
observed or unobserved) is a fundamental challenge, wherein message passing
neural networks (MPNNs) have become a de facto class of model solutions. In
this paper, inspired by physical systems, we propose an energy-constrained
diffusion model, which combines the inductive bias of diffusion on manifolds
with layer-wise constraints of energy minimization. We identify that the
diffusion operators have a one-to-one correspondence with the energy functions
implicitly descended by the diffusion process, and the finite-difference
iteration for solving the energy-constrained diffusion system induces the
propagation layers of various types of MPNNs operating on observed or latent
structures. This leads to a unified mathematical framework for common neural
architectures whose computational flows can be cast as message passing (or its
special case), including MLPs, GNNs, and Transformers. Building on these
insights, we devise a new class of neural message passing models, dubbed
diffusion-inspired Transformers, whose global attention layers are derived from
the principled energy-constrained diffusion framework. Across diverse datasets
ranging from real-world networks to images, texts, and physical particles, we
demonstrate that the new model achieves promising performance in scenarios
where the data structures are observed (as a graph), partially observed, or
entirely unobserved.

</details>


### [737] [CauSkelNet: Causal Representation Learning for Human Behaviour Analysis](https://arxiv.org/pdf/2409.15564)
*Xingrui Gu, Chuyi Jiang, Erte Wang, Qiang Cui, Leimin Tian, Lianlong Wu, Siyang Song, Chuang Yu*

Main category: cs.LG

TL;DR: A novel causal inference-based framework improves movement recognition by capturing joint interactions, outperforming traditional methods in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional machine learning in movement recognition, such as poor interpretability and lack of insight into movement dynamics.

Method: A two-stage approach using the Peter-Clark (PC) algorithm and Kullback-Leibler (KL) divergence to identify and quantify causal relationships between human joints, integrated into a causal Graph Convolutional Network (GCN).

Result: The causal GCN outperforms traditional GCNs in accuracy, F1 score, and recall on the EmoPain dataset, especially in detecting protective behaviors.

Conclusion: The framework advances human motion analysis and supports adaptive, intelligent healthcare solutions.

Abstract: Traditional machine learning methods for movement recognition often struggle
with limited model interpretability and a lack of insight into human movement
dynamics. This study introduces a novel representation learning framework based
on causal inference to address these challenges. Our two-stage approach
combines the Peter-Clark (PC) algorithm and Kullback-Leibler (KL) divergence to
identify and quantify causal relationships between human joints. By capturing
joint interactions, the proposed causal Graph Convolutional Network (GCN)
produces interpretable and robust representations. Experimental results on the
EmoPain dataset demonstrate that the causal GCN outperforms traditional GCNs in
accuracy, F1 score, and recall, particularly in detecting protective behaviors.
This work contributes to advancing human motion analysis and lays a foundation
for adaptive and intelligent healthcare solutions.

</details>


### [738] [Deep Unlearn: Benchmarking Machine Unlearning for Image Classification](https://arxiv.org/pdf/2410.01276)
*Xavier F. Cadet, Anastasia Borovykh, Mohammad Malekzadeh, Sara Ahmadi-Abhari, Hamed Haddadi*

Main category: cs.LG

TL;DR: The paper evaluates 18 machine unlearning (MU) methods for deep neural networks (DNNs), identifying MSG and CT as top performers in accuracy and efficiency, while advocating for better baselines like NG+ over traditional ones like GA or SRL.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of rigorous studies on MU for DNNs, especially given privacy and safety concerns, the paper aims to comprehensively evaluate MU methods.

Method: The study tests 18 MU methods across various datasets and models, with 10 initializations each, totaling 100K models, using MIA and U-LiRA for assessment.

Result: MSG and CT outperform others in accuracy and efficiency, while traditional baselines like GA or SRL are deemed inadequate compared to NG+.

Conclusion: The paper underscores the need for better MU baselines and highlights MSG and CT as effective methods for DNN unlearning.

Abstract: Machine unlearning (MU) aims to remove the influence of particular data
points from the learnable parameters of a trained machine learning model. This
is a crucial capability in light of data privacy requirements, trustworthiness,
and safety in deployed models. MU is particularly challenging for deep neural
networks (DNNs), such as convolutional nets or vision transformers, as such
DNNs tend to memorize a notable portion of their training dataset.
Nevertheless, the community lacks a rigorous and multifaceted study that looks
into the success of MU methods for DNNs. In this paper, we investigate 18
state-of-the-art MU methods across various benchmark datasets and models, with
each evaluation conducted over 10 different initializations, a comprehensive
evaluation involving MU over 100K models. We show that, with the proper
hyperparameters, Masked Small Gradients (MSG) and Convolution Transpose (CT),
consistently perform better in terms of model accuracy and run-time efficiency
across different models, datasets, and initializations, assessed by
population-based membership inference attacks (MIA) and per-sample unlearning
likelihood ratio attacks (U-LiRA). Furthermore, our benchmark highlights the
fact that comparing a MU method only with commonly used baselines, such as
Gradient Ascent (GA) or Successive Random Relabeling (SRL), is inadequate, and
we need better baselines like Negative Gradient Plus (NG+) with proper
hyperparameter selection.

</details>


### [739] [Enhancing Adversarial Robustness through Multi-Objective Representation Learning](https://arxiv.org/pdf/2410.01697)
*Sedjro Salomon Hotegni, Sebastian Peitz*

Main category: cs.LG

TL;DR: MOREL improves DNN robustness by aligning natural and adversarial features during training using cosine similarity and multi-positive contrastive losses.


<details>
  <summary>Details</summary>
Motivation: DNNs are vulnerable to adversarial perturbations, and existing defenses add complexity or are architecture-dependent. Robust feature learning during training can enhance robustness.

Method: Proposes MOREL, a multi-objective approach using cosine similarity and multi-positive contrastive losses to align features of same-class inputs.

Result: MOREL significantly improves robustness against white-box and black-box attacks.

Conclusion: Robust feature learning during training, as demonstrated by MOREL, effectively enhances DNN robustness without adding complexity.

Abstract: Deep neural networks (DNNs) are vulnerable to small adversarial
perturbations, which are tiny changes to the input data that appear
insignificant but cause the model to produce drastically different outputs.
Many defense methods require modifying model architectures during evaluation or
performing test-time data purification. This not only introduces additional
complexity but is often architecture-dependent. We show, however, that robust
feature learning during training can significantly enhance DNN robustness. We
propose MOREL, a multi-objective approach that aligns natural and adversarial
features using cosine similarity and multi-positive contrastive losses to
encourage similar features for same-class inputs. Extensive experiments
demonstrate that MOREL significantly improves robustness against both white-box
and black-box attacks. Our code is available at
https://github.com/salomonhotegni/MOREL

</details>


### [740] [MOE-Enhanced Explanable Deep Manifold Transformation for Complex Data Embedding and Visualization](https://arxiv.org/pdf/2410.19504)
*Zelin Zang, Yuhao Wang, Jinlin Wu, Hong Liu, Yue Shen, Zhen Lei, Stan. Z Li*

Main category: cs.LG

TL;DR: DMT-ME combines hyperbolic embeddings and Mixture of Experts to improve dimensionality reduction accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between accuracy and explainability in dimensionality reduction for high-dimensional data.

Method: Uses hyperbolic embeddings for hierarchical data representation and Mixture of Experts for dynamic task allocation.

Result: Superior performance in both accuracy and explainability compared to traditional methods.

Conclusion: DMT-ME is a robust solution for complex data analysis, balancing accuracy and explainability.

Abstract: Dimensionality reduction (DR) plays a crucial role in various fields,
including data engineering and visualization, by simplifying complex datasets
while retaining essential information. However, achieving both high DR accuracy
and strong explainability remains a fundamental challenge, especially for users
dealing with high-dimensional data. Traditional DR methods often face a
trade-off between precision and transparency, where optimizing for performance
can lead to reduced explainability, and vice versa. This limitation is
especially prominent in real-world applications such as image, tabular, and
text data analysis, where both accuracy and explainability are critical. To
address these challenges, this work introduces the MOE-based Explainable Deep
Manifold Transformation (DMT-ME). The proposed approach combines hyperbolic
embeddings, which effectively capture complex hierarchical structures, with
Mixture of Experts (MOE) models, which dynamically allocate tasks based on
input features. DMT-ME enhances DR accuracy by leveraging hyperbolic embeddings
to represent the hierarchical nature of data, while also improving
explainability by explicitly linking input data, embedding outcomes, and key
features through the MOE structure. Extensive experiments demonstrate that
DMT-ME consistently achieves superior performance in both DR accuracy and model
explainability, making it a robust solution for complex data analysis. The code
is available at https://github.com/zangzelin/code_dmtme

</details>


### [741] [Graph Fourier Neural ODEs: Modeling Spatial-temporal Multi-scales in Molecular Dynamics](https://arxiv.org/pdf/2411.01600)
*Fang Sun, Zijie Huang, Haixin Wang, Huacong Tang, Xiao Luo, Wei Wang, Yizhou Sun*

Main category: cs.LG

TL;DR: GF-NODE, a method combining graph Fourier transform and Neural ODEs, improves long-horizon molecular dynamics predictions by capturing multi-scale interactions.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with long-horizon MD predictions due to difficulties in modeling interactions across spatial and temporal scales.

Method: GF-NODE decomposes molecular configurations into spatial frequency modes using graph Laplacian, evolves them via Neural ODE, and reconstructs geometry.

Result: GF-NODE achieves state-of-the-art accuracy in MD benchmarks, preserving geometrical features over extended simulations.

Conclusion: Bridging spectral decomposition with continuous-time modeling enhances MD simulation robustness and predictive power.

Abstract: Accurately predicting long-horizon molecular dynamics (MD) trajectories
remains a significant challenge, as existing deep learning methods often
struggle to retain fidelity over extended simulations. We hypothesize that one
key factor limiting accuracy is the difficulty of capturing interactions that
span distinct spatial and temporal scales, ranging from high-frequency local
vibrations to low-frequency global conformational changes. To address these
limitations, we propose Graph Fourier Neural ODEs (GF-NODE), integrating a
graph Fourier transform for spatial frequency decomposition with a Neural ODE
framework for continuous-time evolution. Specifically, GF-NODE first decomposes
molecular configurations into multiple spatial frequency modes using the graph
Laplacian, then evolves the frequency components in time via a learnable Neural
ODE module that captures both local and global dynamics, and finally
reconstructs the updated molecular geometry through an inverse graph Fourier
transform. By explicitly modeling high- and low-frequency phenomena in this
unified pipeline, GF-NODE captures long-range correlations and local
fluctuations more effectively. We provide theoretical insight through heat
equation analysis on a simplified diffusion model, demonstrating how graph
Laplacian eigenvalues can determine temporal dynamics scales, and crucially
validate this correspondence through comprehensive empirical analysis on real
molecular dynamics trajectories showing quantitative spatial-temporal
correlations across diverse molecular systems. Experimental results on
challenging MD benchmarks demonstrate that GF-NODE achieves state-of-the-art
accuracy while preserving essential geometrical features over extended
simulations. These findings highlight the promise of bridging spectral
decomposition with continuous-time modeling to improve the robustness and
predictive power of MD simulations.

</details>


### [742] [Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional Trajectories Through Manifold Learning: A Linear Approach](https://arxiv.org/pdf/2411.02058)
*Gionni Marchetti*

Main category: cs.LG

TL;DR: The paper proposes an unsupervised machine learning approach to infer the intrinsic dimension of trajectories in the FPUT model, revealing a link between dimension and nonlinear strength.


<details>
  <summary>Details</summary>
Motivation: To understand the intrinsic dimensionality of high-dimensional trajectories in the FPUT model and its relationship with nonlinearity.

Method: Principal component analysis (PCA) is applied to trajectory data, and intrinsic dimension is estimated using participation ratio, Kaiser rule, and the Kneedle algorithm.

Result: The intrinsic dimension increases with nonlinearity. In weakly nonlinear regimes, quasi-periodic motion on a low-dimensional manifold explains energy recurrences.

Conclusion: The study highlights the role of low-dimensional manifolds in the FPUT model's dynamics, particularly in weakly nonlinear regimes.

Abstract: A data-driven approach based on unsupervised machine learning is proposed to
infer the intrinsic dimension $m^{\ast}$ of the high-dimensional trajectories
of the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis
(PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints,
of the FPUT $\beta$ model with $N = 32$ coupled oscillators, revealing a
critical relationship between $m^{\ast}$ and the model's nonlinear strength. By
estimating the intrinsic dimension $m^{\ast}$ using multiple methods
(participation ratio, Kaiser rule, and the Kneedle algorithm), it is found that
$m^{\ast}$ increases with the model nonlinearity. Interestingly, in the weakly
nonlinear regime, for trajectories initialized by exciting the first mode, the
participation ratio estimates $m^{\ast} = 2, 3$, strongly suggesting that
quasi-periodic motion on a low-dimensional Riemannian manifold underlies the
characteristic energy recurrences observed in the FPUT model.

</details>


### [743] [Foundation Models for Wearable Movement Data in Mental Health Research](https://arxiv.org/pdf/2411.15240)
*Franklin Y. Ruan, Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, Nicholas C. Jacobson*

Main category: cs.LG

TL;DR: PAT is a lightweight, interpretable foundation model for wearable movement data, achieving state-of-the-art performance in mental health tasks.


<details>
  <summary>Details</summary>
Motivation: Advancements in health data modeling lag behind AI breakthroughs like LLMs; wearable movement data is a promising but underexplored avenue.

Method: Uses transformer architectures, patch embeddings, and pretraining on data from 29,307 participants.

Result: PAT achieves state-of-the-art performance in mental health prediction tasks.

Conclusion: PAT is a robust, open-source tool for mental health research.

Abstract: Pretrained foundation models and transformer architectures have driven the
success of large language models (LLMs) and other modern AI breakthroughs.
However, similar advancements in health data modeling remain limited due to the
need for innovative adaptations. Wearable movement data offers a valuable
avenue for exploration, as it's a core feature in nearly all commercial
smartwatches, well established in clinical and mental health research, and the
sequential nature of the data shares similarities to language. We introduce the
Pretrained Actigraphy Transformer (PAT), the first open source foundation model
designed for time-series wearable movement data. Leveraging transformer-based
architectures and novel techniques, such as patch embeddings, and pretraining
on data from 29,307 participants in a national U.S. sample, PAT achieves
state-of-the-art performance in several mental health prediction tasks. PAT is
also lightweight and easily interpretable, making it a robust tool for mental
health research.
  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/

</details>


### [744] [Scaling Laws for Black box Adversarial Attacks](https://arxiv.org/pdf/2411.16782)
*Chuan Liu, Huanran Chen, Yichi Zhang, Yinpeng Dong, Jun Zhu*

Main category: cs.LG

TL;DR: Scaling the number of surrogate models in adversarial attacks improves transferability, achieving high success rates on proprietary models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To explore whether increasing the number of surrogate models enhances adversarial transferability, inspired by scaling laws in large foundation models.

Method: Theoretical analysis and empirical evaluations using diverse adversarial attack methods on standard and defended models, including multimodal large language models.

Result: Clear scaling laws confirm that more surrogate models boost transferability, achieving over 90% success on proprietary models. Visualization also reveals scaling laws in perturbation interpretability and semantics.

Conclusion: Scaling the number of surrogate models significantly improves adversarial transferability, with implications for black-box attack strategies.

Abstract: Adversarial examples usually exhibit good cross-model transferability,
enabling attacks on black-box models with limited information about their
architectures and parameters, which are highly threatening in commercial
black-box scenarios. Model ensembling is an effective strategy to improve the
transferability of adversarial examples by attacking multiple surrogate models.
However, since prior studies usually adopt few models in the ensemble, there
remains an open question of whether scaling the number of models can further
improve black-box attacks. Inspired by the scaling law of large foundation
models, we investigate the scaling laws of black-box adversarial attacks in
this work. Through theoretical analysis and empirical evaluations, we conclude
with clear scaling laws that using more surrogate models enhances adversarial
transferability. Comprehensive experiments verify the claims on standard image
classifiers, diverse defended models and multimodal large language models using
various adversarial attack methods. Specifically, by scaling law, we achieve
90%+ transfer attack success rate on even proprietary models like GPT-4o.
Further visualization indicates that there is also a scaling law on the
interpretability and semantics of adversarial perturbations.

</details>


### [745] [A Library for Learning Neural Operators](https://arxiv.org/pdf/2412.10354)
*Jean Kossaifi, Nikola Kovachki, Zongyi Li, David Pitt, Miguel Liu-Schiaffini, Valentin Duruisseaux, Robert Joseph George, Boris Bonev, Kamyar Azizzadenesheli, Julius Berner, Anima Anandkumar*

Main category: cs.LG

TL;DR: NeuralOperator is a Python library for operator learning, enabling neural networks to map between function spaces with discretization convergence, built on PyTorch for ease of use.


<details>
  <summary>Details</summary>
Motivation: To generalize neural networks for function space mappings and provide a flexible, high-quality tool for operator learning.

Method: Built on PyTorch, NeuralOperator supports training and inference on varied discretizations, ensuring convergence properties.

Result: A tested, open-source library with cutting-edge models, customizability, and a user-friendly interface.

Conclusion: NeuralOperator successfully bridges the gap between neural networks and function space mappings, offering a practical tool for newcomers and experts.

Abstract: We present NeuralOperator, an open-source Python library for operator
learning. Neural operators generalize neural networks to maps between function
spaces instead of finite-dimensional Euclidean spaces. They can be trained and
inferenced on input and output functions given at various discretizations,
satisfying a discretization convergence properties. Built on top of PyTorch,
NeuralOperator provides all the tools for training and deploying neural
operator models, as well as developing new ones, in a high-quality, tested,
open-source package. It combines cutting-edge models and customizability with a
gentle learning curve and simple user interface for newcomers.

</details>


### [746] [Machine learning in wastewater treatment: insights from modelling a pilot denitrification reactor](https://arxiv.org/pdf/2412.14030)
*Eivind Bøhn, Sølve Eidnes, Kjell Rune Jonassen*

Main category: cs.LG

TL;DR: Machine learning is applied to optimize denitrification in wastewater treatment, focusing on foundational requirements like data quality and model properties. Nonlinear models perform best on training data, but linear models generalize better to unseen test data. Temperature variability impacts model robustness, necessitating multi-year data for reliable results.


<details>
  <summary>Details</summary>
Motivation: Wastewater treatment plants are important and data-rich, but their varied designs and conditions complicate automation. This study explores machine learning to optimize denitrification, prioritizing foundational modeling requirements over just predictive accuracy.

Method: Data from a pilot reactor in Norway is used to evaluate machine learning for denitrification. The study identifies critical process parameters, data needs, and model properties, comparing nonlinear and linear models.

Result: Nonlinear models excel on training data, but linear models generalize better to unseen test data. Temperature variability significantly affects model performance, highlighting the need for multi-year data.

Conclusion: Multi-year data is essential for robust models in wastewater treatment, especially in variable climates. The study provides a structured approach to machine learning in this domain and shares data and code publicly.

Abstract: Wastewater treatment plants are increasingly recognized as promising
candidates for machine learning applications, due to their societal importance
and high availability of data. However, their varied designs, operational
conditions, and influent characteristics hinder straightforward automation. In
this study, we use data from a pilot reactor at the Veas treatment facility in
Norway to explore how machine learning can be used to optimize biological
nitrate ($\mathrm{NO_3^-}$) reduction to molecular nitrogen ($\mathrm{N_2}$) in
the biogeochemical process known as \textit{denitrification}. Rather than
focusing solely on predictive accuracy, our approach prioritizes understanding
the foundational requirements for effective data-driven modelling of wastewater
treatment. Specifically, we aim to identify which process parameters are most
critical, the necessary data quantity and quality, how to structure data
effectively, and what properties are required by the models. We find that
nonlinear models perform best on the training and validation data sets,
indicating nonlinear relationships to be learned, but linear models transfer
better to the unseen test data, which comes later in time. The variable
measuring the water temperature has a particularly detrimental effect on the
models, owing to a significant change in distributions between training and
test data. We therefore conclude that multiple years of data is necessary to
learn robust machine learning models. By addressing foundational elements,
particularly in the context of the climatic variability faced by northern
regions, this work lays the groundwork for a more structured and tailored
approach to machine learning for wastewater treatment. We share publicly both
the data and code used to produce the results in the paper.

</details>


### [747] [Rethinking Aleatoric and Epistemic Uncertainty](https://arxiv.org/pdf/2412.20892)
*Freddie Bickford Smith, Jannik Kossen, Eleanor Trollope, Mark van der Wilk, Adam Foster, Tom Rainforth*

Main category: cs.LG

TL;DR: The paper critiques the aleatoric-epistemic uncertainty framework as insufficiently expressive and proposes a decision-theoretic perspective to clarify uncertainty, predictive performance, and data dispersion. It also evaluates information-theoretic quantities, noting their limitations and utility in data acquisition.


<details>
  <summary>Details</summary>
Motivation: Existing discussions of aleatoric and epistemic uncertainty are incoherent and lack expressiveness, prompting the need for a clearer framework.

Method: The authors adopt a decision-theoretic perspective to rigorously relate uncertainty, predictive performance, and statistical dispersion.

Result: The proposed framework clarifies uncertainty discussions, and insights reveal that information-theoretic quantities can be poor estimators but useful for data acquisition.

Conclusion: A decision-theoretic approach improves clarity in uncertainty discussions, and information-theoretic measures, despite limitations, aid in guiding data acquisition.

Abstract: The ideas of aleatoric and epistemic uncertainty are widely used to reason
about the probabilistic predictions of machine-learning models. We identify
incoherence in existing discussions of these ideas and suggest this stems from
the aleatoric-epistemic view being insufficiently expressive to capture all the
distinct quantities that researchers are interested in. To address this we
present a decision-theoretic perspective that relates rigorous notions of
uncertainty, predictive performance and statistical dispersion in data. This
serves to support clearer thinking as the field moves forward. Additionally we
provide insights into popular information-theoretic quantities, showing they
can be poor estimators of what they are often purported to measure, while also
explaining how they can still be useful in guiding data acquisition.

</details>


### [748] [An Investigation into Seasonal Variations in Energy Forecasting for Student Residences](https://arxiv.org/pdf/2501.07423)
*Muhammad Umair Danish, Mathumitha Sureshkumar, Tehara Fonseka, Umeshika Uthayakumar, Vinura Galwaduge*

Main category: cs.LG

TL;DR: Evaluation of machine learning models for energy forecasting in student residences, highlighting seasonal challenges and model adaptability.


<details>
  <summary>Details</summary>
Motivation: Address the unique challenges of seasonal variations and irregular human activities in energy forecasting for student residential settings.

Method: Assessed baseline models (LSTM, GRU) and advanced methods (Autoregressive Feedforward Neural Networks, Transformers, hybrid approaches) for energy forecasting.

Result: No single model consistently outperforms others across all seasons; Hyper Network-based LSTM and MiniAutoEncXGBoost showed strong adaptability to seasonal changes.

Conclusion: Season-specific model selection or tailored designs are crucial for accurate energy forecasting, emphasizing the role of seasonal dynamics.

Abstract: This research provides an in-depth evaluation of various machine learning
models for energy forecasting, focusing on the unique challenges of seasonal
variations in student residential settings. The study assesses the performance
of baseline models, such as LSTM and GRU, alongside state-of-the-art
forecasting methods, including Autoregressive Feedforward Neural Networks,
Transformers, and hybrid approaches. Special attention is given to predicting
energy consumption amidst challenges like seasonal patterns, vacations,
meteorological changes, and irregular human activities that cause sudden
fluctuations in usage. The findings reveal that no single model consistently
outperforms others across all seasons, emphasizing the need for season-specific
model selection or tailored designs. Notably, the proposed Hyper Network based
LSTM and MiniAutoEncXGBoost models exhibit strong adaptability to seasonal
variations, effectively capturing abrupt changes in energy consumption during
summer months. This study advances the energy forecasting field by emphasizing
the critical role of seasonal dynamics and model-specific behavior in achieving
accurate predictions.

</details>


### [749] [Towards Automated Self-Supervised Learning for Truly Unsupervised Graph Anomaly Detection](https://arxiv.org/pdf/2501.14694)
*Zhong Li, Yuhang Wang, Matthijs van Leeuwen*

Main category: cs.LG

TL;DR: The paper highlights issues in SSL-based graph anomaly detection, such as arbitrary hyperparameter choices and label leakage, and proposes an internal evaluation strategy to improve performance.


<details>
  <summary>Details</summary>
Motivation: Current SSL-based graph anomaly detection methods often rely on arbitrary or label-guided hyperparameter choices, leading to subpar performance or label leakage.

Method: The authors propose an internal evaluation strategy for hyperparameter selection in SSL, supported by theoretical analysis and extensive experiments.

Result: Experiments on benchmark datasets show the issues with prior hyperparameter selection methods and validate the effectiveness of the proposed strategy.

Conclusion: The internal evaluation strategy mitigates label leakage and improves performance in SSL-based graph anomaly detection.

Abstract: Self-supervised learning (SSL) is an emerging paradigm that exploits
supervisory signals generated from the data itself, and many recent studies
have leveraged SSL to conduct graph anomaly detection. However, we empirically
found that three important factors can substantially impact detection
performance across datasets: 1) the specific SSL strategy employed; 2) the
tuning of the strategy's hyperparameters; and 3) the allocation of combination
weights when using multiple strategies. Most SSL-based graph anomaly detection
methods circumvent these issues by arbitrarily or selectively (i.e., guided by
label information) choosing SSL strategies, hyperparameter settings, and
combination weights. While an arbitrary choice may lead to subpar performance,
using label information in an unsupervised setting is label information leakage
and leads to severe overestimation of a method's performance. Leakage has been
criticized as "one of the top ten data mining mistakes", yet many recent
studies on SSL-based graph anomaly detection have been using label information
to select hyperparameters. To mitigate this issue, we propose to use an
internal evaluation strategy (with theoretical analysis) to select
hyperparameters in SSL for unsupervised anomaly detection. We perform extensive
experiments using 10 recent SSL-based graph anomaly detection algorithms on
various benchmark datasets, demonstrating both the prior issues with
hyperparameter selection and the effectiveness of our proposed strategy.

</details>


### [750] [RegionGCN: Spatial-Heterogeneity-Aware Graph Convolutional Networks](https://arxiv.org/pdf/2501.17599)
*Hao Guo, Han Wang, Di Zhu, Lun Wu, A. Stewart Fotheringham, Yu Liu*

Main category: cs.LG

TL;DR: The paper introduces RegionGCN, a spatial-heterogeneity-aware graph convolutional network, to address spatial heterogeneity in neural networks by modeling heterogeneity at the regional level, improving prediction accuracy in geospatial tasks like the 2016 US presidential election.


<details>
  <summary>Details</summary>
Motivation: Neural networks often assume spatial stationarity, limiting performance in spatially heterogeneous processes. Current geographically weighted approaches fail for graph neural networks, prompting the need for a better solution.

Method: Proposes RegionGCN, which models spatial heterogeneity at the regional level to reduce over-fitting. Includes a heuristic optimization procedure for adaptive region partitioning during training.

Result: RegionGCN outperforms basic and geographically weighted GCNs in predicting county-level vote shares, demonstrating significant accuracy improvements.

Conclusion: RegionGCN effectively addresses spatial heterogeneity in GeoAI, offering a practical tool for spatial prediction and exploratory analysis of non-linear relationships.

Abstract: Modeling spatial heterogeneity in the data generation process is essential
for understanding and predicting geographical phenomena. Despite their
prevalence in geospatial tasks, neural network models usually assume spatial
stationarity, which could limit their performance in the presence of spatial
process heterogeneity. By allowing model parameters to vary over space, several
approaches have been proposed to incorporate spatial heterogeneity into neural
networks. However, current geographically weighting approaches are ineffective
on graph neural networks, yielding no significant improvement in prediction
accuracy. We assume the crux lies in the over-fitting risk brought by a large
number of local parameters. Accordingly, we propose to model spatial process
heterogeneity at the regional level rather than at the individual level, which
largely reduces the number of spatially varying parameters. We further develop
a heuristic optimization procedure to learn the region partition adaptively in
the process of model training. Our proposed spatial-heterogeneity-aware graph
convolutional network, named RegionGCN, is applied to the spatial prediction of
county-level vote share in the 2016 US presidential election based on
socioeconomic attributes. Results show that RegionGCN achieves significant
improvement over the basic and geographically weighted GCNs. We also offer an
exploratory analysis tool for the spatial variation of non-linear relationships
through ensemble learning of regional partitions from RegionGCN. Our work
contributes to the practice of Geospatial Artificial Intelligence (GeoAI) in
tackling spatial heterogeneity.

</details>


### [751] [Drivetrain simulation using variational autoencoders](https://arxiv.org/pdf/2501.17653)
*Pallavi Sharma, Jorge-Humberto Urrea-Quintero, Bogdan Bogdan, Adrian-Dumitru Ciotec, Laura Vasilie, Henning Wessels, Matteo Skull*

Main category: cs.LG

TL;DR: VAEs predict vehicle jerk signals from torque demand, reducing reliance on real-world data and manual modeling.


<details>
  <summary>Details</summary>
Motivation: Limited real-world drivetrain datasets and the need to reduce costly experiments and manual modeling.

Method: Unconditional and conditional VAEs trained on experimental data from electric SUVs, leveraging latent space for jerk signal synthesis.

Result: VAEs outperform physics-based and hybrid models, generating realistic or tailored jerk signals without detailed system knowledge.

Conclusion: VAEs can enhance drivetrain simulations for data augmentation and scenario exploration, speeding up vehicle development.

Abstract: This work proposes variational autoencoders (VAEs) to predict a vehicle's
jerk signals from torque demand in the context of limited real-world drivetrain
datasets. We implement both unconditional and conditional VAEs, trained on
experimental data from two variants of a fully electric SUV with differing
torque and drivetrain configurations. The VAEs synthesize jerk signals that
capture characteristics from multiple drivetrain scenarios by leveraging the
learned latent space. A performance comparison with baseline physics-based and
hybrid models confirms the effectiveness of the VAEs, without requiring
detailed system parametrization. Unconditional VAEs generate realistic jerk
signals without prior system knowledge, while conditional VAEs enable the
generation of signals tailored to specific torque inputs. This approach reduces
the dependence on costly and time-intensive real-world experiments and
extensive manual modeling. The results support the integration of generative
models such as VAEs into drivetrain simulation pipelines, both for data
augmentation and for efficient exploration of complex operational scenarios,
with the potential to streamline validation and accelerate vehicle development.

</details>


### [752] [Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size](https://arxiv.org/pdf/2501.18164)
*Kanata Oowada, Hideaki Iiduka*

Main category: cs.LG

TL;DR: Using increasing batch sizes in Riemannian stochastic gradient descent (RSGD) improves convergence rates and reduces computational complexity compared to constant batch sizes.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and convergence rate of RSGD by exploring the impact of increasing batch sizes.

Method: Theoretical analysis and numerical experiments (PCA and low-rank matrix completion) to compare RSGD performance with increasing vs. constant batch sizes.

Result: Increasing batch sizes improve convergence from $O(\sqrt{T^{-1}+\text{const.}})$ to $O(T^{-\frac{1}{2}})$ and reduce SFO complexity.

Conclusion: Increasing batch sizes in RSGD combines the benefits of small and large constant batch sizes, offering faster convergence and lower computational costs.

Abstract: We have theoretically analyzed the use of Riemannian stochastic gradient
descent (RSGD) and found that using an increasing batch size leads to faster
RSGD convergence rate than using a constant batch size not only with a constant
learning rate but also with a decaying learning rate, such as cosine annealing
decay and polynomial decay. The convergence rate of RSGD improves from
$O(\sqrt{T^{-1}+\text{const.}})$ with a constant batch size to
$O(T^{-\frac{1}{2}})$ with an increasing batch size, where $T$ denotes the
number of iterations. Using principal component analysis and low-rank matrix
completion tasks, we investigated, both theoretically and numerically, how
increasing batch size affects computational time as measured by stochastic
first-order oracle (SFO) complexity. Increasing batch size reduces the SFO
complexity of RSGD. Furthermore, our numerical results demonstrated that
increasing batch size offers the advantages of both small and large constant
batch sizes.

</details>


### [753] [Time to Rethink AI for Combinatorial Optimization: Classical Algorithms Remain Tough to Match](https://arxiv.org/pdf/2502.03669)
*Yikai Wu, Haoyu Zhao, Sanjeev Arora*

Main category: cs.LG

TL;DR: The paper critiques AI-inspired methods for combinatorial optimization, showing they underperform classical solvers like KaMIS on the Maximum Independent Set problem. It identifies three core issues and calls for better benchmarking, understanding learning limits, and integrating classical heuristics.


<details>
  <summary>Details</summary>
Motivation: To highlight the underperformance of AI-inspired methods in combinatorial optimization compared to classical solvers and identify systemic issues in their development and evaluation.

Method: Empirical benchmarks comparing AI-inspired GPU-based methods with classical CPU-based solvers on the Maximum Independent Set problem, supplemented by a novel serialization analysis.

Result: AI-inspired methods consistently underperform classical solvers like KaMIS, often failing to surpass simple greedy heuristics. Serialization reveals their reasoning resembles basic heuristics.

Conclusion: Future research must improve benchmarking, understand learning limitations, and integrate classical heuristics to advance AI-inspired methods in combinatorial optimization.

Abstract: This position paper argues that the machine learning community should
fundamentally rethink how AI-inspired methods are developed and evaluated for
combinatorial optimization (CO). We present comprehensive empirical benchmarks
comparing various recent AI-inspired GPU-based methods with several classical
CPU-based solvers on the Maximum Independent Set (MIS) problem. Strikingly,
even on in-distribution random graphs, leading AI-inspired methods are
consistently outperformed by the state-of-the-art classical solver KaMIS, and
some AI-inspired methods frequently fail to surpass even the simplest
degree-based greedy heuristic. To better understand the source of these
failures, we introduce a novel analysis, serialization, which reveals that
non-backtracking AI methods, such as LTFT (based on GFlowNets), end up
reasoning similarly to the simplest degree-based greedy heuristic, and thus
worse than KaMIS.
  Our findings reveal three core issues: (1) Limited benchmarks and evaluation
- AI-inspired methods are often tested only on small instances with very
limited inference time, which covers up issues with scalability and resource
usage; (2) Intrinsic hardness and learning limits - even under ideal,
in-distribution conditions, learning-based approaches lag behind classical
heuristics, highlighting inherent barriers that receive little attention; and
(3) Insufficient use and understanding of classical heuristics - current
learning frameworks often neglect to incorporate effective classical
techniques.
  Although we use MIS as a testbed, similar gaps and challenges have been
reported in other combinatorial optimization problems, suggesting broader
relevance for our recommendations. We propose that future research must address
these issues by rigorous benchmarking, deepening understanding of learning
limitations, and integrating classical heuristics into AI-inspired methods.

</details>


### [754] [Efficient Online Reinforcement Learning for Diffusion Policy](https://arxiv.org/pdf/2502.00361)
*Haitong Ma, Tianyi Chen, Kai Wang, Na Li, Bo Dai*

Main category: cs.LG

TL;DR: The paper introduces Reweighted Score Matching (RSM) to efficiently train diffusion policies in online RL, eliminating the need for target distribution samples. Two algorithms, DPMD and SDAC, outperform existing methods on MuJoCo benchmarks.


<details>
  <summary>Details</summary>
Motivation: Conventional diffusion training in online RL is impractical due to the inability to sample from the optimal policy and high computational costs. RSM addresses these issues.

Method: Generalizes denoising score matching by reweighting the loss function, leading to RSM. Introduces DPMD and SDAC for policy optimization.

Result: DPMD and SDAC outperform recent diffusion-policy online RL methods, with DPMD showing a 120% improvement over soft actor-critic on Humanoid and Ant tasks.

Conclusion: RSM enables efficient diffusion policy training in online RL, with DPMD and SDAC demonstrating superior performance on benchmarks.

Abstract: Diffusion policies have achieved superior performance in imitation learning
and offline reinforcement learning (RL) due to their rich expressiveness.
However, the conventional diffusion training procedure requires samples from
target distribution, which is impossible in online RL since we cannot sample
from the optimal policy. Backpropagating policy gradient through the diffusion
process incurs huge computational costs and instability, thus being expensive
and not scalable. To enable efficient training of diffusion policies in online
RL, we generalize the conventional denoising score matching by reweighting the
loss function. The resulting Reweighted Score Matching (RSM) preserves the
optimal solution and low computational cost of denoising score matching, while
eliminating the need to sample from the target distribution and allowing
learning to optimize value functions. We introduce two tractable reweighted
loss functions to solve two commonly used policy optimization problems, policy
mirror descent and max-entropy policy, resulting in two practical algorithms
named Diffusion Policy Mirror Descent (DPMD) and Soft Diffusion Actor-Critic
(SDAC). We conducted comprehensive comparisons on MuJoCo benchmarks. The
empirical results show that the proposed algorithms outperform recent
diffusion-policy online RLs on most tasks, and the DPMD improves more than 120%
over soft actor-critic on Humanoid and Ant.

</details>


### [755] [Progressive Binarization with Semi-Structured Pruning for LLMs](https://arxiv.org/pdf/2502.01705)
*Xianglong Yan, Tianao Zhang, Zhiteng Li, Yulun Zhang*

Main category: cs.LG

TL;DR: PBS$^2$P is a post-training compression framework combining progressive binarization and semi-structured pruning to optimize LLMs for efficiency without severe performance loss.


<details>
  <summary>Details</summary>
Motivation: High computational and memory costs of LLMs hinder deployment on resource-constrained devices, and existing binarization methods still leave redundancy.

Method: Proposes PBS$^2$P with SPBO for joint pruning and binarization error reduction, and CFS for effective pruning element selection.

Result: Outperforms state-of-the-art binary post-training quantization methods in perplexity and downstream accuracy.

Conclusion: PBS$^2$P effectively balances efficiency and performance for LLM deployment.

Abstract: Large language models (LLMs) have achieved remarkable progress in natural
language processing, but their high computational and memory costs hinder
deployment on resource-constrained devices. Binarization, which reduces model
weights to 1 bit, is a promising solution for efficient inference. However,
binarized LLMs still exhibit redundancy that can be further compressed.
Semi-structured pruning offers a favorable trade-off between model performance
and hardware efficiency, but naively combining it with binarization often leads
to severe performance degradation. To address this, we propose Progressive
Binarization with Semi-Structured Pruning (PBS$^2$P), a novel post-training
compression framework. We propose Stepwise semi-structured Pruning with
Binarization Optimization (SPBO) to jointly reduce pruning and binarization
error. Additionally, we develop a Coarse-to-Fine Search (CFS) strategy to more
effectively select pruning elements. Extensive experiments across multiple LLM
families show that PBS$^2$P consistently outperforms state-of-the-art binary
post-training quantization methods in both perplexity and downstream accuracy.
The code and models will be available at:
https://github.com/XIANGLONGYAN/PBS2P.

</details>


### [756] [A general language model for peptide identification](https://arxiv.org/pdf/2502.15610)
*Jixiu Zhai, Tianchi Lu, Haitian Zhong, Ziyang Xu, Yuhuan Liu, Shengrui Xu, Jingwan Wang, Dan Huang*

Main category: cs.LG

TL;DR: PDeepPP is a deep learning framework for identifying bioactive peptides and PTMs, achieving state-of-the-art performance in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of bioactive peptides and PTMs is crucial for understanding protein function and therapeutic discovery, but existing methods lack generalizability.

Method: PDeepPP integrates pretrained protein language models with a hybrid transformer-convolutional architecture, addressing data imbalance and extracting global/local sequence features.

Result: PDeepPP excels in 25/33 tasks, with high accuracy in antimicrobial (0.9726) and phosphorylation site (0.9984) identification, and 99.5% specificity in glycosylation prediction.

Conclusion: PDeepPP enables large-scale, accurate peptide analysis, supporting biomedical research and therapeutic target discovery. Code and models are publicly available.

Abstract: Accurate identification of bioactive peptides (BPs) and protein
post-translational modifications (PTMs) is essential for understanding protein
function and advancing therapeutic discovery. However, most computational
methods remain limited in their generalizability across diverse peptide
functions. Here, we present PDeepPP, a unified deep learning framework that
integrates pretrained protein language models with a hybrid
transformer-convolutional architecture, enabling robust identification across
diverse peptide classes and PTM sites. We curated comprehensive benchmark
datasets and implemented strategies to address data imbalance, allowing PDeepPP
to systematically extract both global and local sequence features. Through
extensive analyses-including dimensionality reduction and comparison
studies-PDeepPP demonstrates strong, interpretable peptide representations and
achieves state-of-the-art performance in 25 of the 33 biological identification
tasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and
phosphorylation site (0.9984) identification, with 99.5% specificity in
glycosylation site prediction and substantial reduction in false negatives in
antimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP
supports biomedical research and the discovery of novel therapeutic targets for
disease treatment. All code, datasets, and pretrained models are publicly
available via GitHub:https://github.com/fondress/PDeepPP and Hugging
Face:https://huggingface.co/fondress/PDeppPP.

</details>


### [757] [Recovering Imbalanced Clusters via Gradient-Based Projection Pursuit](https://arxiv.org/pdf/2502.02668)
*Martin Eppert, Satyaki Mukherjee, Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: A gradient-based method for recovering projections with Imbalanced Clusters or Bernoulli-Rademacher distributions, showing better recovery for imbalanced clusters and outperforming others on real-world datasets with limited samples.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sample complexity in Projection Pursuit and explore the recovery of specific data distributions (Imbalanced Clusters and Bernoulli-Rademacher) using gradient-based optimization.

Method: A gradient-based technique to optimize the projection index, analyzed within a Planted Vector setting, with comparisons to computational lower bounds in the Low-Degree-Polynomial Framework.

Result: Imbalanced Clusters are easier to recover than balanced ones. The method outperforms others on real-world datasets (FashionMNIST, Human Activity Recognition) with few samples.

Conclusion: The proposed method effectively recovers specific projections and performs well under limited sample conditions, demonstrating practical applicability.

Abstract: Projection Pursuit is a classic exploratory technique for finding interesting
projections of a dataset. We propose a method for recovering projections
containing either Imbalanced Clusters or a Bernoulli-Rademacher distribution
using a gradient-based technique to optimize the projection index. As sample
complexity is a major limiting factor in Projection Pursuit, we analyze our
algorithm's sample complexity within a Planted Vector setting where we can
observe that Imbalanced Clusters can be recovered more easily than balanced
ones. Additionally, we give a generalized result that works for a variety of
data distributions and projection indices. We compare these results to
computational lower bounds in the Low-Degree-Polynomial Framework. Finally, we
experimentally evaluate our method's applicability to real-world data using
FashionMNIST and the Human Activity Recognition Dataset, where our algorithm
outperforms others when only a few samples are available.

</details>


### [758] [Benefits of Early Stopping in Gradient Descent for Overparameterized Logistic Regression](https://arxiv.org/pdf/2502.13283)
*Jingfeng Wu, Peter Bartlett, Matus Telgarsky, Bin Yu*

Main category: cs.LG

TL;DR: Early-stopped GD in overparameterized logistic regression shows better statistical performance than asymptotic GD, with vanishing excess risk and polynomial sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand the regularization effects of early stopping in high-dimensional logistic regression and compare its statistical benefits with asymptotic GD.

Method: Analyze early-stopped GD in well-specified high-dimensional logistic regression, comparing its excess logistic risk, zero-one risk, and sample efficiency with asymptotic GD.

Result: Early-stopped GD achieves vanishing excess risk and requires polynomially many samples for small zero-one risk, while asymptotic GD is inconsistent and needs exponentially many samples.

Conclusion: Early stopping in GD provides statistical advantages over asymptotic GD, connecting implicit regularization with explicit ℓ2-regularization.

Abstract: In overparameterized logistic regression, gradient descent (GD) iterates
diverge in norm while converging in direction to the maximum $\ell_2$-margin
solution -- a phenomenon known as the implicit bias of GD. This work
investigates additional regularization effects induced by early stopping in
well-specified high-dimensional logistic regression. We first demonstrate that
the excess logistic risk vanishes for early-stopped GD but diverges to infinity
for GD iterates at convergence. This suggests that early-stopped GD is
well-calibrated, whereas asymptotic GD is statistically inconsistent. Second,
we show that to attain a small excess zero-one risk, polynomially many samples
are sufficient for early-stopped GD, while exponentially many samples are
necessary for any interpolating estimator, including asymptotic GD. This
separation underscores the statistical benefits of early stopping in the
overparameterized regime. Finally, we establish nonasymptotic bounds on the
norm and angular differences between early-stopped GD and $\ell_2$-regularized
empirical risk minimizer, thereby connecting the implicit regularization of GD
with explicit $\ell_2$-regularization.

</details>


### [759] [PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization](https://arxiv.org/pdf/2503.01328)
*Xinyi Wan, Penghui Qi, Guangxing Huang, Min Lin, Jialin Li*

Main category: cs.LG

TL;DR: The paper addresses high activation memory consumption in pipeline parallelism (PP) for large language models (LLMs) by leveraging memory offload strategies, achieving significant memory reduction and performance improvements.


<details>
  <summary>Details</summary>
Motivation: Pipeline parallelism (PP) is constrained by high activation memory consumption as the number of in-flight microbatches increases, limiting scalability for LLM training.

Method: The paper proposes a memory offload strategy, including selective offloading, and integrates it with other techniques to balance throughput and memory limits.

Result: Experiments show reduced per-device activation memory, making PP more scalable than tensor parallelism (TP), with up to 19% speedup and lower memory usage.

Conclusion: Memory offload in PP effectively addresses scalability issues, offering better performance and efficiency for LLM training.

Abstract: Pipeline parallelism (PP) is widely used for training large language models
(LLMs), yet its scalability is often constrained by high activation memory
consumption as the number of in-flight microbatches grows with the degree of
PP. In this paper, we focus on addressing this challenge by leveraging the
under-explored memory offload strategy in PP. With empirical study, we discover
that in the majority of standard configurations, at least half, and potentially
all, of the activations can be offloaded with negligible overhead. In the cases
where full overload is not possible, we introduce a novel selective offload
strategy that decreases peak activation memory in a better-than-linear manner.
Furthermore, we integrate memory offload with other techniques to jointly
consider overall throughput and memory limitation. Our experiments proves that
the per-device activation memory effectively reduces with the total number of
stages, making PP a stronger alternative than TP, offering up to a 19\%
acceleration with even lower memory consumption. The implementation is
open-sourced at
\href{https://github.com/sail-sg/zero-bubble-pipeline-parallelism}{this url}.

</details>


### [760] [Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy](https://arxiv.org/pdf/2503.07661)
*Wei Junhao, Yu Zhe, Sakuma Jun*

Main category: cs.LG

TL;DR: Proposes a proactive defense against model merging by disrupting merged models while preserving standalone functionality.


<details>
  <summary>Details</summary>
Motivation: To prevent free-riders from cheaply inheriting specialized capabilities via model merging, as existing methods only detect merging after the fact.

Method: Modifies model parameters (rearranging MLP parameters, scaling attention heads) to disrupt merging while keeping standalone functionality intact.

Result: Defense significantly degrades merging performance across image classification, generation, and text classification tasks.

Conclusion: The proactive defense is effective, and robustness is further improved with dropout-based pruning against adaptive attacks.

Abstract: Model merging is a technique that combines multiple finetuned models into a
single model without additional training, allowing a free-rider to cheaply
inherit specialized capabilities. This study investigates methodologies to
suppress unwanted model merging by free-riders. Existing methods such as model
watermarking or fingerprinting can only detect merging in hindsight. In
contrast, we propose a first proactive defense against model merging.
Specifically, our defense method modifies the model parameters so that the
model is disrupted if the model is merged with any other model, while its
functionality is kept unchanged if not merged with others. Our approach
consists of two modules, rearranging MLP parameters and scaling attention
heads, which push the model out of the shared basin in parameter space, causing
the merging performance with other models to degrade significantly. We conduct
extensive experiments on image classification, image generation, and text
classification to demonstrate that our defense severely disrupts merging while
retaining the functionality of the post-protect model. Moreover, we analyze
potential adaptive attacks and further propose a dropout-based pruning to
improve our proposal's robustness.

</details>


### [761] [ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism](https://arxiv.org/pdf/2503.15758)
*Venmugil Elango*

Main category: cs.LG

TL;DR: ATTENTION2D improves transformer efficiency by parallelizing self-attention along query and key/value dimensions, achieving faster training and inference without extra costs.


<details>
  <summary>Details</summary>
Motivation: Address the high computational and memory costs of self-attention in transformers, especially for long sequences.

Method: Introduces ATTENTION2D, a parallelization technique for self-attention along query and key/value dimensions.

Result: Achieves up to 5x and 9.4x performance boosts on GPT-3-like models with 64 GPUs.

Conclusion: ATTENTION2D offers scalable, efficient transformer training and inference without approximations or overheads.

Abstract: Transformer-based models have emerged as a leading architecture for natural
language processing, natural language generation, and image generation tasks. A
fundamental element of the transformer architecture is self-attention, which
allows the model to capture intricate dependencies within the data. However,
the self-attention mechanism also incurs significant computational and memory
costs, particularly for long sequences.
  In this paper, we introduce ATTENTION2D, a novel approach that exploits
parallelism along two dimensions - query and key/value - of the self-attention
operation. This method enables efficient distribution and parallelization of
computations across multiple devices. Our approach facilitates asymptotically
faster training and inference phases compared to previous methods, without
relying on approximations or incurring additional computational or memory
overheads. Furthermore, unlike existing techniques that struggle to scale with
an increasing number of processing units, our approach effectively scales with
additional processing units.
  Our experimental results confirm the effectiveness of our method in improving
communication efficiency and scalability. Compared to Ring Attention, our
approach demonstrated up to a 5x performance boost on a GPT-3-like model using
64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64
NVIDIA H100 GPUs across 64 nodes.

</details>


### [762] [FedMM-X: A Trustworthy and Interpretable Framework for Federated Multi-Modal Learning in Dynamic Environments](https://arxiv.org/pdf/2503.19564)
*Sree Bhargavi Balija*

Main category: cs.LG

TL;DR: The paper introduces FedMM-X, a framework combining federated learning with explainable multi-modal reasoning to enhance trustworthiness in AI systems operating in real-world environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges like data heterogeneity, modality imbalance, and out-of-distribution generalization in multi-modal AI systems while ensuring trustworthiness.

Method: The proposed FedMM-X framework uses cross-modal consistency checks, client-level interpretability mechanisms, and dynamic trust calibration.

Result: The framework improves accuracy and interpretability, reduces vulnerabilities to adversarial attacks, and introduces a trust score aggregation method for global model reliability.

Conclusion: FedMM-X advances robust, interpretable, and socially responsible AI systems for real-world applications.

Abstract: As artificial intelligence systems increasingly operate in Real-world
environments, the integration of multi-modal data sources such as vision,
language, and audio presents both unprecedented opportunities and critical
challenges for achieving trustworthy intelligence. In this paper, we propose a
novel framework that unifies federated learning with explainable multi-modal
reasoning to ensure trustworthiness in decentralized, dynamic settings. Our
approach, called FedMM-X (Federated Multi-Modal Explainable Intelligence),
leverages cross-modal consistency checks, client-level interpretability
mechanisms, and dynamic trust calibration to address challenges posed by data
heterogeneity, modality imbalance, and out-of-distribution generalization.
Through rigorous evaluation across federated multi-modal benchmarks involving
vision-language tasks, we demonstrate improved performance in both accuracy and
interpretability while reducing vulnerabilities to adversarial and spurious
correlations. Further, we introduce a novel trust score aggregation method to
quantify global model reliability under dynamic client participation. Our
findings pave the way toward developing robust, interpretable, and socially
responsible AI systems in Real-world environments.

</details>


### [763] [A Consequentialist Critique of Binary Classification Evaluation Practices](https://arxiv.org/pdf/2504.04528)
*Gerardo Flores, Abigail Schiff, Alyssa H. Smith, Julia A Fukuyama, Ashia C. Wilson*

Main category: cs.LG

TL;DR: The paper advocates for using Brier scores and Log loss for evaluating ML-supported binary decisions, highlighting a gap between theoretical preference and empirical practice in major conferences.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between decision-theoretic recommendations (favoring Brier scores and Log loss) and the prevalent use of top-K metrics or fixed thresholds in ML evaluations.

Method: Empirical analysis of evaluation metrics in major conferences (ICML, FAccT, CHIL), theoretical mapping of metrics to use cases, and development of a Python package (briertools) to promote Brier scores.

Result: Reveals a preference for top-K metrics or fixed thresholds in practice, despite theoretical advantages of Brier scores. Also uncovers new connections between Brier Score and Decision Curve Analysis.

Conclusion: Advocates for adopting Brier scores and Log loss in evaluations, supported by theoretical and empirical insights, and introduces tools to facilitate this shift.

Abstract: ML-supported decisions, such as ordering tests or determining preventive
custody, often involve binary classification based on probabilistic forecasts.
Evaluation frameworks for such forecasts typically consider whether to
prioritize independent-decision metrics (e.g., Accuracy) or top-K metrics
(e.g., Precision@K), and whether to focus on fixed thresholds or
threshold-agnostic measures like AUC-ROC. We highlight that a consequentialist
perspective, long advocated by decision theorists, should naturally favor
evaluations that support independent decisions using a mixture of thresholds
given their prevalence, such as Brier scores and Log loss. However, our
empirical analysis reveals a strong preference for top-K metrics or fixed
thresholds in evaluations at major conferences like ICML, FAccT, and CHIL. To
address this gap, we use this decision-theoretic framework to map evaluation
metrics to their optimal use cases, along with a Python package, briertools, to
promote the broader adoption of Brier scores. In doing so, we also uncover new
theoretical connections, including a reconciliation between the Brier Score and
Decision Curve Analysis, which clarifies and responds to a longstanding
critique by (Assel, et al. 2017) regarding the clinical utility of proper
scoring rules.

</details>


### [764] [Achieving binary weight and activation for LLMs using Post-Training Quantization](https://arxiv.org/pdf/2504.05352)
*Siqing Song, Chuang Wang, Ruiqi Wang, Yi Yang, Xu-Yao Zhang*

Main category: cs.LG

TL;DR: A post-training quantization framework for LLMs using W(1+1)A(1*4) configuration, achieving better performance than SOTA methods with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing quantization techniques degrade performance below 4-bit precision, limiting efficiency gains.

Method: Uses Hessian-aware fine-grained grouping for weights and decomposes INT4 activations into 4*INT1 with smoothed scaling factors.

Result: Outperforms SOTA baselines on W2A4 tasks, advancing toward fully binarized models.

Conclusion: The proposed method pushes LLM quantization boundaries, enabling efficient 1-bit models with minimal performance loss.

Abstract: Quantizing large language models (LLMs) to 1-bit precision significantly
reduces computational costs, but existing quantization techniques suffer from
noticeable performance degradation when using weight and activation precisions
below 4 bits (W4A4). In this paper, we propose a post-training quantization
framework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit
with an additional 1 bit for fine-grain grouping and activations are quantized
to 1 bit with a 4-fold increase in the number of channels. For weight
quantization, we propose utilizing Hessian-aware fine-grained grouping along
with an EM-based quantization scheme. For activation quantization, we decompose
INT4-quantized activations into a 4 * INT1 format equivalently and
simultaneously smooth the scaling factors based on quantization errors, which
further reduces the quantization errors in activations. Our method surpasses
state-of-the-art (SOTA) LLM quantization baselines on W2A4 across multiple
tasks, pushing the boundaries of existing LLM quantization methods toward fully
binarized models. Code is available at
https://github.com/JimmyCrave/LLM-PTQ-binarization.

</details>


### [765] [A Framework of Decision-Relevant Observability: Reinforcement Learning Converges Under Relative Ignorability](https://arxiv.org/pdf/2504.07722)
*MaryLena Bleile*

Main category: cs.LG

TL;DR: The paper unifies causal inference and reinforcement learning to address missing data in sequential decision-making, introducing 'relative ignorability' to ensure Q-learning convergence without strict Markovian observability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of missing or incomplete data in sequential decision-making systems, where classical reinforcement learning assumes Markovian observability, which may not hold.

Method: Introduces 'relative ignorability,' a graphical-causal criterion, to generalize and unify causal inference and reinforcement learning, enabling Q-learning convergence under partial observability.

Result: Theoretical and simulation results show that standard reinforcement learning algorithms can optimize non-Markovian processes if missingness is relatively ignorable.

Conclusion: The work expands the theoretical foundations of AI for real-world environments with incomplete information, ensuring safe and data-efficient decision-making.

Abstract: From clinical dosing algorithms to autonomous robots, sequential
decision-making systems routinely operate with missing or incomplete data.
Classical reinforcement learning theory, which is commonly used to solve
sequential decision problems, assumes Markovian observability, which may not
hold under partial observability. Causal inference paradigms formalise
ignorability of missingness. We show these views can be unified and generalized
in order to guarantee Q-learning convergence even when the Markov property
fails. To do so, we introduce the concept of \emph{relative ignorability}.
Relative ignorability is a graphical-causal criterion which refines the
requirements for accurate decision-making based on incomplete data. Theoretical
results and simulations both reveal that non-markovian stochastic processes
whose missingness is relatively ignorable with respect to causal estimands can
still be optimized using standard Reinforcement Learning algorithms. These
results expand the theoretical foundations of safe, data-efficient AI to
real-world environments where complete information is unattainable.

</details>


### [766] [Transformer Encoder and Multi-features Time2Vec for Financial Prediction](https://arxiv.org/pdf/2504.13801)
*Nguyen Kim Hai Bui, Nguyen Duy Chien, Péter Kovács, Gergő Bognár*

Main category: cs.LG

TL;DR: The paper introduces a neural network combining Time2Vec with Transformer's Encoder for financial prediction, outperforming benchmarks by leveraging correlation features.


<details>
  <summary>Details</summary>
Motivation: Existing models focus on individual stock predictions, missing broader market trends. The paper addresses this by incorporating correlated stock movements within industries.

Method: Integrates Time2Vec with Transformer's Encoder, introduces correlation feature selection, and fine-tunes hyperparameters.

Result: Outperforms state-of-the-art encoding methods like positional encoding and improves multi-stock prediction accuracy.

Conclusion: The proposed method enhances financial prediction by capturing broader market trends through correlation features and advanced encoding.

Abstract: Financial prediction is a complex and challenging task of time series
analysis and signal processing, expected to model both short-term fluctuations
and long-term temporal dependencies. Transformers have remarkable success
mostly in natural language processing using attention mechanism, which also
influenced the time series community. The ability to capture both short and
long-range dependencies helps to understand the financial market and to
recognize price patterns, leading to successful applications of Transformers in
stock prediction. Although, the previous research predominantly focuses on
individual features and singular predictions, that limits the model's ability
to understand broader market trends. In reality, within sectors such as finance
and technology, companies belonging to the same industry often exhibit
correlated stock price movements.
  In this paper, we develop a novel neural network architecture by integrating
Time2Vec with the Encoder of the Transformer model. Based on the study of
different markets, we propose a novel correlation feature selection method.
Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a
comparative analysis of our results against benchmark models. We conclude that
our method outperforms other state-of-the-art encoding methods such as
positional encoding, and we also conclude that selecting correlation features
enhance the accuracy of predicting multiple stock prices.

</details>


### [767] [Online model learning with data-assimilated reservoir computers](https://arxiv.org/pdf/2504.16767)
*Andrea Nóvoa, Luca Magri*

Main category: cs.LG

TL;DR: An online learning framework for forecasting nonlinear spatio-temporal signals using dimensionality reduction, autoregressive modeling, and online adaptation, demonstrated on a Navier-Stokes wake flow.


<details>
  <summary>Details</summary>
Motivation: To improve forecasting of nonlinear spatio-temporal signals by integrating data-driven reduced order modeling with Bayesian data assimilation for scalable online learning.

Method: Combines proper orthogonal decomposition (POD), reservoir computing for reduced dynamics, and ensemble sequential data assimilation for online adaptation. Tested on a cylinder wake flow with full and sparse sensor data.

Result: The two-fold estimation strategy outperforms the naive approach in convergence and error reduction. The three-fold strategy enables robust online training of partially-trained models.

Conclusion: The framework unifies reduced order modeling and data assimilation, offering scalable online learning for nonlinear time series forecasting.

Abstract: We propose an online learning framework for forecasting nonlinear
spatio-temporal signals (fields). The method integrates (i) dimensionality
reduction, here, a simple proper orthogonal decomposition (POD) projection;
(ii) a generalized autoregressive model to forecast reduced dynamics, here, a
reservoir computer; (iii) online adaptation to update the reservoir computer
(the model), here, ensemble sequential data assimilation. We demonstrate the
framework on a wake past a cylinder governed by the Navier-Stokes equations,
exploring the assimilation of full flow fields (projected onto POD modes) and
sparse sensors. Three scenarios are examined: a na\"ive physical state
estimation; a two-fold estimation of physical and reservoir states; and a
three-fold estimation that also adjusts the model parameters. The two-fold
strategy significantly improves ensemble convergence and reduces reconstruction
error compared to the na\"ive approach. The three-fold approach enables robust
online training of partially-trained reservoir computers, overcoming
limitations of a priori training. By unifying data-driven reduced order
modelling with Bayesian data assimilation, this work opens new opportunities
for scalable online model learning for nonlinear time series forecasting.

</details>


### [768] [Perturbation Analysis of Singular Values in Concatenated Matrices](https://arxiv.org/pdf/2505.01427)
*Maksym Shamrai*

Main category: cs.LG

TL;DR: The paper explores how the singular value spectrum of a concatenated matrix relates to its submatrices, using perturbation techniques to derive stability bounds for singular values under small perturbations.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between the singular value spectra of concatenated matrices and their individual components, and to provide theoretical insights for matrix clustering and compression.

Method: Develops a perturbation technique extending classical results like Weyl's inequality to concatenated matrices, setting analytical bounds for singular value stability.

Result: Shows that dominant singular values of the concatenated matrix remain stable if submatrices are close in norm, enabling accuracy-compression trade-offs.

Conclusion: Provides a theoretical foundation for improved matrix clustering and compression, with applications in numerical linear algebra, signal processing, and data-driven modeling.

Abstract: Concatenating matrices is a common technique for uncovering shared structures
in data through singular value decomposition (SVD) and low-rank approximations.
The fundamental question arises: How does the singular value spectrum of the
concatenated matrix relate to the spectra of its individual components? In the
present work, we develop a perturbation technique that extends classical
results such as Weyl's inequality to concatenated matrices. We setup analytical
bounds that quantify stability of singular values under small perturbations in
submatrices. The results demonstrate that if submatrices are close in a norm,
dominant singular values of the concatenated matrix remain stable enabling
controlled trade-offs between accuracy and compression. These provide a
theoretical basis for improved matrix clustering and compression strategies
with applications in the numerical linear algebra, signal processing, and
data-driven modeling.

</details>


### [769] [RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference](https://arxiv.org/pdf/2505.02922)
*Yaoqi Chen, Jinkai Zhang, Baotong Lu, Qianxi Zhang, Chengruidong Zhang, Jingjia Luo, Di Liu, Huiqiang Jiang, Qi Chen, Jing Liu, Bailu Ding, Xiao Yan, Jiawei Jiang, Chen Chen, Mingxing Zhang, Yuqing Yang, Fan Yang, Mao Yang*

Main category: cs.LG

TL;DR: RetroInfer accelerates long-context LLM inference by reimagining the KV cache as a vector storage system, leveraging attention sparsity for efficiency without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Addressing GPU memory and bandwidth constraints in LLMs with growing context lengths.

Method: Uses a wave index for efficient token retrieval and a wave buffer for KV cache coordination, combining tripartite attention approximation and segmented clustering.

Result: Achieves up to 4.5X speedup over full attention and 10.5X over sparse baselines, maintaining full-attention accuracy.

Conclusion: RetroInfer offers a robust solution for efficient long-context LLM inference, balancing performance and accuracy.

Abstract: The growing context lengths of large language models (LLMs) pose significant
challenges for efficient inference, primarily due to GPU memory and bandwidth
constraints. We present RetroInfer, a novel system that reconceptualizes the
key-value (KV) cache as a vector storage system which exploits the inherent
attention sparsity to accelerate long-context LLM inference. At its core is the
wave index, an Attention-aWare VEctor index that enables efficient and accurate
retrieval of critical tokens through techniques such as tripartite attention
approximation, accuracy-bounded attention estimation, and segmented clustering.
Complementing this is the wave buffer, which coordinates KV cache placement and
overlaps computation and data transfer across GPU and CPU to sustain high
throughput. Unlike prior sparsity-based methods that struggle with token
selection and hardware coordination, RetroInfer delivers robust performance
without compromising model accuracy. Experiments on long-context benchmarks
show up to 4.5X speedup over full attention within GPU memory limits and up to
10.5X over sparse attention baselines when KV cache is extended to CPU memory,
all while preserving full-attention-level accuracy.

</details>


### [770] [Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast](https://arxiv.org/pdf/2505.04396)
*Jingnan Wang, Jie Chao, Shangshang Yang, Kaijun Ren, Kefeng Deng, Xi Chen, Yaxin Liu, Hanqiuzi Wen, Ziniu Xiao, Lifeng Zhang, Xiaodong Wang, Jiping Guan, Baoxiang Pan*

Main category: cs.LG

TL;DR: A method combining learned high-resolution climatological data with coarse-grid forecasts improves wind power weather predictions efficiently.


<details>
  <summary>Details</summary>
Motivation: Accurate, high-resolution weather forecasts are critical for renewable energy planning, but current methods face scale, cost, and uncertainty challenges.

Method: Learns climatological distribution from high-resolution simulations and combines it with coarse-grid forecasts for fine-grained, ensemble predictions.

Result: Outperforms existing methods in accuracy and cost, with forecasts generated in <1 hour on a GPU versus thousands of CPU hours traditionally.

Conclusion: The method enables efficient, reliable renewable energy planning by reducing computational costs while maintaining accuracy.

Abstract: The planning and operation of renewable energy, especially wind power, depend
crucially on accurate, timely, and high-resolution weather information.
Coarse-grid global numerical weather forecasts are typically downscaled to meet
these requirements, introducing challenges of scale inconsistency, process
representation error, computation cost, and entanglement of distinct
uncertainty sources from chaoticity, model bias, and large-scale forcing. We
address these challenges by learning the climatological distribution of a
target wind farm using its high-resolution numerical weather simulations. An
optimal combination of this learned high-resolution climatological prior with
coarse-grid large scale forecasts yields highly accurate, fine-grained,
full-variable, large ensemble of weather pattern forecasts. Using observed
meteorological records and wind turbine power outputs as references, the
proposed methodology verifies advantageously compared to existing
numerical/statistical forecasting-downscaling pipelines, regarding either
deterministic/probabilistic skills or economic gains. Moreover, a 100-member,
10-day forecast with spatial resolution of 1 km and output frequency of 15 min
takes < 1 hour on a moderate-end GPU, as contrast to $\mathcal{O}(10^3)$ CPU
hours for conventional numerical simulation. By drastically reducing
computational costs while maintaining accuracy, our method paves the way for
more efficient and reliable renewable energy planning and operation.

</details>


### [771] [TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks](https://arxiv.org/pdf/2505.12884)
*Yuanze Hu, Zhaoxin Fan, Xinyu Wang, Gen Li, Ye Qiu, Zhichao Yang, Wenjun Wu, Kejian Wu, Yifan Sun, Xiaotie Deng, Jin Dong*

Main category: cs.LG

TL;DR: TinyAlign improves alignment in lightweight VLMs by retrieving context to enhance multimodal inputs, achieving better performance with less data.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods for lightweight VLMs rely on frozen models, limiting performance due to constrained language model capacity.

Method: Proposes TinyAlign, a framework using retrieval-augmented generation to enrich inputs and improve alignment via a memory bank.

Result: TinyAlign reduces training loss, speeds convergence, and achieves baseline performance with 40% of fine-tuning data.

Conclusion: TinyAlign offers a practical solution for lightweight VLMs and provides new insights into alignment bottlenecks.

Abstract: Lightweight Vision-Language Models (VLMs) are indispensable for
resource-constrained applications. The prevailing approach to aligning vision
and language models involves freezing both the vision encoder and the language
model while training small connector modules. However, this strategy heavily
depends on the intrinsic capabilities of the language model, which can be
suboptimal for lightweight models with limited representational capacity. In
this work, we investigate this alignment bottleneck through the lens of mutual
information, demonstrating that the constrained capacity of the language model
inherently limits the Effective Mutual Information (EMI) between multimodal
inputs and outputs, thereby compromising alignment quality. To address this
challenge, we propose TinyAlign, a novel framework inspired by
Retrieval-Augmented Generation, which strategically retrieves relevant context
from a memory bank to enrich multimodal inputs and enhance their alignment.
Extensive empirical evaluations reveal that TinyAlign significantly reduces
training loss, accelerates convergence, and enhances task performance.
Remarkably, it allows models to achieve baseline-level performance with only
40\% of the fine-tuning data, highlighting exceptional data efficiency. Our
work thus offers a practical pathway for developing more capable lightweight
VLMs while introducing a fresh theoretical lens to better understand and
address alignment bottlenecks in constrained multimodal systems.

</details>


### [772] [Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis](https://arxiv.org/pdf/2505.13768)
*Ruiquan Huang, Donghao Li, Chengshuai Shi, Cong Shen, Jing Yang*

Main category: cs.LG

TL;DR: A hybrid learning framework for RL combines offline datasets and online interactions, outperforming pure online or offline methods. It achieves state-of-the-art results in sub-optimality gap and regret minimization, with theoretical and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To improve RL performance by leveraging both offline datasets and online interactions, addressing limitations of pure online or offline methods.

Method: A unified algorithm combining confidence-based online RL with offline datasets, analyzed under sub-optimality gap and regret metrics.

Result: The algorithm achieves improved sub-optimality gap and regret minimization, with theoretical bounds and experimental validation in linear contextual bandits and MDPs.

Conclusion: The hybrid framework outperforms pure methods, revealing insights into offline dataset coverage for different learning objectives.

Abstract: This paper investigates a hybrid learning framework for reinforcement
learning (RL) in which the agent can leverage both an offline dataset and
online interactions to learn the optimal policy. We present a unified algorithm
and analysis and show that augmenting confidence-based online RL algorithms
with the offline dataset outperforms any pure online or offline algorithm alone
and achieves state-of-the-art results under two learning metrics, i.e.,
sub-optimality gap and online learning regret. Specifically, we show that our
algorithm achieves a sub-optimality gap
$\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where
$\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$
are the numbers of offline and online samples, respectively. For regret
minimization, we show that it achieves a constant $\tilde{O}(
\sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure
online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability
coefficient over all sub-optimal policies. Our results also reveal an
interesting separation on the desired coverage properties of the offline
dataset for sub-optimality gap minimization and regret minimization. We further
validate our theoretical findings in several experiments in special RL models
such as linear contextual bandits and Markov decision processes (MDPs).

</details>


### [773] [Table Foundation Models: on knowledge pre-training for tabular learning](https://arxiv.org/pdf/2505.14415)
*Myung Jun Kim, Félix Lefebvre, Gaëtan Brison, Alexandre Perez-Lebel, Gaël Varoquaux*

Main category: cs.LG

TL;DR: TARTE is a foundation model for tabular data that enhances vector representations using strings to capture semantics, improving prediction accuracy and computation efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing table foundation models require fine-tuning, incur high computation costs, and lack reusability, limiting their convenience and performance.

Method: TARTE pre-trains on large relational data to generate knowledge-enhanced vector representations, which can be fine-tuned or combined with other models.

Result: TARTE improves prediction accuracy and computation performance, offering domain-specific representations for further learning.

Conclusion: TARTE provides an effective approach to knowledge pre-training for tabular learning, addressing limitations of existing models.

Abstract: Table foundation models bring high hopes to data science: pre-trained on
tabular data to embark knowledge or priors, they should facilitate downstream
tasks on tables. One specific challenge is that of data semantics: numerical
entries take their meaning from context, e.g., column name. Pre-trained neural
networks that jointly model column names and table entries have recently
boosted prediction accuracy. While these models outline the promises of world
knowledge to interpret table values, they lack the convenience of popular
foundation models in text or vision. Indeed, they must be fine-tuned to bring
benefits, come with sizeable computation costs, and cannot easily be reused or
combined with other architectures. Here we introduce TARTE, a foundation model
that transforms tables to knowledge-enhanced vector representations using the
string to capture semantics. Pre-trained on large relational data, TARTE yields
representations that facilitate subsequent learning with little additional
cost. These representations can be fine-tuned or combined with other learners,
giving models that push the state-of-the-art prediction performance and improve
the prediction/computation performance trade-off. Specialized to a task or a
domain, TARTE gives domain-specific representations that facilitate further
learning. Our study demonstrates an effective approach to knowledge
pre-training for tabular learning.

</details>


### [774] [Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks](https://arxiv.org/pdf/2505.20137)
*Cédric Goemaere, Gaspard Oliviers, Rafal Bogacz, Thomas Demeester*

Main category: cs.LG

TL;DR: The paper introduces Error Optimization (EO) to solve signal decay in Predictive Coding (PC), enabling deeper neural networks by preserving gradients and matching backpropagation's performance.


<details>
  <summary>Details</summary>
Motivation: Predictive Coding (PC) struggles with deeper architectures due to signal decay, limiting its practical use compared to backpropagation.

Method: The authors propose Error Optimization (EO), a reparameterization that optimizes prediction errors instead of states, preventing gradient attenuation.

Result: EO eliminates signal decay, allowing faster convergence and matching backpropagation's performance in deeper models.

Conclusion: EO advances PC by enabling deeper architectures, offering theoretical insights and practical scalability for biologically-inspired learning.

Abstract: Predictive Coding (PC) offers a biologically plausible alternative to
backpropagation for neural network training, yet struggles with deeper
architectures. This paper identifies the root cause: an inherent signal decay
problem where gradients attenuate exponentially with depth, becoming
computationally negligible due to numerical precision constraints. To address
this fundamental limitation, we introduce Error Optimization (EO), a novel
reparameterization that preserves PC's theoretical properties while eliminating
signal decay. By optimizing over prediction errors rather than states, EO
enables signals to reach all layers simultaneously and without attenuation,
converging orders of magnitude faster than standard PC. Experiments across
multiple architectures and datasets demonstrate that EO matches
backpropagation's performance even for deeper models where conventional PC
struggles. Besides practical improvements, our work provides theoretical
insight into PC dynamics and establishes a foundation for scaling
biologically-inspired learning to deeper architectures on digital hardware and
beyond.

</details>


### [775] [Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs](https://arxiv.org/pdf/2505.17662)
*Tianheng Ling, Chao Qian, Lukas Johannes Haßler, Gregor Schiele*

Main category: cs.LG

TL;DR: A unified, automated framework for deploying Tiny Transformers on embedded FPGAs, achieving low energy and latency with 4-bit quantization.


<details>
  <summary>Details</summary>
Motivation: Transformer models are resource-heavy for constrained devices; existing FPGA deployments lack automation and flexibility.

Method: Combines quantization-aware training, hardware-aware hyperparameter search, and automatic VHDL generation for deployment.

Result: Achieves 0.033 mJ per inference with millisecond latency on AMD Spartan-7, validated on six datasets.

Conclusion: The framework enables efficient, task-specific Transformer deployment on embedded FPGAs, with code publicly available.

Abstract: Transformer-based models have shown strong performance across diverse
time-series tasks, but their deployment on resource-constrained devices remains
challenging due to high memory and computational demand. While prior work
targeting Microcontroller Units (MCUs) has explored hardware-specific
optimizations, such approaches are often task-specific and limited to 8-bit
fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater
flexibility, enabling fine-grained control over data precision and
architecture. However, existing FPGA-based deployments of Transformers for
time-series analysis typically focus on high-density platforms with manual
configuration. This paper presents a unified and fully automated deployment
framework for Tiny Transformers on embedded FPGAs. Our framework supports a
compact encoder-only Transformer architecture across three representative
time-series tasks (forecasting, classification, and anomaly detection). It
combines quantization-aware training (down to 4 bits), hardware-aware
hyperparameter search using Optuna, and automatic VHDL generation for seamless
deployment. We evaluate our framework on six public datasets across two
embedded FPGA platforms. Results show that our framework produces integer-only,
task-specific Transformer accelerators achieving as low as 0.033 mJ per
inference with millisecond latency on AMD Spartan-7, while also providing
insights into deployment feasibility on Lattice iCE40. All source code will be
released in the GitHub repository
(https://github.com/Edwina1030/TinyTransformer4TS).

</details>


### [776] [NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation](https://arxiv.org/pdf/2505.21020)
*Yuan Gao, Ruiqi Shu, Hao Wu, Fan Xu, Yanfei Xiang, Ruijian Gou, Qingsong Wen, Xian Wu, Xiaomeng Huang*

Main category: cs.LG

TL;DR: Proposes NeuralOM, a neural ocean model for S2S simulation using a multi-scale interactive graph neural network, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current ML models for S2S ocean simulation lack physical consistency and fail to account for the ocean's slow-changing properties.

Method: Uses a multi-stage framework and multi-scale interactive messaging module to model ocean dynamics, capturing gradient changes and multiplicative coupling.

Result: NeuralOM outperforms state-of-the-art models in S2S and extreme event simulation.

Conclusion: NeuralOM effectively addresses limitations of current ML models for S2S ocean simulation, offering improved accuracy and physical consistency.

Abstract: Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically
important for marine research, yet remains challenging due to its substantial
thermal inertia and extended time delay. Machine learning (ML)-based models
have demonstrated significant advancements in simulation accuracy and
computational efficiency compared to traditional numerical methods.
Nevertheless, a significant limitation of current ML models for S2S ocean
simulation is their inadequate incorporation of physical consistency and the
slow-changing properties of the ocean system. In this work, we propose a neural
ocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive
graph neural network to emulate diverse physical phenomena associated with
ocean systems effectively. Specifically, we propose a multi-stage framework
tailored to model the ocean's slowly changing nature. Additionally, we
introduce a multi-scale interactive messaging module to capture complex
dynamical behaviors, such as gradient changes and multiplicative coupling
relationships inherent in ocean dynamics. Extensive experimental evaluations
confirm that our proposed NeuralOM outperforms state-of-the-art models in S2S
and extreme event simulation. The codes are available at
https://github.com/YuanGao-YG/NeuralOM.

</details>


### [777] [TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction](https://arxiv.org/pdf/2505.21807)
*Tommy Xu, Zhitian Zhang, Xiangyu Sun, Lauren Kelly Zung, Hossein Hajimirsadeghi, Greg Mori*

Main category: cs.LG

TL;DR: A new approach combines reasoning-based LLMs with reinforcement learning for accurate and interpretable tabular data predictions.


<details>
  <summary>Details</summary>
Motivation: Existing methods like gradient boosting and deep models lack interpretability, while LLMs underperform in tabular data prediction.

Method: Uses reinforcement learning-trained LLMs with custom reward functions for accuracy and human-understandable explanations.

Result: Evaluated on financial datasets, outperforming established LLMs.

Conclusion: The method improves both prediction accuracy and interpretability for tabular data.

Abstract: Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward better prediction accuracy but also toward human-understandable reasons
for its predictions. The proposed method is evaluated on financial benchmark
datasets and compared against established LLMs.

</details>


### [778] [Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting](https://arxiv.org/pdf/2505.22768)
*Mert Onur Cakiroglu, Idil Bilge Altun, Mehmet Dalkilic, Elham Buxton, Hasan Kurban*

Main category: cs.LG

TL;DR: DRAGON introduces Multivariate de Bruijn Graphs (MdBGs) to enhance time series forecasting by combining symbolic representations with neural modeling.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like temporal heterogeneity and high dimensionality in time series forecasting.

Method: Discretizes sequences, maps them to MdBGs, and uses graph-based attention in a dual-branch architecture with CNNs.

Result: Enables dynamic context recovery and structure-aware representations.

Conclusion: DRAGON effectively bridges symbolic and neural approaches for improved forecasting.

Abstract: Time series forecasting remains a challenging task for foundation models due
to temporal heterogeneity, high dimensionality, and the lack of inherent
symbolic structure. In this work, we propose DRAGON (Discrete Representation
and Augmented Graph encoding Over de BruijN Graphs), a novel encoder that
introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between
symbolic representations and neural modeling. DRAGON discretizes continuous
input sequences and maps them onto a fixed graph structure, enabling dynamic
context recovery via graph-based attention. Integrated as an auxiliary module
within a dual-branch architecture, DRAGON augments conventional CNN-based
encoders with symbolic, structure-aware representations. All code developed for
this study is available at:
https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library

</details>


### [779] [Orthogonal Gradient Descent Improves Neural Calibration](https://arxiv.org/pdf/2506.04487)
*C. Evans Hedges*

Main category: cs.LG

TL;DR: Orthogonalizing gradients ($\perp$Grad) improves model calibration without losing accuracy, outperforming SGD in metrics like test loss and predictive entropy, even under input corruption.


<details>
  <summary>Details</summary>
Motivation: To enhance model calibration while maintaining accuracy, addressing issues like overconfidence and degraded performance under corruption.

Method: Introduces $\perp$Grad, an optimizer-agnostic technique that orthogonalizes gradients during training, with minimal computational overhead.

Result: $\perp$Grad matches SGD in accuracy but improves calibration metrics (lower test loss, reduced overconfidence, higher entropy) and degrades more gracefully under corruption.

Conclusion: $\perp$Grad is effective for improving calibration, works with post-hoc techniques, and theoretically converges to solutions favoring confidence scaling over boundary improvement.

Abstract: We provide evidence that orthogonalizing gradients during training improves
model calibration without sacrificing accuracy. On CIFAR-10 with 10\% labeled
data, $\perp$Grad matches SGD in accuracy but yields consistently improved
calibration metrics such as lower test loss, reduced softmax overconfidence,
and higher predictive entropy. These benefits persist under input corruption
(CIFAR-10C) and extended training, where $\perp$Grad models degrade more
gracefully than SGD-trained counterparts. $\perp$Grad is optimizer-agnostic,
incurs minimal overhead, and works well with post-hoc calibration techniques
like temperature scaling.
  Theoretically, we prove convergence of a simplified version of $\perp$Grad
under mild assumptions and characterize its stationary points in positive
homogeneous networks: $\perp$Grad converges to solutions where further loss
reduction requires confidence scaling rather than decision boundary
improvement. Code for this paper can be found at:
https://github.com/evanshedges2/orthograd\_improves\_calibration.

</details>


### [780] [Two-dimensional Taxonomy for N-ary Knowledge Representation Learning Methods](https://arxiv.org/pdf/2506.05626)
*Xiaohua Lu, Liubov Tupikina, Mehwish Alam*

Main category: cs.LG

TL;DR: The paper surveys methods for handling n-ary relational data, comparing knowledge graphs and hypergraphs, and introduces a taxonomy for modeling approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of knowledge graphs (losing higher-order relational details) and hypergraphs (overlooking entity roles), the paper explores hybrid approaches like knowledge hypergraphs and hyper-relational knowledge graphs.

Method: Proposes a two-dimensional taxonomy: methodology-based (e.g., translation-based, tensor factorization) and role/position awareness (e.g., aware-less, role-aware).

Result: Comprehensive review of n-ary relational data methods, datasets, and training strategies, highlighting gaps.

Conclusion: Identifies open challenges and encourages future research in modeling complex knowledge structures.

Abstract: Real-world knowledge can take various forms, including structured,
semi-structured, and unstructured data. Among these, knowledge graphs are a
form of structured human knowledge that integrate heterogeneous data sources
into structured representations but typically reduce complex n-ary relations to
simple triples, thereby losing higher-order relational details. In contrast,
hypergraphs naturally represent n-ary relations with hyperedges, which directly
connect multiple entities together. Yet hypergraph representation learning
often overlooks entity roles in hyperedges, limiting the finegrained semantic
modelling. To address these issues, knowledge hypergraphs and hyper-relational
knowledge graphs combine the advantages of knowledge graphs and hypergraphs to
better capture the complex structures and role-specific semantics of real world
knowledge. This survey provides a comprehensive review of methods handling
n-ary relational data, covering both knowledge hypergraphs and hyper-relational
knowledge graphs literatures. We propose a two-dimensional taxonomy: the first
dimension categorises models based on their methodology, i.e.,
translation-based models, tensor factorisation-based models, deep neural
network-based models, logic rules-based models, and hyperedge expansion-based
models. The second dimension classifies models according to their awareness of
entity roles and positions in n-ary relations, dividing them into aware-less,
position-aware, and role-aware approaches. Finally, we discuss existing
datasets, training settings and strategies, and outline open challenges to
inspire future research.

</details>


### [781] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/pdf/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: Vision-QRWKV, a quantum-classical hybrid model, enhances RWKV for image classification, outperforming classical models on noisy datasets.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum computing for improving nonlinear feature transformations in visual tasks, addressing challenges in noisy or subtle class distinctions.

Method: Integrates a variational quantum circuit (VQC) into RWKV's channel mixing component, applied to 14 image classification benchmarks.

Result: Quantum-enhanced RWKV outperforms classical RWKV, especially on datasets like ChestMNIST, RetinaMNIST, and BloodMNIST.

Conclusion: Quantum-enhanced RWKV shows promise for lightweight vision tasks, highlighting architectural trade-offs and future potential.

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [782] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/pdf/2506.09034)
*Sizhe Dang, Yangyang Guo, Yanjun Zhao, Haishan Ye, Xiaodong Zheng, Guang Dai, Ivor Tsang*

Main category: cs.LG

TL;DR: FZOO, a fast zeroth-order optimizer, improves memory efficiency and convergence speed for fine-tuning large language models, outperforming MeZO in accuracy and reducing forward passes.


<details>
  <summary>Details</summary>
Motivation: Address GPU memory bottlenecks in fine-tuning LLMs by improving the trade-off between speed and memory in zeroth-order optimizers.

Method: FZOO uses batched one-sided gradient estimates, adaptive step sizes, and Rademacher random vector perturbations with CUDA parallel processing.

Result: FZOO outperforms MeZO by 3% in accuracy, reduces forward passes by 3x, and achieves Adam-scale convergence speeds.

Conclusion: FZOO enables practical single-GPU, high-speed full-parameter fine-tuning and suggests future work on memory-efficient pre-training.

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [783] [State Entropy Regularization for Robust Reinforcement Learning](https://arxiv.org/pdf/2506.07085)
*Yonatan Ashlag, Uri Koren, Mirco Mutti, Esther Derman, Pierre-Luc Bacon, Shie Mannor*

Main category: cs.LG

TL;DR: State entropy regularization in RL improves robustness to structured and spatially correlated perturbations, unlike standard robust RL methods. Theoretical guarantees and practical insights are provided, contrasting it with policy entropy regularization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of state entropy regularization's benefits in RL, particularly its robustness to structured and spatially correlated perturbations, which are common in transfer learning.

Method: Analyzes state entropy regularization's robustness properties, providing formal guarantees under reward and transition uncertainty, and contrasts it with policy entropy regularization.

Result: State entropy regularization offers better robustness to certain perturbations but is more sensitive to the number of rollouts for policy evaluation compared to policy entropy.

Conclusion: State entropy regularization is theoretically and practically advantageous for specific robustness scenarios, though its effectiveness depends on rollout count.

Abstract: State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.

</details>


### [784] [Aircraft Trajectory Dataset Augmentation in Latent Space](https://arxiv.org/pdf/2506.07585)
*Seokbin Yoon, Keumjin Lee*

Main category: cs.LG

TL;DR: ATRADA is a novel framework for augmenting aircraft trajectory datasets using Transformer encoders, PCA, GMM, and MLP to generate high-quality synthetic data.


<details>
  <summary>Details</summary>
Motivation: Enhancing aircraft trajectory models for robust Air Traffic Management (ATM) tasks like conflict detection and landing time prediction requires sufficient and balanced datasets, which ATRADA addresses through synthetic data generation.

Method: ATRADA uses a Transformer encoder to learn trajectory patterns, projects data into latent space via PCA, fits a GMM for probability distribution, and generates synthetic samples with an MLP.

Result: The framework successfully produces high-quality synthetic trajectory data, outperforming baseline methods in experiments.

Conclusion: ATRADA provides an effective solution for dataset augmentation in aircraft trajectory modeling, improving robustness for downstream ATM tasks.

Abstract: Aircraft trajectory modeling plays a crucial role in Air Traffic Management
(ATM) and is important for various downstream tasks, including conflict
detection and landing time prediction. Dataset augmentation through the
addition of synthetically generated trajectory data is necessary to develop a
more robust aircraft trajectory model and ensure that the trajectory dataset is
sufficient and balanced. In this work, we propose a novel framework called
ATRADA for aircraft trajectory dataset augmentation. In the proposed framework,
a Transformer encoder learns the underlying patterns in the original trajectory
dataset and converts each data point into a context vector in the learned
latent space. The converted dataset in the latent space is projected into
reduced dimensions using principal component analysis (PCA), and a Gaussian
mixture model (GMM) is applied to fit the probability distribution of the data
points in the reduced-dimensional space. Finally, new samples are drawn from
the fitted GMM, the dimension of the samples is reverted to the original
dimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several
experiments demonstrate that the framework effectively generates new,
high-quality synthetic aircraft trajectory data, which were compared to the
results of several baselines.

</details>


### [785] [Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment](https://arxiv.org/pdf/2506.10186)
*Yuhui Ding, Thomas Hofmann*

Main category: cs.LG

TL;DR: The paper proposes a method to relax equivariance constraints in diffusion models for 3D molecule generation, achieving comparable performance to equivariant models with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Specialized equivariant architectures limit the scalability and efficiency of diffusion models for 3D molecule generation.

Method: The approach learns a sample-dependent SO(3) transformation for each molecule to create an aligned latent space, then trains a non-equivariant diffusion model on these representations.

Result: The method outperforms non-equivariant models and matches state-of-the-art equivariant models in sample quality while improving efficiency.

Conclusion: The proposed approach offers a scalable and efficient alternative to equivariant diffusion models without sacrificing performance.

Abstract: Equivariant diffusion models have achieved impressive performance in 3D
molecule generation. These models incorporate Euclidean symmetries of 3D
molecules by utilizing an SE(3)-equivariant denoising network. However,
specialized equivariant architectures limit the scalability and efficiency of
diffusion models. In this paper, we propose an approach that relaxes such
equivariance constraints. Specifically, our approach learns a sample-dependent
SO(3) transformation for each molecule to construct an aligned latent space. A
non-equivariant diffusion model is then trained over the aligned
representations. Experimental results demonstrate that our approach performs
significantly better than previously reported non-equivariant models. It yields
sample quality comparable to state-of-the-art equivariant diffusion models and
offers improved training and sampling efficiency. Our code is available at
https://github.com/skeletondyh/RADM

</details>


### [786] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/pdf/2506.08240)
*Dongkyu Cho, Rumi Chunara*

Main category: cs.LG

TL;DR: Revisiting random augmentation to improve its generalization effect by addressing feature distortion, achieving strong performance in single source domain generalization.


<details>
  <summary>Details</summary>
Motivation: Random augmentation is inexpensive but suboptimal due to limited generalization effect, while targeted augmentations are costly.

Method: Propose a solution to address the stochastic nature of random augmentation, which distorts learned features, similar to catastrophic forgetting.

Result: Improved generalization effect of random augmentation, demonstrated across single source domain generalization benchmarks.

Conclusion: A simple solution to random augmentation's shortcomings enhances its generalization performance effectively.

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [787] [Value-Free Policy Optimization via Reward Partitioning](https://arxiv.org/pdf/2506.13702)
*Bilal Faye, Hanane Azzag, Mustapha Lebbah*

Main category: cs.LG

TL;DR: RPO (Reward Partitioning Optimization) is introduced as a new method for single-trajectory RL, outperforming DRO and KTO by eliminating the need for value function approximation and providing direct policy supervision.


<details>
  <summary>Details</summary>
Motivation: Single-trajectory RL methods like DRO face limitations due to value function approximation, leading to high variance and unstable training. RPO aims to address these issues.

Method: RPO normalizes rewards using a partitioning approach estimated from data, avoiding auxiliary models and joint optimization. It simplifies policy supervision.

Result: RPO outperforms DRO and KTO on scalar-feedback tasks, demonstrating robustness and ease of implementation.

Conclusion: RPO is a simple, effective, and theoretically grounded method for single-trajectory policy optimization.

Abstract: Single-trajectory reinforcement learning (RL) methods aim to optimize
policies from datasets consisting of (prompt, response, reward) triplets, where
scalar rewards are directly available. This supervision format is highly
practical, as it mirrors real-world human feedback, such as thumbs-up/down
signals, and avoids the need for structured preference annotations. In
contrast, pairwise preference-based methods like Direct Preference Optimization
(DPO) rely on datasets with both preferred and dispreferred responses, which
are harder to construct and less natural to collect. Among single-trajectory
approaches, Direct Reward Optimization (DRO) has shown strong empirical
performance due to its simplicity and stability. However, DRO requires
approximating a value function, which introduces several limitations: high
off-policy variance, coupling between policy and value learning, and a lack of
absolute supervision on the policy itself. We introduce Reward Partitioning
Optimization (RPO), a new method that resolves these limitations by removing
the need to model the value function. Instead, RPO normalizes observed rewards
using a partitioning approach estimated directly from data. This leads to a
straightforward supervised learning objective on the policy, with no auxiliary
models and no joint optimization. RPO provides direct and stable supervision on
the policy, making it robust and easy to implement in practice. We validate RPO
on scalar-feedback language modeling tasks using Flan-T5 encoder-decoder
models. Our results demonstrate that RPO outperforms existing single-trajectory
baselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings
confirm that RPO is a simple, effective, and theoretically grounded method for
single-trajectory policy optimization.

</details>


### [788] [Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models](https://arxiv.org/pdf/2506.14291)
*Ben Finkelshtein, İsmail İlkan Ceylan, Michael Bronstein, Ron Levie*

Main category: cs.LG

TL;DR: The paper proposes a recipe for designing graph foundation models for node-level tasks, emphasizing symmetries like label permutation-equivariance and feature permutation-invariance. It validates the approach with strong zero-shot performance on 29 datasets.


<details>
  <summary>Details</summary>
Motivation: Current graph ML architectures lack broader applicability due to task-specific designs, prompting the need for generalizable graph foundation models.

Method: Systematically investigates symmetries (node, label, feature permutations) and designs linear transformations respecting these. Proves universality of the resulting network.

Result: Demonstrates strong zero-shot performance and consistent improvement with more training graphs on 29 node classification datasets.

Conclusion: The proposed symmetry-respecting framework effectively generalizes across arbitrary graphs and features, advancing graph foundation models.

Abstract: Graph machine learning architectures are typically tailored to specific tasks
on specific datasets, which hinders their broader applicability. This has led
to a new quest in graph machine learning: how to build graph foundation models
capable of generalizing across arbitrary graphs and features? In this work, we
present a recipe for designing graph foundation models for node-level tasks
from first principles. The key ingredient underpinning our study is a
systematic investigation of the symmetries that a graph foundation model must
respect. In a nutshell, we argue that label permutation-equivariance alongside
feature permutation-invariance are necessary in addition to the common node
permutation-equivariance on each local neighborhood of the graph. To this end,
we first characterize the space of linear transformations that are equivariant
to permutations of nodes and labels, and invariant to permutations of features.
We then prove that the resulting network is a universal approximator on
multisets that respect the aforementioned symmetries. Our recipe uses such
layers on the multiset of features induced by the local neighborhood of the
graph to obtain a class of graph foundation models for node property
prediction. We validate our approach through extensive experiments on 29
real-world node classification datasets, demonstrating both strong zero-shot
empirical performance and consistent improvement as the number of training
graphs increases.

</details>


### [789] [Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs](https://arxiv.org/pdf/2506.14540)
*Gerardo A. Flores, Alyssa H. Smith, Julia A. Fukuyama, Ashia C. Wilson*

Main category: cs.LG

TL;DR: The paper proposes a clinical evaluation framework for machine learning models, addressing limitations of traditional metrics like accuracy and AUC-ROC by incorporating calibration, robustness, and cost asymmetry.


<details>
  <summary>Details</summary>
Motivation: Current scoring rules (e.g., accuracy, AUC-ROC) inadequately reflect clinical priorities such as calibration, robustness to shifts, and asymmetric error costs.

Method: The framework uses an adjusted cross-entropy (log score) derived from proper scoring rules, averaging cost-weighted performance over relevant class balance ranges.

Result: The proposed evaluation is practical, sensitive to clinical conditions, and prioritizes calibrated, robust models.

Conclusion: The framework improves model selection for clinical decision-making by aligning evaluation with real-world deployment needs.

Abstract: Machine learning-based decision support systems are increasingly deployed in
clinical settings, where probabilistic scoring functions are used to inform and
prioritize patient management decisions. However, widely used scoring rules,
such as accuracy and AUC-ROC, fail to adequately reflect key clinical
priorities, including calibration, robustness to distributional shifts, and
sensitivity to asymmetric error costs. In this work, we propose a principled
yet practical evaluation framework for selecting calibrated thresholded
classifiers that explicitly accounts for the uncertainty in class prevalences
and domain-specific cost asymmetries often found in clinical settings. Building
on the theory of proper scoring rules, particularly the Schervish
representation, we derive an adjusted variant of cross-entropy (log score) that
averages cost-weighted performance over clinically relevant ranges of class
balance. The resulting evaluation is simple to apply, sensitive to clinical
deployment conditions, and designed to prioritize models that are both
calibrated and robust to real-world variations.

</details>


### [790] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/pdf/2506.14436)
*Shen Yuan, Yin Zheng, Taifeng Wang, Binbin Liu, Hongteng Xu*

Main category: cs.LG

TL;DR: The paper proposes MoORE, a method for multi-task adaptation of foundation models using a Mixture of Orthogonal Rank-one Experts to avoid task conflict and oblivion.


<details>
  <summary>Details</summary>
Motivation: Large-scale foundation models face issues like task conflict and oblivion in multi-task scenarios, necessitating a robust adaptation method.

Method: The method applies SVD to a pre-trained model's weight matrix, introduces a learnable router for task-specific adjustments, and ensures orthogonality among experts.

Result: MoORE outperforms existing methods in multi-task adaptation, demonstrating superior conflict- and oblivion-resistance.

Conclusion: MoORE is an effective solution for multi-task adaptation, maintaining model performance while avoiding common pitfalls.

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers
from task conflict and oblivion. To mitigate such issues, we propose a novel
''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant
multi-task adaptation method. Given a weight matrix of a pre-trained model, our
method applies SVD to it and introduces a learnable router to adjust its
singular values based on tasks and samples. Accordingly, the weight matrix
becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert
corresponds to the outer product of a left singular vector and the
corresponding right one. We can improve the model capacity by imposing a
learnable orthogonal transform on the right singular vectors. Unlike low-rank
adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts'
orthogonality and maintains the column space of the original weight matrix.
These two properties make the adapted model resistant to the conflicts among
the new tasks and the oblivion of its original tasks, respectively. Experiments
on various datasets demonstrate that MoORE outperforms existing multi-task
adaptation methods consistently, showing its superiority in terms of conflict-
and oblivion-resistance. The code of the experiments is available at
https://github.com/DaShenZi721/MoORE.

</details>


### [791] [Neural Canonical Polyadic Factorization for Traffic Analysis](https://arxiv.org/pdf/2506.15079)
*Yikai Hou, Peng Tang*

Main category: cs.LG

TL;DR: The paper introduces NCPF, a model combining low-rank tensor algebra and deep learning for robust traffic data imputation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Missing traffic data due to sensor failures and sensing gaps hinders reliable traffic modeling, necessitating advanced imputation techniques.

Method: NCPF integrates CP decomposition with neural networks, using learnable embeddings and hierarchical feature fusion to model spatiotemporal interactions.

Result: NCPF outperforms six state-of-the-art baselines across six urban traffic datasets.

Conclusion: NCPF unifies interpretable factor analysis with neural networks, offering a flexible solution for traffic data imputation and supporting future transportation systems.

Abstract: Modern intelligent transportation systems rely on accurate spatiotemporal
traffic analysis to optimize urban mobility and infrastructure resilience.
However, pervasive missing data caused by sensor failures and heterogeneous
sensing gaps fundamentally hinders reliable traffic modeling. This paper
proposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes
low-rank tensor algebra with deep representation learning for robust traffic
data imputation. The model innovatively embeds CP decomposition into neural
architecture through learnable embedding projections, where sparse traffic
tensors are encoded into dense latent factors across road segments, time
intervals, and mobility metrics. A hierarchical feature fusion mechanism
employs Hadamard products to explicitly model multilinear interactions, while
stacked multilayer perceptron layers nonlinearly refine these representations
to capture complex spatiotemporal couplings. Extensive evaluations on six urban
traffic datasets demonstrate NCPF's superiority over six state-of-the-art
baselines. By unifying CP decomposition's interpretable factor analysis with
neural network's nonlinear expressive power, NCPF provides a principled yet
flexible approaches for high-dimensional traffic data imputation, offering
critical support for next-generation transportation digital twins and adaptive
traffic control systems.

</details>


### [792] [Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data](https://arxiv.org/pdf/2506.16629)
*Eric V. Strobl*

Main category: cs.LG

TL;DR: DEBIAS optimizes outcome definitions to maximize causal identifiability, outperforming existing methods in psychiatric longitudinal data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of causal inference in psychiatry due to symptom heterogeneity and latent confounding.

Method: DEBIAS algorithm learns non-negative, interpretable weights for outcome aggregation, minimizing confounding.

Result: Outperforms state-of-the-art methods in depression and schizophrenia data.

Conclusion: DEBIAS provides a robust, interpretable solution for causal inference in psychiatry.

Abstract: Causal inference in longitudinal biomedical data remains a central challenge,
especially in psychiatry, where symptom heterogeneity and latent confounding
frequently undermine classical estimators. Most existing methods for treatment
effect estimation presuppose a fixed outcome variable and address confounding
through observed covariate adjustment. However, the assumption of
unconfoundedness may not hold for a fixed outcome in practice. To address this
foundational limitation, we directly optimize the outcome definition to
maximize causal identifiability. Our DEBIAS (Durable Effects with
Backdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,
clinically interpretable weights for outcome aggregation, maximizing durable
treatment effects and empirically minimizing both observed and latent
confounding by leveraging the time-limited direct effects of prior treatments
in psychiatric longitudinal data. The algorithm also furnishes an empirically
verifiable test for outcome unconfoundedness. DEBIAS consistently outperforms
state-of-the-art methods in recovering causal effects for clinically
interpretable composite outcomes across comprehensive experiments in depression
and schizophrenia.

</details>


### [793] [Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains](https://arxiv.org/pdf/2506.17718)
*Zhuo He, Shuang Li, Wenze Song, Longhui Yuan, Jian Liang, Han Li, Kun Gai*

Main category: cs.LG

TL;DR: SYNC introduces a time-aware causal model to address evolving domain generalization by capturing dynamic causal factors and mechanism drifts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve model generalization in dynamic scenarios by addressing spurious correlations in evolving domain generalization (EDG).

Method: Proposes SYNC, a time-aware structural causal model (SCM) with information-theoretic objectives in a sequential VAE framework to learn causal representations.

Result: SYNC achieves superior temporal generalization performance on synthetic and real-world datasets.

Conclusion: SYNC effectively learns time-aware causal representations, providing optimal causal predictors for each time domain.

Abstract: Endowing deep models with the ability to generalize in dynamic scenarios is
of vital significance for real-world deployment, given the continuous and
complex changes in data distribution. Recently, evolving domain generalization
(EDG) has emerged to address distribution shifts over time, aiming to capture
evolving patterns for improved model generalization. However, existing EDG
methods may suffer from spurious correlations by modeling only the dependence
between data and targets across domains, creating a shortcut between
task-irrelevant factors and the target, which hinders generalization. To this
end, we design a time-aware structural causal model (SCM) that incorporates
dynamic causal factors and the causal mechanism drifts, and propose
\textbf{S}tatic-D\textbf{YN}amic \textbf{C}ausal Representation Learning
(\textbf{SYNC}), an approach that effectively learns time-aware causal
representations. Specifically, it integrates specially designed
information-theoretic objectives into a sequential VAE framework which captures
evolving patterns, and produces the desired representations by preserving
intra-class compactness of causal factors both across and within domains.
Moreover, we theoretically show that our method can yield the optimal causal
predictor for each time domain. Results on both synthetic and real-world
datasets exhibit that SYNC can achieve superior temporal generalization
performance.

</details>


### [794] [Decoding Federated Learning: The FedNAM+ Conformal Revolution](https://arxiv.org/pdf/2506.17872)
*Sree Bhargavi Balija, Amitash Nanda, Debashis Sahoo*

Main category: cs.LG

TL;DR: FedNAM+ is a federated learning framework integrating Neural Additive Models (NAMs) and conformal prediction for interpretable, reliable uncertainty estimation, validated on datasets like MNIST and CIFAR.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning frameworks lack solutions combining uncertainty quantification, interpretability, and robustness.

Method: FedNAM+ integrates NAMs with conformal prediction, using gradient-based sensitivity maps for dynamic level adjustment and pixel-wise uncertainty estimates.

Result: High prediction accuracy (e.g., 0.1% loss on MNIST) with transparent uncertainty measures, outperforming methods like Monte Carlo Dropout in efficiency.

Conclusion: FedNAM+ enhances trust and transparency in decentralized predictive modeling with robustness, interpretability, and computational efficiency.

Abstract: Federated learning has significantly advanced distributed training of machine
learning models across decentralized data sources. However, existing frameworks
often lack comprehensive solutions that combine uncertainty quantification,
interpretability, and robustness. To address this, we propose FedNAM+, a
federated learning framework that integrates Neural Additive Models (NAMs) with
a novel conformal prediction method to enable interpretable and reliable
uncertainty estimation. Our method introduces a dynamic level adjustment
technique that utilizes gradient-based sensitivity maps to identify key input
features influencing predictions. This facilitates both interpretability and
pixel-wise uncertainty estimates. Unlike traditional interpretability methods
such as LIME and SHAP, which do not provide confidence intervals, FedNAM+
offers visual insights into prediction reliability. We validate our approach
through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high
prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with
transparent uncertainty measures. Visual analysis highlights variable
uncertainty intervals, revealing low-confidence regions where model performance
can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+
delivers efficient and global uncertainty estimates with reduced computational
overhead, making it particularly suitable for federated learning scenarios.
Overall, FedNAM+ provides a robust, interpretable, and computationally
efficient framework that enhances trust and transparency in decentralized
predictive modeling.

</details>


### [795] [Experimenting, Fast and Slow: Bayesian Optimization of Long-term Outcomes with Online Experiments](https://arxiv.org/pdf/2506.18744)
*Qing Feng, Samuel Daulton, Benjamin Letham, Maximilian Balandat, Eytan Bakshy*

Main category: cs.LG

TL;DR: A novel approach combines fast experiments and offline proxies with slow experiments for efficient Bayesian optimization in large action spaces.


<details>
  <summary>Details</summary>
Motivation: Optimizing long-term treatment effects in online experiments (A/B tests) is challenging due to non-stationarity and lengthy sequential experimentation.

Method: Combines fast experiments (biased, short-term) and offline proxies (e.g., off-policy evaluation) with slow experiments for sequential Bayesian optimization.

Result: Enables efficient optimization over large action spaces in shorter timeframes.

Conclusion: The approach addresses the inefficiency of traditional sequential experimentation by integrating fast and slow methods.

Abstract: Online experiments in internet systems, also known as A/B tests, are used for
a wide range of system tuning problems, such as optimizing recommender system
ranking policies and learning adaptive streaming controllers. Decision-makers
generally wish to optimize for long-term treatment effects of the system
changes, which often requires running experiments for a long time as short-term
measurements can be misleading due to non-stationarity in treatment effects
over time. The sequential experimentation strategies--which typically involve
several iterations--can be prohibitively long in such cases. We describe a
novel approach that combines fast experiments (e.g., biased experiments run
only for a few hours or days) and/or offline proxies (e.g., off-policy
evaluation) with long-running, slow experiments to perform sequential, Bayesian
optimization over large action spaces in a short amount of time.

</details>


### [796] [Training Flexible Models of Genetic Variant Effects from Functional Annotations using Accelerated Linear Algebra](https://arxiv.org/pdf/2506.19598)
*Alan N. Amin, Andres Potapczynski, Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: DeepWAS uses fast linear algebra to train large neural networks for genome-wide association studies, improving predictions by optimizing likelihood, unlike traditional methods limited by summary statistics.


<details>
  <summary>Details</summary>
Motivation: Geneticists face bottlenecks in predicting phenotypes due to expensive linear algebra problems and reliance on simplified summary statistics, limiting model performance.

Method: DeepWAS leverages modern fast linear algebra techniques to train large neural networks, optimizing the full likelihood of the statistical model.

Result: Larger models improve performance only with full likelihood training, not with traditional summary statistics, enabling better disease predictions and therapeutic target identification.

Conclusion: DeepWAS demonstrates the potential of large, flexible models in genomics when trained with full likelihood, overcoming limitations of traditional methods.

Abstract: To understand how genetic variants in human genomes manifest in phenotypes --
traits like height or diseases like asthma -- geneticists have sequenced and
measured hundreds of thousands of individuals. Geneticists use this data to
build models that predict how a genetic variant impacts phenotype given genomic
features of the variant, like DNA accessibility or the presence of nearby
DNA-bound proteins. As more data and features become available, one might
expect predictive models to improve. Unfortunately, training these models is
bottlenecked by the need to solve expensive linear algebra problems because
variants in the genome are correlated with nearby variants, requiring inversion
of large matrices. Previous methods have therefore been restricted to fitting
small models, and fitting simplified summary statistics, rather than the full
likelihood of the statistical model. In this paper, we leverage modern fast
linear algebra techniques to develop DeepWAS (Deep genome Wide Association
Studies), a method to train large and flexible neural network predictive models
to optimize likelihood. Notably, we find that larger models only improve
performance when using our full likelihood approach; when trained by fitting
traditional summary statistics, larger models perform no better than small
ones. We find larger models trained on more features make better predictions,
potentially improving disease predictions and therapeutic target
identification.

</details>


### [797] [EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement Learning Framework](https://arxiv.org/pdf/2506.22200)
*Chen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yue Wang, Yuzhi Zhang*

Main category: cs.LG

TL;DR: EFRame enhances GRPO by adding exploration, filtering low-quality samples, and using replay, improving robustness and reasoning in RL.


<details>
  <summary>Details</summary>
Motivation: GRPO's limitations in exploration, sample efficiency, and stability hinder performance in complex reasoning tasks.

Method: EFRame introduces additional rollouts, online filtering, and experience replay to augment GRPO.

Result: EFRame improves training robustness, efficiency, and unlocks deeper reasoning capabilities.

Conclusion: EFRame provides a structured learning cycle and better sample analysis, outperforming vanilla GRPO.

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [798] [Early Stopping Tabular In-Context Learning](https://arxiv.org/pdf/2506.21387)
*Jaris Küken, Lennart Purucker, Frank Hutter*

Main category: cs.LG

TL;DR: Early-stopping in-context learning for tabular foundation models accelerates inference by up to 2.2x with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: High inference-time costs of tabular foundation models, especially for larger datasets, necessitate efficiency improvements.

Method: Dynamically evaluate stopping in-context learning after each Transformer encoder layer, then decode embeddings with a pre-trained layer-wise decoder.

Result: Speedups of 1.3x on small tasks and 2.2x on larger tasks with negligible performance degradation.

Conclusion: Early exiting is a practical strategy for enhancing tabular in-context learning efficiency.

Abstract: Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [799] [Cooperation as Black Box: Conceptual Fluctuation and Diagnostic Tools for Misalignment in MAS](https://arxiv.org/pdf/2506.22876)
*Shayak Nandi, Fernanda M. Eliott*

Main category: cs.MA

TL;DR: The paper identifies semantic ambiguity in multi-agent systems (MAS) as a key source of misalignment, introduces the Misalignment Mosaic framework to diagnose such issues, and generalizes the approach to other overloaded concepts.


<details>
  <summary>Details</summary>
Motivation: To address upstream misalignment in MAS caused by semantic ambiguity and normative projection, particularly the conflation of cooperation and coordination.

Method: Uses the Rabbit-Duck illusion to illustrate perspective-dependent behavior and introduces the Misalignment Mosaic, a diagnostic framework with four components.

Result: The framework helps diagnose meaning-level misalignment in MAS, extending beyond policy or reward structures to language and design assumptions.

Conclusion: The Misalignment Mosaic provides a tool to address semantic ambiguity in MAS, applicable to other overloaded concepts like alignment and trust.

Abstract: Misalignment in multi-agent systems (MAS) is often treated as a technical
failure; yet many such failures originate upstream, during the conceptual
design phase, where semantic ambiguity and normative projection take place.
This paper identifies a foundational source of interpretive misalignment in
MAS: the systemic conflation of cooperation and coordination, and the moral
overreading that follows. Using the Rabbit-Duck illusion, we illustrate how
perspective-dependent readings of agent behavior can create epistemic
instability. To address this, we introduce the Misalignment Mosaic, a
diagnostic framework for diagnosing meaning-level misalignment in MAS. It
comprises four components: 1. Terminological Inconsistency, 2. Concept-to-Code
Decay, 3. Morality as Cooperation, and 4. Interpretive Ambiguity. The Mosaic
enables researchers to examine how misalignment arises not only through policy
or reward structures but also through language, framing, and design
assumptions. While this paper focuses on the specific ambiguity between
coordination and cooperation, the Mosaic generalizes to other overloaded
concepts in MAS, such as alignment, autonomy, and trust. Rather than define
cooperation once and for all, we offer a framework to diagnose meaning itself
as a source of misalignment.

</details>


### [800] [Interaction Identification of a Heterogeneous NDS with Quadratic-Bilinear Subsystems](https://arxiv.org/pdf/2412.02547)
*Tong Zhou, Yubing Li*

Main category: cs.MA

TL;DR: The paper presents time-domain identification methods for heterogeneous networked dynamic systems (NDS) using quadratic-bilinear time-invariant (QBTI) models, with applications to lumped QBTI systems. It derives explicit formulas for transient and steady-state responses and proposes parameter estimation algorithms.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying interaction parameters in heterogeneous NDS and extend the results to lumped QBTI systems without sampling rate restrictions.

Method: Derives explicit formulas for NDS responses under LTI probing signals, establishes relations between steady-state responses and frequency-domain mappings, and proposes parameter estimation algorithms.

Result: The derived relations allow estimation of generalized TFMs and their derivatives from time-domain data. A numerical example demonstrates the algorithm's effectiveness.

Conclusion: The paper provides a framework for time-domain identification in NDS and QBTI systems, with practical estimation algorithms validated by numerical results.

Abstract: This paper attacks time-domain identification for interaction parameters of a
heterogeneous networked dynamic system (NDS), with each of its subsystems being
described by a continuous-time descriptor quadratic-bilinear time-invariant
(QBTI) model. The obtained results can also be applied to parameter estimations
for a lumped QBTI system. No restrictions are put on the sampling rate.
Explicit formulas are derived respectively for the transient and steady-state
responses of the NDS, provided that the probing signal is generated by a linear
time invariant (LTI) system. Some relations have been derived between the NDS
steady-state response and its frequency domain input-output mappings. These
relations reveal that the value of some NDS associated generalized TFMs can in
principle be estimated at almost any interested point of the imaginary axis
from time-domain input-output experimental data, as well as its derivatives and
a right tangential interpolation along an arbitrary direction. Based on these
relations, an estimation algorithm is suggested respectively for the parameters
of the NDS and the values of these generalized TFMs. A numerical example is
included to illustrate characteristics of the suggested estimation algorithms.

</details>


### [801] [A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems](https://arxiv.org/pdf/2505.22814)
*Jonghan Lim, Ilya Kovalenko*

Main category: cs.MA

TL;DR: A large language model-enabled control architecture for multi-agent manufacturing systems improves resilience and flexibility in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Manufacturing complexity and unpredictability demand real-time adaptation, which traditional and current multi-agent systems struggle with.

Method: Proposes a large language model-enabled control architecture for dynamic resource capability exploration in multi-agent systems.

Result: Simulation shows improved throughput and resource utilization compared to existing methods.

Conclusion: The architecture enhances system resilience and flexibility in dynamic manufacturing settings.

Abstract: Manufacturing environments are becoming more complex and unpredictable due to
factors such as demand variations and shorter product lifespans. This
complexity requires real-time decision-making and adaptation to disruptions.
Traditional control approaches highlight the need for advanced control
strategies capable of overcoming unforeseen challenges, as they demonstrate
limitations in responsiveness within dynamic industrial settings. Multi-agent
systems address these challenges through decentralization of decision-making,
enabling systems to respond dynamically to operational changes. However,
current multi-agent systems encounter challenges related to real-time
adaptation, context-aware decision-making, and the dynamic exploration of
resource capabilities. Large language models provide the possibility to
overcome these limitations through context-aware decision-making capabilities.
This paper introduces a large language model-enabled control architecture for
multi-agent manufacturing systems to dynamically explore resource capabilities
in response to real-time disruptions. A simulation-based case study
demonstrates that the proposed architecture improves system resilience and
flexibility. The case study findings show improved throughput and efficient
resource utilization compared to existing approaches.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [802] [TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity](https://arxiv.org/pdf/2506.23484)
*Yuzhuo Chen, Zehua Ma, Han Fang, Weiming Zhang, Nenghai Yu*

Main category: cs.MM

TL;DR: TAG-WM is a tamper-aware generative watermarking method that enhances robustness and localization while preserving image quality.


<details>
  <summary>Details</summary>
Motivation: Address copyright and authenticity risks in AI-generated content by improving watermarking robustness and tampering localization.

Method: Proposes TAG-WM with four modules: DMJS for watermark embedding, WLR for reconstruction, DVRD for tamper detection, and TAD for decoding.

Result: Achieves state-of-the-art tampering robustness and localization with lossless quality and 256-bit capacity.

Conclusion: TAG-WM effectively addresses current watermarking limitations for AI-generated content.

Abstract: AI-generated content (AIGC) enables efficient visual creation but raises
copyright and authenticity risks. As a common technique for integrity
verification and source tracing, digital image watermarking is regarded as a
potential solution to above issues. Among these, watermarking methods capable
of preserving the generation quality are receiving increased attention.
However, the proliferation and high performance of generative image editing
applications have elevated the risks of malicious tampering, creating new
demands. 1) The tamper robustness of current lossless visual quality watermarks
remains constrained by the modification-sensitive diffusion inversion process,
necessitating enhanced robustness. 2) The improved tampering quality and rapid
iteration cycles render passive tampering detection methods inadequate, making
proactive tampering localization capability a desired feature for watermarks.
To address these requirements, this paper proposes a Tamper-Aware Generative
image WaterMarking method named TAG-WM. The proposed method comprises four key
modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright
and localization watermarks into the latent space while preserving generative
quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a
dense variation region detector (DVRD) leveraging diffusion inversion
sensitivity to identify tampered areas via statistical deviation analysis, and
the tamper-aware decoding (TAD) guided by localization results. The
experimental results indicate that TAG-WM achieves SOTA tampering robustness
and tampering localization capability with distortions while maintaining
lossless generation quality and a considerable capacity of 256 bits.

</details>


### [803] [Efficient and Accurate Image Provenance Analysis: A Scalable Pipeline for Large-scale Images](https://arxiv.org/pdf/2506.23707)
*Jiewei Lai, Lan Zhang, Chen Tang, Pengcheng Sun*

Main category: cs.MM

TL;DR: A scalable end-to-end pipeline for image provenance analysis improves accuracy and reduces complexity, outperforming existing methods in large-scale scenarios.


<details>
  <summary>Details</summary>
Motivation: The need for robust forensic tools to trace modified images on social networks, addressing accuracy and scalability limitations of current methods.

Method: Proposes a pipeline integrating modification relationship tracing, local features matching, and compression artifact capturing to construct a directed provenance graph with linear complexity.

Result: Achieves 16.7-56.1% higher accuracy and significantly faster response times (3.0 seconds vs. 12 minutes for 10M images).

Conclusion: The pipeline effectively addresses scalability and accuracy challenges in image provenance analysis, making it suitable for large-scale applications.

Abstract: The rapid proliferation of modified images on social networks that are driven
by widely accessible editing tools demands robust forensic tools for digital
governance. Image provenance analysis, which filters various query image
variants and constructs a directed graph to trace their phylogeny history, has
emerged as a critical solution. However, existing methods face two fundamental
limitations: First, accuracy issues arise from overlooking heavily modified
images due to low similarity while failing to exclude unrelated images and
determine modification directions under diverse modification scenarios. Second,
scalability bottlenecks stem from pairwise image analysis incurs quadratic
complexity, hindering application in large-scale scenarios. This paper presents
a scalable end-to-end pipeline for image provenance analysis that achieves high
precision with linear complexity. This improves filtering effectiveness through
modification relationship tracing, which enables the comprehensive discovery of
image variants regardless of their visual similarity to the query. In addition,
the proposed pipeline integrates local features matching and compression
artifact capturing, enhancing robustness against diverse modifications and
enabling accurate analysis of images' relationships. This allows the generation
of a directed provenance graph that accurately characterizes the image's
phylogeny history. Furthermore, by optimizing similarity calculations and
eliminating redundant pairwise analysis during graph construction, the pipeline
achieves a linear time complexity, ensuring its scalability for large-scale
scenarios. Experiments demonstrate pipeline's superior performance, achieving a
16.7-56.1% accuracy improvement. Notably, it exhibits significant scalability
with an average 3.0-second response time on 10 million scale images, which is
far shorter than the SOTA approach's 12-minute duration.

</details>


### [804] [Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks](https://arxiv.org/pdf/2502.05695)
*Zijiang Yan, Jianhua Pei, Hongda Wu, Hina Tabassum, Ping Wang*

Main category: cs.MM

TL;DR: A novel Semantic Communication framework using Latent Diffusion Models (LDMs) for adaptive-bitrate video streaming, improving bandwidth, storage, and QoE over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges like high bandwidth usage, storage inefficiencies, and QoE degradation in traditional video streaming methods.

Method: Integrates LDMs to compress I-frames into latent space, retains B/P-frames as metadata, and uses denoising and Video Frame Interpolation for quality.

Result: Achieves high-quality streaming with optimized bandwidth, outperforming existing solutions in QoE and resource efficiency.

Conclusion: Enables scalable real-time video streaming for 5G and future networks.

Abstract: This paper proposes a novel Semantic Communication (SemCom) framework for
real-time adaptive-bitrate video streaming by integrating Latent Diffusion
Models (LDMs) within the FFmpeg techniques. This solution addresses the
challenges of high bandwidth usage, storage inefficiencies, and quality of
experience (QoE) degradation associated with traditional Constant Bitrate
Streaming (CBS) and Adaptive Bitrate Streaming (ABS). The proposed approach
leverages LDMs to compress I-frames into a latent space, offering significant
storage and semantic transmission savings without sacrificing high visual
quality. While retaining B-frames and P-frames as adjustment metadata to
support efficient refinement of video reconstruction at the user side, the
proposed framework further incorporates state-of-the-art denoising and Video
Frame Interpolation (VFI) techniques. These techniques mitigate semantic
ambiguity and restore temporal coherence between frames, even in noisy wireless
communication environments. Experimental results demonstrate the proposed
method achieves high-quality video streaming with optimized bandwidth usage,
outperforming state-of-the-art solutions in terms of QoE and resource
efficiency. This work opens new possibilities for scalable real-time video
streaming in 5G and future post-5G networks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [805] [Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR](https://arxiv.org/pdf/2506.22646)
*Weiqing Wang, Taejin Park, Ivan Medennikov, Jinhan Wang, Kunal Dhawan, He Huang, Nithin Rao Koluguri, Jagadeesh Balam, Boris Ginsburg*

Main category: eess.AS

TL;DR: A self-speaker adaptation method for streaming multi-talker ASR eliminates explicit speaker queries by dynamically adapting ASR instances via speaker-wise speech activity prediction.


<details>
  <summary>Details</summary>
Motivation: Conventional methods require target speaker embeddings or enrollment audio, which is inefficient. This work aims to streamline speaker adaptation without such dependencies.

Method: Speaker-specific kernels, generated via speaker supervision activations, are injected into selected ASR encoder layers for dynamic adaptation.

Result: State-of-the-art performance in offline and streaming scenarios, handling fully overlapped speech effectively.

Conclusion: The self-speaker adaptation method is robust for multi-talker ASR under severe overlapping speech conditions.

Abstract: We propose a self-speaker adaptation method for streaming multi-talker
automatic speech recognition (ASR) that eliminates the need for explicit
speaker queries. Unlike conventional approaches requiring target speaker
embeddings or enrollment audio, our technique dynamically adapts individual ASR
instances through speaker-wise speech activity prediction. The key innovation
involves injecting speaker-specific kernels generated via speaker supervision
activations into selected ASR encoder layers. This enables instantaneous
speaker adaptation to target speakers while handling fully overlapped speech
even in a streaming scenario. Experiments show state-of-the-art performance in
both offline and streaming scenarios, demonstrating that our self-adaptive
method effectively addresses severe speech overlap through streamlined
speaker-focused recognition. The results validate the proposed self-speaker
adaptation approach as a robust solution for multi-talker ASR under severe
overlapping speech conditions.

</details>


### [806] [Adaptable Non-parametric Approach for Speech-based Symptom Assessment: Isolating Private Medical Data in a Retrieval Datastore](https://arxiv.org/pdf/2506.22972)
*Yu-Wen Chen, Julia Hirschberg*

Main category: eess.AS

TL;DR: NoNPSA is a non-parametric framework for speech-based symptom assessment, prioritizing privacy and adaptability by isolating medical data in a retrieval datastore and using self-supervised learning for feature extraction.


<details>
  <summary>Details</summary>
Motivation: To address privacy and adaptability challenges in parametric models for health-related acoustic cue assessment.

Method: Uses a retrieval datastore for medical data, self-supervised learning for feature extraction, metadata-aware refinement, and similarity-based retrieval for assessment.

Result: Competitive performance compared to fine-tuning SSL-based methods, with added benefits of privacy, update efficiency, and adaptability.

Conclusion: NoNPSA demonstrates the potential of non-parametric approaches in healthcare for privacy-preserving and adaptable symptom assessment.

Abstract: The automatic assessment of health-related acoustic cues has the potential to
improve healthcare accessibility and affordability. Although parametric models
are promising, they face challenges in privacy and adaptability. To address
these, we propose a NoN-Parametric framework for Speech-based symptom
Assessment (NoNPSA). By isolating medical data in a retrieval datastore, NoNPSA
avoids encoding private information in model parameters and enables efficient
data updates. A self-supervised learning (SSL) model pre-trained on
general-purpose datasets extracts features, which are used for similarity-based
retrieval. Metadata-aware refinement filters the retrieved data, and associated
labels are used to compute an assessment score. Experimental results show that
NoNPSA achieves competitive performance compared to fine-tuning SSL-based
methods, while enabling greater privacy, update efficiency, and
adaptability--showcasing the potential of non-parametric approaches in
healthcare.

</details>


### [807] [Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation](https://arxiv.org/pdf/2506.23371)
*Frank Cwitkowitz, Zhiyao Duan*

Main category: eess.AS

TL;DR: The paper explores combining supervised and self-supervised learning for Multi-Pitch Estimation (MPE), showing improved performance but uncovering a dual overfitting-degeneration issue.


<details>
  <summary>Details</summary>
Motivation: Existing MPE methods rely heavily on supervised learning, which faces challenges due to limited annotated data. Self-supervised techniques show promise but lag behind supervised methods.

Method: Extends supervised MPE by incorporating self-supervised objectives based on pitch-invariant and pitch-equivariant properties, using joint training.

Result: Substantial improvement under closed training conditions, but reveals a dual overfitting-degeneration issue when applied to broader data.

Conclusion: The study highlights the potential and challenges of combining supervised and self-supervised learning in MPE, offering insights into the observed phenomenon.

Abstract: Multi-Pitch Estimation (MPE) continues to be a sought after capability of
Music Information Retrieval (MIR) systems, and is critical for many
applications and downstream tasks involving pitch, including music
transcription. However, existing methods are largely based on supervised
learning, and there are significant challenges in collecting annotated data for
the task. Recently, self-supervised techniques exploiting intrinsic properties
of pitch and harmonic signals have shown promise for both monophonic and
polyphonic pitch estimation, but these still remain inferior to supervised
methods. In this work, we extend the classic supervised MPE paradigm by
incorporating several self-supervised objectives based on pitch-invariant and
pitch-equivariant properties. This joint training results in a substantial
improvement under closed training conditions, which naturally suggests that
applying the same objectives to a broader collection of data will yield further
improvements. However, in doing so we uncover a phenomenon whereby our model
simultaneously overfits to the supervised data while degenerating on data used
for self-supervision only. We demonstrate and investigate this and offer our
insights on the underlying problem.

</details>


### [808] [Human-CLAP: Human-perception-based contrastive language-audio pretraining](https://arxiv.org/pdf/2506.23553)
*Taisei Takano, Yuki Okamoto, Yusuke Kanamori, Yuki Saito, Ryotaro Nagase, Hiroshi Saruwatari*

Main category: eess.AS

TL;DR: The paper reveals that CLAPScore, a metric for audio-text relevance, poorly correlates with human subjective scores. It introduces Human-CLAP, improving correlation by over 0.25.


<details>
  <summary>Details</summary>
Motivation: To address the unclear relationship between CLAPScore and human subjective evaluations in audio-text tasks.

Method: Proposes Human-CLAP by training a contrastive language-audio model using subjective scores.

Result: Human-CLAP improves SRCC by more than 0.25 compared to conventional CLAP.

Conclusion: Human-CLAP better aligns with human perception, enhancing evaluation metrics for audio-text tasks.

Abstract: Contrastive language-audio pretraining (CLAP) is widely used for audio
generation and recognition tasks. For example, CLAPScore, which utilizes the
similarity of CLAP embeddings, has been a major metric for the evaluation of
the relevance between audio and text in text-to-audio. However, the
relationship between CLAPScore and human subjective evaluation scores is still
unclarified. We show that CLAPScore has a low correlation with human subjective
evaluation scores. Additionally, we propose a human-perception-based CLAP
called Human-CLAP by training a contrastive language-audio model using the
subjective evaluation score. In our experiments, the results indicate that our
Human-CLAP improved the Spearman's rank correlation coefficient (SRCC) between
the CLAPScore and the subjective evaluation scores by more than 0.25 compared
with the conventional CLAP.

</details>


### [809] [Less is More: Data Curation Matters in Scaling Speech Enhancement](https://arxiv.org/pdf/2506.23859)
*Chenda Li, Wangyou Zhang, Wei Wang, Robin Scheibler, Kohei Saijo, Samuele Cornell, Yihui Fu, Marvin Sach, Zhaoheng Ni, Anurag Kumar, Tim Fingscheidt, Shinji Watanabe, Yanmin Qian*

Main category: eess.AS

TL;DR: Prioritizing high-quality training data over volume improves speech enhancement model performance.


<details>
  <summary>Details</summary>
Motivation: To address diminishing returns in scaling speech enhancement data by focusing on quality issues in "clean" training labels.

Method: Re-examining data scaling effects and comparing models trained on a curated 700-hour subset versus a full 2,500-hour dataset.

Result: Models trained on the smaller, high-quality subset outperformed those trained on the larger dataset.

Conclusion: Data curation is crucial for effective scaling of speech enhancement systems.

Abstract: The vast majority of modern speech enhancement systems rely on data-driven
neural network models. Conventionally, larger datasets are presumed to yield
superior model performance, an observation empirically validated across
numerous tasks in other domains. However, recent studies reveal diminishing
returns when scaling speech enhancement data. We focus on a critical factor:
prevalent quality issues in ``clean'' training labels within large-scale
datasets. This work re-examines this phenomenon and demonstrates that, within
large-scale training sets, prioritizing high-quality training data is more
important than merely expanding the data volume. Experimental findings suggest
that models trained on a carefully curated subset of 700 hours can outperform
models trained on the 2,500-hour full dataset. This outcome highlights the
crucial role of data curation in scaling speech enhancement systems
effectively.

</details>


### [810] [URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech Enhancement Competition](https://arxiv.org/pdf/2506.23874)
*Jiahe Wang, Chenda Li, Wei Wang, Wangyou Zhang, Samuele Cornell, Marvin Sach, Robin Scheibler, Kohei Saijo, Yihui Fu, Zhaoheng Ni, Anurag Kumar, Tim Fingscheidt, Shinji Watanabe, Yanmin Qian*

Main category: eess.AS

TL;DR: URGENT-PK is a ranking approach for speech quality assessment using pairwise comparisons, outperforming baselines with limited data.


<details>
  <summary>Details</summary>
Motivation: MOS acquisition is costly, and existing deep learning methods lack sufficient training data. Reliable system comparison is prioritized over absolute scores.

Method: URGENT-PK uses homologous speech pairs to predict relative quality rankings, efficiently leveraging limited data through pairwise permutations.

Result: URGENT-PK achieves superior system-level ranking performance on open test sets despite simple architecture and limited training data.

Conclusion: Pairwise comparisons offer an effective solution for speech quality ranking with limited data, outperforming traditional MOS-based methods.

Abstract: The Mean Opinion Score (MOS) is fundamental to speech quality assessment.
However, its acquisition requires significant human annotation. Although deep
neural network approaches, such as DNSMOS and UTMOS, have been developed to
predict MOS to avoid this issue, they often suffer from insufficient training
data. Recognizing that the comparison of speech enhancement (SE) systems
prioritizes a reliable system comparison over absolute scores, we propose
URGENT-PK, a novel ranking approach leveraging pairwise comparisons. URGENT-PK
takes homologous enhanced speech pairs as input to predict relative quality
rankings. This pairwise paradigm efficiently utilizes limited training data, as
all pairwise permutations of multiple systems constitute a training instance.
Experiments across multiple open test sets demonstrate URGENT-PK's superior
system-level ranking performance over state-of-the-art baselines, despite its
simple network architecture and limited training data.

</details>


### [811] [Identification and Clustering of Unseen Ragas in Indian Art Music](https://arxiv.org/pdf/2411.18611)
*Parampreet Singh, Adwik Gupta, Aakarsh Mishra, Vipul Arora*

Main category: eess.AS

TL;DR: The paper addresses raga classification in Indian Art Music as an open-set problem, using OOD detection and NCD to handle unseen classes.


<details>
  <summary>Details</summary>
Motivation: Traditional methods treat raga classification as a closed-set problem, ignoring unseen classes. This work aims to address this gap.

Method: Combines Uncertainty-based OOD detection for identifying unknown classes and Novel Class Discovery (NCD) with contrastive learning to cluster unseen ragas.

Result: Demonstrates the impact of loss function components on clustering and examines openness effects on NCD performance.

Conclusion: The approach effectively handles unseen raga classes, improving classification in open-set scenarios.

Abstract: Raga classification in Indian Art Music is an open-set problem where unseen
classes may appear during testing. However, traditional approaches often treat
it as a closed set problem, rejecting the possibility of encountering unseen
classes. In this work, we try to tackle this problem by first employing an
Uncertainty-based Out-Of-Distribution (OOD) detection, given a set containing
known and unknown classes. Next, for the audio samples identified as OOD, we
employ Novel Class Discovery (NCD) approach to cluster them into distinct
unseen Raga classes. We achieve this by harnessing information from labelled
data and further applying contrastive learning on unlabelled data. With
thorough analysis, we demonstrate the influence of different components of the
loss function on clustering performance and examine how varying openness
affects the NCD task in hand.

</details>


### [812] [MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous Decoding](https://arxiv.org/pdf/2505.19577)
*Yu Xi, Haoyu Li, Xiaoyu Gu, Yidi Jiang, Kai Yu*

Main category: eess.AS

TL;DR: The paper proposes MFA-KWS, a keyword-specific KWS framework combining CTC-Transducer with multi-head frame-asynchronous decoding, achieving state-of-the-art performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional ASR-based KWS methods lack explicit keyword prioritization, leading to suboptimal performance.

Method: Introduces MFA-KWS with keyword-specific phone-synchronous decoding for CTC and Token-and-Duration Transducer, along with score fusion strategies.

Result: MFA-KWS outperforms baselines on datasets like Snips and LibriKWS-20, with 47-63% speed-up and robustness in noise.

Conclusion: MFA-KWS is an efficient and effective framework, suitable for on-device deployment.

Abstract: Keyword spotting (KWS) is essential for voice-driven applications, demanding
both accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy
and beam search, explore the entire search space without explicitly
prioritizing keyword detection, often leading to suboptimal performance. In
this paper, we propose an effective keyword-specific KWS framework by
introducing a streaming-oriented CTC-Transducer-combined frame-asynchronous
system with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,
MFA-KWS employs keyword-specific phone-synchronous decoding for CTC and
replaces conventional RNN-T with Token-and-Duration Transducer to enhance both
performance and efficiency. Furthermore, we explore various score fusion
strategies, including single-frame-based and consistency-based methods.
Extensive experiments demonstrate the superior performance of MFA-KWS, which
achieves state-of-the-art results on both fixed keyword and arbitrary keywords
datasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting
strong robustness in noisy environments. Among fusion strategies, the
consistency-based CDC-Last method delivers the best performance. Additionally,
MFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines
across various datasets. Extensive experimental results confirm that MFA-KWS is
an effective and efficient KWS framework, making it well-suited for on-device
deployment.

</details>


### [813] [From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data](https://arxiv.org/pdf/2505.20166)
*Chun-Yi Kuan, Hung-yi Lee*

Main category: eess.AS

TL;DR: Proposes BALSa, a framework for training audio-aware LLMs using synthetic data to mitigate audio hallucinations and enhance cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in adapting text-based LLMs to audio tasks, such as catastrophic forgetting and resource-intensive alignment methods.

Method: Introduces a data generation framework for contrastive-like training and extends it to multi-audio scenarios for better alignment.

Result: Effectively reduces audio hallucinations while maintaining strong performance on audio understanding and instruction-following tasks.

Conclusion: BALSa provides an efficient and scalable solution for developing high-performing ALLMs.

Abstract: Audio-aware large language models (ALLMs) have recently made great strides in
understanding and processing audio inputs. These models are typically adapted
from text-based large language models (LLMs) through additional training on
audio-related tasks. However, this adaptation process presents two major
limitations. First, ALLMs often suffer from catastrophic forgetting, where
crucial textual capabilities like instruction-following are lost after training
on audio data. In some cases, models may even hallucinate sounds that are not
present in the input audio, raising concerns about reliability. Second,
achieving cross-modal alignment between audio and language typically relies on
large collections of task-specific question-answer pairs for instruction
tuning, making it resource-intensive. To address these issues, previous works
have leveraged the backbone LLMs to synthesize general-purpose, caption-style
alignment data. In this paper, we propose a data generation framework that
produces contrastive-like training data, designed to enhance ALLMs' ability to
differentiate between present and absent sounds. We further extend our approach
to multi-audio scenarios, enabling the model to either explain differences
between audio inputs or produce unified captions that describe all inputs,
thereby enhancing audio-language alignment. We refer to the entire ALLM
training framework as bootstrapping audio-language alignment via synthetic data
generation from backbone LLMs (BALSa). Experimental results indicate that our
method effectively mitigates audio hallucinations while reliably maintaining
strong performance on audio understanding and reasoning benchmarks, as well as
instruction-following skills. Moreover, incorporating multi-audio training
further enhances the model's comprehension and reasoning capabilities. Overall,
BALSa offers an efficient and scalable approach to developing ALLMs.

</details>


### [814] [CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following](https://arxiv.org/pdf/2506.12285)
*Yinghao Ma, Siyou Li, Juntao Yu, Emmanouil Benetos, Akira Maezawa*

Main category: eess.AS

TL;DR: CMI-Bench is a new benchmark for evaluating audio-text LLMs on diverse music tasks, revealing gaps and biases compared to supervised models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for music understanding in LLMs are limited and don't reflect real-world complexity.

Method: Reinterpret traditional MIR annotations as instruction-following tasks and introduce CMI-Bench with standardized metrics.

Result: LLMs show performance gaps and biases (cultural, chronological, gender) compared to supervised models.

Conclusion: CMI-Bench provides a unified evaluation foundation for advancing music-aware LLMs.

Abstract: Recent advances in audio-text large language models (LLMs) have opened new
possibilities for music understanding and generation. However, existing
benchmarks are limited in scope, often relying on simplified tasks or
multi-choice evaluations that fail to reflect the complexity of real-world
music analysis. We reinterpret a broad range of traditional MIR annotations as
instruction-following formats and introduce CMI-Bench, a comprehensive music
instruction following benchmark designed to evaluate audio-text LLMs on a
diverse set of music information retrieval (MIR) tasks. These include genre
classification, emotion regression, emotion tagging, instrument classification,
pitch estimation, key detection, lyrics transcription, melody extraction, vocal
technique recognition, instrument performance technique detection, music
tagging, music captioning, and (down)beat tracking: reflecting core challenges
in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized
evaluation metrics consistent with previous state-of-the-art MIR models,
ensuring direct comparability with supervised approaches. We provide an
evaluation toolkit supporting all open-source audio-textual LLMs, including
LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant
performance gaps between LLMs and supervised models, along with their culture,
chronological and gender bias, highlighting the potential and limitations of
current models in addressing MIR tasks. CMI-Bench establishes a unified
foundation for evaluating music instruction following, driving progress in
music-aware LLMs.

</details>


### [815] [M3SD: Multi-modal, Multi-scenario and Multi-language Speaker Diarization Dataset](https://arxiv.org/pdf/2506.14427)
*Shilong Wu*

Main category: eess.AS

TL;DR: Proposed an automated method to create speaker diarization datasets using audio-video fusion, releasing the diverse M3SD dataset to address data scarcity and model generalization issues.


<details>
  <summary>Details</summary>
Motivation: Addressing insufficient data and poor generalization in speaker diarization by leveraging multi-modal (audio-video) data for accurate pseudo-labeling.

Method: Combined audio and video to generate accurate pseudo-labels, constructing the M3SD dataset from real network videos.

Result: Released the M3SD dataset, which is diverse and open-sourced, aiding in speaker diarization research.

Conclusion: The M3SD dataset and method provide a solution to data scarcity and generalization challenges in speaker diarization.

Abstract: In the field of speaker diarization, the development of technology is
constrained by two problems: insufficient data resources and poor
generalization ability of deep learning models. To address these two problems,
firstly, we propose an automated method for constructing speaker diarization
datasets, which generates more accurate pseudo-labels for massive data through
the combination of audio and video. Relying on this method, we have released
Multi-modal, Multi-scenario and Multi-language Speaker Diarization (M3SD)
datasets. This dataset is derived from real network videos and is highly
diverse. Our dataset and code have been open-sourced at
https://huggingface.co/spaces/OldDragon/m3sd.

</details>


### [816] [ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing](https://arxiv.org/pdf/2506.21448)
*Huadai Liu, Jialei Wang, Kaicheng Luo, Wen Wang, Qian Chen, Zhou Zhao, Wei Xue*

Main category: eess.AS

TL;DR: ThinkSound is a novel framework using Chain-of-Thought reasoning for high-fidelity video-to-audio generation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Producing authentic audio from videos requires nuanced reasoning about visual dynamics and acoustics, which current methods struggle with.

Method: ThinkSound decomposes audio generation into three stages: foundational foley generation, interactive refinement, and targeted editing, guided by multimodal CoT reasoning.

Result: ThinkSound achieves state-of-the-art performance in audio and CoT metrics and excels in out-of-distribution benchmarks.

Conclusion: ThinkSound advances video-to-audio generation by integrating structured reasoning and user interaction, setting a new benchmark for fidelity and adaptability.

Abstract: While end-to-end video-to-audio generation has greatly improved, producing
high-fidelity audio that authentically captures the nuances of visual content
remains challenging. Like professionals in the creative industries, such
generation requires sophisticated reasoning about items such as visual
dynamics, acoustic environments, and temporal relationships. We present
ThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning
to enable stepwise, interactive audio generation and editing for videos. Our
approach decomposes the process into three complementary stages: foundational
foley generation that creates semantically coherent soundscapes, interactive
object-centric refinement through precise user interactions, and targeted
editing guided by natural language instructions. At each stage, a multimodal
large language model generates contextually aligned CoT reasoning that guides a
unified audio foundation model. Furthermore, we introduce AudioCoT, a
comprehensive dataset with structured reasoning annotations that establishes
connections between visual content, textual descriptions, and sound synthesis.
Experiments demonstrate that ThinkSound achieves state-of-the-art performance
in video-to-audio generation across both audio metrics and CoT metrics and
excels in out-of-distribution Movie Gen Audio benchmark. The demo page is
available at https://ThinkSound-Project.github.io.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [817] [High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning](https://arxiv.org/pdf/2506.22532)
*Mark Wrobel, Michele Pascale, Tina Yao, Ruaraidh Campbell, Elena Milano, Michael Quail, Jennifer Steeden, Vivek Muthurangu*

Main category: eess.IV

TL;DR: DL models transform 2D real-time cine images into 3D cine datasets for pediatric CMR, showing good agreement with conventional methods.


<details>
  <summary>Details</summary>
Motivation: To streamline CMR in pediatric and congenital heart disease by reducing acquisition and processing time while maintaining accuracy.

Method: Four DL models were trained for contrast correction, motion correction, super-resolution, and segmentation, validated on 10 patients.

Result: Successful 3D cine creation in <1 min, with good agreement for ventricular volumes and vessel diameters, except slight RPA overestimation.

Conclusion: DL-based 3D cine creation from 2D images is feasible, fast, and accurate, potentially speeding up clinical CMR.

Abstract: Background: Conventional cardiovascular magnetic resonance (CMR) in
paediatric and congenital heart disease uses 2D, breath-hold, balanced steady
state free precession (bSSFP) cine imaging for assessment of function and
cardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for
anatomical assessment. Our aim is to concatenate a stack 2D free-breathing
real-time cines and use Deep Learning (DL) to create an isotropic a fully
segmented 3D cine dataset from these images. Methods: Four DL models were
trained on open-source data that performed: a) Interslice contrast correction;
b) Interslice respiratory motion correction; c) Super-resolution (slice
direction); and d) Segmentation of right and left atria and ventricles (RA, LA,
RV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients
undergoing routine cardiovascular examination, our method was validated on
prospectively acquired sagittal stacks of real-time cine images. Quantitative
metrics (ventricular volumes and vessel diameters) and image quality of the 3D
cines were compared to conventional breath hold cine and whole heart imaging.
Results: All real-time data were successfully transformed into 3D cines with a
total post-processing time of <1 min in all cases. There were no significant
biases in any LV or RV metrics with reasonable limits of agreement and
correlation. There is also reasonable agreement for all vessel diameters,
although there was a small but significant overestimation of RPA diameter.
Conclusion: We have demonstrated the potential of creating a 3D-cine data from
concatenated 2D real-time cine images using a series of DL models. Our method
has short acquisition and reconstruction times with fully segmented data being
available within 2 minutes. The good agreement with conventional imaging
suggests that our method could help to significantly speed up CMR in clinical
practice.

</details>


### [818] [FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation](https://arxiv.org/pdf/2506.22580)
*Vasilis Siomos, Jonathan Passerat-Palmbach, Giacomo Tarroni*

Main category: eess.IV

TL;DR: FedCLAM improves federated learning in medical imaging by adapting to client-specific conditions and aligning image intensities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Medical imaging faces challenges like device and population diversity, reducing global model effectiveness in federated learning.

Method: FedCLAM uses client-adaptive momentum, personalized dampening, and intensity alignment loss to address heterogeneity.

Result: FedCLAM outperforms eight state-of-the-art methods in medical segmentation tasks.

Conclusion: FedCLAM effectively handles medical imaging heterogeneity, enhancing federated learning performance.

Abstract: Federated learning is a decentralized training approach that keeps data under
stakeholder control while achieving superior performance over isolated
training. While inter-institutional feature discrepancies pose a challenge in
all federated settings, medical imaging is particularly affected due to diverse
imaging devices and population variances, which can diminish the global model's
effectiveness. Existing aggregation methods generally fail to adapt across
varied circumstances. To address this, we propose FedCLAM, which integrates
\textit{client-adaptive momentum} terms derived from each client's loss
reduction during local training, as well as a \textit{personalized dampening
factor} to curb overfitting. We further introduce a novel \textit{intensity
alignment} loss that matches predicted and ground-truth foreground
distributions to handle heterogeneous image intensity profiles across
institutions and devices. Extensive evaluations on two datasets show that
FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,
underscoring its efficacy. The code is available at
https://github.com/siomvas/FedCLAM.

</details>


### [819] [ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge](https://arxiv.org/pdf/2506.22790)
*Yixu Chen, Bowen Chen, Hai Wei, Alan C. Bovik, Baojun Li, Wei Sun, Linhan Cao, Kang Fu, Dandan Zhu, Jun Jia, Menghan Hu, Xiongkuo Min, Guangtao Zhai, Dounia Hammou, Fei Yin, Rafal Mantiuk, Amritha Premkumar, Prajit T Rajendran, Vignesh V Menon*

Main category: eess.IV

TL;DR: The paper discusses a 2025 ICME Grand Challenge focused on developing generalizable VQA methods for HDR and SDR videos, highlighting the need for robust models. Five teams submitted models, with four outperforming the VMAF baseline and one achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of video technology, particularly HDR and SDR, necessitates robust and generalizable VQA methods, as existing models struggle with consistency across dynamic ranges and distortions.

Method: The challenge benchmarked VQA approaches for jointly handling HDR and SDR content. Five teams submitted seven models for FR and NR tracks.

Result: Four methods outperformed the VMAF baseline, with the top model achieving state-of-the-art performance, setting a new benchmark.

Conclusion: The challenge successfully advanced generalizable VQA methods, demonstrating improved performance over existing benchmarks.

Abstract: This paper reports IEEE International Conference on Multimedia \& Expo (ICME)
2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.
With the rapid development of video technology, especially High Dynamic Range
(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and
generalizable Video Quality Assessment (VQA) methods has become increasingly
demanded. Existing VQA models often struggle to deliver consistent performance
across varying dynamic ranges, distortion types, and diverse content. This
challenge was established to benchmark and promote VQA approaches capable of
jointly handling HDR and SDR content. In the final evaluation phase, five teams
submitted seven models along with technical reports to the Full Reference (FR)
and No Reference (NR) tracks. Among them, four methods outperformed VMAF
baseline, while the top-performing model achieved state-of-the-art performance,
setting a new benchmark for generalizable video quality assessment.

</details>


### [820] [Multi-Domain FeFET-Based Pixel for In-Sensor Multiply-and-Accumulate Operations](https://arxiv.org/pdf/2506.22596)
*Md Rahatul Islam Udoy, Wantong Li, Kai Ni, Ahmedullah Aziz*

Main category: eess.IV

TL;DR: An FeFET-based active pixel sensor performs in-sensor MAC operations using multi-domain polarization states, enabling compact, power-efficient edge computing.


<details>
  <summary>Details</summary>
Motivation: To minimize data movement and enable efficient in-sensor computing for real-time applications like neuromorphic vision and secure sensing.

Method: Integrates a programmable FeFET into a 3-transistor pixel circuit, using its non-volatile conductance for weights and photodiode voltage for inputs, generating analog multiplication. Accumulation is done via shared column lines.

Result: HSPICE simulations with 45 nm CMOS models validate the design's operation and scalability.

Conclusion: The compact, power-efficient architecture is ideal for edge computing, neuromorphic vision, and secure sensing.

Abstract: This paper presents an FeFET-based active pixel sensor that performs
in-sensor multiply-and-accumulate (MAC) operations by leveraging the
multi-domain polarization states of ferroelectric layers. The proposed design
integrates a programmable FeFET into a 3-transistor pixel circuit, where the
FeFET's non-volatile conductance encodes the weight, and the photodiode voltage
drop encodes the input. Their interaction generates an output current
proportional to the product, enabling in-pixel analog multiplication.
Accumulation is achieved by summing output currents along shared column lines,
realizing full MAC functionality within the image sensor array. Extensive
HSPICE simulations, using 45 nm CMOS models, validate the operation and confirm
the scalability of the design. This compact and power-efficient architecture
minimizes data movement, making it ideal for real-time edge computing,
neuromorphic vision, and secure sensing applications.

</details>


### [821] [CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation](https://arxiv.org/pdf/2506.22882)
*Qilong Xing, Zikai Song, Yuteng Ye, Yuke Chen, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang*

Main category: eess.IV

TL;DR: Proposes CA-Diff, a framework combining anatomical features with diffusion models for improved brain MRI segmentation, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing CNN and transformer-based methods fail to accurately segment complex brain structures in MRI, and current diffusion models neglect anatomical information.

Method: Integrates spatial anatomical features via distance field as auxiliary condition, collaborative diffusion process, consistency loss, and time-adapted channel attention in U-Net.

Result: CA-Diff achieves superior segmentation accuracy compared to state-of-the-art methods.

Conclusion: The proposed CA-Diff effectively leverages anatomical features for precise brain MRI segmentation, demonstrating significant improvements over existing approaches.

Abstract: Segmentation of brain structures from MRI is crucial for evaluating brain
morphology, yet existing CNN and transformer-based methods struggle to
delineate complex structures accurately. While current diffusion models have
shown promise in image segmentation, they are inadequate when applied directly
to brain MRI due to neglecting anatomical information. To address this, we
propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating
spatial anatomical features to enhance segmentation accuracy of the diffusion
model. Specifically, we introduce distance field as an auxiliary anatomical
condition to provide global spatial context, alongside a collaborative
diffusion process to model its joint distribution with anatomical structures,
enabling effective utilization of anatomical features for segmentation.
Furthermore, we introduce a consistency loss to refine relationships between
the distance field and anatomical structures and design a time adapted channel
attention module to enhance the U-Net feature fusion procedure. Extensive
experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.

</details>


### [822] [Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization](https://arxiv.org/pdf/2506.22952)
*Yanwu Yang, Thomas Wolfers*

Main category: eess.IV

TL;DR: The paper proposes HST, a hierarchical state space-based tokenization network, to quantify brain dynamics and transitions using a refined VQ-VAE, improving metastability representation and performance in fMRI analysis.


<details>
  <summary>Details</summary>
Motivation: Understanding brain dynamics via fMRI is challenging, especially in capturing transitions between functional states. Existing methods lack transition dependencies and stable embeddings.

Method: HST combines a state space-based model with a refined VQ-VAE, incorporating quantization error feedback and clustering for better quantization and metastability.

Result: Validated on two fMRI datasets, HST effectively quantifies hierarchical brain dynamics and shows potential in disease diagnosis and reconstruction.

Conclusion: HST provides a promising framework for analyzing brain metastability and dynamics, enhancing fMRI interpretation.

Abstract: Understanding brain dynamics through functional Magnetic Resonance Imaging
(fMRI) remains a fundamental challenge in neuroscience, particularly in
capturing how the brain transitions between various functional states.
Recently, metastability, which refers to temporarily stable brain states, has
offered a promising paradigm to quantify complex brain signals into
interpretable, discretized representations. In particular, compared to
cluster-based machine learning approaches, tokenization approaches leveraging
vector quantization have shown promise in representation learning with powerful
reconstruction and predictive capabilities. However, most existing methods
ignore brain transition dependencies and lack a quantification of brain
dynamics into representative and stable embeddings. In this study, we propose a
Hierarchical State space-based Tokenization network, termed HST, which
quantizes brain states and transitions in a hierarchical structure based on a
state space-based model. We introduce a refined clustered Vector-Quantization
Variational AutoEncoder (VQ-VAE) that incorporates quantization error feedback
and clustering to improve quantization performance while facilitating
metastability with representative and stable token representations. We validate
our HST on two public fMRI datasets, demonstrating its effectiveness in
quantifying the hierarchical dynamics of the brain and its potential in disease
diagnosis and reconstruction performance. Our method offers a promising
framework for the characterization of brain dynamics, facilitating the analysis
of metastability.

</details>


### [823] [An Image Processing Based Blur Reduction Technique in Smartphone-to-Smartphone Visible Light Communication System](https://arxiv.org/pdf/2506.23002)
*Vaigai Nayaki Yokar, Hoa Le-Minh, Zabih Ghassemlooy, Wai Lok Woo*

Main category: eess.IV

TL;DR: A blur reduction technique for smartphone-to-smartphone visible light communications (S2SVLC) is proposed, improving recognition efficiency and data rate by processing images to reduce blur.


<details>
  <summary>Details</summary>
Motivation: To enhance S2SVLC by reducing blur and minimizing data loss at the receiver end, improving overall system performance.

Method: Converts RGB images to grayscale, applies contrast enhancement, scaling, and binarization to reduce blur. Experiments are conducted under varying conditions (distance, rotation, tilt, illumination) using ASCII and QR codes.

Result: The technique achieves a 96% recovery efficiency at the receiver end under diverse conditions.

Conclusion: The proposed method effectively reduces blur in S2SVLC, significantly improving data recovery and system efficiency.

Abstract: In this paper, we present a blur reduction technique for
smartphone-to-smartphone visible light communications (S2SVLC). The key
technique it to avoid the repeated scanning of the transmitted data and to
lower the amount of data discarded at the receiver end of the S2SVLC system.
This image processing method will improve the system recognition efficiency and
data rate. The proposed method includes converting the red-green-blue (RGB)
image into grayscale, applying contrast enhancement, scaling and binarizing the
image to reduce the blur levels in the image. The experiment includes practical
data acquisition and further processing and estimation in MATLAB. The
experiment is carried out in different conditions like distance, rotation, and
tilt also considering different surrounding illuminations like ambient light
and no light conditions to estimate the blur levels in S2SVLC. In this
experimental investigation two types of coding, American Standard code for
information interchange (ASCII), and quick response (QR) code are used for data
transmission in S2SVLC. The obtained results indicate that, the proposed
technique is proven to improve the recovery efficiency to 96% in the receiver
end at different conditions.

</details>


### [824] [Channel characterization in screen-to-camera based optical camera communication](https://arxiv.org/pdf/2506.23005)
*Vaigai Nayaki Yokar, Hoa Le Minh, Zabih Ghassemlooy, Wai Lok Woo*

Main category: eess.IV

TL;DR: The paper demonstrates a smartphone-to-smartphone visible light communication (S2SVLC) system using VLC technology, analyzing screen Lambertian order and channel characterization over a 20 cm link.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of screen-to-camera communication in visible light communication (VLC) systems, leveraging smartphone technology.

Method: Experimental demonstration of S2SVLC using smartphone screens and cameras, analyzing Lambertian order and channel characterization under specific conditions.

Result: Successful implementation and analysis of a 20 cm S2SVLC link, providing insights into screen-to-camera VLC performance.

Conclusion: The study validates the feasibility of S2SVLC systems, highlighting their potential for practical applications in VLC.

Abstract: With the increase in optical camera communication (OCC), a screen to
camera-based communication can be established. This opens a new field of
visible light communication (VLC) known as smartphone to smartphone based
visible light communication (S2SVLC) system. In this paper, we experimentally
demonstrate a S2SVLC system based on VLC technology using a smartphone screen
and a smartphone camera over a link span of 20 cms. We analyze the Lambertian
order of the smartphone screen and carry out a channel characterization of a
screen to camera link-based VLC system under specific test conditions.

</details>


### [825] [MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation](https://arxiv.org/pdf/2506.23102)
*Sunggu Kyung, Jinyoung Seo, Hyunseok Lim, Dongyeong Kim, Hyungbin Park, Jimin Sung, Jihyun Kim, Wooyoung Jo, Yoojin Nam, Namkug Kim*

Main category: eess.IV

TL;DR: MedRegion-CT is a region-focused MLLM framework for CT report generation, improving on global feature methods by capturing region-specific details with innovations like R² Token Pooling, pseudo-masks, and patient-specific attributions. It outperforms existing methods in quality and relevance.


<details>
  <summary>Details</summary>
Motivation: Existing CT report generation methods focus on global features, missing region-specific details, which can overlook abnormalities. MedRegion-CT aims to address this gap.

Method: Proposes MedRegion-CT with three innovations: R² Token Pooling for 3D CT feature extraction, pseudo-masks for region-centric features, and patient-specific attributions converted to text prompts.

Result: Achieves state-of-the-art performance on RadGenome-Chest CT, excelling in natural language generation quality and clinical relevance.

Conclusion: MedRegion-CT effectively addresses the limitations of global feature methods, enhancing report generation with region-specific focus and interpretability.

Abstract: The recent release of RadGenome-Chest CT has significantly advanced CT-based
report generation. However, existing methods primarily focus on global
features, making it challenging to capture region-specific details, which may
cause certain abnormalities to go unnoticed. To address this, we propose
MedRegion-CT, a region-focused Multi-Modal Large Language Model (MLLM)
framework, featuring three key innovations. First, we introduce Region
Representative ($R^2$) Token Pooling, which utilizes a 2D-wise pretrained
vision model to efficiently extract 3D CT features. This approach generates
global tokens representing overall slice features and region tokens
highlighting target areas, enabling the MLLM to process comprehensive
information effectively. Second, a universal segmentation model generates
pseudo-masks, which are then processed by a mask encoder to extract
region-centric features. This allows the MLLM to focus on clinically relevant
regions, using six predefined region masks. Third, we leverage segmentation
results to extract patient-specific attributions, including organ size,
diameter, and locations. These are converted into text prompts, enriching the
MLLM's understanding of patient-specific contexts. To ensure rigorous
evaluation, we conducted benchmark experiments on report generation using the
RadGenome-Chest CT. MedRegion-CT achieved state-of-the-art performance,
outperforming existing methods in natural language generation quality and
clinical relevance while maintaining interpretability. The code for our
framework is publicly available.

</details>


### [826] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/pdf/2506.23121)
*Xinlei Yu, Chanmiao Wang, Hui Jin, Ahmed Elazab, Gangyong Jia, Xiang Wan, Changqing Zou, Ruiquan Ge*

Main category: eess.IV

TL;DR: CRISP-SAM2 is a novel multi-organ medical segmentation model using cross-modal interaction and semantic prompting to improve accuracy and reduce reliance on geometric prompts.


<details>
  <summary>Details</summary>
Motivation: Current multi-organ segmentation models often lack detail accuracy, depend on geometric prompts, and lose spatial information, limiting their effectiveness.

Method: The model converts visual and textual inputs into cross-modal semantics, uses semantic prompting, and incorporates memory self-updating and mask-refining strategies.

Result: CRISP-SAM2 outperforms existing models on seven public datasets, demonstrating superior performance in detail accuracy and adaptability.

Conclusion: CRISP-SAM2 effectively addresses key limitations in multi-organ segmentation, offering a promising solution for medical image processing.

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [827] [Score-based Diffusion Model for Unpaired Virtual Histology Staining](https://arxiv.org/pdf/2506.23184)
*Anran Liu, Xiaofei Wang, Jing Cai, Chao Li*

Main category: eess.IV

TL;DR: A mutual-information-guided diffusion model is proposed for virtual staining of H&E images to IHC, addressing challenges like style decomposition, controllable staining, and structural consistency.


<details>
  <summary>Details</summary>
Motivation: H&E staining lacks specificity for diagnostic markers, while IHC is limited by tissue and antibody constraints. Virtual staining offers a computational solution.

Method: The study introduces a mutual-information-guided score-based diffusion model, featuring global and local MI strategies for disentangling staining styles and ensuring structural consistency.

Result: The model outperforms state-of-the-art methods, demonstrating its biomedical potential.

Conclusion: The proposed method effectively addresses key challenges in virtual staining, with plans to open-source the code.

Abstract: Hematoxylin and eosin (H&E) staining visualizes histology but lacks
specificity for diagnostic markers. Immunohistochemistry (IHC) staining
provides protein-targeted staining but is restricted by tissue availability and
antibody specificity. Virtual staining, i.e., computationally translating the
H&E image to its IHC counterpart while preserving the tissue structure, is
promising for efficient IHC generation. Existing virtual staining methods still
face key challenges: 1) effective decomposition of staining style and tissue
structure, 2) controllable staining process adaptable to diverse tissue and
proteins, and 3) rigorous structural consistency modelling to handle the
non-pixel-aligned nature of paired H&E and IHC images. This study proposes a
mutual-information (MI)-guided score-based diffusion model for unpaired virtual
staining. Specifically, we design 1) a global MI-guided energy function that
disentangles the tissue structure and staining characteristics across
modalities, 2) a novel timestep-customized reverse diffusion process for
precise control of the staining intensity and structural reconstruction, and 3)
a local MI-driven contrastive learning strategy to ensure the cellular level
structural consistency between H&E-IHC images. Extensive experiments
demonstrate the our superiority over state-of-the-art approaches, highlighting
its biomedical potential. Codes will be open-sourced upon acceptance.

</details>


### [828] [Multi-Source COVID-19 Detection via Variance Risk Extrapolation](https://arxiv.org/pdf/2506.23208)
*Runtian Yuan, Qingqiu Li, Junlin Hou, Jilan Xu, Yuejie Zhang, Rui Feng, Hao Chen*

Main category: eess.IV

TL;DR: A method combining Variance Risk Extrapolation (VREx) and Mixup data augmentation improves COVID-19 detection in chest CT scans across diverse hospital datasets, achieving high generalization with a 0.96 F1 score.


<details>
  <summary>Details</summary>
Motivation: The challenge is to classify COVID-19 in CT scans across varied hospital datasets, addressing domain shifts from differing imaging protocols and patient populations.

Method: VREx minimizes risk variance across domains to learn invariant features, while Mixup enhances robustness via input-label interpolation.

Result: Achieves 0.96 macro F1 score on validation, showing strong cross-domain generalization.

Conclusion: VREx and Mixup effectively address domain shifts, improving COVID-19 detection accuracy across diverse datasets.

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which aims to classify chest CT scans into COVID and Non-COVID categories
across data collected from four distinct hospitals and medical centers. A major
challenge in this task lies in the domain shift caused by variations in imaging
protocols, scanners, and patient populations across institutions. To enhance
the cross-domain generalization of our model, we incorporate Variance Risk
Extrapolation (VREx) into the training process. VREx encourages the model to
maintain consistent performance across multiple source domains by explicitly
minimizing the variance of empirical risks across environments. This
regularization strategy reduces overfitting to center-specific features and
promotes learning of domain-invariant representations. We further apply Mixup
data augmentation to improve generalization and robustness. Mixup interpolates
both the inputs and labels of randomly selected pairs of training samples,
encouraging the model to behave linearly between examples and enhancing its
resilience to noise and limited data. Our method achieves an average macro F1
score of 0.96 across the four sources on the validation set, demonstrating
strong generalization.

</details>


### [829] [Improving Myocardial Infarction Detection via Synthetic ECG Pretraining](https://arxiv.org/pdf/2506.23259)
*Lachin Naghashyar*

Main category: eess.IV

TL;DR: A physiology-aware pipeline synthesizes realistic 12-lead ECGs for MI detection, improving deep learning model performance in low-data settings.


<details>
  <summary>Details</summary>
Motivation: Accurate early diagnosis of myocardial infarction (MI) from ECGs is critical, but labeled data scarcity limits deep learning models.

Method: Proposes a pipeline to synthesize tunable MI ECGs with realistic noise and pre-trains classifiers using self-supervised masked-autoencoding and joint reconstruction-classification.

Result: Synthetic ECGs preserved key features, and pretraining improved classification performance (AUC gains up to 4 percentage points), especially in low-data scenarios.

Conclusion: Controlled synthetic ECGs enhance MI detection when real clinical data is limited.

Abstract: Myocardial infarction is a major cause of death globally, and accurate early
diagnosis from electrocardiograms (ECGs) remains a clinical priority. Deep
learning models have shown promise for automated ECG interpretation, but
require large amounts of labeled data, which are often scarce in practice. We
propose a physiology-aware pipeline that (i) synthesizes 12-lead ECGs with
tunable MI morphology and realistic noise, and (ii) pre-trains recurrent and
transformer classifiers with self-supervised masked-autoencoding plus a joint
reconstruction-classification objective. We validate the realism of synthetic
ECGs via statistical and visual analysis, confirming that key morphological
features are preserved. Pretraining on synthetic data consistently improved
classification performance, particularly in low-data settings, with AUC gains
of up to 4 percentage points. These results show that controlled synthetic ECGs
can help improve MI detection when real clinical data is limited.

</details>


### [830] [Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification](https://arxiv.org/pdf/2506.23298)
*Xing Shen, Justin Szeto, Mingyang Li, Hengguan Huang, Tal Arbel*

Main category: eess.IV

TL;DR: CALIN, an inference-time calibration method, addresses calibration biases and demographic unfairness in MLLMs for medical image classification, improving accuracy and fairness.


<details>
  <summary>Details</summary>
Motivation: To ensure safe deployment of MLLMs in clinical practice by analyzing and mitigating calibration errors and biases across demographic subgroups.

Method: CALIN uses a bi-level procedure to estimate calibration matrices (population to subgroup level) and applies them during inference to calibrate confidence scores.

Result: CALIN improves prediction accuracy and fairness on three medical imaging datasets (PAPILA, HAM10000, MIMIC-CXR) with minimal fairness-utility trade-off.

Conclusion: CALIN effectively mitigates biases in MLLMs, ensuring fair and accurate predictions for medical image classification.

Abstract: Multimodal large language models (MLLMs) have enormous potential to perform
few-shot in-context learning in the context of medical image analysis. However,
safe deployment of these models into real-world clinical practice requires an
in-depth analysis of the accuracies of their predictions, and their associated
calibration errors, particularly across different demographic subgroups. In
this work, we present the first investigation into the calibration biases and
demographic unfairness of MLLMs' predictions and confidence scores in few-shot
in-context learning for medical image classification. We introduce CALIN, an
inference-time calibration method designed to mitigate the associated biases.
Specifically, CALIN estimates the amount of calibration needed, represented by
calibration matrices, using a bi-level procedure: progressing from the
population level to the subgroup level prior to inference. It then applies this
estimation to calibrate the predicted confidence scores during inference.
Experimental results on three medical imaging datasets: PAPILA for fundus image
classification, HAM10000 for skin cancer classification, and MIMIC-CXR for
chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair
confidence calibration in its prediction, while improving its overall
prediction accuracies and exhibiting minimum fairness-utility trade-off.

</details>


### [831] [BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia](https://arxiv.org/pdf/2506.23305)
*Rachit Saluja, Arzu Kovanlikaya, Candace Chien, Lauren Kathryn Blatt, Jeffrey M. Perlman, Stefan Worgall, Mert R. Sabuncu, Jonathan P. Dyke*

Main category: eess.IV

TL;DR: The paper introduces a dataset of 3D MRI scans and semantic segmentations for 40 neonates, mostly with BPD, as a non-invasive alternative to X-ray imaging, aiming to improve diagnosis and research.


<details>
  <summary>Details</summary>
Motivation: To provide a safer, non-invasive diagnostic tool for BPD in preterm neonates, avoiding sedation and radiation exposure while offering detailed lung insights.

Method: Uses high-resolution 3D MRI (StarVIBE series) paired with semantic segmentation algorithms to analyze lung and trachea structures.

Result: A dataset of MRI scans and segmentations for 40 neonates, validated against clinical assessments, is presented, along with baseline models.

Conclusion: The dataset and models support advanced research in neonatal lung imaging, offering a promising alternative to traditional X-ray diagnostics.

Abstract: Bronchopulmonary dysplasia (BPD) is a common complication among preterm
neonates, with portable X-ray imaging serving as the standard diagnostic
modality in neonatal intensive care units (NICUs). However, lung magnetic
resonance imaging (MRI) offers a non-invasive alternative that avoids sedation
and radiation while providing detailed insights into the underlying mechanisms
of BPD. Leveraging high-resolution 3D MRI data, advanced image processing and
semantic segmentation algorithms can be developed to assist clinicians in
identifying the etiology of BPD. In this dataset, we present MRI scans paired
with corresponding semantic segmentations of the lungs and trachea for 40
neonates, the majority of whom are diagnosed with BPD. The imaging data consist
of free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as
the StarVIBE series. Additionally, we provide comprehensive clinical data and
baseline segmentation models, validated against clinical assessments, to
support further research and development in neonatal lung imaging.

</details>


### [832] [SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting](https://arxiv.org/pdf/2506.23309)
*Yiming Huang, Long Bai, Beilei Cui, Kun Yuan, Guankun Wang, Mobarakol Islam, Nicolas Padoy, Nassir Navab, Hongliang Ren*

Main category: eess.IV

TL;DR: SurgTPGS introduces a text-promptable Gaussian Splatting method for 3D surgical scene understanding, combining vision-language models and semantic-aware techniques for precise reconstruction and real-time interaction.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D surgical scene comprehension with text-promptable capabilities is vital for surgical planning and real-time guidance, but existing methods lack integration of these features.

Method: The method integrates Segment Anything and vision-language models for 3D semantics feature learning, semantic-aware deformation tracking, and semantic region-aware optimization.

Result: Experiments on surgical datasets show SurgTPGS outperforms state-of-the-art methods in reconstruction quality and semantic understanding.

Conclusion: SurgTPGS enhances surgical precision and safety, paving the way for next-generation intelligent surgical systems.

Abstract: In contemporary surgical research and practice, accurately comprehending 3D
surgical scenes with text-promptable capabilities is particularly crucial for
surgical planning and real-time intra-operative guidance, where precisely
identifying and interacting with surgical tools and anatomical structures is
paramount. However, existing works focus on surgical vision-language model
(VLM), 3D reconstruction, and segmentation separately, lacking support for
real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a
novel text-promptable Gaussian Splatting method to fill this gap. We introduce
a 3D semantics feature learning strategy incorporating the Segment Anything
model and state-of-the-art vision-language models. We extract the segmented
language features for 3D surgical scene reconstruction, enabling a more
in-depth understanding of the complex surgical environment. We also propose
semantic-aware deformation tracking to capture the seamless deformation of
semantic features, providing a more precise reconstruction for both texture and
semantic features. Furthermore, we present semantic region-aware optimization,
which utilizes regional-based semantic information to supervise the training,
particularly promoting the reconstruction quality and semantic smoothness. We
conduct comprehensive experiments on two real-world surgical datasets to
demonstrate the superiority of SurgTPGS over state-of-the-art methods,
highlighting its potential to revolutionize surgical practices. SurgTPGS paves
the way for developing next-generation intelligent surgical systems by
enhancing surgical precision and safety. Our code is available at:
https://github.com/lastbasket/SurgTPGS.

</details>


### [833] [Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction](https://arxiv.org/pdf/2506.23311)
*Perla Mayo, Carolin M. Pirkl, Alin Achim, Bjoern Menze, Mohammad Golbabaee*

Main category: eess.IV

TL;DR: MRF-DiPh is a physics-informed denoising diffusion method for accurate tissue mapping from accelerated MRI, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: To improve multiparametric tissue mapping from highly accelerated MRI by integrating physical constraints and a denoising diffusion prior.

Method: Proximal splitting with a pretrained denoising diffusion model, enforcing k-space consistency and Bloch response adherence.

Result: Outperforms deep learning and compressed sensing baselines, yielding more accurate parameter maps with better fidelity.

Conclusion: MRF-DiPh reliably solves inverse problems in medical imaging by combining physical constraints with advanced priors.

Abstract: We introduce MRF-DiPh, a novel physics informed denoising diffusion approach
for multiparametric tissue mapping from highly accelerated, transient-state
quantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our
method is derived from a proximal splitting formulation, incorporating a
pretrained denoising diffusion model as an effective image prior to regularize
the MRF inverse problem. Further, during reconstruction it simultaneously
enforces two key physical constraints: (1) k-space measurement consistency and
(2) adherence to the Bloch response model. Numerical experiments on in-vivo
brain scans data show that MRF-DiPh outperforms deep learning and compressed
sensing MRF baselines, providing more accurate parameter maps while better
preserving measurement fidelity and physical model consistency-critical for
solving reliably inverse problems in medical imaging.

</details>


### [834] [Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation](https://arxiv.org/pdf/2506.23334)
*Hongyi Pan, Ziliang Hong, Gorkem Durak, Ziyue Xu, Ulas Bagci*

Main category: eess.IV

TL;DR: A generative AI framework improves federated learning for breast cancer diagnosis by augmenting data with synthetic images, enhancing model performance.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) faces challenges like limited data and non-IID distributions, which degrade model performance in medical tasks like breast cancer diagnosis.

Method: Proposes a generative AI framework using class-specific DCGANs (for benign and malignant lesions) to augment data in FL, tested with FedAvg and FedProx on breast ultrasound datasets.

Result: Synthetic images improved AUC scores (FedAvg: 0.9206 to 0.9237; FedProx: 0.9429 to 0.9538), but excessive synthetic data reduced performance.

Conclusion: Generative AI-based data augmentation can enhance FL for medical imaging, but balance between real and synthetic data is crucial.

Abstract: Federated learning (FL) has emerged as a promising paradigm for
collaboratively training deep learning models across institutions without
exchanging sensitive medical data. However, its effectiveness is often hindered
by limited data availability and non-independent, identically distributed data
across participating clients, which can degrade model performance and
generalization. To address these challenges, we propose a generative AI based
data augmentation framework that integrates synthetic image sharing into the
federated training process for breast cancer diagnosis via ultrasound images.
Specifically, we train two simple class-specific Deep Convolutional Generative
Adversarial Networks: one for benign and one for malignant lesions. We then
simulate a realistic FL setting using three publicly available breast
ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are
adopted as baseline FL algorithms. Experimental results show that incorporating
a suitable number of synthetic images improved the average AUC from 0.9206 to
0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that
excessive use of synthetic data reduced performance, underscoring the
importance of maintaining a balanced ratio of real and synthetic samples. Our
findings highlight the potential of generative AI based data augmentation to
enhance FL results in the breast ultrasound image classification task.

</details>


### [835] [FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction](https://arxiv.org/pdf/2506.23466)
*Qiqing Liu, Guoquan Wei, Zekun Zhou, Yiyang Wen, Liu Shi, Qiegen Liu*

Main category: eess.IV

TL;DR: FD-DiT, a frequency domain-directed diffusion transformer, improves LDCT reconstruction by focusing on noise distribution alignment and frequency decoupling, outperforming existing methods in noise and artifact suppression.


<details>
  <summary>Details</summary>
Motivation: LDCT reduces radiation exposure but suffers from noise and artifacts, impacting diagnostic accuracy. Existing methods fail to preserve fine details.

Method: FD-DiT uses a diffusion strategy for noise alignment, frequency decoupling to focus noise in high-frequency domains, a hybrid denoising network, sliding sparse local attention, and dynamic fusion for optimal integration.

Result: FD-DiT achieves superior noise and artifact suppression in LDCT images compared to state-of-the-art methods at identical dose levels.

Conclusion: FD-DiT effectively addresses LDCT reconstruction challenges, enhancing diagnostic accuracy by preserving fine details and suppressing noise.

Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but suffers
from image artifacts and loss of detail due to quantum and electronic noise,
potentially impacting diagnostic accuracy. Transformer combined with diffusion
models has been a promising approach for image generation. Nevertheless,
existing methods exhibit limitations in preserving finegrained image details.
To address this issue, frequency domain-directed diffusion transformer (FD-DiT)
is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy
that progressively introduces noise until the distribution statistically aligns
with that of LDCT data, followed by denoising processing. Furthermore, we
employ a frequency decoupling technique to concentrate noise primarily in
high-frequency domain, thereby facilitating effective capture of essential
anatomical structures and fine details. A hybrid denoising network is then
utilized to optimize the overall data reconstruction process. To enhance the
capability in recognizing high-frequency noise, we incorporate sliding sparse
local attention to leverage the sparsity and locality of shallow-layer
information, propagating them via skip connections for improving feature
representation. Finally, we propose a learnable dynamic fusion strategy for
optimal component integration. Experimental results demonstrate that at
identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior
noise and artifact suppression compared to state-of-the-art methods.

</details>


### [836] [UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound](https://arxiv.org/pdf/2506.23490)
*Junxuan Yu, Yaofei Duan, Yuhao Huang, Yu Wang, Rongbo Ling, Weihao Luo, Ang Zhang, Jingxian Xu, Qiongying Ni, Yongsong Zhou, Binghan Li, Haoran Dou, Liping Liu, Yanfen Chu, Feng Geng, Zhe Sheng, Zhifeng Ding, Dingxin Zhang, Rui Huang, Yuhang Zhang, Xiaowei Xu, Tao Tan, Dong Ni, Zhongshan Gou, Xin Yang*

Main category: eess.IV

TL;DR: UltraTwin is a generative framework for creating cardiac anatomical twins from sparse 2D ultrasound images, addressing challenges like rare paired data and US noise. It uses a coarse-to-fine reconstruction scheme and an implicit autoencoder, validated by a high-quality dataset and outperforming competitors.


<details>
  <summary>Details</summary>
Motivation: 2D ultrasound struggles with 3D cardiac structure accuracy, while 3D ultrasound has limitations like low resolution and availability. A solution is needed for precise treatment planning.

Method: UltraTwin employs a coarse-to-fine hierarchical reconstruction scheme and an implicit autoencoder for topology-aware constraints, using a dataset of paired 2D US and CT images.

Result: UltraTwin reconstructs high-quality anatomical twins, outperforming competitors, as shown in extensive experiments.

Conclusion: UltraTwin advances anatomical twin modeling, offering potential for personalized cardiac care.

Abstract: Echocardiography is routine for cardiac examination. However, 2D ultrasound
(US) struggles with accurate metric calculation and direct observation of 3D
cardiac structures. Moreover, 3D US is limited by low resolution, small field
of view and scarce availability in practice. Constructing the cardiac
anatomical twin from 2D images is promising to provide precise treatment
planning and clinical quantification. However, it remains challenging due to
the rare paired data, complex structures, and US noises. In this study, we
introduce a novel generative framework UltraTwin, to obtain cardiac anatomical
twin from sparse multi-view 2D US. Our contribution is three-fold. First,
pioneered the construction of a real-world and high-quality dataset containing
strictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we
propose a coarse-to-fine scheme to achieve hierarchical reconstruction
optimization. Last, we introduce an implicit autoencoder for topology-aware
constraints. Extensive experiments show that UltraTwin reconstructs
high-quality anatomical twins versus strong competitors. We believe it advances
anatomical twin modeling for potential applications in personalized cardiac
care.

</details>


### [837] [Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI](https://arxiv.org/pdf/2506.23506)
*Bowen Xin, Rohan Hickey, Tamara Blake, Jin Jin, Claire E Wainwright, Thomas Benkert, Alto Stemmer, Peter Sly, David Coman, Jason Dowling*

Main category: eess.IV

TL;DR: The paper introduces APL scoring, an AI-assisted pixel-level method for quantifying lung damage in CF using UTE-MRI, showing faster and more accurate results compared to traditional grid-level scoring.


<details>
  <summary>Details</summary>
Motivation: To address the lack of quantitative scoring systems for structural lung MRI (e.g., UTE-MRI) in diseases like cystic fibrosis (CF), and to provide a faster, more accurate alternative to CT and existing MRI scoring methods.

Method: APL scoring involves 5 stages: image loading, AI lung segmentation, slice sampling, pixel-level annotation, and quantification. It leverages AI for segmentation and pixel-level analysis.

Result: APL scoring was twice as fast (8.2 minutes/subject) and statistically more accurate (p=0.021) than grid-level scoring, with strong correlation (R=0.973, p=5.85e-9).

Conclusion: APL scoring is a promising tool for clinical UTE-MRI workflows, with potential applications in other MRI sequences and lung diseases.

Abstract: Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)
represents a recent breakthrough in lung structure imaging, providing image
resolution and quality comparable to computed tomography (CT). Due to the
absence of ionising radiation, MRI is often preferred over CT in paediatric
diseases such as cystic fibrosis (CF), one of the most common genetic disorders
in Caucasians. To assess structural lung damage in CF imaging, CT scoring
systems provide valuable quantitative insights for disease diagnosis and
progression. However, few quantitative scoring systems are available in
structural lung MRI (e.g., UTE-MRI). To provide fast and accurate
quantification in lung MRI, we investigated the feasibility of novel Artificial
intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring
consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)
lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification
and reporting. The results shows that our APL scoring took 8.2 minutes per
subject, which was more than twice as fast as the previous grid-level scoring.
Additionally, our pixel-level scoring was statistically more accurate
(p=0.021), while strongly correlating with grid-level scoring (R=0.973,
p=5.85e-9). This tool has great potential to streamline the workflow of UTE
lung MRI in clinical settings, and be extended to other structural lung MRI
sequences (e.g., BLADE MRI), and for other lung diseases (e.g.,
bronchopulmonary dysplasia).

</details>


### [838] [AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm](https://arxiv.org/pdf/2506.23537)
*Xinyue Li, Zhangkai Ni, Wenhan Yang*

Main category: eess.IV

TL;DR: AFUNet proposes a deep unfolding network for HDR image reconstruction, decoupling alignment and fusion tasks for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack theoretical foundations, impacting reliability. AFUNet addresses this by integrating MAP estimation and joint constraints.

Method: AFUNet uses alternating refinement with Alignment-Fusion Modules (AFM), combining Spatial Alignment (SAM) and Channel Fusion (CFM) iteratively.

Result: AFUNet outperforms state-of-the-art methods in qualitative and quantitative evaluations.

Conclusion: AFUNet provides a robust, theoretically grounded approach for HDR reconstruction, achieving superior results.

Abstract: Existing learning-based methods effectively reconstruct HDR images from
multi-exposure LDR inputs with extended dynamic range and improved detail, but
they rely more on empirical design rather than theoretical foundation, which
can impact their reliability. To address these limitations, we propose the
cross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR
reconstruction is systematically decoupled into two interleaved subtasks --
alignment and fusion -- optimized through alternating refinement, achieving
synergy between the two subtasks to enhance the overall performance. Our method
formulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP)
estimation perspective, explicitly incorporating spatial correspondence priors
across LDR images and naturally bridging the alignment and fusion subproblems
through joint constraints. Building on the mathematical foundation, we
reimagine traditional iterative optimization through unfolding -- transforming
the conventional solution process into an end-to-end trainable AFUNet with
carefully designed modules that work progressively. Specifically, each
iteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that
alternates between a Spatial Alignment Module (SAM) for alignment and a Channel
Fusion Module (CFM) for adaptive feature fusion, progressively bridging
misaligned content and exposure discrepancies. Extensive qualitative and
quantitative evaluations demonstrate AFUNet's superior performance,
consistently surpassing state-of-the-art methods. Our code is available at:
https://github.com/eezkni/AFUNet

</details>


### [839] [A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation](https://arxiv.org/pdf/2506.23584)
*Renjie Liang, Zhengkang Fan, Jinqian Pan, Chenkun Sun, Russell Terry, Jie Xu*

Main category: eess.IV

TL;DR: A two-stage framework for generating renal radiology reports from 2D CT slices, combining structured abnormality features with a vision-language model, outperforms baselines and shows promise for clinical use.


<details>
  <summary>Details</summary>
Motivation: The complexity of medical imaging and variability in clinical documentation make radiology report generation challenging.

Method: A multi-task learning model extracts structured abnormality features (e.g., lesion attributes), which are combined with CT images and fed into a vision-language model to generate reports.

Result: The model outperforms random baselines, capturing key clinical content with reasonable accuracy.

Conclusion: The framework is feasible for renal imaging, with future work aimed at 3D CT volumes and improved clinical fidelity.

Abstract: Generating radiology reports from CT scans remains a complex task due to the
nuanced nature of medical imaging and the variability in clinical
documentation. In this study, we propose a two-stage framework for generating
renal radiology reports from 2D CT slices. First, we extract structured
abnormality features using a multi-task learning model trained to identify
lesion attributes such as location, size, enhancement, and attenuation. These
extracted features are subsequently combined with the corresponding CT image
and fed into a fine-tuned vision-language model to generate natural language
report sentences aligned with clinical findings. We conduct experiments on a
curated dataset of renal CT studies with manually annotated
sentence-slice-feature triplets and evaluate performance using both
classification metrics and natural language generation metrics. Our results
demonstrate that the proposed model outperforms random baselines across all
abnormality types, and the generated reports capture key clinical content with
reasonable textual accuracy. This exploratory work highlights the feasibility
of modular, feature-informed report generation for renal imaging. Future
efforts will focus on extending this pipeline to 3D CT volumes and further
improving clinical fidelity in multimodal medical AI systems.

</details>


### [840] [Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation](https://arxiv.org/pdf/2506.23664)
*Fangyijie Wang, Kevin Whelan, Félix Balado, Guénolé Silvestre, Kathleen M. Curran*

Main category: eess.IV

TL;DR: A novel mask-guided GenAI approach using diffusion models generates synthetic fetal head ultrasound images with segmentation masks, enhancing real datasets for improved fetal head segmentation.


<details>
  <summary>Details</summary>
Motivation: Medical image data is hard to access due to privacy and labeling constraints, requiring costly expert annotation. Synthetic data generation offers a solution.

Method: Proposes a mask-guided GenAI approach using diffusion models to create synthetic fetal head ultrasound images paired with segmentation masks for dataset augmentation.

Result: Synthetic data effectively captures real features, achieving state-of-the-art fetal head segmentation (Dice Scores of 94.66% and 94.38% for Spanish and African cohorts).

Conclusion: The approach significantly improves segmentation performance with limited real data, demonstrating the potential of synthetic data in medical imaging.

Abstract: Medical image data is less accessible than in other domains due to privacy
and regulatory constraints. In addition, labeling requires costly,
time-intensive manual image annotation by clinical experts. To overcome these
challenges, synthetic medical data generation offers a promising solution.
Generative AI (GenAI), employing generative deep learning models, has proven
effective at producing realistic synthetic images. This study proposes a novel
mask-guided GenAI approach using diffusion models to generate synthetic fetal
head ultrasound images paired with segmentation masks. These synthetic pairs
augment real datasets for supervised fine-tuning of the Segment Anything Model
(SAM). Our results show that the synthetic data captures real image features
effectively, and this approach reaches state-of-the-art fetal head
segmentation, especially when trained with a limited number of real image-mask
pairs. In particular, the segmentation reaches Dice Scores of 94.66\% and
94.38\% using a handful of ultrasound images from the Spanish and African
cohorts, respectively. Our code, models, and data are available on GitHub.

</details>


### [841] [GUSL: A Novel and Efficient Machine Learning Model for Prostate Segmentation on MRI](https://arxiv.org/pdf/2506.23688)
*Jiaxin Yang, Vasileios Magoulianitis, Catherine Aurelia Christie Alexander, Jintang Xue, Masatomo Kaneko, Giovanni Cacciamani, Andre Abreu, Vinay Duddalwar, C. -C. Jay Kuo, Inderbir S. Gill, Chrysostomos Nikias*

Main category: eess.IV

TL;DR: GUSL is a lightweight, interpretable machine learning model for prostate segmentation, outperforming DL-based methods with high efficiency and transparency.


<details>
  <summary>Details</summary>
Motivation: Address the 'black-box' nature of deep learning in medical image segmentation by proposing an interpretable, energy-efficient alternative.

Method: Introduces Green U-shaped Learning (GUSL), a feed-forward model with multi-layer regression, linear feature extraction, and attention on prostate boundaries. Uses a two-step pipeline to handle class imbalance.

Result: Achieves state-of-the-art performance (DSC > 0.9) on gland segmentation across datasets, with smaller model size and lower complexity than DL-based solutions.

Conclusion: GUSL offers a practical, competitive solution for medical imaging due to its interpretability, efficiency, and high performance.

Abstract: Prostate and zonal segmentation is a crucial step for clinical diagnosis of
prostate cancer (PCa). Computer-aided diagnosis tools for prostate segmentation
are based on the deep learning (DL) paradigm. However, deep neural networks are
perceived as "black-box" solutions by physicians, thus making them less
practical for deployment in the clinical setting. In this paper, we introduce a
feed-forward machine learning model, named Green U-shaped Learning (GUSL),
suitable for medical image segmentation without backpropagation. GUSL
introduces a multi-layer regression scheme for coarse-to-fine segmentation. Its
feature extraction is based on a linear model, which enables seamless
interpretability during feature extraction. Also, GUSL introduces a mechanism
for attention on the prostate boundaries, which is an error-prone region, by
employing regression to refine the predictions through residue correction. In
addition, a two-step pipeline approach is used to mitigate the class imbalance,
an issue inherent in medical imaging problems. After conducting experiments on
two publicly available datasets and one private dataset, in both prostate gland
and zonal segmentation tasks, GUSL achieves state-of-the-art performance among
other DL-based models. Notably, GUSL features a very energy-efficient pipeline,
since it has a model size several times smaller and less complexity than the
rest of the solutions. In all datasets, GUSL achieved a Dice Similarity
Coefficient (DSC) performance greater than $0.9$ for gland segmentation.
Considering also its lightweight model size and transparency in feature
extraction, it offers a competitive and practical package for medical imaging
applications.

</details>


### [842] [MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation](https://arxiv.org/pdf/2506.23700)
*Peiting Tian, Xi Chen, Haixia Bi, Fan Li*

Main category: eess.IV

TL;DR: MedSAM-CA is a fine-tuning approach for medical image segmentation, reducing reliance on large annotated datasets and improving boundary delineation with CBR-Net and Atte-FFB.


<details>
  <summary>Details</summary>
Motivation: Address challenges in medical image segmentation, including limited annotated data and poor boundary clarity in low-contrast or blurry images.

Method: Proposes MedSAM-CA with CBR-Net for boundary refinement and Atte-FFB for feature fusion, leveraging the pretrained MedSAM model.

Result: Achieves 94.43% Dice on dermoscopy data with only 2% training data, nearing full-data performance (97.25%).

Conclusion: MedSAM-CA effectively improves segmentation accuracy in low-resource settings, validated across multiple imaging modalities.

Abstract: Medical image segmentation plays a crucial role in clinical diagnosis and
treatment planning, where accurate boundary delineation is essential for
precise lesion localization, organ identification, and quantitative assessment.
In recent years, deep learning-based methods have significantly advanced
segmentation accuracy. However, two major challenges remain. First, the
performance of these methods heavily relies on large-scale annotated datasets,
which are often difficult to obtain in medical scenarios due to privacy
concerns and high annotation costs. Second, clinically challenging scenarios,
such as low contrast in certain imaging modalities and blurry lesion boundaries
caused by malignancy, still pose obstacles to precise segmentation. To address
these challenges, we propose MedSAM-CA, an architecture-level fine-tuning
approach that mitigates reliance on extensive manual annotations by adapting
the pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA
introduces two key components: the Convolutional Attention-Enhanced Boundary
Refinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block
(Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover
boundary information potentially overlooked by long-range attention mechanisms,
leveraging hierarchical convolutional processing. Atte-FFB, embedded in the
MedSAM decoder, fuses multi-level fine-grained features from skip connections
in CBR-Net with global representations upsampled within the decoder to enhance
boundary delineation accuracy. Experiments on publicly available datasets
covering dermoscopy, CT, and MRI imaging modalities validate the effectiveness
of MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only
2% of full training data, reaching 97.25% of full-data training performance,
demonstrating strong effectiveness in low-resource clinical settings.

</details>


### [843] [MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction](https://arxiv.org/pdf/2506.23701)
*Lingtong Zhang, Mengdie Song, Xiaohan Hao, Huayu Mai, Bensheng Qiu*

Main category: eess.IV

TL;DR: The paper proposes Multi-domain Diffusion Prior Guidance (MDPG) using latent diffusion models (LDMs) to improve MRI reconstruction by enhancing data consistency through multi-domain priors and a novel fusion strategy.


<details>
  <summary>Details</summary>
Motivation: MRI reconstruction is critical for diagnostics, but diffusion models struggle with high-fidelity images due to stochasticity. LDMs offer compact prior knowledge in latent domains, which can guide better learning of data distribution.

Method: The method includes a Visual-Mamba-based backbone for encoding/reconstruction, pre-trained LDMs for conditional priors, Latent Guided Attention (LGA) for fusion, and Dual-domain Fusion Branch (DFB) for k-space and image domain fusion. A k-space regularization strategy is also introduced.

Result: Extensive experiments on public MRI datasets validate the effectiveness of MDPG in enhancing MRI reconstruction.

Conclusion: MDPG successfully leverages LDMs and multi-domain fusion to improve MRI reconstruction, demonstrating superior performance in experiments.

Abstract: Magnetic Resonance Imaging (MRI) reconstruction is essential in medical
diagnostics. As the latest generative models, diffusion models (DMs) have
struggled to produce high-fidelity images due to their stochastic nature in
image domains. Latent diffusion models (LDMs) yield both compact and detailed
prior knowledge in latent domains, which could effectively guide the model
towards more effective learning of the original data distribution. Inspired by
this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by
pre-trained LDMs to enhance data consistency in MRI reconstruction tasks.
Specifically, we first construct a Visual-Mamba-based backbone, which enables
efficient encoding and reconstruction of under-sampled images. Then pre-trained
LDMs are integrated to provide conditional priors in both latent and image
domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion
in multi-level latent domains. Simultaneously, to effectively utilize a prior
in both the k-space and image domain, under-sampled images are fused with
generated full-sampled images by the Dual-domain Fusion Branch (DFB) for
self-adaption guidance. Lastly, to further enhance the data consistency, we
propose a k-space regularization strategy based on the non-auto-calibration
signal (NACS) set. Extensive experiments on two public MRI datasets fully
demonstrate the effectiveness of the proposed methodology. The code is
available at https://github.com/Zolento/MDPG.

</details>


### [844] [Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound](https://arxiv.org/pdf/2506.23721)
*Gijs Luijten, Roberto Maria Scardigno, Lisle Faray de Paiva, Peter Hoyer, Jens Kleesiek, Domenico Buongiorno, Vitoantonio Bevilacqua, Jan Egger*

Main category: eess.IV

TL;DR: The paper proposes integrating deep learning (DL) for real-time kidney segmentation in ultrasound (US) and augmented reality (AR) to improve usability. Two AR-DL-assisted pipelines on HoloLens-2 are introduced, evaluated for feasibility and accuracy, and made open-source.


<details>
  <summary>Details</summary>
Motivation: Ultrasound has a steep learning curve and requires focus shifts between screen and patient. Manual kidney volumetric measurements are time-consuming and prone to fatigue.

Method: DL-based semantic segmentation for real-time kidney measurements and AR to project US displays into the clinician's view. Two HoloLens-2 pipelines are proposed: one wireless and one for any US device.

Result: The pipelines are evaluated using the Open Kidney Dataset and open-source models (nnU-Net, Segmenter, YOLO with MedSAM/LiteMedSAM). An open-source GitHub pipeline is provided.

Conclusion: The AR-DL integration enhances US usability, reduces cognitive load, and improves training and diagnostics, especially in point-of-care settings.

Abstract: Ultrasound (US) is widely accessible and radiation-free but has a steep
learning curve due to its dynamic nature and non-standard imaging planes.
Additionally, the constant need to shift focus between the US screen and the
patient poses a challenge. To address these issues, we integrate deep learning
(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric
measurements, which are essential for clinical assessment but are traditionally
time-consuming and prone to fatigue. This automation allows clinicians to
concentrate on image interpretation rather than manual measurements.
Complementing DL, augmented reality (AR) enhances the usability of US by
projecting the display directly into the clinician's field of view, improving
ergonomics and reducing the cognitive load associated with screen-to-patient
transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one
streams directly via the application programming interface for a wireless
setup, while the other supports any US device with video output for broader
accessibility. We evaluate RT feasibility and accuracy using the Open Kidney
Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with
MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model
implementations, measurement algorithms, and a Wi-Fi-based streaming solution,
enhancing US training and diagnostics, especially in point-of-care settings.

</details>


### [845] [Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos](https://arxiv.org/pdf/2506.23759)
*Zheng Fang, Xiaoming Qi, Chun-Mei Feng, Jialun Pei, Weixin Si, Yueming Jin*

Main category: eess.IV

TL;DR: The paper proposes FedST, a personalized FL scheme for surgical instrument segmentation, leveraging domain knowledge to improve local and global training.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods overlook surgical domain characteristics like diverse backgrounds and synthetic data potential.

Method: FedST uses RSC for local training (decoupling query embeddings) and SERQ for global training (synthetic data-based representation quantification).

Result: The method enhances segmentation by capturing consistent instrument representations and adapting to site-specific features.

Conclusion: FedST effectively integrates surgical domain knowledge into FL, improving model generalization and segmentation performance.

Abstract: Surgical instrument segmentation under Federated Learning (FL) is a promising
direction, which enables multiple surgical sites to collaboratively train the
model without centralizing datasets. However, there exist very limited FL works
in surgical data science, and FL methods for other modalities do not consider
inherent characteristics in surgical domain: i) different scenarios show
diverse anatomical backgrounds while highly similar instrument representation;
ii) there exist surgical simulators which promote large-scale synthetic data
generation with minimal efforts. In this paper, we propose a novel Personalized
FL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST),
which wisely leverages surgical domain knowledge during both local-site and
global-server training to boost segmentation. Concretely, our model embraces a
Representation Separation and Cooperation (RSC) mechanism in local-site
training, which decouples the query embedding layer to be trained privately, to
encode respective backgrounds. Meanwhile, other parameters are optimized
globally to capture the consistent representations of instruments, including
the temporal layer to capture similar motion patterns. A textual-guided channel
selection is further designed to highlight site-specific features, facilitating
model adapta tion to each site. Moreover, in global-server training, we propose
Synthesis-based Explicit Representation Quantification (SERQ), which defines an
explicit representation target based on synthetic data to synchronize the model
convergence during fusion for improving model generalization.

</details>


### [846] [ShapeKit](https://arxiv.org/pdf/2506.24003)
*Junqi Liu, Dongli He, Wenxuan Li, Ningyu Wang, Alan L. Yuille, Zongwei Zhou*

Main category: eess.IV

TL;DR: ShapeKit, a shape-focused toolkit, improves whole-body medical segmentation accuracy by over 8% without model re-training, outperforming architecture modifications.


<details>
  <summary>Details</summary>
Motivation: The observation that shape-focused tools can significantly enhance segmentation performance more than model architecture changes.

Method: Introduces ShapeKit, a flexible toolkit for refining anatomical shapes in medical segmentation.

Result: ShapeKit boosts segmentation accuracy by over 8%, surpassing the marginal gains (under 3%) from model architecture changes.

Conclusion: Shape-based tools like ShapeKit are undervalued and can greatly impact the medical segmentation community.

Abstract: In this paper, we present a practical approach to improve anatomical shape
accuracy in whole-body medical segmentation. Our analysis shows that a
shape-focused toolkit can enhance segmentation performance by over 8%, without
the need for model re-training or fine-tuning. In comparison, modifications to
model architecture typically lead to marginal gains of less than 3%. Motivated
by this observation, we introduce ShapeKit, a flexible and easy-to-integrate
toolkit designed to refine anatomical shapes. This work highlights the
underappreciated value of shape-based tools and calls attention to their
potential impact within the medical segmentation community.

</details>


### [847] [Simultaneous Super-Resolution of Spatial and Spectral Imaging with a Camera Array and Notch Filters](https://arxiv.org/pdf/2506.24014)
*Peng Lin, Xuesong Wang, Yating Chen, Xianyu Wu, Feng Huang, Shouqian Chen*

Main category: eess.IV

TL;DR: Proposes an algorithm using a notch filter camera array for super-resolution imaging and spectral reconstruction, achieving higher resolution and faster processing than existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance spatial resolution and multispectral imaging capabilities by integrating multi-aperture super-resolution, pan-sharpening, and spectral reconstruction techniques.

Method: Utilizes sub-pixel offset and spectral disparities from 9 low-resolution images to reconstruct 31 super-resolution spectral images. Validated via simulations and comparisons with snapshot coded aperture systems.

Result: Achieved a peak signal-to-noise ratio of 35.6dB, 5dB better than advanced systems, with reduced processing time.

Conclusion: Provides an effective solution for high temporal, spectral, and spatial resolution using multi-aperture imaging systems.

Abstract: This study proposes an algorithm based on a notch filter camera array system
for simultaneous super-resolution imaging and spectral reconstruction,
enhancing the spatial resolution and multispectral imaging capabilities of
targets. In this study, multi-aperture super-resolution algorithms,
pan-sharpening techniques, and spectral reconstruction algorithms were
investigated and integrated. The sub-pixel level offset information and
spectral disparities among the 9 low-resolution images captured by the 9
distinct imaging apertures were utilized, leading to the successful
reconstruction of 31 super-resolution spectral images. By conducting
simulations with a publicly available dataset and performing qualitative and
quantitative comparisons with snapshot coded aperture spectral imaging systems,
the experimental results demonstrate that our system and algorithm attained a
peak signal-to-noise ratio of 35.6dB, representing a 5dB enhancement over the
most advanced snapshot coded aperture spectral imaging systems, while also
reducing processing time. This research offers an effective solution for
achieving high temporal, spectral, and spatial resolution through the
utilization of multi-aperture imaging systems.

</details>


### [848] [C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism](https://arxiv.org/pdf/2506.24074)
*Mayank V. Golhar, Lucas Sebastian Galeano Fretes, Loren Ayers, Venkata S. Akshintala, Taylor L. Bobrow, Nicholas J. Durr*

Main category: eess.IV

TL;DR: C3VDv2 is an enhanced 3D colonoscopy dataset designed to improve the development and evaluation of 3D reconstruction algorithms by providing realistic and diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: The lack of 3D colonoscopy datasets hinders the development of computer vision techniques for colonoscopy diagnostics.

Method: The dataset includes 192 video sequences from silicone colon phantoms, with ground truth data like depth, surface normals, and 3D models. It also features simulated colonoscopy videos and deformation cases.

Result: C3VDv2 offers diverse and challenging scenarios (e.g., fecal debris, fast camera motion) for robust algorithm testing.

Conclusion: The dataset's enhanced realism supports more accurate development and evaluation of 3D reconstruction algorithms for colonoscopy.

Abstract: Computer vision techniques have the potential to improve the diagnostic
performance of colonoscopy, but the lack of 3D colonoscopy datasets for
training and validation hinders their development. This paper introduces
C3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video
Dataset, featuring enhanced realism designed to facilitate the quantitative
evaluation of 3D colon reconstruction algorithms. 192 video sequences were
captured by imaging 60 unique, high-fidelity silicone colon phantom segments.
Ground truth depth, surface normals, optical flow, occlusion,
six-degree-of-freedom pose, coverage maps, and 3D models are provided for 169
colonoscopy videos. Eight simulated screening colonoscopy videos acquired by a
gastroenterologist are provided with ground truth poses. The dataset includes
15 videos featuring colon deformations for qualitative assessment. C3VDv2
emulates diverse and challenging scenarios for 3D reconstruction algorithms,
including fecal debris, mucous pools, blood, debris obscuring the colonoscope
lens, en-face views, and fast camera motion. The enhanced realism of C3VDv2
will allow for more robust and representative development and evaluation of 3D
reconstruction algorithms.

</details>


### [849] [BraTS-PEDs: Results of the Multi-Consortium International Pediatric Brain Tumor Segmentation Challenge 2023](https://arxiv.org/pdf/2407.08855)
*Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan Haldar, Zhifan Jiang, Anna Zapaishchykova, Julija Pavaine, Lubdha M. Shah, Blaise V. Jones, Nakul Sheth, Sanjay P. Prabhu, Aaron S. McAllister, Wenxin Tu, Khanak K. Nandolia, Andres F. Rodriguez, Ibraheem Salman Shaikh, Mariana Sanchez Montano, Hollie Anne Lai, Maruf Adewole, Jake Albrecht, Udunna Anazodo, Hannah Anderson, Syed Muhammed Anwar, Alejandro Aristizabal, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, James Eddy, Ivan Ezhov, Ariana M. Familiar, Keyvan Farahani, Deep Gandhi, Anurag Gottipati, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Elaine, Alexandros Karargyris, Hasan Kassem, Neda Khalili, Florian Kofler, Dominic LaBella, Koen Van Leemput, Hongwei B. Li, Nazanin Maleki, Zeke Meier, Bjoern Menze, Ahmed W. Moawad, Sarthak Pati, Marie Piraud, Tina Poussaint, Zachary J. Reitman, Jeffrey D. Rudie, Rachit Saluja, MIcah Sheller, Russell Takeshi Shinohara, Karthik Viswanathan, Chunhao Wang, Benedikt Wiestler, Walter F. Wiggins, Christos Davatzikos, Phillip B. Storm, Miriam Bornhorst, Roger Packer, Trent Hummel, Peter de Blank, Lindsey Hoffman, Mariam Aboian, Ali Nabavizadeh, Jeffrey B. Ware, Benjamin H. Kann, Brian Rood, Adam Resnick, Spyridon Bakas, Arastoo Vossough, Marius George Linguraru*

Main category: eess.IV

TL;DR: The BraTS-PEDs 2023 challenge focused on pediatric brain tumor segmentation, using AI to improve diagnosis and treatment. Top methods included nnU-Net and Swin UNETR.


<details>
  <summary>Details</summary>
Motivation: Pediatric CNS tumors are deadly, with low survival rates. New treatments need accurate, centralized response assessment.

Method: Multi-institutional data was used to evaluate AI segmentation algorithms for pediatric gliomas via standardized metrics.

Result: Top-performing AI methods were nnU-Net, Swin UNETR, and Auto3DSeg. Collaboration between clinicians and AI scientists was enhanced.

Conclusion: The challenge improved data sharing and automated analysis, potentially aiding clinical trials and pediatric brain tumor care.

Abstract: Pediatric central nervous system tumors are the leading cause of
cancer-related deaths in children. The five-year survival rate for high-grade
glioma in children is less than 20%. The development of new treatments is
dependent upon multi-institutional collaborative clinical trials requiring
reproducible and accurate centralized response assessment. We present the
results of the BraTS-PEDs 2023 challenge, the first Brain Tumor Segmentation
(BraTS) challenge focused on pediatric brain tumors. This challenge utilized
data acquired from multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. BraTS-PEDs 2023 aimed to evaluate
volumetric segmentation algorithms for pediatric brain gliomas from magnetic
resonance imaging using standardized quantitative performance evaluation
metrics employed across the BraTS 2023 challenges. The top-performing AI
approaches for pediatric tumor analysis included ensembles of nnU-Net and Swin
UNETR, Auto3DSeg, or nnU-Net with a self-supervised framework. The BraTSPEDs
2023 challenge fostered collaboration between clinicians (neuro-oncologists,
neuroradiologists) and AI/imaging scientists, promoting faster data sharing and
the development of automated volumetric analysis techniques. These advancements
could significantly benefit clinical trials and improve the care of children
with brain tumors.

</details>


### [850] [Iterative approach to reconstructing neural disparity fields from light-field data](https://arxiv.org/pdf/2407.15380)
*Ligen Shi, Chang Liu, Xing Zhao, Jun Qiu*

Main category: eess.IV

TL;DR: The paper introduces a neural disparity field (NDF) for implicit, continuous disparity representation from light-field data, overcoming traditional limitations like sampling errors. It uses hash encoding and MLPs for detailed disparity capture and an iterative, dataset-free optimization scheme.


<details>
  <summary>Details</summary>
Motivation: Traditional disparity maps suffer from sampling errors and interpolation inaccuracies. The study aims to provide a seamless, precise, and resolution-flexible disparity representation using neural fields.

Method: Proposes NDF with hash encoding and MLPs for detailed disparity. Develops a differentiable forward model for light-field data and an iterative optimization scheme for NDF reconstruction without training data.

Result: High-quality NDF can be reconstructed, enabling high-resolution disparity recovery and implicit, continuous representation of scene disparities.

Conclusion: NDF effectively addresses traditional disparity map limitations, offering a robust solution for complex scene disparity representation from light-field data.

Abstract: This study proposes a neural disparity field (NDF) that establishes an
implicit, continuous representation of scene disparity based on a neural field
and an iterative approach to address the inverse problem of NDF reconstruction
from light-field data. NDF enables seamless and precise characterization of
disparity variations in three-dimensional scenes and can discretize disparity
at any arbitrary resolution, overcoming the limitations of traditional
disparity maps that are prone to sampling errors and interpolation
inaccuracies. The proposed NDF network architecture utilizes hash encoding
combined with multilayer perceptrons to capture detailed disparities in texture
levels, thereby enhancing its ability to represent the geometric information of
complex scenes. By leveraging the spatial-angular consistency inherent in
light-field data, a differentiable forward model to generate a central view
image from the light-field data is developed. Based on the forward model, an
optimization scheme for the inverse problem of NDF reconstruction using
differentiable propagation operators is established. Furthermore, an iterative
solution method is adopted to reconstruct the NDF in the optimization scheme,
which does not require training datasets and applies to light-field data
captured by various acquisition methods. Experimental results demonstrate that
high-quality NDF can be reconstructed from light-field data using the proposed
method. High-resolution disparity can be effectively recovered by NDF,
demonstrating its capability for the implicit, continuous representation of
scene disparities.

</details>


### [851] [Ring Artifacts Removal Based on Implicit Neural Representation of Sinogram Data](https://arxiv.org/pdf/2409.15731)
*Ligen Shi, Xu Jiang, YunZe Liu, Chang Liu, Ping Yang, Shifeng Guo, Xing Zhao*

Main category: eess.IV

TL;DR: Proposes an unsupervised method using implicit neural representation (INR) to correct stripe artifacts in sinogram data, improving CT image quality by removing ring artifacts.


<details>
  <summary>Details</summary>
Motivation: Stripe artifacts in sinogram data cause ring artifacts in CT images, degrading quality. Current methods may not fully address this issue.

Method: Uses INR to correct defective pixel responses and learns stripe features in the sinogram's angular direction, combined in an optimization framework.

Result: Outperforms state-of-the-art techniques in removing ring artifacts while preserving CT image clarity.

Conclusion: The method effectively corrects stripe artifacts, enhancing CT image quality without supervision.

Abstract: Inconsistent responses of X-ray detector elements lead to stripe artifacts in
the sinogram data, which manifest as ring artifacts in the reconstructed CT
images, severely degrading image quality. This paper proposes a method for
correcting stripe artifacts in the sinogram data. The proposed method leverages
implicit neural representation (INR) to correct defective pixel response values
using implicit continuous functions and simultaneously learns stripe features
in the angular direction of the sinogram data. These two components are
combined within an optimization constraint framework, achieving unsupervised
iterative correction of stripe artifacts in the projection domain. Experimental
results demonstrate that the proposed method significantly outperforms current
state-of-the-art techniques in removing ring artifacts while maintaining the
clarity of CT images.

</details>


### [852] [Segment as You Wish -- Free-Form Language-Based Segmentation for Medical Images](https://arxiv.org/pdf/2410.12831)
*Longchao Da, Rui Wang, Xiaojian Xu, Parminder Bhatia, Taha Kass-Hout, Hua Wei, Cao Xiao*

Main category: eess.IV

TL;DR: A novel medical image segmentation model, FLanS, is introduced, leveraging free-form text prompts and a symmetry-aware module for precise segmentation across diverse medical imaging scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in text-related prompts for medical image segmentation, as clinicians often use natural language descriptions, unlike existing methods relying on bounding boxes or point-based prompts.

Method: Proposes a RAG-based text prompt generator and FLanS, a model handling diverse text prompts, including anatomy-informed and position-driven queries, with a symmetry-aware module for consistency.

Result: FLanS outperforms SOTA baselines in language understanding and segmentation precision, validated on 100k+ medical images from 7 datasets.

Conclusion: FLanS demonstrates superior performance in medical image segmentation by integrating free-form text prompts and symmetry-awareness, enhancing diagnostic accuracy.

Abstract: Medical imaging is crucial for diagnosing a patient's health condition, and
accurate segmentation of these images is essential for isolating regions of
interest to ensure precise diagnosis and treatment planning. Existing methods
primarily rely on bounding boxes or point-based prompts, while few have
explored text-related prompts, despite clinicians often describing their
observations and instructions in natural language. To address this gap, we
first propose a RAG-based free-form text prompt generator, that leverages the
domain corpus to generate diverse and realistic descriptions. Then, we
introduce FLanS, a novel medical image segmentation model that handles various
free-form text prompts, including professional anatomy-informed queries,
anatomy-agnostic position-driven queries, and anatomy-agnostic size-driven
queries. Additionally, our model also incorporates a symmetry-aware
canonicalization module to ensure consistent, accurate segmentations across
varying scan orientations and reduce confusion between the anatomical position
of an organ and its appearance in the scan. FLanS is trained on a large-scale
dataset of over 100k medical images from 7 public datasets. Comprehensive
experiments demonstrate the model's superior language understanding and
segmentation precision, along with a deep comprehension of the relationship
between them, outperforming SOTA baselines on both in-domain and out-of-domain
datasets.

</details>


### [853] [Pixel super-resolved virtual staining of label-free tissue using diffusion models](https://arxiv.org/pdf/2410.20073)
*Yijie Zhang, Luzhe Huang, Nir Pillar, Yuzhu Li, Hanlong Chen, Aydogan Ozcan*

Main category: eess.IV

TL;DR: A diffusion model-based super-resolution virtual staining method enhances label-free tissue images, outperforming traditional methods in resolution and accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the spatial resolution and fidelity of virtual tissue staining, addressing limitations of deep learning-based methods.

Method: Uses a Brownian bridge process and novel sampling techniques in a diffusion model to reduce variance and enhance stability in virtual staining.

Result: Achieves 4-5x super-resolution, 16-25x higher space-bandwidth product, and better structural similarity and perceptual accuracy than conventional methods.

Conclusion: The approach improves virtual staining reliability and quality, offering clinical diagnostic potential without chemical staining.

Abstract: Virtual staining of tissue offers a powerful tool for transforming label-free
microscopy images of unstained tissue into equivalents of histochemically
stained samples. This study presents a diffusion model-based super-resolution
virtual staining approach utilizing a Brownian bridge process to enhance both
the spatial resolution and fidelity of label-free virtual tissue staining,
addressing the limitations of traditional deep learning-based methods. Our
approach integrates novel sampling techniques into a diffusion model-based
image inference process to significantly reduce the variance in the generated
virtually stained images, resulting in more stable and accurate outputs.
Blindly applied to lower-resolution auto-fluorescence images of label-free
human lung tissue samples, the diffusion-based super-resolution virtual
staining model consistently outperformed conventional approaches in resolution,
structural similarity and perceptual accuracy, successfully achieving a
super-resolution factor of 4-5x, increasing the output space-bandwidth product
by 16-25-fold compared to the input label-free microscopy images.
Diffusion-based super-resolved virtual tissue staining not only improves
resolution and image quality but also enhances the reliability of virtual
staining without traditional chemical staining, offering significant potential
for clinical diagnostics.

</details>


### [854] [Assessing workflow impact and clinical utility of AI-assisted brain aneurysm detection: a multi-reader study](https://arxiv.org/pdf/2503.17786)
*Tommaso Di Noto, Sofyan Jankowski, Francesco Puccinelli, Guillaume Marie, Sebastien Tourbier, Yasser Aleman-Gomez, Oscar Esteban, Ricardo Corredor-Jerez, Guillaume Saliou, Patric Hagmann, Meritxell Bach Cuadra, Jonas Richiardi*

Main category: eess.IV

TL;DR: The study evaluates an AI-based brain aneurysm detection model's clinical utility, finding no significant improvement in radiologists' performance or confidence, and increased reading time with AI assistance.


<details>
  <summary>Details</summary>
Motivation: To assess the real-world applicability and impact of AI algorithms in clinical radiology, focusing on whether AI assistance improves radiologists' performance and workflow efficiency.

Method: The study reused and expanded an open-access dataset (N=460), training/validating the AI model on 360 subjects and testing on 100. Two radiologists with different experience levels (2 and 13 years) evaluated the model's impact on their performance and workflow.

Result: The AI model achieved state-of-the-art performance (74% sensitivity, 1.6% false positive rate), but neither radiologist showed significant sensitivity improvement with AI assistance. Reading time increased by 15 seconds on average, with no change in diagnostic confidence.

Conclusion: Clinical validation of AI algorithms is crucial, as real-world effectiveness and workflow impact may differ from theoretical performance. The study emphasizes the need for thorough evaluation before clinical integration.

Abstract: Despite the plethora of AI-based algorithms developed for anomaly detection
in radiology, subsequent integration into clinical setting is rarely evaluated.
In this work, we assess the applicability and utility of an AI-based model for
brain aneurysm detection comparing the performance of two readers with
different levels of experience (2 and 13 years). We aim to answer the following
questions: 1) Do the readers improve their performance when assisted by the AI
algorithm? 2) How much does the AI algorithm impact routine clinical workflow?
We reuse and enlarge our open-access, Time-Of-Flight Magnetic Resonance
Angiography dataset (N=460). We use 360 subjects for training/validating our
algorithm and 100 as unseen test set for the reading session. Even though our
model reaches state-of-the-art results on the test set (sensitivity=74%, false
positive rate=1.6), we show that neither the junior nor the senior reader
significantly increase their sensitivity (p=0.59, p=1, respectively). In
addition, we find that reading time for both readers is significantly higher in
the "AI-assisted" setting than in the "Unassisted" (+15 seconds, on average;
p=3x10^(-4) junior, p=3x10^(-5) senior). The confidence reported by the readers
is unchanged across the two settings, indicating that the AI assistance does
not influence the certainty of the diagnosis. Our findings highlight the
importance of clinical validation of AI algorithms in a clinical setting
involving radiologists. This study should serve as a reminder to the community
to always examine the real-word effectiveness and workflow impact of proposed
algorithms.

</details>


### [855] [MedSegNet10: A Publicly Accessible Network Repository for Split Federated Medical Image Segmentation](https://arxiv.org/pdf/2503.20830)
*Chamani Shiranthika, Zahra Hafezi Kafshgari, Hadi Hadizadeh, Parvaneh Saeedi*

Main category: eess.IV

TL;DR: MedSegNet10 is a public repository for medical image segmentation using split-federated learning, addressing privacy and data scarcity challenges.


<details>
  <summary>Details</summary>
Motivation: To overcome data privacy concerns and limited annotated data in medical image segmentation using decentralized learning approaches.

Method: Utilizes split-federated learning (SplitFed/SFL) for collaborative training on privately stored, horizontally split data.

Result: Provides pre-trained neural network architectures optimized for various medical image types, ensuring privacy and integrity.

Conclusion: MedSegNet10 supports researchers and practitioners in advancing medical image segmentation while maintaining patient data privacy.

Abstract: Machine Learning (ML) and Deep Learning (DL) have shown significant promise
in healthcare, particularly in medical image segmentation, which is crucial for
accurate disease diagnosis and treatment planning. Despite their potential,
challenges such as data privacy concerns, limited annotated data, and
inadequate training data persist. Decentralized learning approaches such as
federated learning (FL), split learning (SL), and split federated learning
(SplitFed/SFL) address these issues effectively. This paper introduces
"MedSegNet10," a publicly accessible repository designed for medical image
segmentation using split-federated learning. MedSegNet10 provides a collection
of pre-trained neural network architectures optimized for various medical image
types, including microscopic images of human blastocysts, dermatoscopic images
of skin lesions, and endoscopic images of lesions, polyps, and ulcers, with
applications extending beyond these examples. By leveraging SplitFed's
benefits, MedSegNet10 allows collaborative training on privately stored,
horizontally split data, ensuring privacy and integrity. This repository
supports researchers, practitioners, trainees, and data scientists, aiming to
advance medical image segmentation while maintaining patient data privacy. The
repository is available at: https://vault.sfu.ca/index.php/s/ryhf6t12O0sobuX
(password upon request to the authors).

</details>


### [856] [A Narrative Review on Large AI Models in Lung Cancer Screening, Diagnosis, and Treatment Planning](https://arxiv.org/pdf/2506.07236)
*Jiachen Zhong, Yiting Wang, Di Zhu, Ziwei Wang*

Main category: eess.IV

TL;DR: A review of large AI models in lung cancer care, covering screening, diagnosis, prognosis, and treatment, highlighting key models, datasets, and applications, while addressing limitations and future directions.


<details>
  <summary>Details</summary>
Motivation: Lung cancer's high prevalence and fatality necessitate improved diagnosis and treatment, with AI models offering transformative potential.

Method: Systematic survey of large AI models (e.g., CLIP, BLIP, Flamingo) categorized into modality-specific encoders, encoder-decoder frameworks, and joint encoder architectures, evaluated on datasets like LIDC-IDRI and NLST.

Result: AI models show promise in tasks like nodule detection, gene mutation prediction, and personalized treatment, with some clinical deployment.

Conclusion: Large AI models can optimize lung cancer care but face challenges in generalizability and interpretability; future work should focus on scalable, explainable, and clinically integrated systems.

Abstract: Lung cancer remains one of the most prevalent and fatal diseases worldwide,
demanding accurate and timely diagnosis and treatment. Recent advancements in
large AI models have significantly enhanced medical image understanding and
clinical decision-making. This review systematically surveys the
state-of-the-art in applying large AI models to lung cancer screening,
diagnosis, prognosis, and treatment. We categorize existing models into
modality-specific encoders, encoder-decoder frameworks, and joint encoder
architectures, highlighting key examples such as CLIP, BLIP, Flamingo,
BioViL-T, and GLoRIA. We further examine their performance in multimodal
learning tasks using benchmark datasets like LIDC-IDRI, NLST, and MIMIC-CXR.
Applications span pulmonary nodule detection, gene mutation prediction,
multi-omics integration, and personalized treatment planning, with emerging
evidence of clinical deployment and validation. Finally, we discuss current
limitations in generalizability, interpretability, and regulatory compliance,
proposing future directions for building scalable, explainable, and clinically
integrated AI systems. Our review underscores the transformative potential of
large AI models to personalize and optimize lung cancer care.

</details>


### [857] [Score-based Generative Diffusion Models to Synthesize Full-dose FDG Brain PET from MRI in Epilepsy Patients](https://arxiv.org/pdf/2506.11297)
*Jiaqi Wu, Jiahong Ouyang, Farshad Moradi, Mohammad Mehdi Khalighi, Greg Zaharchuk*

Main category: eess.IV

TL;DR: The study compares diffusion- and non-diffusion-based deep learning models for MRI-to-PET image translation in epilepsy patients, finding SGM-KD superior for pure MRI-to-PET synthesis and all models effective with low-dose PET inputs.


<details>
  <summary>Details</summary>
Motivation: To address the suboptimal radiation dose in FDG PET for epilepsy patients by synthesizing diagnostic-quality PET images from MRI or ultralow-dose PET using generative AI.

Method: Evaluated three models (SGM-KD, SGM-VP, Transformer-Unet) for MRI-to-PET translation in 52 subjects, using clinical and image metrics like Congruence Index and SUVR.

Result: SGM-KD performed best for pure MRI-to-PET synthesis. All models improved significantly with 1% low-dose PET inputs, becoming interchangeable in performance.

Conclusion: SGMs show promise for MRI-to-PET translation, while all models can accurately synthesize full-dose PET using MRI and ultralow-dose PET.

Abstract: Fluorodeoxyglucose (FDG) PET to evaluate patients with epilepsy is one of the
most common applications for simultaneous PET/MRI, given the need to image both
brain structure and metabolism, but is suboptimal due to the radiation dose in
this young population. Little work has been done synthesizing diagnostic
quality PET images from MRI data or MRI data with ultralow-dose PET using
advanced generative AI methods, such as diffusion models, with attention to
clinical evaluations tailored for the epilepsy population. Here we compared the
performance of diffusion- and non-diffusion-based deep learning models for the
MRI-to-PET image translation task for epilepsy imaging using simultaneous
PET/MRI in 52 subjects (40 train/2 validate/10 hold-out test). We tested three
different models: 2 score-based generative diffusion models (SGM-Karras
Diffusion [SGM-KD] and SGM-variance preserving [SGM-VP]) and a
Transformer-Unet. We report results on standard image processing metrics as
well as clinically relevant metrics, including congruency measures (Congruence
Index and Congruency Mean Absolute Error) that assess hemispheric metabolic
asymmetry, which is a key part of the clinical analysis of these images. The
SGM-KD produced the best qualitative and quantitative results when synthesizing
PET purely from T1w and T2 FLAIR images with the least mean absolute error in
whole-brain specific uptake value ratio (SUVR) and highest intraclass
correlation coefficient. When 1% low-dose PET images are included in the
inputs, all models improve significantly and are interchangeable for
quantitative performance and visual quality. In summary, SGMs hold great
potential for pure MRI-to-PET translation, while all 3 model types can
synthesize full-dose FDG-PET accurately using MRI and ultralow-dose PET.

</details>


### [858] [Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images](https://arxiv.org/pdf/2506.20407)
*Fangyijie Wang, Yuan Liang, Sourav Bhattacharjee, Abey Campbell, Kathleen M. Curran, Guénolé Silvestre*

Main category: eess.IV

TL;DR: A novel feature fusion framework using deep learning and radiomics for accurate gestational age estimation from fetal ultrasound images, achieving a mean absolute error of 8.0 days.


<details>
  <summary>Details</summary>
Motivation: Manual GA estimation is operator-dependent and time-consuming; automatic methods are needed for clinical practice.

Method: Combines deep learning for image representations and radiomics for fetal brain growth patterns, fusing both for GA estimation.

Result: Achieves a mean absolute error of 8.0 days, outperforming existing methods, with robustness across diverse populations.

Conclusion: The framework provides an accurate, interpretable, and automated solution for GA estimation in clinical settings.

Abstract: Accurate gestational age (GA) estimation, ideally through fetal ultrasound
measurement, is a crucial aspect of providing excellent antenatal care.
However, deriving GA from manual fetal biometric measurements depends on the
operator and is time-consuming. Hence, automatic computer-assisted methods are
demanded in clinical practice. In this paper, we present a novel feature fusion
framework to estimate GA using fetal ultrasound images without any measurement
information. We adopt a deep learning model to extract deep representations
from ultrasound images. We extract radiomic features to reveal patterns and
characteristics of fetal brain growth. To harness the interpretability of
radiomics in medical imaging analysis, we estimate GA by fusing radiomic
features and deep representations. Our framework estimates GA with a mean
absolute error of 8.0 days across three trimesters, outperforming current
machine learning-based methods at these gestational ages. Experimental results
demonstrate the robustness of our framework across different populations in
diverse geographical regions. Our code is publicly available on
\href{https://github.com/13204942/RadiomicsImageFusion_FetalUS}.

</details>


### [859] [Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism](https://arxiv.org/pdf/2506.22397)
*Anirban Ray, Ashesh, Florian Jug*

Main category: eess.IV

TL;DR: HazeMatching is a novel iterative method for dehazing microscopy images, balancing fidelity and realism without needing an explicit degradation operator.


<details>
  <summary>Details</summary>
Motivation: Cheaper microscopy modalities like widefield produce hazy images. Computational dehazing aims to combine affordability with image clarity, but existing methods prioritize either fidelity or realism, not both.

Method: HazeMatching adapts conditional flow matching, guiding the generative process with hazy observations in the conditional velocity field.

Result: Evaluated on 5 datasets, HazeMatching outperforms 7 baselines, balancing fidelity and realism while producing well-calibrated predictions.

Conclusion: HazeMatching effectively balances fidelity and realism in dehazing microscopy images, applicable even without an explicit degradation model.

Abstract: Fluorescence microscopy is a major driver of scientific progress in the life
sciences. Although high-end confocal microscopes are capable of filtering
out-of-focus light, cheaper and more accessible microscopy modalities, such as
widefield microscopy, can not, which consequently leads to hazy image data.
Computational dehazing is trying to combine the best of both worlds, leading to
cheap microscopy but crisp-looking images. The perception-distortion trade-off
tells us that we can optimize either for data fidelity, e.g. low MSE or high
PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.
Existing methods either prioritize fidelity at the expense of realism, or
produce perceptually convincing results that lack quantitative accuracy. In
this work, we propose HazeMatching, a novel iterative method for dehazing light
microscopy images, which effectively balances these objectives. Our goal was to
find a balanced trade-off between the fidelity of the dehazing results and the
realism of individual predictions (samples). We achieve this by adapting the
conditional flow matching framework by guiding the generative process with a
hazy observation in the conditional velocity field. We evaluate HazeMatching on
5 datasets, covering both synthetic and real data, assessing both distortion
and perceptual quality. Our method is compared against 7 baselines, achieving a
consistent balance between fidelity and realism on average. Additionally, with
calibration analysis, we show that HazeMatching produces well-calibrated
predictions. Note that our method does not need an explicit degradation
operator to exist, making it easily applicable on real microscopy data. All
data used for training and evaluation and our code will be publicly available
under a permissive license.

</details>
