<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 135]
- [cs.CV](#cs.CV) [Total: 145]
- [cs.AI](#cs.AI) [Total: 56]
- [cs.SD](#cs.SD) [Total: 17]
- [cs.LG](#cs.LG) [Total: 209]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 0]
- [eess.AS](#eess.AS) [Total: 4]
- [eess.IV](#eess.IV) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/pdf/2506.08430)
*Ziqi. Liu, Ziyang. Zhou, Mingxuan. Hu*

Main category: cs.CL

TL;DR: CAF-I, a multi-agent LLM framework, improves sarcasm detection by addressing single-perspective limitations, lack of understanding, and interpretability issues, achieving SOTA performance with a 4.98% boost in Macro-F1.


<details>
  <summary>Details</summary>
Motivation: Existing LLM methods for sarcasm detection suffer from single-perspective analysis, insufficient understanding, and poor interpretability.

Method: CAF-I uses specialized agents (Context, Semantics, Rhetoric) for multidimensional analysis and collaborative optimization, with a Decision Agent and Refinement Evaluator Agent for consolidation and feedback.

Result: CAF-I achieves a Macro-F1 of 76.31, a 4.98 absolute improvement over prior baselines, demonstrating SOTA zero-shot performance.

Conclusion: CAF-I's human-like multi-perspective analysis enhances sarcasm detection accuracy and interpretability, setting a new benchmark.

Abstract: Large language model (LLM) have become mainstream methods in the field of
sarcasm detection. However, existing LLM methods face challenges in irony
detection, including: 1. single-perspective limitations, 2. insufficient
comprehensive understanding, and 3. lack of interpretability. This paper
introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven
multi-agent system designed to overcome these issues. CAF-I employs specialized
agents for Context, Semantics, and Rhetoric, which perform multidimensional
analysis and engage in interactive collaborative optimization. A Decision Agent
then consolidates these perspectives, with a Refinement Evaluator Agent
providing conditional feedback for optimization. Experiments on benchmark
datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving
SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of
76.31, a 4.98 absolute improvement over the strongest prior baseline. This
success is attained by its effective simulation of human-like multi-perspective
analysis, enhancing detection accuracy and interpretability.

</details>


### [2] [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/pdf/2506.08120)
*Toyin Aguda, Erik Wilson, Allan Anzagira, Simerjot Kaur, Charese Smiley*

Main category: cs.CL

TL;DR: LLMs show conservative bias in relation extraction, often choosing 'No_Relation' to avoid errors, but this leads to information loss. The study evaluates this trade-off and introduces 'Hobson's choice' to describe safe but uninformative labels. Conservative bias is twice as common as hallucination.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the conservative bias in LLMs during relation extraction tasks, which causes information loss despite preventing incorrect assignments.

Method: Systematic evaluation across prompts, datasets, and relation types using SBERT and LLM prompts to measure semantic similarity between conservative bias behaviors.

Result: Conservative bias occurs twice as often as hallucination, highlighting a significant trade-off between safety and informativeness.

Conclusion: The study underscores the need to balance conservative bias and hallucination in LLMs for relation extraction, proposing further research to mitigate information loss.

Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation
extraction tasks, frequently defaulting to No_Relation label when an
appropriate option is unavailable. While this behavior helps prevent incorrect
relation assignments, our analysis reveals that it also leads to significant
information loss when reasoning is not explicitly included in the output. We
systematically evaluate this trade-off across multiple prompts, datasets, and
relation types, introducing the concept of Hobson's choice to capture scenarios
where models opt for safe but uninformative labels over hallucinated ones. Our
findings suggest that conservative bias occurs twice as often as hallucination.
To quantify this effect, we use SBERT and LLM prompts to capture the semantic
similarity between conservative bias behaviors in constrained prompts and
labels generated from semi-constrained and open-ended prompts.

</details>


### [3] [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/pdf/2506.08123)
*Jacob Dineen, Aswin RRV, Qin Liu, Zhikun Xu, Xiao Ye, Ming Shen, Zhaonan Li, Shijie Lu, Chitta Baral, Muhao Chen, Ben Zhou*

Main category: cs.CL

TL;DR: QA-LIGN decomposes rewards by principles for transparent AI alignment, matching or outperforming DPO.


<details>
  <summary>Details</summary>
Motivation: Standard reward-based alignment methods collapse diverse feedback into a single opaque signal, hindering interpretability.

Method: QA-LIGN uses principle-specific evaluation questions to derive separate reward components, replacing monolithic reward models.

Result: QA-LIGN offers greater transparency and adaptability, performing on par or better than DPO.

Conclusion: QA-LIGN advances interpretable and controllable alignment without sacrificing performance.

Abstract: Alignment of large language models with explicit principles (such as
helpfulness, honesty, and harmlessness) is crucial for ensuring safe and
reliable AI systems. However, standard reward-based alignment methods typically
collapse diverse feedback into a single scalar reward, entangling multiple
objectives into one opaque training signal, which hinders interpretability. In
this work, we introduce QA-LIGN, an automatic symbolic reward decomposition
approach that preserves the structure of each constitutional principle within
the reward mechanism. Instead of training a black-box reward model that outputs
a monolithic score, QA-LIGN formulates principle-specific evaluation questions
and derives separate reward components for each principle, making it a drop-in
reward model replacement. Experiments aligning an uncensored large language
model with a set of constitutional principles demonstrate that QA-LIGN offers
greater transparency and adaptability in the alignment process. At the same
time, our approach achieves performance on par with or better than a DPO
baseline. Overall, these results represent a step toward more interpretable and
controllable alignment of language models, achieved without sacrificing
end-task performance.

</details>


### [4] [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/pdf/2506.08136)
*Zefang Liu, Yinzhu Quan*

Main category: cs.CL

TL;DR: EconWebArena is a benchmark for evaluating autonomous agents on complex economic tasks in web environments, featuring 360 tasks from 82 authoritative sources. It tests navigation, multimodal understanding, and data extraction, with rigorous human curation. Evaluations reveal performance gaps in grounding and reasoning.


<details>
  <summary>Details</summary>
Motivation: To create a realistic benchmark for assessing autonomous agents' ability to handle complex, multimodal economic tasks on live websites, emphasizing authoritative data and grounded reasoning.

Method: Tasks were generated by LLMs and curated by humans for clarity and reliability. The benchmark evaluates multimodal LLMs as web agents, analyzing grounding, navigation, and interaction.

Result: Substantial performance gaps were found, highlighting challenges in grounding, navigation, and multimodal understanding.

Conclusion: EconWebArena serves as a rigorous testbed for advancing economic web intelligence, identifying key areas for improvement in autonomous agents.

Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on
complex, multimodal economic tasks in realistic web environments. The benchmark
comprises 360 curated tasks from 82 authoritative websites spanning domains
such as macroeconomics, labor, finance, trade, and public policy. Each task
challenges agents to navigate live websites, interpret structured and visual
content, interact with real interfaces, and extract precise, time-sensitive
data through multi-step workflows. We construct the benchmark by prompting
multiple large language models (LLMs) to generate candidate tasks, followed by
rigorous human curation to ensure clarity, feasibility, and source reliability.
Unlike prior work, EconWebArena emphasizes fidelity to authoritative data
sources and the need for grounded web-based economic reasoning. We evaluate a
diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure
cases, and conduct ablation studies to assess the impact of visual grounding,
plan-based reasoning, and interaction design. Our results reveal substantial
performance gaps and highlight persistent challenges in grounding, navigation,
and multimodal understanding, positioning EconWebArena as a rigorous testbed
for economic web intelligence.

</details>


### [5] [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/pdf/2506.08717)
*Mehedi Hasan Bijoy, Dejan Porjazovski, Tamás Grósz, Mikko Kurimo*

Main category: cs.CL

TL;DR: A novel language-aware multi-teacher knowledge distillation method is introduced to create a multilingual Speech Emotion Recognition (SER) model, achieving state-of-the-art performance in English, Finnish, and French.


<details>
  <summary>Details</summary>
Motivation: Extending monolingual SER to multilingual systems is challenging, and the goal is to train a single model capable of multilingual SER by distilling knowledge from multiple teacher models.

Method: The method uses Wav2Vec2.0 as the foundation for monolingual teacher models and distills their knowledge into a single multilingual student model.

Result: The student model achieves a weighted recall of 72.9 on English and 63.4 on Finnish datasets, outperforming baselines. It excels in recognizing sad and neutral emotions but struggles with anger and happiness.

Conclusion: The proposed method advances multilingual SER performance but has limitations in recognizing certain emotions.

Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer
interaction. Despite strides in monolingual SER, extending them to build a
multilingual system remains challenging. Our goal is to train a single model
capable of multilingual SER by distilling knowledge from multiple teacher
models. To address this, we introduce a novel language-aware multi-teacher
knowledge distillation method to advance SER in English, Finnish, and French.
It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and
then distills their knowledge into a single multilingual student model. The
student model demonstrates state-of-the-art performance, with a weighted recall
of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish
dataset, surpassing fine-tuning and knowledge distillation baselines. Our
method excels in improving recall for sad and neutral emotions, although it
still faces challenges in recognizing anger and happiness.

</details>


### [6] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/pdf/2506.08147)
*Muhammad Usman, Muhammad Ahmad, M. Shahiki Tash, Irina Gelbukh, Rolando Quintero Tellez, Grigori Sidorov*

Main category: cs.CL

TL;DR: The paper introduces a trilingual dataset for hate speech detection in English, Urdu, and Spanish, leveraging attention layers and transformer models to achieve strong performance, with significant improvements over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored area of hate speech detection in Urdu and enhancing multilingual approaches using translation-based methods.

Method: Uses attention layers with transformer models (e.g., GPT-3.5 Turbo, Qwen 2.5 72B) and TF-IDF for non-transformer models, benchmarked against traditional ML models like SVM.

Result: Achieves macro F1 scores of 0.87 (English), 0.85 (Spanish), 0.81 (Urdu), and 0.88 (multilingual), with improvements of 5-9% over baselines.

Conclusion: The framework provides a robust solution for multilingual hate speech detection, promoting safer online communities.

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [7] [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/pdf/2506.08158)
*Lijing Zhu, Qizhen Lan, Qing Tian, Wenbo Sun, Li Yang, Lu Xia, Yixin Xie, Xi Xiao, Tiehang Duan, Cui Tao, Shuteng Niu*

Main category: cs.CL

TL;DR: ETT-CKGE introduces task-driven tokens for efficient continual knowledge graph embedding, improving scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing CKGE methods are inefficient and struggle with knowledge preservation due to suboptimal scoring and expensive graph traversal.

Method: ETT-CKGE uses learnable task-driven tokens to capture task-relevant signals, avoiding explicit node scoring or traversal, and employs token-masked embedding alignment.

Result: ETT-CKGE achieves superior or competitive predictive performance while significantly reducing training time and memory usage.

Conclusion: ETT-CKGE is a scalable and efficient solution for continual knowledge graph embedding, outperforming existing methods.

Abstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge
while preserving past information. However, existing methods struggle with
efficiency and scalability due to two key limitations: (1) suboptimal knowledge
preservation between snapshots caused by manually designed node/relation
importance scores that ignore graph dependencies relevant to the downstream
task, and (2) computationally expensive graph traversal for node/relation
importance calculation, leading to slow training and high memory overhead. To
address these limitations, we introduce ETT-CKGE (Efficient, Task-driven,
Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE
method that leverages efficient task-driven tokens for efficient and effective
knowledge transfer between snapshots. Our method introduces a set of learnable
tokens that directly capture task-relevant signals, eliminating the need for
explicit node scoring or traversal. These tokens serve as consistent and
reusable guidance across snapshots, enabling efficient token-masked embedding
alignment between snapshots. Importantly, knowledge transfer is achieved
through simple matrix operations, significantly reducing training time and
memory usage. Extensive experiments across six benchmark datasets demonstrate
that ETT-CKGE consistently achieves superior or competitive predictive
performance, while substantially improving training efficiency and scalability
compared to state-of-the-art CKGE methods. The code is available at:
https://github.com/lijingzhu1/ETT-CKGE/tree/main

</details>


### [8] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/pdf/2506.08400)
*Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: The paper introduces mSTEB, a benchmark to evaluate LLMs on low-resource languages, revealing performance gaps between high- and low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations lack standardization for low-resource languages, limiting their applicability.

Method: The authors created mSTEB, a benchmark covering tasks like language identification, text classification, QA, and translation across speech and text. Leading LLMs (e.g., Gemini 2.0 Flash, GPT-4o) and open models (e.g., Qwen 2 Audio) were evaluated.

Result: A significant performance gap exists between high- and low-resource languages, particularly in African and Americas/Oceania languages.

Conclusion: More investment is needed to improve LLM coverage for underrepresented languages.

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [9] [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/pdf/2506.08172)
*Gerardo Aleman Manzanarez, Nora de la Cruz Arana, Jorge Garcia Flores, Yobany Garcia Medina, Raul Monroy, Nathalie Pernelle*

Main category: cs.CL

TL;DR: The paper introduces GrAImes, a literary theory-based protocol for evaluating AI-generated microfiction, addressing gaps in assessing literary merit.


<details>
  <summary>Details</summary>
Motivation: Despite AI's ability to generate coherent short fiction, rigorous evaluation of literary qualities like thematic coherence and aesthetic quality is lacking.

Method: The authors propose GrAImes, an evaluation protocol grounded in literary theory, and validate it with literature experts and enthusiasts.

Result: The protocol provides an objective framework for assessing AI-generated microfiction, validated by expert and enthusiast feedback.

Conclusion: GrAImes offers a foundational tool for evaluating the literary value of AI-generated microfictions.

Abstract: Automated story writing has been a subject of study for over 60 years. Large
language models can generate narratively consistent and linguistically coherent
short fiction texts. Despite these advancements, rigorous assessment of such
outputs for literary merit - especially concerning aesthetic qualities - has
received scant attention. In this paper, we address the challenge of evaluating
AI-generated microfictions and argue that this task requires consideration of
literary criteria across various aspects of the text, such as thematic
coherence, textual clarity, interpretive depth, and aesthetic quality. To
facilitate this, we present GrAImes: an evaluation protocol grounded in
literary theory, specifically drawing from a literary perspective, to offer an
objective framework for assessing AI-generated microfiction. Furthermore, we
report the results of our validation of the evaluation protocol, as answered by
both literature experts and literary enthusiasts. This protocol will serve as a
foundation for evaluating automatically generated microfictions and assessing
their literary value.

</details>


### [10] [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/pdf/2506.08564)
*Tuukka Törö, Antti Suni, Juraj Šimko*

Main category: cs.CL

TL;DR: The study uses ML-derived speech embeddings to analyze relationships among 106 languages, showing alignment with traditional measures and highlighting scalability for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To explore linguistic relationships globally using data-driven methods, overcoming limitations of traditional approaches.

Method: Employs embeddings from XLS-R model and LDA to cluster languages, comparing results with genealogical, lexical, and geographical distances.

Result: Embedding-based distances align with traditional measures, capturing typological patterns but revealing visualization challenges.

Conclusion: The approach offers scalable analysis of language variation, with potential for studying low-resource languages and integrating sociolinguistic factors.

Abstract: Investigating linguistic relationships on a global scale requires analyzing
diverse features such as syntax, phonology and prosody, which evolve at varying
rates influenced by internal diversification, language contact, and
sociolinguistic factors. Recent advances in machine learning (ML) offer
complementary alternatives to traditional historical and typological
approaches. Instead of relying on expert labor in analyzing specific linguistic
features, these new methods enable the exploration of linguistic variation
through embeddings derived directly from speech, opening new avenues for
large-scale, data-driven analyses.
  This study employs embeddings from the fine-tuned XLS-R self-supervised
language identification model voxlingua107-xls-r-300m-wav2vec, to analyze
relationships between 106 world languages based on speech recordings. Using
linear discriminant analysis (LDA), language embeddings are clustered and
compared with genealogical, lexical, and geographical distances. The results
demonstrate that embedding-based distances align closely with traditional
measures, effectively capturing both global and local typological patterns.
Challenges in visualizing relationships, particularly with hierarchical
clustering and network-based methods, highlight the dynamic nature of language
change.
  The findings show potential for scalable analyses of language variation based
on speech embeddings, providing new perspectives on relationships among
languages. By addressing methodological considerations such as corpus size and
latent space dimensionality, this approach opens avenues for studying
low-resource languages and bridging macro- and micro-level linguistic
variation. Future work aims to extend these methods to underrepresented
languages and integrate sociolinguistic variation for a more comprehensive
understanding of linguistic diversity.

</details>


### [11] [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/pdf/2506.08174)
*Li Weigang, Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: LLM-BT is a back-translation framework using LLMs to automate multilingual technical term standardization, ensuring high consistency and semantic alignment across languages.


<details>
  <summary>Details</summary>
Motivation: Traditional expert-driven standardization struggles with the rapid growth of technical terms, especially in fast-evolving fields like AI and quantum computing. Manual methods fail to ensure multilingual consistency.

Method: LLM-BT employs a back-translation framework with term-level consistency validation, a multi-path verification workflow, and conceptualizes back-translation as dynamic semantic embedding.

Result: Achieves over 90% exact or semantic matches in term consistency, strong cross-lingual robustness (BLEU > 0.45; Portuguese accuracy 100%), and provides transparent path-based embeddings.

Conclusion: LLM-BT transforms back-translation into an active engine for multilingual terminology standardization, enabling human-AI collaboration for semantic fidelity and cultural interpretation.

Abstract: The rapid growth of English technical terms challenges traditional
expert-driven standardization, especially in fast-evolving fields like AI and
quantum computing. Manual methods struggle to ensure multilingual consistency.
We propose \textbf{LLM-BT}, a back-translation framework powered by large
language models (LLMs) to automate terminology verification and standardization
via cross-lingual semantic alignment. Our contributions are: \textbf{(1)
Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate
language $\rightarrow$ English back-translation, LLM-BT achieves high term
consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies
showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path
Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''
pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw
$\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese
$\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong
cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%).
\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as
dynamic semantic embedding, revealing latent meaning trajectories. Unlike
static embeddings, LLM-BT provides transparent path-based embeddings shaped by
model evolution. LLM-BT transforms back-translation into an active engine for
multilingual terminology standardization, enabling human--AI collaboration:
machines ensure semantic fidelity, humans guide cultural interpretation. This
infrastructure supports terminology governance across scientific and
technological fields worldwide.

</details>


### [12] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/pdf/2506.08184)
*Chupei Wang, Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: The paper investigates how interference in long contexts affects LLM retrieval, showing accuracy declines as interference accumulates, and mitigation efforts are limited.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of intra-context interference on LLM retrieval, inspired by cognitive science's proactive interference paradigm.

Method: Introduces PI-LLM, an evaluation streaming related key-value updates and querying final values to measure interference effects.

Result: LLM retrieval accuracy declines log-linearly with interference; prompt engineering mitigates poorly.

Conclusion: LLMs face a working memory bottleneck, needing better methods to suppress irrelevant content during retrieval.

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [13] ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/pdf/2506.08221)
*Samra Zafar, Shaheer Minhas, Syed Ali Hassan Zaidi, Arfa Naeem, Zahra Ali*

Main category: cs.CL

TL;DR: Using writing process data (keystroke logging and snapshots) alongside final essays improves LLM feedback, making it more relatable and useful for students.


<details>
  <summary>Details</summary>
Motivation: Current LLM feedback lacks context from the writing process, missing insights into how students think and revise.

Method: Built a digital writing tool to capture writing traces; tested with 20 students via LLM feedback (using process data) and surveys.

Result: Students preferred process-aware feedback, finding it more aligned with their thinking. Certain edits correlated with higher scores.

Conclusion: Incorporating writing process data into LLM feedback enhances its relevance and supportiveness for learners.

Abstract: Large language models(LLMs) like Gemini are becoming common tools for
supporting student writing. But most of their feedback is based only on the
final essay missing important context about how that text was written. In this
paper, we explore whether using writing process data, collected through
keystroke logging and periodic snapshots, can help LLMs give feedback that
better reflects how learners think and revise while writing. We built a digital
writing tool that captures both what students type and how their essays evolve
over time. Twenty students used this tool to write timed essays, which were
then evaluated in two ways: (i) LLM generated feedback using both the final
essay and the full writing trace, and (ii) After the task, students completed
surveys about how useful and relatable they found the feedback. Early results
show that learners preferred the process-aware LLM feedback, finding it more in
tune with their own thinking. We also found that certain types of edits, like
adding new content or reorganizing paragraphs, aligned closely with higher
scores in areas like coherence and elaboration. Our findings suggest that
making LLMs more aware of the writing process can lead to feedback that feels
more meaningful, personal, and supportive.

</details>


### [14] [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/pdf/2505.17114)
*Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam*

Main category: cs.CL

TL;DR: RAVEN introduces QuART, a query-conditioned cross-modal gating module for multimodal QA, improving accuracy and robustness by dynamically weighting tokens across modalities.


<details>
  <summary>Details</summary>
Motivation: Address modality disagreements in multimodal QA, where irrelevant signals (e.g., off-camera speech) mislead models, by dynamically filtering and amplifying relevant tokens.

Method: RAVEN uses QuART for token relevance scoring, trained via a three-stage pipeline: unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning.

Result: RAVEN achieves up to 14.5% accuracy gains over SOTA models, with sensor data adding 16.4% improvement. It remains robust under modality corruption, outperforming baselines by 50.23%.

Conclusion: RAVEN's dynamic token weighting and staged training pipeline effectively address multimodal QA challenges, demonstrating significant performance improvements and robustness.

Abstract: Multimodal question answering (QA) often requires identifying which video,
audio, or sensor tokens are relevant to the question. Yet modality
disagreements are common: off-camera speech, background noise, or motion
outside the field of view often mislead fusion models that weight all streams
equally. We present RAVEN, a unified QA architecture whose core is QuART, a
query-conditioned cross-modal gating module that assigns scalar relevance
scores to each token across modalities, enabling the model to amplify
informative signals and suppress distractors before fusion. RAVEN is trained
through a three-stage pipeline comprising unimodal pretraining, query-aligned
fusion, and disagreement-oriented fine-tuning -- each stage targeting a
distinct challenge in multi-modal reasoning: representation quality,
cross-modal relevance, and robustness to modality mismatch. To support training
and evaluation, we release AVS-QA, a dataset of 300K synchronized
Audio--Video-Sensor streams paired with automatically generated question-answer
pairs. Experimental results on seven multi-modal QA benchmarks -- including
egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and
8.0\% gains in accuracy compared to state-of-the-art multi-modal large language
models, respectively. Incorporating sensor data provides an additional 16.4\%
boost, and the model remains robust under modality corruption, outperforming
SOTA baselines by 50.23\%. Our code and dataset are available at
https://github.com/BASHLab/RAVEN.

</details>


### [15] [Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR](https://arxiv.org/pdf/2409.08797)
*Mingyu Cui, Yifan Yang, Jiajun Deng, Jiawen Kang, Shujie Hu, Tianzi Wang, Zhaoqing Li, Shiliang Zhang, Xie Chen, Xunying Liu*

Main category: cs.CL

TL;DR: SSL-based discrete speech features improve ASR performance by modeling cross-utterance contexts, achieving significant WER reductions.


<details>
  <summary>Details</summary>
Motivation: To enhance ASR systems by leveraging SSL discrete speech features for better cross-utterance context modeling.

Method: Use WavLM-extracted discrete tokens in Zipformer-Transducer ASR, replacing Fbank features, and evaluate on Gigaspeech corpus.

Result: Achieved WER reductions of 0.32%-0.41% absolute (2.78%-3.54% relative) and lowest published WERs of 11.15% and 11.14%.

Conclusion: SSL discrete tokens effectively improve ASR performance, with open-source implementation available.

Abstract: Self-supervised learning (SSL) based discrete speech representations are
highly compact and domain adaptable. In this paper, SSL discrete speech
features extracted from WavLM models are used as additional cross-utterance
acoustic context features in Zipformer-Transducer ASR systems. The efficacy of
replacing Fbank features with discrete token features for modelling either
cross-utterance contexts (from preceding and future segments), or current
utterance's internal contexts alone, or both at the same time, are demonstrated
thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer
system using discrete tokens based cross-utterance context features outperforms
the baseline using utterance internal context only with statistically
significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78%
to 3.54% relative) on the dev and test data. The lowest published WER of 11.15%
and 11.14% were obtained on the dev and test sets. Our work is open-source and
publicly available at
https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\_ASR.

</details>


### [16] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/pdf/2506.08234)
*Yu-Ang Lee, Guan-Ting Yi, Mei-Yi Liu, Jui-Chao Lu, Guan-Bo Yang, Yun-Nung Chen*

Main category: cs.CL

TL;DR: A review of recent progress in optimizing compound AI systems, covering numerical and language-based techniques, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of compound AI systems necessitates new optimization methods beyond traditional approaches like SFT and RL.

Method: Systematic review and classification of optimization techniques, including natural language feedback for non-differentiable systems.

Result: Formalization of compound AI system optimization, classification of methods, and identification of open challenges.

Conclusion: The field is rapidly evolving, with promising directions for optimizing interactions in complex AI workflows.

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [17] [Summarizing Speech: A Comprehensive Survey](https://arxiv.org/pdf/2504.08024)
*Fabian Retkowski, Maike Züfle, Andreas Sudmann, Dinah Pfau, Shinji Watanabe, Jan Niehues, Alexander Waibel*

Main category: cs.CL

TL;DR: The paper surveys speech summarization, covering datasets, evaluation protocols, and advancements like fine-tuned cascaded architectures and end-to-end models, while highlighting challenges like multilingual datasets and long-context handling.


<details>
  <summary>Details</summary>
Motivation: Speech summarization is crucial for managing spoken and audiovisual content but lacks clear definition and faces challenges in evaluation and multilingual support.

Method: The survey examines existing datasets, evaluation protocols, and synthesizes recent advancements in speech summarization, including traditional systems and modern models.

Result: The paper highlights advancements in the field and identifies ongoing challenges, such as the need for better benchmarks and multilingual datasets.

Conclusion: Speech summarization is evolving with advanced models, but challenges like realistic evaluation and long-context handling remain.

Abstract: Speech summarization has become an essential tool for efficiently managing
and accessing the growing volume of spoken and audiovisual content. However,
despite its increasing importance, speech summarization remains loosely
defined. The field intersects with several research areas, including speech
recognition, text summarization, and specific applications like meeting
summarization. This survey not only examines existing datasets and evaluation
protocols, which are crucial for assessing the quality of summarization
approaches, but also synthesizes recent developments in the field, highlighting
the shift from traditional systems to advanced models like fine-tuned cascaded
architectures and end-to-end solutions. In doing so, we surface the ongoing
challenges, such as the need for realistic evaluation benchmarks, multilingual
datasets, and long-context handling.

</details>


### [18] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/pdf/2506.08235)
*Shashidhar Reddy Javaji, Yupeng Cao, Haohang Li, Yangyang Yu, Nikhil Muralidhar, Zining Zhu*

Main category: cs.CL

TL;DR: CLAIM-BENCH evaluates LLMs' ability to extract and validate scientific claims and evidence, revealing limitations and performance gaps between closed-source and open-source models.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' deeper comprehension of scientific argumentation, particularly in linking claims and evidence, which remains underexplored.

Method: Systematically compares three divide-and-conquer-inspired approaches across six LLMs, testing over 300 claim-evidence pairs.

Result: Closed-source models (e.g., GPT-4, Claude) outperform open-source ones in precision and recall. Three-pass and one-by-one prompting improve accuracy but increase computational cost.

Conclusion: CLAIM-BENCH provides a benchmark for evaluating LLMs' scientific comprehension, highlighting current limitations and suggesting improvements for deeper reasoning.

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [19] [CASPER: A Large Scale Spontaneous Speech Dataset](https://arxiv.org/pdf/2506.00267)
*Cihan Xiao, Ruixing Liang, Xiangyu Zhang, Mehmet Emre Tiryaki, Veronica Bae, Lavanya Shankar, Rong Yang, Ethan Poon, Emmanuel Dupoux, Sanjeev Khudanpur, Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: A novel pipeline for collecting spontaneous speech data is introduced, addressing the scarcity of high-quality natural dialogue datasets.


<details>
  <summary>Details</summary>
Motivation: The lack of spontaneous speech data in existing datasets limits the development of speech processing capabilities.

Method: A pipeline for eliciting and recording natural dialogues, producing a 100+ hour dataset of diverse, fluid conversations.

Result: A reproducible framework and dataset for spontaneous speech, fostering future research.

Conclusion: The dataset and methodology provide a foundation for addressing data scarcity, with plans for expansion.

Abstract: The success of large language models has driven interest in developing
similar speech processing capabilities. However, a key challenge is the
scarcity of high-quality spontaneous speech data, as most existing datasets
contain scripted dialogues. To address this, we present a novel pipeline for
eliciting and recording natural dialogues and release our dataset with 100+
hours of spontaneous speech. Our approach fosters fluid, natural conversations
while encouraging a diverse range of topics and interactive exchanges. Unlike
traditional methods, it facilitates genuine interactions, providing a
reproducible framework for future data collection. This paper introduces our
dataset and methodology, laying the groundwork for addressing the shortage of
spontaneous speech data. We plan to expand this dataset in future stages,
offering a growing resource for the research community.

</details>


### [20] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/pdf/2506.08260)
*Wanjing Anya Ma, Michael Flor, Zuowei Wang*

Main category: cs.CL

TL;DR: The paper introduces a taxonomy for inference types in reading comprehension (RC) and evaluates GPT-4o's ability to generate diagnostic RC questions, finding high quality but limited accuracy in matching targeted inference types.


<details>
  <summary>Details</summary>
Motivation: To improve reading instruction by developing a scalable method for generating diagnostic RC questions using AI.

Method: A taxonomy of inference types is created and applied to analyze an RC item bank. GPT-4o is used to generate questions via few-shot prompting, with and without chain-of-thought prompts, and evaluated for quality and inference type accuracy.

Result: GPT-4o produced 93.8% good-quality questions, but only 42.6% matched the targeted inference type. High inter-rater agreement (above 0.90) was achieved.

Conclusion: Combining AI-generated items with human judgment is promising for scalable, high-quality RC assessments.

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [21] [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/pdf/2506.08300)
*Matteo Cargnelutti, Catherine Brobston, John Hess, Jack Cushman, Kristi Mukk, Aristana Scourtas, Kyle Courtney, Greg Leppert, Amanda Watson, Martha Whitehead, Jonathan Zittrain*

Main category: cs.CL

TL;DR: The paper introduces Institutional Books 1.0, a high-quality dataset of public domain books from Harvard Library, addressing the scarcity of training data for LLMs.


<details>
  <summary>Details</summary>
Motivation: The rapid development of LLMs highlights the need for sustainable, high-quality datasets with clear provenance.

Method: Harvard Library's digitized books were extracted, analyzed, and processed into a well-documented dataset.

Result: A dataset of 983,004 volumes (242B tokens) with OCR-extracted text and metadata was released.

Conclusion: The project aims to improve accessibility and usability of historical texts for both humans and machines.

Abstract: Large language models (LLMs) use data to learn about the world in order to
produce meaningful correlations and predictions. As such, the nature, scale,
quality, and diversity of the datasets used to train these models, or to
support their work at inference time, have a direct impact on their quality.
The rapid development and adoption of LLMs of varying quality has brought into
focus the scarcity of publicly available, high-quality training data and
revealed an urgent need to ground the stewardship of these datasets in
sustainable practices with clear provenance chains. To that end, this technical
report introduces Institutional Books 1.0, a large collection of public domain
books originally digitized through Harvard Library's participation in the
Google Books project, beginning in 2006. Working with Harvard Library, we
extracted, analyzed, and processed these volumes into an extensively-documented
dataset of historic texts. This analysis covers the entirety of Harvard
Library's collection scanned as part of that project, originally spanning
1,075,899 volumes written in over 250 different languages for a total of
approximately 250 billion tokens. As part of this initial release, the
OCR-extracted text (original and post-processed) as well as the metadata
(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,
identified as being in the public domain have been made available. This report
describes this project's goals and methods as well as the results of the
analyses we performed, all in service of making this historical collection more
accessible and easier for humans and machines alike to filter, read and use.

</details>


### [22] [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/pdf/2506.08343)
*Chenlong Wang, Yuanning Feng, Dongping Chen, Zhaoyang Chu, Ranjay Krishna, Tianyi Zhou*

Main category: cs.CL

TL;DR: NoWait, a method disabling explicit self-reflection tokens like 'Wait' and 'Hmm', reduces reasoning trajectory length by 27%-51% without losing utility.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency caused by verbose and redundant outputs in large reasoning models.

Method: Proposes NoWait, which suppresses self-reflection tokens during inference.

Result: Reduces chain-of-thought trajectory length by 27%-51% across ten benchmarks.

Conclusion: NoWait is a plug-and-play solution for efficient, utility-preserving multimodal reasoning.

Abstract: Recent advances in large reasoning models have enabled complex, step-by-step
reasoning but often introduce significant overthinking, resulting in verbose
and redundant outputs that hinder efficiency. In this study, we examine whether
explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is
necessary for advanced reasoning. We propose NoWait, a simple yet effective
approach that disables explicit self-reflection by suppressing these tokens
during inference. Extensive experiments on ten benchmarks across textual,
visual, and video reasoning tasks show that NoWait reduces chain-of-thought
trajectory length by up to 27%-51% in five R1-style model series, without
compromising model utility. NoWait thus offers a plug-and-play solution for
efficient and utility-preserving multimodal reasoning.

</details>


### [23] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/pdf/2506.08349)
*Yuxuan Zhou, Xien Liu, Chenwei Yan, Chen Ning, Xiao Zhang, Boxun Li, Xiangling Fu, Shijin Wang, Guoping Hu, Yu Wang, Ji Wu*

Main category: cs.CL

TL;DR: The paper introduces a multi-cognitive-level evaluation framework for LLMs in the medical domain, revealing performance declines as cognitive complexity increases.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' capabilities across different cognitive levels in the medical domain, inspired by Bloom's Taxonomy.

Method: Proposes a framework integrating medical datasets and tasks targeting three cognitive levels, evaluating six LLM families.

Result: Performance declines with higher cognitive complexity, with model size being more critical at advanced levels.

Conclusion: Highlights the need to improve LLMs' higher cognitive capabilities for real-world medical applications.

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [24] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/pdf/2506.08354)
*Yiqun Sun, Qiang Huang, Anthony K. H. Tung, Jun Yu*

Main category: cs.CL

TL;DR: The paper advocates for a shift in text embedding research to focus on implicit semantics, beyond surface-level meaning, to better align with real-world language complexity.


<details>
  <summary>Details</summary>
Motivation: Current text embedding models focus on surface-level semantics, neglecting implicit meaning shaped by pragmatics, intent, and context, leading to poor performance on deeper semantic tasks.

Method: The authors highlight the gap through a pilot study comparing state-of-the-art models to simplistic baselines on implicit semantics tasks.

Result: State-of-the-art models perform only marginally better than baselines on tasks requiring implicit meaning.

Conclusion: The paper calls for a paradigm shift: diverse training data, linguistically grounded benchmarks, and explicit modeling of implicit semantics to improve embeddings.

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [25] [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/pdf/2506.08359)
*Li-Ming Zhan, Bo Liu, Zexin Lu, Chengqiang Xie, Jiannong Cao, Xiao-Ming Wu*

Main category: cs.CL

TL;DR: A causal-attribution framework identifies behavior-relevant attention heads in LLMs using VQ-AE and binary classification, improving inference-time steering accuracy.


<details>
  <summary>Details</summary>
Motivation: Current module selection methods for LLM behavior steering rely on superficial cues, leading to suboptimal outcomes. A principled approach is needed.

Method: Train VQ-AE on attention activations, partition latent space, and use binary classification to score heads' behavioral relevance.

Result: Superior performance in truthfulness-steering tasks and strong zero-shot generalization in cross-domain scenarios.

Conclusion: The framework enables more accurate inference-time interventions by identifying and weighting behavior-relevant heads.

Abstract: Inference-time steering aims to alter the response characteristics of large
language models (LLMs) without modifying their underlying parameters. A
critical step in this process is the identification of internal modules within
LLMs that are associated with the target behavior. However, current approaches
to module selection often depend on superficial cues or ad-hoc heuristics,
which can result in suboptimal or unintended outcomes. In this work, we propose
a principled causal-attribution framework for identifying behavior-relevant
attention heads in transformers. For each head, we train a vector-quantized
autoencoder (VQ-AE) on its attention activations, partitioning the latent space
into behavior-relevant and behavior-irrelevant subspaces, each quantized with a
shared learnable codebook. We assess the behavioral relevance of each head by
quantifying the separability of VQ-AE encodings for behavior-aligned versus
behavior-violating responses using a binary classification metric. This yields
a behavioral relevance score that reflects each head discriminative capacity
with respect to the target behavior, guiding both selection and importance
weighting. Experiments on seven LLMs from two model families and five
behavioral steering datasets demonstrate that our method enables more accurate
inference-time interventions, achieving superior performance on the
truthfulness-steering task. Furthermore, the heads selected by our approach
exhibit strong zero-shot generalization in cross-domain truthfulness-steering
scenarios.

</details>


### [26] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/pdf/2506.08364)
*Jash Rajesh Parekh, Pengcheng Jiang, Jiawei Han*

Main category: cs.CL

TL;DR: CC-RAG improves causal reasoning in LLMs by structuring evidence into a DAG of cause-effect triples, outperforming standard RAG and zero-shot LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLMs in understanding causal relationships, especially in specialized domains where flat context retrieval is insufficient.

Method: Integrates zero-shot triple extraction and theme-aware graph chaining into RAG, constructing a DAG of <cause, relation, effect> triples for structured multi-hop inference.

Result: Outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity, validated by LLM and human evaluations.

Conclusion: Explicitly modeling causal structure enhances LLM accuracy and interpretability, particularly in specialized domains.

Abstract: Understanding cause and effect relationships remains a formidable challenge
for Large Language Models (LLMs), particularly in specialized domains where
reasoning requires more than surface-level correlations. Retrieval-Augmented
Generation (RAG) improves factual accuracy, but standard RAG pipelines treat
evidence as flat context, lacking the structure required to model true causal
dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that
integrates zero-shot triple extraction and theme-aware graph chaining into the
RAG pipeline, enabling structured multi-hop inference. Given a domain specific
corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,
effect> triples and uses forward/backward chaining to guide structured answer
generation. Experiments on two real-world domains: Bitcoin price fluctuations
and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot
LLMs in chain similarity, information density, and lexical diversity. Both
LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results
demonstrate that explicitly modeling causal structure enables LLMs to generate
more accurate and interpretable responses, especially in specialized domains
where flat retrieval fails.

</details>


### [27] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/pdf/2506.08371)
*Zikai Xiao, Ziyang Wang, Wen Ma, Yan Zhang, Wei Shen, Yan Wang, Luqi Gong, Zuozhu Liu*

Main category: cs.CL

TL;DR: The paper addresses performance degradation in LLMs for long contexts, proposing a training-free method (PCD) to improve long-text performance by leveraging attention contrasts.


<details>
  <summary>Details</summary>
Motivation: LLMs degrade in performance for long contexts, and current solutions are costly. The paper explores statistical behaviors and cost-effective approaches.

Method: Proposes Positional Contrastive Decoding (PCD), contrasting logits from long-aware and local-aware attention to focus on gains from short-to-long training.

Result: PCD alleviates attention score degradation and achieves state-of-the-art performance on long-context benchmarks.

Conclusion: PCD is an effective, training-free solution for improving LLM performance in long-context scenarios.

Abstract: While Large Language Models (LLMs) support long contexts, they struggle with
performance degradation within the context window. Current solutions incur
prohibitive training costs, leaving statistical behaviors and cost-effective
approaches underexplored. From the decoding perspective, we identify the
Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio
correlates with long-text performance degradation. Notably, despite the
attenuation, gold tokens still occupy high-ranking positions in the decoding
space. Motivated by it, we propose the training-free Positional Contrastive
Decoding (PCD) that contrasts the logits derived from long-aware attention with
those from designed local-aware attention, enabling the model to focus on the
gains introduced by large-scale short-to-long training. Through the analysis of
long-term decay simulation, we demonstrate that PCD effectively alleviates
attention score degradation. Experimental results show that PCD achieves
state-of-the-art performance on long-context benchmarks.

</details>


### [28] [Draft-based Approximate Inference for LLMs](https://arxiv.org/pdf/2506.08373)
*Kevin Galim, Ethan Ewer, Wonjun Kang, Minjae Lee, Hyung Il Koo, Kangwook Lee*

Main category: cs.CL

TL;DR: A novel framework using draft models to optimize LLM inference by predicting token and KV pair importance, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The quadratic compute and linear memory complexity of Transformers in long-context LLMs necessitate efficient inference methods. Existing approaches rely on rough importance predictions, lacking accuracy.

Method: Proposes SpecKV for KV cache dropping and SpecPC for prompt compression, both leveraging draft models to predict importance more accurately.

Result: Demonstrates higher accuracy than baselines while maintaining improvements in memory, latency, and throughput.

Conclusion: The framework extends draft models' utility beyond speculative decoding, offering a promising direction for efficient LLM inference.

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [29] [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/pdf/2506.08375)
*Tao Zou, Xinghua Zhang, Haiyang Yu, Minzheng Wang, Fei Huang, Yongbin Li*

Main category: cs.CL

TL;DR: EIFBENCH is a benchmark for evaluating LLMs in multi-task, constrained scenarios, revealing performance gaps and proposing SegPO for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack complexity for real-world LLM applications, necessitating a more robust evaluation tool.

Method: Developed EIFBENCH for multi-task, constrained scenarios and proposed SegPO algorithm to enhance LLM performance.

Result: EIFBENCH exposed significant performance gaps in LLMs under complex instructions.

Conclusion: Ongoing optimization is needed to address LLM challenges in complex workflows.

Abstract: With the development and widespread application of large language models
(LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands
higher capabilities to address complex user needs, often requiring precise
workflow execution which involves the accurate understanding of multiple tasks.
However, existing benchmarks focusing on single-task environments with limited
constraints lack the complexity required to fully reflect real-world scenarios.
To bridge this gap, we present the Extremely Complex Instruction Following
Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and
robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that
enable comprehensive assessment across diverse task types concurrently, but
also integrates a variety of constraints, replicating complex operational
environments. Furthermore, we propose the Segment Policy Optimization (SegPO)
algorithm to enhance the LLM's ability to accurately fulfill multi-task
workflow. Evaluations on EIFBENCH have unveiled considerable performance
discrepancies in existing LLMs when challenged with these extremely complex
instructions. This finding underscores the necessity for ongoing optimization
to navigate the intricate challenges posed by LLM applications.

</details>


### [30] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/pdf/2506.08403)
*Weiya Li, Junjie Chen, Bei Li, Boyang Liu, Zichen Wen, Nuanqiao Shan, Xiaoqian Liu, Anping Liu, Huajie Liu, Youyan Wang, Wujiuge Yin, Hu Song, Bing Huang, Zhiyuan Xia, Jialiang Chen, Linfeng Zhang*

Main category: cs.CL

TL;DR: The paper introduces TACTIC, a cognitively informed multi-agent framework for machine translation, leveraging insights from human cognitive strategies to enhance LLM-based translation quality.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent translation frameworks overlook cognitive insights from human translation, limiting their potential.

Method: Proposes TACTIC, a framework with six specialized agents simulating human cognitive processes like drafting, refinement, and evaluation.

Result: TACTIC outperforms GPT-4.1 and DeepSeek-R1 on FLORES-200 and WMT24 benchmarks, achieving state-of-the-art performance.

Conclusion: TACTIC demonstrates the value of integrating cognitive theory into multi-agent systems for superior machine translation.

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [31] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/pdf/2506.08410)
*Ziyang Ma, Qingyue Yuan, Zhenglin Wang, Deyu Zhou*

Main category: cs.CL

TL;DR: The paper introduces AutoMeco and MIRA to evaluate and improve LLM meta-cognition, showing effectiveness on mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Previous research lacks step-level analysis of LLM meta-cognition, which is crucial for reliability.

Method: Proposes AutoMeco for benchmarking meta-cognition lenses and MIRA, a training-free strategy to enhance them.

Result: AutoMeco is validated against Best-of-N verification, and MIRA improves meta-cognition evaluation.

Conclusion: The study advances LLM meta-cognition evaluation and enhancement, demonstrating practical benefits.

Abstract: Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.

</details>


### [32] [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/pdf/2506.08427)
*Jiaxiang Liu, Boxuan Xing, Chenhao Yuan, Chenxiang Zhang, Di Wu, Xiusheng Huang, Haida Yu, Chuhan Lang, Pengfei Cao, Jun Zhao, Kang Liu*

Main category: cs.CL

TL;DR: Know-MRI is an open-source tool designed to systematically analyze and interpret the knowledge mechanisms of large language models (LLMs) by matching input data with appropriate interpretation methods and consolidating outputs.


<details>
  <summary>Details</summary>
Motivation: The need to enhance the interpretability of LLMs' internal knowledge mechanisms, given the diversity and limitations of current interpretation methods.

Method: Development of an extensible core module in Know-MRI that automatically matches input data with interpretation methods and consolidates outputs.

Result: Know-MRI enables users to freely choose interpretation methods based on inputs, facilitating comprehensive diagnosis of LLMs' knowledge mechanisms.

Conclusion: Know-MRI addresses the limitations of current interpretation tools, offering a flexible and systematic approach to understanding LLMs' knowledge mechanisms.

Abstract: As large language models (LLMs) continue to advance, there is a growing
urgency to enhance the interpretability of their internal knowledge mechanisms.
Consequently, many interpretation methods have emerged, aiming to unravel the
knowledge mechanisms of LLMs from various perspectives. However, current
interpretation methods differ in input data formats and interpreting outputs.
The tools integrating these methods are only capable of supporting tasks with
specific inputs, significantly constraining their practical applications. To
address these challenges, we present an open-source Knowledge Mechanisms
Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms
within LLMs systematically. Specifically, we have developed an extensible core
module that can automatically match different input data with interpretation
methods and consolidate the interpreting outputs. It enables users to freely
choose appropriate interpretation methods based on the inputs, making it easier
to comprehensively diagnose the model's internal knowledge mechanisms from
multiple perspectives. Our code is available at
https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on
https://youtu.be/NVWZABJ43Bs.

</details>


### [33] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/pdf/2506.08433)
*Hernán Maina, Nicolás Wolovick, Luciana Benotti*

Main category: cs.CL

TL;DR: The paper explores how numerical precisions and data parallelization affect training speed and accuracy for LLMs, aiming to make domain adaptation more accessible in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: High costs and cultural biases in LLM training limit accessibility. Domain adaptation is promising but computationally expensive, especially for groups with limited infrastructure.

Method: Evaluates the impact of numerical precisions and data parallelization strategies on training speed and model accuracy.

Result: Findings show trade-offs between training efficiency and accuracy, relevant for energy-efficient and low-resource settings.

Conclusion: The study provides insights for optimizing domain adaptation in resource-constrained environments, addressing energy efficiency and accessibility.

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [34] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/pdf/2506.08436)
*Jiujun He, Huazhen Lin*

Main category: cs.CL

TL;DR: Olica is a pruning framework for LLMs that avoids retraining by using PCA and SVD, improving efficiency without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for LLMs are costly due to retraining needs; Olica aims to eliminate this requirement.

Method: Uses PCA on MHA layers and SVD-based linear calibration for FFN layers to compress LLMs without retraining.

Result: Olica reduces computational costs and maintains performance across benchmarks.

Conclusion: Olica offers an efficient, retraining-free pruning solution for LLMs.

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [35] [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/pdf/2506.08477)
*Fengjun Pan, Anh Tuan Luu, Xiaobao Wu*

Main category: cs.CL

TL;DR: U-CoT+ is a framework for harmful meme detection using a meme-to-text pipeline and human-crafted guidelines, enabling resource-efficient and explainable classification with small-scale LLMs.


<details>
  <summary>Details</summary>
Motivation: Current harmful meme detection methods lack resource efficiency, flexibility, and explainability, limiting practical deployment.

Method: Develops a meme-to-text pipeline for detail-preserving descriptions, then uses zero-shot CoT prompting with human-crafted guidelines for classification.

Result: Validated on seven benchmark datasets, showing effectiveness for explainable and low-resource detection.

Conclusion: U-CoT+ offers a flexible, explainable, and efficient solution for harmful meme detection, adaptable across platforms and regions.

Abstract: Detecting harmful memes is essential for maintaining the integrity of online
environments. However, current approaches often struggle with resource
efficiency, flexibility, or explainability, limiting their practical deployment
in content moderation systems. To address these challenges, we introduce
U-CoT+, a novel framework for harmful meme detection. Instead of relying solely
on prompting or fine-tuning multimodal models, we first develop a high-fidelity
meme-to-text pipeline that converts visual memes into detail-preserving textual
descriptions. This design decouples meme interpretation from meme
classification, thus avoiding immediate reasoning over complex raw visual
content and enabling resource-efficient harmful meme detection with general
large language models (LLMs). Building on these textual descriptions, we
further incorporate targeted, interpretable human-crafted guidelines to guide
models' reasoning under zero-shot CoT prompting. As such, this framework allows
for easy adaptation to different harmfulness detection criteria across
platforms, regions, and over time, offering high flexibility and
explainability. Extensive experiments on seven benchmark datasets validate the
effectiveness of our framework, highlighting its potential for explainable and
low-resource harmful meme detection using small-scale LLMs. Codes and data are
available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.

</details>


### [36] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/pdf/2506.08479)
*Chihiro Taguchi, Seiji Maekawa, Nikita Bhutani*

Main category: cs.CL

TL;DR: Adaptive-$k$ retrieval dynamically selects the number of passages for QA tasks, outperforming fixed methods while using fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of optimal context retrieval in QA, where fixed sizes waste tokens or miss key evidence.

Method: Adaptive-$k$ retrieval selects passages based on similarity scores, without fine-tuning or extra LLM inferences.

Result: Matches or outperforms fixed baselines, uses fewer tokens, and retrieves 70% of relevant passages.

Conclusion: Dynamic context adjustment improves QA efficiency and accuracy across models.

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [37] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/pdf/2506.08480)
*Huixuan Zhang, Xiaojun Wan*

Main category: cs.CL

TL;DR: The paper critiques current text-to-image evaluation frameworks for lacking reliability and proposes improvements.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations focus on human agreement but miss other critical properties for trustworthy assessment.

Method: Identifies key aspects for reliable evaluation and tests current frameworks against them.

Result: Current frameworks fail to fully meet these properties across metrics and models.

Conclusion: Recommends improvements for better image-text alignment evaluation.

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [38] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/pdf/2506.08487)
*Sumanth Manduru, Carlotta Domeniconi*

Main category: cs.CL

TL;DR: The paper audits instruction-tuned Small Language Models (SLMs) for ethical risks, revealing insights on fairness, bias, and trade-offs in resource-constrained deployments.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of SLMs has outpaced understanding of their ethical risks, prompting the need for a large-scale audit to evaluate fairness and utility.

Method: The study evaluates nine open-source SLMs (0.5-5B parameters) using the BBQ benchmark under zero-shot prompting, analyzing utility and fairness in ambiguous/disambiguated contexts.

Result: Key findings: 1) Phi models show high competence and minimal bias; 2) Social bias varies by architecture (Qwen 2.5 appears fair but may lack genuine alignment, LLaMA 3.2 shows overconfidence); 3) Compression (e.g., 4-bit AWQ) introduces nuanced trade-offs in performance and bias.

Conclusion: The insights guide responsible SLM deployment, balancing fairness and efficiency, especially for small enterprises and resource-constrained environments.

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [39] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/pdf/2506.08488)
*Ashutosh Dwivedi, Siddhant Shivdutt Singh, Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces EtiCor++, a corpus for evaluating LLMs' cultural sensitivity to etiquettes, highlighting biases in LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resources for evaluating LLMs' understanding and bias regarding region-specific etiquettes.

Method: Introduces EtiCor++, a global etiquette corpus, and tasks/metrics for LLM evaluation.

Result: Experiments reveal inherent biases in LLMs towards certain regions.

Conclusion: EtiCor++ provides a framework to assess and mitigate cultural biases in LLMs.

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [40] [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/pdf/2506.08490)
*Xiao Wei, Xiaobao Wang, Ning Zhuang, Chenyang Wang, Longbiao Wang, Jianwu dang*

Main category: cs.CL

TL;DR: The paper proposes a consistency-driven prototype-prompting framework for Generalized Intent Discovery (GID) to address the limitations of supervised methods in handling out-of-domain intents.


<details>
  <summary>Details</summary>
Motivation: Supervised intent detection methods struggle with out-of-domain (OOD) intents due to reliance on labeled in-domain (IND) data. GID aims to discover new intents from unlabeled OOD data without additional annotation.

Method: The framework includes a prototype-prompting component for transferring old knowledge and a hierarchical consistency constraint for learning new knowledge from target domains.

Result: The method outperforms baseline approaches, achieving state-of-the-art results, demonstrating its effectiveness and generalization.

Conclusion: The proposed framework effectively integrates old and new knowledge for GID, offering a practical solution for intent discovery in diverse domains.

Abstract: Intent detection aims to identify user intents from natural language inputs,
where supervised methods rely heavily on labeled in-domain (IND) data and
struggle with out-of-domain (OOD) intents, limiting their practical
applicability. Generalized Intent Discovery (GID) addresses this by leveraging
unlabeled OOD data to discover new intents without additional annotation.
However, existing methods focus solely on clustering unsupervised data while
neglecting domain adaptation. Therefore, we propose a consistency-driven
prototype-prompting framework for GID from the perspective of integrating old
and new knowledge, which includes a prototype-prompting framework for
transferring old knowledge from external sources, and a hierarchical
consistency constraint for learning new knowledge from target domains. We
conducted extensive experiments and the results show that our method
significantly outperforms all baseline methods, achieving state-of-the-art
results, which strongly demonstrates the effectiveness and generalization of
our methods. Our source code is publicly available at
https://github.com/smileix/cpp.

</details>


### [41] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/pdf/2506.08500)
*Arie Cattan, Alon Jacovi, Ori Ram, Jonathan Herzig, Roee Aharoni, Sasha Goldshtein, Eran Ofek, Idan Szpektor, Avi Caciularu*

Main category: cs.CL

TL;DR: The paper introduces a taxonomy for knowledge conflicts in RAG, proposes the CONFLICTS benchmark, and shows LLMs struggle with resolving conflicts, though reasoning prompts help.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of conflicting information in retrieved sources for RAG and improve LLM handling of such discrepancies.

Method: Proposes a taxonomy of knowledge conflict types, introduces the CONFLICTS benchmark, and tests LLMs with reasoning prompts.

Result: LLMs often fail to resolve conflicts, but explicit reasoning prompts improve response quality.

Conclusion: Future research is needed to further enhance LLM conflict resolution in RAG.

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [42] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/pdf/2506.08504)
*Divyaksh Shukla, Ritesh Baviskar, Dwijesh Gohil, Aniket Tiwari, Atul Shree, Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces CoMuMDR, a code-mixed multi-modal corpus for discourse parsing in Hindi-English conversations, highlighting challenges for SoTA models.


<details>
  <summary>Details</summary>
Motivation: Existing discourse parsing datasets are limited to single-domain written English dialogues, lacking diversity.

Method: The authors introduce CoMuMDR, a corpus with audio, transcribed text, and nine discourse relations, and test SoTA models.

Result: SoTA models perform poorly on the multi-domain code-mixed corpus, indicating its complexity.

Conclusion: The results emphasize the need for better models to handle multi-domain code-mixed discourse parsing.

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [43] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/pdf/2506.08552)
*Xinyuan Wang, Dongjie Wang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Sixun Dong, Kunpeng Liu, Yanjie Fu*

Main category: cs.CL

TL;DR: A lightweight post-training framework refines latent reasoning in LLMs using contrastive feedback and residual embedding refinement, achieving a 5% accuracy boost on MathQA.


<details>
  <summary>Details</summary>
Motivation: Address limitations of Chain-of-Thought prompting (token overhead, fixed reasoning) and latent reasoning challenges (effective embedding updates).

Method: Proposes two strategies: contrastive reasoning feedback and residual embedding refinement to update reasoning embeddings post-training.

Result: Demonstrates effectiveness on five benchmarks, notably a 5% accuracy gain on MathQA without extra training.

Conclusion: The framework successfully refines latent reasoning, improving model accuracy efficiently.

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [44] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/pdf/2506.00160)
*Qihui Fan, Enfu Nan, Wenbo Li, Lei Lu, Pu Zhao, Yanzhi Wang*

Main category: cs.CL

TL;DR: A novel LLM-based Werewolf game system is proposed, leveraging advanced LLMs like DeepSeek R1 and V3 for better reasoning and persuasion, eliminating the need for extra components.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs with improved reasoning and persuasion capabilities motivates the development of a more engaging and compatible Werewolf game system.

Method: The system combines LLMs with tuned Text-to-Speech (TTS) models for enhanced compatibility and user engagement, avoiding complex fine-tuning or additional components.

Result: The proposed system achieves improved engagement and compatibility with various LLM models.

Conclusion: With advancing LLM reasoning, extra components are unnecessary for creating engaging Werewolf games.

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [45] [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/pdf/2506.08584)
*Yahan Li, Jifan Yao, John Bosco S. Bunyi, Adam C. Frank, Angel Hwang, Ruishan Liu*

Main category: cs.CL

TL;DR: CounselBench evaluates LLMs in mental health counseling, showing they outperform human therapists in quality but raise safety concerns. Human experts highlight issues LLM judges miss.


<details>
  <summary>Details</summary>
Motivation: To assess LLM behavior in realistic mental health counseling scenarios, given their growing use but lack of rigorous testing.

Method: Developed CounselBench with 100 professionals, including CounselBench-EVAL (2,000 expert evaluations) and CounselBench-Adv (120 adversarial questions). Evaluated responses from GPT-4, LLaMA 3, Gemini, and human therapists.

Result: LLMs often outperform human therapists in perceived quality but are flagged for safety issues like unauthorized medical advice. LLM judges overrate responses and miss safety concerns.

Conclusion: CounselBench provides a clinically grounded framework for benchmarking and improving LLMs in mental health settings.

Abstract: Large language models (LLMs) are increasingly proposed for use in mental
health support, yet their behavior in realistic counseling scenarios remains
largely untested. We introduce CounselBench, a large-scale benchmark developed
with 100 mental health professionals to evaluate and stress-test LLMs in
single-turn counseling. The first component, CounselBench-EVAL, contains 2,000
expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human
therapists to real patient questions. Each response is rated along six
clinically grounded dimensions, with written rationales and span-level
annotations. We find that LLMs often outperform online human therapists in
perceived quality, but experts frequently flag their outputs for safety
concerns such as unauthorized medical advice. Follow-up experiments show that
LLM judges consistently overrate model responses and overlook safety issues
identified by human experts. To probe failure modes more directly, we construct
CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling
questions designed to trigger specific model issues. Evaluation across 2,880
responses from eight LLMs reveals consistent, model-specific failure patterns.
Together, CounselBench establishes a clinically grounded framework for
benchmarking and improving LLM behavior in high-stakes mental health settings.

</details>


### [46] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/pdf/2506.08592)
*Liyan Xu, Zhenlin Su, Mo Yu, Jiangnan Li, Fandong Meng, Jie Zhou*

Main category: cs.CL

TL;DR: The paper addresses the limitation of text encoders in recognizing fine-grained entities/events, introduces a Chinese dataset (CapRetrieval), and proposes finetuning strategies to improve performance.


<details>
  <summary>Details</summary>
Motivation: To examine and address the failure of text encoders in fine-grained semantic recognition, especially in dense retrieval tasks.

Method: Introduces CapRetrieval dataset, evaluates zero-shot performance, and proposes finetuning strategies with data generation.

Result: Finetuned encoders achieve the best performance on CapRetrieval, but a granularity dilemma is identified.

Conclusion: The work highlights encoder limitations, offers solutions, and releases resources for further research.

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [47] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/pdf/2506.05695)
*Lingyuan Liu, Mengxiang Zhang*

Main category: cs.CL

TL;DR: The paper proposes POCL, a curriculum learning framework for Knowledge Distillation (KD) in LLMs, addressing issues like catastrophic forgetting and mode collapse by progressively introducing training samples from easy to hard.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods for LLMs suffer from distribution shifts during training, causing problems like catastrophic forgetting and training-inference mismatch.

Method: POCL uses a difficulty measurer to rank samples and a training scheduler to introduce them progressively, with rising loss temperatures.

Result: Experiments show POCL improves student model performance across various KD methods and model families.

Conclusion: Sorted training samples enhance KD stability and performance, demonstrating the value of structured training data in LLM distillation.

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [48] [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/pdf/2506.08593)
*Shuzhou Yuan, Ercong Nie, Mario Tawfelis, Helmut Schmid, Hinrich Schütze, Michael Färber*

Main category: cs.CL

TL;DR: The paper explores how MBTI-based personality traits influence hate speech detection in LLMs, revealing significant biases and inconsistencies.


<details>
  <summary>Details</summary>
Motivation: To investigate the unexplored impact of personality traits on LLM performance in hate speech classification, given the subjective nature of the task.

Method: Conducted a human annotation survey to confirm MBTI's effect on labeling, then prompted four open-source LLMs with MBTI personas and evaluated their outputs on three datasets.

Result: Found substantial persona-driven variation, including inconsistencies with ground truth, inter-persona disagreement, and logit-level biases.

Conclusion: Highlights the need for careful persona prompt design in LLM workflows to ensure fairness and alignment with human values.

Abstract: Hate speech detection is a socially sensitive and inherently subjective task,
with judgments often varying based on personal traits. While prior work has
examined how socio-demographic factors influence annotation, the impact of
personality traits on Large Language Models (LLMs) remains largely unexplored.
In this paper, we present the first comprehensive study on the role of persona
prompts in hate speech classification, focusing on MBTI-based traits. A human
annotation survey confirms that MBTI dimensions significantly affect labeling
behavior. Extending this to LLMs, we prompt four open-source models with MBTI
personas and evaluate their outputs across three hate speech datasets. Our
analysis uncovers substantial persona-driven variation, including
inconsistencies with ground truth, inter-persona disagreement, and logit-level
biases. These findings highlight the need to carefully define persona prompts
in LLM-based annotation workflows, with implications for fairness and alignment
with human values.

</details>


### [49] [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/pdf/2506.08625)
*Minhae Oh, Jeonghye Kim, Nakyung Lee, Donggeon Seo, Taeuk Kim, Jungwoo Lee*

Main category: cs.CL

TL;DR: RAISE is a retrieval-augmented framework for scientific reasoning, outperforming baselines by retrieving logically relevant documents.


<details>
  <summary>Details</summary>
Motivation: Scientific reasoning involves complex processes and domain-specific knowledge, requiring adaptive solutions.

Method: RAISE uses problem decomposition, logical query generation, and logical retrieval to retrieve relevant documents.

Result: RAISE consistently outperforms baselines on scientific reasoning benchmarks by retrieving logically relevant documents.

Conclusion: RAISE's logical retrieval approach enhances scientific reasoning by focusing on relevance beyond domain similarity.

Abstract: Scientific reasoning requires not only long-chain reasoning processes, but
also knowledge of domain-specific terminologies and adaptation to updated
findings. To deal with these challenges for scientific reasoning, we introduce
RAISE, a step-by-step retrieval-augmented framework which retrieves logically
relevant documents from in-the-wild corpus. RAISE is divided into three steps:
problem decomposition, logical query generation, and logical retrieval. We
observe that RAISE consistently outperforms other baselines on scientific
reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves
documents that are not only similar in terms of the domain knowledge, but also
documents logically more relevant.

</details>


### [50] [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/pdf/2506.08643)
*Son The Nguyen, Theja Tulabandhula*

Main category: cs.CL

TL;DR: MEMETRON is a task-agnostic framework for LLM decoding, using metaheuristic algorithms to optimize responses without retraining, outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM decoding strategies lack control and task-specific optimization, limiting performance and alignment with objectives.

Method: MEMETRON formulates decoding as a discrete black-box optimization problem, using hybrid metaheuristic algorithms (GENETRON, ANNETRON) guided by reward models and LLM operations.

Result: MEMETRON outperforms standard decoding and reranking methods, especially in human preference alignment tasks.

Conclusion: MEMETRON offers a modular, efficient solution for improving LLM alignment and performance without retraining.

Abstract: Large language models (LLMs) are increasingly used for both open-ended and
structured tasks, yet their inference-time behavior is still largely dictated
by heuristic decoding strategies such as greedy search, sampling, or reranking.
These methods provide limited control and do not explicitly optimize for
task-specific objectives. We introduce MEMETRON, a task-agnostic framework that
formulates LLM decoding as a discrete black-box optimization problem. MEMETRON
leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the
response space, guided by reward models and contextual operations performed by
the LLM itself. This approach enables efficient discovery of high-reward
responses without requiring model retraining or gradient access. The framework
is modular and generalizes across diverse tasks, requiring only a reward
function and lightweight prompt templates. We evaluate our framework on the
critical human preference alignment task and demonstrate that it significantly
outperforms standard decoding and reranking methods, highlighting its potential
to improve alignment without model retraining.

</details>


### [51] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/pdf/2506.08646)
*Mingyu Zheng, Zhifan Feng, Jia Wang, Lanrui Wang, Zheng Lin, Yang Hao, Weiping Wang*

Main category: cs.CL

TL;DR: TableDreamer addresses limitations in LLM-based table instruction tuning data synthesis by diversifying input space exploration and targeting model weaknesses, improving accuracy by 11.62%.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack diversity in table understanding tasks and ignore model weaknesses, leading to inefficient data synthesis.

Method: A progressive, weakness-guided framework synthesizes diverse seed data and iteratively refines it based on identified weaknesses.

Result: Boosts Llama3.1-8B-instruct accuracy by 11.62% with 27K synthetic data, outperforming baselines.

Conclusion: TableDreamer effectively enhances table instruction tuning by addressing diversity and efficiency issues.

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [52] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/pdf/2506.08647)
*Oumaima El Khettari, Solen Quiniou, Samuel Chaffron*

Main category: cs.CL

TL;DR: A generative relation extraction (RE) pipeline for the intestinal microbiome uses LLM summarization to refine context, improving performance but still lagging behind BERT-based methods.


<details>
  <summary>Details</summary>
Motivation: To study interactions in the low-resource intestinal microbiome domain using generative RE.

Method: Leverages LLM summarization to refine context before extracting relations via instruction-tuned generation.

Result: Summarization improves generative RE performance, but BERT-based methods still outperform.

Conclusion: Generative methods show promise for specialized, low-resource domains, though further development is needed.

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [53] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/pdf/2506.08672)
*Yang Liu, Jiaqi Li, Zilong Zheng*

Main category: cs.CL

TL;DR: RuleReasoner, a reinforced rule-based reasoning method, outperforms large reasoning models (LRMs) with improved generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of small reasoning models (SRMs) effectively learning rule-based reasoning across diverse tasks and domains.

Method: Uses a domain-aware dynamic sampling approach for reinforcement learning, updating sampling weights based on historical rewards.

Result: Outperforms LRMs by 4.1% on in-distribution tasks and 10.4% on out-of-distribution tasks, with higher computational efficiency.

Conclusion: RuleReasoner is a simple yet effective method for robust rule-based reasoning, demonstrating superior performance and efficiency.

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [54] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/pdf/2506.08686)
*Soham Poddar, Paramita Koley, Janardan Misra, Sanjay Podder, Navveen Balani, Niloy Ganguly, Saptarshi Ghosh*

Main category: cs.CL

TL;DR: The paper explores energy-efficient inference for LLMs by reducing response length through prompt-engineering, achieving 25-60% energy savings without compromising quality.


<details>
  <summary>Details</summary>
Motivation: Energy consumption in LLM inference is high, and output compression is understudied. The paper aims to address this by optimizing response length.

Method: Benchmarked 12 decoder-only LLMs across 5 datasets, identified redundant information, and tested prompt-engineering strategies for length reduction.

Result: Prompt-engineering reduced response length significantly, achieving 25-60% energy savings while maintaining response quality.

Conclusion: Simple prompt-engineering can effectively optimize LLM energy use by reducing unnecessary response length.

Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)
arises from their inference processes; hence developing energy-efficient
methods for inference is crucial. While several techniques exist for inference
optimization, output compression remains relatively unexplored, with only a few
preliminary efforts addressing this aspect. In this work, we first benchmark 12
decoder-only LLMs across 5 datasets, revealing that these models often produce
responses that are substantially longer than necessary. We then conduct a
comprehensive quality assessment of LLM responses, formally defining six
information categories present in LLM responses. We show that LLMs often tend
to include redundant or additional information besides the minimal answer. To
address this issue of long responses by LLMs, we explore several simple and
intuitive prompt-engineering strategies. Empirical evaluation shows that
appropriate prompts targeting length reduction and controlling information
content can achieve significant energy optimization between 25-60\% by reducing
the response length while preserving the quality of LLM responses.

</details>


### [55] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/pdf/2506.08700)
*Ruiran Su, Jiasheng Si, Zhijiang Guo, Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: ClimateViz introduces a benchmark for scientific fact-checking using charts, showing current models struggle with chart-based reasoning, lagging behind human performance.


<details>
  <summary>Details</summary>
Motivation: Existing fact-checking overlooks scientific charts, which are crucial for quantitative evidence.

Method: Created ClimateViz with 49,862 claims linked to 2,896 charts, labeled and explained via knowledge graphs. Evaluated multimodal models in zero-shot and few-shot settings.

Result: Best models (Gemini 2.5, InternVL 2.5) achieved 76.2-77.8% accuracy, below human performance (89.3-92.7%). Explanation-augmented outputs helped some models.

Conclusion: Current models are inadequate for chart-based reasoning; ClimateViz provides a valuable benchmark for future improvements.

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


### [56] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/pdf/2506.08712)
*Hee Suk Yoon, Eunseop Yoon, Mark A. Hasegawa-Johnson, Sungwoong Kim, Chang D. Yoo*

Main category: cs.CL

TL;DR: ConfPO is a lightweight, model-free method for preference learning in LLMs that optimizes preference-critical tokens based on training policy confidence, outperforming uniform methods like DPO.


<details>
  <summary>Details</summary>
Motivation: Prior methods like DPO adjust all tokens uniformly, leading to inefficiency and overoptimization. ConfPO aims to improve alignment by focusing on impactful tokens without auxiliary models.

Method: ConfPO identifies and optimizes preference-critical tokens using the training policy's confidence, avoiding reliance on external models or compute.

Result: ConfPO outperforms uniform DAAs on benchmarks like AlpacaEval 2 and Arena-Hard, achieving better alignment with no extra computational cost.

Conclusion: ConfPO offers a simple, efficient alternative to uniform alignment methods, enhancing performance without additional overhead.

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [57] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/pdf/2506.08713)
*Fariz Ikhwantri, Dusica Marijan*

Main category: cs.CL

TL;DR: The paper proposes EXCLAIM, an NLI-based method for explainable compliance detection in assurance cases, addressing challenges like complex texts and data scarcity by using LLMs for case generation and multi-hop reasoning.


<details>
  <summary>Details</summary>
Motivation: Challenges in regulatory compliance include complex legal/technical texts, lack of model explanations, and limited assurance case data.

Method: EXCLAIM uses NLI for multi-hop reasoning in assurance cases, generates cases with LLMs, and introduces coverage/consistency metrics.

Result: Demonstrated effectiveness with GDPR requirements, showing NLI's potential in automating compliance.

Conclusion: NLI-based approaches like EXCLAIM can automate and improve regulatory compliance processes.

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [58] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/pdf/2506.08726)
*Nelvin Tan, Zian Seng, Liang Zhang, Yu-Ching Shih, Dong Yang, Amol Salunkhe*

Main category: cs.CL

TL;DR: The paper explores the limitations of traditional critic agents in numerical QA for financial documents without oracle labels and introduces an improved critic agent and calculator agent, which outperform prior methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with numerical QA in financial documents, especially without oracle labels, prompting the need for better self-correction methods.

Method: The study evaluates traditional critic agents without oracle labels, proposes an improved critic agent, and introduces a calculator agent, analyzing their interactions.

Result: The improved critic agent and calculator agent outperform the state-of-the-art (program-of-thought) and are safer.

Conclusion: The new agents enhance performance in numerical QA for financial documents, highlighting the importance of agent interaction.

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [59] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/pdf/2506.08738)
*Dror Kris Markus, Fabrizio Gilardi, Daria Stetsenko*

Main category: cs.CL

TL;DR: The study analyzes AI papers to assess societal and ethical integration, finding technical teams increasingly address societal concerns, challenging assumptions about interdisciplinary roles.


<details>
  <summary>Details</summary>
Motivation: To understand how ethical and societal values are integrated into AI research and whether interdisciplinary teams drive this shift.

Method: Analyzed 100,000+ AI papers on ArXiv (2014-2024), developed a classifier to identify societal content, and measured its prevalence.

Result: Interdisciplinary teams still lead in societal research, but computer science-only teams are growing in societal output, addressing fairness, safety, healthcare, and misinformation.

Conclusion: Findings challenge assumptions about interdisciplinary drivers of societal AI, raising questions about AI governance and the role of non-technical fields.

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [60] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/pdf/2506.08746)
*Muhammad Anwar, Mishca de Costa, Issam Hammad, Daniel Lau*

Main category: cs.CL

TL;DR: A domain-specific LLM for nuclear applications is developed using a compact Transformer, trained on a single GPU for data security. It captures nuclear vocabulary but lacks syntactic coherence. Future work includes dataset expansion and refinement.


<details>
  <summary>Details</summary>
Motivation: To create a secure, in-house LLM for nuclear applications that adheres to cybersecurity and confidentiality standards.

Method: Uses a compact Transformer-based architecture trained on a single GPU with data from the Essential CANDU textbook.

Result: The model captures specialized nuclear vocabulary but sometimes lacks syntactic coherence. Early text generation shows promise.

Conclusion: The approach is feasible but requires richer corpora, better preprocessing, and fine-tuning for real-world nuclear applications.

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [61] [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/pdf/2506.08750)
*Muhammad Anwar, Daniel Lau, Mishca de Costa, Issam Hammad*

Main category: cs.CL

TL;DR: Synthetic data generation transforms unstructured nuclear industry text into structured Q&A pairs for LLM applications, addressing data scarcity and privacy issues.


<details>
  <summary>Details</summary>
Motivation: The nuclear industry has valuable unstructured text data, but it's unusable for LLMs without structured Q&A pairs. Synthetic data can bridge this gap.

Method: Leverages LLMs to analyze text, extract key info, generate Q&A pairs, and evaluate dataset quality.

Result: Enables robust LLMs for nuclear domain, improving info retrieval, knowledge sharing, and decision-making.

Conclusion: Synthetic data unlocks LLM potential in the nuclear industry, offering scalable solutions for data challenges.

Abstract: The nuclear industry possesses a wealth of valuable information locked away
in unstructured text data. This data, however, is not readily usable for
advanced Large Language Model (LLM) applications that require clean, structured
question-answer pairs for tasks like model training, fine-tuning, and
evaluation. This paper explores how synthetic data generation can bridge this
gap, enabling the development of robust LLMs for the nuclear domain. We discuss
the challenges of data scarcity and privacy concerns inherent in the nuclear
industry and how synthetic data provides a solution by transforming existing
text data into usable Q&A pairs. This approach leverages LLMs to analyze text,
extract key information, generate relevant questions, and evaluate the quality
of the resulting synthetic dataset. By unlocking the potential of LLMs in the
nuclear industry, synthetic data can pave the way for improved information
retrieval, enhanced knowledge sharing, and more informed decision-making in
this critical sector.

</details>


### [62] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/pdf/2506.08753)
*Pradyoth Hegde, Santosh Kesiraju, Jan Švec, Šimon Sedláček, Bolaji Yusuf, Oldřich Plchot, Deepak K T, Jan Černocký*

Main category: cs.CL

TL;DR: The study applies in-context learning (ICL) to dialogue state tracking (DST) using a k-nearest neighbor method for demonstration retrieval, analyzes influencing factors, and evaluates performance on LLMs like OLMo-7B-instruct.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of ICL for DST and identify factors impacting its performance.

Method: Uses a sentence embedding-based k-nearest neighbor method to retrieve demonstrations, structures them with test samples in a template, and evaluates on MultiWoZ2.4 dataset with specific LLMs.

Result: Provides insights into the in-context learning abilities of LLMs for DST, focusing on demonstration selection and prompt context.

Conclusion: The study offers valuable findings on how ICL can be optimized for DST tasks using LLMs.

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [63] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/pdf/2506.08757)
*Mishca de Costa, Muhammad Anwar, Dave Mercier, Mark Randall, Issam Hammad*

Main category: cs.CL

TL;DR: The paper proposes using function-calling LLMs instead of direct NL-to-SQL for safer and more accurate data retrieval in nuclear power plants.


<details>
  <summary>Details</summary>
Motivation: Traditional NL-to-SQL poses risks due to unvalidated queries and complex legacy databases, reducing trust and accuracy.

Method: A function-based approach with pre-approved, purpose-specific functions encapsulating validated SQL logic, assisted by NL-to-SQL tools for initial code generation.

Result: Improved accuracy and maintainability compared to direct NL-to-SQL, with validated SQL queries.

Conclusion: Balancing user accessibility with operational safety, the framework offers a robust solution for critical systems.

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [64] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/pdf/2506.08768)
*Ahmed Hasanaath, Aisha Alansari, Ahmed Ashraf, Chafik Salmane, Hamzah Luqman, Saad Ezzini*

Main category: cs.CL

TL;DR: The paper benchmarks reasoning-focused LLMs, especially DeepSeek models, on Arabic NLP tasks, showing significant performance gains with few-shot learning and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' performance on Arabic data, which is underexplored due to its complexity, and evaluate reasoning abilities across diverse tasks.

Method: Evaluates LLMs using zero-shot, few-shot, and fine-tuning strategies on 15 Arabic NLP tasks, comparing DeepSeek models to a GPT-4-mini baseline.

Result: Few-shot learning boosts performance (e.g., sentiment analysis from 35.3% to 87.5%), DeepSeek outperforms GPT-4-mini by 12 F1 points, and LoRA fine-tuning adds 8 F1/BLEU points.

Conclusion: Careful in-context example selection and fine-tuning significantly enhance LLM performance on Arabic NLP tasks, with DeepSeek models showing superior reasoning capabilities.

Abstract: Large language models (LLMs) have shown remarkable progress in reasoning
abilities and general natural language processing (NLP) tasks, yet their
performance on Arabic data, characterized by rich morphology, diverse dialects,
and complex script, remains underexplored. This paper presents a comprehensive
benchmarking study of multiple reasoning-focused LLMs, with a special emphasis
on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP
tasks. We experiment with various strategies, including zero-shot, few-shot,
and fine-tuning. This allows us to systematically evaluate performance on
datasets covering a range of applications to examine their capacity for
linguistic reasoning under different levels of complexity. Our experiments
reveal several key findings. First, carefully selecting just three in-context
examples delivers an average uplift of over 13 F1 points on classification
tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection
from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures
outperform a strong GPT o4-mini baseline by an average of 12 F1 points on
complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning
yields up to an additional 8 points in F1 and BLEU compared to equivalent
increases in model scale. The code is available at
https://anonymous.4open.science/r/AraReasoner41299

</details>


### [65] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/pdf/2506.08827)
*Francisco Vargas, Alejandro González Coene, Gaston Escalante, Exequiel Lobón, Manuel Pulido*

Main category: cs.CL

TL;DR: A two-step method for extracting traffic accident details from legal documents outperforms classic methods, with GPT-4 Turbo achieving the highest accuracy (86.1%).


<details>
  <summary>Details</summary>
Motivation: Traffic accident details in legal documents are hard to extract due to complex reasoning, impacting insurance cost quantification.

Method: Segments documents first, then extracts entities using vectorization and LLMs (e.g., LLaMA-2, GPT-4 Turbo), with some models fine-tuned.

Result: Vectorization + LLMs outperforms classic methods (39.5% vs. up to 86.1%). LLaMA-3 8B matches fine-tuned LLaMA-2 70B (76.6%).

Conclusion: Advanced LLMs, especially GPT-4 Turbo, significantly improve entity extraction accuracy, with open-source models like LLaMA-3 showing rapid progress.

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [66] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/pdf/2506.08836)
*Flavio D'Intino, Hans-Peter Hutter*

Main category: cs.CL

TL;DR: The paper introduces the SRB-300 dataset, a 300-hour annotated speech corpus for Swiss German, and fine-tunes Whisper models, achieving significant improvements in transcription accuracy.


<details>
  <summary>Details</summary>
Motivation: Swiss German lacks standardized written forms, and existing STT models struggle with spontaneous speech. The SRB-300 dataset addresses this gap by providing real-world conversational data.

Method: The authors fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset, which includes diverse Swiss German dialects from radio and TV recordings.

Result: Fine-tuning improved WER by 19-33% and BLEU scores by 8-40%, with the best model (large-v3) achieving a WER of 17.1% and BLEU of 74.8.

Conclusion: The SRB-300 dataset and fine-tuned Whisper models significantly advance STT systems for Swiss German and other low-resource languages in real-world settings.

Abstract: Swiss German is a low-resource language represented by diverse dialects that
differ significantly from Standard German and from each other, lacking a
standardized written form. As a result, transcribing Swiss German involves
translating into Standard German. Existing datasets have been collected in
controlled environments, yielding effective speech-to-text (STT) models, but
these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from 39
Swiss German radio and TV stations. It captures spontaneous speech across all
major Swiss dialects recorded in various realistic environments and overcomes
the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance metrics.
Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores
increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a
WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for
developing effective and robust STT systems for Swiss German and other
low-resource languages in real-world contexts.

</details>


### [67] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/pdf/2506.08885)
*Danush Khanna, Krishna Kumar, Basab Ghosh, Vinija Jain, Vasu Sharma, Aman Chadha, Amitava Das*

Main category: cs.CL

TL;DR: The paper exposes a geometric blind spot in LLM alignment, introduces ALKALI (an adversarial benchmark), and proposes GRACE (a defense framework) to mitigate latent camouflage vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Adversarial threats against LLMs are outpacing defenses, exploiting latent geometry to evade detection.

Method: Introduces ALKALI benchmark and GRACE framework, which enforces latent separation and adversarial cohesion via geometric regularization.

Result: GRACE reduces Attack Success Rates by up to 39%, and AVQI metric quantifies latent alignment failures.

Conclusion: The work highlights latent camouflage vulnerabilities and offers practical solutions for improving LLM safety.

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [68] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/pdf/2506.08897)
*Hiba Khey, Amine Lakhder, Salma Rouichi, Imane El Ghabi, Kamal Hejjaoui, Younes En-nahli, Fahd Kalloubi, Moez Amri*

Main category: cs.CL

TL;DR: PlantBert is a transformer-based language model tailored for plant science, specifically for extracting structured knowledge from plant stress-response literature, built on DeBERTa and fine-tuned on expert-annotated abstracts.


<details>
  <summary>Details</summary>
Motivation: Plant science lacks domain-adapted NLP tools despite advancements in transformer models. PlantBert aims to fill this gap by providing a specialized tool for plant stress-response literature.

Method: PlantBert uses DeBERTa architecture, fine-tuned on expert-annotated abstracts, combined with rule-enhanced post-processing and ontology-grounded entity normalization.

Result: PlantBert shows strong generalization across entity types and robust domain adaptation in low-resource fields, enabling precise, biologically meaningful relationship extraction.

Conclusion: PlantBert bridges a gap in agricultural NLP, offering a scalable framework for entity recognition and fostering innovation in computational plant science.

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [69] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/pdf/2506.08899)
*Elias Horner, Cristinel Mateis, Guido Governatori, Agata Ciabattoni*

Main category: cs.CL

TL;DR: A novel pipeline using LLMs transforms legal texts into Defeasible Deontic Logic, showing promising results in aligning machine-generated and expert formalizations.


<details>
  <summary>Details</summary>
Motivation: To automate semantic analysis of legal texts and transform them into formal representations for scalable legal informatics.

Method: A structured pipeline segments legal texts, extracts deontic rules, and evaluates coherence, tested with various LLM configurations.

Result: Empirical results show alignment between machine-generated and expert-crafted formalizations, especially with effective prompting.

Conclusion: LLMs, when prompted well, can significantly aid scalable legal informatics by automating legal text analysis.

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [70] [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/pdf/2506.08907)
*Antonios Dimakis, John Pavlopoulos, Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: A new method combining rule-based transformations and LLMs with few-shot prompting improves dialect-to-standard normalization for Greek dialects, evaluated on proverbs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of low-resource languages and dialects by enabling standard-language tools to process dialectal text.

Method: Combines rule-based linguistically informed transformations and LLMs with few-shot prompting, requiring no parallel data.

Result: Human evaluation shows effectiveness; downstream experiments reveal previous reliance on superficial linguistic information.

Conclusion: The method successfully normalizes dialectal text, uncovering deeper semantic insights beyond orthographic artifacts.

Abstract: Natural language understanding systems struggle with low-resource languages,
including many dialects of high-resource ones. Dialect-to-standard
normalization attempts to tackle this issue by transforming dialectal text so
that it can be used by standard-language tools downstream. In this study, we
tackle this task by introducing a new normalization method that combines
rule-based linguistically informed transformations and large language models
(LLMs) with targeted few-shot prompting, without requiring any parallel data.
We implement our method for Greek dialects and apply it on a dataset of
regional proverbs, evaluating the outputs using human annotators. We then use
this dataset to conduct downstream experiments, finding that previous results
regarding these proverbs relied solely on superficial linguistic information,
including orthographic artifacts, while new observations can still be made
through the remaining semantics.

</details>


### [71] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/pdf/2506.08920)
*Zeyu Leo Liu, Greg Durrett, Eunsol Choi*

Main category: cs.CL

TL;DR: PropMEND, a hypernetwork-based approach, improves knowledge propagation in LLMs for multi-hop reasoning, outperforming existing methods on RippleEdit and Controlled RippleEdit datasets.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge editing techniques for LLMs lack the ability to propagate injected knowledge for reasoning tasks.

Method: A hypernetwork meta-learns gradient modifications to enable multi-hop reasoning with injected knowledge.

Result: PropMEND achieves nearly 2x accuracy on multi-hop questions and generalizes to unseen relations/entities, though with a reduced performance gap.

Conclusion: PropMEND advances knowledge propagation but highlights the need for further work on broader relation coverage.

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [72] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/pdf/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: A 1.5B parameter mathematical reasoning model trained on a single gaming GPU (RTX 3080 Ti) achieves comparable or better performance than larger models, challenging the need for massive infrastructure.


<details>
  <summary>Details</summary>
Motivation: To democratize access to high-performance AI research by reducing computational resource demands for training capable mathematical reasoning models.

Method: Integrates reinforcement learning and memory optimization techniques to train a 1.5B parameter model on a single gaming GPU (RTX 3080 Ti).

Result: The model performs comparably or better than larger models on mathematical reasoning benchmarks, despite limited hardware.

Conclusion: State-of-the-art mathematical reasoning can be achieved without massive infrastructure, making high-performance AI research more accessible.

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [73] [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/pdf/2506.08938)
*Qinggang Zhang, Zhishang Xiang, Yilin Xiao, Le Wang, Junhui Li, Xinrun Wang, Jinsong Su*

Main category: cs.CL

TL;DR: FaithfulRAG is a new framework addressing unfaithfulness in retrieval-augmented LLMs by modeling knowledge conflicts and enabling reasoning before response generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods suppress LLMs' parametric knowledge to enforce faithfulness, risking misinterpretation and undermining internal knowledge.

Method: FaithfulRAG identifies fact-level conflicts and uses a self-thinking process for reasoning and integration.

Result: Outperforms state-of-the-art methods in experiments.

Conclusion: FaithfulRAG effectively resolves knowledge conflicts while preserving LLMs' parametric knowledge.

Abstract: Large language models (LLMs) augmented with retrieval systems have
demonstrated significant potential in handling knowledge-intensive tasks.
However, these models often struggle with unfaithfulness issues, generating
outputs that either ignore the retrieved context or inconsistently blend it
with the LLM`s parametric knowledge. This issue is particularly severe in cases
of knowledge conflict, where the retrieved context conflicts with the model`s
parametric knowledge. While existing faithful RAG approaches enforce strict
context adherence through well-designed prompts or modified decoding
strategies, our analysis reveals a critical limitation: they achieve
faithfulness by forcibly suppressing the model`s parametric knowledge, which
undermines the model`s internal knowledge structure and increases the risk of
misinterpreting the context. To this end, this paper proposes FaithfulRAG, a
novel framework that resolves knowledge conflicts by explicitly modeling
discrepancies between the model`s parametric knowledge and retrieved context.
Specifically, FaithfulRAG identifies conflicting knowledge at the fact level
and designs a self-thinking process, allowing LLMs to reason about and
integrate conflicting facts before generating responses. Extensive experiments
demonstrate that our method outperforms state-of-the-art methods. The code is
available at https:// github.com/DeepLearnXMU/Faithful-RAG

</details>


### [74] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/pdf/2506.08952)
*Clara Lachenmaier, Judith Sieker, Sina Zarrieß*

Main category: cs.CL

TL;DR: LLMs struggle with grounding and correcting false beliefs in political discourse, raising concerns about misinformation.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs handle common ground and misinformation in political contexts.

Method: Evaluating LLMs' responses to direct and loaded questions about political facts.

Result: LLMs often fail to correct false beliefs, showing challenges in grounding.

Conclusion: LLMs' limitations in grounding pose risks for misinformation in political discourse.

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [75] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/pdf/2506.08966)
*Marek Kadlčík, Michal Štefánik, Timothee Mickus, Michal Spiegel, Josef Kuchař*

Main category: cs.CL

TL;DR: A novel probing technique reveals that pretrained LMs accurately represent numbers, and aligning embeddings with this pattern can reduce arithmetic errors.


<details>
  <summary>Details</summary>
Motivation: Pretrained LMs often make arithmetic errors, and existing probing methods fail to capture the sinusoidal patterns in number embeddings.

Method: Proposed a new probing technique to decode numeric values from embeddings with high accuracy.

Result: LMs represent numbers precisely after pre-training, and embedding precision correlates with arithmetic errors.

Conclusion: Aligning embeddings with the discovered pattern can mitigate arithmetic errors in LMs.

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [76] [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/pdf/2506.08972)
*Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, Zhuosheng Zhang*

Main category: cs.CL

TL;DR: UI-NEXUS is a benchmark for evaluating mobile agents on compositional tasks, revealing challenges like under/over-execution. AGENT-NEXUS, a proposed scheduling system, improves task success rates by 24-40%.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on atomic tasks, neglecting compositional tasks crucial for real-world applications.

Method: UI-NEXUS benchmark evaluates agents on three compositional operations. AGENT-NEXUS dynamically decomposes tasks into subtasks.

Result: Existing agents struggle with performance-efficiency balance. AGENT-NEXUS improves success rates by 24-40%.

Conclusion: AGENT-NEXUS effectively addresses compositional task challenges, enhancing mobile agent performance.

Abstract: Autonomous agents powered by multimodal large language models have been
developed to facilitate task execution on mobile devices. However, prior work
has predominantly focused on atomic tasks -- such as shot-chain execution tasks
and single-screen grounding tasks -- while overlooking the generalization to
compositional tasks, which are indispensable for real-world applications. This
work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile
agents on three categories of compositional operations: Simple Concatenation,
Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in
20 fully controllable local utility app environments, as well as 30 online
Chinese and English service apps. It comprises 100 interactive task templates
with an average optimal step count of 14.05. Experimental results across a
range of mobile agents with agentic workflow or agent-as-a-model show that
UI-NEXUS presents significant challenges. Specifically, existing agents
generally struggle to balance performance and efficiency, exhibiting
representative failure modes such as under-execution, over-execution, and
attention drift, causing visible atomic-to-compositional generalization gap.
Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient
scheduling system to tackle compositional mobile tasks. AGENT-NEXUS
extrapolates the abilities of existing mobile agents by dynamically decomposing
long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS
achieves 24% to 40% task success rate improvement for existing mobile agents on
compositional operation tasks within the UI-NEXUS benchmark without
significantly sacrificing inference overhead. The demo video, dataset, and code
are available on the project page at https://ui-nexus.github.io.

</details>


### [77] [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/pdf/2506.08981)
*Satu Hopponen, Tomi Kinnunen, Alexandre Nikolaev, Rosa González Hautamäki, Lauri Tavi, Einar Meister*

Main category: cs.CL

TL;DR: The paper introduces the FROST-EMA corpus, a dataset of bilingual speech in native and second languages, including imitated accents, for phonetic and technological research. Two case studies demonstrate its utility.


<details>
  <summary>Details</summary>
Motivation: To enable research on language variability in bilingual speech from phonetic and technological perspectives.

Method: Creation of the FROST-EMA corpus with 18 bilingual speakers producing speech in L1, L2, and imitated L2. Two case studies: one on speaker verification performance, another on articulatory patterns.

Result: The corpus supports analysis of language variability. Case studies show impacts on speaker verification and articulatory differences.

Conclusion: The FROST-EMA corpus is a valuable resource for studying bilingual speech variability, with demonstrated applications in technology and phonetics.

Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of
Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers,
who produced speech in their native language (L1), second language (L2), and
imitated L2 (fake foreign accent). The new corpus enables research into
language variability from phonetic and technological points of view.
Accordingly, we include two preliminary case studies to demonstrate both
perspectives. The first case study explores the impact of L2 and imitated L2 on
the performance of an automatic speaker verification system, while the second
illustrates the articulatory patterns of one speaker in L1, L2, and a fake
accent.

</details>


### [78] [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/pdf/2506.08986)
*Yuejiao Wang, Xianmin Gong, Xixin Wu, Patrick Wong, Hoi-lam Helene Fung, Man Wai Mak, Helen Meng*

Main category: cs.CL

TL;DR: A novel language-related fMRI task was developed for early detection of cognitive decline in older adults, achieving high accuracy (AUC=0.86) using machine learning.


<details>
  <summary>Details</summary>
Motivation: Early detection of neurocognitive disorder (NCD) is vital for timely intervention, and language-related fMRI shows promise for this purpose.

Method: A naturalistic language-related fMRI task was tested on 97 non-demented Chinese older adults, with machine-learning models analyzing fMRI features and demographics.

Result: The models achieved an AUC of 0.86, with key features from language-processing brain regions like the superior temporal gyrus.

Conclusion: The study highlights the potential of language-related fMRI for early NCD detection.

Abstract: Early detection is crucial for timely intervention aimed at preventing and
slowing the progression of neurocognitive disorder (NCD), a common and
significant health problem among the aging population. Recent evidence has
suggested that language-related functional magnetic resonance imaging (fMRI)
may be a promising approach for detecting cognitive decline and early NCD. In
this paper, we proposed a novel, naturalistic language-related fMRI task for
this purpose. We examined the effectiveness of this task among 97 non-demented
Chinese older adults from Hong Kong. The results showed that machine-learning
classification models based on fMRI features extracted from the task and
demographics (age, gender, and education year) achieved an average area under
the curve of 0.86 when classifying participants' cognitive status (labeled as
NORMAL vs DECLINE based on their scores on a standard neurcognitive test).
Feature localization revealed that the fMRI features most frequently selected
by the data-driven approach came primarily from brain regions associated with
language processing, such as the superior temporal gyrus, middle temporal
gyrus, and right cerebellum. The study demonstrated the potential of the
naturalistic language-related fMRI task for early detection of aging-related
cognitive decline and NCD.

</details>


### [79] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/pdf/2506.08999)
*Theo Zhang, Madurya Suresh, Anne S. Warlaumont, Kasia Hitczenko, Alejandrina Cristia, Margaret Cychosz*

Main category: cs.CL

TL;DR: A novel dataset, SpeechMaturity, is used to train transformer models for classifying child vocalizations, outperforming previous models and achieving human-like accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of child speech recognition due to limited training data and inherent difficulties in child speech.

Method: Applied the SpeechMaturity dataset to state-of-the-art transformer models for classifying child vocalizations (cry, laughter, mature, immature speech).

Result: Models trained on the dataset outperformed previous models, achieved human-comparable accuracy, and were robust across diverse settings.

Conclusion: The SpeechMaturity dataset significantly improves child speech classification, demonstrating its effectiveness and ecological validity.

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [80] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/pdf/2506.09003)
*Lei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, Junyang Lin*

Main category: cs.CL

TL;DR: SWE-Flow is a TDD-based data synthesis framework that generates incremental development steps from unit tests, creating verifiable TDD tasks and a benchmark (SWE-Flow-Eval).


<details>
  <summary>Details</summary>
Motivation: Existing software engineering data relies on human-submitted issues; SWE-Flow automates the process by inferring development steps from unit tests.

Method: Constructs a Runtime Dependency Graph (RDG) to capture function interactions, generating structured development schedules with partial codebases, tests, and modifications.

Result: Produced 16,061 training and 2,020 test instances, improving TDD-based coding performance when fine-tuning models.

Conclusion: SWE-Flow enables scalable, verifiable TDD task generation and releases resources for further research.

Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in
Test-Driven Development (TDD). Unlike existing software engineering data that
rely on human-submitted issues, **SWE-Flow** automatically infers incremental
development steps directly from unit tests, which inherently encapsulate
high-level requirements. The core of **SWE-Flow** is the construction of a
Runtime Dependency Graph (RDG), which precisely captures function interactions,
enabling the generation of a structured, step-by-step *development schedule*.
At each step, **SWE-Flow** produces a partial codebase, the corresponding unit
tests, and the necessary code modifications, resulting in fully verifiable TDD
tasks. With this approach, we generated 16,061 training instances and 2,020
test instances from real-world GitHub projects, creating the **SWE-Flow-Eval**
benchmark. Our experiments show that fine-tuning open model on this dataset
significantly improves performance in TDD-based coding. To facilitate further
research, we release all code, datasets, models, and Docker images at
[Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [81] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/pdf/2506.09009)
*Hakyung Sung, Gyu-Ho Shin, Chanyoung Lee, You Kyung Sung, Boo Kyung Jung*

Main category: cs.CL

TL;DR: A semi-automated framework for aligning XPOS-UPOS in L2 Korean improves annotation consistency and model accuracy.


<details>
  <summary>Details</summary>
Motivation: Extend Universal Dependencies annotations for L2 Korean and enhance morphosyntactic analysis.

Method: Introduce a semi-automated framework for XPOS-UPOS alignment, expand the L2-Korean corpus, and evaluate models with/without alignments.

Result: Aligned dataset improves consistency, morphosyntactic tagging, and dependency-parsing accuracy, especially with limited data.

Conclusion: XPOS-UPOS alignment benefits L2 Korean analysis, offering better performance in NLP tasks.

Abstract: The present study extends recent work on Universal Dependencies annotations
for second-language (L2) Korean by introducing a semi-automated framework that
identifies morphosyntactic constructions from XPOS sequences and aligns those
constructions with corresponding UPOS categories. We also broaden the existing
L2-Korean corpus by annotating 2,998 new sentences from argumentative essays.
To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean
morphosyntactic analysis models on datasets both with and without these
alignments, using two NLP toolkits. Our results indicate that the aligned
dataset not only improves consistency across annotation layers but also
enhances morphosyntactic tagging and dependency-parsing accuracy, particularly
in cases of limited annotated data.

</details>


### [82] [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/pdf/2506.09014)
*Jianing Qi, Xi Ye, Hao Tang, Zhigang Zhu, Eunsol Choi*

Main category: cs.CL

TL;DR: A compact LLM, Sample Set Aggregator (SSA), is trained to aggregate multiple samples for improved answer accuracy, outperforming other test-time scaling methods.


<details>
  <summary>Details</summary>
Motivation: To enhance performance of large language models (LLMs) by efficiently leveraging multiple samples for better answer aggregation.

Method: Train SSA to concatenate and analyze multiple samples, optimizing accuracy with reinforcement learning.

Result: SSA outperforms methods like reward model-based re-ranking and generalizes well across tasks and model scales.

Conclusion: SSA provides an efficient, scalable solution for aggregating LLM outputs, compatible with black-box models.

Abstract: Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.

</details>


### [83] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/pdf/2506.09021)
*Hakyung Sung, Karla Csuros, Min-Chang Sung*

Main category: cs.CL

TL;DR: Human and LLM proofreading improve intelligibility in second language writing, with LLMs showing more generative changes. Outcomes are consistent across three LLM models.


<details>
  <summary>Details</summary>
Motivation: To compare the effectiveness and consistency of human and LLM proofreading in enhancing second language writing intelligibility.

Method: Examined lexical and syntactic interventions in identical second language writings by humans and three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b).

Result: Both human and LLM proofreading improved bigram lexical features. LLMs were more generative, using diverse vocabulary and sentence structures. Outcomes were consistent across models.

Conclusion: LLM proofreading is effective and consistent, offering generative improvements in vocabulary and syntax, though human proofreading remains valuable.

Abstract: This study examines the lexical and syntactic interventions of human and LLM
proofreading aimed at improving overall intelligibility in identical second
language writings, and evaluates the consistency of outcomes across three LLMs
(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and
LLM proofreading enhance bigram lexical features, which may contribute to
better coherence and contextual connectedness between adjacent words. However,
LLM proofreading exhibits a more generative approach, extensively reworking
vocabulary and sentence structures, such as employing more diverse and
sophisticated vocabulary and incorporating a greater number of adjective
modifiers in noun phrases. The proofreading outcomes are highly consistent in
major lexical and syntactic features across the three models.

</details>


### [84] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/pdf/2506.09033)
*Haozhen Zhang, Tao Feng, Jiaxuan You*

Main category: cs.CL

TL;DR: Router-R1 is an RL-based framework for multi-LLM routing, outperforming baselines by dynamically invoking models and optimizing performance-cost trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing LLM routers lack the ability to handle complex tasks requiring multiple LLMs' complementary strengths.

Method: Router-R1 uses RL to interleave deliberation and dynamic model invocation, guided by a rule-based reward system.

Result: Outperforms baselines on QA benchmarks, with strong generalization and cost management.

Conclusion: Router-R1 advances multi-LLM routing by optimizing performance-cost trade-offs via RL.

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [85] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/pdf/2506.09047)
*Yaniv Nikankin, Dana Arad, Yossi Gandelsman, Yonatan Belinkov*

Main category: cs.CL

TL;DR: The paper investigates the accuracy gap between visual and textual tasks in Vision-Language Models (VLMs), attributing it to disjoint but functionally similar circuits in different modalities. A simple intervention—patching visual data tokens from later layers into earlier ones—reduces the performance gap by a third.


<details>
  <summary>Details</summary>
Motivation: VLMs perform better on textual tasks than visual ones, despite similar functionalities. The study aims to understand and mitigate this gap by analyzing modality-specific circuits.

Method: The authors compare circuits in VLMs for visual and textual tasks, identify misalignments in data representations, and propose patching visual tokens from later layers into earlier ones to improve performance.

Result: Patching visual representations reduces the performance gap by a third on average across multiple tasks and models.

Conclusion: The study highlights the multi-modal performance gap in VLMs and offers a training-free method to address it, improving visual task accuracy.

Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions
on visual inputs (e.g., counting objects in an image), yet demonstrate higher
accuracies when performing an analogous task on text (e.g., counting words in a
text). We investigate this accuracy gap by identifying and comparing the
\textit{circuits} - the task-specific computational sub-graphs - in different
modalities. We show that while circuits are largely disjoint between
modalities, they implement relatively similar functionalities: the differences
lie primarily in processing modality-specific data positions (an image or a
text sequence). Zooming in on the image data representations, we observe they
become aligned with the higher-performing analogous textual representations
only towards later layers, too late in processing to effectively influence
subsequent positions. To overcome this, we patch the representations of visual
data tokens from later layers back into earlier layers. In experiments with
multiple tasks and models, this simple intervention closes a third of the
performance gap between the modalities, on average. Our analysis sheds light on
the multi-modal performance gap in VLMs and suggests a training-free approach
for reducing it.

</details>


### [86] [A Decomposition-Based Approach for Evaluating and Analyzing Inter-Annotator Disagreement](https://arxiv.org/pdf/2206.05446)
*Effi Levi, Shaul R. Shenhav*

Main category: cs.CL

TL;DR: A novel method decomposes annotations into levels to analyze inter-annotator disagreements separately, using theoretical and exploration-based strategies.


<details>
  <summary>Details</summary>
Motivation: To better understand and analyze sources of annotation disagreements and reveal latent structures in annotation tasks.

Method: Two strategies: theoretically-driven (researcher-defined decomposition) and exploration-based (inductive computation of decompositions).

Result: Demonstrated potential in testing hypotheses about annotation disagreements and uncovering latent structures in narrative analysis.

Conclusion: The approach can be extended, generalized, and adapted for other purposes.

Abstract: We propose a novel method to conceptually decompose an existing annotation
into separate levels, allowing the analysis of inter-annotators disagreement in
each level separately. We suggest two distinct strategies in order to actualize
this approach: a theoretically-driven one, in which the researcher defines a
decomposition based on prior knowledge of the annotation task, and an
exploration-based one, in which many possible decompositions are inductively
computed and presented to the researcher for interpretation and evaluation.
Utilizing a recently constructed dataset for narrative analysis as our
use-case, we apply each of the two strategies to demonstrate the potential of
our approach in testing hypotheses regarding the sources of annotation
disagreements, as well as revealing latent structures and relations within the
annotation task. We conclude by suggesting how to extend and generalize our
approach, as well as use it for other purposes.

</details>


### [87] [A Survey on Long Text Modeling with Transformers](https://arxiv.org/pdf/2302.14502)
*Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao*

Main category: cs.CL

TL;DR: A survey on recent advances in long text modeling using Transformer models, covering input processing, architecture improvements, and applications.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of modeling long texts in NLP due to complex semantics and length limitations.

Method: Review and discuss methods for processing long inputs, extending Transformer architectures, and adapting them for long text characteristics.

Result: Provides an overview of techniques and applications for long text modeling.

Conclusion: Summarizes current advancements and suggests future research directions in long text modeling.

Abstract: Modeling long texts has been an essential technique in the field of natural
language processing (NLP). With the ever-growing number of long documents, it
is important to develop effective modeling methods that can process and analyze
such texts. However, long texts pose important research challenges for existing
text models, with more complex semantics and special characteristics. In this
paper, we provide an overview of the recent advances on long texts modeling
based on Transformer models. Firstly, we introduce the formal definition of
long text modeling. Then, as the core content, we discuss how to process long
input to satisfy the length limitation and design improved Transformer
architectures to effectively extend the maximum context length. Following this,
we discuss how to adapt Transformer models to capture the special
characteristics of long texts. Finally, we describe four typical applications
involving long text modeling and conclude this paper with a discussion of
future directions. Our survey intends to provide researchers with a synthesis
and pointer to related work on long text modeling.

</details>


### [88] [Cross-lingual Transfer in Programming Languages: An Extensive Empirical Study](https://arxiv.org/pdf/2310.16937)
*Razan Baltaji, Saurabh Pujar, Louis Mandel, Martin Hirzel, Luca Buratti, Lav Varshney*

Main category: cs.CL

TL;DR: The paper explores transfer learning to improve LLM performance on low-resource programming languages by leveraging high-resource language data, demonstrating its effectiveness over zero-shot learning and providing a predictive model for optimal source languages.


<details>
  <summary>Details</summary>
Motivation: The scarcity of openly available code for low-resource programming languages like COBOL, Rust, and Swift limits LLM effectiveness, increasing software maintenance costs and hindering innovation.

Method: The study evaluates transfer learning across 10 to 41 programming languages and five tasks (code generation, clone detection, etc.), develops a performance prediction model, and tests generalizability with a larger LLM.

Result: Cross-lingual transfer outperforms zero-shot learning, with effectiveness varying by language pair. The predictive model reliably identifies optimal source languages.

Conclusion: The work aids LLM tool development for low-resource languages and offers insights into transferable features across languages.

Abstract: Large language models (LLMs) have achieved state-of-the-art performance in
various software engineering tasks, including error detection, clone detection,
and code translation, primarily leveraging high-resource programming languages
like Python and Java. However, many critical languages, such as COBOL, as well
as emerging languages, such as Rust and Swift, remain low-resource due to
limited openly available code. This scarcity hampers the training and
effectiveness of LLMs for these languages, increasing software maintenance
costs and stifling innovation. Addressing this gap, we investigate the
potential of transfer learning to enhance LLM performance on low-resource
programming languages by leveraging data from high-resource counterparts. Our
extensive empirical study evaluates transferability across 10 to 41 programming
languages and five key tasks: code generation, clone detection, code repair,
solution domain classification, and error detection. Additionally, we develop a
performance prediction model to guess the best source languages for a given
target and task, and analyze the features that influence transfer performance.
We further replicate a representative subset of experiments with a larger model
to test the generalizability of our conclusions to contemporary large-scale
LLMs. Our findings demonstrate that cross-lingual transfer significantly
outperforms zero-shot learning, with effectiveness varying based on both source
and target languages. Furthermore, our model reliably predicts successful
transfer sources by considering linguistic and dataset-specific features,
offering practical guidance for data acquisition and model training. This work
contributes to the development of LLM-driven tools for low-resource programming
languages and provides insights into the characteristics that facilitate
transfer across language pairs.

</details>


### [89] [Poro 34B and the Blessing of Multilinguality](https://arxiv.org/pdf/2404.01856)
*Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville Komulainen, Väinö Hatanpää, Peter Sarlin, Sampo Pyysalo*

Main category: cs.CL

TL;DR: Multilingual training can enhance model performance for low-resource languages like Finnish, as shown by Poro 34B, which outperforms monolingual models while maintaining competitive results in English and programming languages.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of training data for most languages by leveraging multilingual datasets to improve model capabilities.

Method: Train Poro 34B, a 34B parameter model, on 1T tokens of Finnish, English, and programming languages.

Result: Poro 34B advances Finnish model capabilities, excels in translation, and performs competitively in English and programming tasks.

Conclusion: Multilingual training is beneficial for low-resource languages, and Poro 34B demonstrates its effectiveness.

Abstract: The pretraining of state-of-the-art large language models now requires
trillions of words of text, which is orders of magnitude more than available
for the vast majority of languages. While including text in more than one
language is an obvious way to acquire more pretraining data, multilinguality is
often seen as a curse, and most model training efforts continue to focus
near-exclusively on individual large languages. We believe that multilinguality
can be a blessing: when the lack of training data is a constraint for
effectively training larger models for a target language, augmenting the
dataset with other languages can offer a way to improve over the capabilities
of monolingual models for that language. In this study, we introduce Poro 34B,
a 34 billion parameter model trained for 1 trillion tokens of Finnish, English,
and programming languages, and demonstrate that a multilingual training
approach can produce a model that substantially advances over the capabilities
of existing models for Finnish and excels in translation, while also achieving
competitive performance in its class for English and programming languages. We
release the model parameters, scripts, and data under open licenses at
https://huggingface.co/LumiOpen/Poro-34B.

</details>


### [90] [P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts](https://arxiv.org/pdf/2406.12548)
*Yuhao Dan, Jie Zhou, Qin Chen, Junfeng Tian, Liang He*

Main category: cs.CL

TL;DR: The paper introduces P-React, a personalized LLM that models Big Five personality traits using a MoE-based approach and a novel Personality Specialization Loss (PSL). It also presents OCEAN-Chat, a dataset for training LLMs in personality expression.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs focus on explicit character profiles, neglecting underlying personality traits, limiting anthropomorphic and psychologically-grounded AI development.

Method: Proposes P-React, a MoE-based LLM with PSL to capture trait expressions, and introduces OCEAN-Chat dataset for training.

Result: P-React effectively maintains consistent and realistic personality traits, as demonstrated in experiments.

Conclusion: The work advances personalized LLMs by integrating psychological traits, offering a more nuanced and grounded personality simulacrum.

Abstract: Personalized large language models (LLMs) have attracted great attention in
many applications, such as emotional support and role-playing. However,
existing works primarily focus on modeling explicit character profiles, while
ignoring the underlying personality traits that truly shape behaviors and
decision-making, hampering the development of more anthropomorphic and
psychologically-grounded AI systems. In this paper, we explore the modeling of
Big Five personality traits, which is the most widely used trait theory in
psychology, and propose P-React, a mixture of experts (MoE)-based personalized
LLM. Particularly, we integrate a Personality Specialization Loss (PSL) to
better capture individual trait expressions, providing a more nuanced and
psychologically grounded personality simulacrum. To facilitate research in this
field, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to
train LLMs in expressing personality traits across diverse topics. Extensive
experiments demonstrate the effectiveness of P-React in maintaining consistent
and real personality.

</details>


### [91] [SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs](https://arxiv.org/pdf/2406.19593)
*Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard*

Main category: cs.CL

TL;DR: SK-VQA is a synthetic multimodal dataset for context-augmented generation, improving MLLMs' performance in KB-VQA and multimodal RAG tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack design for context-augmented generation, limiting their effectiveness in KB-VQA. Synthetic data for this purpose is underexplored.

Method: Introduces SK-VQA, a synthetic dataset with 2M+ visual QA pairs and context documents, designed for training and benchmarking MLLMs.

Result: SK-VQA enhances MLLMs' generalization in context-aware VQA and multimodal RAG, validated by human evaluations and experiments.

Conclusion: SK-VQA is a high-quality, scalable resource for advancing MLLMs in context-augmented generation, publicly available for further research.

Abstract: Multimodal retrieval augmented generation (RAG) plays a crucial role in
domains such as knowledge-based visual question answering (KB-VQA), where
external knowledge is needed to answer a question. However, existing multimodal
LLMs (MLLMs) are not designed for context-augmented generation, limiting their
effectiveness in such tasks. While synthetic data generation has recently
gained attention for training MLLMs, its application for context-augmented
generation remains underexplored. To address this gap, we introduce SK-VQA, a
large-scale synthetic multimodal dataset containing over 2 million visual
question-answer pairs, each associated with context documents containing
information necessary to determine the final answer. Compared to previous
datasets, SK-VQA contains 11x more unique questions, exhibits greater domain
diversity, and covers a broader spectrum of image sources. Through human
evaluations, we confirm the high quality of the generated question-answer pairs
and their contextual relevance. Extensive experiments show that SK-VQA serves
both as a challenging KB-VQA benchmark and as an effective training resource
for adapting MLLMs to context-augmented generation. Our results further
indicate that models trained on SK-VQA demonstrate enhanced generalization in
both context-aware VQA and multimodal RAG settings. SK-VQA is publicly
available via Hugging Face Hub.

</details>


### [92] [High-Throughput Phenotyping of Clinical Text Using Large Language Models](https://arxiv.org/pdf/2408.01214)
*Daniel B. Hier, S. Ilyas Munzir, Anne Stahlfeld, Tayo Obafemi-Ajayi, Michael D. Carrithers*

Main category: cs.CL

TL;DR: GPT-4 outperforms GPT-3.5-Turbo in automating phenotyping of clinical summaries, achieving results close to manual annotation, with high generalizability and no need for training data.


<details>
  <summary>Details</summary>
Motivation: To evaluate the automation of phenotyping in clinical summaries using large language models, leveraging OMIM database data as surrogates for physician notes.

Method: Performance comparison of GPT-4 and GPT-3.5-Turbo in identifying, categorizing, and normalizing signs from OMIM clinical summaries.

Result: GPT-4 surpasses GPT-3.5-Turbo, achieving concordance with manual annotators comparable to inter-rater agreement, though with some limitations in sign normalization.

Conclusion: Large language models, especially GPT-4, are poised to dominate high-throughput phenotyping of clinical text due to their performance and generalizability.

Abstract: High-throughput phenotyping automates the mapping of patient signs to
standardized ontology concepts and is essential for precision medicine. This
study evaluates the automation of phenotyping of clinical summaries from the
Online Mendelian Inheritance in Man (OMIM) database using large language
models. Due to their rich phenotype data, these summaries can be surrogates for
physician notes. We conduct a performance comparison of GPT-4 and
GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in
identifying, categorizing, and normalizing signs, achieving concordance with
manual annotators comparable to inter-rater agreement. Despite some limitations
in sign normalization, the extensive pre-training of GPT-4 results in high
performance and generalizability across several phenotyping tasks while
obviating the need for manually annotated training data. Large language models
are expected to be the dominant method for automating high-throughput
phenotyping of clinical text.

</details>


### [93] [Guidelines for Fine-grained Sentence-level Arabic Readability Annotation](https://arxiv.org/pdf/2410.08674)
*Nizar Habash, Hanada Taha-Thomure, Khalid N. Elmadani, Zeina Zeino, Abdallah Abushmaes*

Main category: cs.CL

TL;DR: The paper introduces BAREC, a large-scale Arabic readability corpus with 69,441 sentences labeled across 19 levels, refined with educator input, and reports high annotator agreement (81.8% Kappa). It benchmarks readability models and makes the corpus public.


<details>
  <summary>Details</summary>
Motivation: To create a fine-grained, large-scale resource for Arabic readability assessment, addressing the lack of such datasets and refining guidelines with educator expertise.

Method: Developed BAREC using the Taha/Arabi21 framework, iteratively refined guidelines with native Arabic educators, and evaluated inter-annotator agreement. Benchmarked automatic readability models at various granularities.

Result: Achieved high inter-annotator agreement (81.8% Quadratic Weighted Kappa) and provided benchmarks for readability models across 19-, 7-, 5-, and 3-level classifications.

Conclusion: BAREC is a valuable, publicly available resource for Arabic readability research, with robust guidelines and high-quality annotations.

Abstract: This paper presents the annotation guidelines of the Balanced Arabic
Readability Evaluation Corpus (BAREC), a large-scale resource for fine-grained
sentence-level readability assessment in Arabic. BAREC includes 69,441
sentences (1M+ words) labeled across 19 levels, from kindergarten to
postgraduate. Based on the Taha/Arabi21 framework, the guidelines were refined
through iterative training with native Arabic-speaking educators. We highlight
key linguistic, pedagogical, and cognitive factors in determining readability
and report high inter-annotator agreement: Quadratic Weighted Kappa 81.8%
(substantial/excellent agreement) in the last annotation phase. We also
benchmark automatic readability models across multiple classification
granularities (19-, 7-, 5-, and 3-level). The corpus and guidelines are
publicly available.

</details>


### [94] [Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning](https://arxiv.org/pdf/2410.15639)
*Yoichi Ishibashi, Taro Yano, Masafumi Oyamada*

Main category: cs.CL

TL;DR: Self-Developing framework enables LLMs to autonomously create and refine improvement algorithms, outperforming human-designed methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM improvement methods are limited by human design, prompting the need for autonomous self-improvement.

Method: Uses an iterative cycle of generating algorithmic candidates, evaluating them, and refining via Direct Preference Optimization.

Result: Autonomously discovered algorithms improved GSM8k performance by 6% and outperformed human-designed methods by 4.3%.

Conclusion: LLMs can autonomously develop novel optimization techniques, advancing toward self-improving methodologies.

Abstract: Large Language Models (LLMs) have achieved remarkable capabilities, yet their
improvement methods remain fundamentally constrained by human design. We
present Self-Developing, a framework that enables LLMs to autonomously
discover, implement, and refine their own improvement algorithms. Our approach
employs an iterative cycle where a seed model generates algorithmic candidates
as executable code, evaluates their effectiveness, and uses Direct Preference
Optimization to recursively improve increasingly sophisticated improvement
strategies. We demonstrate this framework through model merging, a practical
technique for combining specialized models. Self-Developing successfully
discovered novel merging algorithms that outperform existing human-designed
algorithms. On mathematical reasoning benchmarks, the autonomously discovered
algorithms improve the seed model's GSM8k performance by 6\% and exceed
human-designed approaches like Task Arithmetic by 4.3\%. Remarkably, these
algorithms exhibit strong generalization, achieving 7.4\% gains on
out-of-domain models without re-optimization. Our findings demonstrate that
LLMs can transcend their training to invent genuinely novel optimization
techniques. This capability represents a crucial step toward a new era where
LLMs not only solve problems but autonomously develop the methodologies for
their own advancement.

</details>


### [95] [FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs](https://arxiv.org/pdf/2410.19317)
*Zhiting Fan, Ruizhe Chen, Tianxiang Hu, Zuozhu Liu*

Main category: cs.CL

TL;DR: The paper introduces FairMT-Bench, a fairness benchmark for LLMs in multi-turn dialogues, highlighting biases and performance variations across tasks and models.


<details>
  <summary>Details</summary>
Motivation: Address fairness concerns in LLMs, especially in multi-turn dialogues, which are more complex and prone to bias accumulation than single-turn scenarios.

Method: Develop FairMT-Bench with a task taxonomy (context understanding, user interaction, instruction trade-offs) and construct FairMT-10K dataset. Evaluate using GPT-4, bias classifiers, and human validation.

Result: LLMs exhibit more biased responses in multi-turn dialogues, with significant performance variations. FairMT-1K dataset tests 15 SOTA LLMs, revealing current fairness gaps.

Conclusion: FairMT-Bench is a novel tool for assessing LLM fairness in realistic multi-turn dialogues, urging future work to improve fairness and adopt FairMT-1K.

Abstract: The growing use of large language model (LLM)-based chatbots has raised
concerns about fairness. Fairness issues in LLMs can lead to severe
consequences, such as bias amplification, discrimination, and harm to
marginalized communities. While existing fairness benchmarks mainly focus on
single-turn dialogues, multi-turn scenarios, which in fact better reflect
real-world conversations, present greater challenges due to conversational
complexity and potential bias accumulation. In this paper, we propose a
comprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios,
\textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM
fairness capabilities across three stages: context understanding, user
interaction, and instruction trade-offs, with each stage comprising two tasks.
To ensure coverage of diverse bias types and attributes, we draw from existing
fairness datasets and employ our template to construct a multi-turn dialogue
dataset, \texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias
classifiers including Llama-Guard-3 and human validation to ensure robustness.
Experiments and analyses on \texttt{FairMT-10K} reveal that in multi-turn
dialogue scenarios, current LLMs are more likely to generate biased responses,
and there is significant variation in performance across different tasks and
models. Based on this, we curate a challenging dataset, \texttt{FairMT-1K}, and
test 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show
the current state of fairness in LLMs and showcase the utility of this novel
approach for assessing fairness in more realistic multi-turn dialogue contexts,
calling for future work to focus on LLM fairness improvement and the adoption
of \texttt{FairMT-1K} in such efforts.

</details>


### [96] [SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script](https://arxiv.org/pdf/2410.20682)
*Eunwon Kim, Chanho Park, Buru Chang*

Main category: cs.CL

TL;DR: The paper introduces SHARE, a long-term dialogue dataset from movie scripts, and EPISODE, a framework leveraging shared memories to enhance dialogue engagement.


<details>
  <summary>Details</summary>
Motivation: To improve long-term dialogue engagement by utilizing shared memories between individuals.

Method: Constructed the SHARE dataset from movie scripts, capturing explicit and implicit shared memories. Developed EPISODE, a framework to manage and utilize these memories in dialogue.

Result: Shared memories enhance dialogue engagement and sustainability; EPISODE effectively manages these memories.

Conclusion: The study demonstrates the value of shared memories in long-term dialogue and provides tools (SHARE and EPISODE) for further research.

Abstract: Shared memories between two individuals strengthen their bond and are crucial
for facilitating their ongoing conversations. This study aims to make long-term
dialogue more engaging by leveraging these shared memories. To this end, we
introduce a new long-term dialogue dataset named SHARE, constructed from movie
scripts, which are a rich source of shared memories among various
relationships. Our dialogue dataset contains the summaries of persona
information and events of two individuals, as explicitly revealed in their
conversation, along with implicitly extractable shared memories. We also
introduce EPISODE, a long-term dialogue framework based on SHARE that utilizes
shared experiences between individuals. Through experiments using SHARE, we
demonstrate that shared memories between two individuals make long-term
dialogues more engaging and sustainable, and that EPISODE effectively manages
shared memories during dialogue. Our dataset and code are available at
https://github.com/e1kim/SHARE.

</details>


### [97] [Length-Induced Embedding Collapse in PLM-based Models](https://arxiv.org/pdf/2410.24200)
*Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu*

Main category: cs.CL

TL;DR: The paper identifies Length Collapse, where embeddings of longer texts cluster together, degrading performance. It proposes TempScale to mitigate this, improving performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the performance degradation of text embeddings from PLM-based models on longer texts due to Length Collapse.

Method: Theoretical analysis of self-attention as a low-pass filter and introduction of TempScale to balance filtering rates.

Result: TempScale improves performance by 0.94% on MTEB and 1.10% on LongEmbed.

Conclusion: TempScale effectively mitigates Length Collapse, enhancing embedding consistency and performance across text lengths.

Abstract: Text embeddings from PLM-based models enable a wide range of applications,
yet their performance often degrades on longer texts. In this paper, we
introduce a phenomenon we call Length Collapse, where embeddings of longer
texts tend to cluster together. This clustering results in a distributional
inconsistency between the embeddings of short and long texts. We further
investigate how these differences contribute to the performance decline
observed with longer texts across various downstream tasks. Through a rigorous
theoretical analysis of the self-attention mechanism, which acts as a low-pass
filter in PLM-based models, we demonstrate that as text length increases, the
strength of low-pass filtering intensifies, causing embeddings to retain more
low-frequency components. As a result, input token features become more
similar, leading to clustering and ultimately the collapse of embeddings for
longer texts. To address this issue, we propose a simple method, TempScale,
which mitigates the Length Collapse phenomenon. By narrowing the gap in
low-pass filtering rates between long and short texts, TempScale ensures more
consistent embeddings across different text lengths. This approach leads to
performance improvements of 0.94% on MTEB and 1.10% on LongEmbed, which focuses
specifically on long-context retrieval, providing strong evidence for the
validity of our analysis. The source code is available at
https://github.com/Yuqi-Zhou/Length_Collapse.

</details>


### [98] [The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games](https://arxiv.org/pdf/2411.15129)
*Alessandro Trevisan, Harry Giddens, Sarah Dillon, Alan F. Blackwell*

Main category: cs.CL

TL;DR: The paper analyzes ChatGPT's language, comparing it to scientific texts and exploring its resemblance to 'bullshit' in political and workplace contexts.


<details>
  <summary>Details</summary>
Motivation: To understand the nature of language produced by LLM-based chatbots like ChatGPT, especially in light of criticisms labeling it as 'slop' or 'bullshit.'

Method: Empirical study comparing 1,000 scientific publications with ChatGPT-generated text, and analyzing language features in political speech and workplace dysfunction.

Result: A statistical model shows ChatGPT's 'bullshit' aligns with natural human language patterns in political and workplace contexts.

Conclusion: ChatGPT's language mirrors dysfunctional human communication, validating concerns about its 'bullshit' nature.

Abstract: What can we learn about language from studying how it is used by ChatGPT and
other large language model (LLM)-based chatbots? In this paper, we analyse the
distinctive character of language generated by ChatGPT, in relation to
questions raised by natural language processing pioneer, and student of
Wittgenstein, Margaret Masterman. Following frequent complaints that LLM-based
chatbots produce "slop," or even "bullshit," in the sense of Frankfurt's
popular monograph On Bullshit, we conduct an empirical study to contrast the
language of 1,000 scientific publications with typical text generated by
ChatGPT. We then explore whether the same language features can be detected in
two well-known contexts of social dysfunction: George Orwell's critique of
political speech, and David Graeber's characterisation of bullshit jobs. Using
simple hypothesis-testing methods, we demonstrate that a statistical model of
sloppy bullshit can reliably relate the Frankfurtian artificial bullshit of
ChatGPT to the political and workplace functions of bullshit as observed in
natural human language.

</details>


### [99] [From Language Models over Tokens to Language Models over Characters](https://arxiv.org/pdf/2412.03719)
*Tim Vieira, Ben LeBrun, Mario Giulianelli, Juan Luis Gastaldi, Brian DuSell, John Terilla, Timothy J. O'Donnell, Ryan Cotterell*

Main category: cs.CL

TL;DR: The paper presents algorithms to convert token-level language models to character-level ones, addressing challenges in prompt specification and tokenization sensitivity.


<details>
  <summary>Details</summary>
Motivation: Token-level language models pose challenges for user applications due to their sensitivity to prompt specifications (e.g., spaces). Converting them to character-level models can mitigate these issues.

Method: The paper introduces exact and approximate algorithms for converting token-level to character-level models. It benchmarks runtime and approximation quality.

Result: Empirical results show the method accurately approximates character-level distributions with fast speeds and improves compression rates (bits/byte).

Conclusion: The proposed algorithms effectively bridge the gap between token-level and character-level language models, enhancing usability and performance.

Abstract: Modern language models are internally -- and mathematically -- distributions
over $\it{token}$ strings rather than $\it{character}$ strings, posing numerous
challenges for programmers building user applications on top of them. For
example, if a prompt is specified as a character string, it must be tokenized
before passing it to the token-level language model. Thus, the tokenizer and
consequent processing are very sensitive to the specification of the prompt
(e.g., whether the prompt ends with a space or not). This paper presents
algorithms for converting token-level language models to character-level ones.
We present both exact and approximate algorithms. In the empirical portion of
the paper, we benchmark the practical runtime and approximation quality. Across
four publicly available language models, we find that -- even with a small
computation budget -- our method is able to accurately approximate the
character-level distribution at reasonably fast speeds, and that a significant
improvement in the language model's compression rate (bits/byte) is achieved.

</details>


### [100] [JuStRank: Benchmarking LLM Judges for System Ranking](https://arxiv.org/pdf/2412.09569)
*Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden, Asaf Yehudai*

Main category: cs.CL

TL;DR: The paper proposes using LLM-based judges to evaluate generative AI models, emphasizing the need to validate their quality for system-level ranking, and conducts a large-scale study to assess judge behavior.


<details>
  <summary>Details</summary>
Motivation: The rapid progress of generative AI requires systematic evaluation of models, but existing methods overlook system-level biases in LLM judges.

Method: Aggregates judgment scores over multiple system outputs to generate system rankings, comparing them to human-based rankings for validation.

Result: Provides insights into LLM judge behavior, including decisiveness and bias, beyond instance-based evaluations.

Conclusion: Highlights the importance of validating LLM judges for system-level ranking and offers a detailed analysis of their performance.

Abstract: Given the rapid progress of generative AI, there is a pressing need to
systematically compare and choose between the numerous models and
configurations available. The scale and versatility of such evaluations make
the use of LLM-based judges a compelling solution for this challenge.
Crucially, this approach requires first to validate the quality of the LLM
judge itself. Previous work has focused on instance-based assessment of LLM
judges, where a judge is evaluated over a set of responses, or response pairs,
while being agnostic to their source systems. We argue that this setting
overlooks critical factors affecting system-level ranking, such as a judge's
positive or negative bias towards certain systems. To address this gap, we
conduct the first large-scale study of LLM judges as system rankers. System
scores are generated by aggregating judgment scores over multiple system
outputs, and the judge's quality is assessed by comparing the resulting system
ranking to a human-based ranking. Beyond overall judge assessment, our analysis
provides a fine-grained characterization of judge behavior, including their
decisiveness and bias.

</details>


### [101] [Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence](https://arxiv.org/pdf/2412.13949)
*Jinghan He, Kuan Zhu, Haiyun Guo, Junfeng Fang, Zhenglin Hua, Yuheng Jia, Ming Tang, Tat-Seng Chua, Jinqiao Wang*

Main category: cs.CL

TL;DR: The paper investigates hallucinations in large vision-language models (LVLMs), attributing them to overreliance on language patterns. It introduces Vision-aware Head Divergence (VHD) to measure attention head sensitivity to visuals and proposes Vision-aware Head Reinforcement (VHR) to mitigate hallucinations without training.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LVLMs undermine accuracy and reliability. Existing methods address symptoms but not root causes, prompting a deeper investigation into internal mechanisms.

Method: The study introduces VHD to quantify attention head sensitivity to visuals and proposes VHR, a training-free method to enhance vision-aware attention heads.

Result: VHR outperforms state-of-the-art methods in reducing hallucinations while maintaining efficiency with minimal time overhead.

Conclusion: The work identifies vision-aware attention heads as key to mitigating hallucinations and offers a practical solution (VHR) to improve LVLM reliability.

Abstract: Large vision-language models (LVLMs) have made substantial progress in
integrating large language models (LLMs) with visual inputs, enabling advanced
multimodal reasoning. Despite their success, a persistent challenge is
hallucination-where generated text fails to accurately reflect visual
content-undermining both accuracy and reliability. Existing methods focus on
alignment training or decoding refinements but primarily address symptoms at
the generation stage without probing the underlying causes. In this work, we
investigate the internal mechanisms driving hallucination in LVLMs, with an
emphasis on the multi-head attention module. Specifically, we introduce
Vision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of
attention head outputs to visual context. Based on this, our findings reveal
the presence of vision-aware attention heads that are more attuned to visual
information; however, the model's overreliance on its prior language patterns
is closely related to hallucinations. Building on these insights, we propose
Vision-aware Head Reinforcement (VHR), a training-free approach to mitigate
hallucination by enhancing the role of vision-aware attention heads. Extensive
experiments demonstrate that our method achieves superior performance compared
to state-of-the-art approaches in mitigating hallucinations, while maintaining
high efficiency with negligible additional time overhead.

</details>


### [102] [Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models](https://arxiv.org/pdf/2502.02444)
*Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song*

Main category: cs.CL

TL;DR: The study introduces GPLA, a method to create a psychologically grounded value system for LLMs, outperforming Schwartz's values in capturing LLM values and improving alignment.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of a psychologically grounded value system for LLMs, despite their growing influence and concerns about their intrinsic values.

Method: Developed the Generative Psycho-Lexical Approach (GPLA) to construct a five-factor value system for LLMs, validated through benchmarking tasks combining psychology and AI.

Result: The proposed value system meets psychological criteria, better captures LLM values, improves safety prediction, and enhances alignment compared to Schwartz's values.

Conclusion: GPLA offers a scalable and adaptable solution for understanding and aligning LLM values, bridging psychology and AI priorities.

Abstract: Values are core drivers of individual and collective perception, cognition,
and behavior. Value systems, such as Schwartz's Theory of Basic Human Values,
delineate the hierarchy and interplay among these values, enabling
cross-disciplinary investigations into decision-making and societal dynamics.
Recently, the rise of Large Language Models (LLMs) has raised concerns
regarding their elusive intrinsic values. Despite growing efforts in
evaluating, understanding, and aligning LLM values, a psychologically grounded
LLM value system remains underexplored. This study addresses the gap by
introducing the Generative Psycho-Lexical Approach (GPLA), a scalable,
adaptable, and theoretically informed method for constructing value systems.
Leveraging GPLA, we propose a psychologically grounded five-factor value system
tailored for LLMs. For systematic validation, we present three benchmarking
tasks that integrate psychological principles with cutting-edge AI priorities.
Our results reveal that the proposed value system meets standard psychological
criteria, better captures LLM values, improves LLM safety prediction, and
enhances LLM alignment, when compared to the canonical Schwartz's values.

</details>


### [103] [Position: Editing Large Language Models Poses Serious Safety Risks](https://arxiv.org/pdf/2502.02958)
*Paul Youssef, Zhixue Zhao, Daniel Braun, Jörg Schlötterer, Christin Seifert*

Main category: cs.CL

TL;DR: The paper highlights safety risks of knowledge editing in LLMs, warning of malicious uses and calling for tamper-resistant models and ecosystem security.


<details>
  <summary>Details</summary>
Motivation: To address overlooked safety risks posed by knowledge editing methods in LLMs, which are attractive to malicious actors due to their accessibility and stealth.

Method: Analyzes vulnerabilities in the AI ecosystem, discusses malicious use cases of KEs, and identifies gaps in social and institutional awareness.

Result: Identifies serious risks from unrestricted model editing and lack of safeguards, urging action to mitigate these threats.

Conclusion: Calls for research into tamper-resistant models and proactive measures to secure the AI ecosystem against malicious editing.

Abstract: Large Language Models (LLMs) contain large amounts of facts about the world.
These facts can become outdated over time, which has led to the development of
knowledge editing methods (KEs) that can change specific facts in LLMs with
limited side effects. This position paper argues that editing LLMs poses
serious safety risks that have been largely overlooked. First, we note the fact
that KEs are widely available, computationally inexpensive, highly performant,
and stealthy makes them an attractive tool for malicious actors. Second, we
discuss malicious use cases of KEs, showing how KEs can be easily adapted for a
variety of malicious purposes. Third, we highlight vulnerabilities in the AI
ecosystem that allow unrestricted uploading and downloading of updated models
without verification. Fourth, we argue that a lack of social and institutional
awareness exacerbates this risk, and discuss the implications for different
stakeholders. We call on the community to (i) research tamper-resistant models
and countermeasures against malicious model editing, and (ii) actively engage
in securing the AI ecosystem.

</details>


### [104] [LLM Alignment as Retriever Optimization: An Information Retrieval Perspective](https://arxiv.org/pdf/2502.03699)
*Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik*

Main category: cs.CL

TL;DR: A novel direct optimization method, LarPO, is introduced for aligning Large Language Models (LLMs) using Information Retrieval (IR) principles, improving alignment quality significantly.


<details>
  <summary>Details</summary>
Motivation: LLMs need effective alignment to ensure correct, trustworthy, and ethical behavior, addressing challenges like misinformation and bias. Existing RL-based methods are complex, prompting the search for simpler alternatives.

Method: The paper proposes LarPO, a direct optimization approach for LLM alignment, mapping LLM generation and reward models to IR's retriever-reranker paradigm.

Result: LarPO achieves 38.9% and 13.7% averaged improvement on AlpacaEval2 and MixEval-Hard benchmarks, respectively.

Conclusion: The work bridges LLM alignment and IR methodologies, offering a promising direction for future research in LLM alignment.

Abstract: Large Language Models (LLMs) have revolutionized artificial intelligence with
capabilities in reasoning, coding, and communication, driving innovation across
industries. Their true potential depends on effective alignment to ensure
correct, trustworthy and ethical behavior, addressing challenges like
misinformation, hallucinations, bias and misuse. While existing Reinforcement
Learning (RL)-based alignment methods are notoriously complex, direct
optimization approaches offer a simpler alternative. In this work, we introduce
a novel direct optimization approach for LLM alignment by drawing on
established Information Retrieval (IR) principles. We present a systematic
framework that bridges LLM alignment and IR methodologies, mapping LLM
generation and reward models to IR's retriever-reranker paradigm. Building on
this foundation, we propose LLM Alignment as Retriever Preference Optimization
(LarPO), a new alignment method that enhances overall alignment quality.
Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %
averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work
opens new avenues for advancing LLM alignment by integrating IR foundations,
offering a promising direction for future research.

</details>


### [105] [In Praise of Stubbornness: An Empirical Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs](https://arxiv.org/pdf/2502.04390)
*Simone Clemente, Zied Ben Houidi, Alexis Huet, Dario Rossi, Giulio Franzese, Pietro Michiardi*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) suffer catastrophic knowledge corruption when updated with contradictory facts, unlike humans. Selective plasticity helps retain non-contradictory knowledge but fails for contradictions. Contradictions can be detected with high accuracy, suggesting a need for new architectures.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs handle contradictory updates and whether selective plasticity can mitigate catastrophic interference.

Method: Empirical study using targeted network updates (stubborn vs. plastic neurons) and testing across model scales (GPT-2 to GPT-J-6B).

Result: Contradictory updates corrupt up to 80% of unrelated knowledge; selective plasticity helps non-contradictory updates but fails for contradictions. Contradictions can be detected with 95%+ accuracy.

Conclusion: LLMs fundamentally struggle with contradictions, highlighting the need for architectures that resist destructive overwrites like humans.

Abstract: Through systematic empirical investigation, we uncover a fundamental and
concerning property of Large Language Models: while they can safely learn facts
that don't contradict their knowledge, attempting to update facts with
contradictory information triggers catastrophic corruption of unrelated
knowledge. Unlike humans, who naturally resist contradictory information, these
models indiscriminately accept contradictions, leading to devastating
interference, destroying up to 80% of unrelated knowledge even when learning as
few as 10-100 contradicting facts. To understand whether this interference
could be mitigated through selective plasticity, we experiment with targeted
network updates, distinguishing between previously used (stubborn) and rarely
used (plastic) neurons. We uncover another asymmetry: while sparing
frequently-used neurons significantly improves retention of existing knowledge
for non-contradictory updates (98% vs 93% with standard updates), contradictory
updates trigger catastrophic interference regardless of targeting strategy.
This effect which persists across tested model scales (GPT-2 to GPT-J-6B),
suggests a fundamental limitation in how neural networks handle contradictions.
Finally, we demonstrate that contradictory information can be reliably detected
(95%+ accuracy) using simple model features, offering a potential protective
mechanism. These findings motivate new architectures that can, like humans,
naturally resist contradictions rather than allowing destructive overwrites.

</details>


### [106] [Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies](https://arxiv.org/pdf/2502.05202)
*Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Gaurav Jain, Oren Pereg, Moshe Wasserblat, David Harel*

Main category: cs.CL

TL;DR: New speculative decoding methods remove shared-vocabulary constraints, enabling any off-the-shelf model as a drafter, achieving up to 2.8x speedup without retraining.


<details>
  <summary>Details</summary>
Motivation: Accelerating LLM inference is crucial for generative AI, but existing SD methods limit drafter selection due to shared-vocabulary requirements.

Method: Three new SD methods are introduced, eliminating the need for shared vocabulary between drafter and target models, ensuring lossless decoding.

Result: Empirical tests show speedups of up to 2.8x on tasks like summarization and programming, using off-the-shelf models.

Conclusion: This work expands SD's practicality by allowing any model as a drafter, enhancing efficiency without additional training.

Abstract: Accelerating the inference of large language models (LLMs) is a critical
challenge in generative AI. Speculative decoding (SD) methods offer substantial
efficiency gains by generating multiple tokens using a single target forward
pass. However, existing SD approaches require the drafter and target models to
share the same vocabulary, thus limiting the pool of possible drafters, often
necessitating the training of a drafter from scratch. We present three new SD
methods that remove this shared-vocabulary constraint. All three methods
preserve the target distribution (i.e., they are lossless) and work with
off-the-shelf models without requiring additional training or modifications.
Empirically, on summarization, programming, and long-context tasks, our
algorithms demonstrate significant speedups of up to 2.8x over standard
autoregressive decoding. By enabling any off-the-shelf model to serve as a
drafter and requiring no retraining, this work substantially broadens the
applicability of the SD framework in practice.

</details>


### [107] [R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](https://arxiv.org/pdf/2502.12658)
*Wenlong Meng, Zhenyuan Guo, Lenan Wu, Chen Gong, Wenyan Liu, Weixian Li, Chengkun Wei, Wenzhi Chen*

Main category: cs.CL

TL;DR: The paper introduces R.R., a two-step attack to reconstruct PII from scrubbed LLM training data, outperforming baselines in PII identification.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of reconstructing PII from scrubbed LLM training data, which existing attacks like MIAs struggle with.

Method: Proposes R.R.: (1) recollection prompts to fill masked PII, (2) ranking candidates using a new criterion calibrated by a reference model.

Result: R.R. outperforms baselines in PII identification across three datasets, showing LLMs' vulnerability to PII leakage.

Conclusion: LLMs remain vulnerable to PII leakage even with scrubbed data, emphasizing the need for stronger privacy safeguards.

Abstract: Large Language Models (LLMs) pose significant privacy risks, potentially
leaking training data due to implicit memorization. Existing privacy attacks
primarily focus on membership inference attacks (MIAs) or data extraction
attacks, but reconstructing specific personally identifiable information (PII)
in LLMs' training data remains challenging. In this paper, we propose R.R.
(Recollect and Rank), a novel two-step privacy stealing attack that enables
attackers to reconstruct PII entities from scrubbed training data where the PII
entities have been masked. In the first stage, we introduce a prompt paradigm
named recollection, which instructs the LLM to repeat a masked text but fill in
masks. Then we can use PII identifiers to extract recollected PII candidates.
In the second stage, we design a new criterion to score each PII candidate and
rank them. Motivated by membership inference, we leverage the reference model
as a calibration to our criterion. Experiments across three popular PII
datasets demonstrate that the R.R. achieves better PII identification
performance than baselines. These results highlight the vulnerability of LLMs
to PII leakage even when training data has been scrubbed. We release our code
and datasets at GitHub.

</details>


### [108] [Retrieval-augmented systems can be dangerous medical communicators](https://arxiv.org/pdf/2502.14898)
*Lionel Wong, Ayman Ali, Raymond Xiong, Shannon Zeijang Shen, Yoon Kim, Monica Agrawal*

Main category: cs.CL

TL;DR: The paper highlights that even accurate AI-generated health information can mislead patients due to decontextualization and omissions, proposing improvements like better communication pragmatics.


<details>
  <summary>Details</summary>
Motivation: To address the issue of AI-generated health information being misleading despite technical accuracy, as patients may misinterpret it compared to original sources or clinician advice.

Method: Large-scale query analysis on topics like disputed diagnoses and procedure safety, combining quantitative and qualitative evidence.

Result: Current systems decontextualize facts, omit critical sources, and reinforce biases, leading to suboptimal patient interpretations.

Conclusion: Recommendations include integrating communication pragmatics and deeper source comprehension to mitigate misleading outputs, applicable beyond healthcare.

Abstract: Patients have long sought health information online, and increasingly, they
are turning to generative AI to answer their health-related queries. Given the
high stakes of the medical domain, techniques like retrieval-augmented
generation and citation grounding have been widely promoted as methods to
reduce hallucinations and improve the accuracy of AI-generated responses and
have been widely adopted into search engines. This paper argues that even when
these methods produce literally accurate content drawn from source documents
sans hallucinations, they can still be highly misleading. Patients may derive
significantly different interpretations from AI-generated outputs than they
would from reading the original source material, let alone consulting a
knowledgeable clinician. Through a large-scale query analysis on topics
including disputed diagnoses and procedure safety, we support our argument with
quantitative and qualitative evidence of the suboptimal answers resulting from
current systems. In particular, we highlight how these models tend to
decontextualize facts, omit critical relevant sources, and reinforce patient
misconceptions or biases. We propose a series of recommendations -- such as the
incorporation of communication pragmatics and enhanced comprehension of source
documents -- that could help mitigate these issues and extend beyond the
medical domain.

</details>


### [109] [Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews](https://arxiv.org/pdf/2502.15226)
*Mengqiao Liu, Tevin Wang, Cassandra A. Cohen, Sarah Li, Chenyan Xiong*

Main category: cs.CL

TL;DR: CLUE is an LLM-powered interviewer that gathers user opinions on LLMs post-interaction, revealing insights like bipolar views on reasoning processes and demands for freshness/multi-modality.


<details>
  <summary>Details</summary>
Motivation: To understand real user opinions on mainstream LLMs beyond traditional evaluations.

Method: CLUE conducts in-the-moment interviews after users interact with LLMs, analyzing logs from thousands of users.

Result: Captured diverse user opinions, e.g., mixed views on DeepSeek-R1's reasoning and desires for updated/multi-modal info.

Conclusion: CLUE effectively gathers valuable user insights, highlighting gaps like reasoning clarity and feature demands.

Abstract: Which large language model (LLM) is better? Every evaluation tells a story,
but what do users really think about current LLMs? This paper presents CLUE, an
LLM-powered interviewer that conducts in-the-moment user experience interviews,
right after users interact with LLMs, and automatically gathers insights about
user opinions from massive interview logs. We conduct a study with thousands of
users to understand user opinions on mainstream LLMs, recruiting users to first
chat with a target LLM and then be interviewed by CLUE. Our experiments
demonstrate that CLUE captures interesting user opinions, e.g., the bipolar
views on the displayed reasoning process of DeepSeek-R1 and demands for
information freshness and multi-modality. Our code and data are at
https://github.com/cxcscmu/LLM-Interviewer.

</details>


### [110] [Self-Training Elicits Concise Reasoning in Large Language Models](https://arxiv.org/pdf/2502.20122)
*Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun*

Main category: cs.CL

TL;DR: The paper proposes fine-tuning methods to reduce redundant tokens in Chain-of-Thought reasoning, achieving a 30% token reduction while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Typical reasoning traces in LLMs contain redundant tokens, increasing inference costs unnecessarily.

Method: Fine-tuning using self-generated concise reasoning paths via best-of-N sampling and few-shot conditioning.

Result: 30% reduction in output tokens across five model families on GSM8K and MATH, with maintained accuracy.

Conclusion: Self-training effectively elicits concise reasoning in LLMs, leveraging stochasticity and in-context learning.

Abstract: Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to
utilize additional computation through intermediate tokens to solve complex
tasks. However, we posit that typical reasoning traces contain many redundant
tokens, incurring extraneous inference costs. Upon examination of the output
distribution of current LLMs, we find evidence on their latent ability to
reason more concisely, relative to their default behavior. To elicit this
capability, we propose simple fine-tuning methods which leverage self-generated
concise reasoning paths obtained by best-of-N sampling and few-shot
conditioning, in task-specific settings. Our combined method achieves a 30%
reduction in output tokens on average, across five model families on GSM8K and
MATH, while maintaining average accuracy. By exploiting the fundamental
stochasticity and in-context learning capabilities of LLMs, our self-training
approach robustly elicits concise reasoning on a wide range of models,
including those with extensive post-training. Code is available at
https://github.com/TergelMunkhbat/concise-reasoning

</details>


### [111] [Compositional Causal Reasoning Evaluation in Language Models](https://arxiv.org/pdf/2503.04556)
*Jacqueline R. M. A. Maasch, Alihan Hüyük, Xinnuo Xu, Aditya V. Nori, Javier Gonzalez*

Main category: cs.CL

TL;DR: The paper introduces compositional causal reasoning (CCR) as a unified framework to evaluate AI's ability to combine causal and compositional reasoning, testing it on language models like LLama, Phi, and GPT.


<details>
  <summary>Details</summary>
Motivation: To address the need for principled evaluation methods for causal and compositional reasoning in AI, the authors propose CCR as a combined measure.

Method: They instantiate a framework to evaluate CCR for average treatment effect and probability of necessity and sufficiency, testing it on language models.

Result: The framework identified distinct error patterns in models, with CCR errors generally increasing with causal path complexity, except for one model (o1).

Conclusion: CCR provides a systematic way to assess AI's causal-compositional reasoning, revealing model-specific strengths and weaknesses.

Abstract: Causal reasoning and compositional reasoning are two core aspirations in AI.
Measuring the extent of these behaviors requires principled evaluation methods.
We explore a unified perspective that considers both behaviors simultaneously,
termed compositional causal reasoning (CCR): the ability to infer how causal
measures compose and, equivalently, how causal quantities propagate through
graphs. We instantiate a framework for the systematic evaluation of CCR for the
average treatment effect and the probability of necessity and sufficiency. As
proof of concept, we demonstrate CCR evaluation for language models in the
LLama, Phi, and GPT families. On a math word problem, our framework revealed a
range of taxonomically distinct error patterns. CCR errors increased with the
complexity of causal paths for all models except o1.

</details>


### [112] [Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference](https://arxiv.org/pdf/2503.04793)
*Wenjie Qiu, Yi-Chen Li, Xuqin Zhang, Tianyi Zhang, Yihang Zhang, Zongzhang Zhang, Yang Yu*

Main category: cs.CL

TL;DR: The paper proposes an intermediate-grained reward model for aligning LLMs with human preferences by scoring sentences and aggregating them into a response-level score, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing reward models operate at coarse-grained levels, leading to sparse rewards and challenges for reinforcement learning. Token-level models lack explicit semantic information.

Method: The method segments responses into sentences, assigns scores to each, and uses differential operations and a novel attention mechanism to aggregate scores into a response-level reward.

Result: The approach outperforms response-level reward models by 2.7% on RewardBench and surpasses baselines on AlpacaEval.

Conclusion: The proposed intermediate-grained reward model improves alignment effectiveness by addressing the limitations of coarse-grained and token-level models.

Abstract: Learning reward models from human preference datasets and subsequently
optimizing language models via reinforcement learning has emerged as a
fundamental paradigm for aligning LLMs with human preferences. The performance
of the reward model plays a crucial role in the effectiveness of alignment.
Previous reward models operate at a coarse-grained level, requiring the
generation of a complete response to obtain a reward value. The sparse reward
may present challenges for downstream reinforcement learning. While recent
efforts have attempted to learn token-level reward models, the lack of explicit
semantic information makes it difficult to model the credit of every individual
token. In this paper, we propose assigning scores to every sentence,
introducing an intermediate-grained reward model. By segmenting the complete
response into sentences and applying differential operations to reward output
at the start and end positions of each sentence, we can effectively model the
rewards of sentences. Moreover, a novel attention mechanism is introduced to
aggregate the scores of all sentences into a response-level score, which allows
it to be trained using the Bradley-Terry model. On common benchmarks, our
method outperforms the response-level reward model by 2.7% on RewardBench (for
reward modeling evaluation) and surpasses all baselines on AlpacaEval (for
alignment evaluation).

</details>


### [113] [A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization](https://arxiv.org/pdf/2503.10354)
*Nevidu Jayatilleke, Ruvan Weerasinghe*

Main category: cs.CL

TL;DR: The paper proposes a hybrid framework combining extractive and abstractive summarization to efficiently summarize lengthy and complex patent documents, using LexRank and a fine-tuned BART model with LoRA, evaluated with meta-learning for domain generalization.


<details>
  <summary>Details</summary>
Motivation: The need for efficient patent summarization arises from the growing volume of innovations and the complexity of patent documents, which are lengthy and contain technical and legal jargon.

Method: The study combines extractive (LexRank) and abstractive (fine-tuned BART with LoRA) summarization, with meta-learning for domain generalization across patent fields.

Result: The proposed hybrid framework effectively generates abstractive summaries of patent records, addressing the challenges of complexity and length.

Conclusion: The hybrid approach, leveraging advanced NLP techniques, offers a robust solution for patent summarization, with potential applications across diverse patent domains.

Abstract: Automatic patent summarization approaches that help in the patent analysis
and comprehension procedure are in high demand due to the colossal growth of
innovations. The development of natural language processing (NLP), text mining,
and deep learning has notably amplified the efficacy of text summarization
models for abundant types of documents. Summarizing patent text remains a
pertinent challenge due to the labyrinthine writing style of these documents,
which includes technical and legal intricacies. Additionally, these patent
document contents are considerably lengthier than archetypal documents, which
complicates the process of extracting pertinent information for summarization.
Embodying extractive and abstractive text summarization methodologies into a
hybrid framework, this study proposes a system for efficiently creating
abstractive summaries of patent records. The procedure involves leveraging the
LexRank graph-based algorithm to retrieve the important sentences from input
parent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART)
model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for
producing text summaries. This is accompanied by methodical testing and
evaluation strategies. Furthermore, the author employed certain meta-learning
techniques to achieve Domain Generalization (DG) of the abstractive component
across multiple patent fields.

</details>


### [114] [Enhancing Arabic Automated Essay Scoring with Synthetic Data and Error Injection](https://arxiv.org/pdf/2503.17739)
*Chatrine Qwaider, Bashar Alhafni, Kirill Chirkunov, Nizar Habash, Ted Briscoe*

Main category: cs.CL

TL;DR: The paper introduces synthetic Arabic essay generation using LLMs and Transformer models for AES, addressing the lack of annotated datasets. It compares two error-injection methods and develops a BERT-based AES system, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: The lack of annotated Arabic essay datasets hinders AES development. This work aims to fill this gap by generating synthetic essays.

Method: Uses LLMs to generate essays across CEFR levels, introduces two error-injection methods, and develops a BERT-based AES system.

Result: A dataset of 3,040 annotated essays is created, and the BERT-based AES system shows improved performance.

Conclusion: Synthetic datasets enhance Arabic AES, and the work provides publicly available code and data.

Abstract: Automated Essay Scoring (AES) plays a crucial role in assessing language
learners' writing quality, reducing grading workload, and providing real-time
feedback. The lack of annotated essay datasets inhibits the development of
Arabic AES systems. This paper leverages Large Language Models (LLMs) and
Transformer models to generate synthetic Arabic essays for AES. We prompt an
LLM to generate essays across the Common European Framework of Reference (CEFR)
proficiency levels and introduce and compare two approaches to error injection.
We create a dataset of 3,040 annotated essays with errors injected using our
two methods. Additionally, we develop a BERT-based Arabic AES system calibrated
to CEFR levels. Our experimental results demonstrate the effectiveness of our
synthetic dataset in improving Arabic AES performance. We make our code and
data publicly available.

</details>


### [115] [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/pdf/2504.19267)
*Mohamed Gado, Towhid Taliee, Muhammad Memon, Dmitry Ignatov, Radu Timofte*

Main category: cs.CL

TL;DR: The paper introduces VIST-GPT, a transformer-based model for visual storytelling, using novel metrics (RoViST and GROOVIST) to better evaluate narrative quality.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like BLEU and CIDEr are inadequate for visual storytelling, prompting the need for better evaluation methods and improved models.

Method: The VIST-GPT model leverages transformer-based architectures and large multimodal models, trained on the VIST dataset.

Result: VIST-GPT generates visually grounded, coherent narratives, evaluated using RoViST and GROOVIST metrics.

Conclusion: The proposed model and metrics offer a more nuanced and human-aligned evaluation of visual storytelling.

Abstract: Visual storytelling is an interdisciplinary field combining computer vision
and natural language processing to generate cohesive narratives from sequences
of images. This paper presents a novel approach that leverages recent
advancements in multimodal models, specifically adapting transformer-based
architectures and large multimodal models, for the visual storytelling task.
Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT
model produces visually grounded, contextually appropriate narratives. We
address the limitations of traditional evaluation metrics, such as BLEU,
METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we
utilize RoViST and GROOVIST, novel reference-free metrics designed to assess
visual storytelling, focusing on visual grounding, coherence, and
non-redundancy. These metrics provide a more nuanced evaluation of narrative
quality, aligning closely with human judgment.

</details>


### [116] [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/pdf/2504.21299)
*Zhiting Fan, Ruizhe Chen, Zuozhu Liu*

Main category: cs.CL

TL;DR: BiasGuard, a two-stage bias detection tool for LLMs, improves accuracy and reduces misjudgments by reasoning through fairness specifications and using reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of existing methods (fairness classifiers, LLM-based judges) in understanding intentions and lacking fairness criteria.

Method: Two-stage approach: (1) explicit reasoning based on fairness specifications, (2) reinforcement learning to enhance reasoning and judgment.

Result: Outperforms existing tools across five datasets, improving accuracy and reducing over-fairness misjudgments.

Conclusion: BiasGuard demonstrates the effectiveness of reasoning-enhanced decision-making and the two-stage optimization pipeline.

Abstract: Identifying bias in LLM-generated content is a crucial prerequisite for
ensuring fairness in LLMs. Existing methods, such as fairness classifiers and
LLM-based judges, face limitations related to difficulties in understanding
underlying intentions and the lack of criteria for fairness judgment. In this
paper, we introduce BiasGuard, a novel bias detection tool that explicitly
analyzes inputs and reasons through fairness specifications to provide accurate
judgments. BiasGuard is implemented through a two-stage approach: the first
stage initializes the model to explicitly reason based on fairness
specifications, while the second stage leverages reinforcement learning to
enhance its reasoning and judgment capabilities. Our experiments, conducted
across five datasets, demonstrate that BiasGuard outperforms existing tools,
improving accuracy and reducing over-fairness misjudgments. We also highlight
the importance of reasoning-enhanced decision-making and provide evidence for
the effectiveness of our two-stage optimization pipeline.

</details>


### [117] [Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items](https://arxiv.org/pdf/2505.01015)
*Jongwook Han, Dongmin Choi, Woojung Song, Eun-Ju Lee, Yohan Jo*

Main category: cs.CL

TL;DR: The paper introduces the Value Portrait benchmark to assess language models' value orientations, addressing biases in existing benchmarks by using real-life interactions and psychometric validation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for language models are biased and lack real-world relevance, prompting the need for a more authentic and reliable evaluation framework.

Method: The Value Portrait benchmark uses real-life user-LLM interactions and human ratings to correlate responses with actual value scores, ensuring psychometric validity.

Result: Evaluation of 44 LLMs showed prioritization of Benevolence, Security, and Self-Direction values, with biases in demographic group perceptions compared to human data.

Conclusion: The Value Portrait benchmark provides a reliable, real-world-aligned method for assessing LLM values, revealing biases and value emphases in current models.

Abstract: The importance of benchmarks for assessing the values of language models has
been pronounced due to the growing need of more authentic, human-aligned
responses. However, existing benchmarks rely on human or machine annotations
that are vulnerable to value-related biases. Furthermore, the tested scenarios
often diverge from real-world contexts in which models are commonly used to
generate text and express values. To address these issues, we propose the Value
Portrait benchmark, a reliable framework for evaluating LLMs' value
orientations with two key characteristics. First, the benchmark consists of
items that capture real-life user-LLM interactions, enhancing the relevance of
assessment results to real-world LLM usage. Second, each item is rated by human
subjects based on its similarity to their own thoughts, and correlations
between these ratings and the subjects' actual value scores are derived. This
psychometrically validated approach ensures that items strongly correlated with
specific values serve as reliable items for assessing those values. Through
evaluating 44 LLMs with our benchmark, we find that these models prioritize
Benevolence, Security, and Self-Direction values while placing less emphasis on
Tradition, Power, and Achievement values. Also, our analysis reveals biases in
how LLMs perceive various demographic groups, deviating from real human data.

</details>


### [118] [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/pdf/2505.03452)
*Matan Orbach, Ohad Eytan, Benjamin Sznajder, Ariel Gera, Odellia Boni, Yoav Kantor, Gal Bloch, Omri Levy, Hadas Abraham, Nitzan Barzilay, Eyal Shnarch, Michael E. Factor, Shila Ofek-Koifman, Paula Ta-Shma, Assaf Toledo*

Main category: cs.CL

TL;DR: The paper benchmarks RAG hyper-parameter optimization (HPO) methods, showing greedy or random search can efficiently boost performance, with model selection prioritized over pipeline order.


<details>
  <summary>Details</summary>
Motivation: The complexity and cost of finding optimal RAG configurations motivated the study of HPO frameworks, whose effectiveness lacked rigorous benchmarking.

Method: The study evaluates 5 HPO algorithms across 5 diverse datasets, including a new real-world product documentation dataset, using the largest HPO search space to date and three metrics.

Result: RAG HPO is efficient with greedy or random search, significantly improving performance across all datasets. Greedy HPO benefits from prioritizing model selection over pipeline order.

Conclusion: Efficient RAG HPO is achievable, with greedy approaches favoring model selection optimization first, outperforming traditional pipeline-order methods.

Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a
given use case can be complex and expensive. Motivated by this challenge,
frameworks for RAG hyper-parameter optimization (HPO) have recently emerged,
yet their effectiveness has not been rigorously benchmarked. To address this
gap, we present a comprehensive study involving 5 HPO algorithms over 5
datasets from diverse domains, including a new one collected for this work on
real-world product documentation. Our study explores the largest HPO search
space considered to date, with three evaluation metrics as optimization
targets. Analysis of the results shows that RAG HPO can be done efficiently,
either greedily or with random search, and that it significantly boosts RAG
performance for all datasets. For greedy HPO approaches, we show that
optimizing model selection first is preferable to the prevalent practice of
optimizing according to RAG pipeline order.

</details>


### [119] [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/pdf/2505.08167)
*Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang*

Main category: cs.CL

TL;DR: The paper proposes a novel training method for domain-specific LLMs, integrating bidirectional chains of thought and a reward mechanism to address challenges like bias and catastrophic forgetting in ICH data.


<details>
  <summary>Details</summary>
Motivation: To overcome issues like bias, incorrect knowledge inheritance, and catastrophic forgetting when fine-tuning LLMs with Intangible Cultural Heritage (ICH) data.

Method: A bidirectional chains of thought approach (forward and reverse reasoning) combined with a reward mechanism for optimizing decision-making and output quality.

Result: Outperforms baseline methods (0-shot, step-by-step reasoning, etc.) in accuracy, Bleu-4, and Rouge-L scores on question-answering tasks, with demonstrated generalizability across domains like Finance and Wikidata.

Conclusion: The method is adaptable to multiple domains, offering a valuable approach for future model training in diverse fields.

Abstract: The rapid development of large language models (LLMs) has provided
significant support and opportunities for the advancement of domain-specific
LLMs. However, fine-tuning these large models using Intangible Cultural
Heritage (ICH) data inevitably faces challenges such as bias, incorrect
knowledge inheritance, and catastrophic forgetting. To address these issues, we
propose a novel training method that integrates a bidirectional chains of
thought and a reward mechanism. This method is built upon ICH-Qwen, a large
language model specifically designed for the field of intangible cultural
heritage. The proposed method enables the model to not only perform forward
reasoning but also enhances the accuracy of the generated answers by utilizing
reverse questioning and reverse reasoning to activate the model's latent
knowledge. Additionally, a reward mechanism is introduced during training to
optimize the decision-making process. This mechanism improves the quality of
the model's outputs through structural and content evaluations with different
weighting schemes. We conduct comparative experiments on ICH-Qwen, with results
demonstrating that our method outperforms 0-shot, step-by-step reasoning,
knowledge distillation, and question augmentation methods in terms of accuracy,
Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the
paper highlights the effectiveness of combining the bidirectional chains of
thought and reward mechanism through ablation experiments. In addition, a
series of generalizability experiments are conducted, with results showing that
the proposed method yields improvements on various domain-specific datasets and
advanced models in areas such as Finance, Wikidata, and StrategyQA. This
demonstrates that the method is adaptable to multiple domains and provides a
valuable approach for model training in future applications across diverse
fields.

</details>


### [120] [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/pdf/2505.15074)
*Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, Furong Huang*

Main category: cs.CL

TL;DR: DISCO improves GRPO by addressing domain imbalance with domain-aware and difficulty-aware reward scaling, enhancing generalization and fairness in LLMs.


<details>
  <summary>Details</summary>
Motivation: GRPO's assumptions of balanced domains and uniform alignment fail in real-world data, leading to poor generalization and fairness.

Method: DISCO introduces domain-aware reward scaling and difficulty-aware reward scaling to counteract imbalance and prioritize uncertain prompts.

Result: DISCO outperforms GRPO variants by 5% on Qwen3 models and achieves state-of-the-art results on multi-domain benchmarks.

Conclusion: DISCO offers a principled solution for equitable and effective policy learning in LLMs.

Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences
through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,
Group Relative Policy Optimization (GRPO) has gained attention for its
simplicity and strong performance, notably eliminating the need for a learned
value function. However, GRPO implicitly assumes a balanced domain distribution
and uniform semantic alignment across groups - assumptions that rarely hold in
real-world datasets. When applied to multi-domain, imbalanced data, GRPO
disproportionately optimizes for dominant domains, neglecting underrepresented
ones and resulting in poor generalization and fairness. We propose
Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled
extension to GRPO that addresses inter-group imbalance with two key
innovations. Domain-aware reward scaling counteracts frequency bias by
reweighting optimization based on domain prevalence. Difficulty-aware reward
scaling leverages prompt-level self-consistency to identify and prioritize
uncertain prompts that offer greater learning value. Together, these strategies
promote more equitable and effective policy learning across domains. Extensive
experiments across multiple LLMs and skewed training distributions show that
DISCO improves generalization, outperforms existing GRPO variants by 5% on
Qwen3 models, and sets new state-of-the-art results on multi-domain alignment
benchmarks.

</details>


### [121] [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/pdf/2505.15817)
*Tong Zheng, Lichang Chen, Simeng Han, R. Thomas McCoy, Heng Huang*

Main category: cs.CL

TL;DR: The paper introduces Mixture-of-Thought (MoT), a framework enabling LLMs to reason across natural language, code, and symbolic logic (truth-table) modalities, outperforming single-modality approaches by up to +11.7pp accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches use a single reasoning modality (e.g., natural language) during training, missing synergies among modalities. MoT addresses this gap by integrating multiple reasoning modalities.

Method: MoT uses a two-phase design: (1) self-evolving training with filtered, self-generated rationales across modalities, and (2) inference leveraging modality synergy for better predictions.

Result: MoT outperforms single-modality baselines by up to +11.7pp accuracy on logical reasoning benchmarks (FOLIO, ProofWriter), especially on harder problems.

Conclusion: MoT enhances LLM reasoning by combining complementary modalities, with truth-tables mitigating natural language bottlenecks, proving effective in training and inference.

Abstract: Human beings naturally utilize multiple reasoning modalities to learn and
solve logical problems, i.e., different representational formats such as
natural language, code, and symbolic logic. In contrast, most existing
LLM-based approaches operate with a single reasoning modality during training,
typically natural language. Although some methods explored modality selection
or augmentation at inference time, the training process remains modality-blind,
limiting synergy among modalities. To fill in this gap, we propose
Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three
complementary modalities: natural language, code, and a newly introduced
symbolic modality, truth-table, which systematically enumerates logical cases
and partially mitigates key failure modes in natural language reasoning. MoT
adopts a two-phase design: (1) self-evolving MoT training, which jointly learns
from filtered, self-generated rationales across modalities; and (2) MoT
inference, which fully leverages the synergy of three modalities to produce
better predictions. Experiments on logical reasoning benchmarks including FOLIO
and ProofWriter demonstrate that our MoT framework consistently and
significantly outperforms strong LLM baselines with single-modality
chain-of-thought approaches, achieving up to +11.7pp average accuracy gain.
Further analyses show that our MoT framework benefits both training and
inference stages; that it is particularly effective on harder logical reasoning
problems; and that different modalities contribute complementary strengths,
with truth-table reasoning helping to overcome key bottlenecks in natural
language inference.

</details>


### [122] [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/pdf/2505.16694)
*Gouki Minegishi, Hiroki Furuta, Shohei Taniguchi, Yusuke Iwasawa, Yutaka Matsuo*

Main category: cs.CL

TL;DR: The paper explores how transformer models acquire meta-learning abilities during training, beyond just copying answers, by analyzing circuit dynamics in an In-Context Meta Learning setting.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models meta-learn tasks from context, not just copy answers, and clarify the training dynamics behind this ability.

Method: Extends the copy task to an In-Context Meta Learning setting, analyzing model circuit dynamics during training phases.

Result: Reveals multiple phases in acquiring meta-learning abilities, each with unique circuits, differing from single-phase induction heads.

Conclusion: Provides deeper insights into the source of transformers' In-Context Learning ability, linking circuit emergence to known phenomena in large language models.

Abstract: Transformer-based language models exhibit In-Context Learning (ICL), where
predictions are made adaptively based on context. While prior work links
induction heads to ICL through a sudden jump in accuracy, this can only account
for ICL when the answer is included within the context. However, an important
property of practical ICL in large language models is the ability to meta-learn
how to solve tasks from context, rather than just copying answers from context;
how such an ability is obtained during training is largely unexplored. In this
paper, we experimentally clarify how such meta-learning ability is acquired by
analyzing the dynamics of the model's circuit during training. Specifically, we
extend the copy task from previous research into an In-Context Meta Learning
setting, where models must infer a task from examples to answer queries.
Interestingly, in this setting, we find that there are multiple phases in the
process of acquiring such abilities, and that a unique circuit emerges in each
phase, contrasting with the single-phases change in induction heads. The
emergence of such circuits can be related to several phenomena known in large
language models, and our analysis lead to a deeper understanding of the source
of the transformer's ICL ability.

</details>


### [123] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/pdf/2505.17061)
*Xinlong Chen, Yuanxing Zhang, Qiang Liu, Junfei Wu, Fuzheng Zhang, Tieniu Tan*

Main category: cs.CL

TL;DR: MoD dynamically adapts decoding strategies to mitigate hallucinations in LVLMs by evaluating attention correctness on image tokens.


<details>
  <summary>Details</summary>
Motivation: Address the persistent challenge of hallucinations in LVLMs, which hinder their performance.

Method: Proposes Mixture of Decoding (MoD), which measures consistency between outputs from original and attended image tokens to adapt decoding strategies.

Result: MoD outperforms existing methods on multiple benchmarks, effectively reducing hallucinations.

Conclusion: MoD is a novel and effective approach for hallucination mitigation in LVLMs.

Abstract: Large Vision-Language Models (LVLMs) have exhibited impressive capabilities
across various visual tasks, yet they remain hindered by the persistent
challenge of hallucinations. To address this critical issue, we propose Mixture
of Decoding (MoD), a novel approach for hallucination mitigation that
dynamically adapts decoding strategies by evaluating the correctness of the
model's attention on image tokens. Specifically, MoD measures the consistency
between outputs generated from the original image tokens and those derived from
the model's attended image tokens, to distinguish the correctness
aforementioned. If the outputs are consistent, indicating correct attention,
MoD employs a complementary strategy to amplify critical information.
Conversely, if the outputs are inconsistent, suggesting erroneous attention,
MoD utilizes a contrastive strategy to suppress misleading information.
Extensive experiments demonstrate that MoD significantly outperforms existing
decoding methods across multiple mainstream benchmarks, effectively mitigating
hallucinations in LVLMs. The code is available at
https://github.com/xlchen0205/MoD.

</details>


### [124] [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/pdf/2505.21646)
*Lei Zhang, Markus Stricker*

Main category: cs.CL

TL;DR: An iterative framework using Word2Vec and strategic document selection accelerates materials discovery by predicting high-performing compositions for ORR, HER, and OER reactions, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Overcoming the combinatorial explosion in materials discovery by leveraging latent knowledge in scientific texts, simulations, and experiments.

Method: Iterative corpus refinement with Word2Vec models, monitoring convergence of composition-property correlations in embedding space.

Result: Successfully predicted top-performing materials for ORR, HER, and OER, validated experimentally.

Conclusion: Iterative corpus refinement is a scalable tool for materials discovery, especially where data is scarce.

Abstract: The discovery and optimization of materials for specific applications is
hampered by the practically infinite number of possible elemental combinations
and associated properties, also known as the `combinatorial explosion'. By
nature of the problem, data are scarce and all possible data sources should be
used. In addition to simulations and experimental results, the latent knowledge
in scientific texts is not yet used to its full potential. We present an
iterative framework that refines a given scientific corpus by strategic
selection of the most diverse documents, training Word2Vec models, and
monitoring the convergence of composition-property correlations in embedding
space. Our approach is applied to predict high-performing materials for oxygen
reduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions
for a large number of possible candidate compositions. Our method successfully
predicts the highest performing compositions among a large pool of candidates,
validated by experimental measurements of the electrocatalytic performance in
the lab. This work demonstrates and validates the potential of iterative corpus
refinement to accelerate materials discovery and optimization, offering a
scalable and efficient tool for screening large compositional spaces where
reliable data are scarce or non-existent.

</details>


### [125] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/pdf/2505.22107)
*Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan*

Main category: cs.CL

TL;DR: The paper introduces Dynamic Group Attention (DGA) to reduce computational inefficiencies in Transformer-based LLMs by optimizing attention sparsity through a group coding strategy.


<details>
  <summary>Details</summary>
Motivation: Traditional attention mechanisms in LLMs suffer from redundant computations, as all tokens consume equal resources despite sparse attention weights.

Method: The authors reformulate sequence modeling as a supervised task, analyze attention sparsity, and propose a group coding strategy to optimize attention. DGA dynamically aggregates less important tokens.

Result: DGA significantly reduces computational costs while maintaining competitive performance.

Conclusion: DGA effectively addresses redundancy in attention computations, improving efficiency without sacrificing model performance.

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [126] [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/pdf/2506.00551)
*Ming Wang, Peidong Wang, Lin Wu, Xiaocui Yang, Daling Wang, Shi Feng, Yuxin Chen, Bixuan Wang, Yifei Zhang*

Main category: cs.CL

TL;DR: AnnaAgent is an LLM-based conversational agent designed for realistic seeker simulation in mental health counseling, addressing dynamic evolution and multi-session memory challenges.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in simulating real seekers due to ethical and cost constraints, while capturing dynamic mental state fluctuations and multi-session interactions.

Method: AnnaAgent uses an emotion modulator, complaint elicitor, and tertiary memory mechanism trained on real counseling dialogues for dynamic configuration control and memory integration.

Result: AnnaAgent outperforms existing baselines in realistic seeker simulation, validated through automated and manual evaluations.

Conclusion: AnnaAgent advances AI-driven mental health research by providing a more realistic and ethically reviewed simulation tool.

Abstract: Constrained by the cost and ethical concerns of involving real seekers in
AI-driven mental health, researchers develop LLM-based conversational agents
(CAs) with tailored configurations, such as profiles, symptoms, and scenarios,
to simulate seekers. While these efforts advance AI in mental health, achieving
more realistic seeker simulation remains hindered by two key challenges:
dynamic evolution and multi-session memory. Seekers' mental states often
fluctuate during counseling, which typically spans multiple sessions. To
address this, we propose AnnaAgent, an emotional and cognitive dynamic agent
system equipped with tertiary memory. AnnaAgent incorporates an emotion
modulator and a complaint elicitor trained on real counseling dialogues,
enabling dynamic control of the simulator's configurations. Additionally, its
tertiary memory mechanism effectively integrates short-term and long-term
memory across sessions. Evaluation results, both automated and manual,
demonstrate that AnnaAgent achieves more realistic seeker simulation in
psychological counseling compared to existing baselines. The ethically reviewed
and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.

</details>


### [127] [DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments](https://arxiv.org/pdf/2506.00739)
*Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: DefenderBench is an open-source toolkit for evaluating LLM agents in cybersecurity tasks, benchmarking models like Claude-3.7-sonnet and Llama 3.3 70B.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of LLM agents in cybersecurity and provide a practical, affordable evaluation toolkit.

Method: Introduces DefenderBench, a modular toolkit with tasks like network intrusion detection and vulnerability analysis, benchmarking SoTA LLMs.

Result: Claude-3.7-sonnet scored highest (81.65), followed by Claude-3.7-sonnet-think (78.40), with Llama 3.3 70B (71.81) as the top open-weight model.

Conclusion: DefenderBench enables fair, reproducible evaluation of LLMs in cybersecurity, with modularity for custom tasks and models.

Abstract: Large language model (LLM) agents have shown impressive capabilities in human
language comprehension and reasoning, yet their potential in cybersecurity
remains underexplored. We introduce DefenderBench, a practical, open-source
toolkit for evaluating language agents across offense, defense, and
cybersecurity knowledge-based tasks. DefenderBench includes environments for
network intrusion, malicious content detection, code vulnerability analysis,
and cybersecurity knowledge assessment. It is intentionally designed to be
affordable and easily accessible for researchers while providing fair and
rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular
LLMs, including both open- and closed-weight models, using a standardized
agentic framework. Our results show that Claude-3.7-sonnet performs best with a
DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40,
while the best open-weight model, Llama 3.3 70B, is not far behind with a
DefenderBench score of 71.81. DefenderBench's modular design allows seamless
integration of custom LLMs and tasks, promoting reproducibility and fair
comparisons. An anonymized version of DefenderBench is available at
https://github.com/microsoft/DefenderBench.

</details>


### [128] [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/pdf/2506.02204)
*Lindia Tjuatja, Graham Neubig*

Main category: cs.CL

TL;DR: BehaviorBox automates LM comparison by identifying fine-grained text features where one model outperforms another, revealing insights beyond corpus-level metrics.


<details>
  <summary>Details</summary>
Motivation: Language model evaluation is challenging due to brittle prompts, vague perplexities, and endless benchmarks. Automating meaningful comparisons is crucial.

Method: BehaviorBox uses performance-aware contextual embeddings to find fine-grained text features (e.g., specific word groups or contexts) where one LM excels over another.

Result: The method identifies coherent features (e.g., conditional phrases or emotional punctuation) that highlight performance differences, applicable across model sizes, families, and post-training.

Conclusion: BehaviorBox provides actionable insights into LM performance differences, surpassing traditional metrics like perplexity.

Abstract: Language model evaluation is a daunting task: prompts are brittle,
corpus-level perplexities are vague, and the choice of benchmarks are endless.
Finding examples that show meaningful, generalizable differences between two
LMs is crucial to understanding where one model succeeds and another fails. Can
this process be done automatically? In this work, we propose methodology for
automated comparison of language models that uses performance-aware contextual
embeddings to find fine-grained features of text where one LM outperforms
another. Our method, which we name BehaviorBox, extracts coherent features that
demonstrate differences with respect to the ease of generation between two LMs.
Specifically, BehaviorBox finds features that describe groups of words in
fine-grained contexts, such as "conditional 'were' in the phrase 'if you were'"
and "exclamation marks after emotional statements", where one model outperforms
another within a particular datatset. We apply BehaviorBox to compare models
that vary in size, model family, and post-training, and enumerate insights into
specific contexts that illustrate meaningful differences in performance which
cannot be found by measures such as corpus-level perplexity alone.

</details>


### [129] [TL;DR: Too Long, Do Re-weighting for Efficient LLM Reasoning Compression](https://arxiv.org/pdf/2506.02678)
*Zhong-Zhi Li, Xiao Liang, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Xing W, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, Zhijiang Guo, Xiao Liu, Fei Yin, Cheng-Lin Liu*

Main category: cs.CL

TL;DR: A dynamic ratio-based training pipeline for LLMs reduces output tokens by 40% while maintaining reasoning accuracy, tested on DeepSeek models.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of efficient language reasoning, especially for long outputs, without relying on complex annotations or model interpolation.

Method: Dynamic ratio-based training balancing System-1 and System-2 data weights to eliminate redundant reasoning.

Result: 40% reduction in output tokens with preserved reasoning accuracy on DeepSeek-R1-Distill models.

Conclusion: The method effectively optimizes LLM inference efficiency without sacrificing performance.

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by
leveraging Reinforcement Learning and extended Chain-of-Thought (CoT)
techniques. However, the challenge of performing efficient language
reasoning--especially during inference with extremely long outputs--has drawn
increasing attention from the research community. In this work, we propose a
dynamic ratio-based training pipeline that does not rely on sophisticated data
annotations or interpolation between multiple models. We continuously balance
the weights between the model's System-1 and System-2 data to eliminate
redundant reasoning processes while preserving the model's reasoning
capability. We validate our approach across models on DeepSeek-R1-Distill-7B
and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying
difficulty levels. Our method significantly reduces the number of output tokens
by nearly 40% while maintaining the accuracy of the reasoning. Our code and
data will be available soon.

</details>


### [130] [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/pdf/2506.04098)
*Wenhao Li, Wenwu Li, Chuyun Shen, Junjie Sheng, Zixiao Huang, Di Wu, Yun Hua, Wei Yin, Xiangfeng Wang, Hongyuan Zha, Bo Jin*

Main category: cs.CL

TL;DR: TextAtari is a benchmark for evaluating language agents on long-horizon decision-making tasks using textual descriptions of Atari games. It tests three LLMs across various agent frameworks and scenarios, revealing performance gaps between agents and humans.


<details>
  <summary>Details</summary>
Motivation: To bridge sequential decision-making and natural language processing by creating a challenging benchmark for evaluating language agents on long-horizon tasks.

Method: TextAtari translates Atari game visuals into text, offering 100 tasks. It evaluates three LLMs (Qwen2.5-7B, Gemma-7B, Llama3.1-8B) using zero-shot, few-shot chain-of-thought, and reflection reasoning frameworks across four scenarios.

Result: Significant performance gaps between language agents and humans in sequential reasoning, state tracking, and strategic planning were observed.

Conclusion: TextAtari provides a standardized framework for advancing research in language models and planning, with code and baselines available for further study.

Abstract: We present TextAtari, a benchmark for evaluating language agents on very
long-horizon decision-making tasks spanning up to 100,000 steps. By translating
the visual state representations of classic Atari games into rich textual
descriptions, TextAtari creates a challenging test bed that bridges sequential
decision-making with natural language processing. The benchmark includes nearly
100 distinct tasks with varying complexity, action spaces, and planning
horizons, all rendered as text through an unsupervised representation learning
framework (AtariARI). We evaluate three open-source large language models
(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks
(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how
different forms of prior knowledge affect performance on these long-horizon
challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and
Reference-based-investigate the impact of semantic understanding, instruction
comprehension, and expert demonstrations on agent decision-making. Our results
reveal significant performance gaps between language agents and human players
in extensive planning tasks, highlighting challenges in sequential reasoning,
state tracking, and strategic planning across tens of thousands of steps.
TextAtari provides standardized evaluation protocols, baseline implementations,
and a framework for advancing research at the intersection of language models
and planning. Our code is available at
https://github.com/Lww007/Text-Atari-Agents.

</details>


### [131] [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/pdf/2506.04373)
*Matthieu Tehenan, Vikram Natarajan, Jonathan Michala, Milton Lin, Juri Opitz*

Main category: cs.CL

TL;DR: The paper proposes a method to decompose sentence embeddings into interpretable components using dictionary learning, bridging token-level and sentence-level analysis for transparency.


<details>
  <summary>Details</summary>
Motivation: Sentence embeddings are widely used in NLP but lack interpretability due to complex neural transformations and pooling operations.

Method: The authors use dictionary learning on token-level representations to mechanistically decompose sentence embeddings into interpretable components.

Result: Findings reveal that semantic and syntactic aspects are linearly encoded in the embeddings.

Conclusion: The method enhances transparency and controllability of sentence embeddings, providing insights into their internal structure.

Abstract: Sentence embeddings are central to modern NLP and AI systems, yet little is
known about their internal structure. While we can compare these embeddings
using measures such as cosine similarity, the contributing features are not
human-interpretable, and the content of an embedding seems untraceable, as it
is masked by complex neural transformations and a final pooling operation that
combines individual token embeddings. To alleviate this issue, we propose a new
method to mechanistically decompose sentence embeddings into interpretable
components, by using dictionary learning on token-level representations. We
analyze how pooling compresses these features into sentence representations,
and assess the latent features that reside in a sentence embedding. This
bridges token-level mechanistic interpretability with sentence-level analysis,
making for more transparent and controllable representations. In our studies,
we obtain several interesting insights into the inner workings of sentence
embedding spaces, for instance, that many semantic and syntactic aspects are
linearly encoded in the embeddings.

</details>


### [132] [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/pdf/2506.05176)
*Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, Jingren Zhou*

Main category: cs.CL

TL;DR: The Qwen3 Embedding series improves text embedding and reranking using Qwen3 foundation models, offering multilingual capabilities and diverse model sizes.


<details>
  <summary>Details</summary>
Motivation: To advance text embedding and reranking by leveraging Qwen3 LLMs' multilingual understanding and generation, addressing diverse deployment needs.

Method: Multi-stage training pipeline combining unsupervised pre-training and supervised fine-tuning, with model merging for robustness. Qwen3 LLMs synthesize diverse training data.

Result: Achieves state-of-the-art results on benchmarks like MTEB and excels in retrieval tasks (code, cross-lingual, multilingual).

Conclusion: The Qwen3 Embedding series is a robust, adaptable solution for embedding and reranking, publicly available under Apache 2.0.

Abstract: In this work, we introduce the Qwen3 Embedding series, a significant
advancement over its predecessor, the GTE-Qwen series, in text embedding and
reranking capabilities, built upon the Qwen3 foundation models. Leveraging the
Qwen3 LLMs' robust capabilities in multilingual text understanding and
generation, our innovative multi-stage training pipeline combines large-scale
unsupervised pre-training with supervised fine-tuning on high-quality datasets.
Effective model merging strategies further ensure the robustness and
adaptability of the Qwen3 Embedding series. During the training process, the
Qwen3 LLMs serve not only as backbone models but also play a crucial role in
synthesizing high-quality, rich, and diverse training data across multiple
domains and languages, thus enhancing the training pipeline. The Qwen3
Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both
embedding and reranking tasks, addressing diverse deployment scenarios where
users can optimize for either efficiency or effectiveness. Empirical
evaluations demonstrate that the Qwen3 Embedding series achieves
state-of-the-art results across diverse benchmarks. Notably, it excels on the
multilingual evaluation benchmark MTEB for text embedding, as well as in
various retrieval tasks, including code retrieval, cross-lingual retrieval and
multilingual retrieval. To facilitate reproducibility and promote
community-driven research and development, the Qwen3 Embedding models are
publicly available under the Apache 2.0 license.

</details>


### [133] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/pdf/2506.06395)
*Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, Ivan Oseledets*

Main category: cs.CL

TL;DR: RLSC is a post-training method for LLMs that uses the model's self-confidence as reward signals, eliminating the need for human annotations or external reward models. It significantly improves accuracy across multiple benchmarks with minimal samples and training steps.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for LLMs rely on costly human annotations or external reward models, which are inefficient and unscalable. RLSC addresses this by leveraging the model's own confidence as a reward signal.

Method: RLSC uses the model's self-confidence as reward signals for reinforcement learning, requiring only a small number of unlabelled samples and minimal training steps (10 or 20).

Result: RLSC improves accuracy by +13.4% to +21.7% on benchmarks like AIME2024, MATH500, Minerva Math, Olympiadbench, and AMC23.

Conclusion: RLSC offers a simple, scalable post-training method for LLMs, reducing dependency on external labels or reward models while achieving significant performance gains.

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,
RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on
Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a
simple, scalable post-training method for inference models, requiring only a
small number of samples and unlabelled supervision.

</details>


### [134] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044)
*LASA Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, Yu Sun, Junao Shen, Chaojun Wang, Jie Tan, Deli Zhao, Tingyang Xu, Hao Zhang, Yu Rong*

Main category: cs.CL

TL;DR: The paper introduces Lingshu, a medical-specialized MLLM, addressing limitations in existing models by curating a rich multimodal dataset and enhancing reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs underperform in medical applications due to limited knowledge coverage, susceptibility to hallucinations, and lack of tailored reasoning.

Method: Proposes a data curation procedure for diverse medical knowledge, multi-stage training for Lingshu, and explores reinforcement learning for reasoning. Introduces MedEvalKit for evaluation.

Result: Lingshu outperforms existing open-source multimodal models on tasks like multimodal QA, text-based QA, and medical report generation.

Conclusion: Lingshu demonstrates improved effectiveness in medical applications through enriched data and specialized training, with potential for further enhancement via reinforcement learning.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [135] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/pdf/2506.07667)
*Prarabdh Shukla, Wei Yin Chong, Yash Patel, Brennan Schaffner, Danish Pruthi, Arjun Bhagoji*

Main category: cs.CL

TL;DR: The paper audits Twitch's AutoMod tool, revealing its ineffectiveness in flagging hateful content (up to 94% bypass) and over-blocking benign content (89.5%). It highlights reliance on slurs as a signal and the need for contextual understanding.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of automated moderation systems like Twitch's AutoMod in real-time engagement platforms, given the lack of existing research on their performance.

Method: Created streaming accounts to test AutoMod, sent 107,000 comments from 4 datasets via Twitch's APIs, and measured accuracy in flagging hateful content (misogyny, racism, ableism, homophobia).

Result: AutoMod failed to flag up to 94% of hateful content but removed 100% when slurs were added. It also blocked 89.5% of benign content using sensitive words in non-hateful contexts.

Conclusion: AutoMod has significant gaps, relying too heavily on slurs and lacking contextual understanding, underscoring the need for improved moderation systems.

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [136] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/pdf/2506.08048)
*Zheng Han, Jun Zhou, Jialun Pei, Jing Qin, Yingfang Fan, Qi Dou*

Main category: cs.CV

TL;DR: A data-driven biomechanics algorithm improves AR-guided surgical navigation by combining FEM-level accuracy with computational efficiency and a human-in-the-loop mechanism for surgeon interaction.


<details>
  <summary>Details</summary>
Motivation: Accurate deformation modeling in AR-guided surgeries is challenged by high computational costs of FEM and inability to handle large anatomical changes, leading to unreliable AR overlays.

Method: Proposes a data-driven biomechanics algorithm with a human-in-the-loop mechanism, allowing surgeons to correct misalignments interactively.

Result: Achieves a mean target registration error of 3.42 mm, reduced to 2.78 mm with surgeon prompts, outperforming state-of-the-art methods.

Conclusion: The framework enhances accuracy and surgeon-algorithm collaboration, advancing safer and more reliable computer-assisted surgeries.

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [137] [Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization](https://arxiv.org/pdf/2506.08493)
*Qilin Yin, Wei Lu, Xiangyang Luo, Xiaochun Cao*

Main category: cs.CV

TL;DR: The paper proposes UniCaCLF, a context-aware contrastive learning framework for temporal forgery localization (TFL) in videos, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods treat it as classification, ignoring partial tampering. TFL for small fake clips in real videos is more realistic but challenging.

Method: Uses supervised contrastive learning with a context-aware perception layer and adaptive context updater to enhance feature discriminability and localize forged segments.

Result: UniCaCLF significantly outperforms state-of-the-art methods on five public datasets.

Conclusion: The framework effectively addresses TFL challenges, offering precise localization of forged segments in videos.

Abstract: Most research efforts in the multimedia forensics domain have focused on
detecting forgery audio-visual content and reached sound achievements. However,
these works only consider deepfake detection as a classification task and
ignore the case where partial segments of the video are tampered with. Temporal
forgery localization (TFL) of small fake audio-visual clips embedded in real
videos is still challenging and more in line with realistic application
scenarios. To resolve this issue, we propose a universal context-aware
contrastive learning framework (UniCaCLF) for TFL. Our approach leverages
supervised contrastive learning to discover and identify forged instants by
means of anomaly detection, allowing for the precise localization of temporal
forged segments. To this end, we propose a novel context-aware perception layer
that utilizes a heterogeneous activation operation and an adaptive context
updater to construct a context-aware contrastive objective, which enhances the
discriminability of forged instant features by contrasting them with genuine
instant features in terms of their distances to the global context. An
efficient context-aware contrastive coding is introduced to further push the
limit of instant feature distinguishability between genuine and forged instants
in a supervised sample-by-sample manner, suppressing the cross-sample influence
to improve temporal forgery localization performance. Extensive experimental
results over five public datasets demonstrate that our proposed UniCaCLF
significantly outperforms the state-of-the-art competing algorithms.

</details>


### [138] [ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving](https://arxiv.org/pdf/2506.08052)
*Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang*

Main category: cs.CV

TL;DR: ReCogDrive integrates Vision-Language Models (VLMs) with a diffusion planner to improve autonomous driving in rare scenarios, addressing domain gaps and action space mismatches.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems struggle with rare scenarios and domain gaps between VLMs and real-world data. Imitation learning also captures suboptimal behaviors.

Method: A three-stage approach: (1) train VLMs on driving Q&A data, (2) use a diffusion planner for imitation learning, (3) fine-tune with reinforcement learning.

Result: Achieves 89.6 PDMS on NAVSIM, surpassing the previous SOTA by 5.6 PDMS.

Conclusion: ReCogDrive effectively bridges domain gaps and improves driving performance in rare scenarios.

Abstract: Although end-to-end autonomous driving has made remarkable progress, its
performance degrades significantly in rare and long-tail scenarios. Recent
approaches attempt to address this challenge by leveraging the rich world
knowledge of Vision-Language Models (VLMs), but these methods suffer from
several limitations: (1) a significant domain gap between the pre-training data
of VLMs and real-world driving data, (2) a dimensionality mismatch between the
discrete language space and the continuous action space, and (3) imitation
learning tends to capture the average behavior present in the dataset, which
may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an
autonomous driving system that integrates VLMs with diffusion planner, which
adopts a three-stage paradigm for training. In the first stage, we use a
large-scale driving question-answering datasets to train the VLMs, mitigating
the domain discrepancy between generic content and real-world driving
scenarios. In the second stage, we employ a diffusion-based planner to perform
imitation learning, mapping representations from the latent language space to
continuous driving actions. Finally, we fine-tune the diffusion planner using
reinforcement learning with NAVSIM non-reactive simulator, enabling the model
to generate safer, more human-like driving trajectories. We evaluate our
approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6
and setting a new state-of-the-art that surpasses the previous vision-only SOTA
by 5.6 PDMS.

</details>


### [139] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/pdf/2506.08071)
*Aniket Rege, Zinnia Nie, Mahesh Ramesh, Unmesh Raskar, Zhuoran Yu, Aditya Kusupati, Yong Jae Lee, Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: CuRe is a benchmarking tool for evaluating cultural representativeness in text-to-image systems, addressing biases toward Global South cultures.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation of Global South cultures in T2I systems trained on Amero/Euro-centric data.

Method: Leverages marginal utility of attribute specification to create a scalable benchmark (CuRe) with a hierarchical dataset of 300 cultural artifacts across 32 subcategories.

Result: Strong correlations between CuRe scorers and human judgments across various models and T2I systems.

Conclusion: CuRe provides a fine-grained, scalable solution for assessing cultural diversity in T2I systems, with open-sourced code and dataset.

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is
heavily Amero and Euro-centric, underrepresenting the cultures of the Global
South. To analyze these biases, we introduce CuRe, a novel and scalable
benchmarking and scoring suite for cultural representativeness that leverages
the marginal utility of attribute specification to T2I systems as a proxy for
human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy
built from the crowdsourced Wikimedia knowledge graph, with 300 cultural
artifacts across 32 cultural subcategories grouped into six broad cultural axes
(food, art, fashion, architecture, celebrations, and people). Our dataset's
categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing
their response to increasing the informativeness of text conditioning, enabling
fine-grained cultural comparisons. We empirically observe much stronger
correlations of our class of scorers to human judgments of perceptual
similarity, image-text alignment, and cultural diversity across image encoders
(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,
Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three
variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,
and DALL-E 3. The code and dataset is open-sourced and available at
https://aniketrege.github.io/cure/.

</details>


### [140] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/pdf/2506.08591)
*Chengchao Shen, Hourun Zhu, Gongfan Fang, Jianxin Wang, Xinchao Wang*

Main category: cs.CV

TL;DR: DGMR method reduces MLP parameters in large vision transformers with minimal performance loss, achieving over 57% parameter and FLOPs reduction.


<details>
  <summary>Details</summary>
Motivation: Large transformer models are computationally expensive due to MLP modules dominating parameters.

Method: Diversity-Guided MLP Reduction (DGMR) uses Gram-Schmidt pruning to eliminate redundant neurons while preserving weight diversity.

Result: Achieves 57-71.5% parameter and FLOPs reduction with negligible performance loss, requiring only 0.06% of training data.

Conclusion: DGMR effectively compresses large vision transformers, maintaining performance while reducing computational costs.

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [141] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/pdf/2506.08137)
*Oishee Bintey Hoque, Abhijin Adiga, Aniruddha Adiga, Siddharth Chaudhary, Madhav V. Marathe, S. S. Ravi, Kirti Rajagopalan, Amanda Wilson, Samarth Swarup*

Main category: cs.CV

TL;DR: IGraSS is a novel framework combining semantic segmentation and graph-based refinement to improve canal and road network mapping from noisy ground truth.


<details>
  <summary>Details</summary>
Motivation: Accurate canal and road network mapping is crucial for water and infrastructure management, but incomplete ground truth hinders existing methods.

Method: IGraSS integrates RGB, NDWI, and DEM data in a semantic segmentation module and refines ground truth using graph-level properties like reachability and connectivity.

Result: IGraSS reduces unreachable canal segments from 18% to 3% and improves canal identification. It also generalizes to road networks.

Conclusion: IGraSS effectively refines noisy ground truth and enhances infrastructure mapping, proving robust and generalizable.

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [142] [StereoVAE: A lightweight stereo-matching system using embedded GPUs](https://arxiv.org/pdf/2305.11566)
*Qiong Chang, Xiang Li, Xin Xu, Xin Liu, Yun Li, Miyazaki Jun*

Main category: cs.CV

TL;DR: A lightweight system for stereo matching on embedded GPUs balances accuracy and speed using a hybrid approach combining traditional methods and a tiny VAE-based neural network.


<details>
  <summary>Details</summary>
Motivation: To break the trade-off between accuracy and processing speed in stereo matching, enabling real-time performance on embedded systems without sacrificing accuracy.

Method: A hybrid approach: a coarse disparity map is generated traditionally, then upsampled and refined by a tiny VAE-based neural network.

Result: The system improves accuracy of coarse disparity maps from various algorithms and runs in real-time on embedded GPUs, as shown on the KITTI 2015 benchmark.

Conclusion: The proposed hybrid system effectively combines traditional and neural network methods for efficient, accurate stereo matching on embedded GPUs.

Abstract: We present a lightweight system for stereo matching through embedded GPUs. It
breaks the trade-off between accuracy and processing speed in stereo matching,
enabling our embedded system to further improve the matching accuracy while
ensuring real-time processing. The main idea of our method is to construct a
tiny neural network based on variational auto-encoder (VAE) to upsample and
refinement a small size of coarse disparity map, which is first generated by a
traditional matching method. The proposed hybrid structure cannot only bring
the advantage of traditional methods in terms of computational complexity, but
also ensure the matching accuracy under the impact of neural network. Extensive
experiments on the KITTI 2015 benchmark demonstrate that our tiny system
exhibits high robustness in improving the accuracy of the coarse disparity maps
generated by different algorithms, while also running in real-time on embedded
GPUs.

</details>


### [143] [Spectral Domain Neural Reconstruction for Passband FMCW Radars](https://arxiv.org/pdf/2506.08163)
*Harshvardhan Takawale, Nirupam Roy*

Main category: cs.CV

TL;DR: SpINRv2 is an improved neural framework for high-fidelity volumetric reconstruction using FMCW radar, addressing phase aliasing and sub-bin ambiguity at high frequencies.


<details>
  <summary>Details</summary>
Motivation: To enhance volumetric reconstruction accuracy under high-frequency conditions where phase aliasing and sub-bin ambiguity are problematic.

Method: Introduces a differentiable frequency-domain forward model with closed-form synthesis and implicit neural representation (INR) for scene modeling, plus sparsity and smoothness regularization.

Result: Outperforms classical and learning-based baselines, especially in high-frequency regimes, setting a new benchmark for neural radar-based 3D imaging.

Conclusion: SpINRv2 advances neural radar imaging by improving spectral fidelity and computational efficiency, addressing key challenges in high-frequency scenarios.

Abstract: We present SpINRv2, a neural framework for high-fidelity volumetric
reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.
Extending our prior work (SpINR), this version introduces enhancements that
allow accurate learning under high start frequencies-where phase aliasing and
sub-bin ambiguity become prominent. Our core contribution is a fully
differentiable frequency-domain forward model that captures the complex radar
response using closed-form synthesis, paired with an implicit neural
representation (INR) for continuous volumetric scene modeling. Unlike
time-domain baselines, SpINRv2 directly supervises the complex frequency
spectrum, preserving spectral fidelity while drastically reducing computational
overhead. Additionally, we introduce sparsity and smoothness regularization to
disambiguate sub-bin ambiguities that arise at fine range resolutions.
Experimental results show that SpINRv2 significantly outperforms both classical
and learning-based baselines, especially under high-frequency regimes,
establishing a new benchmark for neural radar-based 3D imaging.

</details>


### [144] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/pdf/2506.08185)
*Huixin Zhan, Jason H. Moore*

Main category: cs.CV

TL;DR: The paper proposes a method to model surgeon-specific styles in robotic surgery using a discrete diffusion framework with a vision-language-action pipeline, balancing personalization and privacy risks.


<details>
  <summary>Details</summary>
Motivation: Current AI systems ignore surgeon personalization signals, which vary due to training, experience, and motor behavior.

Method: Uses a discrete diffusion framework with multimodal inputs (video, surgical intent language, and privacy-aware embeddings) to predict gestures and encode surgeon fingerprints via language prompts.

Result: Accurately reconstructs gesture sequences and learns unique surgeon motion fingerprints, but expressive embeddings increase identity leakage risk.

Conclusion: Personalized embeddings improve performance but raise privacy concerns, highlighting the need to balance personalization with privacy in surgical AI.

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [145] [Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer](https://arxiv.org/pdf/2309.14704)
*Zhihao Zhang, Yiwei Chen, Weizhan Zhang, Caixia Yan, Qinghua Zheng, Qi Wang, Wangdu Chen*

Main category: cs.CV

TL;DR: Proposes MFTR, a tile classification-based viewport prediction method using Multi-modal Fusion Transformer for robust and interpretable 360 video streaming.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory-based methods lack robustness and oversimplify multi-modal input fusion, causing error accumulation.

Method: MFTR uses transformer networks to extract long-range dependencies, mine intra- and inter-modality relations, and classify tiles into user-interested or not.

Result: MFTR outperforms state-of-the-art methods in accuracy and overlap ratio, with competitive efficiency.

Conclusion: MFTR offers a robust, interpretable solution for viewport prediction in 360 video streaming.

Abstract: Viewport prediction is a crucial aspect of tile-based 360 video streaming
system. However, existing trajectory based methods lack of robustness, also
oversimplify the process of information construction and fusion between
different modality inputs, leading to the error accumulation problem. In this
paper, we propose a tile classification based viewport prediction method with
Multi-modal Fusion Transformer, namely MFTR. Specifically, MFTR utilizes
transformer-based networks to extract the long-range dependencies within each
modality, then mine intra- and inter-modality relations to capture the combined
impact of user historical inputs and video contents on future viewport
selection. In addition, MFTR categorizes future tiles into two categories: user
interested or not, and selects future viewport as the region that contains most
user interested tiles. Comparing with predicting head trajectories, choosing
future viewport based on tile's binary classification results exhibits better
robustness and interpretability. To evaluate our proposed MFTR, we conduct
extensive experiments on two widely used PVS-HM and Xu-Gaze dataset. MFTR shows
superior performance over state-of-the-art methods in terms of average
prediction accuracy and overlap ratio, also presents competitive computation
efficiency.

</details>


### [146] [Open World Scene Graph Generation using Vision Language Models](https://arxiv.org/pdf/2506.08189)
*Amartya Dutta, Kazi Sajeed Mehrab, Medha Sawhney, Abhilash Neog, Mridul Khurana, Sepideh Fatemi, Aanish Pradhan, M. Maruf, Ismini Lourentzou, Arka Daw, Anuj Karpatne*

Main category: cs.CV

TL;DR: Open-World SGG is a training-free framework leveraging VLMs for zero-shot scene-graph generation, enabling inference on unseen objects and relations without dataset-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current SGG methods rely on dataset-specific supervision or fine-tuning, limiting their applicability in open-world settings with novel objects/relations.

Method: The framework uses multimodal prompting, embedding alignment, and pair-refinement for zero-shot structured reasoning.

Result: Experiments on Visual Genome, Open Images V6, and PSG show VLMs can perform relational understanding without task-level training.

Conclusion: Open-World SGG demonstrates the potential of pretrained VLMs for zero-shot relational understanding, broadening SGG applicability.

Abstract: Scene-Graph Generation (SGG) seeks to recognize objects in an image and
distill their salient pairwise relationships. Most methods depend on
dataset-specific supervision to learn the variety of interactions, restricting
their usefulness in open-world settings, involving novel objects and/or
relations. Even methods that leverage large Vision Language Models (VLMs)
typically require benchmark-specific fine-tuning. We introduce Open-World SGG,
a training-free, efficient, model-agnostic framework that taps directly into
the pretrained knowledge of VLMs to produce scene graphs with zero additional
learning. Casting SGG as a zero-shot structured-reasoning problem, our method
combines multimodal prompting, embedding alignment, and a lightweight
pair-refinement strategy, enabling inference over unseen object vocabularies
and relation sets. To assess this setting, we formalize an Open-World
evaluation protocol that measures performance when no SGG-specific data have
been observed either in terms of objects and relations. Experiments on Visual
Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate
the capacity of pretrained VLMs to perform relational understanding without
task-level training.

</details>


### [147] [EVA: An Embodied World Model for Future Video Anticipation](https://arxiv.org/pdf/2410.15461)
*Xiaowei Chi, Chun-Kai Fan, Hengyuan Zhang, Xingqun Qi, Rongyu Zhang, Anthony Chen, Chi-min Chan, Wei Xue, Qifeng Liu, Shanghang Zhang, Yike Guo*

Main category: cs.CV

TL;DR: The paper introduces RoG and EVA-Bench to enhance video prediction in embodied scenarios, proposing the EVA model for high-fidelity video generation and adaptive generalization.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models lack robust understanding for multi-step predictions and OOD scenarios, limiting their use as world simulators.

Method: Proposes RoG (intermediate reasoning strategies) and EVA-Bench (benchmark), then develops the EVA model with multistage training and autoregressive strategies.

Result: EVA demonstrates efficacy in video generation and robotics tasks, supporting large-scale pre-trained models for real-world applications.

Conclusion: The work advances video prediction in embodied scenarios, with potential for broader real-world applications.

Abstract: Video generation models have made significant progress in simulating future
states, showcasing their potential as world simulators in embodied scenarios.
However, existing models often lack robust understanding, limiting their
ability to perform multi-step predictions or handle Out-of-Distribution (OOD)
scenarios. To address this challenge, we propose the Reflection of Generation
(RoG), a set of intermediate reasoning strategies designed to enhance video
prediction. It leverages the complementary strengths of pre-trained
vision-language and video generation models, enabling them to function as a
world model in embodied scenarios. To support RoG, we introduce Embodied Video
Anticipation Benchmark(EVA-Bench), a comprehensive benchmark that evaluates
embodied world models across diverse tasks and scenarios, utilizing both
in-domain and OOD datasets. Building on this foundation, we devise a world
model, Embodied Video Anticipator (EVA), that follows a multistage training
paradigm to generate high-fidelity video frames and apply an autoregressive
strategy to enable adaptive generalization for longer video sequences.
Extensive experiments demonstrate the efficacy of EVA in various downstream
tasks like video generation and robotics, thereby paving the way for
large-scale pre-trained models in real-world video prediction applications. The
video demos are available at
\hyperlink{https://sites.google.com/view/icml-eva}{https://sites.google.com/view/icml-eva}.

</details>


### [148] [Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes](https://arxiv.org/pdf/2506.08191)
*Antoni Nowinowski, Krzysztof Krawiec*

Main category: cs.CV

TL;DR: Extended DVP autoencoder handles multiple objects, improves training with latent-space losses, and outperforms baselines in reconstruction and decomposition.


<details>
  <summary>Details</summary>
Motivation: To enhance the original DVP by enabling it to process multiple objects in a scene and improve training efficiency using latent-space loss functions.

Method: Extended DVP architecture with latent-space sampling and alternative training modes, evaluated on a new benchmark with multiple 2D objects.

Result: Outperforms MONet and LIVE in reconstruction quality and object decomposition, with improved training efficiency.

Conclusion: The extended DVP shows superior performance but highlights limitations of differentiable rendering in autoencoders, suggesting areas for future work.

Abstract: This study builds on the architecture of the Disentangler of Visual Priors
(DVP), a type of autoencoder that learns to interpret scenes by decomposing the
perceived objects into independent visual aspects of shape, size, orientation,
and color appearance. These aspects are expressed as latent parameters which
control a differentiable renderer that performs image reconstruction, so that
the model can be trained end-to-end with gradient using reconstruction loss. In
this study, we extend the original DVP so that it can handle multiple objects
in a scene. We also exploit the interpretability of its latent by using the
decoder to sample additional training examples and devising alternative
training modes that rely on loss functions defined not only in the image space,
but also in the latent space. This significantly facilitates training, which is
otherwise challenging due to the presence of extensive plateaus in the
image-space reconstruction loss. To examine the performance of this approach,
we propose a new benchmark featuring multiple 2D objects, which subsumes the
previously proposed Multi-dSprites dataset while being more parameterizable. We
compare the DVP extended in these ways with two baselines (MONet and LIVE) and
demonstrate its superiority in terms of reconstruction quality and capacity to
decompose overlapping objects. We also analyze the gradients induced by the
considered loss functions, explain how they impact the efficacy of training,
and discuss the limitations of differentiable rendering in autoencoders and the
ways in which they can be addressed.

</details>


### [149] [GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra](https://arxiv.org/pdf/2506.08194)
*Mateusz Michalkiewicz, Anekha Sokhal, Tadeusz Michalkiewicz, Piotr Pawlikowski, Mahsa Baktashmotlagh, Varun Jampani, Guha Balakrishnan*

Main category: cs.CV

TL;DR: GIQ is a benchmark to evaluate geometric reasoning in vision and vision-language models, revealing significant shortcomings in current models despite their performance on standard tasks.


<details>
  <summary>Details</summary>
Motivation: To assess the true geometric understanding of vision and vision-language models, which remains unclear despite their success on benchmarks.

Method: GIQ includes synthetic and real-world images of 224 diverse polyhedra, tested via monocular 3D reconstruction, symmetry detection, mental rotation, and zero-shot classification.

Result: Current models struggle with basic geometric forms, symmetry detection, and detailed differentiation, while vision-language assistants perform poorly on complex polyhedra.

Conclusion: GIQ highlights critical gaps in geometric intelligence, offering a platform to improve geometry-aware representation learning.

Abstract: Monocular 3D reconstruction methods and vision-language models (VLMs)
demonstrate impressive results on standard benchmarks, yet their true
understanding of geometric properties remains unclear. We introduce GIQ , a
comprehensive benchmark specifically designed to evaluate the geometric
reasoning capabilities of vision and vision-language foundation models. GIQ
comprises synthetic and real-world images of 224 diverse polyhedra - including
Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and
compound shapes - covering varying levels of complexity and symmetry. Through
systematic experiments involving monocular 3D reconstruction, 3D symmetry
detection, mental rotation tests, and zero-shot shape classification tasks, we
reveal significant shortcomings in current models. State-of-the-art
reconstruction algorithms trained on extensive 3D datasets struggle to
reconstruct even basic geometric forms accurately. While foundation models
effectively detect specific 3D symmetry elements via linear probing, they
falter significantly in tasks requiring detailed geometric differentiation,
such as mental rotation. Moreover, advanced vision-language assistants exhibit
remarkably low accuracy on complex polyhedra, systematically misinterpreting
basic properties like face geometry, convexity, and compound structures. GIQ is
publicly available, providing a structured platform to highlight and address
critical gaps in geometric intelligence, facilitating future progress in
robust, geometry-aware representation learning.

</details>


### [150] [How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation](https://arxiv.org/pdf/2505.18956)
*Yining Pan, Qiongjie Cui, Xulei Yang, Na Zhao*

Main category: cs.CV

TL;DR: IAL is a novel multi-modal 3D panoptic segmentation framework that integrates LiDAR and camera data, addressing sparsity and misalignment issues with synchronized augmentation and transformer-based decoding.


<details>
  <summary>Details</summary>
Motivation: Overcome challenges in LiDAR-based 3D panoptic segmentation, such as sparsity and misalignment, by leveraging complementary camera data.

Method: Proposes IAL with PieAug for synchronized data augmentation, GTF for feature fusion, and PQG for query initialization, using a transformer decoder.

Result: Achieves state-of-the-art performance on benchmarks.

Conclusion: IAL effectively combines LiDAR and camera data, improving segmentation accuracy without post-processing.

Abstract: LiDAR-based 3D panoptic segmentation often struggles with the inherent
sparsity of data from LiDAR sensors, which makes it challenging to accurately
recognize distant or small objects. Recently, a few studies have sought to
overcome this challenge by integrating LiDAR inputs with camera images,
leveraging the rich and dense texture information provided by the latter. While
these approaches have shown promising results, they still face challenges, such
as misalignment during data augmentation and the reliance on post-processing
steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel
multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a
modality-synchronized data augmentation strategy, PieAug, to ensure alignment
between LiDAR and image inputs from the start. Next, we adopt a transformer
decoder to directly predict panoptic segmentation results. To effectively fuse
LiDAR and image features into tokens for the decoder, we design a
Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the
complementary strengths of each modality as priors for query initialization
through a Prior-based Query Generation (PQG) module, enhancing the decoder's
ability to generate accurate instance masks. Our IAL framework achieves
state-of-the-art performance compared to previous multi-modal 3D panoptic
segmentation methods on two widely used benchmarks. Code and models are
publicly available at <https://github.com/IMPL-Lab/IAL.git>.

</details>


### [151] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/pdf/2506.08210)
*Andrew Z. Wang, Songwei Ge, Tero Karras, Ming-Yu Liu, Yogesh Balaji*

Main category: cs.CV

TL;DR: The paper explores using modern decoder-only LLMs as text encoders for text-to-image diffusion models, finding that layer-normalized averaging of embeddings outperforms traditional last-layer embeddings and T5 baselines.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models often use outdated text encoders like T5 and CLIP. The study investigates whether modern LLMs can improve performance.

Method: A standardized pipeline trains 27 models with 12 text encoders, analyzing embedding extraction methods, LLM variants, and sizes.

Result: Layer-normalized averaging of embeddings across all layers improves alignment with complex prompts, outperforming T5 and last-layer embeddings.

Conclusion: Modern LLMs, with optimized embedding extraction, enhance text-to-image generation, particularly for complex prompts.

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [152] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/pdf/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: A novel PDE framework for single-image dehazing integrates atmospheric scattering, nonlocal regularization, and dark channel prior, proving existence/uniqueness of solutions and demonstrating promising results.


<details>
  <summary>Details</summary>
Motivation: To improve single-image dehazing by combining physical models (atmospheric scattering) with mathematical techniques (PDEs, nonlocal regularization) for better accuracy and efficiency.

Method: Proposes an improved PDE with edge-preserving diffusion, adaptive regularization, and Gaussian convolution, solved via Lax-Milgram theorem and GPU-accelerated fixed-point iteration.

Result: Proves existence/uniqueness of weak solutions and shows the method is effective and generalizable to deep models.

Conclusion: The framework is a promising dehazing solution, bridging traditional PDEs and modern deep learning paradigms.

Abstract: This paper presents a novel partial differential equation (PDE) framework for
single-image dehazing. By integrating the atmospheric scattering model with
nonlocal regularization and dark channel prior, we propose the improved PDE: \[
-\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \]
where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving
diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and
$\lambda(t)$ is the adaptive regularization parameter based on transmission map
$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$
using Lax-Milgram theorem, and implement an efficient fixed-point iteration
scheme accelerated by PyTorch GPU computation. The experimental results
demonstrate that this method is a promising deghazing solution that can be
generalized to the deep model paradigm.

</details>


### [153] [Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation](https://arxiv.org/pdf/2506.08214)
*Ioannis Iakovidis, Zahra Kalantari, Amir Hossein Payberah, Fernando Jaramillo, Francisco Pena Escobar*

Main category: cs.CV

TL;DR: Self-supervised training with deep clustering and negative sampling improves wetland segmentation in radar images, outperforming supervised models.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of satellite images for wetland monitoring is slow and costly, prompting the need for self-supervised methods.

Method: Combines deep clustering and negative sampling for training without annotations, plus an ensemble approach to reduce variance.

Result: Ensemble of self-supervised models achieves a 0.02 higher Intersection Over Union (IoU) than a supervised model.

Conclusion: Self-supervised methods can effectively replace manual annotations for wetland segmentation, with ensembles enhancing performance.

Abstract: In recent years the wide availability of high-resolution radar satellite
images along with the advancement of computer vision models have enabled the
remote monitoring of the surface area of wetlands. However, these models
require large amounts of manually annotated satellite images, which are slow
and expensive to produce. To overcome this problem, self-supervised training
methods have been deployed to train models without using annotated data. In
this paper we use a combination of deep clustering and negative sampling to
train a model to segment radar satellite images into areas that separate water
from land without the use of any manual annotations. Furthermore, we implement
an ensemble version of the model to reduce variance and improve performance.
Compared to a single fully-supervised model using the same architecture, our
ensemble of self-supervised models achieves a 0.02 improvement in the
Intersection Over Union metric over our test dataset.

</details>


### [154] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/pdf/2506.08809)
*Jiaze E, Srutarshi Banerjee, Tekin Bicer, Guannan Wang, Yanfu Zhang, Bin Ren*

Main category: cs.CV

TL;DR: HiSin is a diffusion-based framework for efficient high-resolution sinogram inpainting, reducing memory and computational demands while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Missing high-frequency projections in computed tomography can cause artifacts and diagnostic errors, requiring robust inpainting methods.

Method: HiSin uses resolution-guided progressive inference, frequency-aware patch skipping, and structure-adaptive step allocation to optimize memory and computation.

Result: HiSin reduces peak memory usage by 31.25% and inference time by 18.15% without compromising accuracy.

Conclusion: HiSin effectively addresses the limitations of diffusion models for high-resolution sinogram inpainting, offering practical efficiency gains.

Abstract: High-resolution sinogram inpainting is essential for computed tomography
reconstruction, as missing high-frequency projections can lead to visible
artifacts and diagnostic errors. Diffusion models are well-suited for this task
due to their robustness and detail-preserving capabilities, but their
application to high-resolution inputs is limited by excessive memory and
computational demands. To address this limitation, we propose HiSin, a novel
diffusion based framework for efficient sinogram inpainting via
resolution-guided progressive inference. It progressively extracts global
structure at low resolution and defers high-resolution inference to small
patches, enabling memory-efficient inpainting. It further incorporates
frequency-aware patch skipping and structure-adaptive step allocation to reduce
redundant computation. Experimental results show that HiSin reduces peak memory
usage by up to 31.25% and inference time by up to 18.15%, and maintains
inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [155] [Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence](https://arxiv.org/pdf/2506.08220)
*Octave Mariotti, Zhipeng Du, Yash Bhalgat, Oisin Mac Aodha, Hakan Bilen*

Main category: cs.CV

TL;DR: A novel method for dense semantic correspondence by lifting 2D keypoints into 3D space, outperforming supervised baselines and showing unsupervised methods generalize better across datasets.


<details>
  <summary>Details</summary>
Motivation: Supervised SC methods generalize poorly beyond sparsely annotated keypoints, limiting their effectiveness.

Method: Lifts 2D keypoints into a canonical 3D space using monocular depth estimation, creating a continuous manifold without 3D supervision.

Result: Outperforms supervised baselines on unseen keypoints and shows unsupervised methods generalize better across datasets.

Conclusion: The proposed approach effectively learns robust correspondences and highlights the limitations of supervised methods.

Abstract: Semantic correspondence (SC) aims to establish semantically meaningful
matches across different instances of an object category. We illustrate how
recent supervised SC methods remain limited in their ability to generalize
beyond sparsely annotated training keypoints, effectively acting as keypoint
detectors. To address this, we propose a novel approach for learning dense
correspondences by lifting 2D keypoints into a canonical 3D space using
monocular depth estimation. Our method constructs a continuous canonical
manifold that captures object geometry without requiring explicit 3D
supervision or camera annotations. Additionally, we introduce SPair-U, an
extension of SPair-71k with novel keypoint annotations, to better assess
generalization. Experiments not only demonstrate that our model significantly
outperforms supervised baselines on unseen keypoints, highlighting its
effectiveness in learning robust correspondences, but that unsupervised
baselines outperform supervised counterparts when generalized across different
datasets.

</details>


### [156] [A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks](https://arxiv.org/pdf/2506.08227)
*Vishaal Udandarao, Mehdi Cherti, Shyamgopal Karthik, Jenia Jitsev, Samuel Albanie, Matthias Bethge*

Main category: cs.CV

TL;DR: The paper critiques 17 benchmarks for vision-language models, revealing biases and flaws in their design, and suggests improvements for more robust evaluation.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of existing benchmarks in measuring compositional understanding in vision-language models and identify their inherent biases.

Method: Analysis of design choices in benchmark construction, including data sources and curation procedures, and comparison of heuristic performance with CLIP models.

Result: Found that benchmarks are flawed due to distribution asymmetry, allowing simple heuristics to perform as well as advanced models.

Conclusion: Proposes recommendations for creating more robust benchmarks to better assess compositional understanding.

Abstract: We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for
measuring compositional understanding capabilities of vision-language models
(VLMs). We scrutinize design choices in their construction, including data
source (e.g. MS-COCO) and curation procedures (e.g. constructing negative
images/captions), uncovering several inherent biases across most benchmarks. We
find that blind heuristics (e.g. token-length, log-likelihood under a language
model) perform on par with CLIP models, indicating that these benchmarks do not
effectively measure compositional understanding. We demonstrate that the
underlying factor is a distribution asymmetry between positive and negative
images/captions, induced by the benchmark construction procedures. To mitigate
these issues, we provide a few key recommendations for constructing more robust
vision-language compositional understanding benchmarks, that would be less
prone to such simple attacks.

</details>


### [157] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/pdf/2506.08257)
*L. Lao Beyer, T. Li, X. Chen, S. Karaman, K. He*

Main category: cs.CV

TL;DR: 1D image tokenizers compress images into 1D sequences, enabling fine-grained editing and generation via heuristic token manipulation and test-time optimization.


<details>
  <summary>Details</summary>
Motivation: To explore the expressivity and capabilities of 1D tokenizers for image editing and generation without training generative models.

Method: Uses gradient-based test-time optimization of tokens with plug-and-play loss functions (e.g., reconstruction, CLIP similarity) for tasks like inpainting and text-guided editing.

Result: Demonstrates fine-grained editing (e.g., appearance transfer) and diverse, realistic image generation without training.

Conclusion: 1D tokenizers offer powerful, training-free solutions for image manipulation and generation.

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [158] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/pdf/2506.08279)
*Aditi Sundararaman, Amogh Adishesha, Andrew Jaegle, Dan Bigioi, Hyoung-Kyu Song, Jon Kyl, Justin Mao, Kevin Lan, Mojtaba Komeili, ShahRukh Athar, Sheila Babayan, Stanislau Beliasau, William Buchwalter*

Main category: cs.CV

TL;DR: Mirage is an audio-to-video foundation model that generates realistic, expressive video from audio inputs, outperforming existing methods in quality and generality.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods either ignore sound or are domain-specific. Mirage aims to integrate audio and visual elements for more compelling, general-purpose video generation.

Method: Mirage uses a unified self-attention-based training approach for audio-to-video generation, either from scratch or with existing weights, without relying on domain-specific architectures or losses.

Result: Mirage produces high-quality, believable video performances from audio inputs, especially when combined with speech synthesis (TTS).

Conclusion: Mirage advances audio-to-video generation by offering superior quality and generality, demonstrated through realistic outputs.

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [159] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/pdf/2506.08297)
*Nhat Thanh Tran, Fanghui Xue, Shuai Zhang, Jiancheng Lyu, Yunling Zheng, Yingyong Qi, Jack Xin*

Main category: cs.CV

TL;DR: The paper introduces SEMA, a scalable and efficient attention mechanism for computer vision, addressing the limitations of vanilla and linear attention by leveraging token localization and arithmetic averaging.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of vanilla attention and the lack of focus in linear attention pose challenges for vision tasks. The dispersion property of generalized attention further motivates the need for a better solution.

Method: SEMA is designed with token localization to prevent dispersion and maintain focus, combined with arithmetic averaging to capture global attention aspects.

Result: SEMA outperforms recent vision Mamba models on Imagenet-1k, demonstrating scalability and effectiveness, especially for larger image scales.

Conclusion: SEMA is a viable alternative to linear attention, offering improved performance and scalability for vision tasks.

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [160] [OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal](https://arxiv.org/pdf/2506.08299)
*Kangning Yang, Ling Ouyang, Huiming Sun, Jie Cai, Lan Fu, Jiaming Ding, Chiu Man Ho, Zibo Meng*

Main category: cs.CV

TL;DR: The paper introduces a new method for collecting high-quality reflection datasets, resulting in the OpenRR-1k dataset, which improves reflection removal robustness in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing reflection removal techniques lack high-quality in-the-wild datasets, limiting their effectiveness.

Method: A novel paradigm for collecting reflection datasets that is convenient, cost-effective, and scalable, ensuring high-quality, aligned, and diverse image pairs.

Result: The OpenRR-1k dataset with 1,000 high-quality transmission-reflection pairs is created and shown to enhance reflection removal robustness.

Conclusion: The proposed dataset and collection method effectively address the limitations of existing techniques, improving performance in real-world environments.

Abstract: Reflection removal technology plays a crucial role in photography and
computer vision applications. However, existing techniques are hindered by the
lack of high-quality in-the-wild datasets. In this paper, we propose a novel
paradigm for collecting reflection datasets from a fresh perspective. Our
approach is convenient, cost-effective, and scalable, while ensuring that the
collected data pairs are of high quality, perfectly aligned, and represent
natural and diverse scenarios. Following this paradigm, we collect a
Real-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which
contains 1,000 high-quality transmission-reflection image pairs collected in
the wild. Through the analysis of several reflection removal methods and
benchmark evaluation experiments on our dataset, we demonstrate its
effectiveness in improving robustness in challenging real-world environments.
Our dataset is available at https://github.com/caijie0620/OpenRR-1k.

</details>


### [161] [Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating](https://arxiv.org/pdf/2506.08324)
*Guandong Li, Mengxia Ye*

Main category: cs.CV

TL;DR: STNet, a novel network architecture, addresses hyperspectral image classification challenges by decoupling spatial-spectral attention and using gating mechanisms, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Challenges like high-dimensional data, sparse ground objects, and spectral redundancy in hyperspectral image classification lead to overfitting and limited generalization.

Method: STNet uses a Spatial-Spectral Transformer module with decoupled attention and gating mechanisms (adaptive attention fusion gating and GFFN) for targeted feature extraction and fusion.

Result: STNet outperforms mainstream methods on IN, UP, and KSC datasets, enhancing representation without increasing network depth or width.

Conclusion: STNet effectively addresses hyperspectral classification challenges, reducing overfitting and improving generalization with innovative attention and gating designs.

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more effectively extract
and fuse spatial context with fine spectral information in hyperspectral image
(HSI) classification, this paper proposes a novel network architecture called
STNet. The core advantage of STNet stems from the dual innovative design of its
Spatial-Spectral Transformer module: first, the fundamental explicit decoupling
of spatial and spectral attention ensures targeted capture of key information
in HSI; second, two functionally distinct gating mechanisms perform intelligent
regulation at both the fusion level of attention flows (adaptive attention
fusion gating) and the internal level of feature transformation (GFFN). This
characteristic demonstrates superior feature extraction and fusion capabilities
compared to traditional convolutional neural networks, while reducing
overfitting risks in small-sample and high-noise scenarios. STNet enhances
model representation capability without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [162] [Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera](https://arxiv.org/pdf/2506.08327)
*Yuto Kase, Kai Ishibe, Ryoma Yasuda, Yudai Washida, Sakiko Hashimoto*

Main category: cs.CV

TL;DR: Proposes a real-time method using event cameras to locate tennis ball impact on a racket, overcoming high-speed camera limitations like memory use and manual errors.


<details>
  <summary>Details</summary>
Motivation: High-speed cameras for impact location are memory-intensive and error-prone, hindering prolonged performance analysis.

Method: Uses event cameras for efficient brightness change detection, with three steps: swing time range, impact timing, and ball/racket contours. Combines conventional vision and event-based processing (PATS).

Result: Experimental results fit permissible performance measurement ranges, with computation suitable for real-time use.

Conclusion: Event cameras enable efficient, real-time impact location, aiding prolonged performance monitoring in tennis.

Abstract: In racket sports, such as tennis, locating the ball's position at impact is
important in clarifying player and equipment characteristics, thereby aiding in
personalized equipment design. High-speed cameras are used to measure the
impact location; however, their excessive memory consumption limits prolonged
scene capture, and manual digitization for position detection is time-consuming
and prone to human error. These limitations make it difficult to effectively
capture the entire playing scene, hindering the ability to analyze the player's
performance. We propose a method for locating the tennis ball impact on the
racket in real time using an event camera. Event cameras efficiently measure
brightness changes (called `events') with microsecond accuracy under high-speed
motion while using lower memory consumption. These cameras enable users to
continuously monitor their performance over extended periods. Our method
consists of three identification steps: time range of swing, timing at impact,
and contours of ball and racket. Conventional computer vision techniques are
utilized along with an original event-based processing to detect the timing at
impact (PATS: the amount of polarity asymmetry in time symmetry). The results
of the experiments were within the permissible range for measuring tennis
players' performance. Moreover, the computation time was sufficiently short for
real-time applications.

</details>


### [163] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/pdf/2506.08351)
*Huixuan Zhang, Junzhe Zhang, Xiaojun Wan*

Main category: cs.CV

TL;DR: Proposes Step AG, a simple adaptive guidance strategy for text-to-vision diffusion models, reducing computational costs by 20-30% while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Classifier-free guidance in diffusion models doubles computational steps, increasing costs. Existing adaptive methods lack analysis and general applicability.

Method: Introduces Step AG, restricting classifier-free guidance to early denoising steps. Evaluates on image quality and text alignment.

Result: Achieves 20-30% speedup without compromising quality, consistent across models and settings.

Conclusion: Step AG is a universally applicable, efficient alternative to traditional classifier-free guidance.

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [164] [MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding](https://arxiv.org/pdf/2506.08356)
*Shivang Chopra, Lingchao Mao, Gabriela Sanchez-Rodriguez, Andrew J Feola, Jing Li, Zsolt Kira*

Main category: cs.CV

TL;DR: MedMoE is a vision-language framework that dynamically adapts visual representation for medical imaging by using modality-specific experts, improving alignment and retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks use uniform feature extraction, ignoring modality-specific needs in medical imaging. MedMoE addresses this gap.

Method: Uses a Mixture-of-Experts (MoE) module with Swin Transformer backbone to route multi-scale features through modality-specialized branches.

Result: Improves alignment and retrieval performance across diverse medical benchmarks.

Conclusion: Modality-specialized visual representations enhance clinical vision-language systems.

Abstract: Different medical imaging modalities capture diagnostic information at
varying spatial resolutions, from coarse global patterns to fine-grained
localized structures. However, most existing vision-language frameworks in the
medical domain apply a uniform strategy for local feature extraction,
overlooking the modality-specific demands. In this work, we present MedMoE, a
modular and extensible vision-language processing framework that dynamically
adapts visual representation based on the diagnostic context. MedMoE
incorporates a Mixture-of-Experts (MoE) module conditioned on the report type,
which routes multi-scale image features through specialized expert branches
trained to capture modality-specific visual semantics. These experts operate
over feature pyramids derived from a Swin Transformer backbone, enabling
spatially adaptive attention to clinically relevant regions. This framework
produces localized visual representations aligned with textual descriptions,
without requiring modality-specific supervision at inference. Empirical results
on diverse medical benchmarks demonstrate that MedMoE improves alignment and
retrieval performance across imaging modalities, underscoring the value of
modality-specialized visual representations in clinical vision-language
systems.

</details>


### [165] [Image Demoiréing Using Dual Camera Fusion on Mobile Phones](https://arxiv.org/pdf/2506.08361)
*Yanting Mei, Zhilu Zhang, Xiaohe Wu, Wangmeng Zuo*

Main category: cs.CV

TL;DR: The paper proposes a dual-camera fusion method (DCID) for removing moiré patterns in images, leveraging ultra-wide-angle (UW) images to assist wide-angle (W) images, achieving better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: Moiré patterns degrade image quality when shooting screens. Existing methods struggle with large moiré. Modern smartphones often have dual lenses (UW and W), and UW images can provide cleaner textures due to different focal lengths.

Method: DCID integrates a lightweight UW image encoder into a demoiréing network and uses a fast two-stage alignment. A large dataset (9,000 samples) was created for validation.

Result: Experiments show DCID outperforms state-of-the-art methods.

Conclusion: The proposed DCID method effectively removes moiré patterns by leveraging dual-camera fusion, validated by a new dataset.

Abstract: When shooting electronic screens, moir\'e patterns usually appear in captured
images, which seriously affects the image quality. Existing image demoir\'eing
methods face great challenges in removing large and heavy moir\'e. To address
the issue, we propose to utilize Dual Camera fusion for Image Demoir\'eing
(DCID), \ie, using the ultra-wide-angle (UW) image to assist the moir\'e
removal of wide-angle (W) image. This is inspired by two motivations: (1) the
two lenses are commonly equipped with modern smartphones, (2) the UW image
generally can provide normal colors and textures when moir\'e exists in the W
image mainly due to their different focal lengths. In particular, we propose an
efficient DCID method, where a lightweight UW image encoder is integrated into
an existing demoir\'eing network and a fast two-stage image alignment manner is
present. Moreover, we construct a large-scale real-world dataset with diverse
mobile phones and monitors, containing about 9,000 samples. Experiments on the
dataset show our method performs better than state-of-the-art methods. Code and
dataset are available at https://github.com/Mrduckk/DCID.

</details>


### [166] [SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding](https://arxiv.org/pdf/2506.08391)
*Woohyeon Park, Woojin Kim, Jaeik Kim, Jaeyoung Do*

Main category: cs.CV

TL;DR: SECOND improves VLMs by reducing object hallucination through selective and contrastive decoding of multi-scale visual information.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of object hallucination in VLMs to enhance visual understanding accuracy.

Method: Proposes SECOND, which selectively integrates and contrasts multi-scale visual information in an object-centric manner.

Result: SECOND reduces perceptual hallucinations and outperforms benchmarks.

Conclusion: Multi-scale visual information prioritization and contrast in VLMs show unexplored potential, outperforming existing methods.

Abstract: Despite significant advancements in Vision-Language Models (VLMs), the
performance of existing VLMs remains hindered by object hallucination, a
critical challenge to achieving accurate visual understanding. To address this
issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach
that enables VLMs to effectively leverage multi-scale visual information with
an object-centric manner, closely aligning with human visual perception. SECOND
progressively selects and integrates multi-scale visual information,
facilitating a more precise interpretation of images. By contrasting these
visual information iteratively, SECOND significantly reduces perceptual
hallucinations and outperforms a wide range of benchmarks. Our theoretical
analysis and experiments highlight the largely unexplored potential of
multi-scale application in VLMs, showing that prioritizing and contrasting
across scales outperforms existing methods.

</details>


### [167] [RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation](https://arxiv.org/pdf/2506.08418)
*Taiqin Chen, Zikun Zhou, Zheng Fang, Wenzhen Zou, Kanjun Liu, Ke Chen, Yongbing Zhang, Yaowei Wang*

Main category: cs.CV

TL;DR: The paper proposes RadioDUN, a deep unfolding network for dense radio map estimation, integrating physical propagation models and adaptive learning to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for radio map estimation lack integration with physical characteristics, leading to inefficiencies.

Method: The paper casts radio map estimation as a sparse signal recovery problem, using a physical propagation model and proposing RadioDUN with a dynamic reweighting module (DRM) and shadowing loss.

Result: RadioDUN outperforms state-of-the-art methods in experiments.

Conclusion: The proposed method effectively integrates physical characteristics into deep learning for improved radio map estimation.

Abstract: The radio map represents the spatial distribution of spectrum resources
within a region, supporting efficient resource allocation and interference
mitigation. However, it is difficult to construct a dense radio map as a
limited number of samples can be measured in practical scenarios. While
existing works have used deep learning to estimate dense radio maps from sparse
samples, they are hard to integrate with the physical characteristics of the
radio map. To address this challenge, we cast radio map estimation as the
sparse signal recovery problem. A physical propagation model is further
incorporated to decompose the problem into multiple factor optimization
sub-problems, thereby reducing recovery complexity. Inspired by the existing
compressive sensing methods, we propose the Radio Deep Unfolding Network
(RadioDUN) to unfold the optimization process, achieving adaptive parameter
adjusting and prior fitting in a learnable manner. To account for the radio
propagation characteristics, we develop a dynamic reweighting module (DRM) to
adaptively model the importance of each factor for the radio map. Inspired by
the shadowing factor in the physical propagation model, we integrate
obstacle-related factors to express the obstacle-induced signal stochastic
decay. The shadowing loss is further designed to constrain the factor
prediction and act as a supplementary supervised objective, which enhances the
performance of RadioDUN. Extensive experiments have been conducted to
demonstrate that the proposed method outperforms the state-of-the-art methods.
Our code will be made publicly available upon publication.

</details>


### [168] [Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring](https://arxiv.org/pdf/2506.08429)
*Mingjie Xu, Andrew Estornell, Hongzheng Yang, Yuzhi Zhao, Zhaowei Zhu, Qi Xuan, Jiaheng Wei*

Main category: cs.CV

TL;DR: SCALE is a quality-driven data selection pipeline for VLM instruction tuning, addressing noisy alignments and ambiguous text by evaluating cross-modality alignment and data quality.


<details>
  <summary>Details</summary>
Motivation: Current VLMs rely on large-scale datasets, but noisy alignments and misleading text hinder performance. SCALE aims to improve dataset quality for better visual understanding.

Method: SCALE integrates a cross-modality framework to assign tasks, generate captions, and evaluate alignment, clarity, task rarity, text coherence, and image clarity.

Result: SCALE shows unimodal methods overlook essential samples and that generated captions efficiently unify multimodal tasks into text.

Conclusion: SCALE improves VLM performance by enhancing dataset quality and alignment, addressing key challenges in visual instruction tuning.

Abstract: The application of visual instruction tuning and other post-training
techniques has significantly enhanced the capabilities of Large Language Models
(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with
more comprehensive visual language datasets. However, the effectiveness of VLMs
is highly dependent on large-scale, high-quality datasets that ensure precise
recognition and accurate reasoning. Two key challenges hinder progress: (1)
noisy alignments between images and the corresponding text, which leads to
misinterpretation, and (2) ambiguous or misleading text, which obscures visual
content. To address these challenges, we propose SCALE (Single modality data
quality and Cross modality Alignment Evaluation), a novel quality-driven data
selection pipeline for VLM instruction tuning datasets. Specifically, SCALE
integrates a cross-modality assessment framework that first assigns each data
entry to its appropriate vision-language task, generates general and
task-specific captions (covering scenes, objects, style, etc.), and evaluates
the alignment, clarity, task rarity, text coherence, and image clarity of each
entry based on the generated captions. We reveal that: (1) current unimodal
quality assessment methods evaluate one modality while overlooking the rest,
which can underestimate samples essential for specific tasks and discard the
lower-quality instances that help build model robustness; and (2) appropriately
generated image captions provide an efficient way to transfer the image-text
multimodal task into a unified text modality.

</details>


### [169] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/pdf/2506.08456)
*June Suk Choi, Kyungmin Lee, Sihyun Yu, Yisol Choi, Jinwoo Shin, Kimin Lee*

Main category: cs.CV

TL;DR: The paper addresses the issue of static videos in image-to-video (I2V) generation by proposing adaptive low-pass guidance (ALG), which improves motion dynamics without sacrificing image quality.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning text-to-video (T2V) models for I2V often results in static videos due to premature exposure to high-frequency details in input images, biasing the sampling process.

Method: The authors propose ALG, which adaptively applies low-pass filtering to the conditioning image during early denoising stages to enhance motion dynamics.

Result: ALG improves temporal dynamics by 36% in dynamic degree (measured by VBench-I2V) while maintaining image fidelity and text alignment.

Conclusion: ALG effectively addresses the static video issue in I2V generation, offering a simple yet impactful solution.

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in
producing high-quality, dynamic videos. To improve the visual controllability,
recent works have considered fine-tuning pre-trained T2V models to support
image-to-video (I2V) generation. However, such adaptation frequently suppresses
motion dynamics of generated outputs, resulting in more static videos compared
to their T2V counterparts. In this work, we analyze this phenomenon and
identify that it stems from the premature exposure to high-frequency details in
the input image, which biases the sampling process toward a shortcut trajectory
that overfits to the static appearance of the reference image. To address this,
we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model
sampling procedure to generate more dynamic videos without compromising
per-frame image quality. Specifically, ALG adaptively modulates the frequency
content of the conditioning image by applying low-pass filtering at the early
stage of denoising. Extensive experiments demonstrate that ALG significantly
improves the temporal dynamics of generated videos, while preserving image
fidelity and text alignment. Especially, under VBench-I2V test suite, ALG
achieves an average improvement of 36% in dynamic degree without a significant
drop in video quality or image fidelity.

</details>


### [170] [MARMOT: Masked Autoencoder for Modeling Transient Imaging](https://arxiv.org/pdf/2506.08470)
*Siyuan Shen, Ziheng Wang, Xingyue Peng, Suan Xia, Ruiqian Li, Shiying Li, Jingyi Yu*

Main category: cs.CV

TL;DR: MARMOT is a masked autoencoder for transient imaging, pretrained on NLOS datasets, outperforming state-of-the-art methods in reconstructing hidden objects.


<details>
  <summary>Details</summary>
Motivation: Existing NLOS transient methods lack dataset-learned priors; MARMOT aims to leverage pretraining for better performance.

Method: Uses a Transformer-based encoder-decoder with a scanning pattern mask for self-supervised pretraining on 500K synthesized transients.

Result: MARMOT shows superior efficiency in downstream tasks via direct feature transfer or finetuning.

Conclusion: MARMOT advances NLOS imaging by integrating pretraining, demonstrating effectiveness in transient modeling.

Abstract: Pretrained models have demonstrated impressive success in many modalities
such as language and vision. Recent works facilitate the pretraining paradigm
in imaging research. Transients are a novel modality, which are captured for an
object as photon counts versus arrival times using a precisely time-resolved
sensor. In particular for non-line-of-sight (NLOS) scenarios, transients of
hidden objects are measured beyond the sensor's direct line of sight. Using
NLOS transients, the majority of previous works optimize volume density or
surfaces to reconstruct the hidden objects and do not transfer priors learned
from datasets. In this work, we present a masked autoencoder for modeling
transient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a
self-supervised model pretrianed on massive and diverse NLOS transient
datasets. Using a Transformer-based encoder-decoder, MARMOT learns features
from partially masked transients via a scanning pattern mask (SPM), where the
unmasked subset is functionally equivalent to arbitrary sampling, and predicts
full measurements. Pretrained on TransVerse-a synthesized transient dataset of
500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature
transfer or decoder finetuning. Comprehensive experiments are carried out in
comparisons with state-of-the-art methods. Quantitative and qualitative results
demonstrate the efficiency of our MARMOT.

</details>


### [171] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/pdf/2506.08512)
*Zhiyi Zhu, Xiaoyu Wu, Zihao Liu, Linlin Yang*

Main category: cs.CV

TL;DR: MLVTG improves video temporal grounding by combining MambaAligner for temporal modeling and LLMRefiner for semantic alignment, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based VTG methods face issues like redundant attention and poor multi-modal alignment, limiting performance.

Method: MLVTG uses MambaAligner (Vision Mamba blocks) for temporal modeling and LLMRefiner (frozen LLM layer) for semantic alignment.

Result: MLVTG achieves state-of-the-art performance on QVHighlights, Charades-STA, and TVSum datasets.

Conclusion: The dual alignment strategy in MLVTG effectively addresses VTG challenges, offering superior localization accuracy.

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [172] [Robust Visual Localization via Semantic-Guided Multi-Scale Transformer](https://arxiv.org/pdf/2506.08526)
*Zhongtao Tian, Wenhao Huang, Zhidong Chen, Xiao Wei Sun*

Main category: cs.CV

TL;DR: A framework combining multi-scale feature learning and semantic scene understanding improves visual localization in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Challenges in visual localization due to dynamic conditions like lighting changes, weather, and moving objects disrupt appearance cues.

Method: Hierarchical Transformer with cross-scale attention and semantic supervision via neural scene representation.

Result: Outperforms existing pose regression methods in dynamic scenarios on TartanAir dataset.

Conclusion: Multi-scale processing with semantic guidance enhances robustness in real-world dynamic environments.

Abstract: Visual localization remains challenging in dynamic environments where
fluctuating lighting, adverse weather, and moving objects disrupt appearance
cues. Despite advances in feature representation, current absolute pose
regression methods struggle to maintain consistency under varying conditions.
To address this challenge, we propose a framework that synergistically combines
multi-scale feature learning with semantic scene understanding. Our approach
employs a hierarchical Transformer with cross-scale attention to fuse geometric
details and contextual cues, preserving spatial precision while adapting to
environmental changes. We improve the performance of this architecture with
semantic supervision via neural scene representation during training, guiding
the network to learn view-invariant features that encode persistent structural
information while suppressing complex environmental interference. Experiments
on TartanAir demonstrate that our approach outperforms existing pose regression
methods in challenging scenarios with dynamic objects, illumination changes,
and occlusions. Our findings show that integrating multi-scale processing with
semantic guidance offers a promising strategy for robust visual localization in
real-world dynamic environments.

</details>


### [173] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/pdf/2506.08529)
*Xijun Wang, Xin Li, Bingchen Li, Zhibo Chen*

Main category: cs.CV

TL;DR: LiftVSR is an efficient video super-resolution framework using diffusion models, achieving state-of-the-art results with reduced computational costs via hybrid temporal modeling.


<details>
  <summary>Details</summary>
Motivation: Existing VSR methods suffer from limited temporal coherence and high computational costs, especially for long videos.

Method: LiftVSR combines Dynamic Temporal Attention (DTA) for short-term modeling and Attention Memory Cache (AMC) for long-term consistency, with an asymmetric sampling strategy for stable inference.

Result: LiftVSR achieves impressive performance on VSR benchmarks with significantly lower computational costs (4×RTX 4090 GPUs).

Conclusion: LiftVSR balances efficiency and long-term consistency, advancing VSR with practical computational feasibility.

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by
enhancing perceptual quality, largely through elaborately designed temporal
modeling to ensure inter-frame consistency. However, existing methods usually
suffer from limited temporal coherence and prohibitively high computational
costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for
long videos. In this work, we propose LiftVSR, an efficient VSR framework that
leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$,
achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To
balance long-term consistency and efficiency, we introduce a hybrid temporal
modeling mechanism that decomposes temporal learning into two complementary
components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal
modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii)
Attention Memory Cache (AMC) for long-term temporal modeling across segments
($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token
flows across frames within multi-head query and key tokens to warp inter-frame
contexts in the value tokens. AMC adaptively aggregates historical segment
information via a cache unit, ensuring long-term coherence with minimal
overhead. To further stabilize the cache interaction during inference, we
introduce an asymmetric sampling strategy that mitigates feature mismatches
arising from different diffusion sampling steps. Extensive experiments on
several typical VSR benchmarks have demonstrated that LiftVSR achieves
impressive performance with significantly lower computational costs.

</details>


### [174] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/pdf/2506.08541)
*Qi Yan, Brian Zhang, Yutong Zhang, Daniel Yang, Joshua White, Di Chen, Jiachao Liu, Langechuan Liu, Binnan Zhuang, Shaoshuai Shi, Renjie Liao*

Main category: cs.CV

TL;DR: TrajFlow is a flow matching-based motion prediction framework for autonomous driving, offering efficient multi-modal trajectory forecasts in a single pass, with improved uncertainty estimation and generalization.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety and informed decision-making in autonomous driving under dynamic conditions requires efficient and accurate motion prediction, particularly for multi-modal forecasts.

Method: TrajFlow uses flow matching to predict multiple trajectories in one pass, employs a ranking loss for uncertainty estimation, and a self-conditioning training technique for better generalization.

Result: TrajFlow achieves state-of-the-art performance on the Waymo Open Motion Dataset, demonstrating efficiency and accuracy.

Conclusion: TrajFlow is effective for safety-critical autonomous driving applications, offering scalable and efficient motion prediction.

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [175] [Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs](https://arxiv.org/pdf/2506.08543)
*Bowei Tian, Xuntao Lyu, Meng Liu, Hongyi Wang, Ang Li*

Main category: cs.CV

TL;DR: The paper introduces the Input-Space Linearity Hypothesis (ISLH) and the Spectral Principal Path (SPP) framework to explain how deep networks form human-interpretable representations, validated with Vision-Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: To enhance AI transparency and control by understanding how deep networks develop structured, human-interpretable representations.

Method: Proposes ISLH and the SPP framework to analyze how linear representations form in deep networks, with empirical validation using VLMs.

Result: Demonstrates that concept-aligned directions originate in the input space and are amplified with depth, showing robustness in VLMs.

Conclusion: Advances a structured theory of representation formation, contributing to AI robustness, fairness, and transparency.

Abstract: High-level representations have become a central focus in enhancing AI
transparency and control, shifting attention from individual neurons or
circuits to structured semantic directions that align with human-interpretable
concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose
the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned
directions originate in the input space and are selectively amplified with
increasing depth. We then introduce the Spectral Principal Path (SPP)
framework, which formalizes how deep networks progressively distill linear
representations along a small set of dominant spectral directions. Building on
this framework, we further demonstrate the multimodal robustness of these
representations in Vision-Language Models (VLMs). By bridging theoretical
insights with empirical validation, this work advances a structured theory of
representation formation in deep networks, paving the way for improving AI
robustness, fairness, and transparency.

</details>


### [176] [From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge](https://arxiv.org/pdf/2506.08553)
*Agnese Taluzzi, Davide Gesualdi, Riccardo Santambrogio, Chiara Plizzari, Francesca Palermo, Simone Mentasti, Matteo Matteucci*

Main category: cs.CV

TL;DR: SceneNet and KnowledgeNet improve VQA performance by combining scene graphs and commonsense knowledge, achieving 44.21% accuracy on HD-EPIC.


<details>
  <summary>Details</summary>
Motivation: To address complex egocentric VQA tasks by capturing fine-grained object interactions and leveraging external commonsense knowledge.

Method: SceneNet uses MLLM-generated scene graphs; KnowledgeNet integrates ConceptNet for semantic reasoning.

Result: Combined framework achieves 44.21% accuracy on HD-EPIC benchmark.

Conclusion: The integration of SceneNet and KnowledgeNet effectively handles complex VQA tasks.

Abstract: This report presents SceneNet and KnowledgeNet, our approaches developed for
the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with
a multi-modal large language model (MLLM) to capture fine-grained object
interactions, spatial relationships, and temporally grounded events. In
parallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge
to introduce high-level semantic connections between entities, enabling
reasoning beyond directly observable visual evidence. Each method demonstrates
distinct strengths across the seven categories of the HD-EPIC benchmark, and
their combination within our framework results in an overall accuracy of 44.21%
on the challenge, highlighting its effectiveness for complex egocentric VQA
tasks.

</details>


### [177] [Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement](https://arxiv.org/pdf/2506.08555)
*Xinyue Niu, Akira Furui*

Main category: cs.CV

TL;DR: Proposes a dual-branch adversarial neural network for cross-subject EMG pattern recognition without calibration, improving generalization and enabling biometric applications.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of inter-subject variability in EMG signals, eliminating the need for time-consuming subject-specific calibration.

Method: Uses an end-to-end dual-branch adversarial neural network to disentangle EMG features into pattern-specific and subject-specific components.

Result: Achieves robust performance on unseen users, outperforming baseline methods in cross-subject scenarios.

Conclusion: Offers a calibration-free approach for EMG pattern recognition and potential for task-independent biometric systems.

Abstract: Cross-subject electromyography (EMG) pattern recognition faces significant
challenges due to inter-subject variability in muscle anatomy, electrode
placement, and signal characteristics. Traditional methods rely on
subject-specific calibration data to adapt models to new users, an approach
that is both time-consuming and impractical for large-scale, real-world
deployment. This paper presents an approach to eliminate calibration
requirements through feature disentanglement, enabling effective cross-subject
generalization. We propose an end-to-end dual-branch adversarial neural network
that simultaneously performs pattern recognition and individual identification
by disentangling EMG features into pattern-specific and subject-specific
components. The pattern-specific components facilitate robust pattern
recognition for new users without model calibration, while the subject-specific
components enable downstream applications such as task-invariant biometric
identification. Experimental results demonstrate that the proposed model
achieves robust performance on data from unseen users, outperforming various
baseline methods in cross-subject scenarios. Overall, this study offers a new
perspective for cross-subject EMG pattern recognition without model calibration
and highlights the proposed model's potential for broader applications, such as
task-independent biometric systems.

</details>


### [178] [Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection](https://arxiv.org/pdf/2506.08562)
*Duc Thanh Pham, Hong Dang Nguyen, Nhat Minh Nguyen Quoc, Linh Ngo Van, Sang Dinh Viet, Duc Anh Nguyen*

Main category: cs.CV

TL;DR: Hier-DETR, a novel IOD framework, improves efficiency and performance using Neural Collapse and hierarchical class relations.


<details>
  <summary>Details</summary>
Motivation: Addressing the impracticality of existing IOD models due to limited performance and slow inference time.

Method: Leverages Neural Collapse for imbalance datasets and hierarchical class label relations.

Result: Ensures efficiency and competitive performance in incremental object detection.

Conclusion: Hier-DETR offers a practical solution for IOD with improved performance and speed.

Abstract: Recently, object detection models have witnessed notable performance
improvements, particularly with transformer-based models. However, new objects
frequently appear in the real world, requiring detection models to continually
learn without suffering from catastrophic forgetting. Although Incremental
Object Detection (IOD) has emerged to address this challenge, these existing
models are still not practical due to their limited performance and prolonged
inference time. In this paper, we introduce a novel framework for IOD, called
Hier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both
efficiency and competitive performance by leveraging Neural Collapse for
imbalance dataset and Hierarchical relation of classes' labels.

</details>


### [179] [Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations](https://arxiv.org/pdf/2506.08566)
*Yibo Cui, Liang Xie, Yu Zhao, Jiawei Sun, Erwei Yin*

Main category: cs.CV

TL;DR: FCA-NIG is a generative framework for creating fine-grained cross-modal annotations in Vision-Language Navigation (VLN), improving agent performance by addressing sub-instruction-level and entity-level alignment gaps.


<details>
  <summary>Details</summary>
Motivation: Existing VLN datasets lack fine-grained cross-modal alignments (sub-instruction and entity levels), hindering accurate navigation decision-making.

Method: FCA-NIG divides trajectories into sub-trajectories, uses GLIP for landmark detection, OFA-Speaker for instruction generation, and CLIP for entity selection, creating aligned sub-instruction-trajectory pairs.

Result: FCA-R2R dataset enhances VLN agent performance (e.g., SF, EnvDrop) by improving state awareness and navigation accuracy.

Conclusion: FCA-NIG generates scalable, high-quality training data without manual annotation, advancing fine-grained cross-modal learning in VLN.

Abstract: Vision-Language Navigation (VLN) enables intelligent agents to navigate
environments by integrating visual perception and natural language
instructions, yet faces significant challenges due to the scarcity of
fine-grained cross-modal alignment annotations. Existing datasets primarily
focus on global instruction-trajectory matching, neglecting
sub-instruction-level and entity-level alignments critical for accurate
navigation action decision-making. To address this limitation, we propose
FCA-NIG, a generative framework that automatically constructs navigation
instructions with dual-level fine-grained cross-modal annotations. In this
framework, an augmented trajectory is first divided into sub-trajectories,
which are then processed through GLIP-based landmark detection, crafted
instruction construction, OFA-Speaker based R2R-like instruction generation,
and CLIP-powered entity selection, generating sub-instruction-trajectory pairs
with entity-landmark annotations. Finally, these sub-pairs are aggregated to
form a complete instruction-trajectory pair. The framework generates the
FCA-R2R dataset, the first large-scale augmentation dataset featuring precise
sub-instruction-sub-trajectory and entity-landmark alignments. Extensive
experiments demonstrate that training with FCA-R2R significantly improves the
performance of multiple state-of-the-art VLN agents, including SF, EnvDrop,
RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances
agents' state awareness and decision accuracy, while entity-landmark alignment
further boosts navigation performance and generalization. These results
highlight the effectiveness of FCA-NIG in generating high-quality, scalable
training data without manual annotation, advancing fine-grained cross-modal
learning in complex navigation tasks.

</details>


### [180] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/pdf/2506.08596)
*Guyang Zhang, Waleed Abdulla*

Main category: cs.CV

TL;DR: A survey of Transformer-based HSI classification, reviewing 300+ papers, categorizing pipeline stages, and addressing challenges like data scarcity and computational overhead.


<details>
  <summary>Details</summary>
Motivation: Transformers are underutilized in HSI despite their effectiveness for long-range dependencies. The study aims to guide researchers in adapting Transformers for HSI.

Method: Review and categorization of Transformer-based HSI classification pipelines, including pre-processing, tokenization, feature extraction, and attention variants.

Result: Identified persistent challenges (e.g., labeled data scarcity, computational overhead) and outlined a research agenda for future work.

Conclusion: The survey provides a roadmap for researchers to optimize Transformers for HSI, focusing on practical solutions and interpretability.

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [181] [Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation](https://arxiv.org/pdf/2506.08611)
*Shiji Zhao, Chi Chen, Ranjie Duan, Xizhe Wang, Xingxing Wei*

Main category: cs.CV

TL;DR: The paper addresses the robust fairness issue in Adversarial Training (AT) and Adversarial Robustness Distillation (ARD), proposing Anti-Bias Soft Label Distillation (ABSLD) to enhance fairness by adjusting class-wise label smoothness.


<details>
  <summary>Details</summary>
Motivation: AT and ARD exhibit robust fairness issues, favoring some classes over others. The paper aims to understand and mitigate this bias.

Method: Proposes ABSLD, which adjusts class-wise smoothness of teacher's soft labels using varying temperatures to reduce error risk gaps between classes.

Result: ABSLD outperforms state-of-the-art methods in robustness and fairness, as shown in extensive experiments.

Conclusion: ABSLD effectively enhances adversarial robust fairness and is adaptable with other methods.

Abstract: Adversarial Training (AT) is widely recognized as an effective approach to
enhance the adversarial robustness of Deep Neural Networks. As a variant of AT,
Adversarial Robustness Distillation (ARD) has shown outstanding performance in
enhancing the robustness of small models. However, both AT and ARD face robust
fairness issue: these models tend to display strong adversarial robustness
against some classes (easy classes) while demonstrating weak adversarial
robustness against others (hard classes). This paper explores the underlying
factors of this problem and points out the smoothness degree of soft labels for
different classes significantly impacts the robust fairness from both empirical
observation and theoretical analysis. Based on the above exploration, we
propose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge
Distillation framework to enhance the adversarial robust fairness.
Specifically, ABSLD adaptively reduces the student's error risk gap between
different classes, which is accomplished by adjusting the class-wise smoothness
degree of teacher's soft labels during the training process, and the adjustment
is managed by assigning varying temperatures to different classes.
Additionally, as a label-based approach, ABSLD is highly adaptable and can be
integrated with the sample-based methods. Extensive experiments demonstrate
ABSLD outperforms state-of-the-art methods on the comprehensive performance of
robustness and fairness.

</details>


### [182] [Data-Efficient Challenges in Visual Inductive Priors: A Retrospective](https://arxiv.org/pdf/2506.08612)
*Robert-Jan Bruintjes, Attila Lengyel, Osman Semih Kayhan, Davide Zambrano, Nergis Tömen, Hadi Jamali-Rad, Jan van Gemert*

Main category: cs.CV

TL;DR: The paper explores deep learning methods for data-deficient settings through the VIPriors workshop, focusing on training models from scratch with limited data and no transfer learning. Successful approaches include model ensembles and data augmentation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training deep learning models effectively in data-deficient settings, particularly for computer vision tasks.

Method: Organized the VIPriors workshop series with data-impaired challenges, restricting participants to training from scratch with limited data and no transfer learning.

Result: Successful entries used large model ensembles (Transformers and CNNs), heavy data augmentation, and prior knowledge-based methods.

Conclusion: Incorporating prior knowledge and innovative methods like ensembles and augmentation can improve data efficiency in deep learning.

Abstract: Deep Learning requires large amounts of data to train models that work well.
In data-deficient settings, performance can be degraded. We investigate which
Deep Learning methods benefit training models in a data-deficient setting, by
organizing the "VIPriors: Visual Inductive Priors for Data-Efficient Deep
Learning" workshop series, featuring four editions of data-impaired challenges.
These challenges address the problem of training deep learning models for
computer vision tasks with limited data. Participants are limited to training
models from scratch using a low number of training samples and are not allowed
to use any form of transfer learning. We aim to stimulate the development of
novel approaches that incorporate prior knowledge to improve the data
efficiency of deep learning models. Successful challenge entries make use of
large model ensembles that mix Transformers and CNNs, as well as heavy data
augmentation. Novel prior knowledge-based methods contribute to success in some
entries.

</details>


### [183] [SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything](https://arxiv.org/pdf/2506.08613)
*Joost van Dalen, Yuki M. Asano, Marc Russwurm*

Main category: cs.CV

TL;DR: SAMSelect is an algorithm for visualizing multispectral images, specifically for marine debris in Sentinel-2 imagery, by selecting optimal band combinations for accurate segmentation and photo-interpretation.


<details>
  <summary>Details</summary>
Motivation: Marine debris is hard to visualize due to heterogeneity in medium-resolution imagery, and current methods rely on manual, heuristic-based band selection.

Method: SAMSelect uses the Segment Anything Model to choose the best band or index combination for classification accuracy on annotated data.

Result: Tested on Sentinel-2 scenes, SAMSelect identified new band combinations (e.g., B8, B2) outperforming literature-based indices.

Conclusion: SAMSelect offers a practical, automated solution for marine scientists, with open-source code for broader application.

Abstract: This work proposes SAMSelect, an algorithm to obtain a salient three-channel
visualization for multispectral images. We develop SAMSelect and show its use
for marine scientists visually interpreting floating marine debris in
Sentinel-2 imagery. These debris are notoriously difficult to visualize due to
their compositional heterogeneity in medium-resolution imagery. Out of these
difficulties, a visual interpretation of imagery showing marine debris remains
a common practice by domain experts, who select bands and spectral indices on a
case-by-case basis informed by common practices and heuristics. SAMSelect
selects the band or index combination that achieves the best classification
accuracy on a small annotated dataset through the Segment Anything Model. Its
central assumption is that the three-channel visualization achieves the most
accurate segmentation results also provide good visual information for
photo-interpretation.
  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine
debris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets
from the Plastic Litter Project. This reveals the potential of new previously
unused band combinations (e.g., a normalized difference index of B8, B2), which
demonstrate improved performance compared to literature-based indices. We
describe the algorithm in this paper and provide an open-source code repository
that will be helpful for domain scientists doing visual photo interpretation,
especially in the marine field.

</details>


### [184] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/pdf/2506.08619)
*Gonçalo Dias Pais, Valter Piedade, Moitreya Chatterjee, Marcus Greiff, Pedro Miraldo*

Main category: cs.CV

TL;DR: The paper introduces a targeted sampling strategy and a new surface reconstruction loss for Neural Radiance Fields (NeRFs) to improve rendering accuracy and 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF variants struggle with scalability and uniform sampling, missing detailed regions of interest.

Method: Leverages implicit surface representation and models a probability density function for targeted ray sampling, along with a novel surface reconstruction loss.

Result: Achieves more accurate 3D reconstructions and improved image rendering, especially in regions of interest.

Conclusion: The proposed method enhances NeRF performance by focusing on key areas and optimizing loss functions.

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly
improved the accuracy of synthesized images and surface reconstruction of 3D
scenes/objects. In all of these methods, a key characteristic is that none can
train the neural network with every possible input data, specifically, every
pixel and potential 3D point along the projection rays due to scalability
issues. While vanilla NeRFs uniformly sample both the image pixels and 3D
points along the projection rays, some variants focus only on guiding the
sampling of the 3D points along the projection rays. In this paper, we leverage
the implicit surface representation of the foreground scene and model a
probability density function in a 3D image projection space to achieve a more
targeted sampling of the rays toward regions of interest, resulting in improved
rendering. Additionally, a new surface reconstruction loss is proposed for
improved performance. This new loss fully explores the proposed 3D image
projection space model and incorporates near-to-surface and empty space
components. By integrating our novel sampling strategy and novel loss into
current state-of-the-art neural implicit surface renderers, we achieve more
accurate and detailed 3D reconstructions and improved image rendering,
especially for the regions of interest in any given scene.

</details>


### [185] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/pdf/2506.08629)
*Feixiang Du, Shengkun Wu*

Main category: cs.CV

TL;DR: The paper proposes ECMNet, a lightweight CNN-Mamba hybrid network for semantic segmentation, combining CNN and Mamba to address global context modeling. It introduces EDAB, MSAU, and FFM modules, achieving high accuracy with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing CNN-Transformer models for semantic segmentation lack adequate global context modeling. Mamba's potential in vision tasks inspired the integration of CNN and Mamba to leverage their complementary strengths.

Method: ECMNet combines CNN and Mamba in a capsule-based framework. It includes EDAB for lightweight bottlenecks, MSAU for multi-scale feature aggregation, and FFM for enhanced feature fusion.

Result: The model achieves 70.6% mIoU on Cityscapes and 73.6% mIoU on CamVid, with 0.87M parameters and 8.27G FLOPs.

Conclusion: ECMNet effectively balances accuracy and efficiency, demonstrating the potential of CNN-Mamba hybrids in semantic segmentation.

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [186] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/pdf/2506.08632)
*Yang Bai, Liudi Yang, George Eskandar, Fengyi Shen, Dong Chen, Mohammad Altillawi, Ziyuan Liu, Gitta Kutyniok*

Main category: cs.CV

TL;DR: RoboSwap introduces a novel video editing pipeline combining GANs and diffusion models to swap robotic arms in videos, enabling cross-embodiment learning with unpaired data.


<details>
  <summary>Details</summary>
Motivation: The scarcity of diverse, high-quality datasets hinders video-conditioned robotic learning, especially for cross-platform generalization.

Method: RoboSwap segments robotic arms, uses GANs for translation, blends with backgrounds, and refines with diffusion models for coherence and realism.

Result: Outperforms state-of-the-art models on three benchmarks in structural coherence and motion consistency.

Conclusion: RoboSwap provides a robust solution for generating reliable cross-embodiment data in robotic learning.

Abstract: Recent advancements in generative models have revolutionized video synthesis
and editing. However, the scarcity of diverse, high-quality datasets continues
to hinder video-conditioned robotic learning, limiting cross-platform
generalization. In this work, we address the challenge of swapping a robotic
arm in one video with another: a key step for crossembodiment learning. Unlike
previous methods that depend on paired video demonstrations in the same
environmental settings, our proposed framework, RoboSwap, operates on unpaired
data from diverse environments, alleviating the data collection needs. RoboSwap
introduces a novel video editing pipeline integrating both GANs and diffusion
models, combining their isolated advantages. Specifically, we segment robotic
arms from their backgrounds and train an unpaired GAN model to translate one
robotic arm to another. The translated arm is blended with the original video
background and refined with a diffusion model to enhance coherence, motion
realism and object interaction. The GAN and diffusion stages are trained
independently. Our experiments demonstrate that RoboSwap outperforms
state-of-the-art video and image editing models on three benchmarks in terms of
both structural coherence and motion consistency, thereby offering a robust
solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [187] [SurfR: Surface Reconstruction with Multi-scale Attention](https://arxiv.org/pdf/2506.08635)
*Siddhant Ranade, Gonçalo Dias Pais, Ross Tyler Whitaker, Jacinto C. Nascimento, Pedro Miraldo, Srikumar Ramalingam*

Main category: cs.CV

TL;DR: A fast and accurate surface reconstruction algorithm for unorganized point clouds using an implicit representation, balancing speed and detail.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between detail and speed in existing learning methods for 3D surface reconstruction.

Method: Proposes a new implicit representation with lazy query, parallel multi-scale grid, and cross-scale attention.

Result: Achieves faster reconstruction than baselines with minimal performance loss.

Conclusion: The method offers the best accuracy-speed trade-off for 3D surface reconstruction.

Abstract: We propose a fast and accurate surface reconstruction algorithm for
unorganized point clouds using an implicit representation. Recent learning
methods are either single-object representations with small neural models that
allow for high surface details but require per-object training or generalized
representations that require larger models and generalize to newer shapes but
lack details, and inference is slow. We propose a new implicit representation
for general 3D shapes that is faster than all the baselines at their optimum
resolution, with only a marginal loss in performance compared to the
state-of-the-art. We achieve the best accuracy-speed trade-off using three key
contributions. Many implicit methods extract features from the point cloud to
classify whether a query point is inside or outside the object. First, to speed
up the reconstruction, we show that this feature extraction does not need to
use the query point at an early stage (lazy query). Second, we use a parallel
multi-scale grid representation to develop robust features for different noise
levels and input resolutions. Finally, we show that attention across scales can
provide improved reconstruction results.

</details>


### [188] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/pdf/2506.08640)
*Yichong Lu, Yuzhuo Tian, Zijin Jiang, Yikun Zhao, Yuanbo Yang, Hao Ouyang, Haoji Hu, Huimin Yu, Yujun Shen, Yiyi Liao*

Main category: cs.CV

TL;DR: The paper introduces orientation-aligned 3D object generation from single images, addressing misalignment issues in existing models. It presents Objaverse-OA, a dataset for training, and fine-tunes models to improve alignment and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generative models produce misaligned results due to inconsistent training data, limiting usability in downstream tasks. The goal is to generate 3D objects with consistent orientations.

Method: The authors construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D models. They fine-tune multi-view diffusion and 3D variational autoencoder models using this dataset.

Result: The method outperforms post-hoc alignment approaches and generalizes well to unseen objects. Downstream applications like zero-shot orientation estimation and efficient object rotation are demonstrated.

Conclusion: The proposed approach successfully addresses orientation misalignment in 3D object generation, enabling practical applications and outperforming existing methods.

Abstract: Humans intuitively perceive object shape and orientation from a single image,
guided by strong priors about canonical poses. However, existing 3D generative
models often produce misaligned results due to inconsistent training data,
limiting their usability in downstream tasks. To address this gap, we introduce
the task of orientation-aligned 3D object generation: producing 3D objects from
single images with consistent orientations across categories. To facilitate
this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D
models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two
representative 3D generative models based on multi-view diffusion and 3D
variational autoencoder frameworks to produce aligned objects that generalize
well to unseen objects across various categories. Experimental results
demonstrate the superiority of our method over post-hoc alignment approaches.
Furthermore, we showcase downstream applications enabled by our aligned object
generation, including zero-shot object orientation estimation via
analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [189] [Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization](https://arxiv.org/pdf/2506.08649)
*Zhiyi Zhu, Xiaoyu Wu, Youwei Lu*

Main category: cs.CV

TL;DR: The paper introduces TMCCL to improve motion feature representation for video memorability prediction and MWCVS to apply memorability in video summarization, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing models underutilize motion cues due to limited labeled data, compromising memorability prediction accuracy.

Method: Proposes TMCCL, leveraging text-motion contrastive learning, and MWCVS for video summarization using memorability scores.

Result: TMCCL achieves top performance on memorability datasets; MWCVS reduces subjectivity in summarization labels.

Conclusion: The work advances video memorability prediction and demonstrates its practical application in summarization.

Abstract: Video memorability refers to the ability of videos to be recalled after
viewing, playing a crucial role in creating content that remains memorable.
Existing models typically focus on extracting multimodal features to predict
video memorability scores but often fail to fully utilize motion cues. The
representation of motion features is compromised during the fine-tuning phase
of the motion feature extractor due to a lack of labeled data. In this paper,
we introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal
video memorability prediction model designed to enhance the representation of
motion features. We tackle the challenge of improving motion feature
representation by leveraging text description similarities across videos to
establish positive and negative motion sample sets for a given target. This
enhancement allows the model to learn similar feature representations for
semantically related motion content, resulting in more accurate memorability
predictions. Our model achieves state-of-the-art performance on two video
memorability prediction datasets. Moreover, the potential applications of video
memorability prediction have been underexplored. To address this gap, we
present Memorability Weighted Correction for Video Summarization (MWCVS), using
video memorability prediction to reduce subjectivity in video summarization
labels. Experimental results on two video summarization datasets demonstrate
the effectiveness of MWCVS, showcasing the promising applications of video
memorability prediction.

</details>


### [190] [Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping](https://arxiv.org/pdf/2506.08650)
*Peter Grönquist, Stepan Tulyakov, Dengxin Dai*

Main category: cs.CV

TL;DR: The paper introduces the Neural Physical Model (NPM) for consistent color reproduction across cameras, outperforming existing methods in adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: Ensuring color consistency across cameras is challenging due to sensor and optic variations, and existing methods have limitations like poor adaptability or high computational costs.

Method: The proposed Neural Physical Model (NPM) simulates raw images under specified illumination to estimate transformations between devices, supporting training with or without paired data.

Result: NPM outperforms state-of-the-art methods on datasets like NUS and BeyondRGB, achieving robust chromatic consistency.

Conclusion: NPM is a lightweight, adaptable solution for color consistency, suitable for diverse illumination and sensor conditions.

Abstract: Achieving consistent color reproduction across multiple cameras is essential
for seamless image fusion and Image Processing Pipeline (ISP) compatibility in
modern devices, but it is a challenging task due to variations in sensors and
optics. Existing raw-to-raw conversion methods face limitations such as poor
adaptability to changing illumination, high computational costs, or impractical
requirements such as simultaneous camera operation and overlapping
fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,
physically-informed approach that simulates raw images under specified
illumination to estimate transformations between devices. The NPM effectively
adapts to varying illumination conditions, can be initialized with physical
measurements, and supports training with or without paired data. Experiments on
public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent
state-of-the-art methods, providing robust chromatic consistency across
different sensors and optical systems.

</details>


### [191] [LLaVA-c: Continual Improved Visual Instruction Tuning](https://arxiv.org/pdf/2506.08666)
*Wenzhuo Liu, Fei Zhu, Haiyang Guo, Longhui Wei, Cheng-Lin Liu*

Main category: cs.CV

TL;DR: LLaVA-c improves continual learning for multimodal models by addressing task balancing and base model degradation, matching or surpassing multitask learning performance.


<details>
  <summary>Details</summary>
Motivation: Multitask learning faces challenges like task balancing and expansion costs, while continual learning often neglects general capabilities.

Method: Proposes spectral-aware consolidation for task balance and unsupervised inquiry regularization to prevent base model degradation.

Result: LLaVA-c enhances benchmark performance and preserves general capabilities, matching or surpassing multitask joint learning.

Conclusion: Continual learning can be as effective as multitask learning with the right modifications.

Abstract: Multimodal models like LLaVA-1.5 achieve state-of-the-art visual
understanding through visual instruction tuning on multitask datasets, enabling
strong instruction-following and multimodal performance. However, multitask
learning faces challenges such as task balancing, requiring careful adjustment
of data proportions, and expansion costs, where new tasks risk catastrophic
forgetting and need costly retraining. Continual learning provides a promising
alternative to acquiring new knowledge incrementally while preserving existing
capabilities. However, current methods prioritize task-specific performance,
neglecting base model degradation from overfitting to specific instructions,
which undermines general capabilities. In this work, we propose a simple but
effective method with two modifications on LLaVA-1.5: spectral-aware
consolidation for improved task balance and unsupervised inquiry regularization
to prevent base model degradation. We evaluate both general and task-specific
performance across continual pretraining and fine-tuning. Experiments
demonstrate that LLaVA-c consistently enhances standard benchmark performance
and preserves general capabilities. For the first time, we show that
task-by-task continual learning can achieve results that match or surpass
multitask joint learning. The code will be publicly released.

</details>


### [192] [ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction](https://arxiv.org/pdf/2506.08678)
*Juan Yeo, Soonwoo Cha, Jiwoo Song, Hyunbin Jin, Taesup Kim*

Main category: cs.CV

TL;DR: ATAS improves CLIP's fine-grained vision-language alignment and semantic coherence via self-distillation, outperforming baselines in dense prediction tasks.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with fine-grained, region-level understanding, limiting its effectiveness in dense prediction tasks.

Method: Proposes Any-to-Any Self-Distillation (ATAS), leveraging unlabeled images and internal self-distillation to refine CLIP's vision encoders.

Result: ATAS achieves significant performance gains in open-vocabulary object detection and semantic segmentation.

Conclusion: Jointly maintaining semantic coherence and fine-grained alignment is crucial for advanced open-vocabulary dense prediction.

Abstract: Vision-language models such as CLIP have recently propelled open-vocabulary
dense prediction tasks by enabling recognition of a broad range of visual
concepts. However, CLIP still struggles with fine-grained, region-level
understanding, hindering its effectiveness on these dense prediction tasks. We
identify two pivotal factors required to address this limitation: semantic
coherence and fine-grained vision-language alignment. Current adaptation
methods often improve fine-grained alignment at the expense of semantic
coherence, and often rely on extra modules or supervised fine-tuning. To
overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel
approach that simultaneously enhances semantic coherence and fine-grained
alignment by leveraging own knowledge of a model across all representation
levels. Unlike prior methods, ATAS uses only unlabeled images and an internal
self-distillation process to refine representations of CLIP vision encoders,
preserving local semantic consistency while sharpening local detail
recognition. On open-vocabulary object detection and semantic segmentation
benchmarks, ATAS achieves substantial performance gains, outperforming baseline
CLIP models. These results validate the effectiveness of our approach and
underscore the importance of jointly maintaining semantic coherence and
fine-grained alignment for advanced open-vocabulary dense prediction.

</details>


### [193] [CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities](https://arxiv.org/pdf/2506.08690)
*Hugo Porta, Emanuele Dalsasso, Jessica L. McCarty, Devis Tuia*

Main category: cs.CV

TL;DR: The paper introduces CanadaFireSat, a benchmark dataset for high-resolution wildfire forecasting in Canada, using multi-modal data and deep learning to improve prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The severe 2023 wildfire season in Canada highlights the need for better wildfire mitigation tools, especially high-resolution forecasting models.

Method: Leverages multi-modal data (Sentinel-2, MODIS, ERA5) and deep learning architectures to create 100 m resolution wildfire probability maps.

Result: Multi-modal inputs outperform single-modal, achieving a 60.3% F1 score for the 2023 wildfire season, unseen during training.

Conclusion: Multi-modal deep learning models show promise for high-resolution, continental-scale wildfire forecasting.

Abstract: Canada experienced in 2023 one of the most severe wildfire seasons in recent
history, causing damage across ecosystems, destroying communities, and emitting
large quantities of CO2. This extreme wildfire season is symptomatic of a
climate-change-induced increase in the length and severity of the fire season
that affects the boreal ecosystem. Therefore, it is critical to empower
wildfire management in boreal communities with better mitigation solutions.
Wildfire probability maps represent an important tool for understanding the
likelihood of wildfire occurrence and the potential severity of future
wildfires. The massive increase in the availability of Earth observation data
has enabled the development of deep learning-based wildfire forecasting models,
aiming at providing precise wildfire probability maps at different spatial and
temporal scales. A main limitation of such methods is their reliance on
coarse-resolution environmental drivers and satellite products, leading to
wildfire occurrence prediction of reduced resolution, typically around $\sim
0.1${\deg}. This paper presents a benchmark dataset: CanadaFireSat, and
baseline methods for high-resolution: 100 m wildfire forecasting across Canada,
leveraging multi-modal data from high-resolution multi-spectral satellite
images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and
environmental factors (ERA5 reanalysis data). Our experiments consider two
major deep learning architectures. We observe that using multi-modal temporal
inputs outperforms single-modal temporal inputs across all metrics, achieving a
peak performance of 60.3% in F1 score for the 2023 wildfire season, a season
never seen during model training. This demonstrates the potential of
multi-modal deep learning models for wildfire forecasting at high-resolution
and continental scale.

</details>


### [194] [VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism](https://arxiv.org/pdf/2506.08691)
*Congzhi Zhang, Jiawei Peng, Zhenglin Wang, Yilong Lai, Haowen Sun, Heng Chang, Fei Ma, Weijiang Yu*

Main category: cs.CV

TL;DR: VReST enhances LVLMs' reasoning via Monte Carlo Tree Search and Self-Reward, outperforming current methods in multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with complex visual reasoning despite strong performance in multimodal tasks.

Method: VReST uses Monte Carlo Tree Search and a Self-Reward mechanism to evaluate reasoning steps without extra models.

Result: Achieves state-of-the-art performance in three multimodal mathematical reasoning benchmarks.

Conclusion: VReST demonstrates test-time scaling laws' efficacy, paving the way for future research.

Abstract: Large Vision-Language Models (LVLMs) have shown exceptional performance in
multimodal tasks, but their effectiveness in complex visual reasoning is still
constrained, especially when employing Chain-of-Thought prompting techniques.
In this paper, we propose VReST, a novel training-free approach that enhances
Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.
VReST meticulously traverses the reasoning landscape by establishing a search
tree, where each node encapsulates a reasoning step, and each path delineates a
comprehensive reasoning sequence. Our innovative multimodal Self-Reward
mechanism assesses the quality of reasoning steps by integrating the utility of
sub-questions, answer correctness, and the relevance of vision-language clues,
all without the need for additional models. VReST surpasses current prompting
methods and secures state-of-the-art performance across three multimodal
mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy
of test-time scaling laws in multimodal tasks, offering a promising direction
for future research.

</details>


### [195] [MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning](https://arxiv.org/pdf/2506.08694)
*Mohammadreza Salehi, Shashanka Venkataramanan, Ioana Simion, Efstratios Gavves, Cees G. M. Snoek, Yuki M Asano*

Main category: cs.CV

TL;DR: A motion-guided self-supervised learning framework improves dense video representation learning by clustering point tracks, enhancing temporal consistency and robustness in dynamic scenes.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised methods for videos struggle with motion dynamics, deformations, and occlusions, leading to inconsistent feature learning.

Method: The framework uses an off-the-shelf point tracker for motion trajectories, optimizes feature clustering via momentum-encoder-based optimal transport, and enforces temporal coherence by propagating cluster assignments.

Result: The method improves state-of-the-art performance by 1% to 6% on six datasets and four benchmarks.

Conclusion: Motion-guided learning enhances spatiotemporal consistency and generalization, making it robust for dynamic scenes and occlusions.

Abstract: Dense self-supervised learning has shown great promise for learning pixel-
and patch-level representations, but extending it to videos remains challenging
due to the complexity of motion dynamics. Existing approaches struggle as they
rely on static augmentations that fail under object deformations, occlusions,
and camera movement, leading to inconsistent feature learning over time. We
propose a motion-guided self-supervised learning framework that clusters dense
point tracks to learn spatiotemporally consistent representations. By
leveraging an off-the-shelf point tracker, we extract long-range motion
trajectories and optimize feature clustering through a momentum-encoder-based
optimal transport mechanism. To ensure temporal coherence, we propagate cluster
assignments along tracked points, enforcing feature consistency across views
despite viewpoint changes. Integrating motion as an implicit supervisory
signal, our method learns representations that generalize across frames,
improving robustness in dynamic scenes and challenging occlusion scenarios. By
initializing from strong image-pretrained models and leveraging video data for
training, we improve state-of-the-art by 1% to 6% on six image and video
datasets and four evaluation benchmarks. The implementation is publicly
available at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main

</details>


### [196] [ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds](https://arxiv.org/pdf/2506.08699)
*Frederik Hagelskjaer*

Main category: cs.CV

TL;DR: A fast 5 DoF pose estimation network for colorless point clouds, achieving state-of-the-art performance and real-time inference.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and accurate pose estimation from colorless point clouds, addressing limitations of existing methods.

Method: Uses a neural network to predict object center and top points for pose estimation, trained on synthetic data and tested on benchmarks.

Result: Outperforms all colorless methods, with inference in 250ms, demonstrating state-of-the-art performance.

Conclusion: The network is practical for real-time applications, with code available for further use.

Abstract: This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose
estimation network for colorless point clouds. The pose estimation is
calculated from center and top points of the object, predicted by the neural
network. The network is trained on synthetic data, and tested on a benchmark
dataset, where it demonstrates state-of-the-art performance and outperforms all
colorless methods. The network is able to run inference in only 250
milliseconds making it usable in many scenarios. Project page with code at
arrowpose.github.io

</details>


### [197] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/pdf/2506.08704)
*Xiaohan Zhang, Sitong Wang, Yushen Yan, Yi Yang, Mingda Xu, Qi Liu*

Main category: cs.CV

TL;DR: TraGraph-GS improves novel view synthesis for large-scale scenes by using a trajectory graph for spatial partitioning and progressive rendering, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for large-scale novel view synthesis struggle with arbitrary camera trajectories and texture distortion due to Gaussian overlap.

Method: TraGraph-GS employs a trajectory graph for spatial partitioning, regularization for texture/distant object rendering, and progressive rendering to reduce artifacts.

Result: Achieves 1.86 dB PSNR improvement on aerial datasets and 1.62 dB on ground datasets over state-of-the-art methods.

Conclusion: TraGraph-GS effectively addresses challenges in large-scale scene rendering, offering superior performance and efficiency.

Abstract: High-quality novel view synthesis for large-scale scenes presents a
challenging dilemma in 3D computer vision. Existing methods typically partition
large scenes into multiple regions, reconstruct a 3D representation using
Gaussian splatting for each region, and eventually merge them for novel view
rendering. They can accurately render specific scenes, yet they do not
generalize effectively for two reasons: (1) rigid spatial partition techniques
struggle with arbitrary camera trajectories, and (2) the merging of regions
results in Gaussian overlap to distort texture details. To address these
challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable
high-precision rendering for arbitrarily large-scale scenes. We present a
spatial partitioning method for large-scale scenes based on graphs, which
incorporates a regularization constraint to enhance the rendering of textures
and distant objects, as well as a progressive rendering strategy to mitigate
artifacts caused by Gaussian overlap. Experimental results demonstrate its
superior performance both on four aerial and four ground datasets and highlight
its remarkable efficiency: our method achieves an average improvement of 1.86
dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to
state-of-the-art approaches.

</details>


### [198] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/pdf/2506.08710)
*Mengjiao Ma, Qi Ma, Yue Li, Jiahuan Cheng, Runyi Yang, Bin Ren, Nikola Popovic, Mingqiang Wei, Nicu Sebe, Luc Van Gool, Theo Gevers, Martin R. Oswald, Danda Pani Paudel*

Main category: cs.CV

TL;DR: The paper introduces a large-scale benchmark for evaluating 3D Gaussian Splatting (3DGS) methods in 3D space, highlighting the superiority of generalizable approaches. It also presents GaussianWorld-49K, a diverse 3DGS dataset.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS methods are limited to 2D views and few scenes, hindering holistic 3D understanding. The paper aims to address this gap by providing a comprehensive benchmark and dataset.

Method: Proposes a large-scale benchmark evaluating three groups of 3DGS methods (per-scene optimization-based, optimization-free, and generalizable) across 1060 scenes in indoor and outdoor datasets. Introduces GaussianWorld-49K, a curated 3DGS dataset.

Result: The generalizable approach outperforms others, enabling fast inference on novel scenes and superior segmentation. GaussianWorld-49K demonstrates the potential of leveraging strong data priors.

Conclusion: The benchmark and dataset advance generalizable 3DGS research, with public release to accelerate progress in 3D scene understanding.

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient
encoding of scene geometry, appearance, and semantics. Moreover, grounding
language in 3D scenes has proven to be an effective strategy for 3D scene
understanding. Current Language Gaussian Splatting line of work fall into three
main groups: (i) per-scene optimization-based, (ii) per-scene
optimization-free, and (iii) generalizable approach. However, most of them are
evaluated only on rendered 2D views of a handful of scenes and viewpoints close
to the training views, limiting ability and insight into holistic 3D
understanding. To address this gap, we propose the first large-scale benchmark
that systematically assesses these three groups of methods directly in 3D
space, evaluating on 1060 scenes across three indoor datasets and one outdoor
dataset. Benchmark results demonstrate a clear advantage of the generalizable
paradigm, particularly in relaxing the scene-specific limitation, enabling fast
feed-forward inference on novel scenes, and achieving superior segmentation
performance. We further introduce GaussianWorld-49K a carefully curated 3DGS
dataset comprising around 49K diverse indoor and outdoor scenes obtained from
multiple sources, with which we demonstrate the generalizable approach could
harness strong data priors. Our codes, benchmark, and datasets will be made
public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [199] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/pdf/2506.08729)
*Dieuwertje Alblas, Patryk Rygiel, Julian Suk, Kaj O. Kappe, Marieke Hofman, Christoph Brune, Kak Khee Yeung, Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: The paper proposes an SE(3)-symmetric transformer model to predict AAA growth using 3D vascular surface features, improving personalized monitoring strategies.


<details>
  <summary>Details</summary>
Motivation: Current AAA monitoring relies on diameter thresholds, ignoring 3D shape complexities, leading to potentially unfit surveillance intervals. Personalized growth predictions could enhance monitoring.

Method: An SE(3)-symmetric transformer model predicts AAA growth directly on the vascular surface, enriched with local multi-physical features, trained on 113 CTA scans from 24 patients.

Result: The model predicts AAA growth with a median diameter error of 1.18 mm and identifies elective repair eligibility within two years (accuracy = 0.93). It generalizes well on an external validation set.

Conclusion: Local directional AAA growth prediction from the vascular surface is feasible and may improve personalized surveillance strategies.

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [200] [InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba](https://arxiv.org/pdf/2506.08735)
*Yuhang Wang, Jun Li, Zhijian Wu, Jianhua Xu*

Main category: cs.CV

TL;DR: InceptionMamba improves spatial and global context modeling over InceptionNeXt by replacing strip convolutions with orthogonal band convolutions and adding a bottleneck Mamba module.


<details>
  <summary>Details</summary>
Motivation: InceptionNeXt's limitations in capturing spatial dependencies and global context modeling inspired the development of InceptionMamba.

Method: Replaces one-dimensional strip convolutions with orthogonal band convolutions and introduces a bottleneck Mamba module for global context modeling.

Result: Achieves state-of-the-art performance in classification and downstream tasks with superior efficiency.

Conclusion: InceptionMamba addresses InceptionNeXt's limitations, offering better spatial and global modeling with high efficiency.

Abstract: Within the family of convolutional neural networks, InceptionNeXt has shown
excellent competitiveness in image classification and a number of downstream
tasks. Built on parallel one-dimensional strip convolutions, however, it
suffers from limited ability of capturing spatial dependencies along different
dimensions and fails to fully explore spatial modeling in local neighborhood.
Besides, inherent locality constraints of convolution operations are
detrimental to effective global context modeling. To overcome these
limitations, we propose a novel backbone architecture termed InceptionMamba in
this study. More specifically, the traditional one-dimensional strip
convolutions are replaced by orthogonal band convolutions in our InceptionMamba
to achieve cohesive spatial modeling. Furthermore, global contextual modeling
can be achieved via a bottleneck Mamba module, facilitating enhanced
cross-channel information fusion and enlarged receptive field. Extensive
evaluations on classification and various downstream tasks demonstrate that the
proposed InceptionMamba achieves state-of-the-art performance with superior
parameter and computational efficiency. The source code will be available at
https://github.com/Wake1021/InceptionMamba.

</details>


### [201] [RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation](https://arxiv.org/pdf/2506.08772)
*Jiayi Song, Kaiyu Li, Xiangyong Cao, Deyu Meng*

Main category: cs.CV

TL;DR: The paper proposes RS-MTDF, a semi-supervised semantic segmentation framework for remote sensing, leveraging Vision Foundation Models (VFMs) to address distribution mismatch and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation in remote sensing relies on costly annotations. Semi-supervised methods struggle with distribution gaps between labeled and unlabeled data. VFMs offer robust generalization to bridge this gap.

Method: RS-MTDF uses multiple frozen VFMs (e.g., DINOv2, CLIP) as teachers for feature-level distillation, fusing their knowledge into the student decoder.

Result: Experiments on ISPRS Potsdam, LoveDA, and DeepGlobe show RS-MTDF achieves state-of-the-art performance, outperforming others in various label ratios and semantic categories.

Conclusion: Multi-teacher VFM guidance enhances generalization and semantic understanding in remote sensing segmentation, validated by ablation studies.

Abstract: Semantic segmentation in remote sensing images is crucial for various
applications, yet its performance is heavily reliant on large-scale,
high-quality pixel-wise annotations, which are notoriously expensive and
time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a
promising alternative to mitigate this data dependency. However, existing SSS
methods often struggle with the inherent distribution mismatch between limited
labeled data and abundant unlabeled data, leading to suboptimal generalization.
We propose that Vision Foundation Models (VFMs), pre-trained on vast and
diverse datasets, possess robust generalization capabilities that can
effectively bridge this distribution gap and provide strong semantic priors for
SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and
Fusion), a novel framework that leverages the powerful semantic knowledge
embedded in VFMs to guide semi-supervised learning in remote sensing.
Specifically, RS-MTDF employs multiple frozen VFMs (\textit{e.g.}, DINOv2 and
CLIP) as expert teachers, utilizing feature-level distillation to align student
features with their robust representations. To further enhance discriminative
power, the distilled knowledge is seamlessly fused into the student decoder.
Extensive experiments on three challenging remote sensing datasets (ISPRS
Potsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves
state-of-the-art performance. Notably, our method outperforms existing
approaches across various label ratios on LoveDA and secures the highest IoU in
the majority of semantic categories. These results underscore the efficacy of
multi-teacher VFM guidance in significantly enhancing both generalization and
semantic understanding for remote sensing segmentation. Ablation studies
further validate the contribution of each proposed module.

</details>


### [202] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/pdf/2506.08777)
*Keyi Liu, Weidong Yang, Ben Fei, Ying He*

Main category: cs.CV

TL;DR: Gaussian2Scene is a novel SSL framework for point cloud pre-training using 3D Gaussian Splatting, improving geometric understanding and computational efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods for point cloud pre-training rely on implicit scene representations and high memory demands, often failing to capture 3D geometric structures.

Method: Gaussian2Scene uses 3D Gaussian Splatting for efficient pre-training, with a two-stage strategy: a dual-branch masked autoencoder for 2D/3D representations, followed by supervised learning with reconstructed point clouds and Gaussian primitives.

Result: The framework shows consistent improvements in downstream 3D object detection tasks compared to existing methods.

Conclusion: Gaussian2Scene effectively addresses limitations of current SSL approaches by leveraging explicit 3D representations and enhancing geometric learning.

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a
cornerstone for many 3D vision tasks, enabling effective learning from
large-scale unannotated data. At the scene level, existing SSL methods often
incorporate volume rendering into the pre-training framework, using RGB-D
images as reconstruction signals to facilitate cross-modal learning. This
strategy promotes alignment between 2D and 3D modalities and enables the model
to benefit from rich visual cues in the RGB-D inputs. However, these approaches
are limited by their reliance on implicit scene representations and high memory
demands. Furthermore, since their reconstruction objectives are applied only in
2D space, they often fail to capture underlying 3D geometric structures. To
address these challenges, we propose Gaussian2Scene, a novel scene-level SSL
framework that leverages the efficiency and explicit nature of 3D Gaussian
Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the
computational burden associated with volume rendering but also supports direct
3D scene reconstruction, thereby enhancing the geometric understanding of the
backbone network. Our approach follows a progressive two-stage training
strategy. In the first stage, a dual-branch masked autoencoder learns both 2D
and 3D scene representations. In the second stage, we initialize training with
reconstructed point clouds and further supervise learning using the geometric
locations of Gaussian primitives and rendered RGB images. This process
reinforces both geometric and cross-modal learning. We demonstrate the
effectiveness of Gaussian2Scene across several downstream 3D object detection
tasks, showing consistent improvements over existing pre-training methods.

</details>


### [203] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/pdf/2506.08780)
*Isaac Corley, Lakshay Sharma, Ruth Crasto*

Main category: cs.CV

TL;DR: Landsat-Bench introduces three benchmarks for Landsat imagery, showing that SSL4EO-L pretrained models outperform ImageNet in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The lack of benchmarks for Landsat data hinders progress in Geospatial Foundation Models (GFMs).

Method: Landsat-Bench adapts three remote sensing datasets (EuroSAT-L, BigEarthNet-L, LC100-L) and evaluates pretrained models.

Result: SSL4EO-L pretrained GFMs outperform ImageNet, with gains of +4% OA and +5.1% mAP on EuroSAT-L and BigEarthNet-L.

Conclusion: Landsat-Bench provides standardized benchmarks, demonstrating the superiority of SSL4EO-L pretrained models for Landsat-based tasks.

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [204] [HomographyAD: Deep Anomaly Detection Using Self Homography Learning](https://arxiv.org/pdf/2506.08784)
*Jongyub Seok, Chanjin Kang*

Main category: cs.CV

TL;DR: The paper proposes HomographyAD, a deep anomaly detection method for industrial datasets, addressing alignment issues in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods perform well only on fully-aligned datasets, unlike real-world industrial environments.

Method: HomographyAD uses ImageNet-pretrained networks, aligns input foregrounds via deep homography estimation, and fine-tunes with self homography learning.

Result: The method enhances performance when applied to various anomaly detection approaches, validated through experiments.

Conclusion: HomographyAD effectively addresses alignment limitations in industrial anomaly detection, improving performance.

Abstract: Anomaly detection (AD) is a task that distinguishes normal and abnormal data,
which is important for applying automation technologies of the manufacturing
facilities. For MVTec dataset that is a representative AD dataset for
industrial environment, many recent works have shown remarkable performances.
However, the existing anomaly detection works have a limitation of showing good
performance for fully-aligned datasets only, unlike real-world industrial
environments. To solve this limitation, we propose HomographyAD, a novel deep
anomaly detection methodology based on the ImageNet-pretrained network, which
is specially designed for actual industrial dataset. Specifically, we first
suggest input foreground alignment using the deep homography estimation method.
In addition, we fine-tune the model by self homography learning to learn
additional shape information from normal samples. Finally, we conduct anomaly
detection based on the measure of how far the feature of test sample is from
the distribution of the extracted normal features. By applying our proposed
method to various existing AD approaches, we show performance enhancement
through extensive experiments.

</details>


### [205] [Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling](https://arxiv.org/pdf/2506.08796)
*Zhiyuan Ma, Ruixun Liu, Sixian Liu, Jianjun Li, Bowen Zhou*

Main category: cs.CV

TL;DR: Discretized-RF improves rectified flow models by introducing momentum fields for better diversity and multi-scale noise modeling.


<details>
  <summary>Details</summary>
Motivation: Address diversity and multi-scale noise modeling limitations in straight-line rectified flow models.

Method: Discretizes straight paths into variable velocity sub-paths, introducing noise on velocity to enhance diversity.

Result: Produces diverse, efficient trajectories and high-quality, diverse results.

Conclusion: Discretized-RF advances rectified flow models with improved performance and flexibility.

Abstract: Recently, the rectified flow (RF) has emerged as the new state-of-the-art
among flow-based diffusion models due to its high efficiency advantage in
straight path sampling, especially with the amazing images generated by a
series of RF models such as Flux 1.0 and SD 3.0. Although a straight-line
connection between the noisy and natural data distributions is intuitive, fast,
and easy to optimize, it still inevitably leads to: 1) Diversity concerns,
which arise since straight-line paths only cover a fairly restricted sampling
space. 2) Multi-scale noise modeling concerns, since the straight line flow
only needs to optimize the constant velocity field $\bm v$ between the two
distributions $\bm\pi_0$ and $\bm\pi_1$. In this work, we present
Discretized-RF, a new family of rectified flow (also called momentum flow
models since they refer to the previous velocity component and the random
velocity component in each diffusion step), which discretizes the straight path
into a series of variable velocity field sub-paths (namely ``momentum fields'')
to expand the search space, especially when close to the distribution
$p_\text{noise}$. Different from the previous case where noise is directly
superimposed on $\bm x$, we introduce noise on the velocity $\bm v$ of the
sub-path to change its direction in order to improve the diversity and
multi-scale noise modeling abilities. Experimental results on several
representative datasets demonstrate that learning momentum flow matching by
sampling random velocity fields will produce trajectories that are both diverse
and efficient, and can consistently generate high-quality and diverse results.
Code is available at https://github.com/liuruixun/momentum-fm.

</details>


### [206] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/pdf/2506.08797)
*Ziyao Huang, Zixiang Zhou, Juan Cao, Yifeng Ma, Yi Chen, Zejing Rao, Zhiyong Xu, Hongmei Wang, Qin Lin, Yuan Zhou, Qinglin Lu, Fan Tang*

Main category: cs.CV

TL;DR: HunyuanVideo-HOMA is a weakly conditioned multimodal-driven framework for HOI video generation, improving controllability and generalization with sparse motion guidance and dual input encoding.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in HOI video generation, such as reliance on curated data, limited generalization, and accessibility issues.

Method: Uses a multimodal diffusion transformer (MMDiT) with dual input space for appearance and motion, plus HOI and facial adapters for efficient training and lip sync.

Result: Achieves state-of-the-art performance in interaction naturalness and generalization under weak supervision.

Conclusion: Demonstrates versatility in text-conditioned generation and interactive manipulation, supported by a user-friendly interface.

Abstract: To address key limitations in human-object interaction (HOI) video generation
-- specifically the reliance on curated motion data, limited generalization to
novel objects/scenarios, and restricted accessibility -- we introduce
HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.
HunyuanVideo-HOMA enhances controllability and reduces dependency on precise
inputs through sparse, decoupled motion guidance. It encodes appearance and
motion signals into the dual input space of a multimodal diffusion transformer
(MMDiT), fusing them within a shared context space to synthesize temporally
consistent and physically plausible interactions. To optimize training, we
integrate a parameter-space HOI adapter initialized from pretrained MMDiT
weights, preserving prior knowledge while enabling efficient adaptation, and a
facial cross-attention adapter for anatomically accurate audio-driven lip
synchronization. Extensive experiments confirm state-of-the-art performance in
interaction naturalness and generalization under weak supervision. Finally,
HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and
interactive object manipulation, supported by a user-friendly demo interface.
The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [207] [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/pdf/2506.08817)
*Shuyi Zhang, Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Pengwei Wang, Zhongyuan Wang, Hongxuan Ma, Shanghang Zhang*

Main category: cs.CV

TL;DR: Video-CoT is a new dataset and benchmark for improving spatiotemporal understanding in video analysis using Chain-of-Thought methodologies, revealing challenges for current vision-language models.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models struggle with nuanced spatiotemporal details in video analysis, necessitating a dedicated dataset and benchmark.

Method: Introduces Video-CoT, a dataset with 192,000 question-answer pairs and 23,000 CoT-annotated samples, plus a benchmark for evaluation.

Result: Experiments show current VLMs perform poorly on spatiotemporal tasks, highlighting the difficulty of such understanding.

Conclusion: Video-CoT advances research in video comprehension and supports future intelligent systems, with resources made public for further exploration.

Abstract: Video content comprehension is essential for various applications, ranging
from video analysis to interactive systems. Despite advancements in large-scale
vision-language models (VLMs), these models often struggle to capture the
nuanced, spatiotemporal details essential for thorough video analysis. To
address this gap, we introduce Video-CoT, a groundbreaking dataset designed to
enhance spatiotemporal understanding using Chain-of-Thought (CoT)
methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal
question-answer pairs and 23,000 high-quality CoT-annotated samples, providing
a solid foundation for evaluating spatiotemporal understanding in video
comprehension. Additionally, we provide a comprehensive benchmark for assessing
these tasks, with each task featuring 750 images and tailored evaluation
metrics. Our extensive experiments reveal that current VLMs face significant
challenges in achieving satisfactory performance, high-lighting the
difficulties of effective spatiotemporal understanding. Overall, the Video-CoT
dataset and benchmark open new avenues for research in multimedia understanding
and support future innovations in intelligent systems requiring advanced video
analysis capabilities. By making these resources publicly available, we aim to
encourage further exploration in this critical area. Project
website:https://video-cot.github.io/ .

</details>


### [208] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/pdf/2506.08835)
*Shravan Nayak, Mehar Bhatia, Xiaofeng Zhang, Verena Rieser, Lisa Anne Hendricks, Sjoerd van Steenkiste, Yash Goyal, Karolina Stańczak, Aishwarya Agrawal*

Main category: cs.CV

TL;DR: The study quantifies cultural misalignment in text-to-image (T2I) models, revealing significant failures in meeting explicit (68%) and implicit (49%) cultural expectations. It introduces CulturalFrames, a benchmark for human evaluation, and highlights poor correlation of existing metrics with human judgments.


<details>
  <summary>Details</summary>
Motivation: Concerns about T2I models' ability to represent diverse cultural contexts accurately drive the need for systematic evaluation.

Method: CulturalFrames, a benchmark spanning 10 countries and 5 socio-cultural domains, is used to evaluate 4 T2I models with 983 prompts, 3637 images, and 10k human annotations.

Result: T2I models miss cultural expectations 44% of the time, with explicit failures at 68% and implicit at 49%. Existing metrics poorly correlate with human judgments.

Conclusion: The study identifies gaps in cultural representation, urging development of more culturally informed T2I models and evaluation methods.

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [209] [Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis](https://arxiv.org/pdf/2506.08849)
*Jingguo Qu, Xinyang Han, Tonghuan Xiao, Jia Ai, Juan Wu, Tong Zhao, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Yingınst*

Main category: cs.CV

TL;DR: The paper proposes domain adaptation methods for vision-language foundation models to enhance ultrasound image analysis, achieving superior performance in segmentation and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Manual contouring in medical ultrasonography is labor-intensive and inconsistent. Vision-language models show promise but struggle with domain differences between natural and medical images.

Method: Fine-tuning pipeline for vision-language models using a large language model as text refiner, with adaptation strategies and task-driven heads.

Result: Outperforms state-of-the-art models on six ultrasound datasets for segmentation and classification.

Conclusion: The approach effectively bridges domain gaps, improving ultrasound image analysis performance.

Abstract: Medical ultrasonography is an essential imaging technique for examining
superficial organs and tissues, including lymph nodes, breast, and thyroid. It
employs high-frequency ultrasound waves to generate detailed images of the
internal structures of the human body. However, manually contouring regions of
interest in these images is a labor-intensive task that demands expertise and
often results in inconsistent interpretations among individuals.
Vision-language foundation models, which have excelled in various computer
vision applications, present new opportunities for enhancing ultrasound image
analysis. Yet, their performance is hindered by the significant differences
between natural and medical imaging domains. This research seeks to overcome
these challenges by developing domain adaptation methods for vision-language
foundation models. In this study, we explore the fine-tuning pipeline for
vision-language foundation models by utilizing large language model as text
refiner with special-designed adaptation strategies and task-driven heads. Our
approach has been extensively evaluated on six ultrasound datasets and two
tasks: segmentation and classification. The experimental results show that our
method can effectively improve the performance of vision-language foundation
models for ultrasound image analysis, and outperform the existing
state-of-the-art vision-language and pure foundation models. The source code of
this study is available at
\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}.

</details>


### [210] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/pdf/2506.08854)
*Junzhuo Liu, Markus Eckstein, Zhixiang Wang, Friedrich Feuerhake, Dorit Merhof*

Main category: cs.CV

TL;DR: A contrastive learning-based deep learning method predicts spatially resolved gene expression from whole-slide images, improving prediction accuracy for key gene types and preserving gene-gene correlations.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics data is costly and scarce, yet crucial for tumor analysis and clinical diagnosis, necessitating a method to predict gene expression from more accessible whole-slide images.

Method: A contrastive learning-based deep learning approach is developed to predict gene expression from whole-slide images, evaluated across six disease datasets.

Result: The method improves prediction accuracy (PCC) for highly expressed, highly variable, and marker genes by 6.27%, 6.11%, and 11.26% respectively, and preserves gene-gene correlations.

Conclusion: The method is effective for predicting gene expression from images, even with limited samples, and shows promise for cancer tissue localization.

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [211] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/pdf/2506.08862)
*Zike Wu, Qi Yan, Xuanyu Yi, Lele Wang, Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat is a feed-forward framework for real-time 3D reconstruction from uncalibrated videos, addressing challenges like dynamic scene modeling and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle uncalibrated inputs, dynamic scenes, and long-term stability efficiently.

Method: Uses probabilistic sampling in a static encoder and bidirectional deformation fields in a dynamic decoder.

Result: Outperforms prior works in reconstruction quality and dynamic modeling, supporting online reconstruction.

Conclusion: StreamSplat offers a robust solution for real-time dynamic 3D scene reconstruction.

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [212] [DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval](https://arxiv.org/pdf/2506.08887)
*Leqi Shen, Guoqiang Gong, Tianxiang Hao, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Jungong Han, Guiguang Ding*

Main category: cs.CV

TL;DR: DiscoVLA addresses vision, language, and alignment discrepancies in adapting CLIP for video-text retrieval, outperforming prior methods by 1.5% in R@1 on MSRVTT.


<details>
  <summary>Details</summary>
Motivation: Existing methods for video-text retrieval focus on vision discrepancies but neglect language and alignment, limiting performance.

Method: DiscoVLA integrates image-video features, generates pseudo captions for fine-grained alignment, and uses image-to-video alignment distillation.

Result: DiscoVLA achieves 50.5% R@1 on MSRVTT, surpassing previous methods by 1.5%.

Conclusion: DiscoVLA effectively mitigates all three discrepancies, demonstrating superior performance in video-text retrieval.

Abstract: The parameter-efficient adaptation of the image-text pretraining model CLIP
for video-text retrieval is a prominent area of research. While CLIP is focused
on image-level vision-language matching, video-text retrieval demands
comprehensive understanding at the video level. Three key discrepancies emerge
in the transfer from image-level to video-level: vision, language, and
alignment. However, existing methods mainly focus on vision while neglecting
language and alignment. In this paper, we propose Discrepancy Reduction in
Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all
three discrepancies. Specifically, we introduce Image-Video Features Fusion to
integrate image-level and video-level features, effectively tackling both
vision and language discrepancies. Additionally, we generate pseudo image
captions to learn fine-grained image-level alignment. To mitigate alignment
discrepancies, we propose Image-to-Video Alignment Distillation, which
leverages image-level alignment knowledge to enhance video-level alignment.
Extensive experiments demonstrate the superiority of our DiscoVLA. In
particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous
methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is
available at https://github.com/LunarShen/DsicoVLA.

</details>


### [213] [Product of Experts for Visual Generation](https://arxiv.org/pdf/2506.08894)
*Yunzhi Zhang, Carson Murtuza-Lanier, Zizhang Li, Yilun Du, Jiajun Wu*

Main category: cs.CV

TL;DR: A Product of Experts (PoE) framework integrates diverse knowledge from heterogeneous models (e.g., generative, language, human-crafted) for improved image and video synthesis, using Annealed Importance Sampling (AIS) for inference-time composition.


<details>
  <summary>Details</summary>
Motivation: To explore the underutilized potential of combining diverse knowledge sources (generative models, language models, human-crafted tools) for enhanced visual synthesis tasks.

Method: Proposes a training-free PoE framework using AIS to sample from the product distribution of heterogeneous experts.

Result: Demonstrates better controllability and flexible user interfaces for visual generation tasks compared to monolithic methods.

Conclusion: The PoE framework effectively composes knowledge from diverse sources, improving visual synthesis and user control.

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [214] [WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos](https://arxiv.org/pdf/2506.08896)
*Negin Ghamsarian, Raphael Sznitman, Klaus Schoeffmann, Jens Kowal*

Main category: cs.CV

TL;DR: WetCat is the first dataset of wetlab cataract surgery videos for automated skill assessment, addressing gaps in traditional manual evaluations.


<details>
  <summary>Details</summary>
Motivation: Traditional wetlab training relies on manual evaluations, which are inefficient and subjective. Automated skill assessment using computer vision can improve surgical education.

Method: Introduces WetCat, a dataset with high-resolution wetlab cataract surgery videos, phase annotations, and semantic segmentations for key phases (capsulorhexis and phacoemulsification).

Result: WetCat enables AI-driven, interpretable skill assessment aligned with clinical standards, advancing objective surgical education.

Conclusion: WetCat sets a benchmark for automated workflow analysis and skill assessment in ophthalmology training, with publicly available data.

Abstract: To meet the growing demand for systematic surgical training, wetlab
environments have become indispensable platforms for hands-on practice in
ophthalmology. Yet, traditional wetlab training depends heavily on manual
performance evaluations, which are labor-intensive, time-consuming, and often
subject to variability. Recent advances in computer vision offer promising
avenues for automated skill assessment, enhancing both the efficiency and
objectivity of surgical education. Despite notable progress in ophthalmic
surgical datasets, existing resources predominantly focus on real surgeries or
isolated tasks, falling short of supporting comprehensive skill evaluation in
controlled wetlab settings. To address these limitations, we introduce WetCat,
the first dataset of wetlab cataract surgery videos specifically curated for
automated skill assessment. WetCat comprises high-resolution recordings of
surgeries performed by trainees on artificial eyes, featuring comprehensive
phase annotations and semantic segmentations of key anatomical structures.
These annotations are meticulously designed to facilitate skill assessment
during the critical capsulorhexis and phacoemulsification phases, adhering to
standardized surgical skill assessment frameworks. By focusing on these
essential phases, WetCat enables the development of interpretable, AI-driven
evaluation tools aligned with established clinical metrics. This dataset lays a
strong foundation for advancing objective, scalable surgical education and sets
a new benchmark for automated workflow analysis and skill assessment in
ophthalmology training. The dataset and annotations are publicly available in
Synapse https://www.synapse.org/Synapse:syn66401174/files.

</details>


### [215] [MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis](https://arxiv.org/pdf/2506.08900)
*José Morano, Botond Fazekas, Emese Sükei, Ronald Fecso, Taha Emre, Markus Gumpinger, Georg Faustmann, Marzieh Oghbaie, Ursula Schmidt-Erfurth, Hrvoje Bogunović*

Main category: cs.CV

TL;DR: MIRAGE is a multimodal foundation model for analyzing OCT and SLO images, outperforming existing models in classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Current AI models for ophthalmic image analysis require extensive annotation and underperform on unseen data. Existing foundation models lack validation and focus on single modalities.

Method: Proposes MIRAGE, a multimodal foundation model for OCT and SLO images, and introduces a new evaluation benchmark.

Result: MIRAGE outperforms general and specialized models in classification and segmentation tasks.

Conclusion: MIRAGE is a robust foundation for AI systems in retinal OCT analysis, with publicly available resources.

Abstract: Artificial intelligence (AI) has become a fundamental tool for assisting
clinicians in analyzing ophthalmic images, such as optical coherence tomography
(OCT). However, developing AI models often requires extensive annotation, and
existing models tend to underperform on independent, unseen data. Foundation
models (FMs), large AI models trained on vast unlabeled datasets, have shown
promise in overcoming these challenges. Nonetheless, available FMs for
ophthalmology lack extensive validation, especially for segmentation tasks, and
focus on a single imaging modality. In this context, we propose MIRAGE, a novel
multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)
images. Additionally, we propose a new evaluation benchmark with OCT/SLO
classification and segmentation tasks. The comparison with general and
specialized FMs and segmentation methods shows the superiority of MIRAGE in
both types of tasks, highlighting its suitability as a basis for the
development of robust AI systems for retinal OCT image analysis. Both MIRAGE
and the evaluation benchmark are publicly available:
https://github.com/j-morano/MIRAGE.

</details>


### [216] [Hyperbolic Dual Feature Augmentation for Open-Environment](https://arxiv.org/pdf/2506.08906)
*Peilin Yu, Yuwei Wu, Zhi Gao, Xiaomeng Fan, Shuo Yang, Yunde Jia*

Main category: cs.CV

TL;DR: A hyperbolic dual feature augmentation method for open-environments, enhancing performance for both seen and unseen classes using neural ODEs and meta-learning.


<details>
  <summary>Details</summary>
Motivation: Existing hyperbolic feature augmentation is limited to closed environments with fixed classes, lacking adaptability for open-environment tasks.

Method: Uses neural ODEs with meta-learning to estimate feature distributions, introduces a regularizer for hierarchical structure preservation, and derives a loss upper bound for infinite augmentations.

Result: Demonstrates improved performance in five open-environment tasks, including class-incremental learning and zero-shot learning.

Conclusion: The method effectively enhances hyperbolic algorithms in open-environments by addressing limitations of existing approaches.

Abstract: Feature augmentation generates novel samples in the feature space, providing
an effective way to enhance the generalization ability of learning algorithms
with hyperbolic geometry. Most hyperbolic feature augmentation is confined to
closed-environment, assuming the number of classes is fixed (\emph{i.e.}, seen
classes) and generating features only for these classes. In this paper, we
propose a hyperbolic dual feature augmentation method for open-environment,
which augments features for both seen and unseen classes in the hyperbolic
space. To obtain a more precise approximation of the real data distribution for
efficient training, (1) we adopt a neural ordinary differential equation
module, enhanced by meta-learning, estimating the feature distributions of both
seen and unseen classes; (2) we then introduce a regularizer to preserve the
latent hierarchical structures of data in the hyperbolic space; (3) we also
derive an upper bound for the hyperbolic dual augmentation loss, allowing us to
train a hyperbolic model using infinite augmentations for seen and unseen
classes. Extensive experiments on five open-environment tasks:
class-incremental learning, few-shot open-set recognition, few-shot learning,
zero-shot learning, and general image classification, demonstrate that our
method effectively enhances the performance of hyperbolic algorithms in
open-environment.

</details>


### [217] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/pdf/2506.08908)
*Jiajun Li, Yue Ma, Xinyu Zhang, Qingyan Wei, Songhua Liu, Linfeng Zhang*

Main category: cs.CV

TL;DR: SkipVAR introduces a sample-adaptive framework to accelerate VAR models by addressing step and unconditional branch redundancies, achieving significant speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: High-frequency components in VAR models cause latency, but computational redundancy in these steps is understudied.

Method: Proposes step-skipping and unconditional branch replacement, combined into SkipVAR, a frequency-aware adaptive framework.

Result: Achieves 1.81x overall acceleration and 2.62x speedup on GenEval, with 0.88 average SSIM.

Conclusion: SkipVAR effectively accelerates VAR models adaptively, maintaining quality, and is publicly available.

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that
high-frequency components, or later steps, in the generation process contribute
disproportionately to inference latency. However, the underlying computational
redundancy involved in these steps has yet to be thoroughly investigated. In
this paper, we conduct an in-depth analysis of the VAR inference process and
identify two primary sources of inefficiency: step redundancy and unconditional
branch redundancy. To address step redundancy, we propose an automatic
step-skipping strategy that selectively omits unnecessary generation steps to
improve efficiency. For unconditional branch redundancy, we observe that the
information gap between the conditional and unconditional branches is minimal.
Leveraging this insight, we introduce unconditional branch replacement, a
technique that bypasses the unconditional branch to reduce computational cost.
Notably, we observe that the effectiveness of acceleration strategies varies
significantly across different samples. Motivated by this, we propose SkipVAR,
a sample-adaptive framework that leverages frequency information to dynamically
select the most suitable acceleration strategy for each instance. To evaluate
the role of high-frequency information, we introduce high-variation benchmark
datasets that test model sensitivity to fine details. Extensive experiments
show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall
acceleration and 2.62x speedup on the GenEval benchmark, maintaining model
quality. These results confirm the effectiveness of frequency-aware,
training-free adaptive acceleration for scalable autoregressive image
generation. Our code is available at https://github.com/fakerone-li/SkipVAR and
has been publicly released.

</details>


### [218] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/pdf/2506.08915)
*Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos*

Main category: cs.CV

TL;DR: The paper proposes a two-stage attention-based method using binary masks to focus on relevant image regions, improving robustness against spurious correlations and out-of-distribution backgrounds.


<details>
  <summary>Details</summary>
Motivation: Context can bias object perception, especially in out-of-distribution backgrounds, while many tasks require identifying relevant regions. The paper aims to balance context use and focus.

Method: A two-stage framework: stage 1 processes the full image to identify task-relevant regions, and stage 2 uses attention masks to focus on these regions, filtering out spurious information. Both stages are trained jointly.

Result: The method significantly improves robustness against spurious correlations and out-of-distribution backgrounds across diverse benchmarks.

Conclusion: The proposed approach effectively balances context use and focused analysis, enhancing object perception in challenging scenarios.

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [219] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/pdf/2506.08927)
*David Acuna, Ximing Lu, Jaehun Jung, Hyunwoo Kim, Amlan Kar, Sanja Fidler, Yejin Choi*

Main category: cs.CV

TL;DR: The paper explores using Monte Carlo Tree Search (MCTS) to induce long reasoning traces in non-reasoning vision-language models without additional training.


<details>
  <summary>Details</summary>
Motivation: To address whether existing non-reasoning models can be leveraged for long-form reasoning without retraining or supervision.

Method: An MCTS-inspired algorithm injects subquestion-subanswer pairs into the model's output to frame reasoning as a search process.

Result: Consistent improvements across benchmarks, including a 2% overall gain on MMMU-PRO and a 9% boost in Liberal Arts.

Conclusion: Search-based reasoning can elicit hidden knowledge and extend reasoning in non-reasoning models without training.

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [220] [What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities](https://arxiv.org/pdf/2506.08933)
*Wendong Bu, Yang Wu, Qifan Yu, Minghe Gao, Bingchen Miao, Zhenkui Zhang, Kaihang Pan, Yunfei Li, Mengze Li, Wei Ji, Juncheng Li, Siliang Tang, Yueting Zhuang*

Main category: cs.CV

TL;DR: OmniBench is a self-generating, cross-platform benchmark for MLLM-based virtual agents, addressing limitations in existing benchmarks with controllable task complexity and automated evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLM-based virtual agents lack controllability, scalability, and multidimensional evaluation, limiting their effectiveness.

Method: Introduces OmniBench, a graph-based benchmark with automated task synthesis, and OmniEval, a multidimensional evaluation framework covering 10 capabilities.

Result: A dataset of 36k graph-structured tasks achieves 91% human acceptance, and training on this data improves agent efficiency.

Conclusion: OmniBench and OmniEval provide a scalable, automated solution for evaluating MLLM-based agents, revealing performance gaps and guiding future advancements.

Abstract: As multimodal large language models (MLLMs) advance, MLLM-based virtual
agents have demonstrated remarkable performance. However, existing benchmarks
face significant limitations, including uncontrollable task complexity,
extensive manual annotation with limited scenarios, and a lack of
multidimensional evaluation. In response to these challenges, we introduce
OmniBench, a self-generating, cross-platform, graph-based benchmark with an
automated pipeline for synthesizing tasks of controllable complexity through
subtask composition. To evaluate the diverse capabilities of virtual agents on
the graph, we further present OmniEval, a multidimensional evaluation framework
that includes subtask-level evaluation, graph-based metrics, and comprehensive
tests across 10 capabilities. Our synthesized dataset contains 36k
graph-structured tasks across 20 scenarios, achieving a 91\% human acceptance
rate. Training on our graph-structured data shows that it can more efficiently
guide agents compared to manually annotated data. We conduct multidimensional
evaluations for various open-source and closed-source models, revealing their
performance across various capabilities and paving the way for future
advancements. Our project is available at https://omni-bench.github.io/.

</details>


### [221] [SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation](https://arxiv.org/pdf/2506.08949)
*Hongjie Zhu, Xiwei Liu, Rundong Xue, Zeyu Zhang, Yong Xu, Daji Ergu, Ying Cai, Yang Zhao*

Main category: cs.CV

TL;DR: SSS (Semi-Supervised SAM-2) leverages SAM-2's feature extraction to enhance semi-supervised medical image segmentation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of leveraging unlabeled medical data efficiently while reducing reliance on costly pixel-level annotations.

Method: Combines SAM-2's feature extraction with a Discriminative Feature Enhancement mechanism and a prompt generator integrating Physical Constraints with a Sliding Window.

Result: Achieves an average Dice score of 53.15 on BHSD, surpassing previous methods by +3.65 Dice.

Conclusion: SSS demonstrates superior performance in semi-supervised medical image segmentation, offering a promising direction for future research.

Abstract: In the era of information explosion, efficiently leveraging large-scale
unlabeled data while minimizing the reliance on high-quality pixel-level
annotations remains a critical challenge in the field of medical imaging.
Semi-supervised learning (SSL) enhances the utilization of unlabeled data by
facilitating knowledge transfer, significantly improving the performance of
fully supervised models and emerging as a highly promising research direction
in medical image analysis. Inspired by the ability of Vision Foundation Models
(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised
SAM-2), a novel approach that leverages SAM-2's robust feature extraction
capabilities to uncover latent knowledge in unlabeled medical images, thus
effectively enhancing feature support for fully supervised medical image
segmentation. Specifically, building upon the single-stream "weak-to-strong"
consistency regularization framework, this paper introduces a Discriminative
Feature Enhancement (DFE) mechanism to further explore the feature
discrepancies introduced by various data augmentation strategies across
multiple views. By leveraging feature similarity and dissimilarity across
multi-scale augmentation techniques, the method reconstructs and models the
features, thereby effectively optimizing the salient regions. Furthermore, a
prompt generator is developed that integrates Physical Constraints with a
Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data,
fulfilling SAM-2's requirement for additional prompts. Extensive experiments
demonstrate the superiority of the proposed method for semi-supervised medical
image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,
SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous
state-of-the-art method by +3.65 Dice. Code will be available at
https://github.com/AIGeeksGroup/SSS.

</details>


### [222] [Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF](https://arxiv.org/pdf/2506.08953)
*Anirudh Nanduri, Siyuan Huang, Rama Chellappa*

Main category: cs.CV

TL;DR: A ViT model pretrained on visible imagery is adapted for cross-spectral body recognition, achieving state-of-the-art results by encoding camera information. The study also explores occlusions in VI-ReID using the IJB-MDF dataset.


<details>
  <summary>Details</summary>
Motivation: To enhance cross-spectral matching by adapting ViTs for visible-infrared body recognition and addressing the underexplored issue of occlusions in VI-ReID.

Method: Integrates Side Information Embedding (SIE) to encode domain and camera information, and evaluates performance on the LLCM dataset. Uses IJB-MDF for occlusion analysis.

Result: Encoding only camera information achieves state-of-the-art performance on LLCM. Occlusion impact is analyzed using IJB-MDF.

Conclusion: Camera information encoding is more effective than domain encoding for cross-spectral matching, and occlusions in VI-ReID warrant further study.

Abstract: Vision Transformers (ViTs) have demonstrated impressive performance across a
wide range of biometric tasks, including face and body recognition. In this
work, we adapt a ViT model pretrained on visible (VIS) imagery to the
challenging problem of cross-spectral body recognition, which involves matching
images captured in the visible and infrared (IR) domains. Recent ViT
architectures have explored incorporating additional embeddings beyond
traditional positional embeddings. Building on this idea, we integrate Side
Information Embedding (SIE) and examine the impact of encoding domain and
camera information to enhance cross-spectral matching. Surprisingly, our
results show that encoding only camera information - without explicitly
incorporating domain information - achieves state-of-the-art performance on the
LLCM dataset. While occlusion handling has been extensively studied in
visible-spectrum person re-identification (Re-ID), occlusions in
visible-infrared (VI) Re-ID remain largely underexplored - primarily because
existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly
feature full-body, unoccluded images. To address this gap, we analyze the
impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain
Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared
images captured at various distances, enabling cross-range, cross-spectral
evaluations.

</details>


### [223] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/pdf/2506.08955)
*Chunming He, Kai Li, Yachao Zhang, Ziyun Yang, Youwei Pang, Longxiang Tang, Chengyu Fang, Yulun Zhang, Linghe Kong, Xiu Li, Sina Farsiu*

Main category: cs.CV

TL;DR: The paper introduces SEE, a unified method for Incompletely-Supervised Concealed Object Segmentation (ISCOS), leveraging SAM for pseudo-label generation and hybrid-granularity feature grouping to improve segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of limited supervision from incomplete annotations and the difficulty of distinguishing concealed objects from backgrounds due to intrinsic similarities.

Method: Proposes SEE, a mean-teacher framework using SAM for pseudo-label generation, strategies for pseudo-label quality management, and a hybrid-granularity feature grouping module.

Result: Achieves state-of-the-art performance in ISCOS tasks and enhances existing models as a plug-and-play solution.

Conclusion: SEE effectively tackles ISCOS challenges, offering robust training and improved segmentation coherence.

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [224] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/pdf/2506.08956)
*DaeEun Yoon, Semin Kim, SangWook Yoo, Jongha Lee*

Main category: cs.CV

TL;DR: Proposes an optimal data augmentation method using Fast AutoAugment to improve small object detection, achieving a 20% performance boost on the DOTA dataset.


<details>
  <summary>Details</summary>
Motivation: Small object detection lags behind large object detection despite overall progress in object detection. Addressing this gap is crucial for computer vision.

Method: Uses Fast AutoAugment to quickly find optimal augmentation policies for small object detection.

Result: Achieves a 20% performance improvement on the DOTA dataset.

Conclusion: The proposed method effectively enhances small object detection performance.

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [225] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/pdf/2506.09040)
*Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng Yu, Zhongyu Wei, Jiaqi Wang*

Main category: cs.CV

TL;DR: ASVR introduces joint learning of visual and textual modalities in LVLMs, improving multimodal understanding by reconstructing semantic representations of images.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs lack full visual modality integration, leading to limitations like incomplete visual detail capture and vision-centric content inadequacy.

Method: ASVR uses autoregressive semantic visual reconstruction within a unified framework, avoiding raw visual appearance reconstruction.

Result: ASVR improves multimodal benchmarks, e.g., boosting LLaVA-1.5 by 5% across 14 benchmarks, and works with various data scales and LLM backbones.

Conclusion: Autoregressive semantic reconstruction enhances LVLMs' multimodal understanding, outperforming raw visual reconstruction.

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [226] [ORIDa: Object-centric Real-world Image Composition Dataset](https://arxiv.org/pdf/2506.08964)
*Jinwoo Kim, Sangmin Han, Jinho Jeong, Jiwoo Choi, Dongyoung Kim, Seon Joo Kim*

Main category: cs.CV

TL;DR: ORIDa is a large-scale dataset for object compositing, featuring 30,000+ images with 200 unique objects in diverse scenes, including factual-counterfactual sets and factual-only scenes.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack diversity and scale for real-world object compositing tasks, limiting research progress.

Method: ORIDa introduces two data types: factual-counterfactual sets (5 images per scene) and factual-only scenes (single image per context).

Result: ORIDa is the first publicly available dataset of its scale and complexity, enabling comprehensive research in object compositing.

Conclusion: ORIDa advances object compositing research by providing a diverse, real-world dataset for extensive analysis and experimentation.

Abstract: Object compositing, the task of placing and harmonizing objects in images of
diverse visual scenes, has become an important task in computer vision with the
rise of generative models. However, existing datasets lack the diversity and
scale required to comprehensively explore real-world scenarios. We introduce
ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,
real-captured dataset containing over 30,000 images featuring 200 unique
objects, each of which is presented across varied positions and scenes. ORIDa
has two types of data: factual-counterfactual sets and factual-only scenes. The
factual-counterfactual sets consist of four factual images showing an object in
different positions within a scene and a single counterfactual (or background)
image of the scene without the object, resulting in five images per scene. The
factual-only scenes include a single image containing an object in a specific
context, expanding the variety of environments. To our knowledge, ORIDa is the
first publicly available dataset with its scale and complexity for real-world
image composition. Extensive analysis and experiments highlight the value of
ORIDa as a resource for advancing further research in object compositing.

</details>


### [227] [ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations](https://arxiv.org/pdf/2506.08968)
*Amirreza Rouhi, Solmaz Arezoomandan, Knut Peterson, Joseph T. Woods, David K. Han*

Main category: cs.CV

TL;DR: ADAM is a training-free, self-refining framework for open-world object labeling, using LLMs and CLIP to generate and refine labels for unknown objects without predefined categories.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of predefined categories in object detection models for identifying novel objects in open-world scenarios.

Method: Leverages LLMs for contextual label generation and CLIP for visual embeddings, constructs an Embedding-Label Repository (ELR), and uses frequency-based voting, cross-modal re-ranking, and a self-refinement loop for robust labeling.

Result: Effectively annotates novel categories on COCO and PASCAL datasets without fine-tuning or retraining.

Conclusion: ADAM demonstrates a viable approach for open-world object labeling by combining contextual and visual signals autonomously.

Abstract: Object detection models typically rely on predefined categories, limiting
their ability to identify novel objects in open-world scenarios. To overcome
this constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,
a training-free, self-refining framework for open-world object labeling. ADAM
leverages large language models (LLMs) to generate candidate labels for unknown
objects based on contextual information from known entities within a scene.
These labels are paired with visual embeddings from CLIP to construct an
Embedding-Label Repository (ELR) that enables inference without category
supervision. For a newly encountered unknown object, ADAM retrieves visually
similar instances from the ELR and applies frequency-based voting and
cross-modal re-ranking to assign a robust label. To further enhance
consistency, we introduce a self-refinement loop that re-evaluates repository
labels using visual cohesion analysis and k-nearest-neighbor-based majority
re-labeling. Experimental results on the COCO and PASCAL datasets demonstrate
that ADAM effectively annotates novel categories using only visual and
contextual signals, without requiring any fine-tuning or retraining.

</details>


### [228] [Rethinking Range-View LiDAR Segmentation in Adverse Weather](https://arxiv.org/pdf/2506.08979)
*Longyu Yang, Ping Hu, Lu Zhang, Jun Liu, Yap-Peng Tan, Heng Tao Shen, Xiaofeng Zhu*

Main category: cs.CV

TL;DR: A lightweight framework improves range-view LiDAR segmentation robustness in adverse weather by separating geometric and reflectance processing.


<details>
  <summary>Details</summary>
Motivation: Generalized performance of range-view LiDAR segmentation under adverse weather is underexplored, limiting real-world reliability.

Method: Proposes a modular framework with two branches (GAS and RDC modules) to handle geometric noise and reflectance distortions separately.

Result: Significantly improves generalization to adverse weather with minimal inference overhead.

Conclusion: Offers a practical solution for robust real-world LiDAR segmentation.

Abstract: LiDAR segmentation has emerged as an important task to enrich multimedia
experiences and analysis. Range-view-based methods have gained popularity due
to their high computational efficiency and compatibility with real-time
deployment. However, their generalized performance under adverse weather
conditions remains underexplored, limiting their reliability in real-world
environments. In this work, we identify and analyze the unique challenges that
affect the generalization of range-view LiDAR segmentation in severe weather.
To address these challenges, we propose a modular and lightweight framework
that enhances robustness without altering the core architecture of existing
models. Our method reformulates the initial stem block of standard range-view
networks into two branches to process geometric attributes and reflectance
intensity separately. Specifically, a Geometric Abnormality Suppression (GAS)
module reduces the influence of weather-induced spatial noise, and a
Reflectance Distortion Calibration (RDC) module corrects reflectance
distortions through memory-guided adaptive instance normalization. The
processed features are then fused and passed to the original segmentation
pipeline. Extensive experiments on different benchmarks and baseline models
demonstrate that our approach significantly improves generalization to adverse
weather with minimal inference overhead, offering a practical and effective
solution for real-world LiDAR segmentation.

</details>


### [229] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/pdf/2506.08990)
*Chenyu Lian, Hong-Yu Zhou, Dongyun Liang, Jing Qin, Liansheng Wang*

Main category: cs.CV

TL;DR: ALTA improves medical vision-language alignment by adapting pretrained vision models from masked record modeling, achieving better performance with fewer parameters and computational costs.


<details>
  <summary>Details</summary>
Motivation: Conventional cross-modal contrastive learning methods have suboptimal visual representation, while masked modeling excels in visual representation but struggles with cross-modal matching. ALTA aims to bridge this gap.

Method: ALTA adapts pretrained vision models from masked record modeling, using only 8% trainable parameters and less than 1/5 computational consumption. It integrates temporal-multiview radiograph inputs for better alignment.

Result: ALTA outperforms counterparts by over 4% in text-to-image accuracy and ~6% in image-to-text retrieval accuracy.

Conclusion: ALTA efficiently aligns vision-language models, enhancing both vision and language understanding while reducing resource usage.

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [230] [Do Concept Replacement Techniques Really Erase Unacceptable Concepts?](https://arxiv.org/pdf/2506.08991)
*Anudeep Das, Gurjot Singh, Prach Chantasantitam, N. Asokan*

Main category: cs.CV

TL;DR: The paper critiques current concept replacement techniques (CRTs) in diffusion models for failing to erase unacceptable concepts in image-to-image (I2I) scenarios, introduces the idea of 'fidelity' for preserving other concepts, and proposes a new technique, AntiMirror, to address these issues.


<details>
  <summary>Details</summary>
Motivation: Aligning generative models to avoid generating unacceptable content (e.g., offensive or copyrighted material) is challenging, especially in emerging I2I scenarios where existing CRTs fail.

Method: The study empirically tests CRTs in I2I models, identifies their shortcomings, and introduces 'fidelity' as a key requirement. It then proposes AntiMirror, a targeted image-editing technique.

Result: Current CRTs are ineffective in I2I settings despite working in text-to-image (T2I) pipelines. AntiMirror demonstrates viability in achieving both effectiveness and fidelity.

Conclusion: The paper highlights the need for improved CRTs in I2I models, introduces 'fidelity' as a critical metric, and presents AntiMirror as a promising solution.

Abstract: Generative models, particularly diffusion-based text-to-image (T2I) models,
have demonstrated astounding success. However, aligning them to avoid
generating content with unacceptable concepts (e.g., offensive or copyrighted
content, or celebrity likenesses) remains a significant challenge. Concept
replacement techniques (CRTs) aim to address this challenge, often by trying to
"erase" unacceptable concepts from models. Recently, model providers have
started offering image editing services which accept an image and a text prompt
as input, to produce an image altered as specified by the prompt. These are
known as image-to-image (I2I) models. In this paper, we first use an I2I model
to empirically demonstrate that today's state-of-the-art CRTs do not in fact
erase unacceptable concepts. Existing CRTs are thus likely to be ineffective in
emerging I2I scenarios, despite their proven ability to remove unwanted
concepts in T2I pipelines, highlighting the need to understand this discrepancy
between T2I and I2I settings. Next, we argue that a good CRT, while replacing
unacceptable concepts, should preserve other concepts specified in the inputs
to generative models. We call this fidelity. Prior work on CRTs have neglected
fidelity in the case of unacceptable concepts. Finally, we propose the use of
targeted image-editing techniques to achieve both effectiveness and fidelity.
We present such a technique, AntiMirror, and demonstrate its viability.

</details>


### [231] [SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction](https://arxiv.org/pdf/2506.08997)
*Fabian Immel, Jan-Hendrik Pauls, Richard Fehler, Frank Bieder, Jonas Merkert, Christoph Stiller*

Main category: cs.CV

TL;DR: SDTagNet enhances online HD map construction by leveraging SD maps and NLP-derived features, improving far-range detection accuracy by up to 45%.


<details>
  <summary>Details</summary>
Motivation: High maintenance costs of HD maps limit scalability; SD maps offer a cheaper alternative but lack detail. SDTagNet aims to bridge this gap.

Method: Incorporates SD map data (polylines and textual annotations) and introduces a point-level encoder with orthogonal identifiers for uniform integration.

Result: Boosts performance by up to +5.9 mAP (45%) over no-prior methods and +3.2 mAP (20%) over prior SD-based methods.

Conclusion: SDTagNet effectively utilizes SD maps and NLP to improve HD map construction, offering a scalable solution with significant accuracy gains.

Abstract: Autonomous vehicles rely on detailed and accurate environmental information
to operate safely. High definition (HD) maps offer a promising solution, but
their high maintenance cost poses a significant barrier to scalable deployment.
This challenge is addressed by online HD map construction methods, which
generate local HD maps from live sensor data. However, these methods are
inherently limited by the short perception range of onboard sensors. To
overcome this limitation and improve general performance, recent approaches
have explored the use of standard definition (SD) maps as prior, which are
significantly easier to maintain. We propose SDTagNet, the first online HD map
construction method that fully utilizes the information of widely available SD
maps, like OpenStreetMap, to enhance far range detection accuracy. Our approach
introduces two key innovations. First, in contrast to previous work, we
incorporate not only polyline SD map data with manually selected classes, but
additional semantic information in the form of textual annotations. In this
way, we enrich SD vector map tokens with NLP-derived features, eliminating the
dependency on predefined specifications or exhaustive class taxonomies. Second,
we introduce a point-level SD map encoder together with orthogonal element
identifiers to uniformly integrate all types of map elements. Experiments on
Argoverse 2 and nuScenes show that this boosts map perception performance by up
to +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP
(+20%) w.r.t. previous approaches that already use SD map priors. Code is
available at https://github.com/immel-f/SDTagNet

</details>


### [232] [Do MIL Models Transfer?](https://arxiv.org/pdf/2506.09022)
*Daniel Shao, Richard J. Chen, Andrew H. Song, Joel Runevic, Ming Y. Lu, Tong Ding, Faisal Mahmood*

Main category: cs.CV

TL;DR: Pretrained MIL models outperform scratch-trained ones in computational pathology, even across different organs, and pancancer pretraining enhances generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of understanding about MIL model transferability in computational pathology, despite its widespread use in other fields.

Method: Systematic evaluation of 11 pretrained MIL models across 21 tasks for morphological and molecular subtype prediction.

Result: Pretrained MIL models consistently outperform scratch-trained models, with pancancer pretraining showing strong generalization.

Conclusion: MIL models are adaptable and benefit from transfer learning in computational pathology; a standardized resource is provided.

Abstract: Multiple Instance Learning (MIL) is a cornerstone approach in computational
pathology (CPath) for generating clinically meaningful slide-level embeddings
from gigapixel tissue images. However, MIL often struggles with small, weakly
supervised clinical datasets. In contrast to fields such as NLP and
conventional computer vision, where transfer learning is widely used to address
data scarcity, the transferability of MIL models remains poorly understood. In
this study, we systematically evaluate the transfer learning capabilities of
pretrained MIL models by assessing 11 models across 21 pretraining tasks for
morphological and molecular subtype prediction. Our results show that
pretrained MIL models, even when trained on different organs than the target
task, consistently outperform models trained from scratch. Moreover,
pretraining on pancancer datasets enables strong generalization across organs
and tasks, outperforming slide foundation models while using substantially less
pretraining data. These findings highlight the robust adaptability of MIL
models and demonstrate the benefits of leveraging transfer learning to boost
performance in CPath. Lastly, we provide a resource which standardizes the
implementation of MIL models and collection of pretrained model weights on
popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab

</details>


### [233] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/pdf/2506.09024)
*Felix Wagner, Pramit Saha, Harry Anthony, J. Alison Noble, Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: The paper introduces DIsoN, a decentralized framework for OOD detection in medical imaging, enabling secure comparison of test and training data without sharing raw data.


<details>
  <summary>Details</summary>
Motivation: Safe ML deployment in medical imaging requires OOD detection, but existing methods either discard training data or assume centralized storage, which is impractical due to privacy and size constraints.

Method: DIsoN uses Isolation Networks to quantify separation difficulty between test and training data via binary classification, enabling decentralized parameter exchange instead of raw data sharing.

Result: DIsoN outperforms existing methods on four medical imaging datasets across 12 OOD tasks while maintaining data privacy.

Conclusion: DIsoN offers a practical, privacy-preserving solution for OOD detection, paving the way for secure remote utilization of training data in ML services.

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


### [234] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/pdf/2506.09027)
*Runqian Wang, Kaiming He*

Main category: cs.CV

TL;DR: The paper introduces Dispersive Loss, a plug-and-play regularizer for diffusion-based generative models, improving their performance without additional requirements.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between generative modeling and representation learning by enhancing diffusion models with explicit regularization.

Method: Proposes Dispersive Loss, a regularization technique that disperses internal representations in hidden space, inspired by contrastive learning but without needing positive pairs.

Result: Demonstrates consistent improvements on ImageNet across various models, outperforming strong baselines like REPA.

Conclusion: Dispersive Loss effectively enhances diffusion models, offering a simple, self-contained solution to improve generative modeling.

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [235] [Princeton365: A Diverse Dataset with Accurate Camera Pose](https://arxiv.org/pdf/2506.09035)
*Karhan Kayan, Stamatis Alexandropoulos, Rishabh Jain, Yiming Zuo, Erich Liang, Jia Deng*

Main category: cs.CV

TL;DR: Princeton365 is a diverse dataset with accurate camera pose, bridging accuracy and diversity gaps in SLAM benchmarks. It includes indoor, outdoor, and object scanning videos with RGB and IMU data, and introduces a scene scale-aware evaluation metric and a Novel View Synthesis benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the gap between accuracy and diversity in SLAM benchmarks by providing a dataset with precise camera pose and diverse scenarios.

Method: Uses a ground truth collection framework with calibration boards and a 360-camera, capturing synchronized monocular/stereo RGB and IMU data. Introduces a scene scale-aware metric for SLAM evaluation.

Result: A dataset (Princeton365) with 365 videos, a new evaluation metric for SLAM, and a Novel View Synthesis benchmark.

Conclusion: Princeton365 enhances SLAM research by offering diverse, accurate data and improved evaluation tools, fostering better analysis of method failures.

Abstract: We introduce Princeton365, a large-scale diverse dataset of 365 videos with
accurate camera pose. Our dataset bridges the gap between accuracy and data
diversity in current SLAM benchmarks by introducing a novel ground truth
collection framework that leverages calibration boards and a 360-camera. We
collect indoor, outdoor, and object scanning videos with synchronized monocular
and stereo RGB video outputs as well as IMU. We further propose a new scene
scale-aware evaluation metric for SLAM based on the the optical flow induced by
the camera pose estimation error. In contrast to the current metrics, our new
metric allows for comparison between the performance of SLAM methods across
scenes as opposed to existing metrics such as Average Trajectory Error (ATE),
allowing researchers to analyze the failure modes of their methods. We also
propose a challenging Novel View Synthesis benchmark that covers cases not
covered by current NVS benchmarks, such as fully non-Lambertian scenes with
360-degree camera trajectories. Please visit
https://princeton365.cs.princeton.edu for the dataset, code, videos, and
submission.

</details>


### [236] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/pdf/2506.09042)
*Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao, Shengyu Huang, Amirmojtaba Sabour, Tianchang Shen, Tobias Pfaff, Jay Zhangjie Wu, Runjian Chen, Seung Wook Kim, Jun Gao, Laura Leal-Taixe, Mike Chen, Sanja Fidler, Huan Ling*

Main category: cs.CV

TL;DR: The paper introduces Cosmos-Drive-Dreams, a synthetic data generation pipeline for creating challenging driving scenarios to improve AV training and testing.


<details>
  <summary>Details</summary>
Motivation: Real-world data collection for AVs is costly and struggles with rare edge cases, which are crucial for system robustness.

Method: The pipeline uses Cosmos-Drive, a suite of models derived from NVIDIA Cosmos, to generate controllable, high-fidelity driving videos.

Result: The generated data enhances dataset diversity, mitigates long-tail distribution issues, and improves tasks like 3D lane detection and driving policy learning.

Conclusion: The open-sourced pipeline and models provide a scalable solution for generating high-quality synthetic driving data.

Abstract: Collecting and annotating real-world data for safety-critical physical AI
systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is
especially challenging to capture rare edge cases, which play a critical role
in training and testing of an AV system. To address this challenge, we
introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline
that aims to generate challenging scenarios to facilitate downstream tasks such
as perception and driving policy training. Powering this pipeline is
Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation
model for the driving domain and are capable of controllable, high-fidelity,
multi-view, and spatiotemporally consistent driving video generation. We
showcase the utility of these models by applying Cosmos-Drive-Dreams to scale
the quantity and diversity of driving datasets with high-fidelity and
challenging scenarios. Experimentally, we demonstrate that our generated data
helps in mitigating long-tail distribution problems and enhances generalization
in downstream tasks such as 3D lane detection, 3D object detection and driving
policy learning. We open source our pipeline toolkit, dataset and model weights
through the NVIDIA's Cosmos platform.
  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [237] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/pdf/2506.09045)
*Zehong Ma, Longhui Wei, Feng Wang, Shiliang Zhang, Qi Tian*

Main category: cs.CV

TL;DR: A novel Magnitude-aware Cache (MagCache) method accelerates video diffusion models by leveraging a unified magnitude law, achieving significant speedups without compromising visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing acceleration techniques for video diffusion models rely on uniform heuristics or time-embedding variants, risking inconsistent outputs and requiring extensive calibration.

Method: MagCache uses a unified magnitude law to adaptively skip unimportant timesteps via error modeling and adaptive caching, requiring only a single calibration sample.

Result: MagCache achieves 2.1x and 2.68x speedups on Open-Sora and Wan 2.1, respectively, with superior visual fidelity and outperforms existing methods in LPIPS, SSIM, and PSNR.

Conclusion: MagCache is a robust and efficient acceleration method for video diffusion models, eliminating the need for extensive calibration while maintaining high-quality outputs.

Abstract: Existing acceleration techniques for video diffusion models often rely on
uniform heuristics or time-embedding variants to skip timesteps and reuse
cached features. These approaches typically require extensive calibration with
curated prompts and risk inconsistent outputs due to prompt-specific
overfitting. In this paper, we introduce a novel and robust discovery: a
unified magnitude law observed across different models and prompts.
Specifically, the magnitude ratio of successive residual outputs decreases
monotonically and steadily in most timesteps while rapidly in the last several
steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)
that adaptively skips unimportant timesteps using an error modeling mechanism
and adaptive caching strategy. Unlike existing methods requiring dozens of
curated samples for calibration, MagCache only requires a single sample for
calibration. Experimental results show that MagCache achieves 2.1x and 2.68x
speedups on Open-Sora and Wan 2.1, respectively, while preserving superior
visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,
and PSNR, under comparable computational budgets.

</details>


### [238] [Mitigating Prior Shape Bias in Point Clouds via Differentiable Center Learning](https://arxiv.org/pdf/2402.02088)
*Zhe Li, Xiying Wang, Jinglin Zhao, Zheng Wang, Debin Liu, Laurence T. Yang*

Main category: cs.CV

TL;DR: The paper introduces DCS-Net to address information leakage in point cloud models by combining global and local feature reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing point cloud models suffer from information leakage and trivial proxy tasks, limiting their ability to learn expressive representations.

Method: Proposes DCS-Net, which integrates global and local feature reconstruction as non-trivial proxy tasks.

Result: DCS-Net improves expressive capacity and mitigates information leakage in point cloud models.

Conclusion: The method successfully enhances model performance by addressing key limitations in existing approaches.

Abstract: Masked autoencoding and generative pretraining have achieved remarkable
success in computer vision and natural language processing, and more recently,
they have been extended to the point cloud domain. Nevertheless, existing point
cloud models suffer from the issue of information leakage due to the
pre-sampling of center points, which leads to trivial proxy tasks for the
models. These approaches primarily focus on local feature reconstruction,
limiting their ability to capture global patterns within point clouds. In this
paper, we argue that the reduced difficulty of pretext tasks hampers the
model's capacity to learn expressive representations. To address these
limitations, we introduce a novel solution called the Differentiable Center
Sampling Network (DCS-Net). It tackles the information leakage problem by
incorporating both global feature reconstruction and local feature
reconstruction as non-trivial proxy tasks, enabling simultaneous learning of
both the global and local patterns within point cloud. Experimental results
demonstrate that our method enhances the expressive capacity of existing point
cloud models and effectively addresses the issue of information leakage.

</details>


### [239] [Multimodal Rationales for Explainable Visual Question Answering](https://arxiv.org/pdf/2402.03896)
*Kun Li, George Vosselman, Michael Ying Yang*

Main category: cs.CV

TL;DR: MRVQA introduces visual and textual rationales for VQA, improving trustworthiness and explainability with a new vtS metric.


<details>
  <summary>Details</summary>
Motivation: Prior VQA models lack transparency, predicting correct answers without proper reasoning, raising trust issues.

Method: MRVQA generates multimodal rationales (visual and textual) and uses a vtS score to evaluate rationale quality.

Result: MRVQA achieves state-of-the-art results on EVQA datasets, enhancing model trustworthiness.

Conclusion: MRVQA advances explainable VQA by providing rationales and a new metric, with code and dataset publicly available.

Abstract: Visual Question Answering (VQA) is a challenging task of predicting the
answer to a question about the content of an image. Prior works directly
evaluate the answering models by simply calculating the accuracy of predicted
answers. However, the inner reasoning behind the predictions is disregarded in
such a "black box" system, and we cannot ascertain the trustworthiness of the
predictions. Even more concerning, in some cases, these models predict correct
answers despite focusing on irrelevant visual regions or textual tokens. To
develop an explainable and trustworthy answering system, we propose a novel
model termed MRVQA (Multimodal Rationales for VQA), which provides visual and
textual rationales to support its predicted answers. To measure the quality of
generated rationales, a new metric vtS (visual-textual Similarity) score is
introduced from both visual and textual perspectives. Considering the extra
annotations distinct from standard VQA, MRVQA is trained and evaluated using
samples synthesized from some existing datasets. Extensive experiments across
three EVQA datasets demonstrate that MRVQA achieves new state-of-the-art
results through additional rationale generation, enhancing the trustworthiness
of the explainable VQA model. The code and the synthesized dataset are released
under https://github.com/lik1996/MRVQA2025.

</details>


### [240] [Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap](https://arxiv.org/pdf/2402.04416)
*Christopher Liao, Christian So, Theodoros Tsiligkaridis, Brian Kulis*

Main category: cs.CV

TL;DR: The paper addresses unsupervised domain generalization (MUDG) by leveraging a task-agnostic unlabeled source dataset and cross-modal search, proposing paired k-means and adaptive text augmentation to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-world applications often lack labeled source data matching the target task, making traditional domain generalization (DG) methods impractical. The paper aims to solve this using multimodal unsupervised DG.

Method: The framework uses cross-modal search in a vision-language space, introducing paired k-means for better nearest neighbor recall and adaptive text augmentation for target labels.

Result: The method outperforms state-of-the-art name-only transfer, source-free DG, and zero-shot methods on 20 diverse datasets.

Conclusion: The proposed MUDG framework effectively generalizes to unseen domains without requiring labeled source data, demonstrating superior performance across benchmarks.

Abstract: Domain generalization (DG) is an important problem that learns a model which
generalizes to unseen test domains leveraging one or more source domains, under
the assumption of shared label spaces. However, most DG methods assume access
to abundant source data in the target label space, a requirement that proves
overly stringent for numerous real-world applications, where acquiring the same
label space as the target task is prohibitively expensive. For this setting, we
tackle the multimodal version of the unsupervised domain generalization (MUDG)
problem, which uses a large task-agnostic unlabeled source dataset during
finetuning. Our framework does not explicitly assume any relationship between
the source dataset and target task. Instead, it relies only on the premise that
the source dataset can be accurately and efficiently searched in a joint
vision-language space. We make three contributions in the MUDG setting.
Firstly, we show theoretically that cross-modal approximate nearest neighbor
search suffers from low recall due to the large distance between text queries
and the image centroids used for coarse quantization. Accordingly, we propose
paired k-means, a simple clustering algorithm that improves nearest neighbor
recall by storing centroids in query space instead of image space. Secondly, we
propose an adaptive text augmentation scheme for target labels designed to
improve zero-shot accuracy and diversify retrieved image data. Lastly, we
present two simple but effective components to further improve downstream
target accuracy. We compare against state-of-the-art name-only transfer,
source-free DG and zero-shot (ZS) methods on their respective benchmarks and
show consistent improvement in accuracy on 20 diverse datasets. Code is
available: https://github.com/Chris210634/mudg

</details>


### [241] [SMCD: High Realism Motion Style Transfer via Mamba-based Diffusion](https://arxiv.org/pdf/2405.02844)
*Ziyun Qian, Zeyu Xiao, Xingliang Jin, Dingkang Yang, Mingcheng Li, Zhenyi Wu, Dongliang Kou, Peng Zhai, Lihua Zhang*

Main category: cs.CV

TL;DR: The paper proposes a Unified Motion Style Diffusion (UMSD) framework with a Motion Style Mamba (MSM) denoiser to improve motion style transfer by addressing content-style relationships and temporal dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing motion style transfer methods overlook intrinsic content-style relationships and struggle with temporal dependencies, leading to unnatural results.

Method: The UMSD framework integrates content and style motion features, uses MSM for sequence modeling, and employs diffusion-based consistency losses.

Result: The method outperforms SOTA techniques, producing more realistic and coherent stylized motions.

Conclusion: The UMSD framework effectively addresses limitations in motion style transfer, enhancing realism and coherence.

Abstract: Motion style transfer is a significant research direction in the field of
computer vision, enabling virtual digital humans to rapidly switch between
different styles of the same motion, thereby significantly enhancing the
richness and realism of movements. It has been widely applied in multimedia
scenarios such as films, games, and the metaverse. However, most existing
methods adopt a two-stream structure, which tends to overlook the intrinsic
relationship between content and style motions, leading to information loss and
poor alignment. Moreover, when handling long-range motion sequences, these
methods fail to effectively learn temporal dependencies, ultimately resulting
in unnatural generated motions. To address these limitations, we propose a
Unified Motion Style Diffusion (UMSD) framework, which simultaneously extracts
features from both content and style motions and facilitates sufficient
information interaction. Additionally, we introduce the Motion Style Mamba
(MSM) denoiser, the first approach in the field of motion style transfer to
leverage Mamba's powerful sequence modelling capability. Better capturing
temporal relationships generates more coherent stylized motion sequences.
Third, we design a diffusion-based content consistency loss and a style
consistency loss to constrain the framework, ensuring that it inherits the
content motion while effectively learning the characteristics of the style
motion. Finally, extensive experiments demonstrate that our method outperforms
state-of-the-art (SOTA) methods qualitatively and quantitatively, achieving
more realistic and coherent motion style transfer.

</details>


### [242] [MedVersa: A Generalist Foundation Model for Medical Image Interpretation](https://arxiv.org/pdf/2405.07988)
*Hong-Yu Zhou, Julián Nicolás Acosta, Subathra Adithan, Suvrankar Datta, Eric J. Topol, Pranav Rajpurkar*

Main category: cs.CV

TL;DR: MedVersa is a generalist medical AI model that outperforms specialized solutions in multiple tasks, reducing report time and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current medical AI systems are narrow in scope, limiting adoption. MedVersa aims to bridge this gap with a generalist approach.

Method: Trained on tens of millions of medical instances, MedVersa handles multimodal inputs and outputs for diverse medical imaging tasks.

Result: Achieves state-of-the-art in nine tasks, outperforming specialists by 10+%, and matches/exceeds human reports in 71% of cases.

Conclusion: Flexible, multimodal AI like MedVersa enhances medical image interpretation and clinical support.

Abstract: Current medical AI systems are often limited to narrow applications,
hindering widespread adoption. We present MedVersa, a generalist foundation
model trained on tens of millions of compiled medical instances. MedVersa
unlocks generalist learning from multimodal inputs and outputs, representing
the first example of a generalist model reaching competitive performance with
leading specialized solutions across a variety of medical imaging scenarios.
MedVersa achieves state-of-the-art performance in nine tasks, sometimes
outperforming counterparts by over 10%. Radiologist evaluation shows
MedVersa-generated reports get superior performance in 95% of normal studies,
while matching or exceeding human reports in 71% of cases overall. User studies
showed notable reductions in report writing time and discrepancies with the use
of MedVersa. Our findings underscore the value of flexible, multimodal AI
systems in advancing medical image interpretation and supporting clinical
expertise.

</details>


### [243] [Markerless Multi-view 3D Human Pose Estimation: a survey](https://arxiv.org/pdf/2407.03817)
*Ana Filipa Rodrigues Nogueira, Hélder P. Oliveira, Luís F. Teixeira*

Main category: cs.CV

TL;DR: A survey on multi-view 3D human pose estimation highlights challenges like occlusions and data scarcity, reviews existing methods, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of comprehensive surveys on multi-view 3D human pose estimation and the challenges hindering real-world deployment.

Method: Reviews existing approaches, focusing on fully-supervised geometric constraints, temporal consistency, and 3D features.

Result: Identifies trade-offs between complexity and performance, noting no method fully solves all challenges.

Conclusion: Proposes future research directions like active learning, low-supervision methods, and multi-modal approaches to improve accuracy and efficiency.

Abstract: 3D human pose estimation involves reconstructing the human skeleton by
detecting the body joints. Accurate and efficient solutions are required for
several real-world applications including animation, human-robot interaction,
surveillance, and sports. However, challenges such as occlusions, 2D pose
mismatches, random camera perspectives, and limited 3D labelled data have been
hampering the models' performance and limiting their deployment in real-world
scenarios. The higher availability of cameras has led researchers to explore
multi-view solutions to take advantage of the different perspectives to
reconstruct the pose.
  Most existing reviews have mainly focused on monocular 3D human pose
estimation, so a comprehensive survey on multi-view approaches has been missing
since 2012. According to the reviewed articles, the majority of the existing
methods are fully-supervised approaches based on geometric constraints, which
are often limited by 2D pose mismatches. To mitigate this, researchers have
proposed incorporating temporal consistency or depth information.
Alternatively, working directly with 3D features has been shown to completely
overcome this issue, albeit at the cost of increased computational complexity.
Additionally, models with lower levels of supervision have been identified to
help address challenges such as annotated data scarcity and generalisation to
new setups. Therefore, no method currently addresses all challenges associated
with 3D pose reconstruction, and a trade-off between complexity and performance
exists. Further research is needed to develop approaches capable of quickly
inferring a highly accurate 3D pose with bearable computation cost. Techniques
such as active learning, low-supervision methods, temporal consistency, view
selection, depth information estimation, and multi-modal approaches are
strategies to consider when developing a new method for this task.

</details>


### [244] [An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification](https://arxiv.org/pdf/2407.21666)
*Aswini Kumar Patra, Ankit Varshney, Lingaraj Sahoo*

Main category: cs.CV

TL;DR: The paper proposes an explainable deep learning pipeline using vision transformers (ViTs) for early drought stress detection in potato crops via aerial imagery, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Early detection of drought stress is crucial to prevent irreversible crop damage, and subtle phenotypical changes can be captured using non-invasive imaging and machine learning.

Method: Two approaches are used: a ViT-SVM combination for feature extraction and classification, and an end-to-end ViT with a classification layer. Attention maps visualize the model's focus on drought stress features.

Result: The methods achieve high accuracy in identifying drought stress and reveal subtle plant features linked to stress, providing interpretable insights.

Conclusion: The pipeline offers a robust, interpretable solution for farmers to monitor drought stress and make informed crop management decisions.

Abstract: Early detection of drought stress is critical for taking timely measures for
reducing crop loss before the drought impact becomes irreversible. The subtle
phenotypical and physiological changes in response to drought stress are
captured by non-invasive imaging techniques and these imaging data serve as
valuable resource for machine learning methods to identify drought stress.
While convolutional neural networks (CNNs) are in wide use, vision transformers
(ViTs) present a promising alternative in capturing long-range dependencies and
intricate spatial relationships, thereby enhancing the detection of subtle
indicators of drought stress. We propose an explainable deep learning pipeline
that leverages the power of ViTs for drought stress detection in potato crops
using aerial imagery. We applied two distinct approaches: a synergistic
combination of ViT and support vector machine (SVM), where ViT extracts
intricate spatial features from aerial images, and SVM classifies the crops as
stressed or healthy and an end-to-end approach using a dedicated classification
layer within ViT to directly detect drought stress. Our key findings explain
the ViT model's decision-making process by visualizing attention maps. These
maps highlight the specific spatial features within the aerial images that the
ViT model focuses as the drought stress signature. Our findings demonstrate
that the proposed methods not only achieve high accuracy in drought stress
identification but also shedding light on the diverse subtle plant features
associated with drought stress. This offers a robust and interpretable solution
for drought stress monitoring for farmers to undertake informed decisions for
improved crop management.

</details>


### [245] [Just Project! Multi-Channel Despeckling, the Easy Way](https://arxiv.org/pdf/2408.11531)
*Loïc Denis, Emanuele Dalsasso, Florence Tupin*

Main category: cs.CV

TL;DR: MuChaPro is a framework for multi-channel SAR image despeckling by leveraging single-channel methods through projections and recombination.


<details>
  <summary>Details</summary>
Motivation: Speckle reduction in multi-channel SAR images is crucial for applications like polarimetric classification and interferometric height estimation, but existing deep learning methods are limited to single-channel.

Method: MuChaPro generates single-channel projections, applies despeckling, and recombines them into a multi-channel estimate, supporting self-supervised training for sensor-specific networks.

Result: The framework effectively reduces speckle in polarimetric and interferometric SAR images.

Conclusion: MuChaPro provides a simple yet effective solution for multi-channel SAR despeckling, with potential for sensor-specific adaptation.

Abstract: Reducing speckle fluctuations in multi-channel SAR images is essential in
many applications of SAR imaging such as polarimetric classification or
interferometric height estimation. While single-channel despeckling has widely
benefited from the application of deep learning techniques, extensions to
multi-channel SAR images are much more challenging. This paper introduces
MuChaPro, a generic framework that exploits existing single-channel despeckling
methods. The key idea is to generate numerous single-channel projections,
restore these projections, and recombine them into the final multi-channel
estimate. This simple approach is shown to be effective in polarimetric and/or
interferometric modalities. A special appeal of MuChaPro is the possibility to
apply a self-supervised training strategy to learn sensor-specific networks for
single-channel despeckling.

</details>


### [246] [A Survey of the Self Supervised Learning Mechanisms for Vision Transformers](https://arxiv.org/pdf/2408.17059)
*Asifullah Khan, Anabia Sohail, Mustansar Fiaz, Mehdi Hassan, Tariq Habib Afridi, Sibghat Ullah Marwat, Farzeen Munir, Safdar Ali, Hannan Naseem, Muhammad Zaigham Zaheer, Kamran Ali, Tangina Sultana, Ziaurrehman Tanoli, Naeem Akhter*

Main category: cs.CV

TL;DR: The paper surveys self-supervised learning (SSL) techniques for Vision Transformers (ViTs), proposing a taxonomy, reviewing tasks, and comparing methods to address data and parameter challenges.


<details>
  <summary>Details</summary>
Motivation: ViTs' reliance on large labeled datasets and parameter intensity motivates exploring SSL as a scalable, label-free alternative for training.

Method: The survey systematically reviews SSL mechanisms for ViTs, classifying techniques by representations and pre-training tasks, and comparing methods.

Result: A taxonomy for SSL techniques is proposed, and advancements, challenges, and comparative strengths of SSL methods for ViTs are highlighted.

Conclusion: SSL integration with ViTs offers a promising, resource-efficient solution for limited labeled data scenarios, with ongoing challenges and opportunities for advancement.

Abstract: Vision Transformers (ViTs) have recently demonstrated remarkable performance
in computer vision tasks. However, their parameter-intensive nature and
reliance on large amounts of data for effective performance have shifted the
focus from traditional human-annotated labels to unsupervised learning and
pretraining strategies that uncover hidden structures within the data. In
response to this challenge, self-supervised learning (SSL) has emerged as a
promising paradigm. SSL leverages inherent relationships within the data itself
as a form of supervision, eliminating the need for manual labeling and offering
a more scalable and resource-efficient alternative for model training. Given
these advantages, it is imperative to explore the integration of SSL techniques
with ViTs, particularly in scenarios with limited labeled data. Inspired by
this evolving trend, this survey aims to systematically review SSL mechanisms
tailored for ViTs. We propose a comprehensive taxonomy to classify SSL
techniques based on their representations and pre-training tasks. Additionally,
we discuss the motivations behind SSL, review prominent pre-training tasks, and
highlight advancements and challenges in this field. Furthermore, we conduct a
comparative analysis of various SSL methods designed for ViTs, evaluating their
strengths, limitations, and applicability to different scenarios.

</details>


### [247] [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/pdf/2410.14669)
*Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan*

Main category: cs.CV

TL;DR: VLMs struggle with natural adversarial samples, prompting the creation of NaturalBench, a challenging benchmark revealing significant performance gaps between models and humans.


<details>
  <summary>Details</summary>
Motivation: To assess the true effectiveness of VLMs by identifying their limitations in handling natural adversarial samples and biases.

Method: A semi-automated approach using CLIP and ChatGPT to generate and verify 10,000 VQA samples, paired with vision-centric design for robust evaluation.

Result: State-of-the-art VLMs lag 50%-70% behind human performance (over 90%) on NaturalBench, highlighting biases and compositional challenges.

Conclusion: NaturalBench exposes critical weaknesses in VLMs, emphasizing the need for diverse skills and unbiased evaluations in future model development.

Abstract: Vision-language models (VLMs) have made significant progress in recent
visual-question-answering (VQA) benchmarks that evaluate complex
visio-linguistic reasoning. However, are these models truly effective? In this
work, we show that VLMs still struggle with natural images and questions that
humans can easily answer, which we term natural adversarial samples. We also
find it surprisingly easy to generate these VQA samples from natural image-text
corpora using off-the-shelf models like CLIP and ChatGPT. We propose a
semi-automated approach to collect a new benchmark, NaturalBench, for reliably
evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a
$\textbf{vision-centric}$ design by pairing each question with two images that
yield different answers, preventing blind solutions from answering without
using the images. This makes NaturalBench more challenging than previous
benchmarks that can be solved with commonsense priors. We evaluate 53
state-of-the-art VLMs on NaturalBench, showing that models like
LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o
lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is
hard from two angles: (1) Compositionality: Solving NaturalBench requires
diverse visio-linguistic skills, including understanding attribute bindings,
object relationships, and advanced reasoning like logic and counting. To this
end, unlike prior work that uses a single tag per sample, we tag each
NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)
Biases: NaturalBench exposes severe biases in VLMs, as models often choose the
same answer regardless of the image. Lastly, we apply our benchmark curation
method to diverse data sources, including long captions (over 100 words) and
non-English languages like Chinese and Hindi, highlighting its potential for
dynamic evaluations of VLMs.

</details>


### [248] [xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs](https://arxiv.org/pdf/2410.16267)
*Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Jongwoo Park, Kanchana Ranasinghe, Silvio Savarese, Ran Xu, Caiming Xiong, Juan Carlos Niebles*

Main category: cs.CV

TL;DR: BLIP-3-Video is a multimodal language model for videos that efficiently captures temporal information using fewer visual tokens than competitors.


<details>
  <summary>Details</summary>
Motivation: To improve video understanding by efficiently processing temporal information across multiple frames with fewer visual tokens.

Method: Uses a 'temporal encoder' alongside a visual tokenizer to map sequences of frames into compact visual tokens. Explores spatio-temporal pooling and sequential models like Token Turing Machines.

Result: Achieves video question-answering accuracy comparable to larger models (34B) while being smaller (4B) and more efficient.

Conclusion: BLIP-3-Video is a compact, efficient model for video understanding, matching larger models' performance with fewer resources.

Abstract: We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for
videos, particularly designed to efficiently capture temporal information over
multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in
addition to the conventional visual tokenizer, which maps a sequence of tokens
over multiple frames into a compact set of visual tokens. This enables
BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32
vs. 4608 tokens). We explore different types of temporal encoders, including
learnable spatio-temporal pooling as well as sequential models like Token
Turing Machines. We experimentally confirm that BLIP-3-Video obtains video
question-answering accuracies comparable to much larger state-of-the-art models
(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using
fewer visual tokens. The project website is at
https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html

</details>


### [249] [UnCLe: Benchmarking Unsupervised Continual Learning for Depth Completion](https://arxiv.org/pdf/2410.18074)
*Xien Chen, Rit Gangopadhyay, Michael Chu, Patrick Rim, Hyoungseob Park, Alex Wong*

Main category: cs.CV

TL;DR: UnCLe is the first benchmark for unsupervised continual learning in multimodal 3D reconstruction, focusing on depth completion. It addresses catastrophic forgetting in nonstationary data distributions and invites further research.


<details>
  <summary>Details</summary>
Motivation: Existing depth completion methods fail in unsupervised continual learning due to catastrophic forgetting when adapting to nonstationary data. UnCLe aims to standardize benchmarking for this challenge.

Method: UnCLe adapts depth completion models to sequences of diverse datasets, simulating nonstationary distributions. It employs continual learning paradigms for unsupervised learning.

Result: Benchmarking reveals catastrophic forgetting in current methods, highlighting the open problem of unsupervised continual learning in depth completion.

Conclusion: UnCLe serves as a platform for advancing research in unsupervised continual learning for depth completion, addressing a critical gap in the field.

Abstract: We propose UnCLe, the first standardized benchmark for Unsupervised Continual
Learning of a multimodal 3D reconstruction task: Depth completion aims to infer
a dense depth map from a pair of synchronized RGB image and sparse depth map.
We benchmark depth completion models under the practical scenario of
unsupervised learning over continuous streams of data. While unsupervised
learning of depth boasts the possibility continual learning of novel data
distributions over time, existing methods are typically trained on a static, or
stationary, dataset. However, when adapting to novel nonstationary
distributions, they ``catastrophically forget'' previously learned information.
UnCLe simulates these non-stationary distributions by adapting depth completion
models to sequences of datasets containing diverse scenes captured from
distinct domains using different visual and range sensors. We adopt
representative methods from continual learning paradigms and translate them to
enable unsupervised continual learning of depth completion. We benchmark these
models across indoor and outdoor environments, and investigate the degree of
catastrophic forgetting through standard quantitative metrics. We find that
unsupervised continual learning of depth completion is an open problem, and we
invite researchers to leverage UnCLe as a development platform.

</details>


### [250] [Grouped Discrete Representation for Object-Centric Learning](https://arxiv.org/pdf/2411.02299)
*Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen*

Main category: cs.CV

TL;DR: GDR improves OCL by decomposing features into combinatorial attributes and quantizing them via tuple code indexes, enhancing object separability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing OCL methods treat features as indivisible units and discretize them via scalar code indexes, losing attribute-level similarities and differences.

Method: Proposes Grouped Discrete Representation (GDR), decomposing features into combinatorial attributes via channel grouping and quantizing them with tuple code indexes.

Result: GDR consistently improves OCL methods across datasets, with superior object separability and interpretability.

Conclusion: GDR enhances OCL by addressing limitations of existing methods, offering better generalization and convergence.

Abstract: Object-Centric Learning (OCL) aims to discover objects in images or videos by
reconstructing the input. Representative methods achieve this by reconstructing
the input as its Variational Autoencoder (VAE) discrete representations, which
suppress (super-)pixel noise and enhance object separability. However, these
methods treat features as indivisible units, overlooking their compositional
attributes, and discretize features via scalar code indexes, losing
attribute-level similarities and differences. We propose Grouped Discrete
Representation (GDR) for OCL. For better generalization, features are
decomposed into combinatorial attributes by organized channel grouping. For
better convergence, features are quantized into discrete representations via
tuple code indexes. Experiments demonstrate that GDR consistently improves both
mainstream and state-of-the-art OCL methods across various datasets.
Visualizations further highlight GDR's superior object separability and
interpretability. The source code is available on
https://github.com/Genera1Z/GroupedDiscreteRepresentation.

</details>


### [251] [Community Forensics: Using Thousands of Generators to Train Fake Image Detectors](https://arxiv.org/pdf/2411.04125)
*Jeongsoo Park, Andrew Owens*

Main category: cs.CV

TL;DR: A new large and diverse dataset of AI-generated images is introduced to improve fake image detection, showing better generalization with increased model diversity and quantity.


<details>
  <summary>Details</summary>
Motivation: The limited diversity in training data hinders detection of AI-generated images from unseen models.

Method: Created a dataset of 2.7M images from 4803 models, including latent diffusion and commercial models, to study detector generalization.

Result: Detection improves with more and diverse models in training; trained detectors generalize better than others.

Conclusion: The dataset enhances fake image detection by addressing training data limitations, with promising generalization results.

Abstract: One of the key challenges of detecting AI-generated images is spotting images
that have been created by previously unseen generative models. We argue that
the limited diversity of the training data is a major obstacle to addressing
this problem, and we propose a new dataset that is significantly larger and
more diverse than prior work. As part of creating this dataset, we
systematically download thousands of text-to-image latent diffusion models and
sample images from them. We also collect images from dozens of popular open
source and commercial models. The resulting dataset contains 2.7M images that
have been sampled from 4803 different models. These images collectively capture
a wide range of scene content, generator architectures, and image processing
settings. Using this dataset, we study the generalization abilities of fake
image detectors. Our experiments suggest that detection performance improves as
the number of models in the training set increases, even when these models have
similar architectures. We also find that detection performance improves as the
diversity of the models increases, and that our trained detectors generalize
better than those trained on other datasets. The dataset can be found in
https://jespark.net/projects/2024/community_forensics

</details>


### [252] [Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual Conversion in Visual Reasoning for Multimodal Large Language Models](https://arxiv.org/pdf/2411.18142)
*Jingming Liu, Yumeng Li, Boyuan Xiao, Yichang Jian, Ziang Qin, Tianjia Shao, Yao-Xiang Ding, Kun Zhou*

Main category: cs.CV

TL;DR: MLLMs struggle with visual tasks like counting and puzzles due to perceptual bottlenecks. The proposed 'autonomous imagination' method decomposes visual-to-textual conversion into iterative visual modifications, enabling MLLMs to solve previously unsolvable tasks without retraining.


<details>
  <summary>Details</summary>
Motivation: MLLMs fail at straightforward visual tasks because their visual-to-textual conversion process lacks decomposition, leading to perceptual bottlenecks.

Method: Introduces 'autonomous imagination,' where MLLMs iteratively modify visual inputs (e.g., isolating objects, rearranging puzzle pieces) to decompose the conversion process into manageable steps.

Result: MLLMs can now solve tasks beyond their initial perceptual capability without retraining, demonstrating the effectiveness of closed-loop visual modification.

Conclusion: Closed-loop visual modification decomposes visual reasoning into solvable substeps, enhancing MLLMs' performance on complex visual tasks.

Abstract: Under pure textual modality, Large Language Models (LLMs) have demonstrated
remarkable success in complex reasoning tasks by decomposing them into simpler
sub-problems. However, Multimodal Large Language Models (MLLMs) still struggle
with some seemingly straightforward visual tasks, such as counting and solving
jigsaw puzzles. We argue that these tasks challenge the ability of
visual-to-textual conversion, where MLLMs convert visual information perceived
from the input scene, to textual information for further reasoning and
generating the answer. If the complexity of the visual input is beyond the
perceptual capability of the MLLMs, without decomposing this conversion
process, simply scaling inference-time reasoning cannot solve the task because
it repeatedly encounters the same perceptual bottleneck. We propose an
approach, autonomous imagination, to enable MLLMs to iteratively modify visual
inputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate
visual states, decomposing visual-to-textual conversion into closed-loop visual
modification steps. We show that, without any retraining, MLLMs can now solve
tasks initially beyond their perceptual capability, highlighting that
closed-loop visual modification can be an effective way of decomposing the
visual reasoning task into solvable substeps. Project page:
https://future-item.github.io/autoimagine-site/

</details>


### [253] [SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device](https://arxiv.org/pdf/2412.10494)
*Yushu Wu, Zhixing Zhang, Yanyu Li, Yanwu Xu, Anil Kag, Yang Sui, Huseyin Coskun, Ke Ma, Aleksei Lebedev, Ju Hu, Dimitris Metaxas, Yanzhi Wang, Sergey Tulyakov, Jian Ren*

Main category: cs.CV

TL;DR: A framework accelerates large-scale video diffusion models for edge devices, reducing computation and enabling faster, high-quality video generation.


<details>
  <summary>Details</summary>
Motivation: Current video generation models are computationally heavy and cloud-dependent, limiting accessibility for content creators.

Method: Proposes an efficient architecture from a compact image backbone, optimizes temporal layers, uses adversarial fine-tuning, and reduces denoising steps to 4.

Result: The model (0.6B parameters) generates a 5-second video on an iPhone 16 PM in 5 seconds, matching server-side quality.

Conclusion: The framework significantly accelerates video generation for edge users without compromising quality.

Abstract: We have witnessed the unprecedented success of diffusion-based video
generation over the past year. Recently proposed models from the community have
wielded the power to generate cinematic and high-resolution videos with smooth
motions from arbitrary input prompts. However, as a supertask of image
generation, video generation models require more computation and are thus
hosted mostly on cloud servers, limiting broader adoption among content
creators. In this work, we propose a comprehensive acceleration framework to
bring the power of the large-scale video diffusion model to the hands of edge
users. From the network architecture scope, we initialize from a compact image
backbone and search out the design and arrangement of temporal layers to
maximize hardware efficiency. In addition, we propose a dedicated adversarial
fine-tuning algorithm for our efficient model and reduce the denoising steps to
4. Our model, with only 0.6B parameters, can generate a 5-second video on an
iPhone 16 PM within 5 seconds. Compared to server-side models that take minutes
on powerful GPUs to generate a single video, we accelerate the generation by
magnitudes while delivering on-par quality.

</details>


### [254] [A Culturally-Aware Benchmark for Person Re-Identification in Modest Attire](https://arxiv.org/pdf/2412.18874)
*Alireza Sedighi Moghaddam, Fatemeh Anvari, Mohammadjavad Mirshekari Haghighi, Mohammadali Fakhari, Mohammad Reza Mohammadi*

Main category: cs.CV

TL;DR: The paper introduces IUST_PersonReId, a dataset for Person Re-Identification (ReID) tailored to Islamic regions like Iran, addressing the lack of diversity in existing datasets. It highlights performance drops in state-of-the-art models on this dataset, emphasizing challenges like occlusion and modest attire.


<details>
  <summary>Details</summary>
Motivation: Existing ReID datasets lack diversity, especially for regions with modest clothing like Iran, limiting model generalization. The paper aims to fill this gap with a culturally representative dataset.

Method: The authors introduce the IUST_PersonReId dataset, capturing diverse scenarios in Iran (markets, campuses, mosques). They evaluate state-of-the-art models (SOLIDER, CLIP-ReID) on this dataset and benchmark against Market1501 and MSMT17.

Result: Performance drops significantly on IUST_PersonReId: SOLIDER shows 50.75% and 23.01% mAP drops, while CLIP-ReID drops 38.09% and 21.74% mAP compared to benchmarks. Sequence-based evaluations improve results by leveraging temporal context.

Conclusion: IUST_PersonReId highlights the need for culturally sensitive ReID datasets and models. It advances fairness and bias research in ReID, offering a resource for robust, globally applicable systems.

Abstract: Person Re-Identification (ReID) is a fundamental task in computer vision with
critical applications in surveillance and security. Despite progress in recent
years, most existing ReID models often struggle to generalize across diverse
cultural contexts, particularly in Islamic regions like Iran, where modest
clothing styles are prevalent. Existing datasets predominantly feature Western
and East Asian fashion, limiting their applicability in these settings. To
address this gap, we introduce Iran University of Science and Technology Person
Re-Identification (IUST_PersonReId), a dataset designed to reflect the unique
challenges of ReID in new cultural environments, emphasizing modest attire and
diverse scenarios from Iran, including markets, campuses, and mosques.
Experiments on IUST_PersonReId with state-of-the-art models, such as Semantic
Controllable Self-supervised Learning (SOLIDER) and Contrastive Language-Image
Pretraining Re-Identification (CLIP-ReID), reveal significant performance drops
compared to benchmarks like Market1501 and Multi-Scene MultiTime (MSMT17),
specifically, SOLIDER shows a drop of 50.75% and 23.01% Mean Average Precision
(mAP) compared to Market1501 and MSMT17 respectively, while CLIP-ReID exhibits
a drop of 38.09% and 21.74% mAP, highlighting the challenges posed by occlusion
and limited distinctive features. Sequence-based evaluations show improvements
by leveraging temporal context, emphasizing the dataset's potential for
advancing culturally sensitive and robust ReID systems. IUST_PersonReId offers
a critical resource for addressing fairness and bias in ReID research globally.

</details>


### [255] [TinyLLaVA-Video: Towards Smaller LMMs for Video Understanding with Group Resampler](https://arxiv.org/pdf/2501.15513)
*Xingjian Zhang, Xi Weng, Yihao Yue, Zhaoxin Fan, Wenjun Wu, Lei Huang*

Main category: cs.CV

TL;DR: TinyLLaVA-Video is a lightweight video understanding model with 3.6B parameters, outperforming larger models while being efficient and accessible.


<details>
  <summary>Details</summary>
Motivation: Addressing the resource-intensive nature of large models and the challenges lightweight models face in processing long visual sequences.

Method: Introduces a video-level group resampler to reduce visual tokens and enhance temporal understanding.

Result: Surpasses 7B-parameter models on benchmarks and trains efficiently in one day on 8 GPUs.

Conclusion: Provides a foundation for lightweight video understanding research, with code and weights publicly available.

Abstract: Video behavior recognition and scene understanding are fundamental tasks in
multimodal intelligence, serving as critical building blocks for numerous
real-world applications. Through large multimodal models (LMMs) have achieved
remarkable progress in video understanding, most existing open-source models
rely on over 7B parameters and require large-scale datasets for training,
making them resource-intensive and inaccessible to many researchers.
Furthermore, lightweight models face persistent challenges in effectively
processing long visual sequences and temporal understanding. In this work, we
introduce TinyLLaVA-Video, a lightweight yet powerful video understanding model
with approximately 3.6B parameters. The cornerstone of our design is the
video-level group resampler, a novel mechanism that significantly reduces and
controls the number of visual tokens at the video level. Unlike traditional
image-level resampler, our approach effectively mitigates redundancy while
enhancing temporal comprehension, leading to improved performance on
video-based tasks. In addition, TinyLLaVA-Video demonstrates exceptional
efficiency, requiring only one day of training on 8 A100-40G GPUs. It surpasses
several existing 7B-parameter models on multiple benchmarks. We believe this
work provides a valuable foundation for future research on lightweight video
understanding models. The code and weights is available at
https://github.com/ZhangXJ199/TinyLLaVA-Video.

</details>


### [256] [Human-Aligned Image Models Improve Visual Decoding from the Brain](https://arxiv.org/pdf/2502.03081)
*Nona Rajabi, Antônio H. Ribeiro, Miguel Vasco, Farzaneh Taleb, Mårten Björkman, Danica Kragic*

Main category: cs.CV

TL;DR: Using human-aligned image encoders improves visual image decoding from brain activity by 21% over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Advance brain-computer interaction and deepen understanding of human perception by improving visual decoding from brain signals.

Method: Introduce human-aligned image encoders to map brain signals to images, hypothesizing better capture of perceptual attributes.

Result: Empirical results show up to 21% improvement in image retrieval accuracy across diverse setups.

Conclusion: Human-aligned encoders enhance visual decoding performance consistently across various conditions.

Abstract: Decoding visual images from brain activity has significant potential for
advancing brain-computer interaction and enhancing the understanding of human
perception. Recent approaches align the representation spaces of images and
brain activity to enable visual decoding. In this paper, we introduce the use
of human-aligned image encoders to map brain signals to images. We hypothesize
that these models more effectively capture perceptual attributes associated
with the rapid visual stimuli presentations commonly used in visual brain data
recording experiments. Our empirical results support this hypothesis,
demonstrating that this simple modification improves image retrieval accuracy
by up to 21% compared to state-of-the-art methods. Comprehensive experiments
confirm consistent performance improvements across diverse EEG architectures,
image encoders, alignment methods, participants, and brain imaging modalities

</details>


### [257] [TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation](https://arxiv.org/pdf/2502.07306)
*Navid Rajabi, Jana Kosecka*

Main category: cs.CV

TL;DR: A modular approach for Vision-Language Navigation (VLN) using LLMs and VLMs in zero-shot, outperforming joint semantic map methods.


<details>
  <summary>Details</summary>
Motivation: To improve VLN by decomposing the task into sub-modules leveraging LLMs and VLMs for better navigation performance.

Method: Decomposes VLN into four sub-modules: landmark extraction, path hypothesis generation, panorama-landmark alignment, and path evaluation using nDTW.

Result: Superior performance on R2R-Habitat dataset, highlighting the impact of visual grounding.

Conclusion: The modular approach effectively enhances VLN by leveraging LLMs and VLMs, outperforming existing methods.

Abstract: In this work, we propose a modular approach for the Vision-Language
Navigation (VLN) task by decomposing the problem into four sub-modules that use
state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs)
in a zero-shot setting. Given navigation instruction in natural language, we
first prompt LLM to extract the landmarks and the order in which they are
visited. Assuming the known model of the environment, we retrieve the top-k
locations of the last landmark and generate $k$ path hypotheses from the
starting location to the last landmark using the shortest path algorithm on the
topological map of the environment. Each path hypothesis is represented by a
sequence of panoramas. We then use dynamic programming to compute the alignment
score between the sequence of panoramas and the sequence of landmark names,
which match scores obtained from VLM. Finally, we compute the nDTW metric
between the hypothesis that yields the highest alignment score to evaluate the
path fidelity. We demonstrate superior performance compared to other approaches
that use joint semantic maps like VLMaps on the complex R2R-Habitat instruction
dataset and quantify in detail the effect of visual grounding on navigation
performance.

</details>


### [258] [MegaLoc: One Retrieval to Place Them All](https://arxiv.org/pdf/2502.17237)
*Gabriele Berton, Carlo Masone*

Main category: cs.CV

TL;DR: MegaLoc is a versatile retrieval model combining existing methods, achieving state-of-the-art results in Visual Place Recognition, Landmark Retrieval, and Visual Localization.


<details>
  <summary>Details</summary>
Motivation: Existing solutions are task-specific and fail under changing requirements or out-of-distribution data. MegaLoc aims to address this by being adaptable across multiple tasks.

Method: Combines various existing methods, training techniques, and datasets to train MegaLoc.

Result: Achieves state-of-the-art in Visual Place Recognition, performs well in Landmark Retrieval, and sets a new benchmark in Visual Localization on LaMAR datasets.

Conclusion: MegaLoc demonstrates strong adaptability and performance across multiple computer vision tasks, outperforming specialized solutions.

Abstract: Retrieving images from the same location as a given query is an important
component of multiple computer vision tasks, like Visual Place Recognition,
Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However,
existing solutions are built to specifically work for one of these tasks, and
are known to fail when the requirements slightly change or when they meet
out-of-distribution data. In this paper we combine a variety of existing
methods, training techniques, and datasets to train a retrieval model, called
MegaLoc, that is performant on multiple tasks. We find that MegaLoc (1)
achieves state of the art on a large number of Visual Place Recognition
datasets, (2) impressive results on common Landmark Retrieval datasets, and (3)
sets a new state of the art for Visual Localization on the LaMAR datasets,
where we only changed the retrieval method to the existing localization
pipeline. The code for MegaLoc is available at
https://github.com/gmberton/MegaLoc

</details>


### [259] [Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment](https://arxiv.org/pdf/2503.09081)
*Xiaowei Bi, Zheyuan Xu*

Main category: cs.CV

TL;DR: UMaT unifies visual and auditory inputs as structured text for LLMs, improving multi-modal learning by addressing semantic alignment, temporal sync, and sparse retrieval. Boosts Long Video QA accuracy by up to 16.9%.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal learning methods suffer from inconsistencies in representation and reasoning across modalities.

Method: Proposes UMaT, a framework converting visual and auditory inputs into structured text for LLMs, focusing on semantic alignment, temporal synchronization, and sparse retrieval.

Result: Significantly improves Long Video QA accuracy (up to 13.7% generally, 16.9% on long videos).

Conclusion: UMaT effectively unifies multi-modal inputs, enhancing reasoning and accuracy in tasks like Long Video QA.

Abstract: While multi-modal learning has advanced significantly, current approaches
often create inconsistencies in representation and reasoning of different
modalities. We propose UMaT, a theoretically-grounded framework that unifies
visual and auditory inputs as structured text for large language models,
addressing semantic alignment, temporal synchronization, and efficient sparse
information retrieval. It significantly improves state-of-the-art Long Video
Question Answering accuracy (up to 13.7%, and 16.9% on long videos) via
redundancy minimization and structured textual representation for unified
multi-modal reasoning

</details>


### [260] [Visualization of a multidimensional point cloud as a 3D swarm of avatars](https://arxiv.org/pdf/2504.06751)
*Leszek Luchowski, Dariusz Pojda*

Main category: cs.CV

TL;DR: The paper introduces a novel technique for visualizing multidimensional data using avatar-like icons inspired by Chernoff faces, combining projection methods with intuitive facial feature mapping.


<details>
  <summary>Details</summary>
Motivation: To leverage human ability to interpret facial traits for better understanding of complex datasets.

Method: Combines projection techniques with assigning data dimensions to avatar features, implemented as a plugin for dpVision.

Result: Enhances interpretability and aids analysis of complex data, as shown with synthetic and wine datasets.

Conclusion: The method effectively improves data visualization and interpretability for multidimensional datasets.

Abstract: This paper proposes an innovative technique for representing multidimensional
datasets using icons inspired by Chernoff faces. Our approach combines
classical projection techniques with the explicit assignment of selected data
dimensions to avatar (facial) features, leveraging the innate human ability to
interpret facial traits. We introduce a semantic division of data dimensions
into intuitive and technical categories, assigning the former to avatar
features and projecting the latter into a four-dimensional (or higher) spatial
embedding. The technique is implemented as a plugin for the open-source
dpVision visualization platform, enabling users to interactively explore data
in the form of a swarm of avatars whose spatial positions and visual features
jointly encode various aspects of the dataset. Experimental results with
synthetic test data and a 12-dimensional dataset of Portuguese Vinho Verde
wines demonstrate that the proposed method enhances interpretability and
facilitates the analysis of complex data structures.

</details>


### [261] [STeP: A Framework for Solving Scientific Video Inverse Problems with Spatiotemporal Diffusion Priors](https://arxiv.org/pdf/2504.07549)
*Bingliang Zhang, Zihui Wu, Berthy T. Feng, Yang Song, Yisong Yue, Katherine L. Bouman*

Main category: cs.CV

TL;DR: A plug-and-play framework with a learned spatiotemporal diffusion prior improves video reconstruction from sparse measurements, outperforming existing methods in tasks like black hole video reconstruction and dynamic MRI.


<details>
  <summary>Details</summary>
Motivation: The challenge of reconstructing coherent videos from sparse measurements in scientific domains, where existing methods fail due to high spatiotemporal uncertainty.

Method: Proposes a plug-and-play framework incorporating a learned spatiotemporal diffusion prior, enabling flexible application to various video inverse problems without task-specific designs.

Result: Achieves significantly improved spatiotemporal coherence in reconstructions compared to baseline methods, validated on black hole video reconstruction and dynamic MRI.

Conclusion: The framework effectively addresses spatiotemporal uncertainty in video reconstruction, offering a versatile and efficient solution for scientific tasks.

Abstract: Reconstructing spatially and temporally coherent videos from time-varying
measurements is a fundamental challenge in many scientific domains. A major
difficulty arises from the sparsity of measurements, which hinders accurate
recovery of temporal dynamics. Existing image diffusion-based methods rely on
extracting temporal consistency directly from measurements, limiting their
effectiveness on scientific tasks with high spatiotemporal uncertainty. We
address this difficulty by proposing a plug-and-play framework that
incorporates a learned spatiotemporal diffusion prior. Due to its plug-and-play
nature, our framework can be flexibly applied to different video inverse
problems without the need for task-specific design and temporal heuristics. We
further demonstrate that a spatiotemporal diffusion model can be trained
efficiently with limited video data. We validate our approach on two
challenging scientific video reconstruction tasks: black hole video
reconstruction and dynamic MRI. While baseline methods struggle to provide
temporally coherent reconstructions, our approach achieves significantly
improved recovery of the spatiotemporal structure of the underlying ground
truth videos.

</details>


### [262] [LMPOcc: 3D Semantic Occupancy Prediction Utilizing Long-Term Memory Prior from Historical Traversals](https://arxiv.org/pdf/2504.13596)
*Shanshuai Yuan, Julong Wei, Muer Tie, Xiangyun Ren, Zhongxue Gan, Wenchao Ding*

Main category: cs.CV

TL;DR: LMPOcc introduces a 3D occupancy prediction method leveraging long-term memory priors from historical traversals, improving performance on static semantic categories and enabling global occupancy modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore historical traversal data, missing opportunities to enhance 3D semantic occupancy prediction under varying conditions.

Method: LMPOcc integrates long-term memory priors via a plug-and-play architecture, using a Current-Prior Fusion module for adaptive feature aggregation and a model-agnostic prior format.

Result: Achieves state-of-the-art performance on Occ3D-nuScenes, excelling in static semantic categories and demonstrating multi-vehicle crowdsourcing potential.

Conclusion: LMPOcc effectively leverages historical data for superior 3D occupancy prediction, offering adaptability and global modeling capabilities.

Abstract: Vision-based 3D semantic occupancy prediction is critical for autonomous
driving, enabling unified modeling of static infrastructure and dynamic agents.
In practice, autonomous vehicles may repeatedly traverse identical geographic
locations under varying environmental conditions, such as weather fluctuations
and illumination changes. Existing methods in 3D occupancy prediction
predominantly integrate adjacent temporal contexts. However, these works
neglect to leverage perceptual information, which is acquired from historical
traversals of identical geographic locations. In this paper, we propose
Longterm Memory Prior Occupancy (LMPOcc), the first 3D occupancy prediction
methodology that exploits long-term memory priors derived from historical
traversal perceptual outputs. We introduce a plug-and-play architecture that
integrates long-term memory priors to enhance local perception while
simultaneously constructing global occupancy representations. To adaptively
aggregate prior features and current features, we develop an efficient
lightweight Current-Prior Fusion module. Moreover, we propose a model-agnostic
prior format to ensure compatibility across diverse occupancy prediction
baselines. LMPOcc achieves state-of-the-art performance validated on the
Occ3D-nuScenes benchmark, especially on static semantic categories.
Additionally, experimental results demonstrate LMPOcc's ability to construct
global occupancy through multi-vehicle crowdsourcing.

</details>


### [263] [Dense Geometry Supervision for Underwater Depth Estimation](https://arxiv.org/pdf/2504.18233)
*Wenxiang Gua, Lin Qia*

Main category: cs.CV

TL;DR: A novel method for monocular depth estimation in underwater scenes, using a cost-effective dataset and a texture-depth fusion module, improves accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Limited research and data for underwater monocular depth estimation, prompting a need for innovative solutions.

Method: Constructs an underwater dataset using multi-view depth estimation and introduces a texture-depth fusion module based on underwater optical principles.

Result: Significant improvement in accuracy and adaptability on the FLSea dataset.

Conclusion: Provides a cost-effective solution with practical potential for underwater depth estimation.

Abstract: The field of monocular depth estimation is continually evolving with the
advent of numerous innovative models and extensions. However, research on
monocular depth estimation methods specifically for underwater scenes remains
limited, compounded by a scarcity of relevant data and methodological support.
This paper proposes a novel approach to address the existing challenges in
current monocular depth estimation methods for underwater environments. We
construct an economically efficient dataset suitable for underwater scenarios
by employing multi-view depth estimation to generate supervisory signals and
corresponding enhanced underwater images. we introduces a texture-depth fusion
module, designed according to the underwater optical imaging principles, which
aims to effectively exploit and integrate depth information from texture cues.
Experimental results on the FLSea dataset demonstrate that our approach
significantly improves the accuracy and adaptability of models in underwater
settings. This work offers a cost-effective solution for monocular underwater
depth estimation and holds considerable promise for practical applications.

</details>


### [264] [SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning](https://arxiv.org/pdf/2504.20024)
*Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jianwen Xie, Alan Yuille*

Main category: cs.CV

TL;DR: SpatialReasoner, a new LVLM, improves 3D spatial reasoning using explicit 3D representations, outperforming Gemini 2.0 by 9.2% on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with 3D spatial reasoning, especially on human-trivial questions, due to implicit methods.

Method: Introduces SpatialReasoner with explicit 3D representations across perception, computation, and reasoning stages.

Result: Achieves better performance on benchmarks and generalizes well to novel questions.

Conclusion: Bridges 3D parsing and reasoning, opening new directions for 3D spatial reasoning.

Abstract: Despite recent advances on multi-modal models, 3D spatial reasoning remains a
challenging task for state-of-the-art open-source and proprietary models.
Recent studies explore data-driven approaches and achieve enhanced spatial
reasoning performance by fine-tuning models on 3D-related visual
question-answering data. However, these methods typically perform spatial
reasoning in an implicit manner and often fail on questions that are trivial to
humans, even with long chain-of-thought reasoning. In this work, we introduce
SpatialReasoner, a novel large vision-language model (LVLM) that addresses 3D
spatial reasoning with explicit 3D representations shared between multiple
stages--3D perception, computation, and reasoning. Explicit 3D representations
provide a coherent interface that supports advanced 3D spatial reasoning and
improves the generalization ability to novel question types. Furthermore, by
analyzing the explicit 3D representations in multi-step reasoning traces of
SpatialReasoner, we study the factual errors and identify key shortcomings of
current LVLMs. Results show that our SpatialReasoner achieves improved
performance on a variety of spatial reasoning benchmarks, outperforming Gemini
2.0 by 9.2% on 3DSRBench, and generalizes better when evaluating on novel 3D
spatial reasoning questions. Our study bridges the 3D parsing capabilities of
prior visual foundation models with the powerful reasoning abilities of large
language models, opening new directions for 3D spatial reasoning.

</details>


### [265] [SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models](https://arxiv.org/pdf/2505.00788)
*Wufei Ma, Luoxin Ye, Celso M de Melo, Jieneng Chen, Alan Yuille*

Main category: cs.CV

TL;DR: SpatialLLM is a new model enhancing 3D spatial reasoning in multimodal models by introducing 3D-informed data and architecture, outperforming GPT-4o by 8.7%.


<details>
  <summary>Details</summary>
Motivation: Current models lack 3D spatial reasoning due to limited 3D data and 2D biases. This paper aims to address this gap.

Method: Developed 3D-informed probing and conversation datasets, integrated with architectural and training designs for LMMs.

Result: SpatialLLM outperforms GPT-4o by 8.7% in 3D reasoning.

Conclusion: The approach provides a roadmap for future 3D reasoning research, advancing machine capabilities.

Abstract: Humans naturally understand 3D spatial relationships, enabling complex
reasoning like predicting collisions of vehicles from different directions.
Current large multimodal models (LMMs), however, lack of this capability of 3D
spatial reasoning. This limitation stems from the scarcity of 3D training data
and the bias in current model designs toward 2D data. In this paper, we
systematically study the impact of 3D-informed data, architecture, and training
setups, introducing SpatialLLM, a large multi-modal model with advanced 3D
spatial reasoning abilities. To address data limitations, we develop two types
of 3D-informed training datasets: (1) 3D-informed probing data focused on
object's 3D location and orientation, and (2) 3D-informed conversation data for
complex spatial relationships. Notably, we are the first to curate VQA data
that incorporate 3D orientation relationships on real images. Furthermore, we
systematically integrate these two types of training data with the
architectural and training designs of LMMs, providing a roadmap for optimal
design aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM
advances machines toward highly capable 3D-informed reasoning, surpassing
GPT-4o performance by 8.7%. Our systematic empirical design and the resulting
findings offer valuable insights for future research in this direction. Our
project page is available at:
https://3d-spatial-reasoning.github.io/spatial-llm/

</details>


### [266] [CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation](https://arxiv.org/pdf/2505.04481)
*Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou*

Main category: cs.CV

TL;DR: The paper introduces CAD-Llama, a framework to enhance LLMs for generating parametric 3D CAD models, addressing challenges like lack of pretraining on parametric sequences and 3D awareness.


<details>
  <summary>Details</summary>
Motivation: To expand LLMs' generative capabilities into domain-specific areas like CAD, where parametric sequences directly influence 3D shapes.

Method: Develops a hierarchical annotation pipeline and SPCC format, followed by adaptive pretraining and CAD-specific instruction tuning.

Result: CAD-Llama outperforms prior autoregressive methods and LLM baselines in generating parametric CAD models.

Conclusion: The framework successfully equips LLMs with spatial knowledge for CAD, demonstrating significant improvements in generative performance.

Abstract: Recently, Large Language Models (LLMs) have achieved significant success,
prompting increased interest in expanding their generative capabilities beyond
general text into domain-specific areas. This study investigates the generation
of parametric sequences for computer-aided design (CAD) models using LLMs. This
endeavor represents an initial step towards creating parametric 3D shapes with
LLMs, as CAD model parameters directly correlate with shapes in
three-dimensional space. Despite the formidable generative capacities of LLMs,
this task remains challenging, as these models neither encounter parametric
sequences during their pretraining phase nor possess direct awareness of 3D
structures. To address this, we present CAD-Llama, a framework designed to
enhance pretrained LLMs for generating parametric 3D CAD models. Specifically,
we develop a hierarchical annotation pipeline and a code-like format to
translate parametric 3D CAD command sequences into Structured Parametric CAD
Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we
propose an adaptive pretraining approach utilizing SPCC, followed by an
instruction tuning process aligned with CAD-specific guidelines. This
methodology aims to equip LLMs with the spatial knowledge inherent in
parametric sequences. Experimental results demonstrate that our framework
significantly outperforms prior autoregressive methods and existing LLM
baselines.

</details>


### [267] [EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution](https://arxiv.org/pdf/2505.05209)
*Haizhen Xie, Kunpeng Du, Qiangyu Yan, Sen Lu, Jianhong Han, Hanting Chen, Hailin Hu, Jie Hu*

Main category: cs.CV

TL;DR: EAM introduces a novel BSR method using DiT, outperforming U-Net-based approaches with a new Ψ-DiT block and progressive Masked Image Modeling.


<details>
  <summary>Details</summary>
Motivation: To enhance Blind Super-Resolution (BSR) by leveraging Diffusion Transformers (DiT) and improving prior knowledge utilization from pre-trained T2I models.

Method: EAM employs Ψ-DiT for triple-flow architecture, progressive Masked Image Modeling for training efficiency, and subject-aware prompt generation for better prior utilization.

Result: EAM achieves state-of-the-art performance in BSR, surpassing existing methods in metrics and visual quality.

Conclusion: EAM demonstrates superior BSR performance by effectively integrating DiT and innovative training strategies.

Abstract: Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind
Super-Resolution (BSR) has become a predominant approach in the field. While
T2I models have traditionally relied on U-Net architectures, recent
advancements have demonstrated that Diffusion Transformers (DiT) achieve
significantly higher performance in this domain. In this work, we introduce
Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and
outperforms previous U-Net-based approaches. We introduce a novel block,
$\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This
block employs a low-resolution latent as a separable flow injection control,
forming a triple-flow architecture that effectively leverages the prior
knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance
capabilities of T2I models and enhance their generalization in BSR, we
introduce a progressive Masked Image Modeling strategy, which also reduces
training costs. Additionally, we propose a subject-aware prompt generation
strategy that employs a robust multi-modal model in an in-context learning
framework. This strategy automatically identifies key image areas, provides
detailed descriptions, and optimizes the utilization of T2I diffusion priors.
Our experiments demonstrate that EAM achieves state-of-the-art results across
multiple datasets, outperforming existing methods in both quantitative metrics
and visual quality.

</details>


### [268] [Zero-Shot Gaze-based Volumetric Medical Image Segmentation](https://arxiv.org/pdf/2505.15256)
*Tatyana Shmykova, Leila Khaertdinova, Ilya Pershin*

Main category: cs.CV

TL;DR: The paper introduces eye gaze as a novel input for interactive 3D medical image segmentation, comparing its efficiency and performance with traditional bounding box prompts in SAM-2 and MedSAM-2.


<details>
  <summary>Details</summary>
Motivation: Current interactive segmentation models rely on manual prompts, which can be time-consuming. Eye gaze offers a faster, complementary input method.

Method: The study evaluates gaze-based prompts using synthetic and real gaze data with SAM-2 and MedSAM-2, comparing them to bounding box prompts.

Result: Gaze-based prompts are more time-efficient but slightly less accurate than bounding boxes.

Conclusion: Eye gaze shows promise as a complementary modality for interactive 3D medical image segmentation.

Abstract: Accurate segmentation of anatomical structures in volumetric medical images
is crucial for clinical applications, including disease monitoring and cancer
treatment planning. Contemporary interactive segmentation models, such as
Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on
manually provided prompts like bounding boxes and mouse clicks. In this study,
we introduce eye gaze as a novel informational modality for interactive
segmentation, marking the application of eye-tracking for 3D medical image
segmentation. We evaluate the performance of using gaze-based prompts with
SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to
bounding boxes, gaze-based prompts offer a time-efficient interaction approach
with slightly lower segmentation quality. Our findings highlight the potential
of using gaze as a complementary input modality for interactive 3D medical
image segmentation.

</details>


### [269] [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/pdf/2505.17017)
*Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng*

Main category: cs.CV

TL;DR: The paper explores the use of RL algorithms (GRPO and DPO) in autoregressive image generation, analyzing their performance, generalization, and the role of reward models, while proposing scaling strategies.


<details>
  <summary>Details</summary>
Motivation: To address the lack of in-depth analysis of RL strategies and domain-specific challenges in autoregressive image generation, particularly for CoT reasoning.

Method: Comprehensive investigation of GRPO and DPO algorithms, evaluating in-domain performance, out-of-domain generalization, and the impact of reward models.

Result: GRPO and DPO have distinct advantages; reward models with strong generalization enhance RL algorithm performance. Scaling strategies improve proficiency.

Conclusion: The study provides insights for developing effective RL algorithms for robust CoT reasoning in autoregressive image generation, with code released for future work.

Abstract: Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT

</details>


### [270] [ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding](https://arxiv.org/pdf/2505.21381)
*Linshuang Diao, Dayong Ren, Sensen Song, Yurong Qian*

Main category: cs.CV

TL;DR: ZigzagPointMamba improves point cloud self-supervised learning by enhancing spatial continuity and local semantic modeling, outperforming existing methods in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing PointMamba-based methods disrupt spatial continuity and local semantic correlations due to complex token ordering and random masking.

Method: Proposes ZigzagPointMamba with a zigzag scan path for global sequencing and a Semantic-Siamese Masking Strategy (SMS) for better local semantic modeling.

Result: Achieves significant improvements: 1.59% mIoU gain on ShapeNetPart, 0.4% higher accuracy on ModelNet40, and better accuracies on ScanObjectNN subsets.

Conclusion: ZigzagPointMamba effectively addresses limitations of existing methods, enhancing performance in point cloud tasks.

Abstract: State Space models (SSMs) such as PointMamba enable efficient feature
extraction for point cloud self-supervised learning with linear complexity,
outperforming Transformers in computational efficiency. However, existing
PointMamba-based methods depend on complex token ordering and random masking,
which disrupt spatial continuity and local semantic correlations. We propose
ZigzagPointMamba to tackle these challenges. The core of our approach is a
simple zigzag scan path that globally sequences point cloud tokens, enhancing
spatial continuity by preserving the proximity of spatially adjacent point
tokens. Nevertheless, random masking undermines local semantic modeling in
self-supervised learning. To address this, we introduce a Semantic-Siamese
Masking Strategy (SMS), which masks semantically similar tokens to facilitate
reconstruction by integrating local features of original and similar tokens.
This overcomes the dependence on isolated local features and enables robust
global semantic modeling. Our pre-trained ZigzagPointMamba weights
significantly improve downstream tasks, achieving a 1.59% mIoU gain on
ShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for
classification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for
the classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of
ScanObjectNN.

</details>


### [271] [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/pdf/2505.22146)
*Guangfu Hao, Haojie Wen, Liangxuna Guo, Yang Chen, Yanchao Bi, Shan Yu*

Main category: cs.CV

TL;DR: A framework using low-dimensional attribute representations bridges visual tool perception and linguistic task understanding, achieving 74% accuracy in tool selection tasks.


<details>
  <summary>Details</summary>
Motivation: To model the complex cognitive ability of flexible tool selection, which distinguishes humans from other species, using computational methods.

Method: Uses visual encoders (ResNet or ViT) for tool images and fine-tuned language models (GPT-2, LLaMA, DeepSeek) for task descriptions, with a dataset (ToolNet) of 115 tools labeled with 13 attributes.

Result: Achieves 74% accuracy, outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), and approaching GPT-4o (73%) with fewer parameters.

Conclusion: Provides a parameter-efficient, interpretable solution mimicking human-like tool cognition, advancing cognitive science and practical tool selection applications.

Abstract: Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.

</details>


### [272] [ATI: Any Trajectory Instruction for Controllable Video Generation](https://arxiv.org/pdf/2505.22944)
*Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, Chongyang Ma*

Main category: cs.CV

TL;DR: A unified framework for motion control in video generation integrates camera movement, object translation, and local motion using trajectory inputs, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Prior methods handle motion types separately or with task-specific designs, lacking cohesion. This work aims for a unified, user-controllable solution.

Method: Projects user-defined trajectories into latent space of pre-trained models via a lightweight motion injector, guiding generative process for consistent motion.

Result: Superior performance in tasks like stylized motion, viewpoint changes, and local manipulation, with better controllability and visual quality.

Conclusion: The framework is broadly compatible with state-of-the-art video generation models and outperforms existing approaches.

Abstract: We propose a unified framework for motion control in video generation that
seamlessly integrates camera movement, object-level translation, and
fine-grained local motion using trajectory-based inputs. In contrast to prior
methods that address these motion types through separate modules or
task-specific designs, our approach offers a cohesive solution by projecting
user-defined trajectories into the latent space of pre-trained image-to-video
generation models via a lightweight motion injector. Users can specify
keypoints and their motion paths to control localized deformations, entire
object motion, virtual camera dynamics, or combinations of these. The injected
trajectory signals guide the generative process to produce temporally
consistent and semantically aligned motion sequences. Our framework
demonstrates superior performance across multiple video motion control tasks,
including stylized motion effects (e.g., motion brushes), dynamic viewpoint
changes, and precise local motion manipulation. Experiments show that our
method provides significantly better controllability and visual quality
compared to prior approaches and commercial solutions, while remaining broadly
compatible with various state-of-the-art video generation backbones. Project
page: https://anytraj.github.io/.

</details>


### [273] [Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss](https://arxiv.org/pdf/2505.23463)
*Han Zhou, Sebastian G. Gruber, Teodora Popordanoska, Matthew B. Blaschko*

Main category: cs.CV

TL;DR: The paper analyzes reweighted risk functionals (e.g., focal loss, inverse focal loss, AURC) and links them to calibration errors. It shows that optimizing a regularized AURC improves calibration, aligning with inverse focal loss's strategy, and introduces a differentiable AURC for gradient-based optimization.


<details>
  <summary>Details</summary>
Motivation: To establish a principled connection between reweighting schemes (like focal loss and AURC) and calibration errors, addressing inconsistencies in existing claims.

Method: The paper revisits weighted risk functions, links calibration error minimization to selective classification, and introduces a differentiable regularized AURC using SoftRank for optimization.

Result: Empirical evaluations show the AURC-based loss achieves competitive class-wise calibration across datasets and architectures.

Conclusion: Optimizing a regularized AURC improves calibration, supporting inverse focal loss's strategy over focal loss, and offers flexibility through choice of confidence score functions.

Abstract: Several variants of reweighted risk functionals, such as focal losss, inverse
focal loss, and the Area Under the Risk-Coverage Curve (AURC), have been
proposed in the literature and claims have been made in relation to their
calibration properties. However, focal loss and inverse focal loss propose
vastly different weighting schemes. In this paper, we revisit a broad class of
weighted risk functions commonly used in deep learning and establish a
principled connection between these reweighting schemes and calibration errors.
We show that minimizing calibration error is closely linked to the selective
classification paradigm and demonstrate that optimizing a regularized variant
of the AURC naturally leads to improved calibration. This regularized AURC
shares a similar reweighting strategy with inverse focal loss, lending support
to the idea that focal loss is less principled when calibration is a desired
outcome. Direct AURC optimization offers greater flexibility through the choice
of confidence score functions (CSFs). To enable gradient-based optimization, we
introduce a differentiable formulation of the regularized AURC using the
SoftRank technique. Empirical evaluations demonstrate that our AURC-based loss
achieves competitive class-wise calibration performance across a range of
datasets and model architectures.

</details>


### [274] [Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs](https://arxiv.org/pdf/2506.01064)
*Yudong Zhang, Ruobing Xie, Yiqing Huang, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Di Wang, Yu Wang*

Main category: cs.CV

TL;DR: F3 is a novel adversarial purification framework for LVLMs that uses noise to counteract adversarial attacks, improving robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: LVLMs are vulnerable to visual adversarial attacks, but existing purification methods are underdeveloped. F3 aims to address this gap.

Method: F3 introduces simple perturbations to adversarial examples, leveraging cross-modal attentions from perturbed examples to refine attention and purify outputs.

Result: F3 effectively mitigates adversarial effects, is training-free, computationally efficient, and suitable for large-scale applications.

Conclusion: F3 provides a practical and efficient solution for adversarial purification in LVLMs, with potential for industrial use.

Abstract: Recent advances in large vision-language models (LVLMs) have showcased their
remarkable capabilities across a wide range of multimodal vision-language
tasks. However, these models remain vulnerable to visual adversarial attacks,
which can substantially compromise their performance. Despite their potential
impact, the development of effective methods for purifying such adversarial
examples has received relatively limited attention. In this paper, we introduce
F3, a novel adversarial purification framework that employs a counterintuitive
"fighting fire with fire" strategy: intentionally introducing simple
perturbations to adversarial examples to mitigate their harmful effects.
Specifically, F3 leverages cross-modal attentions derived from randomly
perturbed adversary examples as reference targets. By injecting noise into
these adversarial examples, F3 effectively refines their attention, resulting
in cleaner and more reliable model outputs. Remarkably, this seemingly
paradoxical approach of employing noise to counteract adversarial attacks
yields impressive purification results. Furthermore, F3 offers several distinct
advantages: it is training-free and straightforward to implement, and exhibits
significant computational efficiency improvements compared to existing
purification methods. These attributes render F3 particularly suitable for
large-scale industrial applications where both robust performance and
operational efficiency are critical priorities. The code will be made publicly
available.

</details>


### [275] [RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation](https://arxiv.org/pdf/2506.06733)
*Ruoxuan Zhang, Jidong Gao, Bin Wen, Hongxia Xie, Chenming Zhang, Hong-Han Shuai, Wen-Huang Cheng*

Main category: cs.CV

TL;DR: RecipeGen is a large-scale benchmark for recipe-based text-to-image, image-to-video, and text-to-video generation, addressing the lack of fine-grained alignment in existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack fine-grained alignment between recipe goals, instructions, and visuals, limiting applications in culinary education and recipe assistants.

Method: Introduces RecipeGen, a dataset with 26,453 recipes, 196,724 images, and 4,491 videos, and proposes domain-specific evaluation metrics.

Result: RecipeGen benchmarks T2I, I2V, and T2V models, providing insights for future recipe generation models.

Conclusion: RecipeGen fills a gap in food computing by offering a comprehensive benchmark and evaluation framework for recipe-based generation tasks.

Abstract: Creating recipe images is a key challenge in food computing, with
applications in culinary education and multimodal recipe assistants. However,
existing datasets lack fine-grained alignment between recipe goals, step-wise
instructions, and visual content. We present RecipeGen, the first large-scale,
real-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video
(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,
196,724 images, and 4,491 videos, covering diverse ingredients, cooking
procedures, styles, and dish types. We further propose domain-specific
evaluation metrics to assess ingredient fidelity and interaction modeling,
benchmark representative T2I, I2V, and T2V models, and provide insights for
future recipe generation models. Project page is available now.

</details>


### [276] [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](https://arxiv.org/pdf/2506.07280)
*Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro*

Main category: cs.CV

TL;DR: Video Diffusion Models (VDMs) are repurposed for diverse tasks using few-shot fine-tuning, demonstrating adaptability beyond video generation.


<details>
  <summary>Details</summary>
Motivation: To explore the internal knowledge of VDMs and their potential as adaptable visual learners for broader applications.

Method: A few-shot fine-tuning framework transforms tasks into visual transitions, training LoRA weights on short sequences without modifying the frozen VDM.

Result: Strong generalization across tasks like segmentation, pose estimation, and high-level reasoning (e.g., ARC-AGI).

Conclusion: VDMs are versatile visual learners, suitable as backbone for future vision foundation models.

Abstract: Video Diffusion Models (VDMs) have emerged as powerful generative tools,
capable of synthesizing high-quality spatiotemporal content. Yet, their
potential goes far beyond mere video generation. We argue that the training
dynamics of VDMs, driven by the need to model coherent sequences, naturally
pushes them to internalize structured representations and an implicit
understanding of the visual world. To probe the extent of this internal
knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs
for new tasks using only a handful of examples. Our method transforms each task
into a visual transition, enabling the training of LoRA weights on short
input-output sequences without altering the generative interface of a frozen
VDM. Despite minimal supervision, the model exhibits strong generalization
across diverse tasks, from low-level vision (for example, segmentation and pose
estimation) to high-level reasoning (for example, on ARC-AGI). These results
reframe VDMs as more than generative engines. They are adaptable visual
learners with the potential to serve as the backbone for future foundation
models in vision.

</details>


### [277] [CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/pdf/2506.07327)
*Dane Williamson, Yangfeng Ji, Matthew Dwyer*

Main category: cs.CV

TL;DR: The paper critiques saliency methods for lacking class sensitivity and introduces CASE, a contrastive method for more reliable explanations.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability of saliency methods in distinguishing between class labels, which undermines their trustworthiness.

Method: Proposes a diagnostic test for class sensitivity and introduces CASE, a contrastive explanation method.

Result: Many saliency methods fail to distinguish class labels; CASE outperforms them in fidelity and class specificity.

Conclusion: CASE provides more faithful and class-specific explanations, addressing a structural limitation of existing methods.

Abstract: Saliency methods are widely used to visualize which input features are deemed
relevant to a model's prediction. However, their visual plausibility can
obscure critical limitations. In this work, we propose a diagnostic test for
class sensitivity: a method's ability to distinguish between competing class
labels on the same input. Through extensive experiments, we show that many
widely used saliency methods produce nearly identical explanations regardless
of the class label, calling into question their reliability. We find that
class-insensitive behavior persists across architectures and datasets,
suggesting the failure mode is structural rather than model-specific. Motivated
by these findings, we introduce CASE, a contrastive explanation method that
isolates features uniquely discriminative for the predicted class. We evaluate
CASE using the proposed diagnostic and a perturbation-based fidelity test, and
show that it produces faithful and more class-specific explanations than
existing methods.

</details>


### [278] [ARGUS: Hallucination and Omission Evaluation in Video-LLMs](https://arxiv.org/pdf/2506.07371)
*Ruchit Rawal, Reza Shirkavand, Heng Huang, Gowthami Somepalli, Tom Goldstein*

Main category: cs.CV

TL;DR: ARGUS is a benchmark for VideoLLMs to measure freeform video captioning performance, focusing on hallucination rates and omission of details.


<details>
  <summary>Details</summary>
Motivation: VideoLLMs hallucinate aggressively in freeform tasks like captioning, unlike multiple-choice tasks, necessitating a better benchmark.

Method: ARGUS compares VideoLLM outputs to human ground truth captions, quantifying hallucination (incorrect statements) and omission (missing details).

Result: The benchmark provides dual metrics: hallucination rate and omission rate, offering a comprehensive view of captioning performance.

Conclusion: ARGUS addresses VideoLLMs' weaknesses by evaluating freeform captioning, improving assessment beyond traditional benchmarks.

Abstract: Video large language models have not yet been widely deployed, largely due to
their tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on
multiple-choice questions. Unfortunately, VideoLLMs hallucinate far more
aggressively on freeform text generation tasks like video captioning than they
do on multiple choice verification tasks. To address this weakness, we propose
ARGUS, a VideoLLM benchmark that measures freeform video captioning
performance. By comparing VideoLLM outputs to human ground truth captions,
ARGUS quantifies dual metrics. First, we measure the rate of hallucinations in
the form of incorrect statements about video content or temporal relationships.
Second, we measure the rate at which the model omits important descriptive
details. Together, these dual metrics form a comprehensive view of video
captioning performance.

</details>


### [279] [ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models](https://arxiv.org/pdf/2506.07739)
*Jing Zhong, Jun Yin, Peilin Li, Pengyu Zeng, Miao Zang, Ran Luo, Shuai Lu*

Main category: cs.CV

TL;DR: The paper introduces ArchDiffBench, a dataset of architectural images, and ArchiLense, a framework for automated style analysis, achieving high accuracy and consistency with expert annotations.


<details>
  <summary>Details</summary>
Motivation: Traditional architectural studies rely on subjective expert interpretations and suffer from regional biases. This study aims to provide an objective, automated solution for analyzing architectural styles.

Method: The study constructs the ArchDiffBench dataset and develops ArchiLense, a framework using Vision-Language Models and machine learning for style recognition and classification.

Result: ArchiLense achieves 92.4% consistency with expert annotations and 84.5% classification accuracy, demonstrating strong performance in style recognition.

Conclusion: The proposed approach offers an objective, accurate alternative to traditional methods, enhancing comparative studies of architectural culture.

Abstract: Architectural cultures across regions are characterized by stylistic
diversity, shaped by historical, social, and technological contexts in addition
to geograph-ical conditions. Understanding architectural styles requires the
ability to describe and analyze the stylistic features of different architects
from various regions through visual observations of architectural imagery.
However, traditional studies of architectural culture have largely relied on
subjective expert interpretations and historical literature reviews, often
suffering from regional biases and limited ex-planatory scope. To address these
challenges, this study proposes three core contributions: (1) We construct a
professional architectural style dataset named ArchDiffBench, which comprises
1,765 high-quality architectural images and their corresponding style
annotations, collected from different regions and historical periods. (2) We
propose ArchiLense, an analytical framework grounded in Vision-Language Models
and constructed using the ArchDiffBench dataset. By integrating ad-vanced
computer vision techniques, deep learning, and machine learning algo-rithms,
ArchiLense enables automatic recognition, comparison, and precise
classi-fication of architectural imagery, producing descriptive language
outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show
that ArchiLense achieves strong performance in architectural style recognition,
with a 92.4% con-sistency rate with expert annotations and 84.5% classification
accuracy, effec-tively capturing stylistic distinctions across images. The
proposed approach transcends the subjectivity inherent in traditional analyses
and offers a more objective and accurate perspective for comparative studies of
architectural culture.

</details>


### [280] [OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation](https://arxiv.org/pdf/2506.07977)
*Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, Hai-Bao Chen*

Main category: cs.CV

TL;DR: OneIG-Bench is a comprehensive benchmark for evaluating text-to-image models across multiple dimensions like alignment, reasoning, and stylization, addressing gaps in existing evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack comprehensive evaluation of T2I models, especially in reasoning and stylization, despite advancements in model capabilities.

Method: Introduces OneIG-Bench, a framework for fine-grained evaluation across dimensions like prompt-image alignment, text rendering, reasoning, and diversity.

Result: The benchmark enables flexible, in-depth analysis of model performance, helping identify strengths and bottlenecks in image generation.

Conclusion: OneIG-Bench facilitates reproducible evaluation and cross-model comparisons, advancing T2I research.

Abstract: Text-to-image (T2I) models have garnered significant attention for generating
high-quality images aligned with text prompts. However, rapid T2I model
advancements reveal limitations in early benchmarks, lacking comprehensive
evaluations, for example, the evaluation on reasoning, text rendering and
style. Notably, recent state-of-the-art models, with their rich knowledge
modeling capabilities, show promising results on the image generation problems
requiring strong reasoning ability, yet existing evaluation systems have not
adequately addressed this frontier. To systematically address these gaps, we
introduce OneIG-Bench, a meticulously designed comprehensive benchmark
framework for fine-grained evaluation of T2I models across multiple dimensions,
including prompt-image alignment, text rendering precision, reasoning-generated
content, stylization, and diversity. By structuring the evaluation, this
benchmark enables in-depth analysis of model performance, helping researchers
and practitioners pinpoint strengths and bottlenecks in the full pipeline of
image generation. Specifically, OneIG-Bench enables flexible evaluation by
allowing users to focus on a particular evaluation subset. Instead of
generating images for the entire set of prompts, users can generate images only
for the prompts associated with the selected dimension and complete the
corresponding evaluation accordingly. Our codebase and dataset are now publicly
available to facilitate reproducible evaluation studies and cross-model
comparisons within the T2I research community.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [281] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/pdf/2506.08026)
*Xibai Wang*

Main category: cs.AI

TL;DR: TIP-Search is a dynamic deep learning model selection framework for real-time market prediction, ensuring accuracy and deadline compliance under uncertain workloads.


<details>
  <summary>Details</summary>
Motivation: Addresses strict latency demands in high-frequency financial systems by dynamically selecting models to meet deadlines and maximize accuracy.

Method: Profiles latency and performance offline, then performs online task-aware model selection without domain labels.

Result: Outperforms static baselines with 8.5% higher accuracy and 100% deadline satisfaction on real-world datasets.

Conclusion: TIP-Search effectively enables robust, low-latency financial inference under uncertainty.

Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling
framework for real-time market prediction under uncertain workloads. Motivated
by the strict latency demands in high-frequency financial systems, TIP-Search
dynamically selects a deep learning model from a heterogeneous pool, aiming to
maximize predictive accuracy while satisfying per-task deadline constraints.
Our approach profiles latency and generalization performance offline, then
performs online task-aware selection without relying on explicit input domain
labels. We evaluate TIP-Search on three real-world limit order book datasets
(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms
static baselines with up to 8.5% improvement in accuracy and 100% deadline
satisfaction. Our results highlight the effectiveness of TIP-Search in robust
low-latency financial inference under uncertainty.

</details>


### [282] [Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph](https://arxiv.org/pdf/2506.08098)
*Akash Vishwakarma, Hojin Lee, Mohith Suresh, Priyam Shankar Sharma, Rahul Vishwakarma, Sparsh Gupta, Yuvraj Anupam Chauhan*

Main category: cs.AI

TL;DR: Cognitive Weave is a novel memory framework for LLM-based agents, using a multi-layered spatio-temporal resonance graph to enhance learning, reasoning, and adaptation, outperforming existing methods by 34% in task completion and 42% in query latency.


<details>
  <summary>Details</summary>
Motivation: Current memory systems lack structural flexibility, temporal awareness, and higher-level insight synthesis, limiting LLM-based agents' capabilities.

Method: Introduces Cognitive Weave with a spatio-temporal resonance graph (STRG) managing insight particles (IPs) and a semantic oracle interface (SOI) for dynamic enrichment, interconnected by relational strands. Includes cognitive refinement for higher-level knowledge synthesis.

Result: Achieves 34% better task completion and 42% lower query latency than baselines in planning, QA, and dialogue tasks.

Conclusion: Cognitive Weave advances LLM memory systems, with ethical considerations and future research directions outlined.

Abstract: The emergence of capable large language model (LLM) based agents necessitates
memory architectures that transcend mere data storage, enabling continuous
learning, nuanced reasoning, and dynamic adaptation. Current memory systems
often grapple with fundamental limitations in structural flexibility, temporal
awareness, and the ability to synthesize higher-level insights from raw
interaction data. This paper introduces Cognitive Weave, a novel memory
framework centered around a multi-layered spatio-temporal resonance graph
(STRG). This graph manages information as semantically rich insight particles
(IPs), which are dynamically enriched with resonance keys, signifiers, and
situational imprints via a dedicated semantic oracle interface (SOI). These IPs
are interconnected through typed relational strands, forming an evolving
knowledge tapestry. A key component of Cognitive Weave is the cognitive
refinement process, an autonomous mechanism that includes the synthesis of
insight aggregates (IAs) condensed, higher-level knowledge structures derived
from identified clusters of related IPs. We present comprehensive experimental
results demonstrating Cognitive Weave's marked enhancement over existing
approaches in long-horizon planning tasks, evolving question-answering
scenarios, and multi-session dialogue coherence. The system achieves a notable
34% average improvement in task completion rates and a 42% reduction in mean
query latency when compared to state-of-the-art baselines. Furthermore, this
paper explores the ethical considerations inherent in such advanced memory
systems, discusses the implications for long-term memory in LLMs, and outlines
promising future research trajectories.

</details>


### [283] [SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents](https://arxiv.org/pdf/2506.08119)
*Subhrangshu Nandi, Arghya Datta, Nikhil Vichare, Indranil Bhattacharya, Huzefa Raja, Jing Xu, Shayan Ray, Giuseppe Carenini, Abhi Srivastava, Aaron Chan, Man Ho Woo, Amar Kandola, Brandon Theresa, Francesco Carbone*

Main category: cs.AI

TL;DR: The paper introduces SOP-Bench, a benchmark for evaluating LLM-based agents on complex industrial SOPs, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: Address the lack of benchmarks for LLMs handling complex, domain-specific SOPs in industrial automation.

Method: Develop a synthetic data generation framework for realistic SOPs and create SOP-Bench with 1,800 tasks across 10 domains. Evaluate Function-Calling and ReAct Agents.

Result: Agents achieved low success rates (27% and 48%) and struggled with incorrect tool usage in larger registries.

Conclusion: Current LLM agents fall short in automating real-world SOPs, emphasizing the need for domain-specific benchmarking and architectural improvements.

Abstract: Large Language Models (LLMs) demonstrate impressive general-purpose reasoning
and problem-solving abilities. However, they struggle with executing complex,
long-horizon workflows that demand strict adherence to Standard Operating
Procedures (SOPs), a critical requirement for real-world industrial automation.
Despite this need, there is a lack of public benchmarks that reflect the
complexity, structure, and domain-specific nuances of SOPs. To address this, we
present three main contributions. First, we introduce a synthetic data
generation framework to create realistic, industry-grade SOPs that rigorously
test the planning, reasoning, and tool-use capabilities of LLM-based agents.
Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800
tasks across 10 industrial domains, each with APIs, tool interfaces, and
human-validated test cases. Third, we evaluate two prominent agent
architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing
average success rates of only 27% and 48%, respectively. Remarkably, when the
tool registry is much larger than necessary, agents invoke incorrect tools
nearly 100% of the time. These findings underscore a substantial gap between
current agentic capabilities of LLMs and the demands of automating real-world
SOPs. Performance varies significantly by task and domain, highlighting the
need for domain-specific benchmarking and architectural choices before
deployment. SOP-Bench is publicly available at
http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the
prompts underpinning the data generation framework to support new
domain-specific SOP benchmarks. We invite the community to extend SOP-Bench
with SOPs from their industrial domains.

</details>


### [284] [FREIDA: A Framework for developing quantitative agent based models based on qualitative expert knowledge](https://arxiv.org/pdf/2308.00505)
*Frederike Oetker, Vittorio Nespeca, Rick Quax*

Main category: cs.AI

TL;DR: FREIDA is a mixed-methods framework integrating qualitative and quantitative data for ABMs, using Expected System Behaviors (ESBs) for calibration and validation.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of systematic approaches to incorporate qualitative and quantitative data in ABMs, especially in data-sparse contexts.

Method: Proposes FREIDA, which extracts ESBs from qualitative data as testable statements (CS for calibration, VS for validation) for quantitative scoring.

Result: Enables qualitative insights to inform model specification, parameterization, and validation, demonstrated via a case study on criminal cocaine networks.

Conclusion: FREIDA bridges the gap between qualitative and quantitative data in ABMs, enhancing their realism and applicability.

Abstract: Agent Based Models (ABMs) often deal with systems where there is a lack of
quantitative data or where quantitative data alone may be insufficient to fully
capture the complexities of real-world systems. Expert knowledge and
qualitative insights, such as those obtained through interviews, ethnographic
research, historical accounts, or participatory workshops, are critical in
constructing realistic behavioral rules, interactions, and decision-making
processes within these models. However, there is a lack of systematic
approaches that are able to incorporate both qualitative and quantitative data
across the entire modeling cycle. To address this, we propose FREIDA (FRamework
for Expert-Informed Data-driven Agent-based models), a systematic mixed-methods
framework to develop, train, and validate ABMs, particularly in data-sparse
contexts. Our main technical innovation is to extract what we call Expected
System Behaviors (ESBs) from qualitative data, which are testable statements
that can be evaluated on model simulations. Divided into Calibration Statements
(CS) for model calibration and Validation Statements (VS) for model validation,
they provide a quantitative scoring mechanism on the same footing as
quantitative data. In this way, qualitative insights can inform not only model
specification but also its parameterization and assessment of fitness for
purpose, which is a long standing challenge. We illustrate the application of
FREIDA through a case study of criminal cocaine networks in the Netherlands.

</details>


### [285] [The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](https://arxiv.org/pdf/2506.08134)
*Qiyao Wei, Samuel Holt, Jing Yang, Markus Wulfmeier, Mihaela van der Schaar*

Main category: cs.AI

TL;DR: The paper advocates for AI-assisted peer review in ML to address the crisis of scale, proposing LLMs as collaborators to enhance review quality without replacing human judgment.


<details>
  <summary>Details</summary>
Motivation: The exponential growth in ML manuscript submissions is overwhelming reviewers, risking review quality and consistency. AI assistance is seen as a solution to maintain integrity and scalability.

Method: Proposes an AI-augmented ecosystem using LLMs for factual verification, reviewer guidance, author assistance, and AC decision support, requiring structured review data.

Result: Outlines a research agenda for developing AI assistants, including experiments, while addressing technical and ethical challenges.

Conclusion: Calls for proactive development of AI-assisted peer review to uphold scientific validation standards and scalability in ML.

Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML),
is strained by a crisis of scale. Exponential growth in manuscript submissions
to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite
capacity of qualified reviewers, leading to concerns about review quality,
consistency, and reviewer fatigue. This position paper argues that AI-assisted
peer review must become an urgent research and infrastructure priority. We
advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language
Models (LLMs) not as replacements for human judgment, but as sophisticated
collaborators for authors, reviewers, and Area Chairs (ACs). We propose
specific roles for AI in enhancing factual verification, guiding reviewer
performance, assisting authors in quality improvement, and supporting ACs in
decision-making. Crucially, we contend that the development of such systems
hinges on access to more granular, structured, and ethically-sourced peer
review process data. We outline a research agenda, including illustrative
experiments, to develop and validate these AI assistants, and discuss
significant technical and ethical challenges. We call upon the ML community to
proactively build this AI-assisted future, ensuring the continued integrity and
scalability of scientific validation, while maintaining high standards of peer
review.

</details>


### [286] [Compiling Metric Temporal Answer Set Programming](https://arxiv.org/pdf/2506.08150)
*Arvid Becker, Pedro Cabalar, Martin Diéguez, Javier Romero, Susana Hahn, Torsten Schaub*

Main category: cs.AI

TL;DR: A computational approach for Metric ASP with quantitative temporal constraints, addressing scalability via difference constraints.


<details>
  <summary>Details</summary>
Motivation: To handle fine-grained timing constraints in ASP without exacerbating grounding bottlenecks.

Method: Extends ASP with difference constraints to manage time-related aspects externally.

Result: Decouples metric ASP from time granularity, ensuring scalability regardless of time precision.

Conclusion: The approach successfully maintains scalability while handling quantitative temporal constraints.

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to
allow for expressing quantitative temporal constrains, like durations and
deadlines. A central challenge is to maintain scalability when dealing with
fine-grained timing constraints, which can significantly exacerbate ASP's
grounding bottleneck. To address this issue, we leverage extensions of ASP with
difference constraints, a simplified form of linear constraints, to handle
time-related aspects externally. Our approach effectively decouples metric ASP
from the granularity of time, resulting in a solution that is unaffected by
time precision.

</details>


### [287] [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://arxiv.org/pdf/2506.08306)
*Tuan Truong, Rithwik Sudharsan, Yibo Yang, Peter Xiangyuan Ma, Ruihan Yang, Stephan Mandt, Joshua S. Bloom*

Main category: cs.AI

TL;DR: AstroCompress introduces neural lossless compression for astrophysics data, outperforming traditional methods and enhancing observatory data collection.


<details>
  <summary>Details</summary>
Motivation: Remote observatories face data transmission bottlenecks; improved compression can unlock billions in additional science.

Method: AstroCompress benchmarks seven methods (three neural, four non-neural) on diverse astrophysics datasets.

Result: Neural compression outperforms classical techniques, improving data collection efficiency.

Conclusion: Neural compression shows promise for observatories, with potential for future lossy methods.

Abstract: The site conditions that make astronomical observatories in space and on the
ground so desirable -- cold and dark -- demand a physical remoteness that leads
to limited data transmission capabilities. Such transmission limitations
directly bottleneck the amount of data acquired and in an era of costly modern
observatories, any improvements in lossless data compression has the potential
scale to billions of dollars worth of additional science that can be
accomplished on the same instrument. Traditional lossless methods for
compressing astrophysical data are manually designed. Neural data compression,
on the other hand, holds the promise of learning compression algorithms
end-to-end from data and outperforming classical techniques by leveraging the
unique spatial, temporal, and wavelength structures of astronomical images.
This paper introduces AstroCompress: a neural compression challenge for
astrophysics data, featuring four new datasets (and one legacy dataset) with
16-bit unsigned integer imaging data in various modes: space-based,
ground-based, multi-wavelength, and time-series imaging. We provide code to
easily access the data and benchmark seven lossless compression methods (three
neural and four non-neural, including all practical state-of-the-art
algorithms). Our results on lossless compression indicate that lossless neural
compression techniques can enhance data collection at observatories, and
provide guidance on the adoption of neural compression in scientific
applications. Though the scope of this paper is restricted to lossless
compression, we also comment on the potential exploration of lossy compression
methods in future studies.

</details>


### [288] [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/pdf/2506.08321)
*Manooshree Patel, Rayna Bhattacharyya, Thomas Lu, Arnav Mehta, Niels Voss, Narges Norouzi, Gireeja Ranade*

Main category: cs.AI

TL;DR: LeanTutor is an LLM-based tutoring system for math proofs, featuring autoformalization, next-step generation, and natural language feedback. It was evaluated using PeanoBench, showing promising results in formalization and error identification.


<details>
  <summary>Details</summary>
Motivation: To enhance math proof tutoring by leveraging LLMs for formal verification, step generation, and pedagogically sound feedback.

Method: LeanTutor uses three modules: autoformalizer/proof-checker, next-step generator, and feedback generator, evaluated on the PeanoBench dataset.

Result: Autoformalizer achieved 57% accuracy in correct proofs and 30% in error identification. LeanTutor outperformed baselines in hint generation.

Conclusion: LeanTutor demonstrates effective LLM-based tutoring for math proofs, with room for improvement in formalization and error detection.

Abstract: We present LeanTutor, a Large Language Model (LLM)-based tutoring system for
math proofs. LeanTutor interacts with the student in natural language, formally
verifies student-written math proofs in Lean, generates correct next steps, and
provides the appropriate instructional guidance. LeanTutor is composed of three
modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and
(iii) a natural language feedback generator. The first module faithfully
autoformalizes student proofs into Lean and verifies proof accuracy via
successful code compilation. If the proof has an error, the incorrect step is
identified. The next-step generator module outputs a valid next Lean tactic for
incorrect proofs via LLM-based candidate generation and proof search. The
feedback generator module leverages Lean data to produce a
pedagogically-motivated natural language hint for the student user. To evaluate
our system, we introduce PeanoBench, a human-written dataset derived from the
Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each
natural language proof step is paired with the corresponding logically
equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of
tactics in correct proofs and accurately identifies the incorrect step in 30%
of incorrect proofs. In generating natural language hints for erroneous proofs,
LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

</details>


### [289] [ORFS-agent: Tool-Using Agents for Chip Design Optimization](https://arxiv.org/pdf/2506.08332)
*Amur Ghose, Andrew B. Kahng, Sayak Kundu, Zhiang Wang*

Main category: cs.AI

TL;DR: ORFS-agent, an LLM-based iterative optimization agent, improves parameter tuning in hardware design flows, outperforming Bayesian optimization with 13% better design metrics and 40% fewer iterations.


<details>
  <summary>Details</summary>
Motivation: Machine learning can optimize complex engineering workflows, like IC design, where parameter tuning impacts performance, power, and area. LLMs offer new opportunities for high-dimensional optimization.

Method: ORFS-agent, an LLM-based agent, adaptively explores parameter configurations in hardware design flows, leveraging natural language objectives for multi-objective optimization.

Result: ORFS-agent improves routed wirelength and clock period by over 13% and uses 40% fewer iterations than Bayesian optimization. It’s flexible, interpretable, and model-agnostic.

Conclusion: ORFS-agent provides an efficient, modular, and interpretable solution for multi-objective optimization in hardware design, adaptable to any frontier LLM without fine-tuning.

Abstract: Machine learning has been widely used to optimize complex engineering
workflows across numerous domains. In the context of integrated circuit design,
modern flows (e.g., going from a register-transfer level netlist to physical
layouts) involve extensive configuration via thousands of parameters, and small
changes to these parameters can have large downstream impacts on desired
outcomes - namely design performance, power, and area. Recent advances in Large
Language Models (LLMs) offer new opportunities for learning and reasoning
within such high-dimensional optimization tasks. In this work, we introduce
ORFS-agent, an LLM-based iterative optimization agent that automates parameter
tuning in an open-source hardware design flow. ORFS-agent adaptively explores
parameter configurations, demonstrating clear improvements over standard
Bayesian optimization approaches in terms of resource efficiency and final
design metrics. Our empirical evaluations on two different technology nodes and
a range of circuit benchmarks indicate that ORFS-agent can improve both routed
wirelength and effective clock period by over 13%, all while using 40% fewer
optimization iterations. Moreover, by following natural language objectives to
trade off certain metrics for others, ORFS-agent demonstrates a flexible and
interpretable framework for multi-objective optimization. Crucially, RFS-agent
is modular and model-agnostic, and can be plugged in to any frontier LLM
without any further fine-tuning.

</details>


### [290] [FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs](https://arxiv.org/pdf/2506.08363)
*Jun Yin, Jing Zhong, Pengyu Zeng, Peilin Li, Miao Zhang, Ran Luo, Shuai Lu*

Main category: cs.AI

TL;DR: FloorplanMAE is a self-supervised learning framework that predicts complete floorplans from partial ones, improving design efficiency.


<details>
  <summary>Details</summary>
Motivation: Floorplan design is iterative; predicting complete plans from partial ones can save time and reduce repetitive work.

Method: Uses Masked Autoencoders (MAE) and a lightweight Vision Transformer (ViT) to reconstruct missing parts of floorplans, trained on FloorplanNet dataset.

Result: Generates high-quality complete floorplans from incomplete ones, outperforming benchmarks.

Conclusion: FloorplanMAE offers a scalable solution for floorplan generation with broad applications.

Abstract: In the architectural design process, floorplan design is often a dynamic and
iterative process. Architects progressively draw various parts of the floorplan
according to their ideas and requirements, continuously adjusting and refining
throughout the design process. Therefore, the ability to predict a complete
floorplan from a partial one holds significant value in the design process.
Such prediction can help architects quickly generate preliminary designs,
improve design efficiency, and reduce the workload associated with repeated
modifications. To address this need, we propose FloorplanMAE, a self-supervised
learning framework for restoring incomplete floor plans into complete ones.
First, we developed a floor plan reconstruction dataset, FloorplanNet,
specifically trained on architectural floor plans. Secondly, we propose a floor
plan reconstruction method based on Masked Autoencoders (MAE), which
reconstructs missing parts by masking sections of the floor plan and training a
lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy
of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,
we validated the model using real sketches from the early stages of
architectural design. Experimental results show that the FloorplanMAE model can
generate high-quality complete floor plans from incomplete partial plans. This
framework provides a scalable solution for floor plan generation, with broad
application prospects.

</details>


### [291] [On Reasoning Strength Planning in Large Reasoning Models](https://arxiv.org/pdf/2506.08390)
*Leheng Sheng, An Zhang, Zijian Wu, Weixiang Zhao, Changshuo Shen, Yi Zhang, Xiang Wang, Tat-Seng Chua*

Main category: cs.AI

TL;DR: Large reasoning models (LRMs) pre-plan reasoning strength via activations, controlled by a directional vector's magnitude, influencing performance and token count.


<details>
  <summary>Details</summary>
Motivation: To understand the unexplored mechanism behind LRMs' automatic reasoning strength allocation for varying problem difficulties.

Method: Analyze model activations, use linear probes to predict reasoning tokens, and manipulate a pre-allocated directional vector to observe its impact.

Result: LRMs encode reasoning strength in activations via a directional vector; modifying it affects token count and performance.

Conclusion: The study reveals LRMs' internal reasoning mechanisms and offers tools for controlling reasoning behaviors, with practical applications.

Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can
automatically allocate more reasoning strengths (i.e., the number of reasoning
tokens) for harder problems, exhibiting difficulty-awareness for better task
performance. While this automatic reasoning strength allocation phenomenon has
been widely observed, its underlying mechanism remains largely unexplored. To
this end, we provide explanations for this phenomenon from the perspective of
model activations. We find evidence that LRMs pre-plan the reasoning strengths
in their activations even before generation, with this reasoning strength
causally controlled by the magnitude of a pre-allocated directional vector.
Specifically, we show that the number of reasoning tokens is predictable solely
based on the question activations using linear probes, indicating that LRMs
estimate the required reasoning strength in advance. We then uncover that LRMs
encode this reasoning strength through a pre-allocated directional vector
embedded in the activations of the model, where the vector's magnitude
modulates the reasoning strength. Subtracting this vector can lead to reduced
reasoning token number and performance, while adding this vector can lead to
increased reasoning token number and even improved performance. We further
reveal that this direction vector consistently yields positive reasoning length
prediction, and it modifies the logits of end-of-reasoning token </think> to
affect the reasoning length. Finally, we demonstrate two potential applications
of our findings: overthinking behavior detection and enabling efficient
reasoning on simple problems. Our work provides new insights into the internal
mechanisms of reasoning in LRMs and offers practical tools for controlling
their reasoning behaviors. Our code is available at
https://github.com/AlphaLab-USTC/LRM-plans-CoT.

</details>


### [292] [The Many Challenges of Human-Like Agents in Virtual Game Environments](https://arxiv.org/pdf/2505.20011)
*Maciej Swiechowski, Dominik Slezak*

Main category: cs.AI

TL;DR: The paper explores human-like AI in games, addressing challenges in implementation and measuring human likeness. It includes a survey of 13 challenges and an empirical study using machine learning to distinguish humans from bots.


<details>
  <summary>Details</summary>
Motivation: Enhancing gaming immersion and interaction with believable AI, plus the need to identify bots in digital interactions.

Method: Survey of challenges in human-like AI and an empirical study using a deep recurrent convolutional neural network to differentiate humans from bots.

Result: The study suggests that harder-to-create human-like AI makes it easier to distinguish humans from bots.

Conclusion: The paper provides insights into human-like AI challenges and demonstrates a method for bot detection, linking AI complexity to detection ease.

Abstract: Human-like agents are an increasingly important topic in games and beyond.
Believable non-player characters enhance the gaming experience by improving
immersion and providing entertainment. They also offer players the opportunity
to engage with AI entities that can function as opponents, teachers, or
cooperating partners. Additionally, in games where bots are prohibited -- and
even more so in non-game environments -- there is a need for methods capable of
identifying whether digital interactions occur with bots or humans. This leads
to two fundamental research questions: (1) how to model and implement
human-like AI, and (2) how to measure its degree of human likeness.
  This article offers two contributions. The first one is a survey of the most
significant challenges in implementing human-like AI in games (or any virtual
environment featuring simulated agents, although this article specifically
focuses on games). Thirteen such challenges, both conceptual and technical, are
discussed in detail. The second is an empirical study performed in a tactical
video game that addresses the research question: "Is it possible to distinguish
human players from bots (AI agents) based on empirical data?" A
machine-learning approach using a custom deep recurrent convolutional neural
network is presented. We hypothesize that the more challenging it is to create
human-like AI for a given game, the easier it becomes to develop a method for
distinguishing humans from AI-driven players.

</details>


### [293] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/pdf/2506.08399)
*Jiachen Ma, Zhanhui Zhou, Chao Yang, Chaochao Lu*

Main category: cs.AI

TL;DR: SafeCoT improves refusal behavior in VLMs using rule-based CoT supervision, reducing overrefusal and enhancing generalization with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ensuring safe and appropriate responses from VLMs in high-risk or ambiguous scenarios.

Method: Introduces SafeCoT, a lightweight framework leveraging rule-based chain-of-thought supervision for context-aware refusals.

Result: Significantly reduces overrefusal and enhances generalization across benchmarks, even with limited data.

Conclusion: SafeCoT provides a scalable solution for aligning VLMs with safety-critical objectives.

Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [294] [Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems](https://arxiv.org/pdf/2506.08401)
*Runze Li, Di Jin, Xiaobao Wang, Dongxiao He, Bingdao Feng, Zhen Wang*

Main category: cs.AI

TL;DR: A novel graph backdoor attack method is proposed to covertly expose target items to users by inserting a single fake node, minimizing impact on system performance.


<details>
  <summary>Details</summary>
Motivation: Address vulnerabilities in graph recommendation systems caused by low stealth and high destructiveness of existing shilling attacks.

Method: Design a single-node trigger generator to expose target items with constraints to limit impact on unrelated nodes.

Result: Target items exposed to 50% of users in 99% of cases, with system performance impact under 5%.

Conclusion: The method effectively enhances target item exposure covertly while maintaining system performance.

Abstract: Graph recommendation systems have been widely studied due to their ability to
effectively capture the complex interactions between users and items. However,
these systems also exhibit certain vulnerabilities when faced with attacks. The
prevailing shilling attack methods typically manipulate recommendation results
by injecting a large number of fake nodes and edges. However, such attack
strategies face two primary challenges: low stealth and high destructiveness.
To address these challenges, this paper proposes a novel graph backdoor attack
method that aims to enhance the exposure of target items to the target user in
a covert manner, without affecting other unrelated nodes. Specifically, we
design a single-node trigger generator, which can effectively expose multiple
target items to the target user by inserting only one fake user node.
Additionally, we introduce constraint conditions between the target nodes and
irrelevant nodes to mitigate the impact of fake nodes on the recommendation
system's performance. Experimental results show that the exposure of the target
items reaches no less than 50% in 99% of the target users, while the impact on
the recommendation system's performance is controlled within approximately 5%.

</details>


### [295] [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/pdf/2506.08422)
*Ikkei Itoku, David Theil, Evelyn Eichelsdoerfer Uehara, Sreyoshi Bhaduri, Junnosuke Kuroda, Toshi Yumoto, Alex Gil, Natalie Perez, Rajesh Cherukuri, Naumaan Nayyar*

Main category: cs.AI

TL;DR: A novel framework combining LLMs, expert calibration, and iterative prompt optimization automates taxonomy alignment, achieving high accuracy (F1-score 0.97) and outperforming human benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional manual and automated taxonomy alignment methods are costly, time-consuming, or lack nuance. A scalable, high-quality solution is needed.

Method: Integrates LLMs with expert-labeled examples, multi-stage prompt engineering, and human validation to automate taxonomy alignment and generate rationales.

Result: Achieved an F1-score of 0.97, significantly surpassing the human benchmark of 0.68 in a domain-specific mapping task.

Conclusion: The framework effectively scales taxonomy alignment, ensuring high-quality mappings and expert oversight for ambiguous cases.

Abstract: Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.

</details>


### [296] [SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy](https://arxiv.org/pdf/2506.08424)
*Yong Liang Goh, Zhiguang Cao, Yining Ma, Jianan Zhou, Mohammad Haroon Dupty, Wee Sun Lee*

Main category: cs.AI

TL;DR: SHIELD introduces a novel model for Multi-Task Multi-Distribution VRP (MTMDVRP), leveraging sparsity and hierarchy to improve generalization on unseen tasks and distributions.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models for routing problems overlook complex real-world customer distributions, prompting the need for a more realistic setting (MTMDVRP).

Method: SHIELD uses a deeper decoder with Mixture-of-Depths (MoD) for sparsity and a context-based clustering layer for hierarchy, improving task/distribution-specific and shared representations.

Result: Empirical results show SHIELD outperforms existing methods on 9 real-world maps with 16 VRP variants each.

Conclusion: SHIELD's sparsity and hierarchy principles significantly enhance generalization, making it superior for MTMDVRP.

Abstract: Recent advances toward foundation models for routing problems have shown
great potential of a unified deep model for various VRP variants. However, they
overlook the complex real-world customer distributions. In this work, we
advance the Multi-Task VRP (MTVRP) setting to the more realistic yet
challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce
SHIELD, a novel model that leverages both sparsity and hierarchy principles.
Building on a deeper decoder architecture, we first incorporate the
Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both
efficiency and generalization by allowing the model to dynamically select nodes
to use or skip each decoder layer, providing the needed capacity to adaptively
allocate computation for learning the task/distribution specific and shared
representations. We also develop a context-based clustering layer that exploits
the presence of hierarchical structures in the problems to produce better local
representations. These two designs inductively bias the network to identify key
features that are common across tasks and distributions, leading to
significantly improved generalization on unseen ones. Our empirical results
demonstrate the superiority of our approach over existing methods on 9
real-world maps with 16 VRP variants each.

</details>


### [297] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/pdf/2506.08446)
*Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Yi-Di Wang, Shu Yan, Cheng-Xing Jia, Xu-Hui Liu, Xin-Wei Chen, Jia-Cheng Xu, Ziniu Li, Yang Yu*

Main category: cs.AI

TL;DR: The survey explores the development of mathematical reasoning in large language models (LLMs), covering comprehension and answer generation phases, and reviews methods like prompting and fine-tuning. Challenges remain, and future directions are suggested.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance the mathematical reasoning capabilities of LLMs, addressing fundamental challenges and exploring new research directions.

Method: Review of methods including training-free prompting, supervised fine-tuning, reinforcement learning, and advanced techniques like Chain-of-Thought (CoT) reasoning.

Result: Notable progress in mathematical reasoning, but challenges in capacity, efficiency, and generalization persist.

Conclusion: Promising future directions include advanced pretraining, formal reasoning frameworks, and meta-generalization to further improve LLMs' reasoning abilities.

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [298] [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/pdf/2506.08462)
*Christos Margadji, Sebastian W. Pattinson*

Main category: cs.AI

TL;DR: CIPHER is a hybrid AI framework for industrial control, combining vision-language-action models with expert knowledge for precise, adaptable, and explainable automation.


<details>
  <summary>Details</summary>
Motivation: Industrial processes need robust, adaptable control systems, but current AI lacks generalization and precision.

Method: CIPHER integrates a process expert, regression model, and retrieval-augmented generation for physics-informed reasoning.

Result: It generalizes well to out-of-distribution tasks, interprets inputs, explains decisions, and generates precise instructions without annotations.

Conclusion: CIPHER enables autonomous, precise, and transparent industrial systems, supporting safe deployment.

Abstract: Industrial processes must be robust and adaptable, as environments and tasks
are often unpredictable, while operational errors remain costly and difficult
to detect. AI-based control systems offer a path forward, yet typically depend
on supervised learning with extensive labelled datasets, which limits their
ability to generalize across variable and data-scarce industrial settings.
Foundation models could enable broader reasoning and knowledge integration, but
rarely deliver the quantitative precision demanded by engineering applications.
Here, we introduceControl and Interpretation of Production via Hybrid Expertise
and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming
to replicate human-like reasoning for industrial control, instantiated in a
commercial-grade 3D printer. It integrates a process expert, a regression model
enabling quantitative characterization of system states required for
engineering tasks. CIPHER also incorporates retrieval-augmented generation to
access external expert knowledge and support physics-informed, chain-of-thought
reasoning. This hybrid architecture exhibits strong generalization to
out-of-distribution tasks. It interprets visual or textual inputs from process
monitoring, explains its decisions, and autonomously generates precise machine
instructions, without requiring explicit annotations. CIPHER thus lays the
foundations for autonomous systems that act with precision, reason with
context, and communicate decisions transparently, supporting safe and trusted
deployment in industrial settings.

</details>


### [299] [RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being](https://arxiv.org/pdf/2506.08486)
*Rahatara Ferdousi, M Anwar Hossain*

Main category: cs.AI

TL;DR: RHealthTwin is a framework for responsible AI-powered digital twins in healthcare, addressing hallucination, bias, and ethical concerns with a dynamic prompt engine (RPE) and achieving high performance and compliance.


<details>
  <summary>Details</summary>
Motivation: To mitigate risks like hallucination and bias in LLM-based healthcare applications, aligning with WHO recommendations for ethical AI use.

Method: Proposes RHealthTwin with a Responsible Prompt Engine (RPE) to structure inputs dynamically, ensuring context-aware, fair, and explainable responses. Includes a feedback loop for continuous improvement.

Result: Achieves state-of-the-art results (BLEU=0.41, ROUGE-L=0.63, BERTScore=0.89) and over 90% ethical compliance, outperforming baselines.

Conclusion: RHealthTwin provides a responsible foundation for LLM-based healthcare applications, ensuring safety and reliability.

Abstract: The rise of large language models (LLMs) has created new possibilities for
digital twins in healthcare. However, the deployment of such systems in
consumer health contexts raises significant concerns related to hallucination,
bias, lack of transparency, and ethical misuse. In response to recommendations
from health authorities such as the World Health Organization (WHO), we propose
Responsible Health Twin (RHealthTwin), a principled framework for building and
governing AI-powered digital twins for well-being assistance. RHealthTwin
processes multimodal inputs that guide a health-focused LLM to produce safe,
relevant, and explainable responses. At the core of RHealthTwin is the
Responsible Prompt Engine (RPE), which addresses the limitations of traditional
LLM configuration. Conventionally, users input unstructured prompt and the
system instruction to configure the LLM, which increases the risk of
hallucination. In contrast, RPE extracts predefined slots dynamically to
structure both inputs. This guides the language model to generate responses
that are context aware, personalized, fair, reliable, and explainable for
well-being assistance. The framework further adapts over time through a
feedback loop that updates the prompt structure based on user satisfaction. We
evaluate RHealthTwin across four consumer health domains including mental
support, symptom triage, nutrition planning, and activity coaching. RPE
achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and
BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical
compliance and instruction-following metrics using LLM-as-judge evaluation,
outperforming baseline strategies. We envision RHealthTwin as a forward-looking
foundation for responsible LLM-based applications in health and well-being.

</details>


### [300] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/pdf/2506.08518)
*Sunny Gupta, Nikita Jangid, Shounak Das, Amit Sethi*

Main category: cs.AI

TL;DR: FedTAIL is a federated domain generalization framework addressing class imbalance and conflicting objectives through sharpness-guided, gradient-aligned optimization.


<details>
  <summary>Details</summary>
Motivation: Existing DG methods struggle with long-tailed class distributions and conflicting optimization goals.

Method: FedTAIL uses gradient coherence regularization, class-wise sharpness minimization, and curvature-aware dynamic weighting. It also integrates sharpness-aware perturbations for conditional alignment.

Result: FedTAIL achieves state-of-the-art performance on DG benchmarks, especially under domain shifts and label imbalance.

Conclusion: FedTAIL effectively unifies optimization harmonization, class-aware regularization, and conditional alignment, proving robust in both centralized and federated settings.

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [301] [Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness](https://arxiv.org/pdf/2506.08532)
*Yanwei Gong, Xiaolin Chang*

Main category: cs.AI

TL;DR: A novel UAV trajectory planning framework combining DRL and LLM reasoning improves efficiency and compliance in complex urban environments.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of the low-altitude economy and UAV adoption highlights the need for efficient, compliant trajectory planning, which existing methods often overlook.

Method: Proposes a framework integrating deep reinforcement learning (DRL) with large language model (LLM) reasoning for enhanced UAV path planning.

Result: Outperforms baselines in data collection, collision avoidance, landing success, compliance, and energy efficiency.

Conclusion: The framework effectively addresses UAV trajectory planning challenges in low-altitude economy contexts.

Abstract: The rapid growth of the low-altitude economy has driven the widespread
adoption of unmanned aerial vehicles (UAVs). This growing deployment presents
new challenges for UAV trajectory planning in complex urban environments.
However, existing studies often overlook key factors, such as urban airspace
constraints and economic efficiency, which are essential in low-altitude
economy contexts. Deep reinforcement learning (DRL) is regarded as a promising
solution to these issues, while its practical adoption remains limited by low
learning efficiency. To overcome this limitation, we propose a novel UAV
trajectory planning framework that combines DRL with large language model (LLM)
reasoning to enable safe, compliant, and economically viable path planning.
Experimental results demonstrate that our method significantly outperforms
existing baselines across multiple metrics, including data collection rate,
collision avoidance, successful landing, regulatory compliance, and energy
efficiency. These results validate the effectiveness of our approach in
addressing UAV trajectory planning key challenges under constraints of the
low-altitude economy networking.

</details>


### [302] [HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning](https://arxiv.org/pdf/2506.08580)
*Yang Lv, Jinlong Lei, Peng Yi*

Main category: cs.AI

TL;DR: HGformer, a hierarchical graph Transformer framework, improves resource allocation in two-stage Colonel Blotto games by combining graph Transformer encoders and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with sequential dependencies and graph constraints in adversarial resource allocation.

Method: HGformer uses a graph Transformer encoder and a two-agent hierarchical decision model, enhanced by a feedback reinforcement learning algorithm.

Result: HGformer outperforms existing methods in resource allocation efficiency and adversarial payoff.

Conclusion: HGformer is effective for complex dynamic game scenarios, offering superior performance.

Abstract: Two-stage Colonel Blotto game represents a typical adversarial resource
allocation problem, in which two opposing agents sequentially allocate
resources in a network topology across two phases: an initial resource
deployment followed by multiple rounds of dynamic reallocation adjustments. The
sequential dependency between game stages and the complex constraints imposed
by the graph topology make it difficult for traditional approaches to attain a
globally optimal strategy. To address these challenges, we propose a
hierarchical graph Transformer framework called HGformer. By incorporating an
enhanced graph Transformer encoder with structural biases and a two-agent
hierarchical decision model, our approach enables efficient policy generation
in large-scale adversarial environments. Moreover, we design a layer-by-layer
feedback reinforcement learning algorithm that feeds the long-term returns from
lower-level decisions back into the optimization of the higher-level strategy,
thus bridging the coordination gap between the two decision-making stages.
Experimental results demonstrate that, compared to existing hierarchical
decision-making or graph neural network methods, HGformer significantly
improves resource allocation efficiency and adversarial payoff, achieving
superior overall performance in complex dynamic game scenarios.

</details>


### [303] [FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings](https://arxiv.org/pdf/2506.08627)
*Douwe Geurtjens, Xixi Lu*

Main category: cs.AI

TL;DR: The paper introduces FoldA, a technique for partial-order alignments using Petri net unfoldings to address state space explosion and concurrency representation issues in conformance checking.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods face state space explosion and fail to represent concurrent behavior accurately.

Method: Proposes FoldA, a technique using directed Petri net unfoldings for partial-order alignments.

Result: FoldA reduces queued states and better represents concurrency, though it requires more computation time.

Conclusion: FoldA improves conformance checking by addressing limitations of sequential alignment methods.

Abstract: Conformance checking is a fundamental task of process mining, which
quantifies the extent to which the observed process executions match a
normative process model. The state-of-the-art approaches compute alignments by
exploring the state space formed by the synchronous product of the process
model and the trace. This often leads to state space explosion, particularly
when the model exhibits a high degree of choice and concurrency. Moreover, as
alignments inherently impose a sequential structure, they fail to fully
represent the concurrent behavior present in many real-world processes. To
address these limitations, this paper proposes a new technique for computing
partial-order alignments {on the fly using directed Petri net unfoldings, named
FoldA. We evaluate our technique on 485 synthetic model-log pairs and compare
it against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6
benchmark pairs. The results show that our unfolding alignment, although it
requires more computation time, generally reduces the number of queued states
and provides a more accurate representation of concurrency.

</details>


### [304] [Modular Recurrence in Contextual MDPs for Universal Morphology Control](https://arxiv.org/pdf/2506.08630)
*Laurens Engwegen, Daan Brinks, Wendelin Böhmer*

Main category: cs.AI

TL;DR: A modular recurrent architecture improves generalization for multi-robot control by inferring partially observable contextual information.


<details>
  <summary>Details</summary>
Motivation: Developing a universal controller for diverse robot morphologies to enhance computational and data efficiency.

Method: Implemented a modular recurrent architecture to infer contextual information through interactions, evaluated on MuJoCo robots.

Result: Substantial performance improvement on robots with unseen dynamics, kinematics, and topologies across four environments.

Conclusion: The approach effectively generalizes to new robots, addressing a key challenge in multi-robot control.

Abstract: A universal controller for any robot morphology would greatly improve
computational and data efficiency. By utilizing contextual information about
the properties of individual robots and exploiting their modular structure in
the architecture of deep reinforcement learning agents, steps have been made
towards multi-robot control. Generalization to new, unseen robots, however,
remains a challenge. In this paper we hypothesize that the relevant contextual
information is partially observable, but that it can be inferred through
interactions for better generalization to contexts that are not seen during
training. To this extent, we implement a modular recurrent architecture and
evaluate its generalization performance on a large set of MuJoCo robots. The
results show a substantial improved performance on robots with unseen dynamics,
kinematics, and topologies, in four different environments.

</details>


### [305] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/pdf/2506.08745)
*Kongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, Dacheng Tao*

Main category: cs.AI

TL;DR: A self-rewarding RL framework (CoVo) for LLM reasoning leverages consistency and volatility of intermediate states, outperforming supervised RL.


<details>
  <summary>Details</summary>
Motivation: To enable scalable RL for LLM reasoning without external supervision by exploiting consistent reasoning patterns.

Method: Introduces CoVo, a reward mechanism combining consistency, volatility, and curiosity for self-rewarding RL.

Result: CoVo matches or surpasses supervised RL on diverse reasoning benchmarks.

Conclusion: CoVo provides a scalable, unsupervised solution for enhancing LLM reasoning.

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [306] [A Sample Efficient Conditional Independence Test in the Presence of Discretization](https://arxiv.org/pdf/2506.08747)
*Boyang Sun, Yu Yao, Xinshuai Dong, Zongfang Liu, Tongliang Liu, Yumou Qiu, Kun Zhang*

Main category: cs.AI

TL;DR: A sample-efficient CI test avoids data binarization, using GMM and nodewise regression to infer latent variable relationships, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Discretized data can mislead CI tests; binarization causes information loss. This paper aims to improve CI testing without binarization.

Method: Proposes a CI test using GMM to address over-identifying restrictions, deriving a test statistic via nodewise regression.

Result: Theoretical and empirical results show the test's superiority and effectiveness across datasets.

Conclusion: The proposed CI test is efficient, avoids information loss, and outperforms binarization-based methods.

Abstract: In many real-world scenarios, interested variables are often represented as
discretized values due to measurement limitations. Applying Conditional
Independence (CI) tests directly to such discretized data, however, can lead to
incorrect conclusions. To address this, recent advancements have sought to
infer the correct CI relationship between the latent variables through
binarizing observed data. However, this process inevitably results in a loss of
information, which degrades the test's performance. Motivated by this, this
paper introduces a sample-efficient CI test that does not rely on the
binarization process. We find that the independence relationships of latent
continuous variables can be established by addressing an over-identifying
restriction problem with Generalized Method of Moments (GMM). Based on this
insight, we derive an appropriate test statistic and establish its asymptotic
distribution correctly reflecting CI by leveraging nodewise regression.
Theoretical findings and Empirical results across various datasets demonstrate
that the superiority and effectiveness of our proposed test. Our code
implementation is provided in https://github.com/boyangaaaaa/DCT

</details>


### [307] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/pdf/2506.08771)
*Yuni Susanti, Michael Färber*

Main category: cs.AI

TL;DR: A novel method integrates Knowledge Graphs (KGs) with Large Language Models (LLMs) to improve knowledge-based causal discovery, outperforming baselines by up to 44.4 points in F1 scores.


<details>
  <summary>Details</summary>
Motivation: Traditional causal discovery methods relying on observational data are unstable when using LLMs. The paper aims to enhance reliability by leveraging KGs.

Method: The approach identifies metapath-based subgraphs in KGs, refines them with Learning-to-Rank models, and integrates top-ranked subgraphs into zero-shot prompts for LLMs.

Result: The method achieves significant improvements, outperforming baselines by up to 44.4 points in F1 scores on biomedical and open-domain datasets.

Conclusion: Integrating KGs with LLMs enhances knowledge-based causal discovery, offering a more reliable alternative to traditional methods.

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [308] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/pdf/2506.08800)
*Irene Testini, José Hernández-Orallo, Lorenzo Pacchiardi*

Main category: cs.AI

TL;DR: The paper surveys the evaluation of LLM assistants and agents in data science, highlighting gaps in focus on data management, human-AI collaboration, and task transformation.


<details>
  <summary>Details</summary>
Motivation: To assess how LLMs are evaluated as assistants and agents in data science, identifying current limitations and overlooked areas.

Method: Survey of existing evaluations of LLM assistants and agents in data science.

Result: Identified gaps: narrow focus on goal-oriented tasks, lack of intermediate human-AI collaboration, and neglect of task transformation potential.

Conclusion: Calls for broader evaluation criteria, including data management, collaborative models, and transformative automation in LLM applications for data science.

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


### [309] [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/pdf/2506.08872)
*Nataliya Kosmyna, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, Pattie Maes*

Main category: cs.AI

TL;DR: The study examines the neural and behavioral effects of LLM-assisted essay writing, revealing cognitive costs like reduced brain connectivity and lower self-reported ownership, despite the convenience of LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand the cognitive and educational implications of using LLMs for writing tasks compared to traditional methods like search engines or unaided writing.

Method: Participants were divided into LLM, Search Engine, and Brain-only groups, with EEG and NLP used to measure cognitive load and essay quality. Some participants switched groups in a fourth session.

Result: LLM users showed weaker brain connectivity, lower essay ownership, and underperformance in neural, linguistic, and behavioral metrics compared to other groups.

Conclusion: LLM reliance may have long-term cognitive costs, raising concerns about its educational impact and highlighting the need for further research on AI's role in learning.

Abstract: This study explores the neural and behavioral consequences of LLM-assisted
essay writing. Participants were divided into three groups: LLM, Search Engine,
and Brain-only (no tools). Each completed three sessions under the same
condition. In a fourth session, LLM users were reassigned to Brain-only group
(LLM-to-Brain), and Brain-only users were reassigned to LLM condition
(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18
completing session 4. We used electroencephalography (EEG) to assess cognitive
load during essay writing, and analyzed essays using NLP, as well as scoring
essays with the help from human teachers and an AI judge. Across groups, NERs,
n-gram patterns, and topic ontology showed within-group homogeneity. EEG
revealed significant differences in brain connectivity: Brain-only participants
exhibited the strongest, most distributed networks; Search Engine users showed
moderate engagement; and LLM users displayed the weakest connectivity.
Cognitive activity scaled down in relation to external tool use. In session 4,
LLM-to-Brain participants showed reduced alpha and beta connectivity,
indicating under-engagement. Brain-to-LLM users exhibited higher memory recall
and activation of occipito-parietal and prefrontal areas, similar to Search
Engine users. Self-reported ownership of essays was the lowest in the LLM group
and the highest in the Brain-only group. LLM users also struggled to accurately
quote their own work. While LLMs offer immediate convenience, our findings
highlight potential cognitive costs. Over four months, LLM users consistently
underperformed at neural, linguistic, and behavioral levels. These results
raise concerns about the long-term educational implications of LLM reliance and
underscore the need for deeper inquiry into AI's role in learning.

</details>


### [310] [Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation](https://arxiv.org/pdf/2506.08898)
*Mingfeng Fan, Jianan Zhou, Yifeng Zhang, Yaoxin Wu, Jinbiao Chen, Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: POCCO is a plug-and-play framework for MOCOPs that adaptively selects model structures for subproblems and optimizes them using preference signals, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat all subproblems equally with a single model, limiting solution space exploration and performance.

Method: POCCO uses a conditional computation block to route subproblems to specialized architectures and a preference-driven optimization algorithm for pairwise preferences.

Result: POCCO significantly outperforms state-of-the-art methods on four MOCOP benchmarks, showing strong generalization.

Conclusion: POCCO's adaptive framework enhances performance in MOCOPs by leveraging specialized architectures and preference-driven optimization.

Abstract: Recent deep reinforcement learning methods have achieved remarkable success
in solving multi-objective combinatorial optimization problems (MOCOPs) by
decomposing them into multiple subproblems, each associated with a specific
weight vector. However, these methods typically treat all subproblems equally
and solve them using a single model, hindering the effective exploration of the
solution space and thus leading to suboptimal performance. To overcome the
limitation, we propose POCCO, a novel plug-and-play framework that enables
adaptive selection of model structures for subproblems, which are subsequently
optimized based on preference signals rather than explicit reward values.
Specifically, we design a conditional computation block that routes subproblems
to specialized neural architectures. Moreover, we propose a preference-driven
optimization algorithm that learns pairwise preferences between winning and
losing solutions. We evaluate the efficacy and versatility of POCCO by applying
it to two state-of-the-art neural methods for MOCOPs. Experimental results
across four classic MOCOP benchmarks demonstrate its significant superiority
and strong generalization.

</details>


### [311] [IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections](https://arxiv.org/pdf/2506.08957)
*Yash Ranjan, Rahul Sengupta, Anand Rangarajan, Sanjay Ranka*

Main category: cs.AI

TL;DR: The paper proposes a data-driven traffic simulator using deep generative models to mimic real-world driving behavior at intersections, introducing new evaluation metrics and a simulation-in-the-loop pipeline.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based traffic simulators fail to accurately mimic real-world driving behavior, especially at intersections, which are critical for safety and efficiency.

Method: The study uses deep generative modeling for trajectory prediction, introduces traffic engineering metrics, and develops a simulation-in-the-loop pipeline. A multi-headed self-attention-based model incorporating signal information is also proposed.

Result: The proposed model outperforms previous models on the new evaluation metrics.

Conclusion: The study demonstrates the feasibility of a data-driven simulator for traffic intersections, improving accuracy and relevance for traffic engineering.

Abstract: Traffic simulators are widely used to study the operational efficiency of
road infrastructure, but their rule-based approach limits their ability to
mimic real-world driving behavior. Traffic intersections are critical
components of the road infrastructure, both in terms of safety risk (nearly 28%
of fatal crashes and 58% of nonfatal crashes happen at intersections) as well
as the operational efficiency of a road corridor. This raises an important
question: can we create a data-driven simulator that can mimic the macro- and
micro-statistics of the driving behavior at a traffic intersection? Deep
Generative Modeling-based trajectory prediction models provide a good starting
point to model the complex dynamics of vehicles at an intersection. But they
are not tested in a "live" micro-simulation scenario and are not evaluated on
traffic engineering-related metrics. In this study, we propose traffic
engineering-related metrics to evaluate generative trajectory prediction models
and provide a simulation-in-the-loop pipeline to do so. We also provide a
multi-headed self-attention-based trajectory prediction model that incorporates
the signal information, which outperforms our previous models on the evaluation
metrics.

</details>


### [312] [Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics](https://arxiv.org/pdf/2506.08963)
*Yash Ranjan, Rahul Sengupta, Anand Rangarajan, Sanjay Ranka*

Main category: cs.AI

TL;DR: The paper introduces a tool to evaluate deep generative models for traffic intersection dynamics, focusing on traffic engineering metrics like rule violations, not just trajectory errors.


<details>
  <summary>Details</summary>
Motivation: Current models for traffic dynamics at intersections lack evaluation on live microsimulations and ignore traffic-specific concerns like red-light violations.

Method: A multi-vehicle trajectory forecasting model is trained on real-world intersection data and evaluated online in a microsimulator under unseen conditions.

Result: Despite low trajectory errors, generated trajectories often violate traffic rules, highlighting the need for better evaluation metrics.

Conclusion: The proposed tool and new metrics provide better insights into model performance from a traffic engineering perspective.

Abstract: Traffic Intersections are vital to urban road networks as they regulate the
movement of people and goods. However, they are regions of conflicting
trajectories and are prone to accidents. Deep Generative models of traffic
dynamics at signalized intersections can greatly help traffic authorities
better understand the efficiency and safety aspects. At present, models are
evaluated on computational metrics that primarily look at trajectory
reconstruction errors. They are not evaluated online in a `live'
microsimulation scenario. Further, these metrics do not adequately consider
traffic engineering-specific concerns such as red-light violations, unallowed
stoppage, etc. In this work, we provide a comprehensive analytics tool to
train, run, and evaluate models with metrics that give better insights into
model performance from a traffic engineering point of view. We train a
state-of-the-art multi-vehicle trajectory forecasting model on a large dataset
collected by running a calibrated scenario of a real-world urban intersection.
We then evaluate the performance of the prediction models, online in a
microsimulator, under unseen traffic conditions. We show that despite using
ideally-behaved trajectories as input, and achieving low trajectory
reconstruction errors, the generated trajectories show behaviors that break
traffic rules. We introduce new metrics to evaluate such undesired behaviors
and present our results.

</details>


### [313] [A Survey of Link Prediction in N-ary Knowledge Graphs](https://arxiv.org/pdf/2506.08970)
*Jiyao Wei, Saiping Guan, Da Li, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng*

Main category: cs.AI

TL;DR: A survey on link prediction in N-ary Knowledge Graphs (NKGs), covering methods, performance, and future directions.


<details>
  <summary>Details</summary>
Motivation: NKGs represent complex facts with multiple entities, but link prediction for missing elements is underexplored.

Method: Systematic categorization and analysis of existing link prediction methods in NKGs.

Result: Overview of current methods, their performance, and application scenarios.

Conclusion: Identifies gaps and suggests future research directions for NKG link prediction.

Abstract: N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph
designed to efficiently represent complex real-world facts. Unlike traditional
knowledge graphs, where a fact typically involves two entities, NKGs can
capture n-ary facts containing more than two entities. Link prediction in NKGs
aims to predict missing elements within these n-ary facts, which is essential
for completing NKGs and improving the performance of downstream applications.
This task has recently gained significant attention. In this paper, we present
the first comprehensive survey of link prediction in NKGs, providing an
overview of the field, systematically categorizing existing methods, and
analyzing their performance and application scenarios. We also outline
promising directions for future research.

</details>


### [314] [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/pdf/2506.09038)
*Polina Kirichenko, Mark Ibrahim, Kamalika Chaudhuri, Samuel J. Bell*

Main category: cs.AI

TL;DR: AbstentionBench is introduced to evaluate LLMs' ability to abstain from answering unreliable queries, revealing it as an unsolved problem unaffected by model scaling or reasoning fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs need to know when not to answer to ensure reliability, but abstention is understudied and lacks systematic evaluation.

Method: AbstentionBench evaluates 20 LLMs across 20 diverse datasets with unanswerable, underspecified, or subjective questions.

Result: Scaling models or reasoning fine-tuning doesn't improve abstention; reasoning fine-tuning even degrades it by 24%. System prompts help but don't solve the core issue.

Conclusion: Abstention remains a challenge for LLMs, requiring further research to enhance reliability.

Abstract: For Large Language Models (LLMs) to be reliably deployed in both everyday and
high-stakes domains, knowing when not to answer is equally critical as
answering correctly. Real-world user queries, which can be underspecified,
ill-posed, or fundamentally unanswerable, require LLMs to reason about
uncertainty and selectively abstain -- i.e., refuse to answer definitively.
However, abstention remains understudied, without a systematic evaluation
framework for modern LLMs. In this work, we introduce AbstentionBench, a
large-scale benchmark for holistically evaluating abstention across 20 diverse
datasets, including questions with unknown answers, underspecification, false
premises, subjective interpretations, and outdated information. Evaluating 20
frontier LLMs reveals abstention is an unsolved problem, and one where scaling
models is of little use. While recent reasoning LLMs have shown impressive
results in complex problem solving, surprisingly, we find that reasoning
fine-tuning degrades abstention (by $24\%$ on average), even for math and
science domains on which reasoning models are explicitly trained. We find that
while a carefully crafted system prompt can boost abstention in practice, it
does not resolve models' fundamental inability to reason about uncertainty. We
release AbstentionBench to foster research into advancing LLM reliability.

</details>


### [315] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/pdf/2506.09049)
*Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin*

Main category: cs.AI

TL;DR: VIKI-Bench is a hierarchical benchmark for embodied multi-agent cooperation, and VIKI-R is a two-stage framework combining VLM fine-tuning and reinforcement learning, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of coordinating diverse embodied agents in dynamic environments, leveraging vision-language models for visual reasoning.

Method: Introduces VIKI-Bench with three levels (agent activation, task planning, trajectory perception) and VIKI-R, a framework fine-tuning VLMs with Chain-of-Thought and reinforcement learning.

Result: VIKI-R outperforms baselines across all task levels and enables compositional cooperation among heterogeneous agents.

Conclusion: VIKI-Bench and VIKI-R provide a unified testbed and method for advancing visual-driven multi-agent cooperation in embodied AI.

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


### [316] [ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering](https://arxiv.org/pdf/2506.09050)
*Yuki Imajuku, Kohki Horie, Yoichi Iwata, Kensho Aoki, Naohiro Takahashi, Takuya Akiba*

Main category: cs.AI

TL;DR: ALE-Bench is a new benchmark for evaluating AI systems on hard optimization problems, revealing gaps in consistency and long-horizon problem-solving compared to humans.


<details>
  <summary>Details</summary>
Motivation: To assess AI performance in algorithm engineering for complex optimization tasks like routing, scheduling, and planning, where exact solutions are unknown.

Method: Introduces ALE-Bench, leveraging real tasks from AtCoder Heuristic Contests, supporting iterative refinement and interactive agent architectures with test-run feedback.

Result: Frontier LLMs show high performance on specific problems but lag behind humans in consistency and long-term problem-solving.

Conclusion: ALE-Bench is essential for driving future AI advancements in tackling hard optimization challenges.

Abstract: How well do AI systems perform in algorithm engineering for hard optimization
problems in domains such as package-delivery routing, crew scheduling, factory
production planning, and power-grid balancing? We introduce ALE-Bench, a new
benchmark for evaluating AI systems on score-based algorithmic programming
contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench
presents optimization problems that are computationally hard and admit no known
exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench
encourages iterative solution refinement over long time horizons. Our software
framework supports interactive agent architectures that leverage test-run
feedback and visualizations. Our evaluation of frontier LLMs revealed that
while they demonstrate high performance on specific problems, a notable gap
remains compared to humans in terms of consistency across problems and
long-horizon problem-solving capabilities. This highlights the need for this
benchmark to foster future AI advancements.

</details>


### [317] [Human-like object concept representations emerge naturally in multimodal large language models](https://arxiv.org/pdf/2407.01067)
*Changde Du, Kaicheng Fu, Bincheng Wen, Yi Sun, Jie Peng, Wei Wei, Ying Gao, Shengpei Wang, Chuncheng Zhang, Jinpeng Li, Shuang Qiu, Le Chang, Huiguang He*

Main category: cs.AI

TL;DR: The study explores whether LLMs and MLLMs develop human-like object representations. Using behavioral and neuroimaging analyses, it finds that model embeddings align with human cognition and neural activity.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs and MLLMs can form human-like object representations from linguistic and multimodal data.

Method: Combined behavioral and neuroimaging analyses, collecting 4.7 million triplet judgments to derive 66-dimensional embeddings for 1,854 objects.

Result: Model embeddings were stable, predictive, and aligned with human mental representations and neural activity in key brain regions.

Conclusion: LLMs and MLLMs develop human-like conceptual representations, advancing machine intelligence and human-like AI development.

Abstract: Understanding how humans conceptualize and categorize natural objects offers
critical insights into perception and cognition. With the advent of Large
Language Models (LLMs), a key question arises: can these models develop
human-like object representations from linguistic and multimodal data? In this
study, we combined behavioral and neuroimaging analyses to explore the
relationship between object concept representations in LLMs and human
cognition. We collected 4.7 million triplet judgments from LLMs and Multimodal
LLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity
structure of 1,854 natural objects. The resulting 66-dimensional embeddings
were stable, predictive, and exhibited semantic clustering similar to human
mental representations. Remarkably, the dimensions underlying these embeddings
were interpretable, suggesting that LLMs and MLLMs develop human-like
conceptual representations of objects. Further analysis showed strong alignment
between model embeddings and neural activity patterns in brain regions such as
EBA, PPA, RSC, and FFA. This provides compelling evidence that the object
representations in LLMs, while not identical to human ones, share fundamental
similarities that reflect key aspects of human conceptual knowledge. Our
findings advance the understanding of machine intelligence and inform the
development of more human-like artificial cognitive systems.

</details>


### [318] [Relational decomposition for program synthesis](https://arxiv.org/pdf/2408.12212)
*Céline Hocquette, Andrew Cropper*

Main category: cs.AI

TL;DR: A relational approach to program synthesis decomposes tasks into simpler subtasks, using input-output facts and learning relations between them, outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: To simplify program synthesis by breaking down tasks into relational subtasks for better performance.

Method: Decompose input-output examples into facts, learn relations between them, and use an inductive logic programming (ILP) system.

Result: Outperforms standard representations and domain-specific approaches on four datasets.

Conclusion: The relational approach with ILP is effective for program synthesis, surpassing traditional methods.

Abstract: We introduce a relational approach to program synthesis. The key idea is to
decompose synthesis tasks into simpler relational synthesis subtasks.
Specifically, our representation decomposes a training input-output example
into sets of input and output facts respectively. We then learn relations
between the input and output facts. We demonstrate our approach using an
off-the-shelf inductive logic programming (ILP) system on four challenging
synthesis datasets. Our results show that (i) our representation can outperform
a standard one, and (ii) an off-the-shelf ILP system with our representation
can outperform domain-specific approaches.

</details>


### [319] [Innate-Values-driven Reinforcement Learning based Cognitive Modeling](https://arxiv.org/pdf/2411.09160)
*Qin Yang*

Main category: cs.AI

TL;DR: The paper introduces Innate-Values-Driven RL (IVRL), a model combining motivations and utility theory to mimic agent behaviors, outperforming benchmarks in RPG tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional RL relies on environmental rewards, but real-world agents act based on innate values. Addressing this gap, IVRL aims to balance internal and external utilities for better learning.

Method: Proposes IVRL, with two variants (IV-DQN and IV-A2C), tested against DQN, DDQN, A2C, and PPO in the VIZDoom RPG platform.

Result: IVRL-based models outperform benchmarks, enabling agents to organize needs rationally and achieve better performance.

Conclusion: IVRL effectively addresses the gap in traditional RL by incorporating innate values, enhancing agent learning and performance.

Abstract: Innate values describe agents' intrinsic motivations, which reflect their
inherent interests and preferences for pursuing goals and drive them to develop
diverse skills that satisfy their various needs. Traditional reinforcement
learning (RL) is learning from interaction based on the feedback rewards of the
environment. However, in real scenarios, the rewards are generated by agents'
innate value systems, which differ vastly from individuals based on their needs
and requirements. In other words, considering the AI agent as a self-organizing
system, developing its awareness through balancing internal and external
utilities based on its needs in different tasks is a crucial problem for
individuals learning to support others and integrate community with safety and
harmony in the long term. To address this gap, we propose a new RL model termed
innate-values-driven RL (IVRL) based on combined motivations' models and
expected utility theory to mimic its complex behaviors in the evolution through
decision-making and learning. Then, we introduce two IVRL-based models: IV-DQN
and IV-A2C. By comparing them with benchmark algorithms such as DQN, DDQN, A2C,
and PPO in the Role-Playing Game (RPG) reinforcement learning test platform
VIZDoom, we demonstrated that the IVRL-based models can help the agent
rationally organize various needs, achieve better performance effectively.

</details>


### [320] [Monte Carlo Tree Diffusion for System 2 Planning](https://arxiv.org/pdf/2502.07202)
*Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Yoshua Bengio, Sungjin Ahn*

Main category: cs.AI

TL;DR: MCTD combines diffusion models with MCTS for scalable planning, outperforming diffusion baselines in long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Diffusion models lack scalability in planning compared to MCTS, which improves with computation. MCTD aims to merge their strengths.

Method: MCTD integrates diffusion models with MCTS by treating denoising as a tree-structured process, enabling iterative evaluation and refinement.

Result: MCTD outperforms diffusion baselines, improving solution quality with increased inference-time computation.

Conclusion: MCTD successfully combines diffusion models and MCTS, offering scalable and high-quality planning solutions.

Abstract: Diffusion models have recently emerged as a powerful tool for planning.
However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally
improves with inference-time computation scaling-standard diffusion-based
planners offer only limited avenues for the scalability. In this paper, we
introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates
the generative strength of diffusion models with the adaptive search
capabilities of MCTS. Our method reconceptualizes denoising as a
tree-structured process, allowing partially denoised plans to be iteratively
evaluated, pruned, and refined. By selectively expanding promising trajectories
while retaining the flexibility to revisit and improve suboptimal branches,
MCTD achieves the benefits of MCTS such as controlling exploration-exploitation
trade-offs within the diffusion framework. Empirical results on challenging
long-horizon tasks show that MCTD outperforms diffusion baselines, yielding
higher-quality solutions as inference-time computation increases.

</details>


### [321] [Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders](https://arxiv.org/pdf/2502.07957)
*Kshitish Ghate, Isaac Slaughter, Kyra Wilson, Mona Diab, Aylin Caliskan*

Main category: cs.AI

TL;DR: CLIP models' intrinsic biases are analyzed, revealing pre-training datasets as the main predictor of bias, with performance-bias correlations and modality-dependent biases.


<details>
  <summary>Details</summary>
Motivation: To understand how upstream pre-training factors in CLIP models relate to intrinsic biases and downstream performance.

Method: Analysis of 131 CLIP models using 26 datasets, 55 architectures, and 26 Embedding Association Tests to evaluate bias.

Result: Pre-training dataset choice is the primary bias predictor; performance and bias are correlated; bias varies by modality.

Conclusion: Sophisticated strategies are needed to mitigate intrinsic bias in vision-language models throughout development.

Abstract: While recent work has found that vision-language models trained under the
Contrastive Language Image Pre-training (CLIP) framework contain intrinsic
social biases, the extent to which different upstream pre-training features of
the framework relate to these biases, and hence how intrinsic bias and
downstream performance are connected has been unclear. In this work, we present
the largest comprehensive analysis to-date of how the upstream pre-training
factors and downstream performance of CLIP models relate to their intrinsic
biases. Studying 131 unique CLIP models, trained on 26 datasets, using 55
architectures, and in a variety of sizes, we evaluate bias in each model using
26 well-established unimodal and cross-modal principled Embedding Association
Tests. We find that the choice of pre-training dataset is the most significant
upstream predictor of bias, whereas architectural variations have minimal
impact. Additionally, datasets curated using sophisticated filtering techniques
aimed at enhancing downstream model performance tend to be associated with
higher levels of intrinsic bias. Finally, we observe that intrinsic bias is
often significantly correlated with downstream performance ($0.3 \leq r \leq
0.8$), suggesting that models optimized for performance inadvertently learn to
amplify representational biases. Comparisons between unimodal and cross-modal
association tests reveal that social group bias depends heavily on the
modality. Our findings imply that more sophisticated strategies are needed to
address intrinsic model bias for vision-language models across the entire model
development pipeline.

</details>


### [322] [EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations](https://arxiv.org/pdf/2502.14760)
*Haotian Zhai, Connor Lawless, Ellen Vitercik, Liu Leqi*

Main category: cs.AI

TL;DR: The paper introduces Quasi-Karp equivalence and EquivaMap, a framework using large language models for automated equivalence checking of optimization formulations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The need for reliable automated equivalence checks in combinatorial optimization, especially for applications like optimization copilots, which lack robust current solutions.

Method: Proposes Quasi-Karp equivalence as a formal criterion and EquivaMap, leveraging large language models to discover variable mappings, with a verification stage ensuring feasibility and optimality.

Result: EquivaMap outperforms existing methods, achieving significant improvements in identifying formulation equivalence, validated on the EquivaFormulation dataset.

Conclusion: The work provides a scalable, reliable solution for equivalence checking in optimization, advancing automation in combinatorial optimization.

Abstract: A fundamental problem in combinatorial optimization is identifying equivalent
formulations. Despite the growing need for automated equivalence checks --
driven, for example, by optimization copilots, which generate problem
formulations from natural language descriptions -- current approaches rely on
simple heuristics that fail to reliably check formulation equivalence. Inspired
by Karp reductions, in this work we introduce Quasi-Karp equivalence, a formal
criterion for determining when two optimization formulations are equivalent
based on the existence of a mapping between their decision variables. We
propose EquivaMap, a framework that leverages large language models to
automatically discover such mappings for scalable, reliable equivalence
checking, with a verification stage that ensures mapped solutions preserve
feasibility and optimality without additional solver calls. To evaluate our
approach, we construct EquivaFormulation, the first open-source dataset of
equivalent optimization formulations, generated by applying transformations
such as adding slack variables or valid inequalities to existing formulations.
Empirically, EquivaMap significantly outperforms existing methods, achieving
substantial improvements in correctly identifying formulation equivalence.

</details>


### [323] [AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence](https://arxiv.org/pdf/2504.04430)
*Matej Šprogar*

Main category: cs.AI

TL;DR: The paper introduces AGITB, a benchmarking suite to evaluate low-level cognitive precursors for artificial general intelligence (AGI), highlighting current AI's shortcomings in general intelligence.


<details>
  <summary>Details</summary>
Motivation: Current AI lacks human-like general intelligence, and existing evaluation frameworks fail to measure core generality. AGITB aims to address this gap.

Method: AGITB includes twelve automatable tests focusing on binary signal prediction, isolating core computational invariants like determinism and generalization.

Result: No current AI system meets AGITB's full criteria, while humans pass all tests, showcasing its rigor and potential.

Conclusion: AGITB serves as a rigorous, interpretable benchmark for guiding progress toward AGI.

Abstract: Despite major advances in machine learning, current artificial intelligence
systems continue to fall short of human-like general intelligence. While large
language models can generate fluent and coherent outputs, they lack the deep
understanding and adaptive reasoning that characterize truly general
intelligence. Existing evaluation frameworks, which are centered on broad
language or perception tasks, fail to capture generality at its core and offer
little guidance for incremental progress. To address this gap, this paper
introduces the artificial general intelligence testbed (AGITB), a novel and
freely available benchmarking suite comprising twelve fully automatable tests
designed to evaluate low-level cognitive precursors through binary signal
prediction. AGITB requires models to forecast temporal sequences without
pretraining, symbolic manipulation, or semantic grounding. The framework
isolates core computational invariants - such as determinism, sensitivity, and
generalization - that align with principles of biological information
processing. Engineered to resist brute-force and memorization-based approaches,
AGITB presumes no prior knowledge and demands learning from first principles.
While humans pass all tests, no current AI system has met the full AGITB
criteria, underscoring its potential as a rigorous, interpretable, and
actionable benchmark for guiding and evaluating progress toward artificial
general intelligence.

</details>


### [324] [Evaluating AI-Driven Automated Map Digitization in QGIS](https://arxiv.org/pdf/2504.18777)
*Diana Febrita*

Main category: cs.AI

TL;DR: The paper evaluates Deepness, an AI-driven tool for automated map digitization, comparing its results with OpenStreetMap to assess effectiveness.


<details>
  <summary>Details</summary>
Motivation: To reduce human involvement in map digitization by leveraging AI and machine learning techniques.

Method: Uses Deepness, a Deep Neural Remote Sensing tool in QGIS, to digitize Google Earth imagery and compares results with OpenStreetMap.

Result: Performance of Deepness in automated digitization is analyzed against OSM outputs.

Conclusion: The study highlights the potential of AI tools like Deepness to streamline map digitization processes.

Abstract: Map digitization is an important process that converts maps into digital
formats that can be used for further analysis. This process typically requires
a deep human involvement because of the need for interpretation and
decision-making when translating complex features. With the advancement of
artificial intelligence, there is an alternative to conducting map digitization
with the help of machine learning techniques. Deepness, or Deep Neural Remote
Sensing, is an advanced AI-driven tool designed and integrated as a plugin in
QGIS application. This research focuses on assessing the effectiveness of
Deepness in automated digitization. This study analyses AI-generated
digitization results from Google Earth imagery and compares them with digitized
outputs from OpenStreetMap (OSM) to evaluate performance.

</details>


### [325] [BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing](https://arxiv.org/pdf/2505.01343)
*Dongliang Guo, Mengxuan Hu, Zihan Guan, Thomas Hartvigsen, Sheng Li*

Main category: cs.AI

TL;DR: The paper addresses the decay of large multi-modal models over time and introduces BalancEdit, a method for balanced model editing that optimizes the generality-locality trade-off.


<details>
  <summary>Details</summary>
Motivation: Traditional fine-tuning is impractical for updating large models, and current editing techniques overlook the influence ranges of facts, compromising performance.

Method: Proposes BalancEdit, which dynamically balances generality and locality by generating positive/negative samples and using a localized codebook of edits.

Result: BalancEdit effectively minimizes trade-offs while maintaining robust editing capabilities.

Conclusion: BalancEdit is the first method to explicitly address the generality-locality trade-off in multi-modal model editing, demonstrating strong performance.

Abstract: Large multi-modal models inevitably decay over time as facts update and
previously learned information becomes outdated. Traditional approaches such as
fine-tuning are often impractical for updating these models due to their size
and complexity. Instead, direct knowledge editing within the models presents a
more viable solution. Current model editing techniques, however, typically
overlook the unique influence ranges of different facts, leading to compromised
model performance in terms of both generality and locality. To address this
issue, we introduce the concept of the generality-locality trade-off in
multi-modal model editing. We develop a new model editing dataset named OKEDIT,
specifically designed to effectively evaluate this trade-off. Building on this
foundation, we propose \textbf{BalancEdit}, a novel method for balanced model
editing that dynamically achieves an optimal balance between generality and
locality. BalancEdit utilizes a unique mechanism that generates both positive
and negative samples for each fact to accurately determine its influence scope
and incorporates these insights into the model's latent space using a discrete,
localized codebook of edits, without modifying the underlying model weights. To
our knowledge, this is the first approach explicitly addressing the
generality-locality trade-off in multi-modal model editing. Our comprehensive
results confirm the effectiveness of BalancEdit, demonstrating minimal
trade-offs while maintaining robust editing capabilities. Our code and dataset
are available at https://github.com/donglgcn/BalancEdit/tree/MMOKVQA.

</details>


### [326] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/pdf/2505.14479)
*Oren Sultan, Eitan Stern, Dafna Shahaf*

Main category: cs.AI

TL;DR: A neuro-symbolic approach combining LLMs with structured components improves logical reasoning in geometry proofs, achieving 58%-70% accuracy gains.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with rigorous logical tasks like mathematical proofs. This work aims to enhance their reliability in such domains.

Method: Retrieve analogous problems to guide LLMs and use a formal verifier to evaluate and correct generated proofs.

Result: Proof accuracy improves significantly (58%-70%) for OpenAI's o1 model, with both analogous problems and verifier feedback contributing.

Conclusion: Enhancing LLMs to generate provably correct outputs can boost their reliability for complex, real-world applications.

Abstract: Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [327] [E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing](https://arxiv.org/pdf/2505.20733)
*Cheonsu Jeong, Seongmin Sim, Hyoyoung Cho, Sungsu Kim, Byounggwan Shin*

Main category: cs.AI

TL;DR: The paper introduces an AI-driven automation system combining generative AI and IDP to enhance financial expense processing, outperforming traditional RPA by reducing processing time by 80% and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional RPA struggles with unstructured data and complex decisions, prompting the need for a more advanced solution integrating AI and IDP.

Method: A four-stage process: document recognition via OCR/IDP, item classification, AI-driven exception handling, and human-in-the-loop decision-making with continuous learning.

Result: Quantitative benefits (80% faster processing, fewer errors) and qualitative improvements (accuracy, employee satisfaction) were achieved in a Korean enterprise.

Conclusion: The integration of generative AI, IDP, and Automation Agents enables E2E automation of complex tasks, with potential applications in other domains.

Abstract: This paper presents an intelligent work automation approach in the context of
contemporary digital transformation by integrating generative AI and
Intelligent Document Processing (IDP) technologies with an Automation Agent to
realize End-to-End (E2E) automation of corporate financial expense processing
tasks. While traditional Robotic Process Automation (RPA) has proven effective
for repetitive, rule-based simple task automation, it faces limitations in
handling unstructured data, exception management, and complex decision-making.
This study designs and implements a four-stage integrated process comprising
automatic recognition of supporting documents such as receipts via OCR/IDP,
item classification based on a policy-driven database, intelligent exception
handling supported by generative AI (large language models, LLMs), and
human-in-the-loop final decision-making with continuous system learning through
an Automation Agent. Applied to a major Korean enterprise (Company S), the
system demonstrated quantitative benefits including over 80% reduction in
processing time for paper receipt expense tasks, decreased error rates, and
improved compliance, as well as qualitative benefits such as enhanced accuracy
and consistency, increased employee satisfaction, and data-driven decision
support. Furthermore, the system embodies a virtuous cycle by learning from
human judgments to progressively improve automatic exception handling
capabilities. Empirically, this research confirms that the organic integration
of generative AI, IDP, and Automation Agents effectively overcomes the
limitations of conventional automation and enables E2E automation of complex
corporate processes. The study also discusses potential extensions to other
domains such as accounting, human resources, and procurement, and proposes
future directions for AI-driven hyper-automation development.

</details>


### [328] [BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies](https://arxiv.org/pdf/2506.00328)
*Kourosh Shahnazari, Seyed Moein Ayyoubzadeh, Mohammadali Keshtparvar*

Main category: cs.AI

TL;DR: BASIL introduces a symbolic, interpretable reinforcement learning method using evolutionary search and quality-diversity optimization to create transparent policies comparable to deep RL baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the opacity of deep RL policies, BASIL aims to ensure interpretability, verification, and human oversight in safety-critical applications.

Method: BASIL uses online evolutionary search with quality-diversity optimization to generate symbolic, rule-based policies represented as ordered lists of predicates. It encourages diversity and compactness via a QD archive and complexity-aware fitness.

Result: Empirical tests on CartPole-v1, MountainCar-v0, and Acrobot-v1 show BASIL produces interpretable, compact policies matching deep RL performance.

Conclusion: BASIL successfully combines symbolic expressiveness, evolutionary diversity, and online learning to advance interpretable RL.

Abstract: The quest for interpretable reinforcement learning is a grand challenge for
the deployment of autonomous decision-making systems in safety-critical
applications. Modern deep reinforcement learning approaches, while powerful,
tend to produce opaque policies that compromise verification, reduce
transparency, and impede human oversight. To address this, we introduce BASIL
(Best-Action Symbolic Interpretable Learning), a systematic approach for
generating symbolic, rule-based policies via online evolutionary search with
quality-diversity (QD) optimization. BASIL represents policies as ordered lists
of symbolic predicates over state variables, ensuring full interpretability and
tractable policy complexity. By using a QD archive, the methodology in the
proposed study encourages behavioral and structural diversity between
top-performing solutions, while a complexity-aware fitness encourages the
synthesis of compact representations. The evolutionary system supports the use
of exact constraints for rule count and system adaptability for balancing
transparency with expressiveness. Empirical comparisons with three benchmark
tasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently
synthesizes interpretable controllers with compact representations comparable
to deep reinforcement learning baselines. Herein, this article introduces a new
interpretable policy synthesis method that combines symbolic expressiveness,
evolutionary diversity, and online learning through a unifying framework.

</details>


### [329] [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design](https://arxiv.org/pdf/2506.04734)
*Lin Sun, Weihong Lin, Jinzhu Wu, Yongfu Zhu, Xiaoqi Jian, Guangxiang Zhao, Change Jia, Linglin Zhang, Sai-er Hu, Yuhan Wu, Xiangzheng Zhang*

Main category: cs.AI

TL;DR: The paper highlights performance evaluation inconsistencies in the Deepseek-R1-Distill series and similar models, advocating for stricter evaluation standards.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability of benchmark results due to fluctuating evaluation conditions in models like Deepseek-R1-Distill and QwQ-32B.

Method: Empirical assessments of the Deepseek-R1-Distill series models to identify evaluation inconsistencies.

Result: Significant fluctuations in benchmark results due to minor evaluation condition changes, making performance claims hard to reproduce.

Conclusion: A call for a more rigorous evaluation paradigm to ensure reliable and reproducible model performance assessments.

Abstract: Reasoning models represented by the Deepseek-R1-Distill series have been
widely adopted by the open-source community due to their strong performance in
mathematics, science, programming, and other domains. However, our study
reveals that their benchmark evaluation results are subject to significant
fluctuations caused by various factors. Subtle differences in evaluation
conditions can lead to substantial variations in results. Similar phenomena are
observed in other open-source inference models fine-tuned based on the
Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their
claimed performance improvements difficult to reproduce reliably. Therefore, we
advocate for the establishment of a more rigorous paradigm for model
performance evaluation and present our empirical assessments of the
Deepseek-R1-Distill series models.

</details>


### [330] [Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties](https://arxiv.org/pdf/2506.05744)
*Gouki Minegishi, Hiroki Furuta, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo*

Main category: cs.AI

TL;DR: The paper analyzes reasoning graphs in large-scale models, revealing structural advantages like cyclicity and small-world properties, which correlate with performance and task difficulty.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms of large-scale reasoning models by examining their reasoning graph structures.

Method: Extracts reasoning graphs by clustering hidden-state representations and analyzes graph-theoretic properties (cyclicity, diameter, small-world index) across tasks and model variants.

Result: Distilled models show more cycles, larger diameters, and small-world traits, correlating with accuracy. Fine-tuning expands diameters and improves performance.

Conclusion: The study links reasoning graph structures to model performance, offering insights for interpretability and dataset design to enhance reasoning capabilities.

Abstract: Recent large-scale reasoning models have achieved state-of-the-art
performance on challenging mathematical benchmarks, yet the internal mechanisms
underlying their success remain poorly understood. In this work, we introduce
the notion of a reasoning graph, extracted by clustering hidden-state
representations at each reasoning step, and systematically analyze three key
graph-theoretic properties: cyclicity, diameter, and small-world index, across
multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled
reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly
more recurrent cycles (about 5 per sample), substantially larger graph
diameters, and pronounced small-world characteristics (about 6x) compared to
their base counterparts. Notably, these structural advantages grow with task
difficulty and model capacity, with cycle detection peaking at the 14B scale
and exploration diameter maximized in the 32B variant, correlating positively
with accuracy. Furthermore, we show that supervised fine-tuning on an improved
dataset systematically expands reasoning graph diameters in tandem with
performance gains, offering concrete guidelines for dataset design aimed at
boosting reasoning capabilities. By bridging theoretical insights into
reasoning graph structures with practical recommendations for data
construction, our work advances both the interpretability and the efficacy of
large reasoning models.

</details>


### [331] [CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents](https://arxiv.org/pdf/2506.05981)
*Qingbin Zeng, Ruotong Zhao, Jinzhu Mao, Haoyang Li, Fengli Xu, Yong Li*

Main category: cs.AI

TL;DR: CrimeMind is an LLM-driven ABM framework integrating Routine Activity Theory for urban crime simulation, outperforming traditional methods in accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing methods (ABMs and deep learning) lack predictive accuracy, interpretability, and adaptability to changing urban environments.

Method: Combines LLMs with ABM, integrates Routine Activity Theory, and aligns perception with human judgment using a training-free textual gradient method.

Result: Achieves up to 24% improvement in crime hotspot prediction and accurately simulates counterfactual scenarios.

Conclusion: CrimeMind offers fine-grained, interpretable crime modeling and effective evaluation of real-world interventions.

Abstract: Modeling urban crime is an important yet challenging task that requires
understanding the subtle visual, social, and cultural cues embedded in urban
environments. Previous work has mainly focused on rule-based agent-based
modeling (ABM) and deep learning methods. ABMs offer interpretability of
internal mechanisms but exhibit limited predictive accuracy. In contrast, deep
learning methods are often effective in prediction but are less interpretable
and require extensive training data. Moreover, both lines of work lack the
cognitive flexibility to adapt to changing environments. Leveraging the
capabilities of large language models (LLMs), we propose CrimeMind, a novel
LLM-driven ABM framework for simulating urban crime within a multi-modal urban
context. A key innovation of our design is the integration of the Routine
Activity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to
process rich multi-modal urban features and reason about criminal behavior.
However, RAT requires LLM agents to infer subtle cues in evaluating
environmental safety as part of assessing guardianship, which can be
challenging for LLMs. To address this, we collect a small-scale human-annotated
dataset and align CrimeMind's perception with human judgment via a
training-free textual gradient method. Experiments across four major U.S.
cities demonstrate that CrimeMind outperforms both traditional ABMs and deep
learning baselines in crime hotspot prediction and spatial distribution
accuracy, achieving up to a 24% improvement over the strongest baseline.
Furthermore, we conduct counterfactual simulations of external incidents and
policy interventions and it successfully captures the expected changes in crime
patterns, demonstrating its ability to reflect counterfactual scenarios.
Overall, CrimeMind enables fine-grained modeling of individual behaviors and
facilitates evaluation of real-world interventions.

</details>


### [332] [NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting](https://arxiv.org/pdf/2506.06285)
*Kaike Sa Teles Rocha Alves, Eduardo Pestana de Aguiar*

Main category: cs.AI

TL;DR: The paper proposes genetic and ensemble fuzzy models to address deep learning's training time and interpretability issues, achieving superior performance in photovoltaic energy forecasting.


<details>
  <summary>Details</summary>
Motivation: Deep learning models suffer from long training times and lack of interpretability, while fuzzy systems offer a balance of accuracy and transparency.

Method: Extends the New Takagi-Sugeno-Kang model to a Mamdani-based regressor, integrates wrapper and ensemble techniques (Genetic Algorithm for feature selection), and introduces ensemble models like Random New Mamdani Regressor.

Result: Genetic and ensemble fuzzy models outperform traditional machine learning and deep learning in photovoltaic energy forecasting, with simpler, interpretable rule-based structures.

Conclusion: The proposed fuzzy models provide a robust, interpretable alternative to deep learning, validated on real-world datasets and available in the nfisis library.

Abstract: Deep learning models, despite their popularity, face challenges such as long
training times and a lack of interpretability. In contrast, fuzzy inference
systems offer a balance of accuracy and transparency. This paper addresses the
limitations of traditional Takagi-Sugeno-Kang fuzzy models by extending the
recently proposed New Takagi-Sugeno-Kang model to a new Mamdani-based
regressor. These models are data-driven, allowing users to define the number of
rules to balance accuracy and interpretability. To handle the complexity of
large datasets, this research integrates wrapper and ensemble techniques. A
Genetic Algorithm is used as a wrapper for feature selection, creating genetic
versions of the models. Furthermore, ensemble models, including the Random New
Mamdani Regressor, Random New Takagi-Sugeno-Kang, and Random Forest New
Takagi-Sugeno-Kang, are introduced to improve robustness. The proposed models
are validated on photovoltaic energy forecasting datasets, a critical
application due to the intermittent nature of solar power. Results demonstrate
that the genetic and ensemble fuzzy models, particularly the Genetic New
Takagi-Sugeno-Kang and Random Forest New Takagi-Sugeno-Kang, achieve superior
performance. They often outperform both traditional machine learning and deep
learning models while providing a simpler and more interpretable rule-based
structure. The models are available online in a library called nfisis
(https://pypi.org/project/nfisis/).

</details>


### [333] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/pdf/2506.06905)
*Akash Gupta, Amos Storkey, Mirella Lapata*

Main category: cs.AI

TL;DR: The paper proposes a meta-learning approach using soft prompts distilled from task-relevant image features to improve few-shot learning in Large Multimodal Models (LMMs), outperforming in-context learning (ICL) and prompt-tuning methods.


<details>
  <summary>Details</summary>
Motivation: Inconsistent ICL performance in smaller LMMs due to overwhelming image embeddings led to the need for a better few-shot learning method.

Method: A meta-learning approach with soft prompts and an attention-mapper module, integrated with LLaVA v1.5, enables task adaptation with minimal examples.

Result: The method outperforms ICL and prompt-tuning on the VL-ICL Bench, even under image perturbations, enhancing task induction and reasoning.

Conclusion: The proposed approach effectively addresses ICL limitations, improving few-shot learning in LMMs with task-adapted soft prompts.

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [334] [GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition](https://arxiv.org/pdf/2506.07553)
*Jingchao Wang, Haote Yang, Jiang Wu, Yifan He, Xingjian Wei, Yinfan Wang, Chengjin Liu, Lingli Ge, Lijun Wu, Bin Wang, Dahua Lin, Conghui He*

Main category: cs.AI

TL;DR: GTR-Mol-VLM, a novel framework for Optical Chemical Structure Recognition (OCSR), improves accuracy by emulating human reasoning and addressing annotation mismatches, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of vision-language models (VLMs) in handling complex molecular structures and inconsistent annotations in OCSR.

Method: Introduces Graph Traversal as Visual Chain of Thought and a data-centric principle for faithful recognition, supported by a large-scale dataset (GTR-CoT-1.3M) and a benchmark (MolRec-Bench).

Result: GTR-Mol-VLM outperforms specialist models and VLMs, especially in handling abbreviated structures, by ~14 percentage points.

Conclusion: The framework advances OCSR technology, benefiting cheminformatics and AI for Science, with plans to release the dataset publicly.

Abstract: Optical Chemical Structure Recognition (OCSR) is crucial for digitizing
chemical knowledge by converting molecular images into machine-readable
formats. While recent vision-language models (VLMs) have shown potential in
this task, their image-captioning approach often struggles with complex
molecular structures and inconsistent annotations. To overcome these
challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key
innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that
emulates human reasoning by incrementally parsing molecular graphs through
sequential atom-bond predictions, and (2) the data-centric principle of
Faithfully Recognize What You've Seen, which addresses the mismatch between
abbreviated structures in images and their expanded annotations. To support
model development, we constructed GTR-CoT-1.3M, a large-scale
instruction-tuning dataset with meticulously corrected annotations, and
introduced MolRec-Bench, the first benchmark designed for a fine-grained
evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments
demonstrate that GTR-Mol-VLM achieves superior results compared to specialist
models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in
scenarios involving molecular images with functional group abbreviations,
GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage
points, both in SMILES-based and graph-based metrics. We hope that this work
will drive OCSR technology to more effectively meet real-world needs, thereby
advancing the fields of cheminformatics and AI for Science. We will release
GTR-CoT at https://github.com/opendatalab/GTR-CoT.

</details>


### [335] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/pdf/2506.07564)
*Peiran Li, Xinkai Zou, Zhuohang Wu, Ruifeng Li, Shuo Xing, Hanwen Zheng, Zhikai Hu, Yuping Wang, Haoxi Li, Qin Yuan, Yingmo Zhang, Zhengzhong Tu*

Main category: cs.AI

TL;DR: SAFEFLOW is a protocol-level framework for trustworthy LLM/VLM-based agents, enforcing fine-grained information flow control and robust multi-agent coordination, validated by SAFEFLOWBENCH.


<details>
  <summary>Details</summary>
Motivation: Current agent frameworks lack mechanisms for secure information flow, reliability, and multi-agent coordination, making them fragile.

Method: SAFEFLOW introduces fine-grained IFC, transactional execution, conflict resolution, secure scheduling, and resilience mechanisms like logging and rollback.

Result: Agents built with SAFEFLOW maintain high task performance and security in adversarial conditions, outperforming state-of-the-art.

Conclusion: SAFEFLOW and SAFEFLOWBENCH advance reliable autonomy by providing principled, robust, and secure agent ecosystems.

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [336] [Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation](https://arxiv.org/pdf/2506.07820)
*Jiaxiang Chen, Zhuo Wang, Mingxi Zou, Qifan Wang, Zenglin Xu*

Main category: cs.AI

TL;DR: Guideline Forest enhances LLM reasoning by inducing reusable guidelines from verified examples, executing them via step-wise aggregation, and outperforms baselines on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with flexible, adaptive reasoning like humans. Existing methods lack structured, efficient reuse of reasoning strategies.

Method: Induces structured reasoning guidelines from verified examples, expands them into diverse variants, executes in parallel, and refines via self-correction.

Result: Outperforms baselines (CoT, ReAct, ToT, FoT, AFlow) on benchmarks (GSM8K, MATH-500, MBPP, HumanEval).

Conclusion: Guideline Forest's multi-path reasoning and stepwise aggregation improve adaptability and generalization, emulating human-like reasoning.

Abstract: Human reasoning is flexible, adaptive, and grounded in prior
experience-qualities that large language models (LLMs) still struggle to
emulate. Existing methods either explore diverse reasoning paths at inference
time or search for optimal workflows through expensive operations, but both
fall short in leveraging multiple reusable strategies in a structured,
efficient manner. We propose Guideline Forest, a framework that enhances LLMs
reasoning by inducing structured reasoning strategies-called guidelines-from
verified examples and executing them via step-wise aggregation. Unlike
test-time search or single-path distillation, our method draws on verified
reasoning experiences by inducing reusable guidelines and expanding each into
diverse variants. Much like human reasoning, these variants reflect alternative
thought patterns, are executed in parallel, refined via self-correction, and
aggregated step by step-enabling the model to adaptively resolve uncertainty
and synthesize robust solutions.We evaluate Guideline Forest on four
benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and
programmatic reasoning. Guideline Forest consistently outperforms strong
baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further
highlight the effectiveness of multi-path reasoning and stepwise aggregation,
underscoring the Guideline Forest's adaptability and generalization potential.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [337] [MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion](https://arxiv.org/pdf/2506.08357)
*Franck Meyer, Kyunghoon Hur, Edward Choi*

Main category: cs.SD

TL;DR: MD-ViSCo is a unified framework for generating any target vital sign waveform from any single input waveform using a single model, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing models are limited to specific source-to-target pairs, requiring multiple models and hindering clinical usability.

Method: MD-ViSCo uses a 1D U-Net with Swin Transformer and AdaIN to capture waveform styles, evaluated on public datasets.

Result: MD-ViSCo reduces MAE by 8.8% and improves PC by 4.9%, meeting AAMI and BHS standards for ABP waveforms.

Conclusion: MD-ViSCo provides a versatile, single-model solution for vital sign waveform generation in healthcare.

Abstract: Despite the remarkable progress of deep-learning methods generating a target
vital sign waveform from a source vital sign waveform, most existing models are
designed exclusively for a specific source-to-target pair. This requires
distinct model architectures, optimization procedures, and pre-processing
pipelines, resulting in multiple models that hinder usability in clinical
settings. To address this limitation, we propose the Multi-Directional
Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any
target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or
arterial blood pressure (ABP) from any single input waveform with a single
model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin
Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture
distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct
multi-directional waveform generation on two publicly available datasets. Our
framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average
across all waveform types, lowering Mean absolute error (MAE) by 8.8% and
improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the
generated ABP waveforms satisfy the Association for the Advancement of Medical
Instrumentation (AAMI) criterion and achieve Grade B on the British
Hypertension Society (BHS) standard, outperforming all baselines. By
eliminating the need for developing a distinct model for each task, we believe
that this work offers a unified framework that can deal with any kind of vital
sign waveforms with a single model in healthcare monitoring.

</details>


### [338] [A Review on Score-based Generative Models for Audio Applications](https://arxiv.org/pdf/2506.08457)
*Ge Zhu, Yutong Wen, Zhiyao Duan*

Main category: cs.SD

TL;DR: A survey on diffusion models for audio applications, focusing on design choices, quality improvement, and conditioning. Includes an open-source codebase for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Existing reviews lack in-depth discussion of design choices in diffusion models for audio, and there's no principled guidance for implementation.

Method: Adopts a score modeling perspective to unify interpretations, examines training/sampling procedures, and introduces an open-source codebase.

Result: Demonstrates capabilities through case studies: audio generation, speech enhancement, and text-to-speech synthesis.

Conclusion: Provides comprehensive guidance and tools for implementing diffusion models in audio applications, promoting reproducible research.

Abstract: Diffusion models have emerged as powerful deep generative techniques,
producing high-quality and diverse samples in applications in various domains
including audio. These models have many different design choices suitable for
different applications, however, existing reviews lack in-depth discussions of
these design choices. The audio diffusion model literature also lacks
principled guidance for the implementation of these design choices and their
comparisons for different applications. This survey provides a comprehensive
review of diffusion model design with an emphasis on design principles for
quality improvement and conditioning for audio applications. We adopt the score
modeling perspective as a unifying framework that accommodates various
interpretations, including recent approaches like flow matching. We
systematically examine the training and sampling procedures of diffusion
models, and audio applications through different conditioning mechanisms. To
address the lack of audio diffusion model codebases and to promote reproducible
research and rapid prototyping, we introduce an open-source codebase at
https://github.com/gzhu06/AudioDiffuser that implements our reviewed framework
for various audio applications. We demonstrate its capabilities through three
case studies: audio generation, speech enhancement, and text-to-speech
synthesis, with benchmark evaluations on standard datasets.

</details>


### [339] [Passive acoustic non-line-of-sight localization without a relay surface](https://arxiv.org/pdf/2506.08471)
*Tal I. Sommer, Ori Katz*

Main category: cs.SD

TL;DR: The paper introduces a method for 3D localization of an acoustic source outside the Line-of-Sight (LOS) using signals diffracted from obstacle edges, addressing doorway and convex corner scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional NLOS localization relies on reflected signals from visible surfaces, which may be limited in certain environments. This study explores edge diffraction for broader applicability.

Method: Two scenarios are addressed: (1) using door edges as virtual detector arrays, and (2) leveraging knife-edge diffraction inspired by HRTF for convex corners.

Result: The proposed methods enable 3D localization of acoustic sources in NLOS environments where conventional approaches may fail.

Conclusion: Edge diffraction extends NLOS acoustic sensing capabilities, offering solutions for challenging scenarios like doorways and convex corners.

Abstract: The detection and localization of a source hidden outside the Line-of-Sight
(LOS) traditionally rely on the acquisition of indirect signals, such as those
reflected from visible relay surfaces such as floors or walls. These reflected
signals are then utilized to reconstruct the obscured scene. In this study, we
present an approach that utilize signals diffracted from an edge of an obstacle
to achieve three-dimensional (3D) localization of an acoustic point source
situated outside the LOS. We address two scenarios - a doorway and a convex
corner - and propose a localization method for each of them. For the first
scenario, we utilize the two edges of the door as virtual detector arrays. For
the second scenario, we exploit the spectral signature of a knife-edge
diffraction, inspired by the human perception of sound location by the
head-related transfer function (HRTF). In both methods, knife-edge diffraction
is utilized to extend the capabilities of non-line-of-sight (NLOS) acoustic
sensing, enabling localization in environments where conventional relay-surface
based approaches may be limited.

</details>


### [340] [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/pdf/2506.08346)
*Wenhan Yao, Fen Xiao, Xiarun Chen, Jia Liu, YongQiang He, Weiping Wen*

Main category: cs.SD

TL;DR: The paper introduces SPBA, a speech backdoor attack method using diverse triggers via SLLM, and proposes MGDA for mitigation. It shows high effectiveness in attack metrics.


<details>
  <summary>Details</summary>
Motivation: Security vulnerabilities in deep speech classification tasks due to backdoor attacks, with existing methods limited by trigger constraints.

Method: Proposes SPBA, leveraging SLLM for diverse triggers (timbre, emotion) and MGDA for mitigation. Tests on speech classification tasks.

Result: SPBA demonstrates significant trigger effectiveness and exceptional performance in attack metrics.

Conclusion: SPBA is an effective backdoor attack method, with MGDA mitigating its challenges, showing promise for future security research.

Abstract: Deep speech classification tasks, including keyword spotting and speaker
verification, are vital in speech-based human-computer interaction. Recently,
the security of these technologies has been revealed to be susceptible to
backdoor attacks. Specifically, attackers use noisy disruption triggers and
speech element triggers to produce poisoned speech samples that train models to
become vulnerable. However, these methods typically create only a limited
number of backdoors due to the inherent constraints of the trigger function. In
this paper, we propose that speech backdoor attacks can strategically focus on
speech elements such as timbre and emotion, leveraging the Speech Large
Language Model (SLLM) to generate diverse triggers. Increasing the number of
triggers may disproportionately elevate the poisoning rate, resulting in higher
attack costs and a lower success rate per trigger. We introduce the Multiple
Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this
challenge. The proposed attack is called the Speech Prompt Backdoor Attack
(SPBA). Building on this foundation, we conducted attack experiments on two
speech classification tasks, demonstrating that SPBA shows significant trigger
effectiveness and achieves exceptional performance in attack metrics.

</details>


### [341] [Higher-Order Network Representation of J. S. Bach's Solo Violin Sonatas and Partitas: Topological and Geometrical Explorations](https://arxiv.org/pdf/2506.08540)
*Dima Mrad, Sara Najem*

Main category: cs.SD

TL;DR: A topological framework using higher-order networks analyzes J.S. Bach's Solo Violin Sonatas and Partitas, revealing genre-specific patterns in geometric and topological properties.


<details>
  <summary>Details</summary>
Motivation: Traditional dyadic representations fail to capture the complexity of music, necessitating higher-order models for accurate analysis.

Method: Higher-order networks represent notes as vertices, chords as edges, and higher structures (e.g., triangles) to model transitions and flow.

Result: Genre-specific patterns emerge in Euler characteristic, curvature, and adherence to the Gauss-Bonnet theorem, distinguishing movement types.

Conclusion: The framework successfully captures higher-order interactions in music, providing insights into Bach's compositions.

Abstract: Music is inherently complex, with structures and interactions that unfold
across multiple layers. Complex networks have emerged as powerful structures
for the quantitative analysis of Western classical music, revealing significant
features of its harmonic and structural organization. Although notable works
have used these approaches to study music, dyadic representations of
interactions fall short in conveying the underlying complexity and depth. In
recent years, the limitations of traditional graph representations have been
questioned and challenged in the context of interactions that could be
higher-dimensional. Effective musical analysis requires models that capture
higher-order interactions and a framework that simultaneously captures
transitions between them. Subsequently, in this paper, we present a topological
framework for analyzing J. S. Bach's Solo Violin Sonatas and Partitas that uses
higher-order networks where single notes are vertices, two-note chords are
edges, three-notes are triangles, etc. We subsequently account for the flow of
music, by modeling transitions between successive notes. We identify
genre-specific patterns in the works' geometric and topological properties. In
particular, we find signatures in the trends of the evolution of the Euler
characteristic and curvature, as well as examining adherence to the
Gauss-Bonnet theorem across different movement types. The distinctions are
revealed between slow movements, Fugues, and Baroque dance movements through
their simplicial complex representation.

</details>


### [342] [Pureformer-VC: Non-parallel Voice Conversion with Pure Stylized Transformer Blocks and Triplet Discriminative Training](https://arxiv.org/pdf/2506.08348)
*Wenhan Yao, Fen Xiao, Xiarun Chen, Jia Liu, YongQiang He, Weiping Wen*

Main category: cs.SD

TL;DR: Pureformer-VC is a novel voice conversion framework using Conformer and Zipformer blocks, with variational decoupled training and attention style transfer, outperforming traditional GAN-based methods in both subjective and objective metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional GAN-based voice conversion methods struggle with precise encoding and natural synthesis of speech elements. Pureformer-VC aims to address these limitations.

Method: The framework uses Conformer blocks for a disentangled encoder and Zipformer blocks for a style transfer decoder, with variational decoupled training (VAE) and triplet discriminative training. ASTM enhances style transfer.

Result: Experiments on multi-speaker datasets show Pureformer-VC achieves comparable subjective scores and better objective metrics than existing methods in many-to-many and many-to-one VC.

Conclusion: Pureformer-VC effectively overcomes traditional GAN limitations, offering improved performance in voice conversion tasks.

Abstract: As a foundational technology for intelligent human-computer interaction,
voice conversion (VC) seeks to transform speech from any source timbre into any
target timbre. Traditional voice conversion methods based on Generative
Adversarial Networks (GANs) encounter significant challenges in precisely
encoding diverse speech elements and effectively synthesising these elements
into natural-sounding converted speech. To overcome these limitations, we
introduce Pureformer-VC, an encoder-decoder framework that utilizes Conformer
blocks to build a disentangled encoder and employs Zipformer blocks to create a
style transfer decoder. We adopt a variational decoupled training approach to
isolate speech components using a Variational Autoencoder (VAE), complemented
by triplet discriminative training to enhance the speaker's discriminative
capabilities. Furthermore, we incorporate the Attention Style Transfer
Mechanism (ASTM) with Zipformer's shared weights to improve the style transfer
performance in the decoder. We conducted experiments on two multi-speaker
datasets. The experimental results demonstrate that the proposed model achieves
comparable subjective evaluation scores while significantly enhancing objective
metrics compared to existing approaches in many-to-many and many-to-one VC
scenarios.

</details>


### [343] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/pdf/2506.08570)
*Or Tal, Felix Kreuk, Yossi Adi*

Main category: cs.SD

TL;DR: The paper compares Auto-Regressive decoding and Conditional Flow-Matching paradigms in text-to-music generation, analyzing their performance and trade-offs.


<details>
  <summary>Details</summary>
Motivation: To isolate the effects of modeling paradigms in text-to-music generation and guide future system designs.

Method: A controlled comparison of Auto-Regressive decoding and Conditional Flow-Matching using identical datasets, training configurations, and similar architectures.

Result: The study highlights distinct strengths and limitations of each paradigm, providing insights into generation quality, robustness, scalability, and editing capabilities.

Conclusion: The findings offer actionable insights for future text-to-music generation systems, emphasizing the importance of modeling paradigm choices.

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


### [344] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/pdf/2506.08524)
*Weiguo Wang, Andy Nie, Wenrui Zhou, Yi Kai, Chengchen Hu*

Main category: cs.SD

TL;DR: ACORN teaches LLMs physical awareness using sound and a physics-based simulator, achieving reasonable results in real-world tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs lack physical awareness, limiting their understanding of real-world phenomena.

Method: ACORN uses a physics-based simulator to generate training data, builds the AQA-PHY dataset, and connects an audio encoder to LLMs.

Result: Demonstrates success in tasks like line-of-sight detection and Doppler effect estimation.

Conclusion: ACORN paves the way for LLMs to understand the physical world through sound.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


### [345] [Multimodal Zero-Shot Framework for Deepfake Hate Speech Detection in Low-Resource Languages](https://arxiv.org/pdf/2506.08372)
*Rishabh Ranjan, Likhith Ayinala, Mayank Vatsa, Richa Singh*

Main category: cs.SD

TL;DR: A novel multimodal framework for hate speech detection in deepfake audio using contrastive learning, outperforming baselines in multilingual and zero-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of hate speech detection in synthetic media, especially in low-resource languages, where unimodal models struggle.

Method: Uses contrastive learning to align audio and text representations across languages, creating a shared semantic embedding space.

Result: Achieves accuracies of 0.819 and 0.701 on multilingual test sets, generalizing well to unseen languages.

Conclusion: The framework demonstrates the effectiveness of multimodal approaches for hate speech detection in low-resource settings.

Abstract: This paper introduces a novel multimodal framework for hate speech detection
in deepfake audio, excelling even in zero-shot scenarios. Unlike previous
approaches, our method uses contrastive learning to jointly align audio and
text representations across languages. We present the first benchmark dataset
with 127,290 paired text and synthesized speech samples in six languages:
English and five low-resource Indian languages (Hindi, Bengali, Marathi, Tamil,
Telugu). Our model learns a shared semantic embedding space, enabling robust
cross-lingual and cross-modal classification. Experiments on two multilingual
test sets show our approach outperforms baselines, achieving accuracies of
0.819 and 0.701, and generalizes well to unseen languages. This demonstrates
the advantage of combining modalities for hate speech detection in synthetic
media, especially in low-resource settings where unimodal models falter. The
Dataset is available at https://www.iab-rubric.org/resources.

</details>


### [346] [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/pdf/2506.08967)
*Ailin Huang, Bingxin Li, Bruce Wang, Boyong Wu, Chao Yan, Chengli Feng, Heng Wang, Hongyu Zhou, Hongyuan Wang, Jingbei Li, Jianjian Sun, Joanna Wang, Mingrui Chen, Peng Liu, Ruihang Miao, Shilei Jiang, Tian Fei, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Ge, Zheng Gong, Zhewei Huang, Zixin Zhang, Bin Wang, Bo Li, Buyun Ma, Changxin Miao, Changyi Wan, Chen Xu, Dapeng Shi, Dingyuan Hu, Enle Liu, Guanzhe Huang, Gulin Yan, Hanpeng Hu, Haonan Jia, Jiahao Gong, Jiaoren Wu, Jie Wu, Jie Yang, Junzhe Lin, Kaixiang Li, Lei Xia, Longlong Gu, Ming Li, Nie Hao, Ranchen Ming, Shaoliang Pang, Siqi Liu, Song Yuan, Tiancheng Cao, Wen Li, Wenqing He, Xu Zhao, Xuelin Zhang, Yanbo Yu, Yinmin Zhong, Yu Zhou, Yuanwei Liang, Yuanwei Lu, Yuxiang Yang, Zidong Yang, Zili Zhang, Binxing Jiao, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Daxin Jiang, Shuchang Zhou, Chen Hu*

Main category: cs.SD

TL;DR: Step-Audio-AQAA is an end-to-end LALM for Audio Query-Audio Answer tasks, integrating a dual-codebook tokenizer, a large LLM, and a neural vocoder, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing LALMs rely on text outputs, limiting natural speech generation for seamless audio interactions.

Method: Uses a dual-codebook audio tokenizer, 130B-parameter LLM, and neural vocoder, with post-training via interleaved token-output and DPO.

Result: Outperforms state-of-the-art LALMs in speech control on StepEval-Audio-360 benchmark.

Conclusion: Step-Audio-AQAA advances end-to-end LALMs, emphasizing the importance of token-based vocoders for AQAA tasks.

Abstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.

</details>


### [347] [Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers](https://arxiv.org/pdf/2309.09652)
*Peter Ochieng*

Main category: cs.SD

TL;DR: UDPNet accelerates reverse diffusion in speech synthesis by unrolling the process into its architecture and predicting latent variables, reducing distortion and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional diffusion models face issues like large prediction errors in early denoising stages, leading to speech distortion. UDPNet aims to address these challenges.

Method: UDPNet unrolls the reverse diffusion process into its architecture, with layers corresponding to diffusion steps, and predicts latent variables instead of conventional targets.

Result: UDPNet outperforms state-of-the-art methods in quality and efficiency on single- and multi-speaker datasets, generalizing well to unseen speech.

Conclusion: UDPNet is a robust solution for real-time speech synthesis, offering improved performance and reduced distortion.

Abstract: This work introduces UDPNet, a novel architecture designed to accelerate the
reverse diffusion process in speech synthesis. Unlike traditional diffusion
models that rely on timestep embeddings and shared network parameters, UDPNet
unrolls the reverse diffusion process directly into the network architecture,
with successive layers corresponding to equally spaced steps in the diffusion
schedule. Each layer progressively refines the noisy input, culminating in a
high-fidelity estimation of the original data, \(x_0\). Additionally, we
redefine the learning target by predicting latent variables instead of the
conventional \(x_0\) or noise \(\epsilon_0\). This shift addresses the common
issue of large prediction errors in early denoising stages, effectively
reducing speech distortion. Extensive evaluations on single- and multi-speaker
datasets demonstrate that UDPNet consistently outperforms state-of-the-art
methods in both quality and efficiency, while generalizing effectively to
unseen speech. These results position UDPNet as a robust solution for real-time
speech synthesis applications. Sample audio is available at
https://onexpeters.github.io/UDPNet/.

</details>


### [348] [Structuring Concept Space with the Musical Circle of Fifths by Utilizing Music Grammar Based Activations](https://arxiv.org/pdf/2403.00790)
*Tofara Moyo*

Main category: cs.SD

TL;DR: The paper explores parallels between spiking neural networks and piano compositions, proposing musical grammar to regulate network activations and represent symbols as attractors.


<details>
  <summary>Details</summary>
Motivation: To leverage music theory, particularly chord progressions and key modulation, to enhance the structure and dynamics of spiking neural networks.

Method: Uses musical grammar and chord progression rules to guide activations in spiking networks, introducing key modulation for navigating attraction basins.

Result: Demonstrates that activations follow patterns akin to musical attraction, with concepts structured by the circle of fifths.

Conclusion: Music theory principles can effectively structure and enhance deep learning algorithms, particularly in spiking neural networks.

Abstract: In this paper, we explore the intriguing similarities between the structure
of a discrete neural network, such as a spiking network, and the composition of
a piano piece. While both involve nodes or notes that are activated
sequentially or in parallel, the latter benefits from the rich body of music
theory to guide meaningful combinations. We propose a novel approach that
leverages musical grammar to regulate activations in a spiking neural network,
allowing for the representation of symbols as attractors. By applying rules for
chord progressions from music theory, we demonstrate how certain activations
naturally follow others, akin to the concept of attraction. Furthermore, we
introduce the concept of modulating keys to navigate different basins of
attraction within the network. Ultimately, we show that the map of concepts in
our model is structured by the musical circle of fifths, highlighting the
potential for leveraging music theory principles in deep learning algorithms.

</details>


### [349] [Enhancing Retrieval-Augmented Audio Captioning with Generation-Assisted Multimodal Querying and Progressive Learning](https://arxiv.org/pdf/2410.10913)
*Choi Changin, Lim Sungjun, Rhee Wonjong*

Main category: cs.SD

TL;DR: The paper proposes a multimodal querying method for retrieval-augmented audio captioning, improving retrieval effectiveness by generating text descriptions of input audio. It also introduces a progressive learning strategy for better training.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on unimodal (audio-only) queries, which may not align well with the audio-text structure of knowledge bases. The goal is to enhance retrieval effectiveness by using multimodal queries.

Method: Proposes Generation-Assisted Multimodal Querying, where text descriptions of input audio are generated for multimodal retrieval. Also introduces a progressive learning strategy to gradually increase interleaved audio-text pairs during training.

Result: Achieves state-of-the-art results on AudioCaps, Clotho, and Auto-ACD benchmarks.

Conclusion: Multimodal querying and progressive learning significantly improve retrieval-augmented audio captioning, outperforming existing methods.

Abstract: Retrieval-augmented generation can improve audio captioning by incorporating
relevant audio-text pairs from a knowledge base. Existing methods typically
rely solely on the input audio as a unimodal retrieval query. In contrast, we
propose Generation-Assisted Multimodal Querying, which generates a text
description of the input audio to enable multimodal querying. This approach
aligns the query modality with the audio-text structure of the knowledge base,
leading to more effective retrieval. Furthermore, we introduce a novel
progressive learning strategy that gradually increases the number of
interleaved audio-text pairs to enhance the training process. Our experiments
on AudioCaps, Clotho, and Auto-ACD demonstrate that our approach achieves
state-of-the-art results across these benchmarks.

</details>


### [350] [Are you really listening? Boosting Perceptual Awareness in Music-QA Benchmarks](https://arxiv.org/pdf/2504.00369)
*Yongyi Zang, Sean O'Brien, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack*

Main category: cs.SD

TL;DR: The paper introduces RUListening, a framework to improve evaluation of audio perception in Music-QA benchmarks, addressing limitations in current methods where text-only LLMs perform well without audio input.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for Large Audio Language Models (LALMs) fail to accurately assess audio perception, as text-only LLMs perform comparably or better, and LALMs still score above chance with noise.

Method: The authors propose RUListening, which uses the Perceptual Index (PI) to measure audio reliance in questions and generates synthetic distractors to create QA pairs requiring genuine audio perception.

Result: Applied to MuchoMusic, the filtered dataset forces models to rely on audio: text-only LLMs perform at chance, and LALMs degrade with noise inputs.

Conclusion: RUListening effectively creates benchmarks that better evaluate audio perception, validated by the results.

Abstract: Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned
with audio input, have made remarkable progress in music understanding.
However, current evaluation methodologies exhibit critical limitations: on the
leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without
audio perception capabilities achieve surprisingly high accuracy of up to
56.4%, on par or above most LALMs. Furthermore, when presented with random
Gaussian noise instead of actual audio, LALMs still perform significantly above
chance. These findings suggest existing benchmarks predominantly assess
reasoning abilities rather than audio perception. To overcome this challenge,
we present RUListening: Robust Understanding through Listening, a framework
that enhances perceptual evaluation in Music-QA benchmarks. We introduce the
Perceptual Index (PI), a quantitative metric that measures a question's
reliance on audio perception by analyzing log probability distributions from
text-only language models. Using this metric, we generate synthetic,
challenging distractors to create QA pairs that necessitate genuine audio
perception. When applied to MuchoMusic, our filtered dataset successfully
forces models to rely on perceptual information-text-only LLMs perform at
chance levels, while LALMs similarly deteriorate when audio inputs are replaced
with noise. These results validate our framework's effectiveness in creating
benchmarks that more accurately evaluate audio perception capabilities.

</details>


### [351] [Voice Impression Control in Zero-Shot TTS](https://arxiv.org/pdf/2506.05688)
*Keinichi Fujita, Shota Horiguchi, Yusuke Ijima*

Main category: cs.SD

TL;DR: A method for controlling voice impressions in zero-shot TTS using low-dimensional vectors and large language models, eliminating manual optimization.


<details>
  <summary>Details</summary>
Motivation: Modulating para-/non-linguistic speech information to control perceived voice impressions is challenging in zero-shot TTS.

Method: Uses a low-dimensional vector to represent voice impression intensities and leverages a large language model for vector generation from natural language descriptions.

Result: Demonstrated effectiveness in impression control through objective and subjective evaluations.

Conclusion: The method successfully enables target-impression generation from natural language, improving zero-shot TTS flexibility.

Abstract: Para-/non-linguistic information in speech is pivotal in shaping the
listeners' impression. Although zero-shot text-to-speech (TTS) has achieved
high speaker fidelity, modulating subtle para-/non-linguistic information to
control perceived voice characteristics, i.e., impressions, remains
challenging. We have therefore developed a voice impression control method in
zero-shot TTS that utilizes a low-dimensional vector to represent the
intensities of various voice impression pairs (e.g., dark-bright). The results
of both objective and subjective evaluations have demonstrated our method's
effectiveness in impression control. Furthermore, generating this vector via a
large language model enables target-impression generation from a natural
language description of the desired impression, thus eliminating the need for
manual optimization. Audio examples are available on our demo page
(https://ntt-hilab-gensp.github.io/is2025voiceimpression/).

</details>


### [352] [Towards Generalized Source Tracing for Codec-Based Deepfake Speech](https://arxiv.org/pdf/2506.07294)
*Xuanjun Chen, I-Ming Lin, Lin Zhang, Haibin Wu, Hung-yi Lee, Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: SASTNet improves source tracing for deepfake speech by combining semantic and acoustic features, outperforming models trained on codec-resynthesized data.


<details>
  <summary>Details</summary>
Motivation: Existing models for tracing deepfake speech (CodecFake) perform poorly, especially when trained on simulated data. The challenge is maintaining performance on real CoSG-generated audio.

Method: Proposes SASTNet, which uses Whisper for semantic features and Wav2vec2 with AudioMAE for acoustic features.

Result: SASTNet achieves state-of-the-art performance on the CoSG test set of CodecFake+.

Conclusion: SASTNet effectively addresses generalization issues in source tracing for deepfake speech.

Abstract: Recent attempts at source tracing for codec-based deepfake speech
(CodecFake), generated by neural audio codec-based speech generation (CoSG)
models, have exhibited suboptimal performance. However, how to train source
tracing models using simulated CoSG data while maintaining strong performance
on real CoSG-generated audio remains an open challenge. In this paper, we show
that models trained solely on codec-resynthesized data tend to overfit to
non-speech regions and struggle to generalize to unseen content. To mitigate
these challenges, we introduce the Semantic-Acoustic Source Tracing Network
(SASTNet), which jointly leverages Whisper for semantic feature encoding and
Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet
achieves state-of-the-art performance on the CoSG test set of the CodecFake+
dataset, demonstrating its effectiveness for reliable source tracing.

</details>


### [353] [An introduction to pitch strength in contemporary popular music analysis and production](https://arxiv.org/pdf/2506.07473)
*Emmanuel Deruty*

Main category: cs.SD

TL;DR: The paper explores pitch strength as a low-level feature in music, suggesting it could enhance AI models for music production by addressing variability, structure, dissonance, and perceptual richness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between high-level text descriptions used in generative AI models and the low-level controls familiar to studio musicians, focusing on pitch strength as a key feature.

Method: Signal and perceptual analyses were conducted to study pitch strength's role in music.

Result: Pitch strength varies across songs, contributes to structure and dissonance handling, and may relate to upper harmonics in perceptual richness.

Conclusion: Pitch strength is a promising low-level feature for improving AI models in music production.

Abstract: Music information retrieval distinguishes between low- and high-level
descriptions of music. Current generative AI models rely on text descriptions
that are higher level than the controls familiar to studio musicians. Pitch
strength, a low-level perceptual parameter of contemporary popular music, may
be one feature that could make such AI models more suited to music production.
Signal and perceptual analyses suggest that pitch strength (1) varies
significantly across and inside songs; (2) contributes to both small- and
large-scale structure; (3) contributes to the handling of polyphonic
dissonance; and (4) may be a feature of upper harmonics made audible in a
perspective of perceptual richness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [354] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/pdf/2506.08018)
*Fei Li, Song Liu, Weiguo Wu, Shiqiang Nie, Jinyu Wang*

Main category: cs.LG

TL;DR: KVmix is a mixed-precision quantization method for KV Cache in LLMs, dynamically allocating bit-widths and optimizing long-context tasks for memory efficiency and speed.


<details>
  <summary>Details</summary>
Motivation: The high memory demands of KV Cache in LLMs limit deployment on resource-constrained platforms, necessitating efficient quantization methods.

Method: KVmix uses gradient-based importance analysis for layer-specific bit-width allocation and dynamic long-context optimization, prioritizing critical KV pairs.

Result: KVmix achieves near-lossless performance with low quantization (Key 2.19bit, Value 2.38bit), 4.9x memory compression, and 5.3x speedup.

Conclusion: KVmix effectively balances accuracy and efficiency, enabling high-performance LLM inference with minimal memory usage.

Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [355] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/pdf/2506.09046)
*Xiaowen Ma, Chenyang Lin, Yao Zhang, Volker Tresp, Yunpu Ma*

Main category: cs.LG

TL;DR: The paper introduces Agentic Neural Network (ANN), a framework for dynamic multi-agent collaboration inspired by neural networks, improving accuracy and adaptability in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems rely on static configurations, limiting flexibility and performance. ANN aims to address this by enabling dynamic, data-driven collaboration.

Method: ANN uses a two-phase optimization: (1) Forward Phase for dynamic task decomposition and agent team formation, and (2) Backward Phase for iterative refinement of collaboration.

Result: ANN outperforms existing multi-agent systems on four benchmark datasets, showing consistent improvements in accuracy and adaptability.

Conclusion: ANN offers a scalable, data-driven approach for multi-agent systems, combining LLM collaboration with neural network efficiency. The framework will be open-sourced.

Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [356] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/pdf/2506.08019)
*Andrew Wells, Geraldine Henningsen, Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: A semi-supervised method disaggregates refugee stats to 0.5-degree grids in 25 African countries, achieving 92.9% accuracy.


<details>
  <summary>Details</summary>
Motivation: To uncover localized displacement patterns obscured in broader statistics.

Method: Integrates UNHCR data, satellite footprints, and OpenStreetMap coordinates using label spreading.

Result: 92.9% accuracy in placing 10M+ refugee observations into grid cells.

Conclusion: High-resolution dataset aids in understanding displacement drivers.

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [357] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/pdf/2506.08020)
*Zi-Ying Chen, Chuan-Xian Ren, Hong Yan*

Main category: cs.LG

TL;DR: The paper proposes a Bi-level Unbalanced Optimal Transport (BUOT) model to address partial domain adaptation by simultaneously handling sample-wise and class-wise relations, improving outlier identification and alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in existing weighting frameworks for partial domain adaptation, which inadequately explore cluster structures and are sensitive to inaccurate predictions.

Method: The method introduces a bi-level transport framework (BUOT) that combines sample-level and class-level transport, leveraging their cooperation for better alignment and outlier identification.

Result: Extensive experiments on benchmark datasets show BUOT's competitiveness in addressing partial domain adaptation.

Conclusion: BUOT effectively unifies sample-wise and class-wise relations, enhancing knowledge transfer and outlier identification in partial domain adaptation.

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [358] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/pdf/2506.08021)
*Weihao Zou, Weibing Feng, Pin Wu*

Main category: cs.LG

TL;DR: A framework using LLM and POD for fast, accurate flow field prediction, reducing computational time from hours to seconds.


<details>
  <summary>Details</summary>
Motivation: Address high computational costs of CFD and limited transferability of deep learning models in fluid dynamics.

Method: Integrates POD dimensionality reduction with fine-tuned LLM, using fluid dynamics-oriented text templates.

Result: Outperforms Transformers in few-shot learning, generalizes well, and achieves >90% accuracy in seconds.

Conclusion: Sets a new direction for rapid fluid dynamics prediction, with applications in aerodynamics and engineering.

Abstract: This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [359] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/pdf/2506.08022)
*Chenxi Liu, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng Guo, Tianyi Zhou, Heng Huang*

Main category: cs.LG

TL;DR: MBPO addresses modality imbalance in LMMs by generating hard negatives and using online responses with verified rewards, improving performance and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing LMMs suffer from modality imbalance and LLM biases, limiting generalization and causing hallucinations. Current methods lack focus on these issues and rely on offline data.

Method: MBPO constructs an offline preference dataset with hard negatives via adversarial image perturbation and uses online responses with verified rewards. GRPO trains the model with hybrid data.

Result: MBPO enhances LMM performance on vision-language tasks and reduces hallucinations.

Conclusion: MBPO effectively balances modalities and improves LMM reasoning, offering a robust solution for alignment challenges.

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [360] [Innate-Values-driven Reinforcement Learning based Cooperative Multi-Agent Cognitive Modeling](https://arxiv.org/pdf/2401.05572)
*Qin Yang*

Main category: cs.LG

TL;DR: The paper proposes a Multi-Agent Innate-Values Reinforcement Learning (IVRL) architecture to address the lack of a general intrinsic model for dynamic motivation in MAS, tested in StarCraft settings.


<details>
  <summary>Details</summary>
Motivation: Current MAS reinforcement learning lacks a model to describe agents' dynamic motivation from an individual needs perspective during cooperation.

Method: Proposes a general MAS innate-values reinforcement learning (IVRL) architecture, tested using a Multi-Agent IVRL Actor-Critic Model in StarCraft Multi-Agent Challenge (SMAC) settings.

Result: The model demonstrated potential to organize group behaviors for better performance in SMAC.

Conclusion: The IVRL architecture effectively balances group utilities and individual needs, enhancing cooperation in MAS.

Abstract: In multi-agent systems (MAS), the dynamic interaction among multiple
decision-makers is driven by their innate values, affecting the environment's
state, and can cause specific behavioral patterns to emerge. On the other hand,
innate values in cognitive modeling reflect individual interests and
preferences for specific tasks and drive them to develop diverse skills and
plans, satisfying their various needs and achieving common goals in
cooperation. Therefore, building the awareness of AI agents to balance the
group utilities and system costs and meet group members' needs in their
cooperation is a crucial problem for individuals learning to support their
community and even integrate into human society in the long term. However, the
current MAS reinforcement learning domain lacks a general intrinsic model to
describe agents' dynamic motivation for decision-making and learning from an
individual needs perspective in their cooperation. To address the gap, this
paper proposes a general MAS innate-values reinforcement learning (IVRL)
architecture from the individual preferences angle. We tested the Multi-Agent
IVRL Actor-Critic Model in different StarCraft Multi-Agent Challenge (SMAC)
settings, which demonstrated its potential to organize the group's behaviours
to achieve better performance.

</details>


### [361] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/pdf/2506.08027)
*Asit Mishra, Dusan Stosic, Simon Layton*

Main category: cs.LG

TL;DR: Precision scaling with MX-formats improves GPU efficiency but requires careful rounding mode selection for LLM pre-training.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of numeric stability in reduced-precision representations like MX-formats during LLM pre-training.

Method: Proposes an improved rounding mode (round-to-infinity) for computing scaling factors in MXFP8.

Result: Successfully pre-trains an 8B model on 15T tokens using MXFP8 with the new rounding mode.

Conclusion: Careful selection of rounding modes is crucial for stable LLM pre-training in reduced-precision formats.

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [362] [FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL](https://arxiv.org/pdf/2410.15876)
*Woosung Koh, Wonbeen Oh, Siyeol Kim, Suhin Shin, Hyeongjin Kim, Jaein Jang, Junghyun Lee, Se-Young Yun*

Main category: cs.LG

TL;DR: FlickerFusion is a novel OOD generalization method for MARL that handles dynamic entity changes by stochastically dropping parts of observations, improving performance and reducing uncertainty.


<details>
  <summary>Details</summary>
Motivation: Existing MARL methods fail in dynamic environments where entity counts change unpredictably, limiting real-world applicability.

Method: Proposes FlickerFusion, an augmentation technique that randomly drops observation parts to simulate OOD conditions during training.

Result: FlickerFusion outperforms existing methods in rewards and reduces uncertainty, validated through benchmarks.

Conclusion: FlickerFusion enhances MARL robustness for dynamic environments, with open-sourced resources for reproducibility.

Abstract: Multi-agent reinforcement learning has demonstrated significant potential in
addressing complex cooperative tasks across various real-world applications.
However, existing MARL approaches often rely on the restrictive assumption that
the number of entities (e.g., agents, obstacles) remains constant between
training and inference. This overlooks scenarios where entities are dynamically
removed or added during the inference trajectory -- a common occurrence in
real-world environments like search and rescue missions and dynamic combat
situations. In this paper, we tackle the challenge of intra-trajectory dynamic
entity composition under zero-shot out-of-domain (OOD) generalization, where
such dynamic changes cannot be anticipated beforehand. Our empirical studies
reveal that existing MARL methods suffer significant performance degradation
and increased uncertainty in these scenarios. In response, we propose
FlickerFusion, a novel OOD generalization method that acts as a universally
applicable augmentation technique for MARL backbone methods. FlickerFusion
stochastically drops out parts of the observation space, emulating being
in-domain when inferenced OOD. The results show that FlickerFusion not only
achieves superior inference rewards but also uniquely reduces uncertainty
vis-\`a-vis the backbone, compared to existing methods. Benchmarks,
implementations, and model weights are organized and open-sourced at
flickerfusion305.github.io, accompanied by ample demo video renderings.

</details>


### [363] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/pdf/2506.08051)
*Mahmuda Sultana Mimi, Md Monzurul Islam, Anannya Ghosh Tusti, Shriyank Somvanshi, Subasish Das*

Main category: cs.LG

TL;DR: ST-GraphNet, a spatio-temporal graph neural network, predicts AV crash severity with high accuracy using multimodal data and spatial aggregation.


<details>
  <summary>Details</summary>
Motivation: Understanding AV crash severity dynamics is crucial for urban safety and infrastructure planning.

Method: ST-GraphNet uses fine-grained and coarse-grained spatial graphs with multimodal data, evaluated via GNN architectures like GCN, GAT, and DSTGCN.

Result: ST-GraphNet achieves 97.74% test accuracy, outperforming fine-grained models (64.7%).

Conclusion: Spatial aggregation and dynamic message passing effectively capture spatio-temporal patterns in AV crash severity.

Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [364] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/pdf/2506.08054)
*Yiming Wang, Hao Peng, Senzhang Wang, Haohua Du, Chunyang Liu, Jia Wu, Guanlin Wu*

Main category: cs.LG

TL;DR: STAMImputer is a SpatioTemporal Attention Mixture of Experts network for traffic data imputation, addressing block-wise missing data and dynamic spatial correlations.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in block-wise missing data and static graph structures limit flexibility for nonstationary traffic data.

Method: Uses Mixture of Experts (MoE) for spatio-temporal features and LrSGAT for dynamic graph generation.

Result: Outperforms SOTA methods on four traffic datasets.

Conclusion: STAMImputer effectively handles block missing data and dynamic spatial correlations, improving traffic data imputation.

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [365] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/pdf/2506.08060)
*Asankhaya Sharma*

Main category: cs.LG

TL;DR: The paper proves that supervised fine-tuning (SFT) capabilities in large language models can be approximated using inference-time techniques like in-context learning (ICL), without parameter updates, under idealized and practical conditions.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational burden of SFT by showing that similar performance can be achieved through inference-time methods, making large language models more resource-efficient.

Method: Theoretical analysis under idealized assumptions (unbounded resources, full dataset access) and extension to practical scenarios (finite context, partial dataset). Provides dataset size bounds for approximating SFT behavior.

Result: For text generation and linear classification tasks, specific dataset sizes suffice to approximate fine-tuned behavior within error bounds, leveraging ICL.

Conclusion: The study offers a theoretical basis for efficient deployment of large language models, with practical implications for techniques like retrieval-augmented generation.

Abstract: Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [366] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/pdf/2506.08062)
*Woosung Kim, Jinho Lee, Jongmin Lee, Byung-Jun Lee*

Main category: cs.LG

TL;DR: FairDICE is the first offline MORL framework optimizing nonlinear welfare objectives, outperforming baselines in fairness-aware performance.


<details>
  <summary>Details</summary>
Motivation: Linear scalarization in MORL fails to capture fairness goals like Nash social welfare or max-min fairness, necessitating a unified offline approach.

Method: FairDICE uses distribution correction estimation for welfare maximization and distributional regularization, enabling stable learning without preference weights.

Result: FairDICE shows strong fairness-aware performance across offline benchmarks.

Conclusion: FairDICE successfully addresses the gap in offline MORL for nonlinear welfare objectives.

Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [367] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/pdf/2506.08063)
*Songqiao Hu, Zeyi Liu, Xiao He*

Main category: cs.LG

TL;DR: Lite-RVFL is a lightweight, fast method for adapting to concept drift in online learning without retraining or drift detection, using an exponential weighting strategy for recent data.


<details>
  <summary>Details</summary>
Motivation: Concept drift in online learning challenges reliability, and existing methods are computationally expensive or unsuitable for real-time applications.

Method: Lite-RVFL uses a novel objective function with exponential weights for new samples and an efficient incremental update rule.

Result: Theoretical and experimental validation shows Lite-RVFL efficiently adapts to drift and captures temporal patterns.

Conclusion: Lite-RVFL offers a practical solution for real-time concept drift adaptation with low computational cost.

Abstract: The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [368] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/pdf/2506.08070)
*Ziheng Qin, Hailun Xu, Wei Chee Yew, Qi Jia, Yang Luo, Kanchan Sarkar, Danhui Guan, Kai Wang, Yang You*

Main category: cs.LG

TL;DR: Info-Coevolution is a framework for efficient dataset annotation and training by selectively annotating data, reducing costs by 32% without performance loss.


<details>
  <summary>Details</summary>
Motivation: Addresses inefficiency in dataset construction and training due to redundant data, aiming to reduce annotation and training costs without bias.

Method: Proposes Info-Coevolution, which selectively annotates and integrates online/web data using task-specific models, enabling coevolution of models and data.

Result: Achieves 32% reduction in annotation and training costs on ImageNet-1K, with potential for 50% reduction using semi-supervised learning.

Conclusion: Info-Coevolution efficiently improves datasets and reduces costs, offering a scalable solution for machine learning data challenges.

Abstract: Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [369] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/pdf/2506.08113)
*Timothée Hornek Amir Sartipi, Igor Tchappi, Gilbert Fridgen*

Main category: cs.LG

TL;DR: Benchmarking pretrained time series foundation models (TSFMs) against traditional methods for electricity price forecasting (EPF) reveals that Chronos-Bolt and Time-MoE perform well, but the biseasonal MSTL model remains the most consistent and effective.


<details>
  <summary>Details</summary>
Motivation: Accurate EPF is vital for power trading, but the effectiveness of recent GenAI and LLM-based TSFMs in this domain is unclear.

Method: Several pretrained TSFMs (Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, TimeGPT) are benchmarked against statistical and ML methods using 2024 DAA electricity prices from multiple European countries.

Result: Chronos-Bolt and Time-MoE perform comparably to traditional models, but the biseasonal MSTL model consistently outperforms all TSFMs across metrics and countries.

Conclusion: While TSFMs show promise, traditional models like MSTL, which capture seasonality, remain superior for EPF.

Abstract: Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [370] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/pdf/2506.08125)
*Hanbing Liu, Lang Cao, Yuanyi Ren, Mengyu Zhou, Haoyu Dong, Xiaojun Ma, Shi Han, Dongmei Zhang*

Main category: cs.LG

TL;DR: Bingo is an RL framework that improves reasoning efficiency in large language models by using significance-aware and dynamic length rewards, balancing accuracy and brevity.


<details>
  <summary>Details</summary>
Motivation: Existing RL approaches for reasoning often focus on accuracy but neglect efficiency, and direct length-based rewards can harm accuracy. Bingo aims to address this gap.

Method: Bingo introduces two mechanisms: a significance-aware length reward to reduce insignificant tokens gradually, and a dynamic length reward that adjusts based on question difficulty.

Result: Experiments show Bingo outperforms baselines, improving both accuracy and efficiency across reasoning benchmarks.

Conclusion: Bingo demonstrates the potential of explicitly training LLMs for efficient reasoning, achieving a favorable trade-off between accuracy and efficiency.

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [371] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/pdf/2506.08139)
*Aviad Susman, Mayte Suárez-Fariñas, Joseph T Colonel*

Main category: cs.LG

TL;DR: Combining neural networks with traditional algorithms like k-NN can boost performance. The paper introduces NONA, a differentiable k-NN proxy, showing improved results.


<details>
  <summary>Details</summary>
Motivation: Traditional predictors often outperform neural networks when used with SFT embeddings, suggesting integrating them into SFT could enhance performance.

Method: Introduces NONA, a differentiable regression layer mimicking k-NN using neural attention and learned masking.

Result: NONA outperforms both dense layers and k-NN on SFT embeddings in regression tasks.

Conclusion: NONA successfully bridges the gap between traditional algorithms and neural networks, offering a differentiable solution for improved performance.

Abstract: It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [372] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/pdf/2506.08140)
*Yifei Li, Hanane Nour Moussa, Ziru Chen, Shijie Chen, Botao Yu, Mingyi Xue, Benjamin Burns, Tzu-Yao Chiu, Vishal Dey, Zitong Lu, Chen Wei, Qianheng Zhang, Tianyu Zhang, Song Gao, Xuhui Huang, Xia Ning, Nesreen K. Ahmed, Ali Payani, Huan Sun*

Main category: cs.LG

TL;DR: AutoSDT is an AI pipeline for creating high-quality coding tasks for scientific discovery, resulting in the AutoSDT-5K dataset. It improves LLM performance on benchmarks, matching GPT-4o in some cases.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in AI-driven scientific discovery by automating the collection of high-quality coding tasks.

Method: AutoSDT uses LLMs to search, select, and synthesize tasks and solutions, creating the AutoSDT-5K dataset.

Result: AutoSDT-5K contains 5,404 tasks; 93% are valid, and 92.2% of programs are correct. AutoSDT-Coder models show significant benchmark improvements.

Conclusion: AutoSDT effectively tackles data scarcity, enhancing AI performance in scientific discovery, with results comparable to GPT-4o.

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [373] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/pdf/2506.08143)
*Francesco Tonin, Alex Lambert, Johan A. K. Suykens, Volkan Cevher*

Main category: cs.LG

TL;DR: A new efficient method for fair spectral clustering (Fair SC) is introduced, leveraging a difference of convex functions (DC) framework and an augmented variable strategy for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing fairness in decision-making algorithms, particularly ensuring proportional representation of demographic groups in clustering.

Method: Proposes Fair SC using a DC framework, novel variable augmentation, and an adapted alternating direction method of multipliers algorithm.

Result: Achieves higher computational efficiency and significant speedups, especially for larger problem sizes, compared to prior methods.

Conclusion: Represents a major advancement for practical adoption of fair clustering in real-world applications.

Abstract: Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [374] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/pdf/2506.08146)
*Vahidullah Taç, Amirhossein Amiri-Hezaveh, Manuel K. Rausch, Grace N. Bechtel, Francisco Sahli Costabal, Adrian Buganza Tepole*

Main category: cs.LG

TL;DR: A novel framework uses neural networks and physics-based data-driven methods to identify mechanical properties of heterogeneous materials without closed-form constitutive equations.


<details>
  <summary>Details</summary>
Motivation: Traditional methods require closed-form constitutive equations, which are often unavailable for heterogeneous materials. The proposed framework aims to overcome this limitation.

Method: Combines Fourier-feature neural networks for strain field approximation and NODEs for constitutive equation discovery, using a hyper-network for heterogeneity. A multi-objective loss function ensures physical constraints.

Result: Demonstrated robustness in identifying mechanical properties for various heterogeneous scenarios, including noisy data and experimental applications.

Conclusion: The framework is a robust, general alternative to classical inverse methods for heterogeneous material analysis.

Abstract: We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [375] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/pdf/2506.08164)
*Hadi Reisizadeh, Jinghan Jia, Zhiqi Bu, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Sijia Liu, Mingyi Hong*

Main category: cs.LG

TL;DR: The paper proposes a bi-level optimization framework (BLUR) for unlearning in LLMs, prioritizing forget loss over retain loss to avoid performance degradation.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between forget and retain losses in LLM unlearning, ensuring compliance with data regulations and ethical AI practices.

Method: Introduces a hierarchical bi-level optimization formulation: lower-level minimizes forget loss, upper-level maintains utility. BLUR algorithm is proposed.

Result: BLUR outperforms state-of-the-art unlearning algorithms across tasks, models, and metrics.

Conclusion: BLUR provides a theoretically sound and effective solution for LLM unlearning, prioritizing ethical and regulatory compliance.

Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [376] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/pdf/2506.08167)
*Sunny Gupta, Nikita Jangid, Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL is a federated learning framework addressing non-IID data issues by using two regularization strategies to mimic IID-like training, improving accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) struggles with non-IID data due to local classifier bias, and existing solutions are costly or ineffective.

Method: UniVarFL employs Classifier Variance Regularization and Hyperspherical Uniformity Regularization during local training to align distributions and enhance generalization.

Result: UniVarFL outperforms existing methods in accuracy on benchmark datasets, proving scalable and efficient for real-world FL.

Conclusion: UniVarFL offers a promising solution for FL in resource-constrained settings by effectively mitigating non-IID data challenges.

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [377] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/pdf/2506.08169)
*Jingqiao Tang, Ryan Bausback, Feng Bao, Richard Archibald*

Main category: cs.LG

TL;DR: Proposes Federated Stochastic Neural Networks (FSNN) to address latent noise in federated learning by using stochastic neural networks as local models.


<details>
  <summary>Details</summary>
Motivation: Federated learning's susceptibility to latent noise in client data due to factors like limited measurement capabilities or human errors.

Method: Incorporates stochastic neural networks as local models to estimate true data states and quantify latent noise.

Result: Numerical experiments show effectiveness, especially with non-IID data.

Conclusion: FSNN improves federated learning by handling latent noise and enhancing model accuracy.

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [378] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/pdf/2506.08176)
*Anh V Nguyen, Diego Klabjan*

Main category: cs.LG

TL;DR: The paper proposes a Genetic Algorithm-based method for constructing personalized decision trees in Federated Learning, outperforming local data training and benchmark algorithms.


<details>
  <summary>Details</summary>
Motivation: Addressing the understudied area of non-parametric models like decision trees in Federated Learning, and overcoming limitations of existing methods restricted to classification trees and categorical data.

Method: Utilizes Genetic Algorithm to build personalized decision trees, accommodating both categorical and numerical data for classification and regression tasks.

Result: The method surpasses decision trees trained on local data and a benchmark algorithm, as shown in comprehensive experiments.

Conclusion: The proposed approach effectively extends Federated Learning to non-parametric models, offering flexibility and improved performance for decision trees.

Abstract: In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [379] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/pdf/2506.08201)
*Krishna Pillutla, Jalaj Upadhyay, Christopher A. Choquette-Choo, Krishnamurthy Dvijotham, Arun Ganesh, Monika Henzinger, Jonathan Katz, Ryan McKenna, H. Brendan McMahan, Keith Rush, Thomas Steinke, Abhradeep Thakurta*

Main category: cs.LG

TL;DR: The paper explores correlated noise mechanisms in differential privacy (DP) for AI/ML model training, showing improved privacy-utility trade-offs compared to independent noise.


<details>
  <summary>Details</summary>
Motivation: To enhance privacy-utility trade-offs in DP by leveraging correlated noise, addressing limitations of independent noise in stochastic gradient descent (SGD).

Method: Uses correlated noise mechanisms (e.g., matrix mechanisms, DP-FTRL) to cancel noise across training steps, applied to weighted prefix sums estimation.

Result: Demonstrates significant improvements in privacy-utility trade-offs, with practical industrial-scale deployment.

Conclusion: Correlated noise mechanisms offer superior performance in DP for AI/ML training, with real-world applicability.

Abstract: This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [380] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/pdf/2506.08205)
*Shadab Anwar Shaikh, Kranthi Balusu, Ayoub Soulami*

Main category: cs.LG

TL;DR: A machine learning-based Residual Stress Generator (RSG) is proposed to infer full-field residual stresses from limited measurements, reducing experimental effort while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Residual stresses degrade performance, and full-field characterization is impractical due to high experimental effort.

Method: A U-Net-based ML model was trained on a dataset from process simulations, then tested on actual characterization data.

Result: The model achieved excellent predictive accuracy and generalization, successfully learning residual stress distribution patterns.

Conclusion: The RSG provides a feasible solution for comprehensive residual stress analysis from limited measurements, significantly cutting experimental work.

Abstract: Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [381] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/pdf/2506.08216)
*Shahaf Bassan, Guy Amir, Meirav Zehavi, Guy Katz*

Main category: cs.LG

TL;DR: The paper explores why ensemble models are hard to interpret, using computational complexity theory to analyze factors like the number, size, and type of base models.


<details>
  <summary>Details</summary>
Motivation: Despite the common belief that ensembles are less interpretable, there's no rigorous understanding of why. This work aims to mathematically explain what makes ensembles (un)interpretable.

Method: The study applies computational complexity theory to analyze the interpretability of ensembles, focusing on factors like base model count, size, and type.

Result: Findings show interpretability varies: small tree ensembles are interpretable, but even constant-sized linear model ensembles are intractable.

Conclusion: The work provides a theoretical foundation for ensemble interpretability, highlighting the value of a computational complexity perspective.

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [382] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/pdf/2506.08226)
*Arthur Feeney, Kuei-Hsiang Huang, Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: Mondrian introduces transformer operators for scalable PDE modeling by decomposing domains into subdomains, decoupling attention from discretization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling transformer-based operator models for high-resolution, multiscale PDE domains due to quadratic attention costs.

Method: Decomposes domains into non-overlapping subdomains, applies attention over subdomain-restricted functions, and uses neural operators within subdomains.

Result: Achieves strong performance on Allen-Cahn and Navier-Stokes PDEs with resolution scaling without retraining.

Conclusion: Domain-decomposed attention shows promise for scalable and general-purpose neural operators.

Abstract: Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [383] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/pdf/2506.08228)
*Mustafa Baniodeh, Kratarth Goel, Scott Ettinger, Carlos Fuertes, Ari Seff, Tim Shen, Cole Gulino, Chenjie Yang, Ghassen Jerfel, Dokook Choe, Rui Wang, Vinutha Kallem, Sergio Casas, Rami Al-Rfou, Benjamin Sapp, Dragomir Anguelov*

Main category: cs.LG

TL;DR: The paper explores scaling laws for encoder-decoder transformer models in autonomous driving, showing performance improves with compute and data scaling, and highlights the importance of optimizing training and inference-time compute.


<details>
  <summary>Details</summary>
Motivation: To understand how scaling compute, model size, and data impacts performance in joint motion forecasting and planning for autonomous driving, and to assess the suitability of open-loop metrics.

Method: Empirical study using a 500K-hour driving dataset, analyzing power-law scaling of compute, model size, and data, and comparing open-loop and closed-loop metrics.

Result: Performance improves with compute scaling; optimal model size grows 1.5x faster than dataset size. Smaller models with sampling/clustering can compete with larger ones up to a point.

Conclusion: Optimizing scaling properties is key for performance. Training on general logged data can address data scarcity for large models.

Abstract: We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [384] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/pdf/2506.08231)
*Melissa Estevez, Nisha Singh, Lauren Dyson, Blythe Adamson, Qianyu Yuan, Megan W. Hildner, Erin Fidyk, Olive Mbah, Farhad Khan, Kathi Seidl-Rathkopf, Aaron B. Cohen*

Main category: cs.LG

TL;DR: A framework for evaluating LLM-extracted clinical data in oncology, addressing reliability, accuracy, and fairness through benchmarking, verification, and bias assessment.


<details>
  <summary>Details</summary>
Motivation: LLMs improve EHR data extraction but introduce reliability and fairness challenges, requiring a robust quality assurance framework.

Method: Proposes a multidimensional framework: benchmarking against human abstraction, automated checks, replication analyses, and bias assessment.

Result: Identifies variables needing improvement, detects errors, and confirms dataset fitness-for-purpose, while assessing bias.

Conclusion: The framework enhances LLM-extracted RWD quality, supporting trustworthy AI use in oncology research and practice.

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [385] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/pdf/2506.08240)
*Dongkyu Cho, Rumi Chunara*

Main category: cs.LG

TL;DR: The paper revisits random augmentation, addressing its shortcomings by mitigating feature distortion caused by colliding augmentations, and proposes a simple solution to improve its generalization effect.


<details>
  <summary>Details</summary>
Motivation: Random augmentation is inexpensive but suboptimal due to limited generalization effects. The paper aims to address its shortcomings, particularly the distortion of learned features caused by colliding augmentations.

Method: The authors propose a solution to mitigate the forgetting effect in random augmentation, enhancing its generalization performance.

Result: The proposed method demonstrates strong generalization performance across various single source domain generalization (sDG) benchmarks.

Conclusion: The study shows that addressing the forgetting issue in random augmentation can significantly improve its effectiveness for out-of-distribution generalization.

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [386] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/pdf/2506.08243)
*Zhenjiang Mao, Artem Bisliouk, Rohith Reddy Nama, Ivan Ruchkin*

Main category: cs.LG

TL;DR: A framework using Signal Temporal Logic (STL) improves LLM confidence calibration in mathematical reasoning by modeling stepwise confidence and enforcing temporal properties.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce incorrect but confident outputs, risking reliability in domains like education where users may lack expertise to verify reasoning.

Method: Proposes a structured framework modeling stepwise confidence as a temporal signal, evaluated via STL. Introduces uncertainty reshaping strategies for smoothness, monotonicity, and causal consistency.

Result: Improves calibration metrics and provides more reliable uncertainty estimates compared to conventional methods.

Conclusion: The STL-based framework enhances LLM confidence calibration, offering structured and interpretable uncertainty estimates for safer deployment.

Abstract: Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [387] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/pdf/2506.08244)
*Riccardo Ali, Pietro Liò, Jamie Vicary*

Main category: cs.LG

TL;DR: A zero-parameter method for approximate equivariance in neural networks, outperforming existing methods in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing equivariant methods are computationally intensive and architecture-specific, prompting a simpler, more flexible solution.

Method: Proposes a zero-parameter approach using an additional loss term to impose approximate equivariance in latent space, learning group representations.

Result: The method learns the regular representation and achieves comparable or better performance with fewer parameters on three datasets.

Conclusion: The approach offers a lightweight, effective alternative to traditional equivariant methods, enhancing flexibility and efficiency.

Abstract: Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [388] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/pdf/2506.08255)
*Patryk Krukowski, Łukasz Gorczyca, Piotr Helm, Kamil Książek, Przemysław Spurek*

Main category: cs.LG

TL;DR: SHIELD combines hypernetwork-based continual learning with interval arithmetic to tackle catastrophic forgetting and adversarial attacks simultaneously.


<details>
  <summary>Details</summary>
Motivation: Address the dual challenges of catastrophic forgetting in continual learning and vulnerability to adversarial attacks, which current models fail to solve together.

Method: Uses a hypernetwork to generate task-specific target models with interval arithmetic for robust predictions within defined ranges.

Result: Provides strict guarantees against adversarial attacks while maintaining adaptability across tasks.

Conclusion: SHIELD successfully integrates security and adaptability in continual learning, solving a previously unaddressed challenge.

Abstract: Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [389] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/pdf/2506.08266)
*Yaswanth Chittepu, Blossom Metevier, Will Schwarzer, Austin Hoag, Scott Niekum, Philip S. Thomas*

Main category: cs.LG

TL;DR: HC-RLHF is a method for aligning language models with high-confidence safety guarantees while maximizing helpfulness, outperforming previous methods in sensitive domains.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat safety as a tradeoff against helpfulness, leading to unreliable responses in sensitive domains. HC-RLHF aims to ensure safety without compromising helpfulness.

Method: HC-RLHF decouples human preferences into helpfulness (reward model) and harmlessness (cost model). It optimizes reward under a pessimistic cost constraint and verifies safety via a test.

Result: HC-RLHF aligns models (Qwen2-1.5B, Qwen2.5-3B, LLaMa3.2-3B) safely with high probability, improving harmlessness and helpfulness over prior methods.

Conclusion: HC-RLHF provides a reliable framework for safe language model alignment, backed by theoretical guarantees and empirical success.

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [390] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/pdf/2506.08267)
*Mansooreh Montazerin, Majd Al Aawar, Antonio Ortega, Ajitesh Srivastava*

Main category: cs.LG

TL;DR: LIES introduces a fixed neural network with interpretable activations for symbolic regression, outperforming existing methods in accuracy and sparsity.


<details>
  <summary>Details</summary>
Motivation: Existing SR methods lack scalability and symbolic consistency, prompting the need for a more efficient and interpretable approach.

Method: LIES uses a neural network with primitive activations, trained with oversampling and a tailored loss for sparsity, followed by pruning.

Result: LIES outperforms baselines in producing sparse and accurate symbolic formulae, validated by benchmarks and ablation studies.

Conclusion: The LIES framework effectively addresses scalability and consistency issues in SR, offering a robust solution for interpretable modeling.

Abstract: Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [391] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/pdf/2506.08270)
*Zitong Huang, Mansooreh Montazerin, Ajitesh Srivastava*

Main category: cs.LG

TL;DR: A novel framework jointly optimizes neural network architecture and weights using a continuous latent space, avoiding manual trial or NAS limitations.


<details>
  <summary>Details</summary>
Motivation: Manual trial and error is laborious, and NAS often separates architecture search from weight optimization. The goal is to unify these processes for efficiency.

Method: Train a universal multi-scale autoencoder to embed architectures and weights into a continuous latent space. Optimize via gradient descent with sparsity and compactness penalties.

Result: Effective discovery of sparse, compact neural networks with strong performance on synthetic regression tasks.

Conclusion: The proposed method successfully unifies architecture and weight optimization, offering a scalable and efficient alternative to traditional approaches.

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [392] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/pdf/2506.08272)
*Tarushri N. S.*

Main category: cs.LG

TL;DR: UDEs combine neural networks with physical differential equations for efficient, interpretable modeling of battery dynamics in smart grids, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in modeling battery dynamics due to solar input stochasticity and household load variability, where traditional methods lack generalization and miss residual dynamics.

Method: Proposes a UDE-based approach embedding a neural residual into a battery ODE, using synthetic solar and load data to learn stochastic corrections.

Result: UDE closely matches ground truth, shows smooth convergence, and maintains stability in long-term forecasts.

Conclusion: UDEs are viable for battery modeling in decentralized energy networks, with potential for real-time control in smart grids.

Abstract: Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [393] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/pdf/2506.08274)
*João Manoel Herrera Pinheiro, Suzana Vilas Boas de Oliveira, Thiago Henrique Segreto Silva, Pedro Antonio Rabelo Saraiva, Enzo Ferreira de Souza, Leonardo André Ambrosio, Marcelo Becker*

Main category: cs.LG

TL;DR: The paper evaluates 12 feature scaling techniques across 14 ML algorithms and 16 datasets, revealing that ensemble methods are robust to scaling, while others like Logistic Regression and SVMs are highly sensitive.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of comprehensive studies on feature scaling by systematically analyzing its impact on predictive performance and computational costs.

Method: Evaluates 12 scaling techniques on 14 ML algorithms and 16 datasets, measuring performance metrics (accuracy, MAE, MSE, $R^2$) and computational costs (training/inference time, memory usage).

Result: Ensemble methods (e.g., Random Forest, XGBoost) are largely unaffected by scaling, while models like Logistic Regression and SVMs show significant performance variations.

Conclusion: Provides model-specific guidance for selecting optimal feature scaling techniques, with all data and code made publicly available for transparency.

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [394] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/pdf/2506.08292)
*Xie Yi, Zhanke Zhou, Chentao Cao, Qiyu Niu, Tongliang Liu, Bo Han*

Main category: cs.LG

TL;DR: ECON introduces a hierarchical reinforcement-learning paradigm for multi-LLM coordination, achieving better performance and scalability with a proven tighter regret bound.


<details>
  <summary>Details</summary>
Motivation: Multi-agent frameworks enhance LLM reasoning but face computational costs and lack convergence guarantees.

Method: Recasts multi-LLM coordination as an incomplete-information game, seeking Bayesian Nash equilibrium (BNE) with distributed reasoning and centralized output.

Result: Outperforms existing multi-LLM approaches by 11.2% on benchmarks and demonstrates scalability.

Conclusion: ECON offers a scalable, efficient solution for multi-LLM coordination with theoretical and empirical advantages.

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [395] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/pdf/2506.08295)
*Zhanke Zhou, Xiao Feng, Zhaocheng Zhu, Jiangchao Yao, Sanmi Koyejo, Bo Han*

Main category: cs.LG

TL;DR: AR-Bench is a new benchmark evaluating LLMs' active reasoning skills, revealing their struggles compared to passive reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on passive reasoning, neglecting active reasoning where LLMs interact with external systems.

Method: AR-Bench includes detective cases, situation puzzles, and guessing numbers to test commonsense, logical, and symbolic reasoning.

Result: Current LLMs perform poorly in active reasoning, with limited improvement from advanced strategies.

Conclusion: The study underscores the need for better methodologies, like interactive learning, to enhance active reasoning in LLMs.

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [396] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/pdf/2506.08298)
*Trung-Kien Nguyen, Heng Ping, Shixuan Li, Peiyu Zhang, Nikos Kanakaris, Nicholas Kotov, Paul Bogdan*

Main category: cs.LG

TL;DR: The paper introduces H²GFM, a framework for generalizing Graph Foundation Models (GFMs) across homogeneous and heterogeneous text-attributed graphs (TAGs), using a context-adaptive graph transformer and mixture of experts for robust node representations.


<details>
  <summary>Details</summary>
Motivation: Existing GFMs focus on homogeneous TAGs, leaving heterogeneous TAGs underexplored. The goal is to enhance GFM capabilities by addressing both graph types.

Method: H²GFM projects meta-relations into a unified textual space, uses context encoding for semantic relationships, and employs a context-adaptive graph transformer (CGT) with a mixture of experts for structural heterogeneity.

Result: Comprehensive experiments show H²GFM's effectiveness across diverse TAGs and learning scenarios.

Conclusion: H²GFM successfully generalizes GFMs to both homogeneous and heterogeneous TAGs, improving robustness and applicability.

Abstract: The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [397] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/pdf/2506.08309)
*Katherine Tieu, Dongqi Fu, Zihao Li, Ross Maciejewski, Jingrui He*

Main category: cs.LG

TL;DR: The paper introduces L-STEP, a learnable spatial-temporal positional encoding method for graphs, addressing limitations of current positional encoding techniques and demonstrating superior performance in temporal link prediction.


<details>
  <summary>Details</summary>
Motivation: Current positional encoding methods are limited by pre-defined functions, lack of adaptability to evolving graph structures, and inefficiency with large-scale data.

Method: Proposes L-STEP, a learnable positional encoding scheme, validated through spatial-temporal spectral analysis, MLP-based expressiveness, and robustness tests.

Result: L-STEP outperforms 10 algorithms on 13 datasets, achieves leading performance in the TGB benchmark, and reduces empirical running time.

Conclusion: L-STEP effectively addresses the limitations of existing positional encoding methods, offering a scalable and efficient solution for graph learning tasks.

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [398] [Private Evolution Converges](https://arxiv.org/pdf/2506.08312)
*Tomás González, Giulia Fanti, Aaditya Ramdas*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [399] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/pdf/2506.08316)
*Alan N. Amin, Nate Gruver, Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: Masking diffusion outperforms other discrete diffusion models by leveraging the known distribution of jump times in discrete Markov processes. The proposed SCUD framework generalizes this idea, improving performance across various data types.


<details>
  <summary>Details</summary>
Motivation: To explain why masking diffusion performs better than other discrete diffusion models and to generalize its advantages to other models.

Method: Introduces schedule-conditioned discrete diffusion (SCUD), which incorporates the known distribution of jump times into any discrete diffusion model.

Result: SCUD models outperform masking diffusion when applied to images, text, and protein data.

Conclusion: Baking in the known distribution of jump times enhances discrete diffusion models, making SCUD a superior framework.

Abstract: Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [400] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/pdf/2506.08326)
*Xingbo Fu, Zehong Wang, Zihan Chen, Jiazheng Li, Yaochen Zhu, Zhenyu Lei, Cong Shen, Yanfang Ye, Chuxu Zhang, Jundong Li*

Main category: cs.LG

TL;DR: A review of graph prompting, covering pre-training methods, prompting techniques, applications, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To systematically explore the advancements in graph prompting, a promising approach for adapting pre-trained graph learning models to downstream tasks.

Method: Review of graph pre-training methods and analysis of how learnable prompts are designed in graph prompting.

Result: Summarized real-world applications and identified open challenges in graph prompting.

Conclusion: Graph prompting is a promising field with potential for further research and development.

Abstract: Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [401] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/pdf/2506.08337)
*Juhyeok Choi, Chenglin Fan*

Main category: cs.LG

TL;DR: A simplified theoretical framework for analyzing Euler-Maruyama discretization in DDPMs, showing convergence rate of O(1/T^(1/2)) and validating discrete noise substitution.


<details>
  <summary>Details</summary>
Motivation: Existing analyses of diffusion models' discretization error rely on complex tools, prompting a need for a simplified approach.

Method: Uses Grönwall's inequality to derive convergence rates and explores replacing Gaussian noise with discrete random variables.

Result: Achieves O(1/T^(1/2)) convergence, validates discrete noise substitution, and shows incorrect scaling degrades performance.

Conclusion: Bridges theoretical rigor with practical efficiency in diffusion models, offering insights for efficient sampling.

Abstract: Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [402] [Dynamical System Optimization](https://arxiv.org/pdf/2506.08340)
*Emo Todorov*

Main category: cs.LG

TL;DR: The paper introduces an optimization framework where policy parameters are optimized autonomously, bypassing traditional control or reinforcement learning methods.


<details>
  <summary>Details</summary>
Motivation: To simplify policy optimization by treating it as an autonomous system, avoiding complex control or reinforcement learning machinery.

Method: Derives algorithms at the autonomous system level, showing equivalence to policy gradients, Hessians, natural gradients, and proximal methods.

Result: Demonstrates that the framework computes the same quantities as traditional methods and extends to behavioral cloning, system identification, and generative AI tuning.

Conclusion: The framework unifies policy and system parameter optimization, offering a simpler and broader alternative to reinforcement learning.

Abstract: We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [403] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/pdf/2506.08347)
*Yinan Huang, Haoteng Ying, Eli Chien, Rongzhe Wei, Pan Li*

Main category: cs.LG

TL;DR: A framework for relational learning with formal entity-level differential privacy guarantees, addressing challenges in sensitivity and privacy amplification for DP-SGD.


<details>
  <summary>Details</summary>
Motivation: Protecting privacy in relational and network-structured data is critical, but applying DP-SGD directly is challenging due to high sensitivity and interdependent sampling.

Method: Proposes an adaptive gradient clipping scheme and extends privacy amplification to coupled sampling, resulting in a tailored DP-SGD variant.

Result: Demonstrates strong utility-privacy trade-offs in experiments on text-attributed network data.

Conclusion: The framework effectively addresses relational learning challenges with provable privacy guarantees.

Abstract: Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [404] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/pdf/2506.08353)
*Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko*

Main category: cs.LG

TL;DR: AdaAct is an optimization algorithm that adjusts learning rates based on activation variance, improving stability and generalization in neural networks.


<details>
  <summary>Details</summary>
Motivation: To enhance neuron output stability and generalization by incorporating neuron-wise adaptivity, bridging the gap between Adam's convergence speed and SGD's generalization.

Method: AdaAct adjusts learning rates according to activation variance during training, complementing conventional activation regularization.

Result: Competitive performance on CIFAR and ImageNet benchmarks, bridging Adam's speed and SGD's generalization while maintaining efficiency.

Conclusion: AdaAct effectively balances convergence speed and generalization, offering a practical alternative to existing methods.

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [405] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/pdf/2506.08360)
*Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko*

Main category: cs.LG

TL;DR: NysAct is a scalable first-order gradient preconditioning method that balances efficiency and generalization, outperforming both first-order and second-order methods in test accuracy while using fewer resources.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between computational efficiency (first-order methods) and generalization (second-order methods) in optimization.

Method: Uses an eigenvalue-shifted Nystrom method to approximate the activation covariance matrix for preconditioning, reducing time and memory costs.

Result: NysAct achieves better test accuracy than first-order and second-order methods with lower computational demands.

Conclusion: NysAct effectively bridges the gap between first-order and second-order methods, offering a practical and efficient solution.

Abstract: Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [406] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/pdf/2506.08365)
*Cheng Tan, Zhenxiao Cao, Zhangyang Gao, Siyuan Li, Yufei Huang, Stan Z. Li*

Main category: cs.LG

TL;DR: The paper introduces DeSAE to debias AlphaFold structures, improving their utility for tasks like inverse folding by aligning them closer to experimental PDB structures.


<details>
  <summary>Details</summary>
Motivation: AlphaFold structures (AFDB) have biases that limit their use in fine-grained tasks like inverse folding, unlike experimental PDB structures which capture natural variability.

Method: A Debiasing Structure AutoEncoder (DeSAE) is trained to reconstruct native-like conformations from corrupted backbones, learning a more natural structural manifold.

Result: DeSAE debiased AFDB structures enhance inverse folding performance across benchmarks, addressing systematic biases.

Conclusion: Debiasing predicted structures like AFDB is crucial for structure-based learning, and DeSAE provides an effective framework for this.

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [407] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/pdf/2506.08379)
*Yurun Yuan, Tengyang Xie*

Main category: cs.LG

TL;DR: DPSDP, a reinforcement learning algorithm, improves LLM reasoning by iterative refinement via direct preference learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing verify-and-improve methods for LLMs have limited feedback and lack coordination, leading to suboptimal performance.

Method: Model refinement as a Markov Decision Process; introduce DPSDP to train an actor-critic LLM system for iterative answer refinement.

Result: DPSDP improves accuracy (e.g., from 58.2% to 63.2% on MATH 500) and shows benefits of multi-agent collaboration.

Conclusion: DPSDP effectively enhances LLM reasoning through coordinated, iterative refinement and generalizes well.

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [408] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/pdf/2506.08383)
*Jiaqi Chen, Rongbin Ye*

Main category: cs.LG

TL;DR: The paper presents a machine learning-based approach for detecting malicious IoT traffic, using the IoT-23 dataset and addressing class imbalance with resampling strategies. Ensemble methods, especially gcForest, outperform traditional techniques.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of IoT networks necessitates real-time malware detection to enhance cybersecurity, particularly for critical infrastructure.

Method: The study employs machine learning techniques on the IoT-23 dataset, using three resampling strategies to handle class imbalance, and compares their performance.

Result: Ensemble methods, particularly gcForest, combined with imbalance treatment, achieve superior detection performance over traditional methods.

Conclusion: The research advances intelligent, efficient automated threat detection for IoT, improving security and resource optimization.

Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [409] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/pdf/2506.08388)
*Edoardo Cetin, Tianyu Zhao, Yujin Tang*

Main category: cs.LG

TL;DR: The paper introduces Reinforcement-Learned Teachers (RLTs), a framework for training reasoning LMs to improve downstream distillation without relying on RL's exploration challenges.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RL for reasoning LMs, particularly the need for exploration at initialization and the focus on distillation for future RL iterations.

Method: RLTs are trained with dense rewards by prompting them with questions and solutions, requiring them to generate detailed explanations for students.

Result: A 7B RLT outperforms larger LMs in distillation and cold-starting pipelines, showing effectiveness with larger students and out-of-distribution tasks.

Conclusion: RLTs enhance efficiency and re-usability in RL reasoning frameworks, offering superior performance for distillation and generalization.

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [410] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/pdf/2506.08397)
*Vamshika Sutar, Amandeep Singh, Rohitash Chandra*

Main category: cs.LG

TL;DR: The paper evaluates deep learning, ensemble learning, and data augmentation to detect cyclone rapid intensification, addressing class imbalance with synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: Rapid intensification in cyclones is rare and complex, causing class imbalance and challenges for conventional models.

Method: Uses deep learning for synthetic data generation and classification to differentiate rapid and non-rapid intensification events.

Result: Data augmentation improves detection, with spatial coordinates being critical input features.

Conclusion: The framework advances synthetic data generation for spatiotemporal data with extreme events.

Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [411] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/pdf/2506.08409)
*Fred Xu, Song Jiang, Zijie Huang, Xiao Luo, Shichang Zhang, Adrian Chen, Yizhou Sun*

Main category: cs.LG

TL;DR: FUSE is a novel fuzzy set embedding framework for taxonomy expansion, improving performance by up to 23% over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing set representations (vectors, boxes) lack closure under set operations and struggle with uncertainty in concept modeling.

Method: Proposes FUSE, a volume approximation of fuzzy sets, ensuring closure under set operations and efficiency with minimal neural architecture.

Result: FUSE achieves up to 23% improvement in taxonomy expansion tasks compared to existing methods.

Conclusion: FUSE is the first efficient fuzzy set embedding framework, preserving information and enabling set operations.

Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [412] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/pdf/2506.08412)
*Saraa Ali, Aleksandr Khizhik, Stepan Svirin, Artem Ryzhikov, Denis Derkach*

Main category: cs.LG

TL;DR: The paper introduces Signature-Guided Data Augmentation (SGDA), a hybrid ML and unsupervised method for diagnosing three-phase engine faults, improving accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Traditional signature analysis methods for engine diagnostics can be enhanced with ML, but lack realistic anomaly generation. The study aims to bridge this gap.

Method: Combines ML with unsupervised anomaly generation using engine physics. SGDA synthesizes faults in the frequency domain of healthy signals, guided by Motor Current Signature Analysis.

Result: Achieves superior diagnostic accuracy and reliability, offering a robust solution for industrial applications.

Conclusion: SGDA significantly advances engine diagnostics by efficiently generating realistic anomalies and improving performance.

Abstract: The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [413] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/pdf/2506.08415)
*Licong Lin, Jingfeng Wu, Peter L. Bartlett*

Main category: cs.LG

TL;DR: Data reuse improves neural scaling laws in linear regression, showing better test error bounds with multi-pass SGD compared to one-pass SGD.


<details>
  <summary>Details</summary>
Motivation: Address the unsustainability of neural scaling laws when new data is exhausted by exploring data reuse.

Method: Derive test error bounds for multi-pass SGD on linear models with sketched features, assuming power-law spectra for data covariance and true parameters.

Result: Multi-pass SGD achieves better test error scaling (Θ(M^{1−b} + L^{(1−b)/a})) than one-pass SGD (Θ(M^{1−b} + N^{(1−b)/a})).

Conclusion: Data reuse via multi-pass SGD offers improved scaling laws in data-constrained scenarios, validated by simulations.

Abstract: Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [414] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/pdf/2506.08417)
*Qingmao Yao, Zhichao Lei, Tianyuan Chen, Ziyue Yuan, Xuefan Chen, Jianxiang Liu, Faguo Wu, Xiao Zhang*

Main category: cs.LG

TL;DR: The paper introduces a novel approach to improve $Q$-value estimation in offline RL by enhancing generalization in OOD regions, using a Smooth Bellman Operator (SBO) and achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Offline RL suffers from $Q$-value overestimation for OOD actions due to distributional shifts, and existing methods are overly conservative, limiting $Q$-function generalization.

Method: Proposes the Smooth Bellman Operator (SBO) to update OOD $Q$-values by smoothing them with neighboring in-sample values, ensuring safety within the Convex Hull and its Neighborhood (CHN).

Result: SQOG, the practical algorithm, achieves near-accurate $Q$-value estimation and outperforms state-of-the-art methods on D4RL benchmarks.

Conclusion: The approach effectively addresses the over-constraint issue, improving $Q$-value generalization and policy performance in offline RL.

Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [415] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/pdf/2506.08419)
*Ruichen Jiang, Ali Kavis, Aryan Mokhtari*

Main category: cs.LG

TL;DR: GALA dynamically adjusts learning rates by tracking gradient alignment and local curvature, eliminating the need for extensive hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning learning rates for large-scale deep learning models is time-consuming and requires extensive grid searches.

Method: GALA adjusts learning rates based on gradient alignment and local curvature, formulated as an online learning problem.

Result: GALA improves optimizer performance across various initial learning rates, reducing the need for tuning.

Conclusion: GALA provides a flexible, adaptive learning rate schedule, enhancing optimizer robustness without manual tuning.

Abstract: The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [416] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/pdf/2506.08426)
*Zheng Lin, Zhe Chen, Xianhao Chen, Wei Ni, Yue Gao*

Main category: cs.LG

TL;DR: HASFL adaptively controls batch sizes and model splitting to mitigate the straggler effect in split federated learning, improving performance in heterogeneous edge networks.


<details>
  <summary>Details</summary>
Motivation: Existing SFL approaches struggle with the straggler effect due to device heterogeneity, limiting efficiency.

Method: Proposes HASFL, a framework that adaptively adjusts batch sizes and model splitting based on a derived convergence bound.

Result: HASFL outperforms benchmarks, balancing latency and convergence in heterogeneous networks.

Conclusion: HASFL effectively addresses resource heterogeneity in SFL, enhancing learning performance.

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [417] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/pdf/2506.08441)
*Anh N. Nhu, Sanghyun Son, Ming Lin*

Main category: cs.LG

TL;DR: TAWM improves model performance and data efficiency by incorporating temporal dynamics and training over diverse time-step sizes.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of fixed time-step sampling in conventional models, which may miss high- or low-frequency dynamics.

Method: Introduces Time-Aware World Model (TAWM), conditioning on time-step size (Δt) and training over diverse Δt values.

Result: TAWM outperforms conventional models across varying observation rates in control tasks with the same training data.

Conclusion: TAWM's time-aware formulation enhances performance and efficiency by adapting to system dynamics.

Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [418] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/pdf/2506.08435)
*Mingyuan Fan, Fuyi Wang, Cen Chen, Jianying Zhou*

Main category: cs.LG

TL;DR: The paper demonstrates that gradient leakage attacks (GLAs) can effectively reconstruct clients' data in realistic federated learning (FL) environments, introducing FedLeak to address performance bottlenecks and highlighting FL's vulnerability.


<details>
  <summary>Details</summary>
Motivation: The debate on FL's privacy safeguards, with some arguing GLAs are ineffective in practical settings, motivates the need to empirically prove data reconstruction risks in realistic FL environments.

Method: The authors develop FedLeak, incorporating partial gradient matching and gradient regularization to overcome GLA limitations, and establish a practical evaluation protocol based on FL literature and industry practices.

Result: FedLeak achieves high-fidelity data reconstruction under realistic conditions, revealing significant vulnerabilities in FL systems.

Conclusion: The study underscores the urgent need for more effective defense mechanisms in FL due to demonstrated privacy risks.

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [419] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/pdf/2506.08438)
*Yuchen Wu, Xinyi Zhong, Zhuoran Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [420] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/pdf/2506.08460)
*Yihong Guo, Yu Yang, Pan Xu, Anqi Liu*

Main category: cs.LG

TL;DR: MOBODY is a Model-Based Off-Dynamics offline RL algorithm that leverages learned dynamics and representation learning to explore target domains beyond offline datasets, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing off-dynamics offline RL methods are limited by target domain transitions, restricting policy exploration. MOBODY aims to overcome this by generating synthetic transitions and learning shared latent representations.

Method: MOBODY uses model rollouts to create synthetic target-domain transitions, learns shared latent representations to handle mismatched dynamics, and employs a Q-weighted behavior cloning loss for stable training.

Result: MOBODY outperforms state-of-the-art baselines on MuJoCo benchmarks, especially in challenging scenarios.

Conclusion: MOBODY effectively addresses the limitations of existing methods by enabling target-domain exploration and leveraging representation learning, achieving superior performance.

Abstract: We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [421] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/pdf/2506.08463)
*Zhishuai Liu, Yu Yang, Ruhan Wang, Pan Xu, Dongruo Zhou*

Main category: cs.LG

TL;DR: Reinforced RCSL improves RCSL by introducing in-distribution optimal return-to-go, outperforming standard RCSL in benchmarks.


<details>
  <summary>Details</summary>
Motivation: RCSL lacks the stitching property, limiting performance based on dataset quality. Reinforced RCSL addresses this.

Method: Introduces in-distribution optimal return-to-go to identify achievable future returns without complex augmentation.

Result: Theoretical and empirical results show Reinforced RCSL outperforms standard RCSL.

Conclusion: Reinforced RCSL offers a simple, effective solution to RCSL's limitations, enhancing performance in sequential decision-making.

Abstract: In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [422] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/pdf/2506.08505)
*Shahaf Bassan, Yizhak Yisrael Elboher, Tobias Ladner, Matthias Althoff, Guy Katz*

Main category: cs.LG

TL;DR: Proposes an abstraction-refinement technique to efficiently compute provably sufficient explanations for neural network predictions, improving scalability.


<details>
  <summary>Details</summary>
Motivation: Current post-hoc explainability methods lack formal guarantees and face scalability issues when computing provable explanations.

Method: Uses an abstraction-refinement approach: constructs a reduced network for faster verification, iteratively refines if needed.

Result: Enhances efficiency in obtaining provably sufficient explanations and provides fine-grained interpretation across abstraction levels.

Conclusion: The method addresses scalability challenges while maintaining formal guarantees for explanations.

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [423] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/pdf/2506.08464)
*Hyunseok Seung, Jaewoo Lee, Hyunsuk Ko*

Main category: cs.LG

TL;DR: The paper proposes MAC, an efficient second-order optimization method for neural networks, improving upon KFAC by approximating Kronecker factors in the Fisher information matrix (FIM). MAC outperforms KFAC and other methods in accuracy, training time, and memory usage.


<details>
  <summary>Details</summary>
Motivation: Second-order methods like KFAC improve convergence but are computationally expensive. The paper aims to reduce this burden by analyzing and approximating components of the FIM.

Method: The authors analyze Kronecker factors (activations and gradients) in the FIM, propose efficient approximations, and apply them to attention layers in transformers, integrating attention scores into preconditioning.

Result: MAC outperforms KFAC and other state-of-the-art methods in accuracy, training time, and memory usage across various architectures and datasets.

Conclusion: MAC is a computationally efficient second-order optimization method with proven convergence properties, offering superior performance for training neural networks.

Abstract: Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [424] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/pdf/2506.08533)
*Nihal Acharya Adde, Alexandra Gianzina, Hanno Gottschalk, Andreas Ebert*

Main category: cs.LG

TL;DR: EMNAS uses genetic algorithms to optimize neural networks for RL in autonomous driving, improving rewards and reducing model size. Parallelization and teacher-student methods enhance scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To automate and optimize neural network design for RL in autonomous driving, improving performance and efficiency.

Method: Genetic algorithms for architecture search, parallelization for speed, and teacher-student methods for scalable optimization.

Result: EMNAS outperforms manual designs, achieving higher rewards with fewer parameters.

Conclusion: EMNAS advances RL for autonomous driving, offering better-performing networks for real-world applications.

Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [425] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/pdf/2506.08473)
*Shuo Yang, Qihui Zhang, Yuyang Liu, Yue Huang, Xiaojun Jia, Kunpeng Ning, Jiayu Yao, Jigang Wang, Hailiang Dai, Yibing Song, Li Yuan*

Main category: cs.LG

TL;DR: AsFT (Anchoring Safety in Fine-Tuning) is a method to enhance LLM safety during fine-tuning by using alignment direction as an anchor to suppress harmful updates, outperforming Safe LoRA in reducing harmful behavior and improving performance.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to safety risks during fine-tuning, where even small malicious or harmless data can compromise safeguards.

Method: Proposes AsFT, which integrates a regularization term using alignment direction to suppress harmful updates, constraining fine-tuning within a narrow safety basin.

Result: AsFT reduces harmful behavior by 7.60%, improves model performance by 3.44%, and maintains robustness across settings.

Conclusion: AsFT effectively anchors safety in fine-tuning, outperforming existing methods and ensuring robust performance.

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [426] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/pdf/2506.08475)
*Xiaolong He, Yeonjong Shin, Anthony Gruber, Sohyeon Jung, Kookjin Lee, Youngsoo Choi*

Main category: cs.LG

TL;DR: The paper introduces tLaSDI, a thermodynamics-informed framework for reduced-order modeling of nonlinear systems, combining autoencoders and pGFINNs to preserve thermodynamic principles while achieving high efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently modeling parametric nonlinear dynamical systems while preserving thermodynamic principles like energy conservation and entropy generation.

Method: Integrates autoencoders for dimensionality reduction with pGFINNs for learning parametric latent dynamics, and uses physics-informed active learning for adaptive data sampling.

Result: Achieves up to 3,528x speed-up with 1-3% relative errors, reduces training (50-90%) and inference (57-61%) costs, and reveals thermodynamic behavior in latent dynamics.

Conclusion: tLaSDI is a highly efficient and accurate framework for reduced-order modeling, offering insights into thermodynamic behavior and outperforming traditional methods.

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [427] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/pdf/2506.08514)
*Jacob Piland, Chris Sweet, Adam Czakja*

Main category: cs.LG

TL;DR: The paper introduces SHAMs for testing CAM robustness and DiffGradCAM, a contrastive method resistant to passive fooling, improving saliency-based explanations.


<details>
  <summary>Details</summary>
Motivation: Standard CAM methods focus on individual logits, ignoring logit differences, making them vulnerable to adversarial manipulation like passive fooling.

Method: Proposes SHAMs for benchmarking CAM robustness and DiffGradCAM, a lightweight, contrastive approach resistant to passive fooling.

Result: Validated across multi-class tasks, SHAMs expose vulnerabilities, while DiffGradCAM matches standard CAMs in non-adversarial cases and resists manipulation.

Conclusion: SHAM and DiffGradCAM form a framework for robust saliency-based explanations, addressing CAM vulnerabilities.

Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [428] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/pdf/2506.08572)
*Waiss Azizian, Michael Kirchhof, Eugene Ndiaye, Louis Bethune, Michal Klein, Pierre Ablin, Marco Cuturi*

Main category: cs.LG

TL;DR: The paper highlights that the 'geometry of truth' in LLM activations is task-specific and doesn't transfer well across tasks, limiting reliability assessments.


<details>
  <summary>Details</summary>
Motivation: To address concerns about LLM reliability by examining if activations can universally distinguish correct answers.

Method: Analyzing linear classifiers trained on activations across tasks, showing their dissimilarity and disjoint supports.

Result: Task-specific 'geometries of truth' don't generalize; sophisticated methods also fail due to separated activation clusters.

Conclusion: Reliability assessments based on activations are inherently task-dependent, limiting broader applicability.

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [429] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/pdf/2506.08516)
*Mouadh Yagoubi, David Danan, Milad Leyli-Abadi, Ahmed Mazari, Jean-Patrick Brunet, Abbas Kabalan, Fabien Casenave, Yuxin Ma, Giovanni Catalani, Jean Fesquet, Jacob Helwig, Xuan Zhang, Haiyang Yu, Xavier Bertrand, Frederic Tost, Michael Baurheim, Joseph Morlier, Shuiwang Ji*

Main category: cs.LG

TL;DR: The ML4CFD competition benchmarked ML-based surrogate models for CFD, showing top entries outperforming traditional solvers like OpenFOAM in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in ML accuracy, generalization, and physical consistency for scientific applications like CFD.

Method: Organized a competition (ML4CFD) with 240 teams, using OpenFOAM-generated data and a multi-criteria evaluation framework.

Result: Top ML models surpassed OpenFOAM in aggregate metrics, demonstrating potential for ML surrogates in scientific simulations.

Conclusion: The competition highlights ML's promise for CFD and provides insights for future scientific ML challenges.

Abstract: The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [430] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/pdf/2506.08577)
*Nicholas A. Pearson, Francesca Cairoli, Luca Bortolussi, Davide Russo, Francesca Zanello*

Main category: cs.LG

TL;DR: A deep learning model using generative AI improves sewerage system forecasting by capturing complex correlations in time series data, with conformal inference ensuring reliable predictions.


<details>
  <summary>Details</summary>
Motivation: Enhancing the accuracy of contextual forecasting in sewerage systems, especially during extreme weather events.

Method: Develops a diffusion-based model for multivariate time series data and calibrates predictions with conformal inference for reliability.

Result: Empirical tests show the model delivers reliable predictions even under severe weather conditions.

Conclusion: The model excels in robust and accurate contextual forecasting for sewerage systems.

Abstract: We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [431] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/pdf/2506.08523)
*Pedro Jiménez-González, Miguel C. Soriano, Lucas Lacasa*

Main category: cs.LG

TL;DR: The paper explores how large learning rates in gradient descent (GD) can shift neural network training from pure exploitation to an exploration-exploitation balance, characterized by chaotic dynamics. This regime accelerates training, as shown in MNIST and other tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the role of learning rates in GD dynamics and how chaotic behavior can improve neural network training efficiency.

Method: Analyze GD dynamics for large learning rates, measuring Lyapunov exponents to characterize chaos and evaluating training time and accuracy.

Result: For certain learning rates, GD exhibits chaotic dynamics, balancing exploration and exploitation, leading to faster training without sacrificing accuracy.

Conclusion: Transient chaotic dynamics in GD can enhance neural network training, suggesting optimal learning rates at the onset of chaos.

Abstract: Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [432] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/pdf/2506.08551)
*Panlong Wu, Ting Wang, Yifei Zhong, Haoqi Zhang, Zitong Wang, Fangxin Wang*

Main category: cs.LG

TL;DR: DeepForm is a specialized LLM for automated communication system formulation, trained using a two-stage strategy (SFT and C-ReMax RL) on a curated dataset (CSFRC), outperforming general-purpose LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing general-purpose LLMs lack specialized domain knowledge and nuanced reasoning for communication system formulation, necessitating a tailored solution.

Method: Two-stage training: SFT with CoT data for domain knowledge, followed by C-ReMax RL for advanced reasoning. Uses the CSFRC dataset.

Result: DeepForm achieves state-of-the-art performance, surpassing larger proprietary LLMs in diverse scenarios.

Conclusion: DeepForm bridges the gap in specialized LLMs for communication systems, with plans to release resources for further research.

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [433] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/pdf/2506.08574)
*Alvise Dei Rossi, Matteo Metaldi, Michal Bechny, Irina Filchenko, Julia van der Meer, Markus H. Schmidt, Claudio L. A. Bassetti, Athina Tzovara, Francesca D. Faraci, Luigi Fiorillo*

Main category: cs.LG

TL;DR: SLEEPYLAND is an open-source sleep staging framework addressing evaluation challenges, generalization, and bias. SOMNUS, an ensemble model, outperforms individual models and human scorers, even in OOD scenarios, while also quantifying biases and predicting scorer disagreement.


<details>
  <summary>Details</summary>
Motivation: Clinical adoption of deep learning for sleep staging is limited by evaluation fairness, generalization, model bias, and annotation variability.

Method: SLEEPYLAND provides standardized evaluation with extensive datasets (ID and OOD). SOMNUS, an ensemble of models, is introduced and evaluated across diverse datasets and configurations.

Result: SOMNUS achieves robust performance (macro-F1 68.7%-87.2%), outperforms individual models (94.9% cases), and surpasses human scorers in consensus reproduction. Bias analysis reveals no consistent minimization across models.

Conclusion: SLEEPYLAND and SOMNUS advance sleep staging by improving evaluation, generalization, and bias awareness, with ensemble disagreement metrics offering insights into human uncertainty.

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [434] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/pdf/2506.08604)
*Giacomo Baldan, Qiang Liu, Alberto Guardone, Nils Thuerey*

Main category: cs.LG

TL;DR: PBFM embeds physical constraints into flow matching, improving accuracy and reducing residuals without hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing generative methods learn physics implicitly; PBFM explicitly incorporates physical constraints for better accuracy.

Method: PBFM combines flow matching loss with physics-based residual loss and uses temporal unrolling for training.

Result: PBFM achieves up to 8× better physical residuals and outperforms existing methods in distributional accuracy.

Conclusion: PBFM offers an efficient framework for surrogate modeling and simulation in physics and engineering.

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [435] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/pdf/2506.08600)
*Hiroshi Kera, Shun Arakawa, Yuta Sato*

Main category: cs.LG

TL;DR: Transformer models can learn symbolic computation via deep learning, and CALT is introduced as a Python library to facilitate training such models.


<details>
  <summary>Details</summary>
Motivation: To leverage deep learning for symbolic computation and make it accessible to non-experts.

Method: Uses Transformer models trained on examples of symbolic expressions to emulate computations.

Result: Development of CALT, a Python library for training symbolic computation models.

Conclusion: CALT enables non-experts to harness deep learning for symbolic computation, opening new research avenues.

Abstract: Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [436] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/pdf/2506.08618)
*Xianquan Yan, Hakan Akgün, Kenji Kawaguchi, N. Duane Loh, Ching Hua Lee*

Main category: cs.LG

TL;DR: HSG-12M introduces the first large-scale dataset of spatial multigraphs, retaining distinct paths as separate edges, derived from spectral potential data. It includes 11.6M static and 5.1M dynamic graphs, enabling geometry-aware learning and scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks oversimplify graphs by collapsing distinct paths into single links, limiting their utility for spatial and geometric analysis. HSG-12M addresses this gap by preserving geometric diversity.

Method: The dataset is built using Hamiltonian spectral graphs from 1-D crystal energy spectra, processed via the Poly2Graph pipeline. It includes 1401 polynomial classes and leverages 177 TB of spectral data.

Result: HSG-12M provides diverse, physics-grounded graph topologies, revealing challenges in learning from multi-edge geometry. It also serves as universal topological fingerprints for polynomials and matrices.

Conclusion: HSG-12M advances geometry-aware graph learning and opens new avenues for scientific discovery in physics and beyond, supported by the open-source Poly2Graph pipeline.

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [437] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/pdf/2506.08607)
*Kiran Purohit, V Venktesh, Sourangshu Bhattacharya, Avishek Anand*

Main category: cs.LG

TL;DR: CASE is a sample-efficient method for selecting top exemplars in LLM prompts, reducing runtime and LLM calls by 7x without performance loss.


<details>
  <summary>Details</summary>
Motivation: Efficient exemplar selection is crucial for LLM performance under context-length constraints, but existing methods are computationally expensive.

Method: Formulates exemplar selection as a top-m best arms problem, using a selective exploration strategy (CASE) with a linear scoring function.

Result: Achieves 7x speedup and 87% fewer LLM calls compared to state-of-the-art methods.

Conclusion: CASE is a highly efficient and effective solution for exemplar selection in LLM prompts.

Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [438] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/pdf/2506.08641)
*Simon Roschmann, Quentin Bouniot, Vasilii Feofanov, Ievgen Redko, Zeynep Akata*

Main category: cs.LG

TL;DR: TiViT converts time series to images for use with pretrained Vision Transformers (ViTs), achieving state-of-the-art performance in time series classification by leveraging OpenCLIP models.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of time series datasets by reusing pretrained vision models for time series classification.

Method: Proposes TiViT, converting time series into images to utilize frozen ViTs, analyzing 2D patching and leveraging OpenCLIP representations.

Result: TiViT achieves top performance on benchmarks, with intermediate layers of high intrinsic dimension being most effective. Combining TiViT and TSFM features yields further gains.

Conclusion: TiViT demonstrates the potential of reusing vision representations for non-visual tasks, offering a new direction for time series classification.

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [439] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/pdf/2506.08652)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: JoFormer is a journey-based Transformer architecture using non-commutative algebra for positional information, outperforming RoFormer in perplexity and convergence.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively incorporating positional information in Transformers.

Method: Introduces JoFormer with learnable directional transforms for relative positions, generalizing existing approaches.

Result: JoFormer achieves lower perplexity and faster convergence than RoFormer on Tiny Shakespeare.

Conclusion: JoFormer provides a principled way to integrate positional structure into Transformers, showing promise for more expressive architectures.

Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [440] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/pdf/2506.08644)
*Woosung Kim, JunHo Seo, Jongmin Lee, Byung-Jun Lee*

Main category: cs.LG

TL;DR: DICE corrects distribution mismatch for OPE and policy optimization, but recent methods degrade OPE. A new semi-gradient DICE method fixes this, excelling in offline constrained RL.


<details>
  <summary>Details</summary>
Motivation: Address the degradation of OPE in DICE-based offline RL methods due to semi-gradient optimization, hindering constrained RL applications.

Method: Proposes a novel semi-gradient DICE method to enable accurate OPE and constrained RL, addressing the limitations of prior approaches.

Result: Achieves state-of-the-art performance on the DSRL benchmark, ensuring reliable cost estimation.

Conclusion: The new method successfully resolves OPE issues in DICE, making it viable for constrained RL.

Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [441] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/pdf/2506.08660)
*Jinkwan Jang, Hyungjin Park, Jinmyeong Choi, Taesup Kim*

Main category: cs.LG

TL;DR: ChannelTokenFormer is a Transformer-based model addressing challenges like cross-channel dependencies, asynchronous sampling, and missing values in multivariate time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Real-world time series data are complex, with varying sampling periods and missing values, but existing models oversimplify these issues.

Method: Proposes ChannelTokenFormer, a flexible Transformer architecture capturing cross-channel interactions, handling asynchronous sampling, and missing values.

Result: Outperforms benchmarks on modified datasets and a real-world industrial dataset, showing robustness and accuracy.

Conclusion: ChannelTokenFormer effectively addresses real-world challenges in multivariate time series forecasting.

Abstract: Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [442] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/pdf/2506.08645)
*Youqi Wu, Jingwei Zhang, Farzan Farnia*

Main category: cs.LG

TL;DR: RP-KrossFuse unifies cross-modal and single-modality embeddings, achieving strong modality-specific performance while preserving cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Cross-modal embeddings underperform in modality-specific tasks, while single-modality embeddings lack cross-modal alignment. The goal is to combine both for optimal performance.

Method: RP-KrossFuse uses random projection-based Kronecker product to integrate embeddings, operating efficiently in a kernel space with scalable implementations.

Result: RP-KrossFuse achieves competitive modality-specific performance while retaining cross-modal alignment, as shown in experiments with CLIP and uni-modal embeddings.

Conclusion: RP-KrossFuse successfully bridges the gap between cross-modal and single-modality embeddings, offering a unified solution.

Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [443] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/pdf/2506.08662)
*Florian Borzechowski, Michael Schäfer, Heiko Schwarz, Jonathan Pfaff, Detlev Marpe, Thomas Wiegand*

Main category: cs.LG

TL;DR: The paper proposes a finetuning training step for image compression with variational autoencoders to address quantization noise issues, improving coding gain without added inference complexity.


<details>
  <summary>Details</summary>
Motivation: Quantization in training produces zero derivatives, and existing approximations fail to model quantization noise correctly, leading to suboptimal networks.

Method: After conventional end-to-end training, parts of the network are retrained on quantized latents obtained during inference, particularly effective for entropy-constraint quantizers.

Result: Retraining on correctly quantized data yields 1%-2.2% savings in Bjøntegaard-Delta bitrate for test sets like Kodak and TecNick.

Conclusion: The proposed finetuning step improves rate-distortion efficiency for learned codecs, especially with entropy-constraint quantization.

Abstract: The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [444] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/pdf/2506.08655)
*Kamil Jerabek, Jan Luxemburk, Richard Plny, Josef Koumar, Jaroslav Pesek, Karel Hynek*

Main category: cs.LG

TL;DR: A study reveals that simple k-NN models using packet metadata can match or outperform complex neural networks in network traffic classification, due to dataset redundancy and flawed evaluation practices.


<details>
  <summary>Details</summary>
Motivation: To investigate why simple k-NN baselines perform well in traffic classification and to highlight issues with current datasets and evaluation methods.

Method: Evaluated the k-NN baseline across 12 datasets and 15 tasks, analyzing dataset redundancy and splitting practices.

Result: Found over 50% redundant samples in most datasets, leading to inflated performance metrics and theoretical accuracy limits.

Conclusion: Proposes new task formulation and evaluation directions to address dataset redundancy and improve traffic classification research.

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [445] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/pdf/2506.08669)
*Dongge Han, Menglin Xia, Daniel Madrigal Diaz, Samuel Kessler, Ankur Mallick, Xuchao Zhang, Mirian Del Carmen Hipolito Garcia, Jin Xu, Victor Rühle, Saravan Rajmohan*

Main category: cs.LG

TL;DR: A framework enhances small language models (SLMs) using LLM-generated blueprints and prompt template search, improving reasoning without increasing model size.


<details>
  <summary>Details</summary>
Motivation: SLMs are efficient but limited in reasoning and sensitive to prompt variations, needing a lightweight solution.

Method: Proposes a framework with LLM-generated blueprints for structured reasoning and prompt template search to reduce sensitivity.

Result: Improved SLM performance in math (GSM8K), coding (MBPP), and logic reasoning (BBH) tasks.

Conclusion: The framework offers a lightweight, deployment-friendly solution for enhancing SLMs without additional training or size increase.

Abstract: Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [446] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/pdf/2506.08673)
*Diptarka Chakraborty, Kushagra Chatterjee, Debarati Das, Tien Long Nguyen, Romina Nobahari*

Main category: cs.LG

TL;DR: The paper introduces fair consensus clustering, ensuring proportional representation of protected groups, and provides approximation algorithms for fairness enforcement.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fairness in consensus clustering by ensuring proportional representation of protected groups, inspired by fair clustering principles.

Method: Develops optimal and near-linear time approximation algorithms for fair consensus clustering, including NP-hardness proof for unequal-sized groups.

Result: Provides constant-factor approximation for fair consensus clustering and optimal solutions for equal group representation.

Conclusion: The work advances fair clustering with broader implications for clustering problems lacking fairness guarantees.

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [447] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/pdf/2506.08698)
*Boyu Xie, Tangtang Xie*

Main category: cs.LG

TL;DR: VAE-LF, a VAE-based model, improves power load forecasting by efficiently representing and completing high-dimensional, incomplete power load monitoring data, outperforming benchmarks in sparse data scenarios.


<details>
  <summary>Details</summary>
Motivation: High-dimensional and incomplete power load monitoring data challenges forecasting models, necessitating an efficient solution for data representation and completion.

Method: VAE-LF uses an encoder-decoder structure to learn low-dimensional latent representations and generate complementary data from split vectors of the input data.

Result: VAE-LF achieves lower RMSE and MAE than benchmarks, especially in 5% and 10% sparsity cases, and excels with low sparsity data.

Conclusion: VAE-LF offers an effective data-completion solution for smart grid load management.

Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [448] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/pdf/2506.08681)
*Phuc Minh Nguyen, Ngoc-Hieu Nguyen, Duy H. M. Nguyen, Anji Liu, An Mai, Binh T. Nguyen, Daniel Sonntag, Khoa D. Doan*

Main category: cs.LG

TL;DR: IS-DAAs, an importance-sampling approach, mitigates over-optimization in Direct Alignment Algorithms (DAAs) like DPO by clipping the importance ratio, improving performance under low regularization.


<details>
  <summary>Details</summary>
Motivation: DAAs (e.g., DPO) align LLMs with human values but suffer from over-optimization, degrading performance as training progresses.

Method: Proposes IS-DAAs, which multiplies the DAA objective with a clipped importance ratio to account for the reference policy distribution.

Result: IS-DAAs effectively mitigate over-optimization, especially with low regularization, outperforming other methods.

Conclusion: IS-DAAs offer a robust solution to over-optimization in DAAs, with public implementations available.

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [449] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/pdf/2506.08989)
*Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, Weizhu Chen*

Main category: cs.LG

TL;DR: RLVR improves LLM training for reasoning tasks, but lacks quality problem sets. The SwS framework synthesizes problems targeting model weaknesses, boosting performance by 10.0% and 7.7% on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack precise, verifiable answers, and problem synthesis often ignores model capabilities, limiting RL effectiveness.

Method: SwS identifies model weaknesses from failure cases, extracts core concepts, and synthesizes targeted problems for augmented training.

Result: Average performance gains of 10.0% (7B model) and 7.7% (32B model) across eight reasoning benchmarks.

Conclusion: SwS enables self-improvement in RL by focusing on weaknesses, enhancing generalization without external distillation.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [450] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/pdf/2506.08727)
*Samarth Sikand, Rohit Mehra, Priyavanshi Pathania, Nikhil Bamby, Vibhu Saujanya Sharma, Vikrant Kaulgud, Sanjay Podder, Adam P. Burden*

Main category: cs.LG

TL;DR: The paper addresses the environmental impact of LLMs and introduces R-ICE, a framework for estimating prompt-level carbon emissions using SOTA benchmarks.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of Generative AI and LLMs poses sustainability challenges due to their high energy consumption, necessitating better tools for emission estimation.

Method: The authors propose R-ICE, a framework leveraging existing LLM benchmarks to estimate carbon emissions non-intrusively and accurately.

Result: Validation results show promise for benchmark-based modeling in emission estimation, enabling use-cases like dynamic LLM routing and carbon accounting.

Conclusion: The study highlights the potential of benchmark-based approaches for sustainable AI and calls for further exploration by the scientific community.

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [451] [Exploration by Random Reward Perturbation](https://arxiv.org/pdf/2506.08737)
*Haozhe Ma, Guoji Fu, Zhengding Luo, Jiele Wu, Tze-Yun Leong*

Main category: cs.LG

TL;DR: RRP is a lightweight exploration strategy for RL that adds zero-mean noise to rewards, enhancing policy diversity and exploration. It integrates easily with existing methods and improves performance in tasks with sparse or dense rewards.


<details>
  <summary>Details</summary>
Motivation: To enhance exploration in RL by diversifying policies through reward perturbation, addressing limitations of existing exploration strategies.

Method: Adds zero-mean noise to environmental rewards, compatible with action-perturbation methods like $ε$-greedy and entropy regularization.

Result: Boosts performance of PPO and SAC, improving sample efficiency and escaping local optima in various tasks.

Conclusion: RRP is a general, effective, and easy-to-implement exploration strategy that complements existing methods and enhances RL performance.

Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [452] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/pdf/2506.09026)
*Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar*

Main category: cs.LG

TL;DR: The paper introduces a method (e3) to improve LLM reasoning by enabling in-context exploration, allowing models to extrapolate beyond their training token budget and achieve better performance on hard problems.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning models fail to extrapolate well beyond their training token limits, limiting their potential for solving harder problems with extended 'thinking' time.

Method: The e3 method involves training LLMs for in-context exploration by chaining asymmetric skills (e.g., verification and generation), leveraging negative gradients for exploration, and using a curriculum to match task difficulty with token budget.

Result: The e3-1.7B model outperforms others on benchmarks (AIME'25, HMMT'25), extrapolates to 2x the training token budget, and improves pass@1 and pass@k scores.

Conclusion: In-context exploration, enabled by e3, significantly enhances LLM reasoning and extrapolation capabilities, offering a scalable solution for improving performance on challenging tasks.

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [453] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/pdf/2506.08740)
*Sidhika Balachandar, Shuvom Sadhuka, Bonnie Berger, Emma Pierson, Nikhil Garg*

Main category: cs.LG

TL;DR: A GNN-based model combines sparse government ratings and dense but biased crowdsourced reports to predict urban incidents, outperforming single-source models and revealing reporting biases.


<details>
  <summary>Details</summary>
Motivation: Government inspection ratings are sparse, while crowdsourced reports are dense but biased. A method is needed to leverage both for accurate incident prediction.

Method: Proposes a multiview, multioutput GNN model integrating both rating and reporting data. Tests on NYC data with 9.6M reports and 1M ratings.

Result: The model outperforms single-source models, especially when ratings are sparse. Reveals demographic biases in reporting (e.g., higher-income areas report more).

Conclusion: The approach effectively predicts latent incident states using heterogeneous, sparse, and biased data, with broad applicability.

Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [454] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/pdf/2506.08764)
*Benjamin Dadoun, Soufiane Hayou, Hanan Salam, Mohamed El Amine Seddik, Pierre Youssef*

Main category: cs.LG

TL;DR: A general stability theorem for deep neural networks is introduced, accommodating sparsity and non-i.i.d. weights, extending prior work on Jacobian stability.


<details>
  <summary>Details</summary>
Motivation: Address limitations of prior work on Jacobian stability, which was restricted to fully connected networks with i.i.d. weights.

Method: Leverage random matrix theory to analyze spectral stability in networks with sparsity and weakly correlated weights.

Result: Rigorous guarantees for spectral stability in a broader class of network models, including pruned and trained networks.

Conclusion: The work extends theoretical foundations for initialization schemes in modern neural networks with structured randomness.

Abstract: Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [455] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/pdf/2506.08837)
*Luca Beurer-Kellner, Beat Buesser Ana-Maria Creţu, Edoardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tramèr, Václav Volhejn*

Main category: cs.LG

TL;DR: Proposes design patterns for AI agents to resist prompt injection attacks, balancing utility and security.


<details>
  <summary>Details</summary>
Motivation: Addressing the critical security challenge of prompt injection attacks in versatile AI agents powered by LLMs.

Method: Introduces principled design patterns for AI agents, analyzing their trade-offs and real-world applicability.

Result: Demonstrates provable resistance to prompt injection through systematic analysis and case studies.

Conclusion: Design patterns offer a practical solution to enhance AI agent security against prompt injection.

Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [456] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/pdf/2506.08844)
*Siyi Sun, David Antony Selby, Yunchuan Huang, Sebastian Vollmer, Seth Flaxman, Anisoara Calinescu*

Main category: cs.LG

TL;DR: The paper introduces IMAGIC-500, a synthetic dataset for benchmarking missing data imputation methods in socioeconomic research, addressing data privacy and reproducibility challenges.


<details>
  <summary>Details</summary>
Motivation: Real-world socioeconomic datasets face strict privacy restrictions, limiting benchmark availability for imputation methods. The study aims to provide a public, reproducible benchmark.

Method: Utilizes the World Bank's synthetic dataset to create IMAGIC-500, evaluating imputation methods under various missing mechanisms and ratios.

Result: Benchmark results reveal performance differences among statistical, machine learning, and deep learning imputation techniques, including diffusion-based methods.

Conclusion: IMAGIC-500 enables robust imputation algorithm development and reproducible social science research.

Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [457] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/pdf/2506.08850)
*Amin Avan, Akramul Azim, Qusay Mahmoud*

Main category: cs.LG

TL;DR: The paper proposes Agile Reinforcement Learning (aRL) for efficient task scheduling in edge computing, addressing the limitations of traditional heuristic and reinforcement learning methods.


<details>
  <summary>Details</summary>
Motivation: The complexity and dynamic nature of edge computing environments make task scheduling challenging, with existing methods struggling to adapt quickly or scale effectively.

Method: The study introduces aRL, which combines informed exploration and action-masking to reduce irrelevant actions and enhance predictability.

Result: Experiments show aRL achieves higher hit-ratios and faster convergence compared to baseline approaches.

Conclusion: aRL is a promising solution for scheduling soft real-time tasks in edge computing due to its adaptability and efficiency.

Abstract: Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [458] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/pdf/2506.08871)
*Victor M. Tenorio, Madeline Navarro, Samuel Rey, Santiago Segarra, Antonio G. Marques*

Main category: cs.LG

TL;DR: SG-GNN improves GNN performance on heterophilic data by creating alternative graph structures with higher label homophily and adaptively combining them with the original graph.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle with heterophilic data due to their reliance on homophily assumptions. This work aims to enhance GNNs by leveraging structural attributes to create more homophilic graphs.

Method: Proposes SG-GNN, which constructs alternative graphs with similar structural attributes, combines them with the original graph, and adaptively learns their contributions.

Result: SG-GNN achieves state-of-the-art or competitive performance on heterophilic benchmark datasets.

Conclusion: Exploiting structural information to guide GNNs is effective, especially for heterophilic data.

Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [459] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/pdf/2506.08882)
*Dimitrios Amaxilatis, Themistoklis Sarantakos, Ioannis Chatzigiannakis, Georgios Mylonas*

Main category: cs.LG

TL;DR: The paper evaluates data imputation techniques for improving water distribution network management using smart meter data, highlighting their impact on accuracy and reliability for applications like leak detection.


<details>
  <summary>Details</summary>
Motivation: Data gaps in smart water meter data due to technical issues can hinder operational efficiency and decision-making, necessitating effective imputation methods.

Method: The study compares various imputation techniques, including k-Nearest Neighbors, MissForest, Transformers, and Recurrent Neural Networks, using real-world IoT water grid data.

Result: Effective data imputation significantly improves the quality of insights from water consumption data, enhancing accuracy and reliability for operational applications.

Conclusion: Data imputation techniques can substantially aid in better monitoring and management of water distribution networks, particularly for leak detection and predictive maintenance.

Abstract: In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [460] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/pdf/2506.08889)
*Yizhao Gao, Shuming Guo, Shijie Cao, Yuqing Xia, Yu Cheng, Lei Wang, Lingxiao Ma, Yutao Sun, Tianzhu Ye, Li Dong, Hayden Kwok-Hay So, Yu Hua, Ting Cao, Fan Yang, Mao Yang*

Main category: cs.LG

TL;DR: SeerAttention-R is a sparse attention framework for long decoding in reasoning models, maintaining accuracy with minimal training and achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient long decoding in reasoning models without compromising accuracy.

Method: Extends SeerAttention with a self-distilled gating mechanism, removes query pooling, and integrates a lightweight plug-in gating for auto-regressive decoding.

Result: Near-lossless reasoning accuracy with 4K tokens, 9x speedup over FlashAttention-3 at 90% sparsity.

Conclusion: SeerAttention-R is a flexible, efficient solution for long decoding in reasoning models.

Abstract: We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [461] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/pdf/2506.08884)
*Shiqin Tang, Shujian Yu*

Main category: cs.LG

TL;DR: InfoDPCCA is a dynamic probabilistic CCA framework for modeling two interdependent sequences, using an information-theoretic objective to extract shared and sequence-specific latent representations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting meaningful latent representations from high-dimensional sequential data, improving interpretability and robustness over prior methods.

Method: Introduces InfoDPCCA with a novel information-theoretic objective, a two-step training scheme, and residual connections for stability.

Result: Demonstrated effectiveness on synthetic and medical fMRI data as a robust representation learning tool.

Conclusion: InfoDPCCA provides a balanced, interpretable, and robust framework for extracting shared and specific latent representations from sequential data.

Abstract: Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [462] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/pdf/2506.08902)
*Chongyi Zheng, Seohong Park, Sergey Levine, Benjamin Eysenbach*

Main category: cs.LG

TL;DR: The paper introduces InFOM, a method for pre-training RL models using flow matching and latent user intentions, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address challenges in RL pre-training, such as long-term action dependencies and sample efficiency, by leveraging generative AI tools.

Method: Proposes intention-conditioned flow occupancy models (InFOM), using flow matching and latent variables for user intentions to predict future states.

Result: Demonstrates a 1.8× median improvement in returns and 36% higher success rates on benchmark tasks.

Conclusion: InFOM effectively enhances RL pre-training by modeling long-term dependencies and user intentions, outperforming alternatives.

Abstract: Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [463] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/pdf/2506.08916)
*Maria-Veronica Ciocanel, John T. Nardini, Kevin B. Flores, Erica M. Rutter, Suzanne S. Sindi, Alexandria Volkening*

Main category: cs.LG

TL;DR: The paper introduces Multi-experiment equation learning (ME-EQL) to improve generalizability and interpretability of learned models from agent-based simulations, reducing errors in parameter recovery.


<details>
  <summary>Details</summary>
Motivation: Agent-based modeling (ABM) is computationally intensive and lacks analytical tractability, while existing equation learning (EQL) methods require extensive simulations for each parameter set, limiting generalizability.

Method: Two ME-EQL methods are proposed: one-at-a-time (OAT) for individual models with interpolation, and embedded structure (ES) for a unified model library across parameters.

Result: Both methods reduce relative error in parameter recovery, with OAT ME-EQL showing better generalizability across parameter space.

Conclusion: ME-EQL enhances the generalizability and interpretability of learned models for complex biological systems.

Abstract: Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [464] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/pdf/2506.08928)
*Zhongyuan Liang, Zachary T. Rewolinski, Abhineet Agarwal, Tiffany M. Tang, Bin Yu*

Main category: cs.LG

TL;DR: LMDI+ extends MDI+ to local feature importance, outperforming LIME and TreeSHAP in accuracy and stability for interpretable predictions in tree-based models.


<details>
  <summary>Details</summary>
Motivation: Tree-based models lack reliable local interpretability methods, as existing approaches like LIME and TreeSHAP rely on approximations and perturbations, ignoring model structure.

Method: LMDI+ adapts the global MDI+ framework to local settings, leveraging the equivalence between decision trees and linear models for sample-specific feature importance.

Result: LMDI+ improves downstream task performance by 10% over baselines, shows greater stability, and enables local interpretability use cases like counterfactual identification.

Conclusion: LMDI+ provides a robust, accurate, and stable method for local feature importance in tree-based models, addressing gaps in existing approaches.

Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [465] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/pdf/2506.08961)
*Chenxu Wang, Huaping Liu*

Main category: cs.LG

TL;DR: The paper addresses environmental state perturbations in DRL, proposing a defense framework (BAT) to enhance agent robustness.


<details>
  <summary>Details</summary>
Motivation: Environmental state perturbations are understudied in DRL, despite their natural occurrence in embodied scenarios.

Method: Introduces a non-targeted attack method for calibration, then proposes BAT: supervised learning tuning followed by adversarial RL training.

Result: Mainstream DRL agents are vulnerable to environmental perturbations; BAT significantly improves robustness.

Conclusion: BAT outperforms existing robust RL methods against environmental state perturbations.

Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [466] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/pdf/2506.08936)
*Amina Mollaysa, Artem Moskale, Pushpak Pati, Tommaso Mansi, Mangal Prakash, Rui Liao*

Main category: cs.LG

TL;DR: BioLangFusion integrates DNA, mRNA, and protein language models into unified representations using codon-level alignment and three fusion techniques, outperforming unimodal baselines in molecular property prediction.


<details>
  <summary>Details</summary>
Motivation: The central dogma of molecular biology (gene to transcript to protein) motivates aligning embeddings at the codon level for cross-modal correspondence.

Method: Three fusion techniques: codon-level concatenation, entropy-regularized attention pooling, and cross-modal multi-head attention, without additional pre-training.

Result: Outperforms unimodal baselines in five molecular property prediction tasks, capturing complementary multi-omic information.

Conclusion: Simple fusion of pre-trained models can effectively combine multi-omic data with minimal overhead.

Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [467] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/pdf/2506.08939)
*Hang Ye, Gaoxiang Duan, Haoran Zeng, Yangxin Zhu, Lingxue Meng, Xiaoying Zheng, Yongxin Zhu*

Main category: cs.LG

TL;DR: KARMA introduces adaptive decomposition and hybrid frequency-time modules to improve long-term multivariate time series forecasting, outperforming traditional and Transformer-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional decomposition methods and Transformer-based models fail to handle complex time dynamics and long sequences effectively.

Method: KARMA uses Adaptive Time Channel Decomposition (ATCD) and Hybrid Frequency-Time Decomposition (HFTD) with Mamba-based KarmaBlock for efficient processing.

Result: Outperforms baselines in accuracy and efficiency across eight real-world datasets.

Conclusion: KARMA is a robust solution for complex time series forecasting, combining dynamic decomposition and efficient processing.

Abstract: Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [468] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/pdf/2506.08965)
*Yiyang Zhao, Huiyu Bai, Xuejiao Zhao*

Main category: cs.LG

TL;DR: A framework for training high-performing reward models with few-shot data using data augmentation and preference refinement, achieving results comparable to large-scale datasets.


<details>
  <summary>Details</summary>
Motivation: Enhancing efficiency and scalability of Reinforcement Learning from Human Feedback (RLHF) by overcoming inefficiencies in sample pairing and limited data diversity.

Method: Introduces preference refinement with Chain-of-Thought sampling, perplexity-based scoring, and Multi-level Direct Preference Optimization (M-DPO).

Result: Significantly improves data efficiency and model performance, matching large-scale dataset results in few-shot settings.

Conclusion: Demonstrates the potential of data-efficient strategies for reward model optimization, benefiting low-resource RLHF applications.

Abstract: The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [469] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/pdf/2506.08977)
*Victoria Hankemeier, Malte Schilling*

Main category: cs.LG

TL;DR: The paper introduces a novel dataset and TimeFlex model to explore connections between time series characteristics and model performance, comparing it to state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how specific time series characteristics align with model strengths, beyond limited real-world data evaluations.

Method: Uses Gaussian Processes to generate a dataset with known characteristics and introduces TimeFlex, a modular model for diverse temporal dynamics.

Result: TimeFlex is compared to state-of-the-art models, providing insights into model adaptability to varied time series conditions.

Conclusion: The study enhances understanding of model performance by linking time series features to architectural strengths, aided by the new dataset and TimeFlex.

Abstract: Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [470] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/pdf/2506.08978)
*Anna Langedijk, Jaap Jumelet, Willem Zuidema*

Main category: cs.LG

TL;DR: The paper examines how neural networks generalize symbolic rules, focusing on Transformers, GCNs, and LSTMs in a propositional logic task. While models perform well in-distribution, generalization to unseen patterns, especially negation, remains challenging.


<details>
  <summary>Details</summary>
Motivation: To understand neural networks' ability to acquire and represent symbolic rules, particularly in structured reasoning tasks like propositional logic.

Method: Evaluated three architectures (Transformers, GCNs, LSTMs) on a controlled task of generating satisfying assignments for logical formulas, using a balanced dataset to test generalization.

Result: All models performed well in-distribution but struggled with unseen patterns, especially negation. Transformers failed without structural biases.

Conclusion: Standard architectures lack systematic representation of logical operators, indicating a need for stronger inductive biases for robust rule-based reasoning.

Abstract: The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [471] [On Finetuning Tabular Foundation Models](https://arxiv.org/pdf/2506.08982)
*Ivan Rubachev, Akim Kotelnikov, Nikolay Kartashev*

Main category: cs.LG

TL;DR: The paper explores finetuning strategies for TabPFNv2, a tabular foundation model, finding full finetuning most effective. It reveals how finetuning improves similarity measures and performance, achieving state-of-the-art results on some datasets but showing instability on others.


<details>
  <summary>Details</summary>
Motivation: The study addresses the underexplored area of finetuning tabular foundation models like TabPFNv2, aiming to optimize their adaptation and understand internal mechanism changes.

Method: Systematic evaluation of finetuning strategies on diverse datasets, analyzing how finetuning alters TabPFNv2's internal mechanisms, particularly similarity measures.

Result: Full finetuning is most effective for TabPFNv2, improving performance on datasets up to 50K objects. It achieves state-of-the-art results on I.I.D. datasets but is less stable on temporally shifted or feature-rich datasets.

Conclusion: Finetuning enhances TabPFNv2's performance by refining similarity measures, though its effectiveness varies by dataset type, suggesting context-specific adaptation is key.

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [472] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/pdf/2506.09018)
*Marton Havasi, Brian Karrer, Itai Gat, Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: Edit Flows is a non-autoregressive model using edit operations for flexible sequence generation, outperforming autoregressive and mask models.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of non-autoregressive models in handling variable-length sequences by introducing edit operations.

Method: Defines a discrete flow over sequences via insertions, deletions, and substitutions, modeled with a Continuous-time Markov Chain. Uses an expanded state space with auxiliary variables for efficient training.

Result: Outperforms autoregressive and mask models in image captioning and excels in text and code generation.

Conclusion: Edit Flows provide a flexible and efficient alternative for sequence generation, aligning better with data structure.

Abstract: Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [473] [Branched Schrödinger Bridge Matching](https://arxiv.org/pdf/2506.09007)
*Sophia Tang, Yinuo Zhang, Alexander Tong, Pranam Chatterjee*

Main category: cs.LG

TL;DR: BranchSBM introduces a framework for learning branched Schrödinger bridges, enabling multi-path transitions between distributions, unlike unimodal methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like flow matching and Schrödinger Bridge Matching are limited to unimodal transitions, unable to model branched or divergent evolution.

Method: BranchSBM parameterizes multiple time-dependent velocity fields and growth processes to represent population-level divergence into multiple terminal distributions.

Result: BranchSBM is more expressive and essential for tasks like multi-path navigation, cell fate bifurcation modeling, and simulating divergent cellular responses.

Conclusion: BranchSBM addresses the limitations of unimodal methods by enabling branched transitions, proving useful for complex tasks involving divergence.

Abstract: Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [474] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/pdf/2506.09034)
*Sizhe Dang, Yangyang Guo, Yanjun Zhao, Haishan Ye, Xiaodong Zheng, Guang Dai, Ivor Tsang*

Main category: cs.LG

TL;DR: FZOO is a fast zeroth-order optimizer that improves the trade-off between speed and memory in fine-tuning LLMs, outperforming MeZO in accuracy and reducing forward passes significantly.


<details>
  <summary>Details</summary>
Motivation: Addressing GPU memory bottlenecks in fine-tuning LLMs by improving zeroth-order optimizers to achieve Adam-scale speed without high memory costs.

Method: FZOO uses batched one-sided gradient estimates, adaptive step sizes based on batch loss deviation, and Rademacher random vector perturbations with CUDA parallel processing.

Result: FZOO outperforms MeZO by 3% in accuracy and reduces forward passes by 3x on average, with RoBERTa-large showing 5.6% accuracy improvement and 18x fewer forward passes.

Conclusion: FZOO enables practical single-GPU, high-speed, full-parameter fine-tuning and suggests potential for memory-efficient pre-training.

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [475] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/pdf/2506.09010)
*Sebastian Schmidt, Prasanga Dhungel, Christoffer Löffler, Björn Nieth, Stephan Günnemann, Leo Schwinn*

Main category: cs.LG

TL;DR: A new framework predicts sample importance using minimal training data, improving efficiency for data pruning without full initial training.


<details>
  <summary>Details</summary>
Motivation: Reduce computational costs of training by pruning redundant samples without needing full initial training.

Method: Introduces importance score extrapolation using k-nearest neighbors and graph neural networks on a small data subset.

Result: Effective for 2 pruning methods, 4 datasets, and 3 training paradigms, showing promise for scaling expensive tasks.

Conclusion: Score extrapolation is a scalable solution for data pruning and similar tasks.

Abstract: Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [476] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/pdf/2506.09016)
*Ruiqi Zhang, Daman Arora, Song Mei, Andrea Zanette*

Main category: cs.LG

TL;DR: SPEED is an adaptive RL curriculum that selects intermediate-difficulty prompts to enhance training efficiency, achieving 2x-6x faster training without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Uniform prompt sampling in RL for large language models is computationally expensive and inefficient.

Method: SPEED selectively chooses intermediate-difficulty prompts to optimize learning efficiency, improving gradient estimator signal-to-noise ratio.

Result: Empirical results show 2x-6x faster training with no accuracy degradation, requiring no manual tuning.

Conclusion: SPEED efficiently accelerates RL training for language models while maintaining performance, seamlessly integrating into standard RL algorithms.

Abstract: Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [477] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/pdf/2506.09044)
*Javier Sanguino, Thomas Kehrenberg, Jose A. Lozano, Novi Quadrianto*

Main category: cs.LG

TL;DR: The paper introduces a visualization method for the loss landscape in Performative Prediction, complementing theoretical work, and extends the framework to scenarios where agents react to a different model than the deployed one.


<details>
  <summary>Details</summary>
Motivation: To provide practical insights into Performative Prediction by visualizing the loss landscape, addressing gaps in theoretical literature.

Method: A decoupled risk visualization method is introduced, analyzing risk landscapes for model and data parameters. This is extended to scenarios where agents react to a different model.

Result: New properties of interest points are proposed, and existing algorithms are examined under realistic conditions, including non-linear strategic classification.

Conclusion: The visualization method and extended framework enhance understanding of Performative Prediction, offering practical tools for analyzing distribution shifts.

Abstract: Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [478] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/pdf/2506.09048)
*Yuxin Dong, Jiachen Jiang, Zhihui Zhu, Xia Ning*

Main category: cs.LG

TL;DR: Task vectors distill task-specific info for ICL, but their principles are unclear. The Linear Combination Conjecture suggests they're linear combos of original demonstrations, supported by theory and experiments.


<details>
  <summary>Details</summary>
Motivation: To clarify the principles behind task vectors in ICL and their functionality.

Method: Proposes the Linear Combination Conjecture, analyzes loss landscapes in linear transformers, and tests task vectors on high-rank mappings.

Result: Task vectors emerge naturally in linear transformers but fail for high-rank mappings. Enhancing them with multiple vectors improves performance.

Conclusion: The study advances understanding of task vectors and ICL mechanisms in transformers.

Abstract: Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


### [479] [Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data](https://arxiv.org/pdf/2306.03346)
*Chongyi Zheng, Benjamin Eysenbach, Homer Walke, Patrick Yin, Kuan Fang, Ruslan Salakhutdinov, Sergey Levine*

Main category: cs.LG

TL;DR: Self-supervised RL for robotics reduces human effort, achieves 2x success rate in simulation, and solves real-world tasks using contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To decrease human annotation and engineering effort in robotic control by leveraging self-supervised RL.

Method: Uses self-supervised RL, inspired by CV/NLP, with contrastive learning for goal-reaching tasks. Tests in simulation first, then applies to real-world robotics.

Result: Achieves 2x higher success rate in simulation and successfully solves image-based robotic manipulation tasks post-training.

Conclusion: Self-supervised RL is practical for robotics, enabling task specification via a single goal image.

Abstract: Robotic systems that rely primarily on self-supervised learning have the
potential to decrease the amount of human annotation and engineering effort
required to learn control strategies. In the same way that prior robotic
systems have leveraged self-supervised techniques from computer vision (CV) and
natural language processing (NLP), our work builds on prior work showing that
the reinforcement learning (RL) itself can be cast as a self-supervised
problem: learning to reach any goal without human-specified rewards or labels.
Despite the seeming appeal, little (if any) prior work has demonstrated how
self-supervised RL methods can be practically deployed on robotic systems. By
first studying a challenging simulated version of this task, we discover design
decisions about architectures and hyperparameters that increase the success
rate by $2 \times$. These findings lay the groundwork for our main result: we
demonstrate that a self-supervised RL algorithm based on contrastive learning
can solve real-world, image-based robotic manipulation tasks, with tasks being
specified by a single goal image provided after training.

</details>


### [480] [Scaling Laws in Linear Regression: Compute, Parameters, and Data](https://arxiv.org/pdf/2406.08466)
*Licong Lin, Jingfeng Wu, Sham M. Kakade, Peter L. Bartlett, Jason D. Lee*

Main category: cs.LG

TL;DR: The paper explains neural scaling laws by showing that variance error, which typically increases with model size, is dominated by other errors due to SGD's implicit regularization, aligning with empirical observations.


<details>
  <summary>Details</summary>
Motivation: To reconcile the discrepancy between conventional wisdom (variance error increases with model size) and empirical neural scaling laws (performance improves with model size).

Method: Theoretical analysis in an infinite-dimensional linear regression setup, using one-pass SGD with sketched covariates and assuming Gaussian prior and power-law data covariance spectrum.

Result: The reducible test error is Θ(M−(a−1) + N−(a−1)/a), with variance error dominated by other errors due to SGD's implicit regularization.

Conclusion: The theory aligns with empirical neural scaling laws and is supported by simulations, explaining why increasing model size improves performance despite conventional expectations.

Abstract: Empirically, large-scale deep learning models often satisfy a neural scaling
law: the test error of the trained model improves polynomially as the model
size and data size grow. However, conventional wisdom suggests the test error
consists of approximation, bias, and variance errors, where the variance error
increases with model size. This disagrees with the general form of neural
scaling laws, which predict that increasing model size monotonically improves
performance.
  We study the theory of scaling laws in an infinite dimensional linear
regression setup. Specifically, we consider a model with $M$ parameters as a
linear function of sketched covariates. The model is trained by one-pass
stochastic gradient descent (SGD) using $N$ data. Assuming the optimal
parameter satisfies a Gaussian prior and the data covariance matrix has a
power-law spectrum of degree $a>1$, we show that the reducible part of the test
error is $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which
increases with $M$, is dominated by the other errors due to the implicit
regularization of SGD, thus disappearing from the bound. Our theory is
consistent with the empirical neural scaling laws and verified by numerical
simulation.

</details>


### [481] [Chip Placement with Diffusion Models](https://arxiv.org/pdf/2407.12282)
*Vint Lee, Minh Nguyen, Leena Elzeiny, Chun Deng, Pieter Abbeel, John Wawrzynek*

Main category: cs.LG

TL;DR: A diffusion model is proposed for zero-shot macro placement in chip design, outperforming RL-based methods by leveraging synthetic data and guided sampling.


<details>
  <summary>Details</summary>
Motivation: Optimizing macro placement is crucial for chip performance, but RL-based methods are slow and lack generalization.

Method: A diffusion model is trained using synthetic data and guided sampling to place macros zero-shot.

Result: The model achieves competitive performance on benchmarks, generating high-quality placements for unseen circuits.

Conclusion: The diffusion model offers a scalable and efficient alternative to RL for macro placement, with strong generalization capabilities.

Abstract: Macro placement is a vital step in digital circuit design that defines the
physical location of large collections of components, known as macros, on a 2D
chip. Because key performance metrics of the chip are determined by the
placement, optimizing it is crucial. Existing learning-based methods typically
fall short because of their reliance on reinforcement learning (RL), which is
slow and struggles to generalize, requiring online training on each new
circuit. Instead, we train a diffusion model capable of placing new circuits
zero-shot, using guided sampling in lieu of RL to optimize placement quality.
To enable such models to train at scale, we designed a capable yet efficient
architecture for the denoising model, and propose a novel algorithm to generate
large synthetic datasets for pre-training. To allow zero-shot transfer to real
circuits, we empirically study the design decisions of our dataset generation
algorithm, and identify several key factors enabling generalization. When
trained on our synthetic data, our models generate high-quality placements on
unseen, realistic circuits, achieving competitive performance on placement
benchmarks compared to state-of-the-art methods.

</details>


### [482] [The Causal Information Bottleneck and Optimal Causal Variable Abstractions](https://arxiv.org/pdf/2410.00535)
*Francisco N. F. Q. Simoes, Mehdi Dastani, Thijs van Ommen*

Main category: cs.LG

TL;DR: The paper introduces the Causal Information Bottleneck (CIB), a causal extension of the Information Bottleneck (IB), to create causally interpretable variable abstractions while preserving causal control over a target variable.


<details>
  <summary>Details</summary>
Motivation: Traditional IB methods ignore causal structures, making them unsuitable for causal tasks. The authors aim to address this gap by incorporating causality into the abstraction process.

Method: The proposed CIB method compresses chosen variables while maintaining causal control over a target variable, ensuring causally interpretable abstractions.

Result: Experimental results show that CIB accurately captures causal relations, providing insights into variable interactions and supporting reasoning about interventions.

Conclusion: CIB successfully extends IB to causal settings, offering a practical tool for studying complex causal systems with interpretable abstractions.

Abstract: To effectively study complex causal systems, it is often useful to construct
abstractions of parts of the system by discarding irrelevant details while
preserving key features. The Information Bottleneck (IB) method is a widely
used approach to construct variable abstractions by compressing random
variables while retaining predictive power over a target variable. Traditional
methods like IB are purely statistical and ignore underlying causal structures,
making them ill-suited for causal tasks. We propose the Causal Information
Bottleneck (CIB), a causal extension of the IB, which compresses a set of
chosen variables while maintaining causal control over a target variable. This
method produces abstractions of (sets of) variables which are causally
interpretable, give us insight about the interactions between the abstracted
variables and the target variable, and can be used when reasoning about
interventions. We present experimental results demonstrating that the learned
abstractions accurately capture causal relations as intended.

</details>


### [483] [Positional Attention: Expressivity and Learnability of Algorithmic Computation](https://arxiv.org/pdf/2410.01686)
*Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veličković, Kimon Fountoulakis*

Main category: cs.LG

TL;DR: The paper explores how Transformers with positional attention (positional Transformers) can execute algorithmic tasks, showing they match parallel computational models' expressivity but with a trade-off in learning complexity.


<details>
  <summary>Details</summary>
Motivation: To understand the role of attention in Transformers for algorithmic execution, inspired by parallel algorithms' use of positional information.

Method: Theoretical and empirical analysis of positional Transformers, focusing on their expressivity, learnability, and parameter norms' impact on sample complexity.

Result: Positional Transformers match parallel models' expressivity but may require more layers, increasing sample complexity. They perform well in tasks relying on positional information.

Conclusion: Positional Transformers offer a learning trade-off: better theoretical properties but potential complexity costs, excelling in position-dependent tasks.

Abstract: There is a growing interest in the ability of neural networks to execute
algorithmic tasks (e.g., arithmetic, summary statistics, and sorting). The goal
of this work is to better understand the role of attention in Transformers for
algorithmic execution. Its importance for algorithmic execution has been
studied theoretically and empirically using parallel computational models.
Notably, many parallel algorithms communicate between processors solely using
positional information. Inspired by this observation, we investigate how
Transformers can execute algorithms using positional attention, where attention
weights depend exclusively on positional encodings. We prove that Transformers
with positional attention (positional Transformers) maintain the same
expressivity of parallel computational models, incurring a logarithmic depth
cost relative to the input length. We analyze their in-distribution
learnability and explore how parameter norms in positional attention affect
sample complexity. Our results show that positional Transformers introduce a
learning trade-off: while they exhibit better theoretical dependence on
parameter norms, certain tasks may require more layers, which can, in turn,
increase sample complexity. Finally, we empirically explore the
out-of-distribution performance of positional Transformers and find that they
perform well in tasks where their underlying algorithmic solution relies on
positional information.

</details>


### [484] [DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models](https://arxiv.org/pdf/2411.03250)
*Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen*

Main category: cs.LG

TL;DR: DiffLM is a framework combining VAE and diffusion models to improve synthetic data generation by LLMs, addressing distribution and format challenges.


<details>
  <summary>Details</summary>
Motivation: Leveraging LLMs for data synthesis is hindered by their limited understanding of target distributions and complex prompt engineering.

Method: DiffLM uses a VAE with a latent diffusion module to better capture data distributions and decouples learning via a plug-and-play feature injection.

Result: DiffLM outperforms real data in downstream tasks by 2%-7% on structured datasets like Tabular, Code, and Tool data.

Conclusion: DiffLM effectively enhances synthetic data quality, demonstrating practical utility for structured data generation.

Abstract: Recent advancements in large language models (LLMs) have significantly
enhanced their knowledge and generative capabilities, leading to a surge of
interest in leveraging LLMs for high-quality data synthesis. However, synthetic
data generation via prompting LLMs remains challenging due to LLMs' limited
understanding of target data distributions and the complexity of prompt
engineering, especially for structured formatted data. To address these issues,
we introduce DiffLM, a controllable data synthesis framework based on
variational autoencoder (VAE), which further (1) leverages diffusion models to
reserve more information of original distribution and format structure in the
learned latent distribution and (2) decouples the learning of target
distribution knowledge from the LLM's generative objectives via a plug-and-play
latent feature injection module. As we observed significant discrepancies
between the VAE's latent representations and the real data distribution, the
latent diffusion module is introduced into our framework to learn a fully
expressive latent distribution. Evaluations on seven real-world datasets with
structured formatted data (i.e., Tabular, Code, and Tool data) demonstrate that
DiffLM generates high-quality data, with performance on downstream tasks
surpassing that of real data by 2%-7% in certain cases. Data and code are
available at https://github.com/bytedance/DiffLM.

</details>


### [485] [Towards a Mechanistic Explanation of Diffusion Model Generalization](https://arxiv.org/pdf/2411.19339)
*Matthew Niedoba, Berend Zwartsenberg, Kevin Murphy, Frank Wood*

Main category: cs.LG

TL;DR: A training-free mechanism explains diffusion models' generalization by identifying a shared local inductive bias and validating it through novel denoising algorithms.


<details>
  <summary>Details</summary>
Motivation: To understand and explain the generalization behavior of diffusion models by comparing them to theoretically optimal empirical counterparts.

Method: Identify a shared local inductive bias, hypothesize localized denoising operations, and validate with novel denoising algorithms.

Result: The approach shows visual similarity to neural network outputs and lower mean squared error than previous methods.

Conclusion: Network denoisers generalize through localized operations, validated by new algorithms.

Abstract: We propose a simple, training-free mechanism which explains the
generalization behaviour of diffusion models. By comparing pre-trained
diffusion models to their theoretically optimal empirical counterparts, we
identify a shared local inductive bias across a variety of network
architectures. From this observation, we hypothesize that network denoisers
generalize through localized denoising operations, as these operations
approximate the training objective well over much of the training distribution.
To validate our hypothesis, we introduce novel denoising algorithms which
aggregate local empirical denoisers to replicate network behaviour. Comparing
these algorithms to network denoisers across forward and reverse diffusion
processes, our approach exhibits consistent visual similarity to neural network
outputs, with lower mean squared error than previously proposed methods.

</details>


### [486] [Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning](https://arxiv.org/pdf/2212.02042)
*Mingyuan Fan, Cen Chen, Chengyu Wang, Xiaodan Li, Wenmeng Zhou*

Main category: cs.LG

TL;DR: The paper introduces Refiner, a novel defense for Federated Learning (FL) against gradient leakage attacks by generating robust data instead of perturbing gradients, balancing privacy and performance.


<details>
  <summary>Details</summary>
Motivation: Existing FL defenses against gradient leakage attacks are ineffective against sophisticated attacks, necessitating a new approach.

Method: Refiner jointly optimizes privacy and utility metrics to generate robust data with low semantic similarity to raw data, obfuscating attackers while maintaining model performance.

Result: Theoretical and empirical evaluations show Refiner effectively defends against state-of-the-art attacks on benchmark datasets.

Conclusion: Refiner offers a superior defense paradigm for FL by focusing on robust data construction rather than gradient manipulation.

Abstract: Recent works have brought attention to the vulnerability of Federated
Learning (FL) systems to gradient leakage attacks. Such attacks exploit
clients' uploaded gradients to reconstruct their sensitive data, thereby
compromising the privacy protection capability of FL. In response, various
defense mechanisms have been proposed to mitigate this threat by manipulating
the uploaded gradients. Unfortunately, empirical evaluations have demonstrated
limited resilience of these defenses against sophisticated attacks, indicating
an urgent need for more effective defenses. In this paper, we explore a novel
defensive paradigm that departs from conventional gradient perturbation
approaches and instead focuses on the construction of robust data. Intuitively,
if robust data exhibits low semantic similarity with clients' raw data, the
gradients associated with robust data can effectively obfuscate attackers. To
this end, we design Refiner that jointly optimizes two metrics for privacy
protection and performance maintenance. The utility metric is designed to
promote consistency between the gradients of key parameters associated with
robust data and those derived from clients' data, thus maintaining model
performance. Furthermore, the privacy metric guides the generation of robust
data towards enlarging the semantic gap with clients' data. Theoretical
analysis supports the effectiveness of Refiner, and empirical evaluations on
multiple benchmark datasets demonstrate the superior defense effectiveness of
Refiner at defending against state-of-the-art attacks.

</details>


### [487] [Tight Lower Bounds and Improved Convergence in Performative Prediction](https://arxiv.org/pdf/2412.03671)
*Pedram Khorsandi, Rushil Gupta, Mehrnaz Mofakhami, Simon Lacoste-Julien, Gauthier Gidel*

Main category: cs.LG

TL;DR: The paper extends the Repeated Risk Minimization (RRM) framework by using historical datasets, introducing Affine Risk Minimizers for faster convergence to performatively stable points. It provides new bounds and proves tightness, showing improvements over last-iterate RRM.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of data distribution shifts caused by model predictions in real-world deployments, aiming for stable solutions in evolving environments.

Method: Extends RRM by leveraging historical datasets, introducing Affine Risk Minimizers, and analyzing bounds for convergence.

Result: Proves tightness of new bounds, shows historical datasets surpass last-iterate RRM lower bounds, and observes faster convergence empirically.

Conclusion: The framework improves convergence speed and stability in performative prediction, with potential for further enhancements.

Abstract: Performative prediction is a framework accounting for the shift in the data
distribution induced by the prediction of a model deployed in the real world.
Ensuring rapid convergence to a stable solution where the data distribution
remains the same after the model deployment is crucial, especially in evolving
environments. This paper extends the Repeated Risk Minimization (RRM) framework
by utilizing historical datasets from previous retraining snapshots, yielding a
class of algorithms that we call Affine Risk Minimizers and enabling
convergence to a performatively stable point for a broader class of problems.
We introduce a new upper bound for methods that use only the final iteration of
the dataset and prove for the first time the tightness of both this new bound
and the previous existing bounds within the same regime. We also prove that
utilizing historical datasets can surpass the lower bound for last iterate RRM,
and empirically observe faster convergence to the stable point on various
performative prediction benchmarks. We offer at the same time the first lower
bound analysis for RRM within the class of Affine Risk Minimizers, quantifying
the potential improvements in convergence speed that could be achieved with
other variants in our framework.

</details>


### [488] [Generalization analysis of an unfolding network for analysis-based Compressed Sensing](https://arxiv.org/pdf/2303.05582)
*Vicky Kouni, Yannis Panagakis*

Main category: cs.LG

TL;DR: The paper analyzes the generalization ability of an ADMM-based unfolding network in Compressed Sensing, providing theoretical bounds and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To investigate the generalization ability of unfolding networks in Compressed Sensing, which is underexplored.

Method: Imposes a structural constraint on the learnable sparsifier, estimates Rademacher complexity, and derives generalization error bounds.

Result: Generalization error bounds scale with the square root of network layers, and experiments show the framework outperforms baselines.

Conclusion: The proposed framework aligns with theoretical predictions and consistently outperforms state-of-the-art methods.

Abstract: Unfolding networks have shown promising results in the Compressed Sensing
(CS) field. Yet, the investigation of their generalization ability is still in
its infancy. In this paper, we perform a generalization analysis of a
state-of-the-art ADMM-based unfolding network, which jointly learns a decoder
for CS and a sparsifying redundant analysis operator. To this end, we first
impose a structural constraint on the learnable sparsifier, which parametrizes
the network's hypothesis class. For the latter, we estimate its Rademacher
complexity. With this estimate in hand, we deliver generalization error bounds
-- which scale like the square root of the number of layers -- for the examined
network. Finally, the validity of our theory is assessed and numerical
comparisons to a state-of-the-art unfolding network are made, on synthetic and
real-world datasets. Our experimental results demonstrate that our proposed
framework complies with our theoretical findings and outperforms the baseline,
consistently for all datasets.

</details>


### [489] [GRAM: Generalization in Deep RL with a Robust Adaptation Module](https://arxiv.org/pdf/2412.04323)
*James Queeney, Xiaoyi Cai, Alexander Schperberg, Radu Corcodel, Mouhacine Benosman, Jonathan P. How*

Main category: cs.LG

TL;DR: A framework (GRAM) for dynamics generalization in deep reinforcement learning unifies in-distribution and out-of-distribution scenarios, achieving strong generalization in simulations and hardware experiments.


<details>
  <summary>Details</summary>
Motivation: To enable reliable deployment of deep reinforcement learning in real-world settings by generalizing across both in-distribution and novel out-of-distribution conditions.

Method: Introduces a robust adaptation module and joint training pipeline for in-distribution adaptation and out-of-distribution robustness.

Result: GRAM demonstrates strong generalization performance in both in-distribution and out-of-distribution scenarios, validated through simulations and hardware experiments.

Conclusion: The framework successfully unifies and addresses generalization challenges in deep reinforcement learning for real-world applications.

Abstract: The reliable deployment of deep reinforcement learning in real-world settings
requires the ability to generalize across a variety of conditions, including
both in-distribution scenarios seen during training as well as novel
out-of-distribution scenarios. In this work, we present a framework for
dynamics generalization in deep reinforcement learning that unifies these two
distinct types of generalization within a single architecture. We introduce a
robust adaptation module that provides a mechanism for identifying and reacting
to both in-distribution and out-of-distribution environment dynamics, along
with a joint training pipeline that combines the goals of in-distribution
adaptation and out-of-distribution robustness. Our algorithm GRAM achieves
strong generalization performance across in-distribution and
out-of-distribution scenarios upon deployment, which we demonstrate through
extensive simulation and hardware locomotion experiments on a quadruped robot.

</details>


### [490] [Mitigating fairwashing using Two-Source Audits](https://arxiv.org/pdf/2305.13883)
*Jade Garcia Bourrée, Erwan Le Merrer, Gilles Tredan, Benoît Rottembourg*

Main category: cs.LG

TL;DR: The paper proposes a Two-Source Audit method to detect fairwashing in online platforms by using discrepancies between API data and a second data source, ensuring reliable audits.


<details>
  <summary>Details</summary>
Motivation: Addressing the problem of fairwashing in audits of online platforms' algorithms, where auditors can be manipulated.

Method: Introduces the Two-Source Audit setup, leveraging API data and a second data source to detect discrepancies and fairwashing attempts.

Result: Formally demonstrates conditions for success and empirically validates the method, achieving a Pareto-optimal balance between audit reliability and fairwashing detection.

Conclusion: The Two-Source Audit method enables reliable audits in manipulation-prone setups under mild assumptions.

Abstract: Recent legislation requires online platforms to provide dedicated APIs to
assess the compliance of their decision-making algorithms with the law.
Research has nevertheless shown that the auditors of such platforms are prone
to manipulation (a practice referred to as \textit{fairwashing}). To address
this salient problem, recent work has considered audits under the assumption of
partial knowledge of the platform's internal mechanisms. In this paper, we
propose a more pragmatic approach with the \textit{Two-Source Audit} setup:
while still leveraging the API, we advocate for the adjunction of a second
source of data to both perform the audit of a platform and the detection of
fairwashing attempts. Our method is based on identifying discrepancies between
the two data sources, using data proxies at use in the fairness literature. We
formally demonstrate the conditions for success in this fairwashing mitigation
task. We then validate our method empirically, demonstrating that Two-Source
Audits can achieve a Pareto-optimal balance between the two objectives. We
believe this paper sets the stage for reliable audits in manipulation-prone
setups, under mild assumptions.

</details>


### [491] [How transformers learn structured data: insights from hierarchical filtering](https://arxiv.org/pdf/2408.15138)
*Jerome Garnier-Brun, Marc Mézard, Emanuele Moscato, Luca Saglietti*

Main category: cs.LG

TL;DR: The paper explores how transformers learn hierarchical correlations in data, showing they can approximate exact inference algorithms when trained on specific tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the learning process and computational mechanisms in transformers for interpretable AI.

Method: A hierarchical filtering procedure for sequence data models on trees, training vanilla encoder-only transformers on root classification and masked language modeling tasks.

Result: Transformers sequentially include correlations at larger distances (hierarchy levels) during training, reconstructing correlations on successive scales.

Conclusion: The study provides evidence of transformers implementing exact inference algorithms by reconstructing hierarchical correlations.

Abstract: Understanding the learning process and the embedded computation in
transformers is becoming a central goal for the development of interpretable
AI. In the present study, we introduce a hierarchical filtering procedure for
data models of sequences on trees, allowing us to hand-tune the range of
positional correlations in the data. Leveraging this controlled setting, we
provide evidence that vanilla encoder-only transformers can approximate the
exact inference algorithm when trained on root classification and masked
language modeling tasks, and study how this computation is discovered and
implemented. We find that correlations at larger distances, corresponding to
increasing layers of the hierarchy, are sequentially included by the network
during training. By comparing attention maps from models trained with varying
degrees of filtering and by probing the different encoder levels, we find clear
evidence of a reconstruction of correlations on successive length scales
corresponding to the various levels of the hierarchy, which we relate to a
plausible implementation of the exact inference algorithm within the same
architecture.

</details>


### [492] [Provably Cost-Sensitive Adversarial Defense via Randomized Smoothing](https://arxiv.org/pdf/2310.08732)
*Yuan Xin, Dingfan Chen, Michael Backes, Xiao Zhang*

Main category: cs.LG

TL;DR: The paper introduces a cost-sensitive robust learning algorithm to address adversarial perturbations, focusing on varying importance of misclassifications. It provides a novel certification method and training approach, validated on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing defenses against adversarial attacks treat all perturbations equally, which doesn't align with real-world scenarios where some misclassifications are more harmful. The paper aims to address this gap by incorporating cost-sensitive robustness.

Method: The authors formalize cost-sensitive certified radius and adapt randomized smoothing for scalable certification. They also design a robust training method to improve certified robustness without losing accuracy.

Result: Experiments on benchmark datasets show the algorithm's effectiveness, even in challenging cases where existing methods fail.

Conclusion: The proposed method successfully bridges the gap between theoretical robustness and practical cost-sensitive requirements, offering a scalable and accurate solution.

Abstract: As ML models are increasingly deployed in critical applications, robustness
against adversarial perturbations is crucial. While numerous defenses have been
proposed to counter such attacks, they typically assume that all adversarial
transformations are equally important, an assumption that rarely aligns with
real-world applications. To address this, we study the problem of robust
learning against adversarial perturbations under cost-sensitive scenarios,
where the potential harm of different types of misclassifications is encoded in
a cost matrix. Our solution introduces a provably robust learning algorithm to
certify and optimize for cost-sensitive robustness, building on the scalable
certification framework of randomized smoothing. Specifically, we formalize the
definition of cost-sensitive certified radius and propose our novel adaptation
of the standard certification algorithm to generate tight robustness
certificates tailored to any cost matrix. In addition, we design a robust
training method that improves certified cost-sensitive robustness without
compromising model accuracy. Extensive experiments on benchmark datasets,
including challenging ones unsolvable by existing methods, demonstrate the
effectiveness of our certification algorithm and training method across various
cost-sensitive scenarios.

</details>


### [493] [TPP-LLM: Modeling Temporal Point Processes by Efficiently Fine-Tuning Large Language Models](https://arxiv.org/pdf/2410.02062)
*Zefang Liu, Yinzhu Quan*

Main category: cs.LG

TL;DR: TPP-LLM integrates LLMs with TPPs to model event sequences by capturing semantic and temporal aspects, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional TPPs lack semantic understanding of event types, relying on categorical representations. TPP-LLM addresses this by leveraging textual descriptions for richer semantics.

Method: TPP-LLM combines LLMs with TPPs, using textual event descriptions, temporal embeddings, and PEFT for efficient temporal learning.

Result: TPP-LLM achieves higher predictive accuracy and computational efficiency, outperforming state-of-the-art baselines.

Conclusion: Combining LLMs with TPPs enhances event sequence modeling, demonstrating the value of integrating semantic and temporal information.

Abstract: Temporal point processes (TPPs) are widely used to model the timing and
occurrence of events in domains such as social networks, transportation
systems, and e-commerce. In this paper, we introduce TPP-LLM, a novel framework
that integrates large language models (LLMs) with TPPs to capture both the
semantic and temporal aspects of event sequences. Unlike traditional methods
that rely on categorical event type representations, TPP-LLM directly utilizes
the textual descriptions of event types, enabling the model to capture rich
semantic information embedded in the text. While LLMs excel at understanding
event semantics, they are less adept at capturing temporal patterns. To address
this, TPP-LLM incorporates temporal embeddings and employs parameter-efficient
fine-tuning (PEFT) methods to effectively learn temporal dynamics without
extensive retraining. This approach improves both predictive accuracy and
computational efficiency. Experimental results across diverse real-world
datasets demonstrate that TPP-LLM outperforms state-of-the-art baselines in
sequence modeling and event prediction, highlighting the benefits of combining
LLMs with TPPs.

</details>


### [494] [Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks](https://arxiv.org/pdf/2405.15481)
*Jialin Zhao, Yingtao Zhang, Xinghang Li, Huaping Liu, Carlo Vittorio Cannistraci*

Main category: cs.LG

TL;DR: Sparse Spectral Training (SST) optimizes GPU memory usage for neural network pre-training by selectively updating singular values and vectors, outperforming other low-rank methods and matching full-rank training performance.


<details>
  <summary>Details</summary>
Motivation: The increasing size of neural networks demands memory-efficient training methods, as existing techniques like LoRA and ReLoRA face limitations in pre-training tasks.

Method: SST updates all singular values and selectively updates singular vectors using multinomial sampling. It uses SVD for initialization and periodic reinitialization to reduce distortion.

Result: SST reduces the perplexity gap by 97.4% on LLaMA-1.3B compared to other low-rank methods, using only 18.7% trainable parameters.

Conclusion: SST is an effective, parameter-efficient technique for model pre-training, achieving performance comparable to full-rank training.

Abstract: The growing demands on GPU memory posed by the increasing number of neural
network parameters call for training approaches that are more memory-efficient.
Previous memory reduction training techniques, such as Low-Rank Adaptation
(LoRA) and ReLoRA, face challenges, with LoRA being constrained by its low-rank
structure, particularly during intensive tasks like pre-training, and ReLoRA
suffering from saddle point issues. In this paper, we propose Sparse Spectral
Training (SST) to optimize memory usage for pre-training. SST updates all
singular values and selectively updates singular vectors through a multinomial
sampling method weighted by the magnitude of the singular values. Furthermore,
SST employs singular value decomposition to initialize and periodically
reinitialize low-rank parameters, reducing distortion relative to full-rank
training compared to other low-rank methods. Through comprehensive testing on
both Euclidean and hyperbolic neural networks across various tasks, SST
demonstrates its ability to outperform existing memory reduction training
methods and is comparable to full-rank training in various cases. On
LLaMA-1.3B, with only 18.7\% of the parameters trainable compared to full-rank
training (using a rank equivalent to 6\% of the embedding dimension), SST
reduces the perplexity gap between other low-rank methods and full-rank
training by 97.4\%. This result highlights SST as an effective
parameter-efficient technique for model pre-training.

</details>


### [495] [RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation](https://arxiv.org/pdf/2501.08617)
*Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fernández Fisac*

Main category: cs.LG

TL;DR: RLHF can cause misalignment due to evaluator feedback depending on foresight, while RLHS (using hindsight) mitigates this and improves alignment.


<details>
  <summary>Details</summary>
Motivation: Address the systematic misalignment in RLHF caused by evaluator feedback relying on foresight, which can be influenced by AI outputs.

Method: Introduce RLHS, which uses hindsight simulation to decouple alignment signals from compromised predictions, validated in consultancy settings with PPO and DPO.

Result: RLHS outperforms RLHF in alignment, showing robust generalization in benchmarks like TruthfulQA and HaluEval.

Conclusion: RLHS is a promising alternative to RLHF, mitigating misalignment and improving generalization.

Abstract: While Reinforcement Learning from Human Feedback (RLHF) has shown promise in
aligning generative AI, we present empirical evidence that it can also cause
severe, systematic misalignment. We hypothesize that this stems from evaluator
feedback depending on downstream outcome predictions (foresight) that can be
influenced by the AI's output, inducing Goodhart's law dynamics. We present a
theoretical analysis showing that conditioning evaluator feedback on downstream
observations (hindsight) inhibits this effect by decoupling the alignment
signal from potentially compromised predictions--crucially, the result holds
even if the observed outcomes are sampled from the AI's own world model.
Building on this insight, we introduce Reinforcement Learning from Hindsight
Simulation (RLHS), which presents plausible simulated outcomes to evaluators
before eliciting feedback. We validate RLHS across three consultancy
settings--marketplace interactions, restaurant recommendations, and online
course advising--using both online (PPO) and offline (DPO) fine-tuning methods,
and show that it substantially improves alignment over RLHF in experiments and
human evaluations. We perform post-hoc benchmark evaluations on TruthfulQA,
HaluEval, and TrustLLM, finding that even after single-task fine-tuning, RLHF
misalignment persists, whereas RLHS consistently outperforms baselines and
demonstrates robust alignment generalization. The project webpage and code are
available at https://rl-hindsight.github.io.

</details>


### [496] [Adapting Prediction Sets to Distribution Shifts Without Labels](https://arxiv.org/pdf/2406.01416)
*Kevin Kasa, Zhiyu Zhang, Heng Yang, Graham W. Taylor*

Main category: cs.LG

TL;DR: The paper introduces ECP and EACP to improve conformal prediction (CP) under distribution shifts using unlabeled test data, achieving near-supervised performance.


<details>
  <summary>Details</summary>
Motivation: Address the performance impairment of confidence set predictions due to distribution shifts and lack of test-time labels.

Method: Proposes ECP and EACP, adjusting CP's score function based on the base model's uncertainty evaluation.

Result: ECP and EACP outperform baselines and nearly match supervised methods in experiments.

Conclusion: The methods effectively enhance CP's robustness to distribution shifts without requiring labeled test data.

Abstract: Recently there has been a surge of interest to deploy confidence set
predictions rather than point predictions in machine learning. Unfortunately,
the effectiveness of such prediction sets is frequently impaired by
distribution shifts in practice, and the challenge is often compounded by the
lack of ground truth labels at test time. Focusing on a standard set-valued
prediction framework called conformal prediction (CP), this paper studies how
to improve its practical performance using only unlabeled data from the shifted
test domain. This is achieved by two new methods called ECP and EACP, whose
main idea is to adjust the score function in CP according to its base model's
own uncertainty evaluation. Through extensive experiments on a number of
large-scale datasets and neural network architectures, we show that our methods
provide consistent improvement over existing baselines and nearly match the
performance of fully supervised methods.

</details>


### [497] [Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction](https://arxiv.org/pdf/2502.15794)
*Yudong W. Xu, Wenhao Li, Scott Sanner, Elias B. Khalil*

Main category: cs.LG

TL;DR: ConsFormer is a self-supervised Transformer framework for solving CSPs without requiring labeled data or complex rewards, outperforming traditional supervised and reinforcement learning methods.


<details>
  <summary>Details</summary>
Motivation: Existing CSP solutions rely on supervised or reinforcement learning, which need feasible solutions or complex rewards. ConsFormer addresses these limitations with self-supervised learning.

Method: Uses a Transformer as a solution refiner, mimicking local search. Differentiable approximations of CSP constraints guide training, enabling iterative improvement of random assignments.

Result: Effective on Sudoku, Graph Coloring, Nurse Rostering, and MAXCUT, even for out-of-distribution problems through additional iterations.

Conclusion: ConsFormer offers a scalable, self-supervised alternative to traditional CSP-solving methods, handling diverse problems without labeled data.

Abstract: We present a Transformer-based framework for Constraint Satisfaction Problems
(CSPs). CSPs find use in many applications and thus accelerating their solution
with machine learning is of wide interest. Most existing approaches rely on
supervised learning from feasible solutions or reinforcement learning,
paradigms that require either feasible solutions to these NP-Complete CSPs or
large training budgets and a complex expert-designed reward signal. To address
these challenges, we propose ConsFormer, a self-supervised framework that
leverages a Transformer as a solution refiner. ConsFormer constructs a solution
to a CSP iteratively in a process that mimics local search. Instead of using
feasible solutions as labeled data, we devise differentiable approximations to
the discrete constraints of a CSP to guide model training. Our model is trained
to improve random assignments for a single step but is deployed iteratively at
test time, circumventing the bottlenecks of supervised and reinforcement
learning. Experiments on Sudoku, Graph Coloring, Nurse Rostering, and MAXCUT
demonstrate that our method can tackle out-of-distribution CSPs simply through
additional iterations.

</details>


### [498] [On the Hardness of Sampling from Mixture Distributions via Langevin Dynamics](https://arxiv.org/pdf/2406.02017)
*Xiwei Cheng, Kexin Fu, Farzan Farnia*

Main category: cs.LG

TL;DR: The paper analyzes Langevin Dynamics (LD) for sampling from mixture distributions, reveals its limitations in high dimensions, and proposes Chained-LD for faster convergence.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored convergence behavior of LD in sampling from mixture distributions with distinct modes.

Method: Theoretical analysis of LD for mixture distributions and introduction of Chained-LD, which processes data in smaller patches.

Result: LD may fail to find all components in high dimensions; Chained-LD shows faster convergence.

Conclusion: Chained-LD is a promising alternative to LD for efficient sampling from mixture distributions, supported by theoretical and experimental validation.

Abstract: The Langevin Dynamics (LD), which aims to sample from a probability
distribution using its score function, has been widely used for analyzing and
developing score-based generative modeling algorithms. While the convergence
behavior of LD in sampling from a uni-modal distribution has been extensively
studied in the literature, the analysis of LD under a mixture distribution with
distinct modes remains underexplored in the literature. In this work, we
analyze LD in sampling from a mixture distribution and theoretically study its
convergence properties. Our theoretical results indicate that for general
mixture distributions of sub-Gaussian components, LD could fail in finding all
the components within a sub-exponential number of steps in the data dimension.
Following our result on the complexity of LD in sampling from high-dimensional
variables, we propose Chained Langevin Dynamics (Chained-LD), which divides the
data vector into patches of smaller sizes and generates every patch
sequentially conditioned on the previous patches. Our theoretical analysis of
Chained-LD indicates its faster convergence speed to the components of a
mixture distribution. We present the results of several numerical experiments
on synthetic and real image datasets, validating our theoretical results on the
iteration complexities of sample generation from mixture distributions using
the vanilla and chained LD algorithms.

</details>


### [499] [Approximation-Aware Bayesian Optimization](https://arxiv.org/pdf/2406.04308)
*Natalie Maus, Kyurae Kim, Geoff Pleiss, David Eriksson, John P. Cunningham, Jacob R. Gardner*

Main category: cs.LG

TL;DR: The paper improves high-dimensional Bayesian optimization (BO) by modifying sparse variational Gaussian processes (SVGPs) to focus on informed data acquisition, outperforming standard SVGPs in tasks like molecular design.


<details>
  <summary>Details</summary>
Motivation: Standard SVGPs reduce computational costs but lead to suboptimal data acquisitions, slowing optimization progress in high-dimensional BO tasks.

Method: The authors modify SVGPs using utility-calibrated variational inference, unifying GP approximation and data acquisition into a joint optimization problem for optimal decisions under limited budgets.

Result: The approach outperforms standard SVGPs in high-dimensional benchmarks, including control and molecular design tasks.

Conclusion: The proposed method aligns SVGPs with BO goals, improving efficiency and performance in high-dimensional optimization tasks.

Abstract: High-dimensional Bayesian optimization (BO) tasks such as molecular design
often require 10,000 function evaluations before obtaining meaningful results.
While methods like sparse variational Gaussian processes (SVGPs) reduce
computational requirements in these settings, the underlying approximations
result in suboptimal data acquisitions that slow the progress of optimization.
In this paper we modify SVGPs to better align with the goals of BO: targeting
informed data acquisition rather than global posterior fidelity. Using the
framework of utility-calibrated variational inference, we unify GP
approximation and data acquisition into a joint optimization problem, thereby
ensuring optimal decisions under a limited computational budget. Our approach
can be used with any decision-theoretic acquisition function and is compatible
with trust region methods like TuRBO. We derive efficient joint objectives for
the expected improvement and knowledge gradient acquisition functions in both
the standard and batch BO settings. Our approach outperforms standard SVGPs on
high-dimensional benchmark tasks in control and molecular design.

</details>


### [500] [Calibrated Physics-Informed Uncertainty Quantification](https://arxiv.org/pdf/2502.04406)
*Vignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timothy Nunn, Daniel Giles, Matt J. Kusner, Stanislas Pamela, Marc Peter Deisenroth*

Main category: cs.LG

TL;DR: The paper introduces a physics-informed conformal prediction framework for neural PDEs to provide guaranteed uncertainty estimates without labeled data, addressing limitations in traditional PDE solvers and neural PDEs.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE solvers are computationally expensive, and neural PDEs lack robust uncertainty quantification, limiting their use in critical applications.

Method: A model-agnostic, physics-informed conformal prediction framework is proposed, using physics residual errors for uncertainty quantification without labeled data.

Result: The method provides marginal and joint coverage guarantees for complex PDEs, validated on neural PDE models for plasma modeling and fusion reactor shot design.

Conclusion: The framework offers a scalable and efficient solution for uncertainty quantification in neural PDEs, enhancing their applicability in critical domains.

Abstract: Simulating complex physical systems is crucial for understanding and
predicting phenomena across diverse fields, such as fluid dynamics and heat
transfer, as well as plasma physics and structural mechanics. Traditional
approaches rely on solving partial differential equations (PDEs) using
numerical methods, which are computationally expensive and often prohibitively
slow for real-time applications or large-scale simulations. Neural PDEs have
emerged as efficient alternatives to these costly numerical solvers, offering
significant computational speed-ups. However, their lack of robust uncertainty
quantification (UQ) limits deployment in critical applications. We introduce a
model-agnostic, physics-informed conformal prediction (CP) framework that
provides guaranteed uncertainty estimates without requiring labelled data. By
utilising a physics-based approach, we can quantify and calibrate the model's
inconsistencies with the physics rather than the uncertainty arising from the
data. Our approach utilises convolutional layers as finite-difference stencils
and leverages physics residual errors as nonconformity scores, enabling
data-free UQ with marginal and joint coverage guarantees across prediction
domains for a range of complex PDEs. We further validate the efficacy of our
method on neural PDE models for plasma modelling and shot design in fusion
reactors.

</details>


### [501] [Identifiable Latent Bandits: Leveraging observational data for personalized decision-making](https://arxiv.org/pdf/2407.16239)
*Ahmet Zahid Balcıoğlu, Newton Mwai, Emil Carlsson, Fredrik D. Johansson*

Main category: cs.LG

TL;DR: The paper introduces an identifiable latent bandit framework for faster personalized decision-making by leveraging historical data and nonlinear independent component analysis.


<details>
  <summary>Details</summary>
Motivation: Historical data alone is insufficient for optimal personalized decisions, and traditional bandit methods require too many trials for practical use.

Method: Proposes a latent bandit framework using nonlinear independent component analysis to learn identifiable representations from historical data.

Result: The method reduces exploration time and outperforms online and offline baselines in simulated and semi-synthetic environments.

Conclusion: The framework enables efficient personalized decision-making when identification conditions are met.

Abstract: For many decision-making tasks, such as precision medicine, historical data
alone are insufficient to determine the right choice for a new problem instance
or patient. Online algorithms like multi-armed bandits can find optimal
personalized decisions but are notoriously sample-hungry. In practice, training
a bandit for a new individual from scratch is often infeasible, as the number
of trials required is larger than the practical number of decision points.
Latent bandits offer rapid exploration and personalization beyond what context
variables can reveal, provided that a latent variable model can be learned
consistently. In this work, we propose an identifiable latent bandit framework
that leads to optimal decision-making with a shorter exploration time than
classical bandits by learning from historical records of decisions and
outcomes. Our method is based on nonlinear independent component analysis that
provably identifies representations from observational data sufficient to infer
the optimal action in new bandit instances. We verify this strategy in
simulated and semi-synthetic environments, showing substantial improvement over
online and offline learning baselines when identifying conditions are
satisfied.

</details>


### [502] [Understanding Bias Reinforcement in LLM Agents Debate](https://arxiv.org/pdf/2503.16814)
*Jihwan Oh, Minchan Jeong, Jongwoo Ko, Se-Young Yun*

Main category: cs.LG

TL;DR: The paper introduces DReaMAD, a framework to improve LLM decision-making by refining strategic prior knowledge and promoting diverse viewpoints, addressing biases in multi-agent debates.


<details>
  <summary>Details</summary>
Motivation: Existing self-correction and multi-agent debate methods for LLMs often reinforce biases and lack perspective diversity, limiting reasoning correctness.

Method: Proposes DReaMAD, which refines LLM's strategic prior knowledge and modifies prompts to encourage diverse reasoning within a single model.

Result: DReaMAD significantly enhances decision accuracy, reasoning diversity, and bias mitigation in strategic tasks.

Conclusion: DReaMAD is a more effective approach for LLM-based decision-making, overcoming limitations of existing methods.

Abstract: Large Language Models $($LLMs$)$ solve complex problems using training-free
methods like prompt engineering and in-context learning, yet ensuring reasoning
correctness remains challenging. While self-correction methods such as
self-consistency and self-refinement aim to improve reliability, they often
reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent
Debate $($MAD$)$ has emerged as an alternative, but we identify two key
limitations: bias reinforcement, where debate amplifies model biases instead of
correcting them, and lack of perspective diversity, as all agents share the
same model and reasoning patterns, limiting true debate effectiveness. To
systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a
benchmark designed to assess LLMs in adversarial strategic decision-making,
where dynamic interactions influence optimal decisions. To overcome MAD's
limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse
$\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate
with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic
prior knowledge to improve reasoning quality and $(2)$ promotes diverse
viewpoints within a single model by systematically modifying prompts, reducing
bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves
decision accuracy, reasoning diversity, and bias mitigation across multiple
strategic tasks, establishing it as a more effective approach for LLM-based
decision-making.

</details>


### [503] [Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer](https://arxiv.org/pdf/2502.04573)
*Yulun Wu, Doron L. Bergman*

Main category: cs.LG

TL;DR: APT is a zero-shot meta-learning model for tabular tasks, pre-trained with adversarial synthetic data, outperforming TabPFN and handling arbitrary class sizes.


<details>
  <summary>Details</summary>
Motivation: To improve zero-shot meta-learning for tabular tasks by addressing limitations like class size constraints and lack of diverse pre-training data.

Method: APT uses adversarial synthetic data agents and a mixture block architecture for flexible classification.

Result: Matches state-of-the-art performance on small tasks, enhances TabPFN, and improves diversity in synthetic data.

Conclusion: Adversarial pre-training and mixture block design boost performance and generalizability in tabular zero-shot learning.

Abstract: We present an Adversarially Pre-trained Transformer (APT) that is able to
perform zero-shot meta-learning on tabular prediction tasks without
pre-training on any real-world dataset, extending on the recent development of
Prior-Data Fitted Networks (PFNs) and TabPFN. Specifically, APT is pre-trained
with adversarial synthetic data agents, who continue to shift their underlying
data generating distribution and deliberately challenge the model with
different synthetic datasets. In addition, we propose a mixture block
architecture that is able to handle classification tasks with arbitrary number
of classes, addressing the class size limitation -- a crucial weakness of prior
deep tabular zero-shot learners. In experiments, we show that our framework
matches state-of-the-art performance on small classification tasks without
filtering on dataset characteristics such as number of classes and number of
missing values, while maintaining an average runtime under one second. On
common benchmark dataset suites in both classification and regression, we show
that adversarial pre-training was able to enhance TabPFN's performance. In our
analysis, we demonstrate that the adversarial synthetic data agents were able
to generate a more diverse collection of data compared to the ordinary random
generator in TabPFN. In addition, we demonstrate that our mixture block neural
design has improved generalizability and greatly accelerated pre-training.

</details>


### [504] [Directed Exploration in Reinforcement Learning from Linear Temporal Logic](https://arxiv.org/pdf/2408.09495)
*Marco Bagatella, Andreas Krause, Georg Martius*

Main category: cs.LG

TL;DR: The paper proposes a method to improve exploration in LTL-based reinforcement learning by leveraging LTL specifications and LDBA dynamics to estimate high-level values and intrinsic rewards.


<details>
  <summary>Details</summary>
Motivation: Current LTL-based RL methods suffer from sparse rewards, limiting scalability to low-dimensional, short-horizon problems. The goal is to enhance exploration.

Method: Translate LTL formulas into LDBA, treat it as a Markov reward process, and estimate values using a Bayesian perspective. Map these to intrinsic rewards.

Result: The method enables better exploration and scalability, demonstrated in tabular and high-dimensional continuous systems.

Conclusion: The approach successfully addresses exploration challenges in LTL-based RL, extending applicability to more complex problems.

Abstract: Linear temporal logic (LTL) is a powerful language for task specification in
reinforcement learning, as it allows describing objectives beyond the
expressivity of conventional discounted return formulations. Nonetheless,
recent works have shown that LTL formulas can be translated into a variable
rewarding and discounting scheme, whose optimization produces a policy
maximizing a lower bound on the probability of formula satisfaction. However,
the synthesized reward signal remains fundamentally sparse, making exploration
challenging. We aim to overcome this limitation, which can prevent current
algorithms from scaling beyond low-dimensional, short-horizon problems. We show
how better exploration can be achieved by further leveraging the LTL
specification and casting its corresponding Limit Deterministic B\"uchi
Automaton (LDBA) as a Markov reward process, thus enabling a form of high-level
value estimation. By taking a Bayesian perspective over LDBA dynamics and
proposing a suitable prior distribution, we show that the values estimated
through this procedure can be treated as a shaping potential and mapped to
informative intrinsic rewards. Empirically, we demonstrate applications of our
method from tabular settings to high-dimensional continuous systems, which have
so far represented a significant challenge for LTL-based reinforcement learning
algorithms.

</details>


### [505] [Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models](https://arxiv.org/pdf/2503.22879)
*Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, Diana Marculescu*

Main category: cs.LG

TL;DR: Quamba2 is a quantization method for State Space Models (SSMs) that supports multiple bit-width configurations, reducing memory usage and improving speed with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: SSMs face challenges in scaling due to storage and computational demands. Quantization can help but must balance efficiency and performance across different scenarios.

Method: Quamba2 uses an offline approach with channel order preserving and activation persistence, sorting and clustering inputs, and per-state-group quantization for parameters.

Result: Quamba2-8B outperforms existing methods, offering speed-ups (1.3× pre-filling, 3× generation) and 4× memory reduction with only a 1.6% accuracy drop.

Conclusion: Quamba2 is a robust and generalizable solution for deploying SSMs efficiently across various platforms.

Abstract: State Space Models (SSMs) are emerging as a compelling alternative to
Transformers because of their consistent memory usage and high performance.
Despite this, scaling up SSMs on cloud services or limited-resource devices is
challenging due to their storage requirements and computational power. To
overcome this, quantizing SSMs with low bit-width data formats can reduce model
size and benefit from hardware acceleration. As SSMs are prone to
quantization-induced errors, recent efforts have focused on optimizing a
particular model or bit-width for efficiency without sacrificing performance.
However, distinct bit-width configurations are essential for different
scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for
enhancing generation speed in short prompt applications for a single user. To
this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both
Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment
on various platforms. Based on the channel order preserving and activation
persistence of SSMs, we propose an offline approach to quantize inputs of a
linear recurrence in 8-bit by sorting and clustering for input $x$, combined
with a per-state-group quantization for input-dependent parameters $B$ and $C$.
To ensure compute-invariance in the SSM output, we rearrange weights offline
according to the clustering sequence. The experiments show that Quamba2-8B
outperforms two state-of-the-art SSM quantization methods and delivers
1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages,
respectively, while offering 4$\times$ memory reduction with only a $1.6\%$
average accuracy drop. The evaluation on MMLU shows the generalizability and
robustness of our framework. The code and quantized models will be released at:
https://github.com/enyac-group/Quamba.

</details>


### [506] [Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty](https://arxiv.org/pdf/2502.06905)
*Yeseul Cho, Baekrok Shin, Changmin Kang, Chulhee Yun*

Main category: cs.LG

TL;DR: The paper introduces DUAL score for dataset pruning, reducing time and cost while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Deep learning relies on large datasets, which are costly. Existing pruning methods are expensive as they require full training.

Method: Proposes DUAL score (Difficulty and Uncertainty-Aware Lightweight) and ratio-adaptive sampling using Beta distribution.

Result: Achieves SOTA performance with reduced time: 66% on ImageNet-1k, 15% on CIFAR datasets.

Conclusion: DUAL score effectively prunes datasets early, saving time and resources without sacrificing accuracy.

Abstract: Recent advances in deep learning rely heavily on massive datasets, leading to
substantial storage and training costs. Dataset pruning aims to alleviate this
demand by discarding redundant examples. However, many existing methods require
training a model with a full dataset over a large number of epochs before being
able to prune the dataset, which ironically makes the pruning process more
expensive than just training the model on the entire dataset. To overcome this
limitation, we introduce a Difficulty and Uncertainty-Aware Lightweight (DUAL)
score, which aims to identify important samples from the early training stage
by considering both example difficulty and prediction uncertainty. To address a
catastrophic accuracy drop at an extreme pruning, we further propose a
ratio-adaptive sampling using Beta distribution. Experiments on various
datasets and learning scenarios such as image classification with label noise
and image corruption, and model architecture generalization demonstrate the
superiority of our method over previous state-of-the-art (SOTA) approaches.
Specifically, on ImageNet-1k, our method reduces the time cost for pruning to
66% compared to previous methods while achieving a SOTA, specifically 60% test
accuracy at a 90% pruning ratio. On CIFAR datasets, the time cost is reduced to
just 15% while maintaining SOTA performance.

</details>


### [507] [Improved Variational Inference in Discrete VAEs using Error Correcting Codes](https://arxiv.org/pdf/2410.07840)
*María Martínez-García, Grace Villacrés, David Mitchell, Pablo M. Olmos*

Main category: cs.LG

TL;DR: A novel method improves discrete Variational Autoencoders by using Error-Correcting Codes (ECCs) to enhance latent representations, boosting generation quality and reducing variational gaps.


<details>
  <summary>Details</summary>
Motivation: Learning discrete latent representations is challenging in deep probabilistic models, prompting a need for better inference methods.

Method: Reframes inference as a communication system, applying ECCs for redundancy in latent variables, tested with binary latents and repetition codes in a hierarchical structure.

Result: Significant improvements in generation quality, data reconstruction, and uncertainty calibration, outperforming uncoded models.

Conclusion: ECCs effectively enhance discrete variational inference, with specific properties required for optimal performance.

Abstract: Despite advances in deep probabilistic models, learning discrete latent
representations remains challenging. This work introduces a novel method to
improve inference in discrete Variational Autoencoders by reframing the
inference problem through a generative perspective. We conceptualize the model
as a communication system, and propose to leverage Error-Correcting Codes
(ECCs) to introduce redundancy in latent representations, allowing the
variational posterior to produce more accurate estimates and reduce the
variational gap. We present a proof-of-concept using a Discrete Variational
Autoencoder with binary latent variables and low-complexity repetition codes,
extending it to a hierarchical structure for disentangling global and local
data features. Our approach significantly improves generation quality, data
reconstruction, and uncertainty calibration, outperforming the uncoded models
even when trained with tighter bounds such as the Importance Weighted
Autoencoder objective. We also outline the properties that ECCs should possess
to be effectively utilized for improved discrete variational inference.

</details>


### [508] [Comprehensive Review of Neural Differential Equations for Time Series Analysis](https://arxiv.org/pdf/2502.09885)
*YongKyung Oh, Seungsu Kam, Jonghun Lee, Dong-Young Lim, Sungil Kim, Alex Bui*

Main category: cs.LG

TL;DR: A review of Neural Differential Equations (NDEs) for time series analysis, addressing their advantages over traditional methods like RNNs and Transformers in modeling continuous dynamics and irregular sampling.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with continuous dynamics and irregular sampling in real-world time series data, motivating the exploration of NDEs.

Method: The paper reviews NDE-based methods, including neural ODEs, controlled ODEs, and stochastic ODEs, detailing their mathematical formulations and numerical methods.

Result: NDEs offer a robust framework for modeling continuous-time dynamics, outperforming conventional methods in certain scenarios.

Conclusion: The survey highlights NDEs' potential for advanced time series analysis and outlines future research directions.

Abstract: Time series modeling and analysis have become critical in various domains.
Conventional methods such as RNNs and Transformers, while effective for
discrete-time and regularly sampled data, face significant challenges in
capturing the continuous dynamics and irregular sampling patterns inherent in
real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm
shift by combining the flexibility of neural networks with the mathematical
rigor of differential equations. This paper presents a comprehensive review of
NDE-based methods for time series analysis, including neural ordinary
differential equations, neural controlled differential equations, and neural
stochastic differential equations. We provide a detailed discussion of their
mathematical formulations, numerical methods, and applications, highlighting
their ability to model continuous-time dynamics. Furthermore, we address key
challenges and future research directions. This survey serves as a foundation
for researchers and practitioners seeking to leverage NDEs for advanced time
series analysis.

</details>


### [509] [MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning](https://arxiv.org/pdf/2410.11226)
*Peter Eckmann, Dongxia Wu, Germano Heinzelmann, Michael K. Gilson, Rose Yu*

Main category: cs.LG

TL;DR: MF-LAL is a generative modeling framework that integrates multi-fidelity oracles for drug discovery, improving binding free energy scores by ~50% over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current generative models rely on inaccurate docking scores, while accurate methods are too costly. MF-LAL addresses this by combining multi-fidelity oracles and active learning.

Method: MF-LAL integrates generative and surrogate models into one framework, using active learning to guide compound generation with varying cost-accuracy oracles.

Result: Experiments show ~50% improvement in mean binding free energy scores compared to other approaches.

Conclusion: MF-LAL offers a practical solution for generating high-activity compounds by leveraging multi-fidelity oracles and active learning.

Abstract: Current generative models for drug discovery primarily use molecular docking
as an oracle to guide the generation of active compounds. However, such models
are often not useful in practice because even compounds with high docking
scores do not consistently show real-world experimental activity. More accurate
methods for activity prediction exist, such as molecular dynamics based binding
free energy calculations, but they are too computationally expensive to use in
a generative model. To address this challenge, we propose Multi-Fidelity Latent
space Active Learning (MF-LAL), a generative modeling framework that integrates
a set of oracles with varying cost-accuracy tradeoffs. Using active learning,
we train a surrogate model for each oracle and use these surrogates to guide
generation of compounds with high predicted activity. Unlike previous
approaches that separately learn the surrogate model and generative model,
MF-LAL combines the generative and multi-fidelity surrogate models into a
single framework, allowing for more accurate activity prediction and higher
quality samples. Our experiments on two disease-relevant proteins show that
MF-LAL produces compounds with significantly better binding free energy scores
than other single and multi-fidelity approaches (~50% improvement in mean
binding free energy score). The code is available at
https://github.com/Rose-STL-Lab/MF-LAL.

</details>


### [510] [Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning](https://arxiv.org/pdf/2502.10550)
*Egor Cherepanov, Nikita Kachaev, Alexey K. Kovalev, Aleksandr I. Panov*

Main category: cs.LG

TL;DR: The paper introduces MIKASA, a benchmark for evaluating memory capabilities in reinforcement learning (RL) agents, focusing on tabletop robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: The lack of a universal benchmark for assessing memory in RL agents, especially in tasks like robotic manipulation with partial observability, motivated this work.

Method: The authors propose MIKASA, which includes a classification framework for memory-intensive tasks, a unified benchmark (MIKASA-Base), and a specialized benchmark for robotic manipulation (MIKASA-Robo).

Result: MIKASA provides a standardized way to evaluate memory-enhanced RL agents across diverse scenarios, including 32 tasks for robotic manipulation.

Conclusion: MIKASA advances memory RL research by offering a unified framework, enabling more robust real-world applications.

Abstract: Memory is crucial for enabling agents to tackle complex tasks with temporal
and spatial dependencies. While many reinforcement learning (RL) algorithms
incorporate memory, the field lacks a universal benchmark to assess an agent's
memory capabilities across diverse scenarios. This gap is particularly evident
in tabletop robotic manipulation, where memory is essential for solving tasks
with partial observability and ensuring robust performance, yet no standardized
benchmarks exist. To address this, we introduce MIKASA (Memory-Intensive Skills
Assessment Suite for Agents), a comprehensive benchmark for memory RL, with
three key contributions: (1) we propose a comprehensive classification
framework for memory-intensive RL tasks, (2) we collect MIKASA-Base -- a
unified benchmark that enables systematic evaluation of memory-enhanced agents
across diverse scenarios, and (3) we develop MIKASA-Robo (pip install
mikasa-robo-suite) -- a novel benchmark of 32 carefully designed
memory-intensive tasks that assess memory capabilities in tabletop robotic
manipulation. Our work introduces a unified framework to advance memory RL
research, enabling more robust systems for real-world use. MIKASA is available
at https://tinyurl.com/membenchrobots.

</details>


### [511] [BioNeMo Framework: a modular, high-performance library for AI model development in drug discovery](https://arxiv.org/pdf/2411.10548)
*Peter St. John, Dejun Lin, Polina Binder, Malcolm Greaves, Vega Shah, John St. John, Adrian Lange, Patrick Hsu, Rajesh Illango, Arvind Ramanathan, Anima Anandkumar, David H Brookes, Akosua Busia, Abhishaike Mahajan, Stephen Malina, Neha Prasad, Sam Sinai, Lindsay Edwards, Thomas Gaudelet, Cristian Regep, Martin Steinegger, Burkhard Rost, Alexander Brace, Kyle Hippe, Luca Naef, Keisuke Kamata, George Armstrong, Kevin Boyd, Zhonglin Cao, Han-Yi Chou, Simon Chu, Allan dos Santos Costa, Sajad Darabi, Eric Dawson, Kieran Didi, Cong Fu, Mario Geiger, Michelle Gill, Darren Hsu, Gagan Kaushik, Maria Korshunova, Steven Kothen-Hill, Youhan Lee, Meng Liu, Micha Livne, Zachary McClure, Jonathan Mitchell, Alireza Moradzadeh, Ohad Mosafi, Youssef Nashed, Saee Paliwal, Yuxing Peng, Sara Rabhi, Farhad Ramezanghorbani, Danny Reidenbach, Camir Ricketts, Brian Roland, Kushal Shah, Tyler Shimko, Hassan Sirelkhatim, Savitha Srinivasan, Abraham C Stern, Dorota Toczydlowska, Srimukh Prasad Veccham, Niccolò Alberto Elia Venanzi, Anton Vorontsov, Jared Wilber, Isabel Wilkinson, Wei Jing Wong, Eva Xue, Cory Ye, Xin Yu, Yang Zhang, Guoqing Zhou, Becca Zandstein, Christian Dallago, Bruno Trentini, Emine Kucukbenli, Saee Paliwal, Timur Rvachov, Eddie Calleja, Johnny Israeli, Harry Clifford, Risto Haukioja, Nicholas Haemel, Kyle Tretina, Neha Tadimeti, Anthony B Costa*

Main category: cs.LG

TL;DR: The BioNeMo Framework is introduced to streamline the training of AI models in computational biology and chemistry, enabling large-scale training across hundreds of GPUs.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on computational scale for training AI models in drug development necessitates a framework like BioNeMo to simplify and enhance the process.

Method: BioNeMo offers a modular design for integrating components like data loaders into workflows, demonstrated through pLM pre-training and fine-tuning.

Result: The framework successfully trained a 3B-parameter BERT-based pLM on over 1T tokens in 4.2 days using 256 NVIDIA A100 GPUs.

Conclusion: BioNeMo is an open-source, free tool designed to support high-throughput, high-quality AI model training in computational biology and chemistry.

Abstract: Artificial Intelligence models encoding biology and chemistry are opening new
routes to high-throughput and high-quality in-silico drug development. However,
their training increasingly relies on computational scale, with recent protein
language models (pLM) training on hundreds of graphical processing units
(GPUs). We introduce the BioNeMo Framework to facilitate the training of
computational biology and chemistry AI models across hundreds of GPUs. Its
modular design allows the integration of individual components, such as data
loaders, into existing workflows and is open to community contributions. We
detail technical features of the BioNeMo Framework through use cases such as
pLM pre-training and fine-tuning. On 256 NVIDIA A100s, BioNeMo Framework trains
a three billion parameter BERT-based pLM on over one trillion tokens in 4.2
days. The BioNeMo Framework is open-source and free for everyone to use.

</details>


### [512] [CENTAUR: Bridging the Impossible Trinity of Privacy, Efficiency, and Performance in Privacy-Preserving Transformer Inference](https://arxiv.org/pdf/2412.10652)
*Jinglong Luo, Guanzhong Chen, Yehong Zhang, Shiyu Liu, Hui Wang, Yue Yu, Xun Zhou, Yuan Qi, Zenglin Xu*

Main category: cs.LG

TL;DR: CENTAUR is a new PPTI framework that combines random permutations and SMPC to balance privacy, efficiency, and performance in Transformer models.


<details>
  <summary>Details</summary>
Motivation: Addressing the 'impossible trinity' in PPTI frameworks—privacy, efficiency, and performance—where existing methods compromise one for the others.

Method: Integrates random permutations and SMPC, with algorithms tailored to Transformer models.

Result: Resists data reconstruction attacks, achieves plaintext-level accuracy, and speeds up inference by 5.0-30.4 times.

Conclusion: CENTAUR successfully balances privacy, efficiency, and performance, enabling secure and efficient AI deployment.

Abstract: With the growing deployment of pre-trained models like Transformers on cloud
platforms, privacy concerns about model parameters and inference data are
intensifying. Existing Privacy-Preserving Transformer Inference (PPTI)
frameworks face the "impossible trinity" of balancing privacy, efficiency, and
performance: Secure Multi-Party Computation (SMPC)-based approaches ensure
strong privacy but suffer from high computational overhead and performance
losses; Conversely, permutation-based methods achieve near-plaintext efficiency
and accuracy but compromise privacy by exposing sensitive model parameters and
intermediate results. Bridging this gap with a single approach presents
substantial challenges, motivating the introduction of CENTAUR, a
groundbreaking PPTI framework that seamlessly integrates random permutations
and SMPC to address the "impossible trinity". By designing efficient PPTI
algorithms tailored to the structural properties of Transformer models, CENTAUR
achieves an unprecedented balance among privacy, efficiency, and performance.
Our experiments demonstrate CENTAUR's ability to resist diverse data
reconstruction attacks, achieve plaintext-level inference accuracy, and boost
inference speed by 5.0-30.4 times, unlocking new possibilities for secure and
efficient AI deployment.

</details>


### [513] [On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis](https://arxiv.org/pdf/2502.13191)
*Junyi Guan, Abhijith Sharma, Chong Tian, Salem Lahlou*

Main category: cs.LG

TL;DR: SNNs, though energy-efficient, are vulnerable to Membership Inference Attacks (MIAs), especially with increased latency. An input dropout strategy enhances MIA success, showing SNNs are not inherently more secure than ANNs.


<details>
  <summary>Details</summary>
Motivation: To examine the privacy risks of SNNs, particularly their susceptibility to MIAs, challenging the assumption of their inherent robustness.

Method: Investigates SNNs' vulnerability to MIAs, introduces an input dropout strategy under black box settings, and compares results with ANNs.

Result: SNNs' resilience to MIAs diminishes with higher latency, and the input dropout method significantly improves attack success, revealing comparable vulnerabilities to ANNs.

Conclusion: SNNs are not inherently more secure against privacy threats like MIAs, and their vulnerabilities are similar to ANNs, despite expectations of robustness.

Abstract: Spiking Neural Networks (SNNs) are increasingly explored for their energy
efficiency and robustness in real-world applications, yet their privacy risks
remain largely unexamined. In this work, we investigate the susceptibility of
SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an
adversary attempts to determine whether a given sample was part of the training
dataset. While prior work suggests that SNNs may offer inherent robustness due
to their discrete, event-driven nature, we find that its resilience diminishes
as latency (T) increases. Furthermore, we introduce an input dropout strategy
under black box setting, that significantly enhances membership inference in
SNNs. Our findings challenge the assumption that SNNs are inherently more
secure, and even though they are expected to be better, our results reveal that
SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial
Neural Networks (ANNs). Our code is available at
https://anonymous.4open.science/r/MIA_SNN-3610.

</details>


### [514] [Improving the Noise Estimation of Latent Neural Stochastic Differential Equations](https://arxiv.org/pdf/2412.17499)
*Linus Heck, Maximilian Gelbrecht, Michael T. Schaub, Niklas Boers*

Main category: cs.LG

TL;DR: Latent neural SDEs underestimate noise in stochastic time series data. Adding noise regularization improves accuracy in capturing stochastic dynamics.


<details>
  <summary>Details</summary>
Motivation: Latent neural SDEs are promising but systematically underestimate noise, limiting their accuracy in modeling stochastic dynamics.

Method: Propose adding explicit noise regularization to the loss function to better capture the diffusion component of the data.

Result: Demonstrated improved capability to model stochastic bistable dynamics on a conceptual model system.

Conclusion: Noise regularization enhances latent neural SDEs' accuracy in capturing stochastic dynamics.

Abstract: Latent neural stochastic differential equations (SDEs) have recently emerged
as a promising approach for learning generative models from stochastic time
series data. However, they systematically underestimate the noise level
inherent in such data, limiting their ability to capture stochastic dynamics
accurately. We investigate this underestimation in detail and propose a
straightforward solution: by including an explicit additional noise
regularization in the loss function, we are able to learn a model that
accurately captures the diffusion component of the data. We demonstrate our
results on a conceptual model system that highlights the improved latent neural
SDE's capability to model stochastic bistable dynamics.

</details>


### [515] [Scalable Equilibrium Sampling with Sequential Boltzmann Generators](https://arxiv.org/pdf/2502.18462)
*Charlie B. Tan, Avishek Joey Bose, Chen Lin, Leon Klein, Michael M. Bronstein, Alexander Tong*

Main category: cs.LG

TL;DR: SBG extends Boltzmann generators with Transformer-based normalizing flows and sequential Monte Carlo, achieving state-of-the-art equilibrium sampling for peptides.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scalable sampling of molecular states in thermodynamic equilibrium by improving Boltzmann generators.

Method: Uses Transformer-based normalizing flows on Cartesian coordinates and sequential Monte Carlo with annealed Langevin dynamics.

Result: Achieves state-of-the-art performance, enabling equilibrium sampling of tri-, tetra-, and hexa-peptides.

Conclusion: SBG advances Boltzmann generators, making previously intractable peptide systems accessible for sampling.

Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a
long-standing challenge in statistical physics. Boltzmann generators tackle
this problem by pairing normalizing flows with importance sampling to obtain
uncorrelated samples under the target distribution. In this paper, we extend
the Boltzmann generator framework with two key contributions, denoting our
framework Sequential Boltzmann Generators (SBG). The first is a highly
efficient Transformer-based normalizing flow operating directly on all-atom
Cartesian coordinates. In contrast to the equivariant continuous flows of prior
methods, we leverage exactly invertible non-equivariant architectures which are
highly efficient during both sample generation and likelihood evaluation. This
efficiency unlocks more sophisticated inference strategies beyond standard
importance sampling. In particular, we perform inference-time scaling of flow
samples using a continuous-time variant of sequential Monte Carlo, in which
flow samples are transported towards the target distribution with annealed
Langevin dynamics. SBG achieves state-of-the-art performance w.r.t. all metrics
on peptide systems, demonstrating the first equilibrium sampling in Cartesian
coordinates of tri-, tetra- and hexa-peptides that were thus far intractable
for prior Boltzmann generators.

</details>


### [516] [Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models](https://arxiv.org/pdf/2501.19090)
*Jialin Zhao, Yingtao Zhang, Carlo Vittorio Cannistraci*

Main category: cs.LG

TL;DR: Pivoting Factorization (PIFA) is a lossless meta low-rank representation method that improves memory savings and inference speed over traditional low-rank pruning, while a retraining-free reconstruction method (M) enhances performance. Combined as MPIFA, it matches semi-structured pruning performance with better GPU efficiency.


<details>
  <summary>Details</summary>
Motivation: The need for efficient model compression techniques for Large Language Models due to memory and computation costs, addressing the limitations of low-rank pruning.

Method: Proposes PIFA for unsupervised learning of compact low-rank representations and introduces a retraining-free reconstruction method (M) to minimize error. Combines these into MPIFA.

Result: PIFA achieves 24.2% memory savings and 24.6% faster inference. MPIFA outperforms low-rank pruning and matches semi-structured pruning in performance with superior GPU efficiency.

Conclusion: MPIFA offers a competitive solution for model compression, balancing performance and efficiency, with potential for broader application in LLMs.

Abstract: The rapid growth of Large Language Models has driven demand for effective
model compression techniques to reduce memory and computation costs. Low-rank
pruning has gained attention for its GPU compatibility across all densities.
However, low-rank pruning struggles to match the performance of semi-structured
pruning, often doubling perplexity at similar densities. In this paper, we
propose Pivoting Factorization (PIFA), a novel lossless meta low-rank
representation that unsupervisedly learns a compact form of any low-rank
representation, effectively eliminating redundant information. PIFA identifies
pivot rows (linearly independent rows) and expresses non-pivot rows as linear
combinations, achieving 24.2% additional memory savings and 24.6% faster
inference over low-rank layers at rank = 50% of dimension. To mitigate the
performance degradation caused by low-rank pruning, we introduce a novel,
retraining-free reconstruction method that minimizes error accumulation (M).
MPIFA, combining M and PIFA into an end-to-end framework, significantly
outperforms existing low-rank pruning methods, and achieves performance
comparable to semi-structured pruning, while surpassing it in GPU efficiency
and compatibility. Our code is available at
https://github.com/biomedical-cybernetics/pivoting-factorization.

</details>


### [517] [Regularized Langevin Dynamics for Combinatorial Optimization](https://arxiv.org/pdf/2502.00277)
*Shengyu Feng, Yiming Yang*

Main category: cs.LG

TL;DR: The paper introduces Regularized Langevin Dynamics (RLD), a sampling framework for combinatorial optimization, improving exploration and avoiding local minima. It outperforms state-of-the-art methods in runtime and performance.


<details>
  <summary>Details</summary>
Motivation: Direct application of discrete Langevin dynamics (LD) in combinatorial optimization often results in limited exploration, prompting the need for a more effective method.

Method: Proposes RLD, which enforces distance between sampled and current solutions. Two solvers are developed: one based on simulated annealing (SA) and another on neural networks (NN).

Result: Empirical results show RLD-based methods achieve comparable or better performance than SOTA SA- and NN-based solvers, with the SA variant reducing runtime by up to 80%.

Conclusion: RLD is a promising framework for enhancing traditional heuristics and NN models in solving combinatorial optimization problems.

Abstract: This work proposes a simple yet effective sampling framework for
combinatorial optimization (CO). Our method builds on discrete Langevin
dynamics (LD), an efficient gradient-guided generative paradigm. However, we
observe that directly applying LD often leads to limited exploration. To
overcome this limitation, we propose the Regularized Langevin Dynamics (RLD),
which enforces an expected distance between the sampled and current solutions,
effectively avoiding local minima. We develop two CO solvers on top of RLD, one
based on simulated annealing (SA), and the other one based on neural network
(NN). Empirical results on three classic CO problems demonstrate that both of
our methods can achieve comparable or better performance against the previous
state-of-the-art (SOTA) SA- and NN-based solvers. In particular, our SA
algorithm reduces the runtime of the previous SOTA SA method by up to 80\%,
while achieving equal or superior performance. In summary, RLD offers a
promising framework for enhancing both traditional heuristics and NN models to
solve CO problems. Our code is available at
https://github.com/Shengyu-Feng/RLD4CO.

</details>


### [518] [Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework](https://arxiv.org/pdf/2502.00846)
*Terje Mildner, Oliver Hamelijnck, Paris Giampouras, Theodoros Damoulas*

Main category: cs.LG

TL;DR: FedGVI is a robust probabilistic Federated Learning framework that handles model misspecification, offering unbiased predictions and calibrated uncertainty. It generalizes prior FL methods, reduces client-side computation, and shows theoretical and empirical improvements.


<details>
  <summary>Details</summary>
Motivation: Address limitations in frequentist and Bayesian FL by ensuring robustness to prior and likelihood misspecification, providing unbiased predictions and calibrated uncertainty.

Method: Generalizes Partitioned Variational Inference, enabling robust conjugate updates and reducing computational complexity at clients. Theoretical analysis includes fixed-point convergence and robustness proofs.

Result: Theoretical guarantees (convergence, optimality, robustness) and empirical improvements in robustness and predictive performance on synthetic and real-world datasets.

Conclusion: FedGVI effectively addresses FL limitations, offering robust, computationally efficient, and theoretically sound solutions with empirical validation.

Abstract: We introduce FedGVI, a probabilistic Federated Learning (FL) framework that
is robust to both prior and likelihood misspecification. FedGVI addresses
limitations in both frequentist and Bayesian FL by providing unbiased
predictions under model misspecification, with calibrated uncertainty
quantification. Our approach generalises previous FL approaches, specifically
Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and
conjugate updates, decreasing computational complexity at the clients. We offer
theoretical analysis in terms of fixed-point convergence, optimality of the
cavity distribution, and provable robustness to likelihood misspecification.
Further, we empirically demonstrate the effectiveness of FedGVI in terms of
improved robustness and predictive performance on multiple synthetic and real
world classification data sets.

</details>


### [519] [Evolutionary Policy Optimization](https://arxiv.org/pdf/2503.19037)
*Jianren Wang, Yifan Su, Abhinav Gupta, Deepak Pathak*

Main category: cs.LG

TL;DR: EPO combines Evolutionary Algorithms and policy gradients for scalable, diverse, and efficient reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: On-policy RL struggles with scalability due to redundant data, while EAs lack sample efficiency.

Method: EPO uses a population of agents with shared actor-critic networks and aggregates experiences into a master agent.

Result: EPO outperforms baselines in sample efficiency, performance, and scalability across diverse tasks.

Conclusion: EPO effectively merges the strengths of EAs and policy gradients for superior RL performance.

Abstract: On-policy reinforcement learning (RL) algorithms are widely used for their
strong asymptotic performance and training stability, but they struggle to
scale with larger batch sizes, as additional parallel environments yield
redundant data due to limited policy-induced diversity. In contrast,
Evolutionary Algorithms (EAs) scale naturally and encourage exploration via
randomized population-based search, but are often sample-inefficient. We
propose Evolutionary Policy Optimization (EPO), a hybrid algorithm that
combines the scalability and diversity of EAs with the performance and
stability of policy gradients. EPO maintains a population of agents conditioned
on latent variables, shares actor-critic network parameters for coherence and
memory efficiency, and aggregates diverse experiences into a master agent.
Across tasks in dexterous manipulation, legged locomotion, and classic control,
EPO outperforms state-of-the-art baselines in sample efficiency, asymptotic
performance, and scalability.

</details>


### [520] [DIME:Diffusion-Based Maximum Entropy Reinforcement Learning](https://arxiv.org/pdf/2502.02316)
*Onur Celik, Zechu Li, Denis Blessing, Ge Li, Daniel Palenicek, Jan Peters, Georgia Chalvatzaki, Gerhard Neumann*

Main category: cs.LG

TL;DR: DIME integrates diffusion-based policies into MaxEnt-RL, overcoming entropy computation challenges with a lower-bound approximation and policy iteration, outperforming other methods in high-dimensional tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional MaxEnt-RL uses Gaussian policies, limiting expressiveness. Diffusion-based policies offer better representation but face entropy computation challenges.

Method: DIME leverages diffusion models for approximate inference, deriving a lower bound on entropy, and introduces a provably convergent policy iteration scheme.

Result: DIME outperforms other diffusion-based methods in high-dimensional control tasks and competes with non-diffusion RL methods, reducing computational complexity.

Conclusion: DIME successfully combines expressive diffusion policies with MaxEnt-RL benefits, offering improved performance and efficiency.

Abstract: Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard
approach to RL due to its beneficial exploration properties. Traditionally,
policies are parameterized using Gaussian distributions, which significantly
limits their representational capacity. Diffusion-based policies offer a more
expressive alternative, yet integrating them into MaxEnt-RL poses
challenges-primarily due to the intractability of computing their marginal
entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL
(DIME). \emph{DIME} leverages recent advances in approximate inference with
diffusion models to derive a lower bound on the maximum entropy objective.
Additionally, we propose a policy iteration scheme that provably converges to
the optimal diffusion policy. Our method enables the use of expressive
diffusion-based policies while retaining the principled exploration benefits of
MaxEnt-RL, significantly outperforming other diffusion-based methods on
challenging high-dimensional control benchmarks. It is also competitive with
state-of-the-art non-diffusion based RL methods while requiring fewer
algorithmic design choices and smaller update-to-data ratios, reducing
computational complexity.

</details>


### [521] [k-NN as a Simple and Effective Estimator of Transferability](https://arxiv.org/pdf/2503.18528)
*Moein Sorkhei, Christos Matsoukas, Johan Fredin Haslum, Emir Konuk, Kevin Smith*

Main category: cs.LG

TL;DR: A study evaluates 23 transferability metrics across 16 datasets, finding none universally effective, but identifies k-nearest neighbor as a superior alternative.


<details>
  <summary>Details</summary>
Motivation: To assess the accuracy of transfer learning metrics in realistic settings with domain shifts, task differences, and architectural changes.

Method: Conducted 42,000+ experiments comparing 23 metrics across 16 datasets.

Result: No existing metric performed well universally; k-nearest neighbor outperformed others in accuracy, efficiency, and ease of use.

Conclusion: k-nearest neighbor is a better transferability metric than existing alternatives.

Abstract: How well can one expect transfer learning to work in a new setting where the
domain is shifted, the task is different, and the architecture changes? Many
transfer learning metrics have been proposed to answer this question. But how
accurate are their predictions in a realistic new setting? We conducted an
extensive evaluation involving over 42,000 experiments comparing 23
transferability metrics across 16 different datasets to assess their ability to
predict transfer performance. Our findings reveal that none of the existing
metrics perform well across the board. However, we find that a simple k-nearest
neighbor evaluation -- as is commonly used to evaluate feature quality for
self-supervision -- not only surpasses existing metrics, but also offers better
computational efficiency and ease of implementation.

</details>


### [522] [Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?](https://arxiv.org/pdf/2504.06006)
*Roman Kochnev, Arash Torabi Goodarzi, Zofia Antonina Bentyn, Dmitry Ignatov, Radu Timofte*

Main category: cs.LG

TL;DR: The paper explores using fine-tuned LLMs (Code Llama with LoRA) for hyperparameter optimization, achieving competitive or better results than traditional methods like Optuna and TPE, with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter selection is crucial for neural network performance, especially as models grow in complexity. Traditional methods are computationally intensive, motivating the search for efficient alternatives.

Method: Fine-tune a parameter-efficient version of Code Llama using LoRA to generate hyperparameter recommendations for diverse neural architectures.

Result: The LLM-based method matches or outperforms state-of-the-art techniques (e.g., TPE) in RMSE while significantly reducing computational overhead.

Conclusion: LLMs are a promising alternative for hyperparameter optimization, offering efficiency, speed, and robustness, particularly in resource-constrained environments. The LEMUR NN Dataset is released as a benchmark.

Abstract: Optimal hyperparameter selection is critical for maximizing neural network
performance, especially as models grow in complexity. This work investigates
the viability of leveraging large language models (LLMs) for hyperparameter
optimization by fine-tuning a parameter-efficient version of Code Llama using
LoRA. The adapted LLM is capable of generating accurate and efficient
hyperparameter recommendations tailored to diverse neural network
architectures. Unlike traditional approaches such as Optuna, which rely on
computationally intensive trial-and-error procedures, our method achieves
competitive or superior results in terms of Root Mean Square Error (RMSE) while
significantly reducing computational overhead. Our findings demonstrate that
LLM-based optimization not only matches the performance of state-of-the-art
techniques like Tree-structured Parzen Estimators (TPE) but also substantially
accelerates the tuning process. This positions LLMs as a promising alternative
for rapid experimentation, particularly in resource-constrained environments
such as edge devices and mobile platforms, where computational efficiency is
essential. In addition to improved efficiency, the method offers time savings
and consistent performance across various tasks, highlighting its robustness
and generalizability. All generated hyperparameters are included in the LEMUR
Neural Network (NN) Dataset, which is publicly available and serves as an
open-source benchmark for hyperparameter optimization research.

</details>


### [523] [Optimal Spectral Transitions in High-Dimensional Multi-Index Models](https://arxiv.org/pdf/2502.02545)
*Leonardo Defilippis, Yatin Dandi, Pierre Mergny, Florent Krzakala, Bruno Loureiro*

Main category: cs.LG

TL;DR: The paper investigates the sample complexity for weakly reconstructing the relevant index subspace in Gaussian multi-index models, proposing spectral algorithms that achieve the optimal threshold.


<details>
  <summary>Details</summary>
Motivation: Understanding the computational limits of neural networks, particularly in multi-index settings, remains challenging. This work aims to bridge gaps in weak learnability.

Method: Spectral algorithms based on linearizing a message passing scheme are introduced and analyzed.

Result: The proposed methods achieve the optimal reconstruction threshold, with the leading eigenvector correlating with the relevant subspace above this threshold.

Conclusion: The work provides theoretical and empirical support for the computational limits of weak learnability in multi-index models, linking it to phenomena in random matrix theory.

Abstract: We consider the problem of how many samples from a Gaussian multi-index model
are required to weakly reconstruct the relevant index subspace. Despite its
increasing popularity as a testbed for investigating the computational
complexity of neural networks, results beyond the single-index setting remain
elusive. In this work, we introduce spectral algorithms based on the
linearization of a message passing scheme tailored to this problem. Our main
contribution is to show that the proposed methods achieve the optimal
reconstruction threshold. Leveraging a high-dimensional characterization of the
algorithms, we show that above the critical threshold the leading eigenvector
correlates with the relevant index subspace, a phenomenon reminiscent of the
Baik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix
theory. Supported by numerical experiments and a rigorous theoretical
framework, our work bridges critical gaps in the computational limits of weak
learnability in multi-index model.

</details>


### [524] [PatchTrAD: A Patch-Based Transformer focusing on Patch-Wise Reconstruction Error for Time Series Anomaly Detection](https://arxiv.org/pdf/2504.08827)
*Samy-Melwan Vilhes, Gilles Gasso, Mokhtar Z Alaya*

Main category: cs.LG

TL;DR: PatchTrAD is a Transformer-based model for time series anomaly detection, using patches and reconstruction, achieving competitive performance with efficient inference.


<details>
  <summary>Details</summary>
Motivation: The rise of connected devices necessitates real-time anomaly detection in streaming data for early irregularity detection.

Method: PatchTrAD employs a Transformer encoder and patches within a reconstruction-based framework for anomaly detection.

Result: The model performs comparably to state-of-the-art deep learning models in detection while being time-efficient.

Conclusion: PatchTrAD is effective and efficient for time series anomaly detection, suitable for real-world applications.

Abstract: Time series anomaly detection (TSAD) focuses on identifying whether
observations in streaming data deviate significantly from normal patterns. With
the prevalence of connected devices, anomaly detection on time series has
become paramount, as it enables real-time monitoring and early detection of
irregular behaviors across various application domains. In this work, we
introduce PatchTrAD, a Patch-based Transformer model for time series anomaly
detection. Our approach leverages a Transformer encoder along with the use of
patches under a reconstructionbased framework for anomaly detection. Empirical
evaluations on multiple benchmark datasets show that PatchTrAD is on par, in
terms of detection performance, with state-of-the-art deep learning models for
anomaly detection while being time efficient during inference.

</details>


### [525] [Curvature Tuning: Provable Training-free Model Steering From a Single Parameter](https://arxiv.org/pdf/2502.07783)
*Leyang Hu, Matteo Gamba, Randall Balestriero*

Main category: cs.LG

TL;DR: The paper introduces Curvature Tuning (CT), a method to improve finetuning by adjusting activation functions with a hyperparameter, enhancing interpretability, generalization, and robustness.


<details>
  <summary>Details</summary>
Motivation: Current finetuning methods lack interpretability and rely on heuristic hyperparameters. The paper aims to shift focus from weights to activation functions for better control and performance.

Method: Proposes CT, which modulates decision boundaries by injecting a hyperparameter into activation functions, projecting models onto smooth function spaces. The hyperparameter is made trainable for efficiency.

Result: CT improves ResNet-50/152 accuracy by 7.14%/8.46% over linear probing and 4.64%/1.70% over LoRA across 12 datasets, and boosts robust accuracy by 1032.64%/1494.46%.

Conclusion: CT offers an interpretable, efficient, and effective alternative to traditional finetuning, enhancing both generalization and robustness.

Abstract: The scaling of model and data sizes has reshaped the AI landscape,
establishing finetuning pretrained models as the standard paradigm for solving
downstream tasks. However, dominant finetuning methods typically rely on weight
adaptation, often lack interpretability, and depend on heuristically chosen
hyperparameters. In this paper, we take a different perspective and shift the
focus from weights to activation functions, viewing them through the lens of
spline operators. We propose Curvature Tuning (CT), an interpretable and
principled steering method that modulates a model's decision boundary by
injecting a single hyperparameter into its activation functions. We show that
CT provably adjusts model decision boundary curvature and, more fundamentally,
projects a model onto a space of smooth functions-thereby complementing current
finetuning methods, whose effect lies primarily in feature adaptation. Making
this hyperparameter trainable gives rise to a novel and highly
parameter-efficient finetuning method. Empirically, CT improves both
generalization and robustness. For example, it boosts downstream accuracy of
ResNet-50/152 by 7.14%/8.46% over linear probing and 4.64%/1.70% over LoRA
across 12 datasets, and improves robust accuracy on the $\ell_\infty$ benchmark
from RobustBench by 1032.64%/1494.46%. Our code is available at
https://github.com/Leon-Leyang/curvature-tuning.

</details>


### [526] [On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion](https://arxiv.org/pdf/2504.08970)
*Nasim Shirvani-Mahdavi, Farahnaz Akrami, Chengkai Li*

Main category: cs.LG

TL;DR: The paper critiques current KGE evaluation methods for their unrealistic benchmarks and proposes a comprehensive evaluation of four KGE models on large-scale datasets, revealing significant flaws in existing protocols.


<details>
  <summary>Details</summary>
Motivation: Current KGE evaluation metrics and protocols are flawed, relying on unrealistic assumptions and small datasets, which misrepresent model performance and obscure strengths and weaknesses.

Method: The study evaluates four representative KGE models on large-scale datasets (FB-CVT-REV and FB+CVT-REV) to analyze performance variations and limitations of current evaluation methods.

Result: Findings show performance disparities between small and large datasets, overestimation of model capabilities when n-ary relations are simplified, and fundamental flaws in evaluation metrics.

Conclusion: The paper highlights the need for improved evaluation protocols and metrics in KGE research to better reflect real-world scenarios and model capabilities.

Abstract: Knowledge graph embedding (KGE) models are extensively studied for knowledge
graph completion, yet their evaluation remains constrained by unrealistic
benchmarks. Standard evaluation metrics rely on the closed-world assumption,
which penalizes models for correctly predicting missing triples, contradicting
the fundamental goals of link prediction. These metrics often compress accuracy
assessment into a single value, obscuring models' specific strengths and
weaknesses. The prevailing evaluation protocol, link prediction, operates under
the unrealistic assumption that an entity's properties, for which values are to
be predicted, are known in advance. While alternative protocols such as
property prediction, entity-pair ranking, and triple classification address
some of these limitations, they remain underutilized. Moreover, commonly used
datasets are either faulty or too small to reflect real-world data. Few studies
examine the role of mediator nodes, which are essential for modeling n-ary
relationships, or investigate model performance variation across domains. This
paper conducts a comprehensive evaluation of four representative KGE models on
large-scale datasets FB-CVT-REV and FB+CVT-REV. Our analysis reveals critical
insights, including substantial performance variations between small and large
datasets, both in relative rankings and absolute metrics, systematic
overestimation of model capabilities when n-ary relations are binarized, and
fundamental limitations in current evaluation protocols and metrics.

</details>


### [527] [Understanding High-Dimensional Bayesian Optimization](https://arxiv.org/pdf/2502.09198)
*Leonard Papenmeier, Matthias Poloczek, Luigi Nardi*

Main category: cs.LG

TL;DR: Simple Bayesian optimization (BO) works well in high dimensions due to local search behaviors and proper GP initialization, with MLE-based methods like MSR achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To understand why simple BO methods succeed in high-dimensional tasks despite prior beliefs and identify key challenges in high-dimensional BO.

Method: Analyze GP initialization's role in HDBO failures, propose MLE-based MSR for better performance, and validate findings with experiments.

Result: Vanishing gradients from GP initialization hinder HDBO; local search and MLE-based methods like MSR excel.

Conclusion: Proper initialization and local search behaviors are crucial for HDBO, with MSR outperforming existing methods.

Abstract: Recent work reported that simple Bayesian optimization (BO) methods perform
well for high-dimensional real-world tasks, seemingly contradicting prior work
and tribal knowledge. This paper investigates why. We identify underlying
challenges that arise in high-dimensional BO and explain why recent methods
succeed. Our empirical analysis shows that vanishing gradients caused by
Gaussian process (GP) initialization schemes play a major role in the failures
of high-dimensional Bayesian optimization (HDBO) and that methods that promote
local search behaviors are better suited for the task. We find that maximum
likelihood estimation (MLE) of GP length scales suffices for state-of-the-art
performance. Based on this, we propose a simple variant of MLE called MSR that
leverages these findings to achieve state-of-the-art performance on a
comprehensive set of real-world applications. We present targeted experiments
to illustrate and confirm our findings.

</details>


### [528] [Activated LoRA: Fine-tuned LLMs for Intrinsics](https://arxiv.org/pdf/2504.12397)
*Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox*

Main category: cs.LG

TL;DR: aLoRA improves LoRA by allowing instant activation without recomputing KV cache, enabling efficient multiturn customization.


<details>
  <summary>Details</summary>
Motivation: Switching between LoRAs in multiturn settings is inefficient due to KV cache recomputation.

Method: aLoRA adapts weights only for tokens after invocation, accepting the base model's KV cache.

Result: Competitive accuracy with standard LoRA and significant inference benefits.

Conclusion: aLoRA enables efficient, specialized model invocation in multiturn settings.

Abstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for
finetuning the weights of large foundation models, and has become the go-to
method for data-driven customization of LLMs. Despite the promise of highly
customized behaviors and capabilities, switching between relevant LoRAs in a
multiturn setting is inefficient, as the key-value (KV) cache of the entire
turn history must be recomputed with the LoRA weights before generation can
begin. To address this problem, we propose Activated LoRA (aLoRA), an adapter
architecture which modifies the LoRA framework to only adapt weights for the
tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially
allows aLoRA to accept the base model's KV cache of the input string, meaning
that aLoRA can be instantly activated whenever needed in a chain without
recomputing the cache. This enables building what we call \emph{intrinsics},
i.e. specialized models invoked to perform well-defined operations on portions
of an input chain or conversation that otherwise uses the base model by
default. We train a set of aLoRA-based intrinsics models, demonstrating
competitive accuracy with standard LoRA while achieving significant inference
benefits. The codebase is at https://github.com/IBM/activated-lora.

</details>


### [529] [Training-Free Guidance Beyond Differentiability: Scalable Path Steering with Tree Search in Diffusion and Flow Models](https://arxiv.org/pdf/2502.11420)
*Yingqing Guo, Yukang Yang, Hui Yuan, Mengdi Wang*

Main category: cs.LG

TL;DR: TreeG is a training-free guidance method for diffusion and flow models, handling non-differentiable objectives and discrete data. It uses tree search and parallel exploration, outperforming baselines in symbolic music, molecule, and DNA design.


<details>
  <summary>Details</summary>
Motivation: Address challenges of non-differentiable objectives and discrete data in training-free guidance for controlled generation.

Method: Proposes TreeG, a tree search-based path steering guidance framework, with candidate proposal, evaluation, and selection, enhanced by parallel exploration.

Result: TreeG outperforms baselines by 29.01%, 16.6%, and 18.43% in symbolic music, molecule, and DNA design, with scalable inference-time computation.

Conclusion: TreeG provides a unified, effective framework for training-free guidance in diverse applications, demonstrating superior performance and scalability.

Abstract: Training-free guidance enables controlled generation in diffusion and flow
models, but most methods rely on gradients and assume differentiable
objectives. This work focuses on training-free guidance addressing challenges
from non-differentiable objectives and discrete data distributions. We propose
TreeG: Tree Search-Based Path Steering Guidance, applicable to both continuous
and discrete settings in diffusion and flow models. TreeG offers a unified
framework for training-free guidance by proposing, evaluating, and selecting
candidates at each step, enhanced with tree search over active paths and
parallel exploration. We comprehensively investigate the design space of TreeG
over the candidate proposal module and the evaluation function, instantiating
TreeG into three novel algorithms. Our experiments show that TreeG consistently
outperforms top guidance baselines in symbolic music generation, small molecule
design, and enhancer DNA design with improvements of 29.01%, 16.6%, and 18.43%.
Additionally, we identify an inference-time scaling law showing TreeG's
scalability in inference-time computation.

</details>


### [530] [Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs](https://arxiv.org/pdf/2502.11672)
*Andrey Kofnov, Daniel Kapla, Ezio Bartocci, Efstathia Bura*

Main category: cs.LG

TL;DR: The paper derives exact bounds for the cdf of a neural network's output under noisy inputs, applicable to various NN architectures, and outperforms competing methods by providing guaranteed error bounds.


<details>
  <summary>Details</summary>
Motivation: To provide exact upper and lower bounds for the cdf of NN outputs under stochastic inputs, addressing limitations of existing approaches.

Method: Bounds general NNs using ReLU NNs, then derives cdf bounds for the NN output. Applies to feedforward and convolutional NNs with specific activation functions.

Result: The method delivers guaranteed bounds for predictive output distributions, offering exact error guarantees.

Conclusion: The approach outperforms competing methods by providing precise bounds for NN output distributions under noisy inputs.

Abstract: We derive exact upper and lower bounds for the cumulative distribution
function (cdf) of the output of a neural network (NN) over its entire support
subject to noisy (stochastic) inputs. The upper and lower bounds converge to
the true cdf over its domain as the resolution increases. Our method applies to
any feedforward NN using continuous monotonic piecewise twice continuously
differentiable activation functions (e.g., ReLU, tanh and softmax) and
convolutional NNs, which were beyond the scope of competing approaches. The
novelty and instrumental tool of our approach is to bound general NNs with ReLU
NNs. The ReLU NN-based bounds are then used to derive the upper and lower
bounds of the cdf of the NN output. Experiments demonstrate that our method
delivers guaranteed bounds of the predictive output distribution over its
support, thus providing exact error guarantees, in contrast to competing
approaches.

</details>


### [531] [Implicit Neural Representations for Chemical Reaction Paths](https://arxiv.org/pdf/2502.15843)
*Kalyan Ramakrishnan, Lars L. Schaaf, Chen Lin, Guangrun Wang, Philip Torr*

Main category: cs.LG

TL;DR: Neural networks optimize minimum energy paths, outperforming NEB in flexibility and accuracy for complex systems.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and efficient alternative to discrete path-search methods like NEB for representing reaction paths.

Method: Parameterizes reaction paths using a neural network trained on a loss function that ignores tangential energy gradients, enabling instant transition state estimation.

Result: Validated on 2D potentials and outperformed NEB in challenging atomistic systems, handling poor initial guesses, multiple paths, and complex mechanisms.

Conclusion: The method is versatile, generalizes to unseen systems, and shows promise for universal reaction path representation.

Abstract: We show that neural networks can be optimized to represent minimum energy
paths as continuous functions, offering a flexible alternative to discrete
path-search methods like Nudged Elastic Band (NEB). Our approach parameterizes
reaction paths with a network trained on a loss function that discards
tangential energy gradients and enables instant estimation of the transition
state. We first validate the method on two-dimensional potentials and then
demonstrate its advantages over NEB on challenging atomistic systems where (i)
poor initial guesses yield unphysical paths, (ii) multiple competing paths
exist, or (iii) the reaction follows a complex multi-step mechanism. Results
highlight the versatility of the method: for instance, a simple adjustment to
the sampling strategy during optimization can help escape local-minimum
solutions. Finally, in a low-dimensional setting, we demonstrate that a single
neural network can learn from existing paths and generalize to unseen systems,
showing promise for a universal reaction path representation.

</details>


### [532] [Scalable Meta-Learning via Mixed-Mode Differentiation](https://arxiv.org/pdf/2505.00793)
*Iurii Kemaev, Dan A Calian, Luisa M Zintgraf, Gregory Farquhar, Hado van Hasselt*

Main category: cs.LG

TL;DR: MixFlow-MG improves efficiency in gradient-based bilevel optimization by using mixed-mode differentiation, achieving significant memory and time savings.


<details>
  <summary>Details</summary>
Motivation: Gradient-based bilevel optimization is widely used but computationally expensive due to second-order and mixed derivatives. Existing methods often fail to exploit problem structure, leading to inefficiency.

Method: Proposes Mixed-Flow Meta-Gradients (MixFlow-MG), a mixed-mode differentiation algorithm to optimize computational graphs.

Result: Achieves over 10x memory savings and up to 25% faster wall-clock time compared to standard implementations.

Conclusion: MixFlow-MG offers a scalable and efficient solution for gradient-based bilevel optimization, enhancing performance in meta-learning and related tasks.

Abstract: Gradient-based bilevel optimisation is a powerful technique with applications
in hyperparameter optimisation, task adaptation, algorithm discovery,
meta-learning more broadly, and beyond. It often requires differentiating
through the gradient-based optimisation itself, leading to
"gradient-of-a-gradient" calculations with computationally expensive
second-order and mixed derivatives. While modern automatic differentiation
libraries provide a convenient way to write programs for calculating these
derivatives, they oftentimes cannot fully exploit the specific structure of
these problems out-of-the-box, leading to suboptimal performance. In this
paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or
MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to
construct more efficient and scalable computational graphs yielding over 10x
memory and up to 25% wall-clock time improvements over standard implementations
in modern meta-learning setups.

</details>


### [533] [Scalable Graph Attention-based Instance Selection via Mini-Batch Sampling and Hierarchical Hashing](https://arxiv.org/pdf/2502.20293)
*Zahiriddin Rustamov, Ayham Zaitouny, Nazar Zaki*

Main category: cs.LG

TL;DR: GAIS introduces a graph attention-based method for instance selection, using scalable graph construction techniques to handle large datasets efficiently. It achieves high reduction rates while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: As datasets grow, current instance selection methods struggle with scalability and capturing complex relationships in high-dimensional spaces.

Method: GAIS employs attention mechanisms to identify informative instances via graph representations. It uses distance-based mini-batch sampling and hierarchical hashing for scalable graph construction.

Result: Experiments on 39 datasets show GAIS achieves >96% reduction rates while maintaining or improving model performance.

Conclusion: GAIS demonstrates that attention-based scoring effectively identifies important instances, with mini-batch methods excelling in scalability and multi-view variants in handling complex data.

Abstract: Instance selection (IS) addresses the critical challenge of reducing dataset
size while keeping informative characteristics, becoming increasingly important
as datasets grow to millions of instances. Current IS methods often struggle
with capturing complex relationships in high-dimensional spaces and scale with
large datasets. This paper introduces a graph attention-based instance
selection (GAIS) method that uses attention mechanisms to identify informative
instances through their structural relationships in graph representations. We
present two approaches for scalable graph construction: a distance-based
mini-batch sampling technique that achieves dataset-size-independent complexity
through strategic batch processing, and a hierarchical hashing approach that
enables efficient similarity computation through random projections. The
mini-batch approach keeps class distributions through stratified sampling,
while the hierarchical hashing method captures relationships at multiple
granularities through single-level, multi-level, and multi-view variants.
Experiments across 39 datasets show that GAIS achieves reduction rates above
96\% while maintaining or improving model performance relative to
state-of-the-art IS methods. The findings show that the distance-based
mini-batch approach offers an optimal efficiency for large-scale datasets,
while multi-view variants excel on complex, high-dimensional data,
demonstrating that attention-based importance scoring can effectively identify
instances important for maintaining decision boundaries while avoiding
computationally prohibitive pairwise comparisons.

</details>


### [534] [Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth](https://arxiv.org/pdf/2505.03802)
*Changhai Zhou, Shijie Han, Shiyang Zhang, Yuhua Zhou, Weizhong Zhang, Cheng Jin*

Main category: cs.LG

TL;DR: QLoRA combines low-bit quantization and LoRA for memory-efficient LLM fine-tuning. QR-Adaptor jointly optimizes quantization and low-rank subspaces, improving performance without minimizing quantization error.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to consistently improve performance by separately optimizing low-rank subspaces or quantization components, ignoring their synergy.

Method: Proposes QR-Adaptor, a gradient-free strategy using partial calibration data to jointly search quantization components and low-rank subspaces.

Result: Achieves 4.89% accuracy improvement on GSM8K, sometimes outperforming 16-bit models while maintaining 4-bit memory usage.

Conclusion: QR-Adaptor effectively balances performance and memory efficiency, outperforming SOTA methods.

Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve
memory-friendly fine-tuning for large language models (LLM). Recently, methods
based on SVD for continuous update iterations to initialize LoRA matrices to
accommodate quantization errors have generally failed to consistently improve
performance. Dynamic mixed precision is a natural idea for continuously
improving the fine-tuning performance of quantized models, but previous methods
often optimize low-rank subspaces or quantization components separately,
without considering their synergy. To address this, we propose
\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial
calibration data to jointly search the quantization components and the rank of
low-rank spaces for each layer, thereby continuously improving model
performance. QR-Adaptor does not minimize quantization error but treats
precision and rank allocation as a discrete optimization problem guided by
actual downstream performance and memory usage. Compared to state-of-the-art
(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\%
accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit
fine-tuned model while maintaining the memory footprint of the 4-bit setting.

</details>


### [535] [Learning surrogate equations for the analysis of an agent-based cancer model](https://arxiv.org/pdf/2503.01718)
*Kevin Burrage, Pamela M. Burrage, Justin N. Kreikemeyer, Adelinde M. Uhrmacher, Hasitha N. Weerasinghe*

Main category: cs.LG

TL;DR: The paper extends a two-species cancer model to include immune cells, explores six scenarios, and derives a unified surrogate model for easier analysis.


<details>
  <summary>Details</summary>
Motivation: To simplify the analysis of cancer-immune cell interactions and reduce computational costs of agent-based models.

Method: Adapts an agent-based model to include immune cells, runs six scenarios, and uses coupled equation learning to create a population-based reaction model.

Result: Derives a unified surrogate model with three ODEs, enabling easier analysis and prediction of cancer dynamics.

Conclusion: The surrogate model provides a computationally efficient way to estimate cancer reduction without complex simulations.

Abstract: In this paper, we adapt a two-species agent-based cancer model that describes
the interaction between cancer cells and healthy cells on a uniform grid to
include the interaction with a third species -- namely immune cells. We run six
different scenarios to explore the competition between cancer and immune cells
and the initial concentration of the immune cells on cancer dynamics. We then
use coupled equation learning to construct a population-based reaction model
for each scenario. We show how they can be unified into a single surrogate
population-based reaction model, whose underlying three coupled ordinary
differential equations are much easier to analyse than the original agent-based
model. As an example, by finding the single steady state of the cancer
concentration, we are able to find a linear relationship between this
concentration and the initial concentration of the immune cells. This then
enables us to estimate suitable values for the competition and initial
concentration to reduce the cancer substantially without performing additional
complex and expensive simulations from an agent-based stochastic model.

</details>


### [536] [DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization](https://arxiv.org/pdf/2505.12366)
*Gang Li, Ming Lin, Tomer Galanti, Zhengzhong Tu, Tianbao Yang*

Main category: cs.LG

TL;DR: DisCO, a new Discriminative Constrained Optimization framework, outperforms GRPO and its variants by eliminating difficulty bias and improving stability in training large reasoning models.


<details>
  <summary>Details</summary>
Motivation: The limitations of GRPO, such as question-level difficulty bias and entropy instability, motivated the development of DisCO.

Method: DisCO replaces GRPO's group relative objective with a discriminative objective, uses non-clipping RL surrogate objectives, and employs constrained optimization for KL divergence.

Result: DisCO achieves average gains of 7% over GRPO and 6% over DAPO on six benchmark tasks for a 1.5B model.

Conclusion: DisCO offers a more stable and effective approach for reinforcing large reasoning models, addressing key limitations of GRPO.

Abstract: The recent success and openness of DeepSeek-R1 have brought widespread
attention to Group Relative Policy Optimization (GRPO) as a reinforcement
learning method for large reasoning models (LRMs). In this work, we analyze the
GRPO objective under a binary reward setting and reveal an inherent limitation
of question-level difficulty bias. We also identify a connection between GRPO
and traditional discriminative methods in supervised learning. Motivated by
these insights, we introduce a new Discriminative Constrained Optimization
(DisCO) framework for reinforcing LRMs, grounded in the principle of
discriminative learning. The main differences between DisCO and GRPO and its
recent variants are: (1) it replaces the group relative objective with a
discriminative objective defined by a scoring function; (2) it abandons
clipping-based surrogates in favor of non-clipping RL surrogate objectives used
as scoring functions; (3) it employs a simple yet effective constrained
optimization approach to enforce the KL divergence constraint, ensuring stable
training. As a result, DisCO offers notable advantages over GRPO and its
variants: (i) it completely eliminates difficulty bias by adopting
discriminative objectives; (ii) it addresses the entropy instability in GRPO
and its variants through the use of non-clipping scoring functions and a
constrained optimization approach; (iii) it allows the incorporation of
advanced discriminative learning techniques to address data imbalance, where a
significant number of questions have more negative than positive generated
answers during training. Our experiments on enhancing the mathematical
reasoning capabilities of SFT-finetuned models show that DisCO significantly
outperforms GRPO and its improved variants such as DAPO, achieving average
gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for an 1.5B
model.

</details>


### [537] [Nearly Optimal Differentially Private ReLU Regression](https://arxiv.org/pdf/2503.06009)
*Meng Ding, Mingxi Lei, Shaowei Wang, Tianhang Zheng, Di Wang, Jinhui Xu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we investigate one of the most fundamental nonconvex learning
problems, ReLU regression, in the Differential Privacy (DP) model. Previous
studies on private ReLU regression heavily rely on stringent assumptions, such
as constant bounded norms for feature vectors and labels. We relax these
assumptions to a more standard setting, where data can be i.i.d. sampled from
$O(1)$-sub-Gaussian distributions. We first show that when $\varepsilon =
\tilde{O}(\sqrt{\frac{1}{N}})$ and there is some public data, it is possible to
achieve an upper bound of $\tilde{O}(\frac{d^2}{N^2 \varepsilon^2})$ for the
excess population risk in $(\epsilon, \delta)$-DP, where $d$ is the dimension
and $N$ is the number of data samples. Moreover, we relax the requirement of
$\epsilon$ and public data by proposing and analyzing a one-pass mini-batch
Generalized Linear Model Perceptron algorithm (DP-MBGLMtron). Additionally,
using the tracing attack argument technique, we demonstrate that the minimax
rate of the estimation error for $(\varepsilon, \delta)$-DP algorithms is lower
bounded by $\Omega(\frac{d^2}{N^2 \varepsilon^2})$. This shows that
DP-MBGLMtron achieves the optimal utility bound up to logarithmic factors.
Experiments further support our theoretical results.

</details>


### [538] [Predicting and Understanding College Student Mental Health with Interpretable Machine Learning](https://arxiv.org/pdf/2503.08002)
*Meghna Roy Chowdhury, Wei Xuan, Shreyas Sen, Yixue Zhao, Yi Ding*

Main category: cs.LG

TL;DR: I-HOPE is an interpretable hierarchical model for personalized mental health prediction in college students, achieving 91% accuracy and outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like the need for large-scale longitudinal data, lack of transparency in models, and aggregated insights by providing individualized understanding.

Method: A two-stage hierarchical model linking raw behavioral features to mental health status through five behavioral categories as interaction labels.

Result: Achieves 91% prediction accuracy on a five-year longitudinal dataset, surpassing baseline methods (60-70%).

Conclusion: I-HOPE offers interpretable, individualized insights for tailored mental health interventions, with potential to improve support.

Abstract: Mental health issues among college students have reached critical levels,
significantly impacting academic performance and overall wellbeing. Predicting
and understanding mental health status among college students is challenging
due to three main factors: the necessity for large-scale longitudinal datasets,
the prevalence of black-box machine learning models lacking transparency, and
the tendency of existing approaches to provide aggregated insights at the
population level rather than individualized understanding.
  To tackle these challenges, this paper presents I-HOPE, the first
Interpretable Hierarchical mOdel for Personalized mEntal health prediction.
I-HOPE is a two-stage hierarchical model that connects raw behavioral features
to mental health status through five defined behavioral categories as
interaction labels. We evaluate I-HOPE on the College Experience Study, the
longest longitudinal mobile sensing dataset. This dataset spans five years and
captures data from both pre-pandemic periods and the COVID-19 pandemic. I-HOPE
achieves a prediction accuracy of 91%, significantly surpassing the 60-70%
accuracy of baseline methods. In addition, I-HOPE distills complex patterns
into interpretable and individualized insights, enabling the future development
of tailored interventions and improving mental health support. The code is
available at https://github.com/roycmeghna/I-HOPE.

</details>


### [539] [Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting](https://arxiv.org/pdf/2505.24511)
*Jiahao Wang, Mingyue Cheng, Qi Liu*

Main category: cs.LG

TL;DR: The paper explores using slow-thinking LLMs for zero-shot time series forecasting, proposing TimeReasoner to evaluate their reasoning capabilities and finding promising results.


<details>
  <summary>Details</summary>
Motivation: Existing time series forecasting methods lack explicit reasoning over temporal dynamics, while slow-thinking LLMs show potential for structured reasoning tasks.

Method: Proposes TimeReasoner, a study formulating TSF as a conditional reasoning task, using prompting strategies to evaluate pretrained LLMs.

Result: Slow-thinking LLMs demonstrate non-trivial zero-shot forecasting capabilities, particularly in capturing trends and contextual shifts.

Conclusion: The study highlights the potential and limitations of LLMs in TSF, encouraging further research into reasoning-based forecasting for interpretability and generalizability.

Abstract: Time series forecasting (TSF) is a fundamental and widely studied task,
spanning methods from classical statistical approaches to modern deep learning
and multimodal language modeling. Despite their effectiveness, these methods
often follow a fast thinking paradigm emphasizing pattern extraction and direct
value mapping, while overlooking explicit reasoning over temporal dynamics and
contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g.,
ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning
capabilities across diverse domains, suggesting a new opportunity for reframing
TSF as a structured reasoning task. This motivates a key question: can
slow-thinking LLMs effectively reason over temporal patterns to support time
series forecasting, even in zero-shot manner? To investigate this, in this
paper, we propose TimeReasoner, an extensive empirical study that formulates
TSF as a conditional reasoning task. We design a series of prompting strategies
to elicit inference-time reasoning from pretrained slow-thinking LLMs and
evaluate their performance across diverse TSF benchmarks. Our findings reveal
that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities,
especially in capturing high-level trends and contextual shifts. While
preliminary, our study surfaces important insights into the reasoning behaviors
of LLMs in temporal domains highlighting both their potential and limitations.
We hope this work catalyzes further research into reasoning-based forecasting
paradigms and paves the way toward more interpretable and generalizable TSF
frameworks.

</details>


### [540] [ASIDE: Architectural Separation of Instructions and Data in Language Models](https://arxiv.org/pdf/2503.10566)
*Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Alexandra Volkova, Soroush Tabesh, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert*

Main category: cs.LG

TL;DR: ASIDE introduces an orthogonal rotation to embeddings to separate instructions and data, improving safety and robustness against prompt injection attacks without losing model utility.


<details>
  <summary>Details</summary>
Motivation: Large language models lack intrinsic safety features, making them vulnerable to malicious attacks like prompt injection due to unclear separation between instructions and data.

Method: ASIDE applies an orthogonal rotation to data token embeddings, creating distinct representations for instructions and data without adding parameters.

Result: Instruction-tuning with ASIDE enhances instruction-data separation, improves robustness to prompt injection, and maintains model utility.

Conclusion: ASIDE effectively addresses prompt injection vulnerabilities by separating instructions and data at the embedding level, offering a lightweight and efficient solution.

Abstract: Despite their remarkable performance, large language models lack elementary
safety features, making them susceptible to numerous malicious attacks. In
particular, previous work has identified the absence of an intrinsic separation
between instructions and data as a root cause of the success of prompt
injection attacks. In this work, we propose a new architectural element, ASIDE,
that allows language models to clearly separate instructions and data at the
level of embeddings. ASIDE applies an orthogonal rotation to the embeddings of
data tokens, thus creating clearly distinct representations of instructions and
data tokens without introducing any additional parameters. As we demonstrate
experimentally across a range of models, instruction-tuning LLMs with ASIDE (1)
leads to highly increased instruction-data separation without a loss in model
utility and (2) makes the models more robust to prompt injection benchmarks,
even without dedicated safety training. Additionally, we provide insights into
the mechanism underlying our method through an analysis of the model
representations. The source code and training scripts are openly accessible at
https://github.com/egozverev/aside.

</details>


### [541] [A Theory of Machine Understanding via the Minimum Description Length Principle](https://arxiv.org/pdf/2504.00395)
*Canlin Zhang, Xiuwen Liu*

Main category: cs.LG

TL;DR: The paper introduces Spectrum VAE, a deep learning architecture that connects explainable representations with the Minimum Description Length (MDL) principle, showing that understanding emerges from compression.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of end-to-end learning in forming explainable representations and rigorously compressing information, proposing a theory that links these issues.

Method: Design Spectrum VAE, evaluate its MDL, and introduce latent dimension combinations (spiking patterns) to achieve MDL.

Result: Demonstrates that achieving MDL leads to explainable latent representations, suggesting 'to understand is to compress.'

Conclusion: Advocates shifting deep network training objectives to include MDL minimization for self-supervised, explainable AI.

Abstract: Deep neural networks trained through end-to-end learning have achieved
remarkable success across various domains in the past decade. However, the
end-to-end learning strategy, originally designed to minimize predictive loss
in a black-box manner, faces two fundamental limitations: the struggle to form
explainable representations in a self-supervised manner, and the inability to
compress information rigorously following the Minimum Description Length (MDL)
principle. These two limitations point to a deeper issue: an end-to-end
learning model is not able to "understand" what it learns. In this paper, we
establish a novel theory connecting these two limitations. We design the
Spectrum VAE, a novel deep learning architecture whose minimum description
length (MDL) can be rigorously evaluated. Then, we introduce the concept of
latent dimension combinations, or what we term spiking patterns, and
demonstrate that the observed spiking patterns should be as few as possible
based on the training data in order for the Spectrum VAE to achieve the MDL.
Finally, our theory demonstrates that when the MDL is achieved with respect to
the given data distribution, the Spectrum VAE will naturally produce
explainable latent representations of the data. In other words, explainable
representations--or "understanding"--can emerge in a self-supervised manner
simply by making the deep network obey the MDL principle. In our opinion, this
also implies a deeper insight: To understand is to compress. At its core, our
theory advocates for a shift in the training objective of deep networks: not
only to minimize predictive loss, but also to minimize the description length
regarding the given data. That is, a deep network should not only learn, but
also understand what it learns. This work is entirely theoretical and aims to
inspire future research toward self-supervised, explainable AI grounded in the
MDL principle.

</details>


### [542] [Why Gradients Rapidly Increase Near the End of Training](https://arxiv.org/pdf/2506.02285)
*Aaron Defazio*

Main category: cs.LG

TL;DR: The paper identifies and fixes an issue in LLM training where gradient norms spike late in training due to weight decay, normalization layers, and learning rate schedules.


<details>
  <summary>Details</summary>
Motivation: To address the unintended interaction causing gradient norm spikes in late-stage LLM training.

Method: Proposes a simple correction to mitigate the interaction between weight decay, normalization layers, and learning rate schedules.

Result: The correction eliminates the gradient norm spike and reduces loss values during training.

Conclusion: The proposed fix effectively resolves the issue and improves training stability and performance.

Abstract: During long-duration Large Language Model (LLM) training runs the gradient
norm increases rapidly near the end of training. In this short note, we show
that this increase is due to an unintended interaction between weight decay,
normalization layers, and the learning rate schedule. We propose a simple
correction that fixes this behavior while also resulting in lower loss values
throughout training.

</details>


### [543] [Provably Accurate Adaptive Sampling for Collocation Points in Physics-informed Neural Networks](https://arxiv.org/pdf/2504.00910)
*Antoine Caradot, Rémi Emonet, Amaury Habrard, Abdel-Rahim Mezidi, Marc Sebban*

Main category: cs.LG

TL;DR: A new Hessian-based sampling method for collocation points in PINNs improves accuracy and efficiency in solving PDEs.


<details>
  <summary>Details</summary>
Motivation: Efficiently solving PDEs is complex and costly; adaptive sampling refinements for PINNs aim to address this.

Method: Introduces a provably accurate sampling method for collocation points using a new quadrature method and Hessian of PDE residuals.

Result: Comparative experiments on 1D and 2D PDEs show the method's benefits.

Conclusion: The proposed Hessian-based sampling method enhances PINN performance for PDE solutions.

Abstract: Despite considerable scientific advances in numerical simulation, efficiently
solving PDEs remains a complex and often expensive problem. Physics-informed
Neural Networks (PINN) have emerged as an efficient way to learn surrogate
solvers by embedding the PDE in the loss function and minimizing its residuals
using automatic differentiation at so-called collocation points. Originally
uniformly sampled, the choice of the latter has been the subject of recent
advances leading to adaptive sampling refinements for PINNs. In this paper,
leveraging a new quadrature method for approximating definite integrals, we
introduce a provably accurate sampling method for collocation points based on
the Hessian of the PDE residuals. Comparative experiments conducted on a set of
1D and 2D PDEs demonstrate the benefits of our method.

</details>


### [544] [Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks](https://arxiv.org/pdf/2506.05434)
*Thomas Massena, Léo andéol, Thibaut Boissin, Franck Mamalet, Corentin Friedrich, Mathieu Serrurier, Sébastien Gerchinovitz*

Main category: cs.LG

TL;DR: The paper introduces lip-rcp, a method using Lipschitz-bounded networks to efficiently compute robust conformal prediction sets, outperforming existing methods in set size and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Classical conformal prediction lacks robustness under adversarial attacks, and existing robust methods are impractical for large-scale problems.

Method: Leverages Lipschitz-bounded networks to estimate robust CP sets, combining with 1-Lipschitz networks for efficiency and precision.

Result: lip-rcp outperforms state-of-the-art methods in robust CP set size and computational efficiency, especially in large-scale scenarios like ImageNet.

Conclusion: The method bridges efficiency and robustness, offering practical deployment of robust CP sets with guarantees.

Abstract: Conformal Prediction (CP) has proven to be an effective post-hoc method for
improving the trustworthiness of neural networks by providing prediction sets
with finite-sample guarantees. However, under adversarial attacks, classical
conformal guarantees do not hold anymore: this problem is addressed in the
field of Robust Conformal Prediction. Several methods have been proposed to
provide robust CP sets with guarantees under adversarial perturbations, but,
for large scale problems, these sets are either too large or the methods are
too computationally demanding to be deployed in real life scenarios. In this
work, we propose a new method that leverages Lipschitz-bounded networks to
precisely and efficiently estimate robust CP sets. When combined with a
1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms
state-of-the-art results in both the size of the robust CP sets and
computational efficiency in medium and large-scale scenarios such as ImageNet.
Taking a different angle, we also study vanilla CP under attack, and derive new
worst-case coverage bounds of vanilla CP sets, which are valid simultaneously
for all adversarial attack levels. Our lip-rcp method makes this second
approach as efficient as vanilla CP while also allowing robustness guarantees.

</details>


### [545] [Low-Loss Space in Neural Networks is Continuous and Fully Connected](https://arxiv.org/pdf/2505.02604)
*Yongding Tian, Zaid Al-Ars, Maksim Kitsak, Peter Hofstee*

Main category: cs.LG

TL;DR: The paper explores the connectivity of low-loss regions in neural network parameter spaces, proposing an algorithm to find continuous paths between minima.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of low-loss regions in neural networks and challenge the notion of isolated minima.

Method: A new algorithm is developed to investigate low-loss paths in the full parameter space, tested on LeNet5, ResNet18, and Compact Convolutional Transformer architectures.

Result: Experiments confirm the existence of continuous low-loss paths, suggesting the low-loss region is fully connected and continuous.

Conclusion: The findings reveal parameter redundancy within models and offer insights into over-parameterization, with potential for improved generalization and new visualization methods.

Abstract: Visualizations of the loss landscape in neural networks suggest that minima
are isolated points. However, both theoretical and empirical studies indicate
that it is possible to connect two different minima with a path consisting of
intermediate points that also have low loss. In this study, we propose a new
algorithm which investigates low-loss paths in the full parameter space, not
only between two minima. Our experiments on LeNet5, ResNet18, and Compact
Convolutional Transformer architectures consistently demonstrate the existence
of such continuous paths in the parameter space. These results suggest that the
low-loss region is a fully connected and continuous space in the parameter
space. Our findings provide theoretical insight into neural network
over-parameterization, highlighting that parameters collectively define a
high-dimensional low-loss space, implying parameter redundancy exists only
within individual models and not throughout the entire low-loss space.
Additionally, our work also provides new visualization methods and
opportunities to improve model generalization by exploring the low-loss space
that is closer to the origin.

</details>


### [546] [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/pdf/2506.06292)
*Tianyuan Shi, Canbin Huang, Fanqi Wan, Longguang Zhong, Ziyi Yang, Weizhou Shen, Xiaojun Quan, Ming Yan*

Main category: cs.LG

TL;DR: Mutual-Taught is a self-training method for LLMs that iteratively improves policy and reward models without human annotation, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Distribution shifts between model samples and reward model training data reduce RM efficacy, impacting policy model performance.

Method: Uses an EM-like approach: E-step updates PM using RM feedback; M-step updates RM with PM outputs.

Result: Consistent improvements in both models; LLaMA-3-8B-Instruct-MT achieves 54.1% win rate, FsfairX-LLaMA3-RM-MT matches GPT-4o performance.

Conclusion: Mutual-Taught effectively addresses distribution shifts, enhancing model performance without extra human input.

Abstract: During the preference optimization of large language models (LLMs),
distribution shifts may arise between newly generated model samples and the
data used to train the reward model (RM). This shift reduces the efficacy of
the RM, which in turn negatively impacts the performance of the policy model
(PM). To address this challenge, we propose Mutual-Taught, a self-training
method that iteratively improves both the PM and RM without requiring
additional human annotation. Our approach mirrors the expectation-maximization
(EM) algorithm. In the E-step, the PM is updated using feedback from the
current RM, guiding the PM toward a better approximation of the latent optimal
preference distribution. In the M-step, we update the RM by constructing
training data from the outputs of the PM before and after the E-step update.
This process ensures that the RM adapts to the evolving policy distribution.
Experimental results demonstrate that this iterative approach leads to
consistent improvements in both models. Specifically, our 8B policy model,
LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on
AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par
with GPT-4o-2024-08-06 on RewardBench.

</details>


### [547] [Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry](https://arxiv.org/pdf/2505.05143)
*Mohammed Adnan, Rohan Jain, Ekansh Sharma, Rahul Krishnan, Yani Ioannou*

Main category: cs.LG

TL;DR: The paper addresses the generalization issue of Lottery Ticket Hypothesis (LTH) masks by proposing permutation alignment to improve sparse training performance.


<details>
  <summary>Details</summary>
Motivation: LTH masks don't generalize to new random initializations due to misaligned optimization basins.

Method: Permute LTH masks to align with new optimization basins during sparse training from different random initializations.

Result: Empirical results show improved generalization on datasets (CIFAR-10, CIFAR-100, ImageNet) and models (VGG11, ResNet20, ResNet50).

Conclusion: Permutation alignment of LTH masks enhances sparse training performance across diverse datasets and models.

Abstract: The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask
and weights that achieve the same generalization performance as the dense model
while using significantly fewer parameters. However, finding a LTH solution is
computationally expensive, and a LTH sparsity mask does not generalize to other
random weight initializations. Recent work has suggested that neural networks
trained from random initialization find solutions within the same basin modulo
permutation, and proposes a method to align trained models within the same loss
basin. We hypothesize that misalignment of basins is the reason why LTH masks
do not generalize to new random initializations and propose permuting the LTH
mask to align with the new optimization basin when performing sparse training
from a different random init. We empirically show a significant increase in
generalization when sparse training from random initialization with the
permuted mask as compared to using the non-permuted LTH mask, on multiple
datasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and
ResNet50).

</details>


### [548] [A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices](https://arxiv.org/pdf/2505.16563)
*Chen Gong, Rui Xing, Zhenzhe Zheng, Fan Wu*

Main category: cs.LG

TL;DR: Titan is a two-stage data selection framework for efficient on-device ML training, improving throughput and accuracy with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Addressing under-utilization of on-device data due to low throughput, storage limits, and varying data importance.

Method: A two-stage framework: coarse-grained filtering followed by fine-grained, theoretically optimal data selection, with pipeline co-execution to avoid resource conflicts.

Result: Up to 43% faster training and 6.2% higher accuracy with minor system overhead.

Conclusion: Titan effectively enhances on-device training efficiency and performance.

Abstract: The demand for machine learning (ML) model training on edge devices is
escalating due to data privacy and personalized service needs. However, we
observe that current on-device model training is hampered by the
under-utilization of on-device data, due to low training throughput, limited
storage and diverse data importance. To improve data resource utilization, we
propose a two-stage data selection framework {\sf Titan} to select the most
important data batch from streaming data for model training with guaranteed
efficiency and effectiveness. Specifically, in the first stage, {\sf Titan}
filters out a candidate dataset with potentially high importance in a
coarse-grained manner.In the second stage of fine-grained selection, we propose
a theoretically optimal data selection strategy to identify the data batch with
the highest model performance improvement to current training round. To further
enhance time-and-resource efficiency, {\sf Titan} leverages a pipeline to
co-execute data selection and model training, and avoids resource conflicts by
exploiting idle computing resources. We evaluate {\sf Titan} on real-world edge
devices and three representative edge computing tasks with diverse models and
data modalities. Empirical results demonstrate that {\sf Titan} achieves up to
$43\%$ reduction in training time and $6.2\%$ increase in final accuracy with
minor system overhead, such as data processing delay, memory footprint and
energy consumption.

</details>


### [549] [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/pdf/2506.07976)
*Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, Aviral Kumar*

Main category: cs.LG

TL;DR: The paper proposes test-time interaction scaling (TTI) to enhance agent adaptability and performance in interactive tasks, achieving state-of-the-art results on web benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current test-time scaling methods lack adaptability and real-time interaction, limiting agent performance in dynamic environments.

Method: Introduces TTI, a curriculum-based online RL approach, using adaptive rollout lengths and a Gemma 3 12B model.

Result: TTI improves task success on web benchmarks and enables adaptive exploration-exploitation balance.

Conclusion: Interaction scaling is a powerful complement to per-step compute scaling, opening new avenues for adaptive agent training.

Abstract: The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.

</details>


### [550] [Optimizing Shortfall Risk Metric for Learning Regression Models](https://arxiv.org/pdf/2505.17777)
*Harish G. Ramaswamy, L. A. Prashanth*

Main category: cs.LG

TL;DR: The paper addresses estimating and optimizing utility-based shortfall risk (UBSR) in regression, proposing a concentration bound for UBSR estimation and a bisection-type algorithm for optimization.


<details>
  <summary>Details</summary>
Motivation: UBSR is a non-linear function of the underlying distribution, making empirical risk minimization challenging. The work aims to provide practical methods for UBSR estimation and optimization.

Method: Derives a concentration bound for UBSR estimation using i.i.d. samples, frames UBSR optimization as minimizing a pseudo-linear function, and constructs gradient and linear minimization oracles to devise a bisection-type algorithm.

Result: Convergence to the UBSR-optimal solution is established using the proposed algorithm.

Conclusion: The paper provides theoretical and algorithmic tools for UBSR estimation and optimization in regression problems.

Abstract: We consider the problem of estimating and optimizing utility-based shortfall
risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression
problem. Empirical risk minimization with a UBSR objective is challenging since
UBSR is a non-linear function of the underlying distribution. We first derive a
concentration bound for UBSR estimation using independent and identically
distributed (i.i.d.) samples. We then frame the UBSR optimization problem as
minimization of a pseudo-linear function in the space of achievable
distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient
oracle for the UBSR objective and a linear minimization oracle (LMO) for the
set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm,
and establish convergence to the UBSR-optimal solution.

</details>


### [551] [Federated Learning: From Theory to Practice](https://arxiv.org/pdf/2505.19183)
*A. Jung*

Main category: cs.LG

TL;DR: The book introduces federated learning (FL) systems, focusing on personalization by training models collaboratively while keeping data private. It frames the problem as a distributed optimization task using generalized total variation minimization (GTVMin).


<details>
  <summary>Details</summary>
Motivation: FL addresses privacy, regulatory, and technical challenges by enabling decentralized ML training without centralizing data. Personalization ensures devices benefit from collaboration while maintaining individual model training.

Method: The approach represents FL systems as networks of devices, using GTVMin for distributed optimization to ensure similar tasks yield similar model parameters.

Result: The method provides a scalable, privacy-preserving solution for FL, balancing collaboration and personalization.

Conclusion: The book offers a practical, accessible guide to designing FL systems, combining mathematical rigor with intuitive explanations for students, engineers, and researchers.

Abstract: This book offers a hands-on introduction to building and understanding
federated learning (FL) systems. FL enables multiple devices -- such as
smartphones, sensors, or local computers -- to collaboratively train machine
learning (ML) models, while keeping their data private and local. It is a
powerful solution when data cannot or should not be centralized due to privacy,
regulatory, or technical reasons. The book is designed for students, engineers,
and researchers who want to learn how to design scalable, privacy preserving FL
systems. Our main focus is on personalization: enabling each device to train
its own model while still benefiting from collaboration with relevant devices.
This is achieved by leveraging similarities between (the learning tasks
associated with) devices that are encoded by the weighted edges (or links) of a
federated learning network (FL network). The key idea is to represent
real-world FL systems as networks of devices, where nodes correspond to device
and edges represent communication links and data similarities between them. The
training of personalized models for these devices can be naturally framed as a
distributed optimization problem. This optimization problem is referred to as
generalized total variation minimization (GTVMin) and ensures that devices with
similar learning tasks learn similar model parameters. Our approach is both
mathematically principled and practically motivated. While we introduce some
advanced ideas from optimization theory and graph-based learning, we aim to
keep the book accessible. Readers are guided through the core ideas step by
step, with intuitive explanations.

</details>


### [552] [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/pdf/2505.22785)
*Marco Fumero, Luca Moschella, Emanuele Rodolà, Francesco Locatello*

Main category: cs.LG

TL;DR: The paper interprets neural networks as dynamical systems on latent manifolds, revealing implicit vector fields and attractor points. This provides tools for analyzing model properties, generalization, memorization, and out-of-distribution detection.


<details>
  <summary>Details</summary>
Motivation: To offer a new perspective on neural models by treating them as dynamical systems, uncovering latent vector fields and their properties without additional training.

Method: Analyzes autoencoder models as dynamical systems, deriving latent vector fields from encoding-decoding maps and identifying attractor points.

Result: Demonstrates that vector fields can analyze model behavior, extract prior knowledge, and detect out-of-distribution samples. Validated on vision foundation models.

Conclusion: The dynamical systems view provides a powerful tool for understanding and analyzing neural networks, with practical applications in model evaluation and data analysis.

Abstract: Neural networks transform high-dimensional data into compact, structured
representations, often modeled as elements of a lower dimensional latent space.
In this paper, we present an alternative interpretation of neural models as
dynamical systems acting on the latent manifold. Specifically, we show that
autoencoder models implicitly define a latent vector field on the manifold,
derived by iteratively applying the encoding-decoding map, without any
additional training. We observe that standard training procedures introduce
inductive biases that lead to the emergence of attractor points within this
vector field. Drawing on this insight, we propose to leverage the vector field
as a representation for the network, providing a novel tool to analyze the
properties of the model and the data. This representation enables to: (i)
analyze the generalization and memorization regimes of neural models, even
throughout training; (ii) extract prior knowledge encoded in the network's
parameters from the attractors, without requiring any input data; (iii)
identify out-of-distribution samples from their trajectories in the vector
field. We further validate our approach on vision foundation models, showcasing
the applicability and effectiveness of our method in real-world scenarios.

</details>


### [553] [Equivalence of stochastic and deterministic policy gradients](https://arxiv.org/pdf/2505.23244)
*Emo Todorov*

Main category: cs.LG

TL;DR: The paper explores the relationship between stochastic and deterministic policy gradients in continuous control, showing their equivalence in certain MDPs and proposing a method to unify them.


<details>
  <summary>Details</summary>
Motivation: To understand the connection between stochastic and deterministic policy gradients and unify policy gradient methods.

Method: Analyzes MDPs with Gaussian control noise and quadratic costs, constructs equivalent MDPs for deterministic policies, and compares gradients and value functions.

Result: Stochastic and deterministic policy gradients, natural gradients, and state value functions are identical in the studied MDPs, while state-control value functions differ.

Conclusion: Policy gradient methods can be unified by approximating state value functions instead of state-control value functions.

Abstract: Policy gradients in continuous control have been derived for both stochastic
and deterministic policies. Here we study the relationship between the two. In
a widely-used family of MDPs involving Gaussian control noise and quadratic
control costs, we show that the stochastic and deterministic policy gradients,
natural gradients, and state value functions are identical; while the
state-control value functions are different. We then develop a general
procedure for constructing an MDP with deterministic policy that is equivalent
to a given MDP with stochastic policy. The controls of this new MDP are the
sufficient statistics of the stochastic policy in the original MDP. Our results
suggest that policy gradient methods can be unified by approximating state
value functions rather than state-control value functions.

</details>


### [554] [Stepsize anything: A unified learning rate schedule for budgeted-iteration training](https://arxiv.org/pdf/2505.24452)
*Anda Tang, Yiming Dong, Yutao Zeng, zhou Xun, Zhouchen Lin*

Main category: cs.LG

TL;DR: The paper introduces the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that outperforms heuristic-based schedules under constrained training budgets.


<details>
  <summary>Details</summary>
Motivation: The need for efficient budgeted-iteration training due to rising computational costs and limited resources, coupled with the lack of theoretical foundations for learning rate schedules, motivates this work.

Method: The authors propose the UBA schedule, derived from a budget-aware optimization framework, controlled by a hyper-parameter φ, which balances flexibility and simplicity. Theoretical connections to the condition number and convergence proofs are provided.

Result: UBA consistently outperforms common schedules across diverse architectures (e.g., ResNet, OLMo) and tasks under varying training budgets.

Conclusion: The UBA schedule offers a practical, theoretically justified solution for budgeted-iteration training, eliminating the need for extensive trial-and-error in learning rate selection.

Abstract: The expanding computational costs and limited resources underscore the
critical need for budgeted-iteration training, which aims to achieve optimal
learning within predetermined iteration budgets. While learning rate schedules
fundamentally govern the performance of different networks and tasks,
particularly in budgeted-iteration scenarios, their design remains largely
heuristic, lacking theoretical foundations. In addition, the optimal learning
rate schedule requires extensive trial-and-error selection, making the training
process inefficient. In this work, we propose the Unified Budget-Aware (UBA)
schedule, a theoretically grounded learning rate schedule that consistently
outperforms commonly-used schedules among diverse architectures and tasks under
different constrained training budgets. First, we bridge the gap by
constructing a novel training budget-aware optimization framework, which
explicitly accounts for the robustness to landscape curvature variations. From
this framework, we derive the UBA schedule, controlled by a single
hyper-parameter \varphi that provides a trade-off between flexibility and
simplicity, eliminating the need for per-network numerical optimization.
Moreover, we establish a theoretical connection between \varphi and the
condition number, adding interpretation and justification to our approach.
Besides, we prove the convergence for different values of \varphi. We offer
practical guidelines for its selection via theoretical analysis and empirical
results. Extensive experimental results show that UBA consistently surpasses
the commonly-used schedules across diverse vision and language tasks, spanning
network architectures (e.g., ResNet, OLMo) and scales, under different
training-iteration budgets.

</details>


### [555] [CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries](https://arxiv.org/pdf/2506.00388)
*Ni Mu, Hao Hu, Xiao Hu, Yiqin Yang, Bo Xu, Qing-Shan Jia*

Main category: cs.LG

TL;DR: CLARIFY improves PbRL by using contrastive learning to resolve ambiguous human feedback, enhancing query selection and embedding quality.


<details>
  <summary>Details</summary>
Motivation: Humans struggle with labeling preferences for similar segments in PbRL, reducing efficiency and applicability.

Method: Proposes CLARIFY, an offline PbRL method using contrastive learning to create a trajectory embedding space that clarifies ambiguous feedback.

Result: CLARIFY outperforms baselines in non-ideal and real human feedback settings, selecting clearer queries and learning meaningful embeddings.

Conclusion: CLARIFY enhances PbRL by addressing ambiguous feedback, improving both query selection and embedding quality.

Abstract: Preference-based reinforcement learning (PbRL) bypasses explicit reward
engineering by inferring reward functions from human preference comparisons,
enabling better alignment with human intentions. However, humans often struggle
to label a clear preference between similar segments, reducing label efficiency
and limiting PbRL's real-world applicability. To address this, we propose an
offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback
(CLARIFY), which learns a trajectory embedding space that incorporates
preference information, ensuring clearly distinguished segments are spaced
apart, thus facilitating the selection of more unambiguous queries. Extensive
experiments demonstrate that CLARIFY outperforms baselines in both non-ideal
teachers and real human feedback settings. Our approach not only selects more
distinguished queries but also learns meaningful trajectory embeddings.

</details>


### [556] [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/pdf/2506.02300)
*Farzaneh Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse, David J. Fleet*

Main category: cs.LG

TL;DR: A novel framework visualizes class transitions in neural networks by amplifying gradients in a steerable pyramid domain, revealing decision boundaries.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods fail to show how models distinguish classes or transition inputs between categories.

Method: Decomposes images using invertible transforms, computes class-conditional gradients, and amplifies them for linear extrapolation.

Result: Produces semantically meaningful morphs, highlighting sensitive directions and decision boundaries.

Conclusion: The framework offers interpretable insights into neural classifiers' internal representations.

Abstract: Understanding the internal representations and decision mechanisms of deep
neural networks remains a critical open challenge. While existing
interpretability methods often identify influential input regions, they may not
elucidate how a model distinguishes between classes or what specific changes
would transition an input from one category to another. To address these
limitations, we propose a novel framework that visualizes the implicit path
between classes by treating the network gradient as a form of infinitesimal
motion. Drawing inspiration from phase-based motion magnification, we first
decompose images using invertible transforms-specifically the Complex Steerable
Pyramid-then compute class-conditional gradients in the transformed space.
Rather than iteratively integrating the gradient to trace a full path, we
amplify the one-step gradient to the input and perform a linear extrapolation
to expose how the model moves from source to target class. By operating in the
steerable pyramid domain, these amplified gradients produce semantically
meaningful, spatially coherent morphs that highlight the classifier's most
sensitive directions, giving insight into the geometry of its decision
boundaries. Experiments on both synthetic and real-world datasets demonstrate
that our phase-focused extrapolation yields perceptually aligned, semantically
meaningful transformations, offering a novel, interpretable lens into neural
classifiers' internal representations.

</details>


### [557] [Selective Matching Losses -- Not All Scores Are Created Equal](https://arxiv.org/pdf/2506.04446)
*Gil I. Shamir, Manfred K. Warmuth*

Main category: cs.LG

TL;DR: The paper introduces selective matching loss functions to prioritize accurate predictions in specific regions of a domain, using link functions to emphasize high-sensitivity areas. It extends these losses to multi-class scenarios and demonstrates their advantages in various applications.


<details>
  <summary>Details</summary>
Motivation: The need to improve prediction accuracy in critical regions of a domain while deprioritizing less important areas drives the development of selective loss functions.

Method: The method involves designing selective matching loss functions using link functions (e.g., Sigmoid, hyperbolic sine) to emphasize high-sensitivity regions. For multi-class, composite Softmax functions are used to address ranking sensitivity.

Result: Selective losses outperform traditional losses in applications like dwell-time prediction, retrieval, ranking, and LLM fine-tuning, by focusing on important score regions.

Conclusion: Selective loss functions effectively address underspecification and improve prediction accuracy in critical regions, with broad applicability in machine learning tasks.

Abstract: Learning systems match predicted scores to observations over some domain.
Often, it is critical to produce accurate predictions in some subset (or
region) of the domain, yet less important to accurately predict in other
regions. We construct selective matching loss functions by design of increasing
link functions over score domains. A matching loss is an integral over the
link. A link defines loss sensitivity as function of the score, emphasizing
high slope high sensitivity regions over flat ones. Loss asymmetry drives a
model and resolves its underspecification to predict better in high sensitivity
regions where it is more important, and to distinguish between high and low
importance regions. A large variety of selective scalar losses can be designed
with scaled and shifted Sigmoid and hyperbolic sine links. Their properties,
however, do not extend to multi-class. Applying them per dimension lacks
ranking sensitivity that assigns importance according to class score ranking.
Utilizing composite Softmax functions, we develop a framework for
multidimensional selective losses. We overcome limitations of the standard
Softmax function, that is good for classification, but not for distinction
between adjacent scores. Selective losses have substantial advantage over
traditional losses in applications with more important score regions, including
dwell-time prediction, retrieval, ranking with either pointwise, contrastive
pairwise, or listwise losses, distillation problems, and fine-tuning alignment
of Large Language Models (LLMs).

</details>


### [558] [Gumbel-max List Sampling for Distribution Coupling with Multiple Samples](https://arxiv.org/pdf/2506.05632)
*Joseph Rowan, Buu Phan, Ashish Khisti*

Main category: cs.LG

TL;DR: The paper introduces a novel method for coupling probability distributions using Gumbel-max sampling, with applications in multi-draft speculative sampling and distributed lossy compression.


<details>
  <summary>Details</summary>
Motivation: To address the problem of coupling probability distributions and extend its applications to practical tasks like language processing and compression.

Method: Proposes a generalization of Gumbel-max sampling for coupling distributions, introduces the list matching lemma for acceptance probability, and applies it to multi-draft speculative sampling and distributed lossy compression.

Result: Achieves competitive performance in language tasks and significant gains in compression experiments.

Conclusion: The method is effective for coupling distributions and has practical utility in speculative sampling and compression.

Abstract: We study a relaxation of the problem of coupling probability distributions --
a list of samples is generated from one distribution and an accept is declared
if any one of these samples is identical to the sample generated from the other
distribution. We propose a novel method for generating samples, which extends
the Gumbel-max sampling suggested in Daliri et al. (arXiv:2408.07978) for
coupling probability distributions. We also establish a corresponding lower
bound on the acceptance probability, which we call the list matching lemma. We
next discuss two applications of our setup. First, we develop a new mechanism
for multi-draft speculative sampling that is simple to implement and achieves
performance competitive with baselines such as SpecTr and SpecInfer across a
range of language tasks. Our method also guarantees a certain degree of drafter
invariance with respect to the output tokens which is not supported by existing
schemes. We also provide a theoretical lower bound on the token level
acceptance probability. As our second application, we consider distributed
lossy compression with side information in a setting where a source sample is
compressed and available to multiple decoders, each with independent side
information. We propose a compression technique that is based on our
generalization of Gumbel-max sampling and show that it provides significant
gains in experiments involving synthetic Gaussian sources and the MNIST image
dataset.

</details>


### [559] [Exploring Microstructural Dynamics in Cryptocurrency Limit Order Books: Better Inputs Matter More Than Stacking Another Hidden Layer](https://arxiv.org/pdf/2506.05764)
*Haochuan Wang*

Main category: cs.LG

TL;DR: Simpler models with proper preprocessing can outperform complex neural networks in cryptocurrency price forecasting, offering speed and interpretability.


<details>
  <summary>Details</summary>
Motivation: To determine if adding complexity to neural networks genuinely improves short-term price forecasting or if gains come from preprocessing and feature engineering.

Method: Benchmarked models from logistic regression to deep architectures (DeepLOB, Conv1D+LSTM) on BTC/USDT LOB data, using data filtering pipelines (Kalman, Savitzky Golay) and binary/ternary labeling schemes.

Result: Simpler models matched or exceeded complex networks' performance with preprocessing and tuning, providing faster inference and better interpretability.

Conclusion: Data preprocessing and feature engineering are key; simpler models can be as effective as complex ones for short-term price forecasting.

Abstract: Cryptocurrency price dynamics are driven largely by microstructural supply
demand imbalances in the limit order book (LOB), yet the highly noisy nature of
LOB data complicates the signal extraction process. Prior research has
demonstrated that deep-learning architectures can yield promising predictive
performance on pre-processed equity and futures LOB data, but they often treat
model complexity as an unqualified virtue. In this paper, we aim to examine
whether adding extra hidden layers or parameters to "blackbox ish" neural
networks genuinely enhances short term price forecasting, or if gains are
primarily attributable to data preprocessing and feature engineering. We
benchmark a spectrum of models from interpretable baselines, logistic
regression, XGBoost to deep architectures (DeepLOB, Conv1D+LSTM) on BTC/USDT
LOB snapshots sampled at 100 ms to multi second intervals using publicly
available Bybit data. We introduce two data filtering pipelines (Kalman,
Savitzky Golay) and evaluate both binary (up/down) and ternary (up/flat/down)
labeling schemes. Our analysis compares models on out of sample accuracy,
latency, and robustness to noise. Results reveal that, with data preprocessing
and hyperparameter tuning, simpler models can match and even exceed the
performance of more complex networks, offering faster inference and greater
interpretability.

</details>


### [560] [Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization](https://arxiv.org/pdf/2506.05957)
*Tianjun Yao, Haoxuan Li, Yongqiang Chen, Tongliang Liu, Le Song, Eric Xing, Zhiqiang Shen*

Main category: cs.LG

TL;DR: PrunE is a pruning-based method to improve Graph Neural Networks' OOD generalization by eliminating spurious edges, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: GNNs suffer performance drops under distribution shifts; existing methods struggle to identify invariant subgraphs due to spurious edge correlations.

Method: PrunE uses two regularization terms: graph size constraint and ε-probability alignment, to prune spurious edges and retain invariant subgraphs.

Result: PrunE achieves superior OOD performance, significantly outperforming state-of-the-art methods.

Conclusion: PrunE effectively enhances GNNs' OOD generalization by pruning spurious edges, validated by theory and experiments.

Abstract: Graph Neural Networks (GNNs) often encounter significant performance
degradation under distribution shifts between training and test data, hindering
their applicability in real-world scenarios. Recent studies have proposed
various methods to address the out-of-distribution generalization challenge,
with many methods in the graph domain focusing on directly identifying an
invariant subgraph that is predictive of the target label. However, we argue
that identifying the edges from the invariant subgraph directly is challenging
and error-prone, especially when some spurious edges exhibit strong
correlations with the targets. In this paper, we propose PrunE, the first
pruning-based graph OOD method that eliminates spurious edges to improve OOD
generalizability. By pruning spurious edges, PrunE retains the invariant
subgraph more comprehensively, which is critical for OOD generalization.
Specifically, PrunE employs two regularization terms to prune spurious edges:
1) graph size constraint to exclude uninformative spurious edges, and 2)
$\epsilon$-probability alignment to further suppress the occurrence of spurious
edges. Through theoretical analysis and extensive experiments, we show that
PrunE achieves superior OOD performance and outperforms previous
state-of-the-art methods significantly. Codes are available at:
\href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.

</details>


### [561] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/pdf/2506.06300)
*Yuanye Zhou, Zhaokun Wang, Kai Zhou, Hui Tang, Xiaofan Li*

Main category: cs.LG

TL;DR: LT-PINNs improve topology optimization by eliminating manual interpolation and enabling precise boundary determination, outperforming conventional PINNs in accuracy and versatility.


<details>
  <summary>Details</summary>
Motivation: Conventional PINNs rely on density-based topology descriptions, requiring manual interpolation and limiting applicability to complex geometries.

Method: LT-PINNs parameterize topology boundary curves as learnable parameters and introduce specialized boundary and topology loss functions for sharp representations.

Result: LT-PINNs reduce relative L2 errors, handle arbitrary boundary conditions, and infer clear topology boundaries without manual interpolation.

Conclusion: LT-PINNs offer a robust, boundary-focused framework for engineering optimization, suitable for a wide range of PDEs and complex topologies.

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [562] [A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks](https://arxiv.org/pdf/2506.06715)
*Minh-Duc Nguyen, Dung D. Le*

Main category: cs.LG

TL;DR: The paper proposes SVH-MOL, a method using SVGD to diversify Pareto solutions while maximizing hypervolume in multi-objective learning.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to balance diversity and hypervolume maximization in Pareto solutions.

Method: Uses SVGD to approximate the Pareto set, employs diverse gradient directions, and incorporates an annealing schedule for stability.

Result: SVH-MOL shows superior performance in multi-objective problems and multi-task learning.

Conclusion: SVH-MOL effectively addresses the challenge of diversifying Pareto solutions while optimizing hypervolume.

Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining
the complete optimal solution in Multi-objective Learning (MOL). A set of
optimal solutions approximates the Pareto set, and its mapping is a set of
dense points in the Pareto front in objective space. However, some current
methods face a challenge: how to make the Pareto solution is diverse while
maximizing the hypervolume value. In this paper, we propose a novel method to
address this challenge, which employs Stein Variational Gradient Descent (SVGD)
to approximate the entire Pareto set. SVGD pushes a set of particles towards
the Pareto set by applying a form of functional gradient descent, which helps
to converge and diversify optimal solutions. Additionally, we employ diverse
gradient direction strategies to thoroughly investigate a unified framework for
SVGD in multi-objective optimization and adapt this framework with an annealing
schedule to promote stability. We introduce our method, SVH-MOL, and validate
its effectiveness through extensive experiments on multi-objective problems and
multi-task learning, demonstrating its superior performance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [563] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/pdf/2506.08507)
*Kuo Yang, Xingjie Yang, Linhui Yu, Qing Xu, Yan Fang, Xu Wang, Zhengyang Zhou, Yang Wang*

Main category: cs.MA

TL;DR: MasHost is an RL-based framework for autonomous Multi-agent system (Mas) design, introducing component rationality and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing Mas methods rely on manual or heuristic rules, limiting autonomy and introducing biases.

Method: Formulates Mas construction as a graph search problem, using probabilistic sampling and HRPO for multi-objective optimization.

Result: Outperforms baselines in experiments across six benchmarks.

Conclusion: MasHost is effective, efficient, and introduces novel design principles for autonomous Mas.

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


### [564] [Incentive-based Platoon Formation: Optimizing the Personal Benefit for Drivers](https://arxiv.org/pdf/2411.00570)
*Julian Heinovski, Doğanalp Ergenç, Kirsten Thommes, Falko Dressler*

Main category: cs.MA

TL;DR: A novel platoon formation algorithm optimizes personal benefits for passenger car drivers by balancing fuel savings and travel time, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The impact of platooning on passenger cars is unclear due to trip and driver heterogeneity. The paper aims to optimize personal benefits for individual drivers.

Method: Proposes a new metric combining fuel and travel time costs into a single monetary value, and a platoon formation algorithm prioritizing driver benefits.

Result: The algorithm outperforms adaptive cruise control and similarity-based platooning, balancing fuel savings and travel time effectively.

Conclusion: The proposed method provides an intuitive way for drivers to evaluate and benefit from platooning, addressing previous limitations.

Abstract: Platooning or cooperative adaptive cruise control (CACC) has been
investigated for decades, but debate about its lasting impact is still ongoing.
While the benefits of platooning and the formation of platoons are well
understood for trucks, they are less clear for passenger cars, which have a
higher heterogeneity in trips and drivers' preferences. Most importantly, it
remains unclear how to form platoons of passenger cars in order to optimize the
personal benefit for the individual driver. To this end, in this paper, we
propose a novel platoon formation algorithm that optimizes the personal benefit
for drivers of individual passenger cars. For computing vehicle-to-platoon
assignments, the algorithm utilizes a new metric that we propose to evaluate
the personal benefits of various driving systems, including platooning. By
combining fuel and travel time costs into a single monetary value, drivers can
estimate overall trip costs according to a personal monetary value for time
spent. This provides an intuitive way for drivers to understand and compare the
benefits of driving systems like human driving, adaptive cruise control (ACC),
and, of course, platooning. Unlike previous similarity-based methods, our
proposed algorithm forms platoons only when beneficial for the driver, rather
than solely for platooning. We demonstrate the new metric for the total trip
cost in a numerical analysis and explain its interpretation. Results of a
large-scale simulation study demonstrate that our proposed platoon formation
algorithm outperforms normal ACC as well as previous similarity-based
platooning approaches by balancing fuel savings and travel time, independent of
traffic and drivers' time cost.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)



<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [565] [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/pdf/2506.08633)
*Šimon Sedláček, Bolaji Yusuf, Ján Švec, Pradyoth Hegde, Santosh Kesiraju, Oldřich Plchot, Jan Černocký*

Main category: eess.AS

TL;DR: The paper introduces a method for spoken Dialogue State Tracking (DST) by connecting speech encoders and LLMs with a small module, achieving state-of-the-art results on SpokenWOZ.


<details>
  <summary>Details</summary>
Motivation: To improve spoken DST by leveraging open-sourced components (WavLM, OLMo) and exploring system optimizations like fine-tuning and post-processing.

Method: Uses a connector module between speech encoders (WavLM) and LLMs (OLMo, Gemma), with ablations on fine-tuning, dialogue history, and fuzzy matching post-processing.

Result: Achieves 34.66% JGA with WavLM + OLMo-1B and 42.17% JGA with Gemma-2-9B on SpokenWOZ.

Conclusion: The approach effectively bridges speech and language models, setting new benchmarks for spoken DST.

Abstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging
the representation spaces of speech encoders and LLMs via a small connector
module, with a focus on fully open-sourced and open-data components
(WavLM-large, OLMo). We focus on ablating different aspects of such systems
including full/LoRA adapter fine-tuning, the effect of agent turns in the
dialogue history, as well as fuzzy matching-based output post-processing, which
greatly improves performance of our systems on named entities in the dialogue
slot values. We conduct our experiments on the SpokenWOZ dataset, and
additionally utilize the Speech-Aware MultiWOZ dataset to augment our training
data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned
models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our
system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%
JGA on SpokenWOZ test.

</details>


### [566] [Phonology-Guided Speech-to-Speech Translation for African Languages](https://arxiv.org/pdf/2410.23323)
*Peter Ochieng, Dennis Kaburu*

Main category: eess.AS

TL;DR: A prosody-guided framework for speech-to-speech translation (S2ST) leverages pause synchrony and introduces SPaDA for alignment and SegUniDiff for translation, improving accuracy and reducing speaker error.


<details>
  <summary>Details</summary>
Motivation: To enable speech-to-speech translation without transcripts by leveraging cross-linguistic pause synchrony, addressing challenges in alignment and translation.

Method: Developed SPaDA, a dynamic-programming alignment algorithm integrating silence consistency, rate synchrony, and semantic similarity, and SegUniDiff, a diffusion-based S2ST model guided by external gradients.

Result: SPaDA improves alignment F1 by +3-4 points and reduces spurious matches by 38%. SegUniDiff matches BLEU scores of cascaded systems (30.3 vs. 28.9), reduces speaker error rate (12.5% to 5.3%), and runs efficiently (RTF 1.02).

Conclusion: Prosodic cues in multilingual speech provide a reliable foundation for scalable, non-autoregressive S2ST, with practical improvements in alignment and translation quality.

Abstract: We present a prosody-guided framework for speech-to-speech translation (S2ST)
that aligns and translates speech \emph{without} transcripts by leveraging
cross-linguistic pause synchrony. Analyzing a 6{,}000-hour East African news
corpus spanning five languages, we show that \emph{within-phylum} language
pairs exhibit 30--40\% lower pause variance and over 3$\times$ higher
onset/offset correlation compared to cross-phylum pairs. These findings
motivate \textbf{SPaDA}, a dynamic-programming alignment algorithm that
integrates silence consistency, rate synchrony, and semantic similarity. SPaDA
improves alignment $F_1$ by +3--4 points and eliminates up to 38\% of spurious
matches relative to greedy VAD baselines. Using SPaDA-aligned segments, we
train \textbf{SegUniDiff}, a diffusion-based S2ST model guided by
\emph{external gradients} from frozen semantic and speaker encoders. SegUniDiff
matches an enhanced cascade in BLEU (30.3 on CVSS-C vs.\ 28.9 for UnitY),
reduces speaker error rate (EER) from 12.5\% to 5.3\%, and runs at an RTF of
1.02. To support evaluation in low-resource settings, we also release a
three-tier, transcript-free BLEU suite (M1--M3) that correlates strongly with
human judgments. Together, our results show that prosodic cues in multilingual
speech provide a reliable scaffold for scalable, non-autoregressive S2ST.

</details>


### [567] [LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement](https://arxiv.org/pdf/2503.00493)
*Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie*

Main category: eess.AS

TL;DR: LLaSE-G1 is a LLaMA-based language model for speech enhancement, addressing acoustic inconsistency and improving generalization by using WavLM and X-Codec2, and outperforming prior models.


<details>
  <summary>Details</summary>
Motivation: Existing LM-based SE approaches neglect acoustic information, causing inconsistency and limited generalization.

Method: LLaSE-G1 uses continuous WavLM representations and X-Codec2 tokens, with dual-channel inputs/outputs for task unification.

Result: Outperforms prior discriminative and generative SE models, showing scaling effects and capabilities for unseen tasks.

Conclusion: LLaSE-G1 advances SE by balancing semantic and acoustic information, with released code/models for further research.

Abstract: Recent advancements in language models (LMs) have demonstrated strong
capabilities in semantic understanding and contextual modeling, which have
flourished in generative speech enhancement (SE). However, many LM-based SE
approaches primarily focus on semantic information, often neglecting the
critical role of acoustic information, which leads to acoustic inconsistency
after enhancement and limited generalization across diverse SE tasks. In this
paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes
generalization capabilities for speech enhancement. LLaSE-G1 offers the
following key contributions: First, to mitigate acoustic inconsistency,
LLaSE-G1 employs continuous representations from WavLM as input and predicts
speech tokens from X-Codec2, maximizing acoustic preservation. Second, to
promote generalization capability, LLaSE-G1 introduces dual-channel inputs and
outputs, unifying multiple SE tasks without requiring task-specific IDs. Third,
LLaSE-G1 outperforms prior task-specific discriminative and generative SE
models, demonstrating scaling effects at test time and emerging capabilities
for unseen SE tasks. Additionally, we release our code and models to support
further research in this area.

</details>


### [568] [The trajectoRIR Database: Room Acoustic Recordings Along a Trajectory of Moving Microphones](https://arxiv.org/pdf/2503.23004)
*Stefano Damiano, Kathleen MacWilliam, Valerio Lorenzoni, Thomas Dietzen, Toon van Waterschoot*

Main category: eess.AS

TL;DR: The paper introduces trajectoRIR, a unique database combining dynamic and stationary acoustic recordings for diverse signal processing tasks.


<details>
  <summary>Details</summary>
Motivation: Data-driven acoustic signal processing requires large, diverse datasets. Existing databases lack combined dynamic and stationary recordings, which trajectoRIR addresses.

Method: The database includes recordings from moving and stationary microphones along an L-shaped trajectory, using various configurations (dummy head, Ambisonics, circular/linear arrays). Motion was controlled via a robotic cart at three speeds.

Result: trajectoRIR offers 8648 stationary RIRs and dynamic recordings (sweeps, speech, music, noise), with MATLAB/Python scripts for access.

Conclusion: trajectoRIR is a versatile database for tasks like sound localization, tracking, and sound field reconstruction, filling a gap in available datasets.

Abstract: Data availability is essential to develop acoustic signal processing
algorithms, especially when it comes to data-driven approaches that demand
large and diverse training datasets. For this reason, an increasing number of
databases have been published in recent years, including either room impulse
responses (RIRs) or recordings of moving audio. In this paper we introduce the
trajectoRIR database, an extensive, multi-array collection of both dynamic and
stationary acoustic recordings along a controlled trajectory in a room.
Specifically, the database features recordings using moving microphones and
stationary RIRs spatially sampling the room acoustics along an L-shaped,
3.74-meter-long trajectory. This combination makes trajectoRIR unique and
applicable in various tasks ranging from sound source localization and tracking
to spatially dynamic sound field reconstruction and system identification. The
recording room has a reverberation time of 0.5 seconds, and the three different
microphone configurations employed include a dummy head, with additional
reference microphones located next to the ears, 3 first-order Ambisonics
microphones, two circular arrays of 16 and 4 channels, and a 12-channel linear
array. The motion of the microphones was achieved using a robotic cart
traversing a rail at three speeds: [0.2,0.4,0.8] m/s. Audio signals were
reproduced using two stationary loudspeakers. The collected database features
8648 stationary RIRs, as well as perfect sweeps, speech, music, and stationary
noise recorded during motion. MATLAB and Python scripts are included to access
the recorded audio as well as to retrieve geometrical information.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [569] [A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/2506.08183)
*Isha Puri, David Cox*

Main category: eess.IV

TL;DR: A new flexible, robust, and accurate model for rodent gaze tracking using a convolutional neural network, addressing unique challenges like small eye size and surrounding hair.


<details>
  <summary>Details</summary>
Motivation: Existing eye tracking methods focus on humans, neglecting rodent-specific challenges like eye variability and hair interference.

Method: Develops a biomedical image segmentation-based CNN for pupil and corneal reflection identification, incrementally trainable for field variability.

Result: Achieves high accuracy in rodent gaze determination, paired with an automated infrared video system for state-of-the-art tracking.

Conclusion: Introduces a pioneering, practical solution for rodent eye tracking in neuroscience and vision science research.

Abstract: Research in neuroscience and vision science relies heavily on careful
measurements of animal subject's gaze direction. Rodents are the most widely
studied animal subjects for such research because of their economic advantage
and hardiness. Recently, video based eye trackers that use image processing
techniques have become a popular option for gaze tracking because they are easy
to use and are completely noninvasive. Although significant progress has been
made in improving the accuracy and robustness of eye tracking algorithms,
unfortunately, almost all of the techniques have focused on human eyes, which
does not account for the unique characteristics of the rodent eye images, e.g.,
variability in eye parameters, abundance of surrounding hair, and their small
size. To overcome these unique challenges, this work presents a flexible,
robust, and highly accurate model for pupil and corneal reflection
identification in rodent gaze determination that can be incrementally trained
to account for variability in eye parameters encountered in the field. To the
best of our knowledge, this is the first paper that demonstrates a highly
accurate and practical biomedical image segmentation based convolutional neural
network architecture for pupil and corneal reflection identification in eye
images. This new method, in conjunction with our automated infrared videobased
eye recording system, offers the state of the art technology in eye tracking
for neuroscience and vision science research for rodents.

</details>


### [570] [Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing](https://arxiv.org/pdf/2506.08280)
*Daniel H. Pak, Shubh Thaker, Kyle Baylous, Xiaoran Zhang, Danny Bluestein, James S. Duncan*

Main category: eess.IV

TL;DR: A snap-and-tune strategy combining DL and test-time optimization improves volumetric meshing from medical images, enhancing spatial accuracy and mesh quality without extra training labels.


<details>
  <summary>Details</summary>
Motivation: High-quality volumetric meshing is crucial for personalized medicine, but existing DL-based methods struggle with high-curvature areas and inter-part distances.

Method: The proposed method uses a sequential DL and test-time optimization approach (snap-and-tune) for fast initial fitting and detailed mesh corrections.

Result: The method significantly improves spatial accuracy and mesh quality, demonstrated via solid mechanics simulations.

Conclusion: The snap-and-tune strategy is versatile, automated, and effective for generating high-quality meshes for medical simulations.

Abstract: High-quality volumetric meshing from medical images is a key bottleneck for
physics-based simulations in personalized medicine. For volumetric meshing of
complex medical structures, recent studies have often utilized deep learning
(DL)-based template deformation approaches to enable fast test-time generation
with high spatial accuracy. However, these approaches still exhibit
limitations, such as limited flexibility at high-curvature areas and
unrealistic inter-part distances. In this study, we introduce a simple yet
effective snap-and-tune strategy that sequentially applies DL and test-time
optimization, which combines fast initial shape fitting with more detailed
sample-specific mesh corrections. Our method provides significant improvements
in both spatial accuracy and mesh quality, while being fully automated and
requiring no additional training labels. Finally, we demonstrate the
versatility and usefulness of our newly generated meshes via solid mechanics
simulations in two different software platforms. Our code is available at
https://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh.

</details>


### [571] [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](https://arxiv.org/pdf/2506.08520)
*Srinivasan Kidambi, Pravin Nair*

Main category: eess.IV

TL;DR: PnP-Nystra is a Nyström-based linear approximation of self-attention, offering a plug-and-play module for efficient acceleration in transformer architectures without retraining.


<details>
  <summary>Details</summary>
Motivation: Quadratic complexity of multi-head self-attention (MHSA) is a computational bottleneck in real-time and resource-constrained environments.

Method: Proposes PnP-Nystra, a Nyström-based linear approximation of self-attention, as a drop-in replacement for MHSA in pre-trained models.

Result: Achieves 2-4x speed-up on GPU and 2-5x on CPU with a maximum PSNR drop of 1.5 dB.

Conclusion: PnP-Nystra is the first linear attention method functioning as a training-free substitute for MHSA in restoration models.

Abstract: Multi-head self-attention (MHSA) has become a core component in modern
computer vision models. However, its quadratic complexity with respect to input
length poses a significant computational bottleneck in real-time and resource
constrained environments. We propose PnP-Nystra, a Nystr\"om based linear
approximation of self-attention, developed as a plug-and-play (PnP) module that
can be integrated into the pre-trained image and video restoration models
without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables
efficient acceleration in various window-based transformer architectures,
including SwinIR, Uformer, and RVRT. Our experiments across diverse image and
video restoration tasks, including denoising, deblurring, and super-resolution,
demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU
and a 2-5x speed-up on CPU inference. Despite these significant gains, the
method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To
the best of our knowledge, we are the first to demonstrate a linear attention
functioning as a training-free substitute for MHSA in restoration models.

</details>


### [572] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/pdf/2506.08534)
*Donglian Li, Hui Guo, Minglang Chen, Huizhen Chen, Jialing Chen, Bocheng Liang, Pengchen Liang, Ying Tan*

Main category: eess.IV

TL;DR: A deep learning model (DCD) is proposed for automatic segmentation of fetal heart structures in A4C view, addressing challenges like noise and variability.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation in fetal echocardiography is crucial for early CHD diagnosis but is hindered by ultrasound artifacts and anatomical variability.

Method: DCD integrates Dense ASPP for multi-scale feature extraction and CBAM for adaptive feature representation, combining local and global context.

Result: DCD achieves precise and robust segmentation, enhancing prenatal cardiac assessment.

Conclusion: The model reduces sonographer workload and improves segmentation accuracy for better CHD diagnosis.

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


### [573] [Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification](https://arxiv.org/pdf/2506.08623)
*Rinat Prochii, Elizaveta Dakhova, Pavel Birulin, Maxim Sharaev*

Main category: eess.IV

TL;DR: A biologically inspired deep learning ensemble framework is introduced to classify 16 fetal structures in ultrasound images, achieving high accuracy despite challenges like low image quality and class imbalance.


<details>
  <summary>Details</summary>
Motivation: The challenge of accurately classifying second-trimester fetal ultrasound images due to low quality, high variability, and class imbalance motivates the development of a robust solution.

Method: The model uses a hierarchical, modular approach with two branches (shallow for coarse cues, detailed for fine features) and combines their outputs. It employs EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss.

Result: The model achieves >0.75 accuracy for 90% of organs and >0.85 for 75%, outperforming simpler models on a large number of classes.

Conclusion: Biologically inspired modular stacking enables scalable and robust fetal anatomy recognition in noisy clinical settings.

Abstract: Accurate classification of second-trimester fetal ultrasound images remains
challenging due to low image quality, high intra-class variability, and
significant class imbalance. In this work, we introduce a simple yet powerful,
biologically inspired deep learning ensemble framework that-unlike prior
studies focused on only a handful of anatomical targets-simultaneously
distinguishes 16 fetal structures. Drawing on the hierarchical, modular
organization of biological vision systems, our model stacks two complementary
branches (a "shallow" path for coarse, low-resolution cues and a "detailed"
path for fine, high-resolution features), concatenating their outputs for final
prediction. To our knowledge, no existing method has addressed such a large
number of classes with a comparably lightweight architecture. We trained and
evaluated on 5,298 routinely acquired clinical images (annotated by three
experts and reconciled via Dawid-Skene), reflecting real-world noise and
variability rather than a "cleaned" dataset. Despite this complexity, our
ensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies
90% of organs with accuracy > 0.75 and 75% of organs with accuracy >
0.85-performance competitive with more elaborate models applied to far fewer
categories. These results demonstrate that biologically inspired modular
stacking can yield robust, scalable fetal anatomy recognition in challenging
clinical settings.

</details>


### [574] [MAMBO: High-Resolution Generative Approach for Mammography Images](https://arxiv.org/pdf/2506.08677)
*Milica Škipina, Nikola Jovišić, Nicola Dall'Asen, Vanja Švenda, Anil Osman Tur, Slobodan Ilić, Elisa Ricci, Dubravko Ćulibrk*

Main category: eess.IV

TL;DR: The paper introduces MAMBO, a patch-based diffusion model for generating high-resolution mammograms to address data scarcity in AI training, enhancing breast cancer detection.


<details>
  <summary>Details</summary>
Motivation: The need for large, diverse datasets for AI in mammography is hindered by privacy and ethical constraints, prompting the development of MAMBO.

Method: MAMBO uses separate diffusion models for local and global contexts, integrating them into a patch-based model to generate realistic 3840x3840 pixel mammograms.

Result: MAMBO successfully generates high-resolution mammograms, aiding in noise removal and improving training for classification and anomaly detection models.

Conclusion: MAMBO demonstrates potential to enhance mammography analysis, enabling more accurate diagnoses and earlier lesion detection through realistic image generation.

Abstract: Mammography is the gold standard for the detection and diagnosis of breast
cancer. This procedure can be significantly enhanced with Artificial
Intelligence (AI)-based software, which assists radiologists in identifying
abnormalities. However, training AI systems requires large and diverse
datasets, which are often difficult to obtain due to privacy and ethical
constraints. To address this issue, the paper introduces MAMmography ensemBle
mOdel (MAMBO), a novel patch-based diffusion approach designed to generate
full-resolution mammograms. Diffusion models have shown breakthrough results in
realistic image generation, yet few studies have focused on mammograms, and
none have successfully generated high-resolution outputs required to capture
fine-grained features of small lesions. To achieve this, MAMBO integrates
separate diffusion models to capture both local and global (image-level)
contexts. The contextual information is then fed into the final patch-based
model, significantly aiding the noise removal process. This thoughtful design
enables MAMBO to generate highly realistic mammograms of up to 3840x3840
pixels. Importantly, this approach can be used to enhance the training of
classification models and extended to anomaly detection. Experiments, both
numerical and radiologist validation, assess MAMBO's capabilities in image
generation, super-resolution, and anomaly detection, highlighting its potential
to enhance mammography analysis for more accurate diagnoses and earlier lesion
detection.

</details>


### [575] [Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment](https://arxiv.org/pdf/2506.08716)
*Maximilian Tschuchnig, Lukas Lamminger, Philipp Steininger, Michael Gadermayr*

Main category: eess.IV

TL;DR: The paper proposes enhancing synthetic CT (sCT) generation from CBCT by integrating preoperative CT via multimodal learning, showing improved results, especially for low-quality CBCT-CT cases.


<details>
  <summary>Details</summary>
Motivation: CBCT has high resolution but suffers from artifacts, leading to lower visual quality compared to conventional CT. Synthetic CT generation aims to mitigate these artifacts.

Method: Multimodal learning is used to integrate intraoperative CBCT with preoperative CT for sCT generation. Validation includes real-world and synthetic datasets.

Result: Multimodal sCT outperforms unimodal baselines, with significant gains in well-aligned, low-quality CBCT-CT cases. Findings are reproducible in clinical datasets.

Conclusion: Multimodal learning effectively enhances sCT generation, particularly improving quality in challenging CBCT-CT scenarios.

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time
intraoperative imaging due to its low radiation dose and high acquisition
speed. However, despite its high resolution, CBCT suffers from significant
artifacts and thereby lower visual quality, compared to conventional Computed
Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT
(sCT) generation, translating CBCT volumes into the CT domain. In this work, we
enhance sCT generation through multimodal learning, integrating intraoperative
CBCT with preoperative CT. Beyond validation on two real-world datasets, we use
a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT
quality affect sCT quality. The results demonstrate that multimodal sCT
consistently outperform unimodal baselines, with the most significant gains
observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate
that these findings are highly reproducible in real-world clinical datasets.

</details>


### [576] [Diver-Robot Communication Dataset for Underwater Hand Gesture Recognition](https://arxiv.org/pdf/2506.08974)
*Igor Kvasić, Derek Orbaugh Antillon, Ðula Nađ, Christopher Walker, Iain Anderson, Nikola Mišković*

Main category: eess.IV

TL;DR: A dataset of diving gesture images for human-robot interaction underwater is presented, comparing visual detection with glove-based recognition for efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore visual detection of diving gestures for AUV communication and compare it with glove-based recognition under varying visibility conditions.

Method: Recorded gestures using an underwater camera and a smart glove with elastomer sensors, annotating over 30,000 frames. Compared visual and glove-based recognition.

Result: Dataset includes gesture performance metrics like reaction time, recognition success rate, and transmission times.

Conclusion: The dataset serves as a baseline for evaluating visual gesture recognition techniques in underwater environments.

Abstract: In this paper, we present a dataset of diving gesture images used for
human-robot interaction underwater. By offering this open access dataset, the
paper aims at investigating the potential of using visual detection of diving
gestures from an autonomous underwater vehicle (AUV) as a form of communication
with a human diver. In addition to the image recording, the same dataset was
recorded using a smart gesture recognition glove. The glove uses elastomer
sensors and on-board processing to determine the selected gesture and transmit
the command associated with the gesture to the AUV via acoustics. Although this
method can be used under different visibility conditions and even without line
of sight, it introduces a communication delay required for the acoustic
transmission of the gesture command. To compare efficiency, the glove was
equipped with visual markers proposed in a gesture-based language called
CADDIAN and recorded with an underwater camera in parallel to the glove's
onboard recognition process. The dataset contains over 30,000 underwater frames
of nearly 900 individual gestures annotated in corresponding snippet folders.
The dataset was recorded in a balanced ratio with five different divers in sea
and five different divers in pool conditions, with gestures recorded at 1, 2
and 3 metres from the camera. The glove gesture recognition statistics are
reported in terms of average diver reaction time, average time taken to perform
a gesture, recognition success rate, transmission times and more. The dataset
presented should provide a good baseline for comparing the performance of state
of the art visual diving gesture recognition techniques under different
visibility conditions.

</details>


### [577] [Variable Rate Learned Wavelet Video Coding using Temporal Layer Adaptivity](https://arxiv.org/pdf/2410.15873)
*Anna Meyer, André Kaup*

Main category: eess.IV

TL;DR: The paper introduces a learned wavelet video coder with variable rate support and quality adaptation, achieving significant bitrate savings.


<details>
  <summary>Details</summary>
Motivation: To enhance coding efficiency and scalability in wavelet-based video coding by introducing variable rate support and quality adaptation.

Method: Proposes a learned wavelet video coder with motion-compensated temporal filtering (MCTF), variable rate support, and a multi-stage training strategy for multiple temporal layers.

Result: Achieves Bjøntegaard Delta bitrate savings of at least -32% compared to a baseline learned MCTF model.

Conclusion: The proposed extensions improve coding efficiency and scalability, with code made publicly available.

Abstract: Learned wavelet video coders provide an explainable framework by performing
discrete wavelet transforms in temporal, horizontal, and vertical dimensions.
With a temporal transform based on motion-compensated temporal filtering
(MCTF), spatial and temporal scalability is obtained. In this paper, we
introduce variable rate support and a mechanism for quality adaption to
different temporal layers for a higher coding efficiency. Moreover, we propose
a multi-stage training strategy that allows training with multiple temporal
layers. Our experiments demonstrate Bj{\o}ntegaard Delta bitrate savings of at
least -32% compared to a learned MCTF model without these extensions. Training
and inference code is available at: https://github.com/FAU-LMS/Learned-pMCTF.

</details>


### [578] [Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection](https://arxiv.org/pdf/2505.17683)
*Dan Yuan, Yi Feng, Ziyun Tang*

Main category: eess.IV

TL;DR: Proposes an enhanced Residual U-Net with dual attention mechanisms (CBAM and SAL) for IVH segmentation in premature infants, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Early and accurate IVH detection from brain US images is critical for improving clinical outcomes in premature infants.

Method: Uses a Residual U-Net with CBAM for spatial/channel refinement and SAL for sparse/dense attention to balance local and global features.

Result: Achieves 89.04% Dice score and 81.84% IoU for ventricle segmentation, outperforming existing methods.

Conclusion: Integrating spatial refinement and attention sparsity enhances robust brain anatomy detection.

Abstract: Intraventricular hemorrhage (IVH) is a severe neurological complication among
premature infants, necessitating early and accurate detection from brain
ultrasound (US) images to improve clinical outcomes. While recent deep learning
methods offer promise for computer-aided diagnosis, challenges remain in
capturing both local spatial details and global contextual dependencies
critical for segmenting brain anatomies. In this work, we propose an enhanced
Residual U-Net architecture incorporating two complementary attention
mechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse
Attention Layer (SAL). The CBAM improves the model's ability to refine spatial
and channel-wise features, while the SAL introduces a dual-branch design,
sparse attention filters out low-confidence query-key pairs to suppress noise,
and dense attention ensures comprehensive information propagation. Extensive
experiments on the Brain US dataset demonstrate that our method achieves
state-of-the-art segmentation performance, with a Dice score of 89.04% and IoU
of 81.84% for ventricle region segmentation. These results highlight the
effectiveness of integrating spatial refinement and attention sparsity for
robust brain anatomy detection. Code is available at:
https://github.com/DanYuan001/BrainImgSegment.

</details>


### [579] [Estimation of Head Motion in Structural MRI and its Impact on Cortical Thickness Measurements in Retrospective Data](https://arxiv.org/pdf/2505.23916)
*Charles Bricout, Samira Ebrahimi Kahou, Sylvain Bouix*

Main category: eess.IV

TL;DR: A 3D CNN is trained to estimate motion artifacts in MRI scans using synthetic motion-corrupted data, validated across multiple datasets, and shown to generalize across scanner brands and protocols.


<details>
  <summary>Details</summary>
Motivation: Motion artifacts in MRI bias neuroanatomical metrics like cortical thickness, especially in populations like children or ADHD patients, but current methods lack objectivity or require specialized hardware.

Method: A 3D convolutional neural network is trained on synthetically motion-corrupted MRI volumes to estimate motion metrics, validated on independent datasets including manual ratings.

Result: Achieved $R^2 = 0.65$ against manual labels, significant thickness-motion correlations in 12/15 datasets, and motion-age correlations aligning with prior studies.

Conclusion: The method provides scalable, objective motion assessment for structural MRI studies, aiding bias evaluation in cortical thickness analyses.

Abstract: Motion-related artifacts are inevitable in Magnetic Resonance Imaging (MRI)
and can bias automated neuroanatomical metrics such as cortical thickness.
These biases can interfere with statistical analysis which is a major concern
as motion has been shown to be more prominent in certain populations such as
children or individuals with ADHD. Manual review cannot objectively quantify
motion in anatomical scans, and existing quantitative automated approaches
often require specialized hardware or custom acquisition protocols. Here, we
train a 3D convolutional neural network to estimate a summary motion metric in
retrospective routine research scans by leveraging a large training dataset of
synthetically motion-corrupted volumes. We validate our method with one
held-out site from our training cohort and with 14 fully independent datasets,
including one with manual ratings, achieving a representative $R^2 = 0.65$
versus manual labels and significant thickness-motion correlations in 12/15
datasets. Furthermore, our predicted motion correlates with subject age in line
with prior studies. Our approach generalizes across scanner brands and
protocols, enabling objective, scalable motion assessment in structural MRI
studies without prospective motion correction. By providing reliable motion
estimates, our method offers researchers a tool to assess and account for
potential biases in cortical thickness analyses.

</details>
