{"id": "2506.09506", "pdf": "https://arxiv.org/pdf/2506.09506", "abs": "https://arxiv.org/abs/2506.09506", "authors": ["Bastian J\u00e4ckl", "Vojt\u011bch Kloda", "Daniel A. Keim", "Jakub Loko\u010d"], "title": "Dynamic Sub-region Search in Homogeneous Collections Using CLIP", "categories": ["cs.MM", "68U10", "H.3.3; I.4.10; H.2.8"], "comment": "18 pages, 4 figures, 5 tables", "summary": "Querying with text-image-based search engines in highly homogeneous\ndomain-specific image collections is challenging for users, as they often\nstruggle to provide descriptive text queries. For example, in an underwater\ndomain, users can usually characterize entities only with abstract labels, such\nas corals and fish, which leads to low recall rates. Our work investigates\nwhether recall can be improved by supplementing text queries with position\ninformation. Specifically, we explore dynamic image partitioning approaches\nthat divide candidates into semantically meaningful regions of interest.\nInstead of querying entire images, users can specify regions they recognize.\nThis enables the use of position constraints while preserving the semantic\ncapabilities of multimodal models. We introduce and evaluate strategies for\nintegrating position constraints into semantic search models and compare them\nagainst static partitioning approaches. Our evaluation highlights both the\npotential and the limitations of sub-region-based search methods using dynamic\npartitioning. Dynamic search models achieve up to double the retrieval\nperformance compared to static partitioning approaches but are highly sensitive\nto perturbations in the specified query positions.", "AI": {"tldr": "The paper explores improving recall in text-image search for homogeneous domains by adding position constraints to queries via dynamic image partitioning, outperforming static methods but being sensitive to position accuracy.", "motivation": "Users struggle with descriptive text queries in homogeneous domains (e.g., underwater imagery), leading to low recall. Position constraints could enhance search effectiveness.", "method": "Dynamic image partitioning divides images into semantically meaningful regions, allowing users to specify regions for queries. Position constraints are integrated into semantic search models.", "result": "Dynamic partitioning doubles retrieval performance over static methods but is highly sensitive to query position accuracy.", "conclusion": "Dynamic sub-region-based search shows promise for improving recall but requires precise position inputs to be effective."}}
{"id": "2506.09795", "pdf": "https://arxiv.org/pdf/2506.09795", "abs": "https://arxiv.org/abs/2506.09795", "authors": ["Amritha Premkumar", "Prajit T Rajendran", "Vignesh V Menon"], "title": "Learning Quality from Complexity and Structure: A Feature-Fused XGBoost Model for Video Quality Assessment", "categories": ["cs.MM"], "comment": "ICME 2025", "summary": "This paper presents a novel approach for reduced-reference video quality\nassessment (VQA), developed as part of the recent VQA Grand Challenge. Our\nmethod leverages low-level complexity and structural information from reference\nand test videos to predict perceptual quality scores. Specifically, we extract\nspatio-temporal features using Video Complexity Analyzer (VCA) and compute SSIM\nvalues from the test video to capture both texture and structural\ncharacteristics. These features are aggregated through temporal pooling, and\nresidual features are calculated by comparing the original and distorted\nfeature sets. The combined features are used to train an XGBoost regression\nmodel that estimates the overall video quality. The pipeline is fully\nautomated, interpretable, and highly scalable, requiring no deep neural\nnetworks or GPU inference. Experimental results on the challenge dataset\ndemonstrate that our proposed method achieves competitive correlation with\nsubjective quality scores while maintaining a low computational footprint. The\nmodel's lightweight design and strong generalization performance suit real-time\nstreaming quality monitoring and adaptive encoding scenarios.", "AI": {"tldr": "A novel reduced-reference VQA method using low-level complexity and structural features, trained with XGBoost, achieves competitive results with low computational cost.", "motivation": "To develop a scalable, interpretable, and efficient VQA method without relying on deep neural networks for real-time applications.", "method": "Extracts spatio-temporal features (VCA, SSIM), aggregates them via temporal pooling, computes residuals, and trains an XGBoost model.", "result": "Competitive correlation with subjective scores and low computational footprint.", "conclusion": "The method is suitable for real-time streaming and adaptive encoding due to its lightweight and generalizable design."}}
{"id": "2506.09650", "pdf": "https://arxiv.org/pdf/2506.09650", "abs": "https://arxiv.org/abs/2506.09650", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.RO", "eess.IV"], "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "AI": {"tldr": "The paper introduces a new task of textual reference-guided human action segmentation in multi-person settings, proposes a dataset (RHAS133), and presents HopaDIFF, a novel framework achieving state-of-the-art results.", "motivation": "Existing action segmentation methods focus on single-person activities, ignoring multi-person scenarios. This work addresses this gap by leveraging textual descriptions to specify target persons.", "method": "The authors propose HopaDIFF, a holistic-partial aware Fourier-conditioned diffusion framework, using cross-input gate attentional xLSTM for long-range reasoning and Fourier conditions for fine-grained control.", "result": "HopaDIFF achieves state-of-the-art performance on the new RHAS133 dataset, outperforming existing methods.", "conclusion": "The work pioneers multi-person action segmentation with textual guidance, introduces a benchmark dataset, and demonstrates the effectiveness of HopaDIFF."}}
{"id": "2506.09792", "pdf": "https://arxiv.org/pdf/2506.09792", "abs": "https://arxiv.org/abs/2506.09792", "authors": ["Wenxuan Wu", "Shuai Wang", "Xixin Wu", "Helen Meng", "Haizhou Li"], "title": "Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.", "AI": {"tldr": "The paper explores using pre-trained speech-language and language models (PSLMs/PLMs) to enhance audio-visual target speaker extraction (AV-TSE) by leveraging linguistic knowledge, improving speech quality and intelligibility without extra inference cost.", "motivation": "Humans use linguistic knowledge (e.g., syntax, semantics) for speech perception, inspiring the integration of PSLMs/PLMs into AV-TSE to provide auxiliary linguistic constraints.", "method": "Proposes incorporating linguistic constraints from PSLMs/PLMs as additional supervision signals for AV-TSE models.", "result": "Consistent improvements in speech quality and intelligibility, with robust performance in multi-language and visual cue-impaired scenarios.", "conclusion": "Leveraging PSLMs/PLMs enhances AV-TSE effectively, demonstrating broad applicability and robustness."}}
{"id": "2506.09231", "pdf": "https://arxiv.org/pdf/2506.09231", "abs": "https://arxiv.org/abs/2506.09231", "authors": ["Saba Tabatabaee", "Suzanne Boyce", "Liran Oren", "Mark Tiede", "Carol Espy-Wilson"], "title": "Enhancing Acoustic-to-Articulatory Speech Inversion by Incorporating Nasality", "categories": ["eess.AS"], "comment": "Accepted to be presented at Interspeech 2025", "summary": "Speech is produced through the coordination of vocal tract constricting\norgans: lips, tongue, velum, and glottis. Previous works developed Speech\nInversion (SI) systems to recover acoustic-to-articulatory mappings for lip and\ntongue constrictions, called oral tract variables (TVs), which were later\nenhanced by including source information (periodic and aperiodic energies, and\nF0 frequency) as proxies for glottal control. Comparison of the nasometric\nmeasures with high-speed nasopharyngoscopy showed that nasalance can serve as\nground truth, and that an SI system trained with it reliably recovers velum\nmovement patterns for American English speakers. Here, two SI training\napproaches are compared: baseline models that estimate oral TVs and nasalance\nindependently, and a synergistic model that combines oral TVs and source\nfeatures with nasalance. The synergistic model shows relative improvements of\n5% in oral TVs estimation and 9% in nasalance estimation compared to the\nbaseline models.", "AI": {"tldr": "The paper compares two speech inversion (SI) training approaches: baseline models (independent estimation of oral tract variables and nasalance) and a synergistic model (combining oral tract variables, source features, and nasalance). The synergistic model outperforms baselines by 5% in oral tract variables and 9% in nasalance estimation.", "motivation": "To improve speech inversion systems by integrating velum movement patterns (nasalance) with oral tract variables and source features for more accurate articulatory mapping.", "method": "Two approaches are compared: baseline models (independent estimation) and a synergistic model (combined estimation). Nasalance is used as ground truth for velum movement.", "result": "The synergistic model achieves 5% better oral tract variables and 9% better nasalance estimation than baseline models.", "conclusion": "Combining oral tract variables, source features, and nasalance in a synergistic model enhances speech inversion accuracy, demonstrating the value of integrated training approaches."}}
{"id": "2506.09147", "pdf": "https://arxiv.org/pdf/2506.09147", "abs": "https://arxiv.org/abs/2506.09147", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perli\u0107", "Ekaterina Borisova", "Markarit Vartampetian"], "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "AI": {"tldr": "The paper introduces LLM-as-a-qualitative-judge, an LLM-based evaluation method for NLG systems, focusing on structured reports of common issues instead of numerical scores. It includes open-ended analysis and issue clustering, validated with human annotations.", "motivation": "Current LLM-as-a-judge methods in NLG focus on numerical scores, lacking qualitative insights for system improvement. This work aims to provide actionable feedback for developers.", "method": "The approach involves open-ended per-instance issue analysis and clustering using an intuitive cumulative algorithm. It is evaluated with ~300 annotations from 12 NLG datasets.", "result": "LLM-as-a-qualitative-judge correctly identifies issues in 2/3 cases and produces reports similar to human annotators.", "conclusion": "The method effectively provides qualitative insights for NLG system improvements, bridging the gap between numerical evaluation and actionable feedback."}}
{"id": "2506.09176", "pdf": "https://arxiv.org/pdf/2506.09176", "abs": "https://arxiv.org/abs/2506.09176", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "ICML 2025 Poster", "summary": "Interactive Imitation Learning (IIL) allows agents to acquire desired\nbehaviors through human interventions, but current methods impose high\ncognitive demands on human supervisors. We propose the Adaptive Intervention\nMechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive\ncriterion for requesting human demonstrations. AIM utilizes a proxy Q-function\nto mimic the human intervention rule and adjusts intervention requests based on\nthe alignment between agent and human actions. By assigning high Q-values when\nthe agent deviates from the expert and decreasing these values as the agent\nbecomes proficient, the proxy Q-function enables the agent to assess the\nreal-time alignment with the expert and request assistance when needed. Our\nexpert-in-the-loop experiments reveal that AIM significantly reduces expert\nmonitoring efforts in both continuous and discrete control tasks. Compared to\nthe uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%\nimprovement in terms of human take-over cost and learning efficiency.\nFurthermore, AIM effectively identifies safety-critical states for expert\nassistance, thereby collecting higher-quality expert demonstrations and\nreducing overall expert data and environment interactions needed. Code and demo\nvideo are available at https://github.com/metadriverse/AIM.", "AI": {"tldr": "AIM, a robot-gated IIL algorithm, reduces human supervision by adaptively requesting demonstrations using a proxy Q-function, improving efficiency and safety.", "motivation": "Current IIL methods demand high cognitive effort from human supervisors, prompting the need for adaptive intervention.", "method": "AIM uses a proxy Q-function to assess agent-expert alignment, adjusting intervention requests dynamically.", "result": "AIM reduces human take-over costs by 40% and improves learning efficiency compared to Thrifty-DAgger.", "conclusion": "AIM enhances IIL by reducing expert effort, improving demonstration quality, and minimizing unnecessary interactions."}}
{"id": "2506.09052", "pdf": "https://arxiv.org/pdf/2506.09052", "abs": "https://arxiv.org/abs/2506.09052", "authors": ["Delower Hossain", "Ehsan Saghapour", "Kevin Song", "Jake Y. Chen"], "title": "Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "7 Pages", "summary": "Antibody-facilitated immune responses are central to the body's defense\nagainst pathogens, viruses, and other foreign invaders. The ability of\nantibodies to specifically bind and neutralize antigens is vital for\nmaintaining immunity. Over the past few decades, bioengineering advancements\nhave significantly accelerated therapeutic antibody development. These\nantibody-derived drugs have shown remarkable efficacy, particularly in treating\ncancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.\nTraditionally, experimental methods for affinity measurement have been\ntime-consuming and expensive. With the advent of artificial intelligence, in\nsilico medicine has been revolutionized; recent developments in machine\nlearning, particularly the use of large language models (LLMs) for representing\nantibodies, have opened up new avenues for AI-based design and improved\naffinity prediction. Herein, we present an advanced antibody-antigen binding\naffinity prediction model (LlamaAffinity), leveraging an open-source Llama 3\nbackbone and antibody sequence data sourced from the Observed Antibody Space\n(OAS) database. The proposed approach shows significant improvement over\nexisting state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)\nacross multiple evaluation metrics. Specifically, the model achieved an\naccuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of\n0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher\ncomputational efficiency, with a five-fold average cumulative training time of\nonly 0.46 hours, significantly lower than in previous studies.", "AI": {"tldr": "The paper introduces LlamaAffinity, an AI-based model for predicting antibody-antigen binding affinity, outperforming existing methods with high accuracy and efficiency.", "motivation": "Traditional experimental methods for antibody affinity measurement are costly and slow, prompting the need for AI-driven solutions.", "method": "The model uses a Llama 3 backbone and antibody sequence data from the OAS database to predict binding affinity.", "result": "LlamaAffinity achieved superior metrics (e.g., 0.9640 accuracy, 0.9936 AUC-ROC) and faster training (0.46 hours).", "conclusion": "The model advances AI-based antibody design, offering a faster, more accurate alternative to traditional methods."}}
{"id": "2506.09066", "pdf": "https://arxiv.org/pdf/2506.09066", "abs": "https://arxiv.org/abs/2506.09066", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "AI": {"tldr": "ReStNet proposes a dynamic network stitching method for adapting pre-trained models to diverse IoT device constraints, achieving flexible efficiency-accuracy trade-offs.", "motivation": "Deploying fixed pre-trained models across heterogeneous IoT devices is challenging due to varying computational resources. Traditional compression methods lack flexibility.", "method": "ReStNet stitches pre-trained models by selecting optimal points via Centered Kernel Alignment (CKA), fine-tuning only the stitching layer, and supports homogeneous and heterogeneous model combinations.", "result": "ReStNet achieves flexible accuracy-efficiency trade-offs, reduces training costs, and adapts to varying resource constraints effectively.", "conclusion": "ReStNet offers a scalable and efficient solution for deploying pre-trained models in resource-constrained IoT environments."}}
{"id": "2506.09335", "pdf": "https://arxiv.org/pdf/2506.09335", "abs": "https://arxiv.org/abs/2506.09335", "authors": ["Moshi Wei", "Sparks Li"], "title": "Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds", "categories": ["cs.MA", "cs.AI"], "comment": "11 pages, 1 figures,", "summary": "The Intelligent System of Emergent Knowledge (ISEK) establishes a\ndecentralized network where human and artificial intelligence agents\ncollaborate as peers, forming a self-organizing cognitive ecosystem. Built on\nWeb3 infrastructure, ISEK combines three fundamental principles: (1) a\ndecentralized multi-agent architecture resistant to censorship, (2) symbiotic\nAI-human collaboration with equal participation rights, and (3) resilient\nself-adaptation through distributed consensus mechanisms.\n  The system implements an innovative coordination protocol featuring a\nsix-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for\ndynamic task allocation, supported by robust fault tolerance and a\nmultidimensional reputation system. Economic incentives are governed by the\nnative $ISEK token, facilitating micropayments, governance participation, and\nreputation tracking, while agent sovereignty is maintained through NFT-based\nidentity management.\n  This synthesis of blockchain technology, artificial intelligence, and\nincentive engineering creates an infrastructure that actively facilitates\nemergent intelligence. ISEK represents a paradigm shift from conventional\nplatforms, enabling the organic development of large-scale, decentralized\ncognitive systems where autonomous agents collectively evolve beyond\ncentralized constraints.", "AI": {"tldr": "ISEK is a decentralized network combining AI and human agents on Web3, using blockchain and incentives to foster emergent intelligence.", "motivation": "To create a self-organizing cognitive ecosystem where AI and humans collaborate equally, free from centralized control.", "method": "Uses a decentralized multi-agent architecture, symbiotic collaboration, and distributed consensus. Implements a six-phase workflow for task allocation, fault tolerance, and reputation tracking with $ISEK tokens.", "result": "Enables large-scale, decentralized cognitive systems where agents evolve beyond centralized constraints.", "conclusion": "ISEK shifts the paradigm by facilitating emergent intelligence through decentralized, collaborative systems."}}
{"id": "2506.09189", "pdf": "https://arxiv.org/pdf/2506.09189", "abs": "https://arxiv.org/abs/2506.09189", "authors": ["Esteban Guti\u00e9rrez", "Rodrigo C\u00e1diz", "Carlos Sing Long", "Frederic Font", "Xavier Serra"], "title": "Fractional Fourier Sound Synthesis", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted to the International Computer Music Conference (ICMC) 2025\n  held in Boston, USA. 6 pages and 2 figures", "summary": "This paper explores the innovative application of the Fractional Fourier\nTransform (FrFT) in sound synthesis, highlighting its potential to redefine\ntime-frequency analysis in audio processing. As an extension of the classical\nFourier Transform, the FrFT introduces fractional order parameters, enabling a\ncontinuous interpolation between time and frequency domains and unlocking\nunprecedented flexibility in signal manipulation. Crucially, the FrFT also\nopens the possibility of directly synthesizing sounds in the alpha-domain,\nproviding a unique framework for creating timbral and dynamic characteristics\nunattainable through conventional methods. This work delves into the\nmathematical principles of the FrFT, its historical evolution, and its\ncapabilities for synthesizing complex audio textures. Through experimental\nanalyses, we showcase novel sound design techniques, such as alpha-synthesis\nand alpha-filtering, which leverage the FrFT's time-frequency rotation\nproperties to produce innovative sonic results. The findings affirm the FrFT's\nvalue as a transformative tool for composers, sound designers, and researchers\nseeking to push the boundaries of auditory creativity.", "AI": {"tldr": "The paper discusses using the Fractional Fourier Transform (FrFT) for sound synthesis, offering new time-frequency analysis and sound design techniques.", "motivation": "To explore FrFT's potential in audio processing, enabling unique sound synthesis methods beyond traditional Fourier Transform.", "method": "Mathematical analysis of FrFT, historical context, and experimental techniques like alpha-synthesis and alpha-filtering.", "result": "Demonstrated innovative sound design capabilities, showcasing FrFT's flexibility in creating complex audio textures.", "conclusion": "FrFT is a transformative tool for advancing auditory creativity in music and sound design."}}
{"id": "2506.09065", "pdf": "https://arxiv.org/pdf/2506.09065", "abs": "https://arxiv.org/abs/2506.09065", "authors": ["Abigail Copiaco", "Christian Ritz", "Yassine Himeur", "Valsamma Eapen", "Ammar Albanna", "Wathiq Mansoor"], "title": "Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "6 pages, 8 figures, and 1 table", "summary": "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the\npast decade, posing significant challenges in communication, behavior, and\nfocus for affected individuals. Current diagnostic techniques, though\neffective, are time-intensive, leading to high social and economic costs. This\nwork introduces an AI-powered assistive technology designed to streamline ASD\ndiagnosis and management, enhancing convenience for individuals with ASD and\nefficiency for caregivers and therapists. The system integrates transfer\nlearning with image transforms derived from eye gaze variables to diagnose ASD.\nThis facilitates and opens opportunities for in-home periodical diagnosis,\nreducing stress for individuals and caregivers, while also preserving user\nprivacy through the use of image transforms. The accessibility of the proposed\nmethod also offers opportunities for improved communication between guardians\nand therapists, ensuring regular updates on progress and evolving support\nneeds. Overall, the approach proposed in this work ensures timely, accessible\ndiagnosis while protecting the subjects' privacy, improving outcomes for\nindividuals with ASD.", "AI": {"tldr": "An AI-powered system using transfer learning and eye gaze data aims to streamline ASD diagnosis, offering in-home convenience and privacy while improving caregiver-therapist communication.", "motivation": "The rising prevalence of ASD and the inefficiency of current diagnostic methods drive the need for a faster, more accessible solution.", "method": "The system employs transfer learning and image transforms from eye gaze variables to diagnose ASD.", "result": "The approach enables in-home diagnosis, reduces stress, and enhances privacy and communication between caregivers and therapists.", "conclusion": "The proposed method provides timely, accessible ASD diagnosis while safeguarding privacy and improving outcomes."}}
{"id": "2506.05683", "pdf": "https://arxiv.org/pdf/2506.05683", "abs": "https://arxiv.org/abs/2506.05683", "authors": ["Fardis Nadimi", "Payam Abdisarabshali", "Kasra Borazjani", "Jacob Chakareski", "Seyyedali Hosseinalipour"], "title": "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MM"], "comment": "16 pages, 4 Figures, 8 Tables", "summary": "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.", "AI": {"tldr": "The paper proposes using multi-modal multi-task federated foundation models (FedFMs) to enhance XR systems, addressing challenges under the SHIFT framework and outlining evaluation metrics and design tradeoffs.", "motivation": "To integrate the strengths of foundation models with federated learning for privacy-preserving, intelligent XR systems.", "method": "A modular architecture for FedFMs with coordination paradigms for training and aggregation, addressing XR challenges under the SHIFT dimensions.", "result": "Identifies key challenges (SHIFT dimensions) and proposes metrics, dataset requirements, and tradeoffs for FedFMs in XR.", "conclusion": "Lays the groundwork for context-aware, privacy-preserving intelligence in next-gen XR systems."}}
{"id": "2506.09521", "pdf": "https://arxiv.org/pdf/2506.09521", "abs": "https://arxiv.org/abs/2506.09521", "authors": ["\u00dcnal Ege Gaznepoglu", "Anna Leschanowsky", "Ahmad Aloradi", "Prachi Singh", "Daniel Tenbrinck", "Emanu\u00ebl A. P. Habets", "Nils Peters"], "title": "You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks", "categories": ["eess.AS", "cs.CL"], "comment": "5 pages, 6 figures, 1 table, accepted at INTERSPEECH 2025", "summary": "Speaker anonymization systems hide the identity of speakers while preserving\nother information such as linguistic content and emotions. To evaluate their\nprivacy benefits, attacks in the form of automatic speaker verification (ASV)\nsystems are employed. In this study, we assess the impact of intra-speaker\nlinguistic content similarity in the attacker training and evaluation datasets,\nby adapting BERT, a language model, as an ASV system. On the VoicePrivacy\nAttacker Challenge datasets, our method achieves a mean equal error rate (EER)\nof 35%, with certain speakers attaining EERs as low as 2%, based solely on the\ntextual content of their utterances. Our explainability study reveals that the\nsystem decisions are linked to semantically similar keywords within utterances,\nstemming from how LibriSpeech is curated. Our study suggests reworking the\nVoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge\nthe reliance on global EER for privacy evaluations.", "AI": {"tldr": "The paper evaluates speaker anonymization by using BERT as an ASV system, revealing biases in VoicePrivacy datasets due to linguistic content similarity.", "motivation": "To assess the privacy benefits of speaker anonymization systems by examining the impact of intra-speaker linguistic content similarity in ASV attacks.", "method": "Adapts BERT as an ASV system to analyze textual content of utterances in VoicePrivacy datasets.", "result": "Achieves a mean EER of 35%, with some speakers as low as 2%, highlighting biases from semantically similar keywords.", "conclusion": "Suggests reworking VoicePrivacy datasets for fair evaluation and questions reliance on global EER for privacy assessments."}}
{"id": "2506.09175", "pdf": "https://arxiv.org/pdf/2506.09175", "abs": "https://arxiv.org/abs/2506.09175", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "AI": {"tldr": "A phrase dictionary biasing method improves speech translation by leveraging phrase mappings, outperforming phrase list biasing by 21% for streaming models and boosting multimodal models by 85% in phrase recall.", "motivation": "Phrases are crucial for understanding conversations but are rare in training data, making their correct translation challenging in speech tasks.", "method": "Proposes a phrase dictionary biasing method using source-to-target phrase mappings, applied to streaming speech translation and multimodal large language models.", "result": "Outperforms phrase list biasing by 21% for streaming models and achieves 85% relative improvement in phrase recall for multimodal models.", "conclusion": "The phrase dictionary biasing method effectively enhances phrase translation in both streaming and multimodal models."}}
{"id": "2506.09250", "pdf": "https://arxiv.org/pdf/2506.09250", "abs": "https://arxiv.org/abs/2506.09250", "authors": ["C. Opus", "A. Lawsen"], "title": "Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "categories": ["cs.AI", "cs.LG"], "comment": "Comment on: arXiv:2506.06941", "summary": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit\n\"accuracy collapse\" on planning puzzles beyond certain complexity thresholds.\nWe demonstrate that their findings primarily reflect experimental design\nlimitations rather than fundamental reasoning failures. Our analysis reveals\nthree critical issues: (1) Tower of Hanoi experiments systematically exceed\nmodel output token limits at reported failure points, with models explicitly\nacknowledging these constraints in their outputs; (2) The authors' automated\nevaluation framework fails to distinguish between reasoning failures and\npractical constraints, leading to misclassification of model capabilities; (3)\nMost concerningly, their River Crossing benchmarks include mathematically\nimpossible instances for N > 5 due to insufficient boat capacity, yet models\nare scored as failures for not solving these unsolvable problems. When we\ncontrol for these experimental artifacts, by requesting generating functions\ninstead of exhaustive move lists, preliminary experiments across multiple\nmodels indicate high accuracy on Tower of Hanoi instances previously reported\nas complete failures. These findings highlight the importance of careful\nexperimental design when evaluating AI reasoning capabilities.", "AI": {"tldr": "The paper critiques Shojaee et al.'s findings on LRMs' \"accuracy collapse,\" attributing it to flawed experimental design rather than inherent reasoning failures.", "motivation": "To address misconceptions about LRMs' reasoning capabilities due to experimental limitations.", "method": "Analyzed three key issues in the original study: token limits, evaluation framework flaws, and unsolvable benchmark instances. Conducted controlled experiments with alternative methods.", "result": "Preliminary experiments showed high accuracy on previously reported failure cases when experimental artifacts were controlled.", "conclusion": "Emphasizes the need for meticulous experimental design in evaluating AI reasoning."}}
{"id": "2506.09080", "pdf": "https://arxiv.org/pdf/2506.09080", "abs": "https://arxiv.org/abs/2506.09080", "authors": ["Jiaxiang Chen", "Mingxi Zou", "Zhuo Wang", "Qifan Wang", "Dongning Sun", "Chi Zhang", "Zenglin Xu"], "title": "FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "comment": null, "summary": "Financial decision-making presents unique challenges for language models,\ndemanding temporal reasoning, adaptive risk assessment, and responsiveness to\ndynamic events. While large language models (LLMs) show strong general\nreasoning capabilities, they often fail to capture behavioral patterns central\nto human financial decisions-such as expert reliance under information\nasymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We\npropose FinHEAR, a multi-agent framework for Human Expertise and Adaptive\nRisk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to\nanalyze historical trends, interpret current events, and retrieve\nexpert-informed precedents within an event-centric pipeline. Grounded in\nbehavioral economics, it incorporates expert-guided retrieval,\nconfidence-adjusted position sizing, and outcome-based refinement to enhance\ninterpretability and robustness. Empirical results on curated financial\ndatasets show that FinHEAR consistently outperforms strong baselines across\ntrend prediction and trading tasks, achieving higher accuracy and better\nrisk-adjusted returns.", "AI": {"tldr": "FinHEAR, a multi-agent framework, enhances financial decision-making by combining LLMs with behavioral economics principles, outperforming baselines in accuracy and risk-adjusted returns.", "motivation": "Addressing the limitations of LLMs in capturing human financial decision-making behaviors like expert reliance, loss aversion, and adaptive reasoning.", "method": "FinHEAR uses specialized LLM-based agents for historical trend analysis, current event interpretation, and expert-informed precedent retrieval in an event-centric pipeline.", "result": "Empirical results show FinHEAR outperforms baselines in trend prediction and trading tasks, achieving higher accuracy and better risk-adjusted returns.", "conclusion": "FinHEAR effectively integrates human expertise and adaptive risk-aware reasoning, improving financial decision-making for language models."}}
{"id": "2506.09067", "pdf": "https://arxiv.org/pdf/2506.09067", "abs": "https://arxiv.org/abs/2506.09067", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "AI": {"tldr": "The paper proposes an inference-time defense strategy for Med-VLMs to mitigate harmful queries without degrading general performance, using synthetic clinical demonstrations.", "motivation": "Security vulnerabilities in Med-VLMs, such as handling harmful queries and avoiding over-defense, remain underexplored.", "method": "A novel inference-time defense strategy leveraging synthetic clinical demonstrations to balance security and performance.", "result": "The strategy enhances safety without significant performance loss, with increased demonstration budgets alleviating over-defense.", "conclusion": "A mixed demonstration strategy is introduced as a trade-off for balancing security and performance under few-shot constraints."}}
{"id": "2506.09434", "pdf": "https://arxiv.org/pdf/2506.09434", "abs": "https://arxiv.org/abs/2506.09434", "authors": ["Michael Amir", "Matteo Bettini", "Amanda Prorok"], "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "The success of teams in robotics, nature, and society often depends on the\ndivision of labor among diverse specialists; however, a principled explanation\nfor when such diversity surpasses a homogeneous team is still missing. Focusing\non multi-agent task allocation problems, our goal is to study this question\nfrom the perspective of reward design: what kinds of objectives are best suited\nfor heterogeneous teams? We first consider an instantaneous, non-spatial\nsetting where the global reward is built by two generalized aggregation\noperators: an inner operator that maps the $N$ agents' effort allocations on\nindividual tasks to a task score, and an outer operator that merges the $M$\ntask scores into the global team reward. We prove that the curvature of these\noperators determines whether heterogeneity can increase reward, and that for\nbroad reward families this collapses to a simple convexity test. Next, we ask\nwhat incentivizes heterogeneity to emerge when embodied, time-extended agents\nmust learn an effort allocation policy. To study heterogeneity in such\nsettings, we use multi-agent reinforcement learning (MARL) as our computational\nparadigm, and introduce Heterogeneous Environment Design (HED), a\ngradient-based algorithm that optimizes the parameter space of underspecified\nMARL environments to find scenarios where heterogeneity is advantageous.\nExperiments in matrix games and an embodied Multi-Goal-Capture environment show\nthat, despite the difference in settings, HED rediscovers the reward regimes\npredicted by our theory to maximize the advantage of heterogeneity, both\nvalidating HED and connecting our theoretical insights to reward design in\nMARL. Together, these results help us understand when behavioral diversity\ndelivers a measurable benefit.", "AI": {"tldr": "The paper explores when diversity in multi-agent teams outperforms homogeneity, focusing on reward design and using multi-agent reinforcement learning (MARL) to validate theoretical insights.", "motivation": "To understand when and why heterogeneous teams surpass homogeneous ones in multi-agent task allocation, particularly through the lens of reward design.", "method": "The study combines theoretical analysis of reward aggregation operators with practical experiments using MARL and a novel algorithm, Heterogeneous Environment Design (HED), to test scenarios favoring heterogeneity.", "result": "The curvature of reward operators determines heterogeneity's advantage, and HED successfully identifies reward regimes where diversity benefits teams, validating theoretical predictions.", "conclusion": "The research provides a framework for understanding when behavioral diversity is advantageous, bridging theory and practical reward design in MARL."}}
{"id": "2506.09206", "pdf": "https://arxiv.org/pdf/2506.09206", "abs": "https://arxiv.org/abs/2506.09206", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models.", "AI": {"tldr": "The paper introduces SimClass, a dataset with synthesized classroom noise and simulated speech, addressing the lack of large-scale classroom speech data for AI education models.", "motivation": "The scarcity of public classroom datasets and dedicated noise corpora limits AI-driven speech model development for education.", "method": "A scalable methodology using game engines synthesizes classroom noise. SimClass pairs a children's speech corpus with YouTube lectures to simulate classroom speech.", "result": "SimClass closely approximates real classroom speech in experiments, proving useful for robust speech recognition and enhancement models.", "conclusion": "SimClass provides a valuable resource for developing AI-driven speech models in educational settings."}}
{"id": "2506.09095", "pdf": "https://arxiv.org/pdf/2506.09095", "abs": "https://arxiv.org/abs/2506.09095", "authors": ["Vivien van Veldhuizen", "Vanessa Botha", "Chunyao Lu", "Melis Erdal Cesur", "Kevin Groot Lipman", "Edwin D. de Jong", "Hugo Horlings", "Cl\u00e1risa Sanchez", "Cees Snoek", "Ritse Mann", "Eric Marcus", "Jonas Teuwen"], "title": "Foundation Models in Medical Imaging -- A Review and Outlook", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.", "AI": {"tldr": "Foundation models (FMs) are transforming medical image analysis by leveraging large unlabeled datasets, reducing reliance on manual annotations, and adapting to clinical tasks with minimal supervision.", "motivation": "To explore how FMs are developed and applied in medical imaging (pathology, radiology, ophthalmology) and their potential to improve analysis efficiency and accuracy.", "method": "Review of over 150 studies, covering FM pipelines (architectures, self-supervised learning, downstream adaptation) and comparing design choices across domains.", "result": "FMs show promise in medical imaging by learning general-purpose features adaptable to specific tasks, with varied applications across domains.", "conclusion": "While FMs offer significant potential, challenges remain, and future research should address open questions to optimize their use in medical imaging."}}
{"id": "2506.08524", "pdf": "https://arxiv.org/pdf/2506.08524", "abs": "https://arxiv.org/abs/2506.08524", "authors": ["Weiguo Wang", "Andy Nie", "Wenrui Zhou", "Yi Kai", "Chengchen Hu"], "title": "Teaching Physical Awareness to LLMs through Sounds", "categories": ["cs.SD", "cs.AI", "cs.MM", "cs.RO", "eess.AS"], "comment": "ICML 2025", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in text and\nmultimodal processing, yet they fundamentally lack physical\nawareness--understanding of real-world physical phenomena. In this work, we\npresent ACORN, a framework that teaches LLMs physical awareness through sound,\nfocusing on fundamental physical phenomena like the Doppler effect, multipath\neffect, and spatial relationships. To overcome data scarcity, ACORN introduce a\nphysics-based simulator combining real-world sound sources with controlled\nphysical channels to generate diverse training data. Using this simulator, we\nbuild AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an\naudio encoder that processes both magnitude and phase information. By\nconnecting our audio encoder to state-of-the-art LLMs, we demonstrate\nreasonable results in both simulated and real-world tasks, such as\nline-of-sight detection, Doppler effect estimation, and Direction-of-Arrival\nestimation, paving the way for enabling LLMs to understand physical world.", "AI": {"tldr": "ACORN teaches LLMs physical awareness through sound, using a physics-based simulator to generate training data and a specialized audio encoder.", "motivation": "LLMs lack physical awareness, limiting their understanding of real-world phenomena. ACORN addresses this gap by focusing on sound-based physical phenomena.", "method": "ACORN uses a physics-based simulator to create diverse training data (AQA-PHY dataset) and an audio encoder processing magnitude and phase. It connects this encoder to LLMs.", "result": "ACORN achieves reasonable performance in tasks like line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation.", "conclusion": "ACORN advances LLMs' physical awareness, demonstrating potential for real-world applications."}}
{"id": "2506.09549", "pdf": "https://arxiv.org/pdf/2506.09549", "abs": "https://arxiv.org/abs/2506.09549", "authors": ["Shafique Ahmed", "Ryandhimas E. Zezario", "Nasir Saleem", "Amir Hussain", "Hsin-Min Wang", "Yu Tsao"], "title": "A Study on Speech Assessment with Visual Cues", "categories": ["eess.AS", "cs.SD", "eess.SP"], "comment": "Accepted to Interspeech 2025", "summary": "Non-intrusive assessment of speech quality and intelligibility is essential\nwhen clean reference signals are unavailable. In this work, we propose a\nmultimodal framework that integrates audio features and visual cues to predict\nPESQ and STOI scores. It employs a dual-branch architecture, where spectral\nfeatures are extracted using STFT, and visual embeddings are obtained via a\nvisual encoder. These features are then fused and processed by a CNN-BLSTM with\nattention, followed by multi-task learning to simultaneously predict PESQ and\nSTOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND\ncorpus, show that our model outperforms the audio-only baseline. Under seen\nnoise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47%\n(0.7403->0.8253) for STOI. These results highlight the effectiveness of\nincorporating visual cues in enhancing the accuracy of non-intrusive speech\nassessment.", "AI": {"tldr": "A multimodal framework combining audio and visual cues improves non-intrusive speech quality (PESQ) and intelligibility (STOI) prediction, outperforming audio-only methods.", "motivation": "Clean reference signals are often unavailable, necessitating non-intrusive methods for assessing speech quality and intelligibility.", "method": "A dual-branch architecture extracts spectral features (STFT) and visual embeddings, fused and processed by a CNN-BLSTM with attention for multi-task learning (PESQ and STOI prediction).", "result": "The model outperforms audio-only baselines, improving LCC by 9.61% for PESQ and 11.47% for STOI under seen noise conditions.", "conclusion": "Visual cues enhance non-intrusive speech assessment accuracy, as demonstrated on the LRS3-TED dataset."}}
{"id": "2506.09218", "pdf": "https://arxiv.org/pdf/2506.09218", "abs": "https://arxiv.org/abs/2506.09218", "authors": ["Bruno Ferenc \u0160egedin"], "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "AI": {"tldr": "The study explores how generative CNNs generalize phonotactic rules from raw audio, showing convolutional layers can generalize beyond lexical constraints when the FC bottleneck is narrowed.", "motivation": "To understand if DNNs can generalize phonotactic rules independently of lexical learning and the impact of reducing the FC bottleneck.", "method": "Train generative CNNs on raw audio waveforms, shrink the FC bottleneck, and probe generalizations by bypassing the FC with randomized feature maps.", "result": "Convolutional layers generalize phonetic dependencies beyond lexical constraints, even with a narrow FC bottleneck.", "conclusion": "CNNs can dynamically generalize phonotactic rules, suggesting their potential for lexically-independent phonetic learning."}}
{"id": "2506.09344", "pdf": "https://arxiv.org/pdf/2506.09344", "abs": "https://arxiv.org/abs/2506.09344", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "18 pages,8 figures", "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "AI": {"tldr": "Ming-Omni is a unified multimodal model for processing images, text, audio, and video, excelling in speech and image generation without needing separate models or task-specific fine-tuning.", "motivation": "To create a single model capable of handling diverse multimodal tasks efficiently, eliminating the need for specialized models or redesigns.", "method": "Uses dedicated encoders and a MoE architecture (Ling) with modality-specific routers to process and fuse multimodal inputs, integrating advanced decoders for audio and image generation.", "result": "Demonstrates strong performance in unified perception and generation across modalities, matching GPT-4o in modality support.", "conclusion": "Ming-Omni is a powerful, open-source solution for multimodal tasks, encouraging further research and development."}}
{"id": "2506.09084", "pdf": "https://arxiv.org/pdf/2506.09084", "abs": "https://arxiv.org/abs/2506.09084", "authors": ["Xinyuan Wang", "Liang Wu", "Yanjie Fu"], "title": "Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Optimizing the presentation of search and recommendation results is crucial\nto enhancing user experience and engagement. Whole Page Optimization (WPO)\nplays a pivotal role in this process, as it directly influences how information\nis surfaced to users. While Pre-trained Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in generating coherent and contextually\nrelevant content, fine-tuning these models for complex tasks like WPO presents\nchallenges. Specifically, the need for extensive human-annotated data to\nmitigate issues such as hallucinations and model instability can be\nprohibitively expensive, especially in large-scale systems that interact with\nmillions of items daily. In this work, we address the challenge of fine-tuning\nLLMs for WPO by using user feedback as the supervision. Unlike manually labeled\ndatasets, user feedback is inherently noisy and less precise. To overcome this,\nwe propose a reward-based fine-tuning approach, PageLLM, which employs a\nmixed-grained reward mechanism that combines page-level and item-level rewards.\nThe page-level reward evaluates the overall quality and coherence, while the\nitem-level reward focuses on the accuracy and relevance of key recommendations.\nThis dual-reward structure ensures that both the holistic presentation and the\ncritical individual components are optimized. We validate PageLLM on both\npublic and industrial datasets. PageLLM outperforms baselines and achieves a\n0.44\\% GMV increase in an online A/B test with over 10 million users,\ndemonstrating its real-world impact.", "AI": {"tldr": "PageLLM optimizes search/recommendation results using user feedback for fine-tuning LLMs, combining page-level and item-level rewards to outperform baselines and boost GMV.", "motivation": "Fine-tuning LLMs for Whole Page Optimization (WPO) is costly due to the need for extensive human-annotated data. User feedback offers a scalable but noisy alternative.", "method": "Proposes PageLLM, a reward-based fine-tuning approach with mixed-grained rewards (page-level for coherence, item-level for relevance).", "result": "Validated on public/industrial datasets, PageLLM outperforms baselines and achieves a 0.44% GMV increase in an A/B test with 10M+ users.", "conclusion": "PageLLM effectively leverages noisy user feedback for WPO, demonstrating real-world impact."}}
{"id": "2506.09068", "pdf": "https://arxiv.org/pdf/2506.09068", "abs": "https://arxiv.org/abs/2506.09068", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "AI": {"tldr": "BG-HOP is a generative prior for modeling 3D bimanual hand-object interactions, extending single-hand priors to address data scarcity.", "motivation": "Limited bimanual interaction data motivates the extension of existing single-hand generative priors.", "method": "BG-HOP extends single-hand generative priors to model bimanual interactions and synthesize grasps.", "result": "The model successfully generates bimanual interactions and grasps for given objects.", "conclusion": "BG-HOP demonstrates promise in capturing joint hand-object distributions, with code and models made public."}}
{"id": "2506.09600", "pdf": "https://arxiv.org/pdf/2506.09600", "abs": "https://arxiv.org/abs/2506.09600", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "title": "Effective Red-Teaming of Policy-Adherent Agents", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks", "AI": {"tldr": "The paper addresses the challenge of ensuring task-oriented LLM-based agents adhere to strict policies while maintaining natural interactions. It introduces CRAFT, a red-teaming system, and tau-break benchmark to test agent robustness, and evaluates defense strategies.", "motivation": "To ensure policy-adherent agents remain resilient against adversarial users exploiting them for personal gain.", "method": "Proposes CRAFT, a multi-agent red-teaming system using policy-aware persuasive strategies, and introduces tau-break benchmark for robustness testing.", "result": "CRAFT outperforms conventional jailbreak methods, and tau-break effectively assesses agent robustness. Defense strategies provide limited protection.", "conclusion": "Stronger research-driven safeguards are needed to protect policy-adherent agents from adversarial attacks."}}
{"id": "2506.09448", "pdf": "https://arxiv.org/pdf/2506.09448", "abs": "https://arxiv.org/abs/2506.09448", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset.", "AI": {"tldr": "Integrating contextual biasing with OWSM v3.1 improves rare word recognition without retraining the model, boosting performance and efficiency.", "motivation": "SFMs struggle with rare/unseen words, and existing CB methods lack pre-trained knowledge, limiting their performance.", "method": "Freezes OWSM v3.1's parameters and integrates a CB method to leverage SFM knowledge for better rare word recognition.", "result": "11.6-point B-WER improvement, 0.9-point overall WER gain, and 7.5% faster processing on LibriSpeech 100 test-clean.", "conclusion": "The approach effectively combines CB with SFMs, enhancing rare word recognition while maintaining SFM advantages."}}
{"id": "2506.09100", "pdf": "https://arxiv.org/pdf/2506.09100", "abs": "https://arxiv.org/abs/2506.09100", "authors": ["Haonan Zhang", "Guoyan Lao", "Yuyao Zhang", "Hongjiang Wei"], "title": "Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Quantitative magnetic resonance imaging (qMRI) provides tissue-specific\nparameters vital for clinical diagnosis. Although simultaneous multi-parametric\nqMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing\nqMRI from highly undersampled, high-dimensional measurements remains a\nsignificant challenge. This difficulty arises primarily because current\nreconstruction methods that rely solely on a single prior or physics-informed\nmodel to solve the highly ill-posed inverse problem, which often leads to\nsuboptimal results. To overcome this limitation, we propose LoREIN, a novel\nunsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI\nreconstruction. Technically, LoREIN incorporates both low-rank prior and\ncontinuity prior via low-rank representation (LRR) and implicit neural\nrepresentation (INR), respectively, to enhance reconstruction fidelity. The\npowerful continuous representation of INR enables the estimation of optimal\nspatial bases within the low-rank subspace, facilitating high-fidelity\nreconstruction of weighted images. Simultaneously, the predicted multi-contrast\nweighted images provide essential structural and quantitative guidance, further\nenhancing the reconstruction accuracy of quantitative parameter maps.\nFurthermore, our work introduces a zero-shot learning paradigm with broad\npotential in complex spatiotemporal and high-dimensional image reconstruction\ntasks, further advancing the field of medical imaging.", "AI": {"tldr": "LoREIN is an unsupervised dual-prior framework for accelerated 3D multi-parametric qMRI reconstruction, combining low-rank and continuity priors for improved fidelity.", "motivation": "Current qMRI reconstruction methods using single priors or physics-informed models yield suboptimal results for highly undersampled data.", "method": "LoREIN integrates low-rank prior (via LRR) and continuity prior (via INR) to enhance reconstruction accuracy.", "result": "The framework enables high-fidelity reconstruction of weighted images and quantitative parameter maps.", "conclusion": "LoREIN advances medical imaging with a zero-shot learning paradigm for complex reconstruction tasks."}}
{"id": "2506.09606", "pdf": "https://arxiv.org/pdf/2506.09606", "abs": "https://arxiv.org/abs/2506.09606", "authors": ["David Combei", "Adriana Stan", "Dan Oneata", "Nicolas M\u00fcller", "Horia Cucu"], "title": "Unmasking real-world audio deepfakes: A data-centric approach", "categories": ["eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "The growing prevalence of real-world deepfakes presents a critical challenge\nfor existing detection systems, which are often evaluated on datasets collected\njust for scientific purposes. To address this gap, we introduce a novel dataset\nof real-world audio deepfakes. Our analysis reveals that these real-world\nexamples pose significant challenges, even for the most performant detection\nmodels. Rather than increasing model complexity or exhaustively search for a\nbetter alternative, in this work we focus on a data-centric paradigm, employing\nstrategies like dataset curation, pruning, and augmentation to improve model\nrobustness and generalization.\n  Through these methods, we achieve a 55% relative reduction in EER on the\nIn-the-Wild dataset, reaching an absolute EER of 1.7%, and a 63% reduction on\nour newly proposed real-world deepfakes dataset, AI4T. These results highlight\nthe transformative potential of data-centric approaches in enhancing deepfake\ndetection for real-world applications. Code and data available at:\nhttps://github.com/davidcombei/AI4T.", "AI": {"tldr": "The paper introduces a real-world audio deepfake dataset (AI4T) and uses data-centric methods (curation, pruning, augmentation) to improve detection, achieving significant EER reductions.", "motivation": "Existing deepfake detection systems struggle with real-world examples, necessitating better datasets and methods.", "method": "Data-centric approaches like dataset curation, pruning, and augmentation.", "result": "55% EER reduction on In-the-Wild dataset (1.7% absolute EER) and 63% reduction on AI4T dataset.", "conclusion": "Data-centric strategies effectively enhance deepfake detection for real-world applications."}}
{"id": "2506.09251", "pdf": "https://arxiv.org/pdf/2506.09251", "abs": "https://arxiv.org/abs/2506.09251", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "AI": {"tldr": "Transformer models can transfer length generalization across related tasks, improving extrapolation to longer inputs.", "motivation": "To understand how transformer models generalize to longer inputs by investigating task association and transfer effects.", "method": "Study length generalization via task association, using auxiliary tasks to train models and analyze transfer effects across algorithmic tasks.", "result": "Models generalize better to longer inputs when trained with related auxiliary tasks, and pretrained models show similar transfer effects.", "conclusion": "Transformers reuse computational structures across tasks, enhancing generalization, with attention head reuse playing a key role."}}
{"id": "2506.09390", "pdf": "https://arxiv.org/pdf/2506.09390", "abs": "https://arxiv.org/abs/2506.09390", "authors": ["Kehan Zheng", "Jinfeng Zhou", "Hongning Wang"], "title": "Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "Large language models are increasingly used in strategic decision-making\nsettings, yet evidence shows that, like humans, they often deviate from full\nrationality. In this study, we compare LLMs and humans using experimental\nparadigms directly adapted from behavioral game-theory research. We focus on\ntwo well-studied strategic games, Rock-Paper-Scissors and the Prisoner's\nDilemma, which are well known for revealing systematic departures from rational\nplay in human subjects. By placing LLMs in identical experimental conditions,\nwe evaluate whether their behaviors exhibit the bounded rationality\ncharacteristic of humans. Our findings show that LLMs reproduce familiar human\nheuristics, such as outcome-based strategy switching and increased cooperation\nwhen future interaction is possible, but they apply these rules more rigidly\nand demonstrate weaker sensitivity to the dynamic changes in the game\nenvironment. Model-level analyses reveal distinctive architectural signatures\nin strategic behavior, and even reasoning models sometimes struggle to find\neffective strategies in adaptive situations. These results indicate that\ncurrent LLMs capture only a partial form of human-like bounded rationality and\nhighlight the need for training methods that encourage flexible opponent\nmodeling and stronger context awareness.", "AI": {"tldr": "LLMs exhibit human-like bounded rationality in strategic games but apply heuristics more rigidly and lack dynamic sensitivity.", "motivation": "To compare LLMs and humans in strategic decision-making using behavioral game-theory paradigms.", "method": "Experimental evaluation of LLMs in Rock-Paper-Scissors and Prisoner's Dilemma games, mirroring human studies.", "result": "LLMs replicate human heuristics but rigidly, showing weaker adaptability to game dynamics.", "conclusion": "Current LLMs partially mimic human bounded rationality; improved training for flexibility and context awareness is needed."}}
{"id": "2506.09085", "pdf": "https://arxiv.org/pdf/2506.09085", "abs": "https://arxiv.org/abs/2506.09085", "authors": ["Xinyuan Wang", "Haoyue Bai", "Nanxu Gong", "Wangyang Ying", "Sixun Dong", "Xiquan Cui", "Yanjie Fu"], "title": "LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Feature transformation enhances data representation by deriving new features\nfrom the original data. Generative AI offers potential for this task, but faces\nchallenges in stable generation (consistent outputs) and valid generation\n(error-free sequences). Existing methods--traditional MLs' low validity and\nLLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,\nwhile ML's gradient-steered search stabilizes performance. To bridge this gap,\nwe propose a teaming framework combining LLMs' symbolic generation with ML's\ngradient optimization. This framework includes four steps: (1) golden examples\ngeneration, aiming to prepare high-quality samples with the ground knowledge of\nthe teacher LLM; (2) feature transformation sequence embedding and search,\nintending to uncover potentially superior embeddings within the latent space;\n(3) student LLM feature transformation, aiming to distill knowledge from the\nteacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the\nstudent LLM probabilities for valid and stable generation. The experiments on\nvarious datasets show that the teaming policy can achieve 5\\% improvement in\ndownstream performance while reducing nearly half of the error cases. The\nresults also demonstrate the efficiency and robustness of the teaming policy.\nAdditionally, we also have exciting findings on LLMs' capacity to understand\nthe original data.", "AI": {"tldr": "A teaming framework combines LLMs and ML for stable and valid feature transformation, improving downstream performance by 5% and reducing errors by half.", "motivation": "Existing methods (traditional ML and LLMs) fail to address both stable generation and valid generation in feature transformation.", "method": "A four-step framework: golden examples generation, feature transformation sequence embedding/search, student LLM feature transformation, and LLM-ML decoder teaming.", "result": "5% improvement in downstream performance, nearly half the error cases, and demonstrated efficiency/robustness.", "conclusion": "The teaming policy effectively bridges the gap between LLMs and ML, enhancing feature transformation while uncovering LLMs' data understanding capacity."}}
{"id": "2506.09071", "pdf": "https://arxiv.org/pdf/2506.09071", "abs": "https://arxiv.org/abs/2506.09071", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "AI": {"tldr": "SAAF is an automatic segmentation model for building facades using multimodal semantic guidance, combining text and image features for improved accuracy and robustness.", "motivation": "To enhance efficiency in building information models and CAD by automating wall and window segmentation.", "method": "Uses multimodal semantic feature extraction and an end-to-end training framework to map text descriptions to image segmentation.", "result": "Outperforms existing methods in mIoU metric, showing high precision across diverse datasets.", "conclusion": "Advances architectural computer vision and explores multimodal learning applications in architecture."}}
{"id": "2506.09195", "pdf": "https://arxiv.org/pdf/2506.09195", "abs": "https://arxiv.org/abs/2506.09195", "authors": ["Haoran Peng", "Ying-Jun Angela Zhang"], "title": "Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms", "categories": ["eess.SP", "cs.AI", "cs.MA"], "comment": null, "summary": "This research focuses on optimizing multi-UAV systems with dual objectives:\nmaximizing service coverage as the primary goal while extending battery\nlifetime as the secondary objective. We propose a Graph Attention-based\nDecentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed\napproach leverages a graph attention network to process UAVs' limited local\nobservation and reduce the dimension of the environment states. Subsequently,\nan actor-double-critic network is developed to manage dual policies for joint\nobjective optimization. The proposed GADC uses a Kullback-Leibler (KL)\ndivergence factor to balance the tradeoff between coverage performance and\nbattery lifetime in the multi-UAV system. We assess the scalability and\nefficiency of GADC through comprehensive benchmarking against state-of-the-art\nmethods, considering both theory and experimental aspects. Extensive testing in\nboth ideal settings and NVIDIA Sionna's realistic ray tracing environment\ndemonstrates GADC's superior performance.", "AI": {"tldr": "The paper proposes a Graph Attention-based Decentralized Actor-Critic (GADC) method to optimize multi-UAV systems, balancing service coverage and battery lifetime using a KL divergence factor.", "motivation": "To address the dual objectives of maximizing service coverage and extending battery lifetime in multi-UAV systems, requiring efficient decentralized optimization.", "method": "Uses a graph attention network for local observation processing and an actor-double-critic network for dual-policy optimization, incorporating KL divergence for tradeoff balance.", "result": "GADC outperforms state-of-the-art methods in scalability and efficiency, validated in both ideal and realistic ray-tracing environments.", "conclusion": "GADC is a scalable and efficient solution for optimizing multi-UAV systems with dual objectives, demonstrating superior performance in diverse settings."}}
{"id": "2506.09487", "pdf": "https://arxiv.org/pdf/2506.09487", "abs": "https://arxiv.org/abs/2506.09487", "authors": ["Taesoo Park", "Mungwi Jeong", "Mingyu Park", "Narae Kim", "Junyoung Kim", "Mujung Kim", "Jisang Yoo", "Hoyun Lee", "Sanghoon Kim", "Soonchul Kwon"], "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.LO", "eess.AS", "I.2.6; H.5.5; I.5.1"], "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper", "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.", "AI": {"tldr": "BemaGANv2 is an advanced GAN-based vocoder with architectural innovations like AMP modules and MED discriminators for high-fidelity audio generation.", "motivation": "To improve audio generation quality by addressing periodicity modeling and long-range dependencies in GAN-based vocoders.", "method": "Introduces AMP modules in the generator and MED discriminators, combined with MRD, for better audio modeling. Evaluated using objective and subjective metrics.", "result": "Demonstrates improved audio generation quality through systematic evaluation and comparisons of discriminator configurations.", "conclusion": "BemaGANv2 offers a reproducible, high-fidelity solution for audio generation, with code and models publicly available."}}
{"id": "2506.09161", "pdf": "https://arxiv.org/pdf/2506.09161", "abs": "https://arxiv.org/abs/2506.09161", "authors": ["Rajan Das Gupta", "Md Imrul Hasan Showmick", "Mushfiqur Rahman Abir", "Shanjida Akter", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "title": "An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted in MECON 2025", "summary": "Early and accurate detection of brain abnormalities, such as tumors and\nstrokes, is essential for timely intervention and improved patient outcomes. In\nthis study, we present a deep learning-based system capable of identifying both\nbrain tumors and strokes from MRI images, along with their respective stages.\nWe have executed two groundbreaking strategies involving convolutional neural\nnetworks, MobileNet V2 and ResNet-50-optimized through transfer learning to\nclassify MRI scans into five diagnostic categories. Our dataset, aggregated and\naugmented from various publicly available MRI sources, was carefully curated to\nensure class balance and image diversity. To enhance model generalization and\nprevent overfitting, we applied dropout layers and extensive data augmentation.\nThe models achieved strong performance, with training accuracy reaching 93\\%\nand validation accuracy up to 88\\%. While ResNet-50 demonstrated slightly\nbetter results, Mobile Net V2 remains a promising option for real-time\ndiagnosis in low resource settings due to its lightweight architecture. This\nresearch offers a practical AI-driven solution for early brain abnormality\ndetection, with potential for clinical deployment and future enhancement\nthrough larger datasets and multi modal inputs.", "AI": {"tldr": "A deep learning system using MobileNet V2 and ResNet-50 detects brain tumors and strokes from MRI images with high accuracy, offering potential for clinical use.", "motivation": "Early and accurate detection of brain abnormalities like tumors and strokes is crucial for timely intervention and better patient outcomes.", "method": "The study employs convolutional neural networks (MobileNet V2 and ResNet-50) optimized via transfer learning to classify MRI scans into five categories, using a curated dataset with dropout and data augmentation to prevent overfitting.", "result": "The models achieved 93% training and 88% validation accuracy, with ResNet-50 performing slightly better, while MobileNet V2 is suitable for low-resource settings.", "conclusion": "The research provides an AI-driven solution for early brain abnormality detection, with potential for clinical deployment and future improvements."}}
{"id": "2506.09653", "pdf": "https://arxiv.org/pdf/2506.09653", "abs": "https://arxiv.org/abs/2506.09653", "authors": ["Sakshi Joshi", "Eldho Ittan George", "Tahir Javed", "Kaushal Bhogale", "Nikhil Narasimhan", "Mitesh M. Khapra"], "title": "Recognizing Every Voice: Towards Inclusive ASR for Rural Bhojpuri Women", "categories": ["eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Digital inclusion remains a challenge for marginalized communities,\nespecially rural women in low-resource language regions like Bhojpuri.\nVoice-based access to agricultural services, financial transactions, government\nschemes, and healthcare is vital for their empowerment, yet existing ASR\nsystems for this group remain largely untested. To address this gap, we create\nSRUTI ,a benchmark consisting of rural Bhojpuri women speakers. Evaluation of\ncurrent ASR models on SRUTI shows poor performance due to data scarcity, which\nis difficult to overcome due to social and cultural barriers that hinder\nlarge-scale data collection. To overcome this, we propose generating synthetic\nspeech using just 25-30 seconds of audio per speaker from approximately 100\nrural women. Augmenting existing datasets with this synthetic data achieves an\nimprovement of 4.7 WER, providing a scalable, minimally intrusive solution to\nenhance ASR and promote digital inclusion in low-resource language.", "AI": {"tldr": "The paper addresses poor ASR performance for rural Bhojpuri women by creating SRUTI, a benchmark, and using synthetic speech from minimal audio samples to improve accuracy by 4.7 WER.", "motivation": "Digital inclusion for marginalized rural women in low-resource language regions like Bhojpuri is hindered by untested ASR systems.", "method": "Develop SRUTI benchmark, generate synthetic speech from 25-30s audio per speaker (100 women), and augment datasets to improve ASR.", "result": "Synthetic data improves ASR performance by 4.7 WER, offering a scalable solution.", "conclusion": "Minimally intrusive synthetic data generation enhances ASR for low-resource languages, promoting digital inclusion."}}
{"id": "2506.09259", "pdf": "https://arxiv.org/pdf/2506.09259", "abs": "https://arxiv.org/abs/2506.09259", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "AI": {"tldr": "The paper introduces a method to detect and classify prosocial behaviors in online game chats using unsupervised discovery and a novel Self-Anchored Attention Model (SAAM), improving performance by 7.9% over existing techniques.", "motivation": "Prior research focused on toxic content detection, but prosocial communication is equally important for moderation and fostering positive interactions. Limited resources exist for identifying prosocial behaviors in game chats.", "method": "Combined unsupervised discovery with game domain expert collaboration to categorize prosocial behaviors. Proposed SAAM, leveraging the entire training set as anchors to improve performance in low-resource settings.", "result": "SAAM achieved a 7.9% improvement over existing techniques. The method was successfully applied to Call of Duty: Modern Warfare II, demonstrating its effectiveness.", "conclusion": "This research pioneers NLP for prosocial behavior classification in game chats, shifting moderation focus from toxicity to promoting positive interactions."}}
{"id": "2506.09420", "pdf": "https://arxiv.org/pdf/2506.09420", "abs": "https://arxiv.org/abs/2506.09420", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": null, "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "AI": {"tldr": "The paper argues against fully autonomous AI agents, advocating for LLM-based Human-Agent Systems (LLM-HAS) where AI collaborates with humans for better reliability and adaptability.", "motivation": "Current autonomous AI systems lack reliability, transparency, and human understanding, prompting a need for collaborative human-AI systems.", "method": "Proposes LLM-HAS, where humans guide AI, with examples from healthcare, finance, and software development.", "result": "Human-AI teamwork outperforms solo AI in complex tasks, offering more trustworthy and adaptable solutions.", "conclusion": "AI progress should focus on enhancing human capabilities through collaboration, not autonomy."}}
{"id": "2506.09087", "pdf": "https://arxiv.org/pdf/2506.09087", "abs": "https://arxiv.org/abs/2506.09087", "authors": ["Sophie Jaffard", "Giulia Mezzadri", "Patricia Reynaud-Bouret", "Etienne Tanr\u00e9"], "title": "Spiking Neural Models for Decision-Making Tasks with Learning", "categories": ["cs.LG", "math.PR", "q-bio.NC", "stat.ML"], "comment": null, "summary": "In cognition, response times and choices in decision-making tasks are\ncommonly modeled using Drift Diffusion Models (DDMs), which describe the\naccumulation of evidence for a decision as a stochastic process, specifically a\nBrownian motion, with the drift rate reflecting the strength of the evidence.\nIn the same vein, the Poisson counter model describes the accumulation of\nevidence as discrete events whose counts over time are modeled as Poisson\nprocesses, and has a spiking neurons interpretation as these processes are used\nto model neuronal activities. However, these models lack a learning mechanism\nand are limited to tasks where participants have prior knowledge of the\ncategories. To bridge the gap between cognitive and biological models, we\npropose a biologically plausible Spiking Neural Network (SNN) model for\ndecision-making that incorporates a learning mechanism and whose neurons\nactivities are modeled by a multivariate Hawkes process. First, we show a\ncoupling result between the DDM and the Poisson counter model, establishing\nthat these two models provide similar categorizations and reaction times and\nthat the DDM can be approximated by spiking Poisson neurons. To go further, we\nshow that a particular DDM with correlated noise can be derived from a Hawkes\nnetwork of spiking neurons governed by a local learning rule. In addition, we\ndesigned an online categorization task to evaluate the model predictions. This\nwork provides a significant step toward integrating biologically relevant\nneural mechanisms into cognitive models, fostering a deeper understanding of\nthe relationship between neural activity and behavior.", "AI": {"tldr": "The paper proposes a biologically plausible Spiking Neural Network (SNN) model for decision-making, bridging cognitive and biological models by incorporating learning mechanisms and using a multivariate Hawkes process for neuron activity.", "motivation": "To address the lack of learning mechanisms in existing models (DDM and Poisson counter) and integrate biologically relevant neural mechanisms into cognitive models.", "method": "Develops an SNN model with a learning mechanism, models neuron activities using a multivariate Hawkes process, and shows coupling between DDM and Poisson counter models.", "result": "Demonstrates that DDM can be approximated by spiking Poisson neurons and derives a DDM with correlated noise from a Hawkes network.", "conclusion": "The work advances the integration of neural mechanisms into cognitive models, enhancing understanding of neural activity and behavior."}}
{"id": "2506.09079", "pdf": "https://arxiv.org/pdf/2506.09079", "abs": "https://arxiv.org/abs/2506.09079", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "AI": {"tldr": "The paper introduces two datasets (DarkEventInfer, MixVidQA) and a model (VersaVid-R1) to advance video-based reasoning, outperforming existing models.", "motivation": "Video-based reasoning is underdeveloped due to lack of quality data and training methods.", "method": "Created datasets (DarkEventInfer, MixVidQA) and trained VersaVid-R1 using reinforcement learning with diverse rewards.", "result": "VersaVid-R1 outperforms existing models in video understanding, reasoning, and captioning tasks.", "conclusion": "The work bridges the gap in video reasoning, demonstrating superior performance across benchmarks."}}
{"id": "2506.09331", "pdf": "https://arxiv.org/pdf/2506.09331", "abs": "https://arxiv.org/abs/2506.09331", "authors": ["Arjun Vaithilingam Sudhakar"], "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "AI": {"tldr": "The paper explores whether LLMs possess a theory of mind by investigating their ability to infer intentions in cooperative multi-agent reinforcement learning (MARL) settings.", "motivation": "Understanding if LLMs can model others' intentions is crucial for effective collaboration between humans and AI, mirroring human social reasoning.", "method": "The study uses cooperative MARL with LLM-based agents to analyze their ability to infer and reason about intentions through natural language interactions.", "result": "The approach aims to enhance AI agents' adaptability and cooperation with both artificial and human partners.", "conclusion": "The work advances hybrid human-AI systems for seamless collaboration, with significant implications for future human-artificial interactions."}}
{"id": "2506.09709", "pdf": "https://arxiv.org/pdf/2506.09709", "abs": "https://arxiv.org/abs/2506.09709", "authors": ["Alexander Lobashev", "Assel Yermekova", "Maria Larchenko"], "title": "Training-Free Voice Conversion with Factorized Optimal Transport", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "comment": "Interspeech 2025", "summary": "This paper introduces Factorized MKL-VC, a training-free modification for\nkNN-VC pipeline. In contrast with original pipeline, our algorithm performs\nhigh quality any-to-any cross-lingual voice conversion with only 5 second of\nreference audio. MKL-VC replaces kNN regression with a factorized optimal\ntransport map in WavLM embedding subspaces, derived from Monge-Kantorovich\nLinear solution. Factorization addresses non-uniform variance across\ndimensions, ensuring effective feature transformation. Experiments on\nLibriSpeech and FLEURS datasets show MKL-VC significantly improves content\npreservation and robustness with short reference audio, outperforming kNN-VC.\nMKL-VC achieves performance comparable to FACodec, especially in cross-lingual\nvoice conversion domain.", "AI": {"tldr": "Factorized MKL-VC improves cross-lingual voice conversion with 5-second reference audio by replacing kNN regression with a factorized optimal transport map, outperforming kNN-VC and matching FACodec.", "motivation": "To enhance the kNN-VC pipeline for high-quality any-to-any cross-lingual voice conversion with minimal reference audio.", "method": "Replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces, addressing non-uniform variance.", "result": "Significantly improves content preservation and robustness, especially with short reference audio, outperforming kNN-VC and matching FACodec.", "conclusion": "Factorized MKL-VC is effective for cross-lingual voice conversion, offering superior performance with minimal reference audio."}}
{"id": "2506.09162", "pdf": "https://arxiv.org/pdf/2506.09162", "abs": "https://arxiv.org/abs/2506.09162", "authors": ["Tyler J. Richards", "Adam E. Flanders", "Errol Colak", "Luciano M. Prevedello", "Robyn L. Ball", "Felipe Kitamura", "John Mongan", "Maryam Vazirabad", "Hui-Ming Lin", "Anne Kendell", "Thanat Kanthawang", "Salita Angkurawaranon", "Emre Altinmakas", "Hakan Dogan", "Paulo Eduardo de Aguiar Kuriki", "Arjuna Somasundaram", "Christopher Ruston", "Deniz Bulja", "Naida Spahovic", "Jennifer Sommer", "Sirui Jiang", "Eduardo Moreno Judice de Mattos Farina", "Eduardo Caminha Nunes", "Michael Brassil", "Megan McNamara", "Johanna Ortiz", "Jacob Peoples", "Vinson L. Uytana", "Anthony Kam", "Venkata N. S. Dola", "Daniel Murphy", "David Vu", "Dataset Contributor Group", "Dataset Annotator Group", "Competition Data Notebook Group", "Jason F. Talbott"], "title": "The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The Radiological Society of North America (RSNA) Lumbar Degenerative Imaging\nSpine Classification (LumbarDISC) dataset is the largest publicly available\ndataset of adult MRI lumbar spine examinations annotated for degenerative\nchanges. The dataset includes 2,697 patients with a total of 8,593 image series\nfrom 8 institutions across 6 countries and 5 continents. The dataset is\navailable for free for non-commercial use via Kaggle and RSNA Medical Imaging\nResource of AI (MIRA). The dataset was created for the RSNA 2024 Lumbar Spine\nDegenerative Classification competition where competitors developed deep\nlearning models to grade degenerative changes in the lumbar spine. The degree\nof spinal canal, subarticular recess, and neural foraminal stenosis was graded\nat each intervertebral disc level in the lumbar spine. The images were\nannotated by expert volunteer neuroradiologists and musculoskeletal\nradiologists from the RSNA, American Society of Neuroradiology, and the\nAmerican Society of Spine Radiology. This dataset aims to facilitate research\nand development in machine learning and lumbar spine imaging to lead to\nimproved patient care and clinical efficiency.", "AI": {"tldr": "The RSNA LumbarDISC dataset is the largest public MRI lumbar spine dataset, annotated for degenerative changes, used in a 2024 competition to develop deep learning models for grading stenosis.", "motivation": "To advance research in machine learning for lumbar spine imaging, improving patient care and clinical efficiency.", "method": "The dataset includes 2,697 patients with 8,593 MRI series, annotated by expert radiologists for degenerative changes.", "result": "The dataset is publicly available for non-commercial use, fostering AI development in spine imaging.", "conclusion": "LumbarDISC supports AI research to enhance diagnostic accuracy and efficiency in lumbar spine care."}}
{"id": "2506.09707", "pdf": "https://arxiv.org/pdf/2506.09707", "abs": "https://arxiv.org/abs/2506.09707", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "categories": ["eess.AS", "cs.CL", "cs.HC", "68T07", "I.2.7; I.5.4; H.5.2"], "comment": "5 pages, 2 figures", "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.", "AI": {"tldr": "A method for automatic temporal localization of key PE therapy fidelity elements using audio and transcripts, achieving a mean absolute error of 5.3 seconds.", "motivation": "Manual review of PE therapy sessions for fidelity evaluation is labor-intensive; automation can improve scalability and efficiency.", "method": "Fine-tuning Qwen2-Audio with LoRA on 30-second audio-transcript windows, using LLM-based prompting for fidelity labels.", "result": "Best configuration (LoRA rank 8, 30s windows) achieves MAE of 5.3 seconds on 313 PE sessions.", "conclusion": "The framework offers scalable fidelity tracking for PE therapy, aiding clinician training and quality assurance."}}
{"id": "2506.09277", "pdf": "https://arxiv.org/pdf/2506.09277", "abs": "https://arxiv.org/abs/2506.09277", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE.", "AI": {"tldr": "A framework for measuring LLM-generated self-NLE faithfulness by comparing explanations with internal hidden states.", "motivation": "Existing methods for assessing self-NLE faithfulness lack examination of neural activity, leading to unfaithful explanations.", "method": "Proposes a flexible framework comparing self-NLE with interpretations of the model's internal hidden states.", "result": "Provides deep insights into self-NLE faithfulness and connects explanations to model reasoning.", "conclusion": "Advances understanding of self-NLE faithfulness and aids in generating more faithful explanations."}}
{"id": "2506.09498", "pdf": "https://arxiv.org/pdf/2506.09498", "abs": "https://arxiv.org/abs/2506.09498", "authors": ["Jaesik Yoon", "Hyeonseo Cho", "Yoshua Bengio", "Sungjin Ahn"], "title": "Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning", "categories": ["cs.AI"], "comment": null, "summary": "Diffusion models have recently emerged as a powerful approach for trajectory\nplanning. However, their inherently non-sequential nature limits their\neffectiveness in long-horizon reasoning tasks at test time. The recently\nproposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by\ncombining diffusion with tree-based search, achieving state-of-the-art\nperformance on complex planning problems. Despite its strengths, our analysis\nshows that MCTD incurs substantial computational overhead due to the sequential\nnature of tree search and the cost of iterative denoising. To address this, we\npropose Fast-MCTD, a more efficient variant that preserves the strengths of\nMCTD while significantly improving its speed and scalability. Fast-MCTD\nintegrates two techniques: Parallel MCTD, which enables parallel rollouts via\ndelayed tree updates and redundancy-aware selection; and Sparse MCTD, which\nreduces rollout length through trajectory coarsening. Experiments show that\nFast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or\nimproving planning performance. Remarkably, it even outperforms Diffuser in\ninference speed on some tasks, despite Diffuser requiring no search and\nyielding weaker solutions. These results position Fast-MCTD as a practical and\nscalable solution for diffusion-based inference-time reasoning.", "AI": {"tldr": "Fast-MCTD improves MCTD by enabling parallel rollouts and reducing rollout length, achieving up to 100x speedup while maintaining performance.", "motivation": "MCTD's computational overhead limits its practicality due to sequential tree search and iterative denoising.", "method": "Fast-MCTD integrates Parallel MCTD (parallel rollouts with delayed updates) and Sparse MCTD (trajectory coarsening).", "result": "Fast-MCTD achieves up to 100x speedup over MCTD, outperforming Diffuser in speed while maintaining or improving planning performance.", "conclusion": "Fast-MCTD is a scalable and efficient solution for diffusion-based inference-time reasoning."}}
{"id": "2506.09090", "pdf": "https://arxiv.org/pdf/2506.09090", "abs": "https://arxiv.org/abs/2506.09090", "authors": ["Arthur Oghlukyan", "Nuria Gomez Blas"], "title": "Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a comprehensive analysis of an enhanced asynchronous\nAdaBoost framework for federated learning (FL), focusing on its application\nacross five distinct domains: computer vision on edge devices, blockchain-based\nmodel transparency, on-device mobile personalization, IoT anomaly detection,\nand federated healthcare diagnostics. The proposed algorithm incorporates\nadaptive communication scheduling and delayed weight compensation to reduce\nsynchronization frequency and communication overhead while preserving or\nimproving model accuracy. We examine how these innovations improve\ncommunication efficiency, scalability, convergence, and robustness in each\ndomain. Comparative metrics including training time, communication overhead,\nconvergence iterations, and classification accuracy are evaluated using data\nand estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical\nresults show, for example, training time reductions on the order of 20-35% and\ncommunication overhead reductions of 30-40% compared to baseline AdaBoost, with\nconvergence achieved in significantly fewer boosting rounds. Tables and charts\nsummarize these improvements by domain. Mathematical formulations of the\nadaptive scheduling rule and error-driven synchronization thresholds are\nprovided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency\nand robustness across diverse FL scenarios, suggesting broad applicability of\nthe approach.", "AI": {"tldr": "An enhanced asynchronous AdaBoost framework for federated learning (FL) improves efficiency and robustness across five domains, reducing training time by 20-35% and communication overhead by 30-40% compared to baseline.", "motivation": "To address synchronization and communication inefficiencies in federated learning while maintaining or improving model accuracy.", "method": "Incorporates adaptive communication scheduling and delayed weight compensation to reduce synchronization frequency and overhead.", "result": "Achieves significant reductions in training time (20-35%) and communication overhead (30-40%), with faster convergence and maintained accuracy.", "conclusion": "The enhanced AdaBoost framework is highly efficient and robust, demonstrating broad applicability in diverse FL scenarios."}}
{"id": "2506.09081", "pdf": "https://arxiv.org/pdf/2506.09081", "abs": "https://arxiv.org/abs/2506.09081", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "AI": {"tldr": "FlagEvalMM is an open-source framework for evaluating multimodal models on vision-language tasks, offering flexibility, efficiency, and accurate insights.", "motivation": "To address the need for a comprehensive and efficient evaluation framework for multimodal models across diverse tasks.", "method": "Decouples model inference from evaluation, uses advanced tools (e.g., vLLM, SGLang) and asynchronous data loading for efficiency.", "result": "FlagEvalMM provides accurate and efficient evaluation, highlighting model strengths and limitations.", "conclusion": "FlagEvalMM is a valuable tool for advancing multimodal research, publicly available on GitHub."}}
{"id": "2506.09513", "pdf": "https://arxiv.org/pdf/2506.09513", "abs": "https://arxiv.org/abs/2506.09513", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "24 pages, 6 figures, 7 tables", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "AI": {"tldr": "ReasonMed introduces a large medical reasoning dataset and a refined training strategy, achieving state-of-the-art performance in medical QA.", "motivation": "To explore and enhance LLMs' capabilities in knowledge-intensive medical question answering, which remains underexplored despite their success in other domains.", "method": "Developed ReasonMed, a 370k-example dataset, using a multi-agent verification and refinement process with an Error Refiner. Combined detailed Chain-of-Thought reasoning with concise answer summaries for fine-tuning.", "result": "ReasonMed-7B outperforms prior sub-10B models by 4.17% and exceeds LLaMA3.1-70B on PubMedQA by 4.60%.", "conclusion": "The approach sets a new benchmark for medical reasoning models, demonstrating the effectiveness of refined datasets and strategic fine-tuning."}}
{"id": "2506.09874", "pdf": "https://arxiv.org/pdf/2506.09874", "abs": "https://arxiv.org/abs/2506.09874", "authors": ["Neta Glazer", "Aviv Navon", "Yael Segal", "Aviv Shamsian", "Hilit Segev", "Asaf Buchnick", "Menachem Pirchi", "Gil Hetz", "Joseph Keshet"], "title": "UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Recent advances in Text-to-Speech (TTS) have enabled highly natural speech\nsynthesis, yet integrating speech with complex background environments remains\nchallenging. We introduce UmbraTTS, a flow-matching based TTS model that\njointly generates both speech and environmental audio, conditioned on text and\nacoustic context. Our model allows fine-grained control over background volume\nand produces diverse, coherent, and context-aware audio scenes. A key challenge\nis the lack of data with speech and background audio aligned in natural\ncontext. To overcome the lack of paired training data, we propose a\nself-supervised framework that extracts speech, background audio, and\ntranscripts from unannotated recordings. Extensive evaluations demonstrate that\nUmbraTTS significantly outperformed existing baselines, producing natural,\nhigh-quality, environmentally aware audios.", "AI": {"tldr": "UmbraTTS is a flow-matching TTS model that generates speech and environmental audio together, addressing the lack of paired data with a self-supervised framework.", "motivation": "Integrating speech with complex background environments is challenging due to the lack of aligned data.", "method": "Proposes UmbraTTS, a flow-matching model, and a self-supervised framework to extract speech, background, and transcripts from unannotated recordings.", "result": "UmbraTTS outperforms baselines, producing natural, high-quality, environmentally aware audio.", "conclusion": "The model enables fine-grained control and diverse, context-aware audio scenes, advancing TTS in complex environments."}}
{"id": "2506.09377", "pdf": "https://arxiv.org/pdf/2506.09377", "abs": "https://arxiv.org/abs/2506.09377", "authors": ["Chenwei Wang", "Renjie Xu", "Congwen Wu", "Cunyi Yin", "Ziyun Liao", "Deqing Mao", "Sitong Zhang", "Hong Yan"], "title": "An Interpretable Two-Stage Feature Decomposition Method for Deep Learning-based SAR ATR", "categories": ["eess.IV"], "comment": null, "summary": "Synthetic aperture radar automatic target recognition (SAR ATR) has seen\nsignificant performance improvements with deep learning. However, the black-box\nnature of deep SAR ATR introduces low confidence and high risks in\ndecision-critical SAR applications, hindering practical deployment. To address\nthis issue, deep SAR ATR should provide an interpretable reasoning basis $r_b$\nand logic $\\lambda_w$, forming the reasoning logic $\\sum_{i} {{r_b^i} \\times\n{\\lambda_w^i}} =pred$ behind the decisions. Therefore, this paper proposes a\nphysics-based two-stage feature decomposition method for interpretable deep SAR\nATR, which transforms uninterpretable deep features into attribute scattering\ncenter components (ASCC) with clear physical meanings. First, ASCCs are\nobtained through a clustering algorithm. To extract independent physical\ncomponents from deep features, we propose a two-stage decomposition method. In\nthe first stage, a feature decoupling and discrimination module separates deep\nfeatures into approximate ASCCs with global discriminability. In the second\nstage, a multilayer orthogonal non-negative matrix tri-factorization (MLO-NMTF)\nfurther decomposes the ASCCs into independent components with distinct physical\nmeanings. The MLO-NMTF elegantly aligns with the clustering algorithms to\nobtain ASCCs. Finally, this method ensures both an interpretable reasoning\nprocess and accurate recognition results. Extensive experiments on four\nbenchmark datasets confirm its effectiveness, showcasing the method's\ninterpretability, robust recognition performance, and strong generalization\ncapability.", "AI": {"tldr": "The paper proposes a physics-based two-stage feature decomposition method for interpretable deep SAR ATR, transforming deep features into interpretable attribute scattering center components (ASCC).", "motivation": "Deep SAR ATR's black-box nature reduces confidence in decision-critical applications, necessitating interpretable reasoning logic.", "method": "A two-stage decomposition: (1) feature decoupling and discrimination for global discriminability, and (2) MLO-NMTF for independent physical components.", "result": "Experiments on four datasets confirm the method's interpretability, robust recognition, and generalization.", "conclusion": "The method provides interpretable reasoning and accurate recognition, addressing the black-box issue in SAR ATR."}}
{"id": "2506.09804", "pdf": "https://arxiv.org/pdf/2506.09804", "abs": "https://arxiv.org/abs/2506.09804", "authors": ["Peter Vieting", "Maximilian Kannen", "Benedikt Hilmes", "Ralf Schl\u00fcter", "Hermann Ney"], "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features.", "AI": {"tldr": "The paper explores regularization methods to improve neural front-ends for ASR, addressing overfitting and outperforming traditional features.", "motivation": "Neural front-ends for ASR often underperform due to overfitting, prompting investigation into better regularization techniques.", "method": "Examines audio perturbation and proposes STFT-domain masking to enhance SpecAugment for learnable front-ends.", "result": "Combining these methods closes the performance gap between traditional and learnable features.", "conclusion": "Effective regularization can make neural front-ends competitive with classical ASR feature extraction."}}
{"id": "2506.09301", "pdf": "https://arxiv.org/pdf/2506.09301", "abs": "https://arxiv.org/abs/2506.09301", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "AI": {"tldr": "The paper introduces $(RSA)^2$, a framework for interpreting figurative language by modeling rhetorical strategies, achieving state-of-the-art performance on irony interpretation.", "motivation": "Existing RSA frameworks struggle with figurative language due to the need for setting-specific motivation modeling.", "method": "The $(RSA)^2$ framework models rhetorical strategies instead of speaker motivations, combined with LLMs.", "result": "Achieves state-of-the-art performance on the PragMega+ irony dataset.", "conclusion": "$(RSA)^2$ effectively interprets figurative language without requiring explicit motivation modeling."}}
{"id": "2506.09655", "pdf": "https://arxiv.org/pdf/2506.09655", "abs": "https://arxiv.org/abs/2506.09655", "authors": ["Kaixuan Xu", "Jiajun Chai", "Sicheng Li", "Yuqian Fu", "Yuanheng Zhu", "Dongbin Zhao"], "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games.", "AI": {"tldr": "DipLLM, a fine-tuned LLM-based agent, simplifies Diplomacy's complex action space using autoregressive factorization, achieving superior performance with minimal data compared to traditional methods.", "motivation": "Traditional AI methods for Diplomacy require extensive computational resources due to equilibrium search. LLMs offer a resource-efficient alternative but face challenges in handling the game's complexity.", "method": "DipLLM uses autoregressive factorization to break multi-unit action assignment into unit-level decisions, fine-tuning an LLM with minimal data to learn equilibrium policies.", "result": "DipLLM outperforms the state-of-the-art Cicero model using only 1.5% of its required data.", "conclusion": "Fine-tuned LLMs like DipLLM show promise for complex strategic decision-making in multiplayer games, offering efficiency and performance gains."}}
{"id": "2506.09091", "pdf": "https://arxiv.org/pdf/2506.09091", "abs": "https://arxiv.org/abs/2506.09091", "authors": ["Kenric Nelson", "Igor Oliveira", "Amenah Al-Najafi", "Fode Zhang", "Hon Keung Tony Ng"], "title": "Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "11 pages, 2 figures, AGI-25", "summary": "We introduce an optimization framework for variational inference based on the\ncoupled free energy, extending variational inference techniques to account for\nthe curved geometry of the coupled exponential family. This family includes\nimportant heavy-tailed distributions such as the generalized Pareto and the\nStudent's t. By leveraging the coupled free energy, which is equal to the\ncoupled evidence lower bound (ELBO) of the inverted probabilities, we improve\nthe accuracy and robustness of the learned model. The coupled generalization of\nFisher Information metric and the affine connection. The method is applied to\nthe design of a coupled variational autoencoder (CVAE). By using the coupling\nfor both the distributions and cost functions, the reconstruction metric is\nderived to still be the mean-square average loss with modified constants. The\nnovelty comes from sampling the heavy-tailed latent distribution with its\nassociated coupled probability, which has faster decaying tails. The result is\nthe ability to train a model with high penalties in the tails, while assuring\nthat the training samples have a reduced number of outliers. The Wasserstein-2\nor Fr\\'echet Inception Distance of the reconstructed CelebA images shows the\nCVAE has a 3\\% improvement over the VAE after 5 epochs of training.", "AI": {"tldr": "The paper introduces a coupled free energy framework for variational inference, improving accuracy and robustness by accounting for heavy-tailed distributions like generalized Pareto and Student's t. Applied to a coupled variational autoencoder (CVAE), it shows a 3% improvement over standard VAE.", "motivation": "To extend variational inference techniques to handle heavy-tailed distributions and improve model robustness by leveraging coupled free energy.", "method": "Uses coupled free energy and ELBO for variational inference, generalizes Fisher Information metric, and designs a CVAE with modified reconstruction metrics.", "result": "The CVAE achieves a 3% improvement in Wasserstein-2 or Fr\u00e9chet Inception Distance over standard VAE on CelebA images after 5 epochs.", "conclusion": "The framework successfully enhances variational inference for heavy-tailed distributions, demonstrating practical benefits in model training and outlier reduction."}}
{"id": "2506.09082", "pdf": "https://arxiv.org/pdf/2506.09082", "abs": "https://arxiv.org/abs/2506.09082", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "First two authors contribute equally", "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "AI": {"tldr": "AVA-Bench is introduced to evaluate vision foundation models (VFMs) by disentangling 14 Atomic Visual Abilities (AVAs), addressing gaps in current VQA benchmarks.", "motivation": "Current VQA benchmarks have blind spots: misalignment of instruction tuning data and inability to pinpoint specific visual shortcomings.", "method": "AVA-Bench decouples 14 AVAs, ensuring matched training and test distributions for precise evaluation.", "result": "AVA-Bench reveals distinct ability fingerprints of VFMs and shows a 0.5B LLM is as effective as a 7B LLM but more efficient.", "conclusion": "AVA-Bench provides a transparent and comprehensive benchmark to guide future VFM development."}}
{"id": "2506.09789", "pdf": "https://arxiv.org/pdf/2506.09789", "abs": "https://arxiv.org/abs/2506.09789", "authors": ["Davide Grossi", "Andreas Nitsche"], "title": "Delegations as Adaptive Representation Patterns: Rethinking Influence in Liquid Democracy", "categories": ["cs.CY", "cs.MA"], "comment": null, "summary": "Liquid democracy is a mechanism for the division of labor in decision-making\nthrough the transitive delegation of influence. In essence, all individuals\npossess the autonomy to determine the issues with which they will engage\ndirectly, while for other matters, they may appoint a representative of their\nchoosing. So far, the literature has studied the delegation structures emerging\nin liquid democracy as static. As a result, transitivity defined as the\ncapacity to transfer acquired authority to another entity, has been identified\nas a concern as it would be conducive to unrestrained accumulation of power.\n  Focusing on the implementation of liquid democracy supported by the\nLiquidFeedback software, we propose a novel approach to assessing the influence\nof voting nodes in a transitive delegation graph, taking into account the\nprocess nature of real-world liquid democracy in which delegation and voting\nare distinct and increasingly independent activities. By introducing a novel\nmodel of delegations in liquid democracy, we show how transitivity may in fact\ncontribute to an effective regulation of deliberation influence and\ndecision-making power. While maintaining the one-person, one-vote paradigm for\nall votes cast, the anticipated influence of an agent, to the extent it is\nstemming from transitivity, experiences a precipitous decline following an\nexponential trajectory.\n  In general, it is our objective to move the first steps towards a rigorous\nanalysis of liquid democracy as an adaptive democratic representation process.\nThe adaptivity aspect of liquid democracy has not yet been explored within the\nexisting academic literature despite it being, we believe, one of its most\nimportant features. We therefore also outline a research agenda focusing on\nthis aspect of liquid democracy.", "AI": {"tldr": "The paper explores liquid democracy, focusing on transitivity in delegation and its impact on power distribution. It proposes a new model to assess influence in delegation graphs, showing transitivity can regulate power effectively.", "motivation": "To address the static view of delegation in liquid democracy and the concern of power accumulation due to transitivity, the study aims to analyze it as an adaptive process.", "method": "The paper introduces a novel model for assessing influence in transitive delegation graphs, using the LiquidFeedback software as a case study.", "result": "Transitivity, when modeled dynamically, can regulate influence and decision-making power, with influence declining exponentially.", "conclusion": "The study advocates for analyzing liquid democracy as an adaptive process and outlines a research agenda to explore its adaptivity further."}}
{"id": "2506.09375", "pdf": "https://arxiv.org/pdf/2506.09375", "abs": "https://arxiv.org/abs/2506.09375", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "AI": {"tldr": "CoLMbo is a Speaker Language Model (SLM) that generates detailed speaker descriptions, addressing limitations of traditional speaker recognition systems by using prompt-based conditioning.", "motivation": "Traditional speaker recognition systems lack the ability to provide detailed speaker characteristics like dialect, gender, and age. CoLMbo aims to overcome this by integrating a speaker encoder with prompt-based conditioning.", "method": "CoLMbo combines a speaker encoder with prompt-based conditioning to dynamically generate detailed captions about speaker attributes, adapting to user-defined prompts.", "result": "The model excels in zero-shot scenarios and provides customized descriptions, including dialect and age-related traits, enhancing speaker profiling.", "conclusion": "CoLMbo represents a significant advancement in speaker recognition by enabling context-rich, detailed speaker descriptions beyond traditional classification tasks."}}
{"id": "2506.09661", "pdf": "https://arxiv.org/pdf/2506.09661", "abs": "https://arxiv.org/abs/2506.09661", "authors": ["Garima Jain", "Sanghamitra Pati", "Mona Duggal", "Amit Sethi", "Abhijeet Patil", "Gururaj Malekar", "Nilesh Kowe", "Jitender Kumar", "Jatin Kashyap", "Divyajeet Rout", "Deepali", "Hitesh", "Nishi Halduniya", "Sharat Kumar", "Heena Tabassum", "Rupinder Singh Dhaliwal", "Sucheta Devi Khuraijam", "Sushma Khuraijam", "Sharmila Laishram", "Simmi Kharb", "Sunita Singh", "K. Swaminadtan", "Ranjana Solanki", "Deepika Hemranjani", "Shashank Nath Singh", "Uma Handa", "Manveen Kaur", "Surinder Singhal", "Shivani Kalhan", "Rakesh Kumar Gupta", "Ravi. S", "D. Pavithra", "Sunil Kumar Mahto", "Arvind Kumar", "Deepali Tirkey", "Saurav Banerjee", "L. Sreelakshmi"], "title": "A Cytology Dataset for Early Detection of Oral Squamous Cell Carcinoma", "categories": ["eess.IV", "cs.CV", "q-bio.TO"], "comment": "7 pages, 2 figurs", "summary": "Oral squamous cell carcinoma OSCC is a major global health burden,\nparticularly in several regions across Asia, Africa, and South America, where\nit accounts for a significant proportion of cancer cases. Early detection\ndramatically improves outcomes, with stage I cancers achieving up to 90 percent\nsurvival. However, traditional diagnosis based on histopathology has limited\naccessibility in low-resource settings because it is invasive,\nresource-intensive, and reliant on expert pathologists. On the other hand, oral\ncytology of brush biopsy offers a minimally invasive and lower cost\nalternative, provided that the remaining challenges, inter observer variability\nand unavailability of expert pathologists can be addressed using artificial\nintelligence. Development and validation of robust AI solutions requires access\nto large, labeled, and multi-source datasets to train high capacity models that\ngeneralize across domain shifts. We introduce the first large and multicenter\noral cytology dataset, comprising annotated slides stained with\nPapanicolaou(PAP) and May-Grunwald-Giemsa(MGG) protocols, collected from ten\ntertiary medical centers in India. The dataset is labeled and annotated by\nexpert pathologists for cellular anomaly classification and detection, is\ndesigned to advance AI driven diagnostic methods. By filling the gap in\npublicly available oral cytology datasets, this resource aims to enhance\nautomated detection, reduce diagnostic errors, and improve early OSCC diagnosis\nin resource-constrained settings, ultimately contributing to reduced mortality\nand better patient outcomes worldwide.", "AI": {"tldr": "A large, multicenter oral cytology dataset is introduced to improve AI-driven early detection of oral squamous cell carcinoma (OSCC), addressing challenges in low-resource settings.", "motivation": "Early detection of OSCC improves survival, but traditional histopathology is invasive and resource-intensive. Oral cytology offers a cheaper, less invasive alternative, but needs AI to overcome variability and lack of experts.", "method": "A labeled, multi-source dataset of oral cytology slides (PAP and MGG stains) from ten Indian medical centers was created for AI training.", "result": "The dataset enables robust AI models for cellular anomaly detection, aiming to reduce diagnostic errors and improve early OSCC diagnosis.", "conclusion": "This resource fills a gap in public datasets, potentially lowering mortality and improving outcomes in resource-limited areas."}}
{"id": "2506.09315", "pdf": "https://arxiv.org/pdf/2506.09315", "abs": "https://arxiv.org/abs/2506.09315", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "The paper improves Alzheimer's dementia (AD) detection using Mistral-7B, boosting accuracy by 3.33% over existing methods and offering interpretable decision boundaries.", "motivation": "To enhance AD detection accuracy and provide clearer decision-making processes compared to opaque existing methods.", "method": "Extends the paired perplexity approach using Mistral-7B, fine-tunes the model, and analyzes its interpretability and language pattern learning.", "result": "Achieves 3.33% higher accuracy than current methods and 6.35% over ADReSS 2020 benchmarks, with interpretable decision boundaries.", "conclusion": "The approach effectively detects AD, learns AD language patterns, and opens avenues for model interpretation and data augmentation."}}
{"id": "2506.09656", "pdf": "https://arxiv.org/pdf/2506.09656", "abs": "https://arxiv.org/abs/2506.09656", "authors": ["Wei Zeng", "Hengshu Zhu", "Chuan Qin", "Han Wu", "Yihang Cheng", "Sirui Zhang", "Xiaowei Jin", "Yinuo Shen", "Zhenxing Wang", "Feimin Zhong", "Hui Xiong"], "title": "Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives", "categories": ["cs.AI"], "comment": null, "summary": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field.", "AI": {"tldr": "The paper reviews value alignment in AI agent systems, organizing value principles hierarchically, categorizing application scenarios, and evaluating alignment methods, while proposing future research directions.", "motivation": "The shift to multi-agent AI systems and the complexity of LLM applications raise situational risks, necessitating alignment of agent goals with human values.", "method": "The paper hierarchically organizes value principles, categorizes agent system scenarios, and systematically evaluates alignment datasets and methods.", "result": "A comprehensive review of value alignment in agent systems, including hierarchical value principles, scenario categorization, and alignment evaluation.", "conclusion": "The paper highlights the importance of value alignment in AI agents and suggests future research directions to address emerging challenges."}}
{"id": "2506.09092", "pdf": "https://arxiv.org/pdf/2506.09092", "abs": "https://arxiv.org/abs/2506.09092", "authors": ["Wentao Chen", "Jiace Zhu", "Qi Fan", "Yehan Ma", "An Zou"], "title": "CUDA-LLM: LLMs Can Write Efficient CUDA Kernels", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ngeneral-purpose code generation. However, generating the code which is deeply\nhardware-specific, architecture-aware, and performance-critical, especially for\nmassively parallel GPUs, remains a complex challenge. In this work, we explore\nthe use of LLMs for the automated generation and optimization of CUDA programs,\nwith the goal of producing high-performance GPU kernels that fully exploit the\nunderlying hardware. To address this challenge, we propose a novel framework\ncalled \\textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes\ncompilation and functional correctness, as well as the runtime performance,\nwhich are validated through extensive and diverse test cases, and measured by\nactual kernel execution latency on the target GPU, respectively. This approach\nenables LLMs not only to generate syntactically and semantically correct CUDA\ncode but also to iteratively refine it for efficiency, tailored to the\ncharacteristics of the GPU architecture. We evaluate FSR on representative CUDA\nkernels, covering AI workloads and computational intensive algorithms. Our\nresults show that LLMs augmented with FSR consistently guarantee correctness\nrates. Meanwhile, the automatically generated kernels can outperform general\nhuman-written code by a factor of up to 179$\\times$ in execution speeds. These\nfindings highlight the potential of combining LLMs with performance\nreinforcement to automate GPU programming for hardware-specific,\narchitecture-sensitive, and performance-critical applications.", "AI": {"tldr": "LLMs augmented with FSR framework generate and optimize CUDA programs, achieving high-performance GPU kernels with up to 179x speedup over human-written code.", "motivation": "Addressing the challenge of generating hardware-specific, architecture-aware, and performance-critical GPU code using LLMs.", "method": "Proposed Feature Search and Reinforcement (FSR) framework to jointly optimize compilation, correctness, and runtime performance.", "result": "FSR ensures correctness and achieves up to 179x speedup in execution speeds compared to human-written code.", "conclusion": "Combining LLMs with performance reinforcement like FSR can automate GPU programming for high-performance applications."}}
{"id": "2506.09083", "pdf": "https://arxiv.org/pdf/2506.09083", "abs": "https://arxiv.org/abs/2506.09083", "authors": ["Jerry Lin", "Partick P. W. Chen"], "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "categories": ["cs.CV", "cs.AI"], "comment": "4 pages, 3 figures, 1 Table", "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "AI": {"tldr": "BakuFlow is a semi-automatic labeling tool for computer vision, enhancing efficiency with features like pixel-precise corrections, data augmentation, label propagation, and an adaptable YOLOE-based auto-labeling module.", "motivation": "Manual data labeling is slow and error-prone, especially for large-scale tasks, necessitating more efficient tools.", "method": "BakuFlow integrates a live magnifier, interactive data augmentation, label propagation for videos, and a modified YOLOE framework for flexible auto-labeling.", "result": "The tool reduces labeling workload and improves efficiency, particularly for object detection and tracking.", "conclusion": "BakuFlow is effective for dynamic, real-world datasets, streamlining annotation in computer vision and industrial applications."}}
{"id": "2411.00570", "pdf": "https://arxiv.org/pdf/2411.00570", "abs": "https://arxiv.org/abs/2411.00570", "authors": ["Julian Heinovski", "Do\u011fanalp Ergen\u00e7", "Kirsten Thommes", "Falko Dressler"], "title": "Incentive-based Platoon Formation: Optimizing the Personal Benefit for Drivers", "categories": ["cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "Platooning or cooperative adaptive cruise control (CACC) has been\ninvestigated for decades, but debate about its lasting impact is still ongoing.\nWhile the benefits of platooning and the formation of platoons are well\nunderstood for trucks, they are less clear for passenger cars, which have a\nhigher heterogeneity in trips and drivers' preferences. Most importantly, it\nremains unclear how to form platoons of passenger cars in order to optimize the\npersonal benefit for the individual driver. To this end, in this paper, we\npropose a novel platoon formation algorithm that optimizes the personal benefit\nfor drivers of individual passenger cars. For computing vehicle-to-platoon\nassignments, the algorithm utilizes a new metric that we propose to evaluate\nthe personal benefits of various driving systems, including platooning. By\ncombining fuel and travel time costs into a single monetary value, drivers can\nestimate overall trip costs according to a personal monetary value for time\nspent. This provides an intuitive way for drivers to understand and compare the\nbenefits of driving systems like human driving, adaptive cruise control (ACC),\nand, of course, platooning. Unlike previous similarity-based methods, our\nproposed algorithm forms platoons only when beneficial for the driver, rather\nthan solely for platooning. We demonstrate the new metric for the total trip\ncost in a numerical analysis and explain its interpretation. Results of a\nlarge-scale simulation study demonstrate that our proposed platoon formation\nalgorithm outperforms normal ACC as well as previous similarity-based\nplatooning approaches by balancing fuel savings and travel time, independent of\ntraffic and drivers' time cost.", "AI": {"tldr": "A novel platoon formation algorithm optimizes personal benefits for passenger car drivers by balancing fuel savings and travel time, outperforming traditional methods.", "motivation": "The impact of platooning for passenger cars is unclear due to trip and driver heterogeneity. The paper aims to optimize personal benefits for individual drivers.", "method": "Proposes a new metric combining fuel and travel time costs into a single monetary value, and a platoon formation algorithm prioritizing driver benefits.", "result": "The algorithm outperforms adaptive cruise control and similarity-based platooning, balancing fuel savings and travel time.", "conclusion": "The approach provides an intuitive way for drivers to evaluate and benefit from platooning, independent of traffic or time cost."}}
{"id": "2506.09984", "pdf": "https://arxiv.org/pdf/2506.09984", "abs": "https://arxiv.org/abs/2506.09984", "authors": ["Zhenzhi Wang", "Jiaqi Yang", "Jianwen Jiang", "Chao Liang", "Gaojie Lin", "Zerong Zheng", "Ceyuan Yang", "Dahua Lin"], "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions", "categories": ["cs.CV", "cs.AI", "cs.SD"], "comment": "TL;DR: The first multi-person dialogue video generation method from\n  pairs of reference image and audio via explicit layout-aligned condition\n  injection. See project page https://zhenzhiwang.github.io/interacthuman/ for\n  more details", "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.", "AI": {"tldr": "A novel framework for multi-concept human animation with precise, region-specific control of conditions from text, image, and audio, addressing limitations of single-entity methods.", "motivation": "Existing methods for human animation lack precise control for multiple concepts and interactions, limiting applications. This work aims to enable high-quality, controllable multi-concept videos.", "method": "The framework uses a mask predictor to infer layout information from reference images, ensuring region-specific binding of conditions. Local audio conditions are injected iteratively for alignment.", "result": "The method achieves high-quality generation of multi-concept human-centric videos with explicit layout control, outperforming implicit and existing methods.", "conclusion": "The proposed framework effectively addresses the limitations of single-entity assumptions, enabling precise control and high-quality results for multi-concept human animation."}}
{"id": "2506.09949", "pdf": "https://arxiv.org/pdf/2506.09949", "abs": "https://arxiv.org/abs/2506.09949", "authors": ["Mahrokh Najaf", "Gregory Ongie"], "title": "Sampling Theory for Super-Resolution with Implicit Neural Representations", "categories": ["eess.IV", "cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2405.18410", "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\nsolving inverse problems in computer vision and computational imaging. INRs\nrepresent images as continuous domain functions realized by a neural network\ntaking spatial coordinates as inputs. However, unlike traditional pixel\nrepresentations, little is known about the sample complexity of estimating\nimages using INRs in the context of linear inverse problems. Towards this end,\nwe study the sampling requirements for recovery of a continuous domain image\nfrom its low-pass Fourier samples by fitting a single hidden-layer INR with\nReLU activation and a Fourier features layer using a generalized form of weight\ndecay regularization. Our key insight is to relate minimizers of this\nnon-convex parameter space optimization problem to minimizers of a convex\npenalty defined over an infinite-dimensional space of measures. We identify a\nsufficient number of Fourier samples for which an image realized by an INR is\nexactly recoverable by solving the INR training problem. To validate our\ntheory, we empirically assess the probability of achieving exact recovery of\nimages realized by low-width single hidden-layer INRs, and illustrate the\nperformance of INRs on super-resolution recovery of continuous domain phantom\nimages.", "AI": {"tldr": "The paper explores the sample complexity of estimating images using implicit neural representations (INRs) for solving linear inverse problems, focusing on recovery from low-pass Fourier samples.", "motivation": "To understand the sampling requirements for image recovery using INRs, which are less studied compared to traditional pixel representations.", "method": "Uses a single hidden-layer INR with ReLU activation and Fourier features, employing generalized weight decay regularization. Links non-convex optimization to convex penalties in infinite-dimensional spaces.", "result": "Identifies sufficient Fourier samples for exact INR-based image recovery and validates theory with empirical tests on low-width INRs and super-resolution tasks.", "conclusion": "INRs can achieve exact image recovery under certain conditions, demonstrating their potential for inverse problems in imaging."}}
{"id": "2402.03710", "pdf": "https://arxiv.org/pdf/2402.03710", "abs": "https://arxiv.org/abs/2402.03710", "authors": ["Xilin Jiang", "Cong Han", "Yinghao Aaron Li", "Nima Mesgarani"], "title": "Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced Auditory Experience", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by IEEE Journal of Selected Topics in Signal Processing\n  (JSTSP)", "summary": "In daily life, we encounter a variety of sounds, both desirable and\nundesirable, with limited control over their presence and volume. Our work\nintroduces \"Listen, Chat, and Remix\" (LCR), a novel multimodal sound remixer\nthat controls each sound source in a mixture based on user-provided text\ninstructions. LCR distinguishes itself with a user-friendly text interface and\nits unique ability to remix multiple sound sources simultaneously within a\nmixture, without needing to separate them. Users input open-vocabulary text\nprompts, which are interpreted by a large language model to create a semantic\nfilter for remixing the sound mixture. The system then decomposes the mixture\ninto its components, applies the semantic filter, and reassembles filtered\ncomponents back to the desired output. We developed a 160-hour dataset with\nover 100k mixtures, including speech and various audio sources, along with text\nprompts for diverse remixing tasks including extraction, removal, and volume\ncontrol of single or multiple sources. Our experiments demonstrate significant\nimprovements in signal quality across all remixing tasks and robust performance\nin zero-shot scenarios with varying numbers and types of sound sources. An\naudio demo is available at: https://listenchatremix.github.io/demo.", "AI": {"tldr": "LCR is a multimodal sound remixer that uses text instructions to control sound sources in a mixture without separation.", "motivation": "Limited control over sound presence and volume in daily life drives the need for a user-friendly, text-based sound remixer.", "method": "LCR uses a large language model to interpret text prompts, decomposes mixtures, applies semantic filters, and reassembles components.", "result": "Improved signal quality in remixing tasks and robust zero-shot performance with diverse sound sources.", "conclusion": "LCR offers a novel, effective approach for sound remixing via text instructions, demonstrated by a large dataset and strong experimental results."}}
{"id": "2506.09329", "pdf": "https://arxiv.org/pdf/2506.09329", "abs": "https://arxiv.org/abs/2506.09329", "authors": ["Yuxin Jiang"], "title": "Towards Efficient and Effective Alignment of Large Language Models", "categories": ["cs.CL"], "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "AI": {"tldr": "The paper introduces novel methodologies for aligning large language models (LLMs) with human expectations, focusing on data collection, training, and evaluation. Key contributions include Lion for adversarial distillation, WebR for automated data synthesis, LTE for knowledge integration, BMC for preference optimization, and FollowBench for constraint adherence evaluation.", "motivation": "Aligning LLMs with human expectations efficiently and effectively is a critical challenge, as existing methods rely on manual curation or proprietary models, limiting scalability and diversity.", "method": "Proposes Lion for adversarial distillation, WebR for automated data synthesis, LTE for meta-learning-based knowledge updates, BMC for token-level preference optimization, and FollowBench for evaluating constraint adherence.", "result": "The methodologies achieve state-of-the-art zero-shot reasoning, improved data diversity and scalability, superior alignment in QA and mathematical reasoning, and expose weaknesses in current models' constraint adherence.", "conclusion": "The introduced frameworks significantly advance LLM alignment, offering scalable, diverse, and effective solutions for data, training, and evaluation, with insights for future improvements."}}
{"id": "2506.09659", "pdf": "https://arxiv.org/pdf/2506.09659", "abs": "https://arxiv.org/abs/2506.09659", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.", "AI": {"tldr": "Intent Factored Generation (IFG) improves diversity in LLM outputs by factorizing sampling into intent and response stages, enhancing reasoning and conversational tasks without sacrificing quality.", "motivation": "Current methods for diversity in LLM outputs are token-level, leading to repetitive responses and poor exploration in reasoning tasks.", "method": "IFG splits sampling into two stages: sampling a semantically dense intent (e.g., summary/keywords) and generating the final response conditioned on the intent and prompt, using varied temperatures for diversity and coherence.", "result": "IFG improves pass@k and RL from Verifier Feedback on math/code tasks, enhances conversational diversity with Direct Preference Optimisation, and maintains quality in general language tasks.", "conclusion": "IFG is a simple, effective method to increase LLM output diversity while preserving performance, easily integrable into existing algorithms."}}
{"id": "2506.09093", "pdf": "https://arxiv.org/pdf/2506.09093", "abs": "https://arxiv.org/abs/2506.09093", "authors": ["Bingjie Zhang", "Hongkang Li", "Changlong Shi", "Guowei Rong", "He Zhao", "Dongsheng Wang", "Dandan Guo", "Meng Wang"], "title": "Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-task learning (MTL) concurrently trains a model on diverse task\ndatasets to exploit common features, thereby improving overall performance\nacross the tasks. Recent studies have dedicated efforts to merging multiple\nindependent model parameters into a unified model for MTL, thus circumventing\nthe need for training data and expanding the scope of applicable scenarios of\nMTL. However, current approaches to model merging predominantly concentrate on\nenhancing performance within in-domain (ID) datasets, often overlooking their\nefficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV\n(Layer-wise Pruning Task Vector) by building a saliency score, measuring the\nredundancy of parameters in task vectors. Designed in this way ours can achieve\nmask vector for each task and thus perform layer-wise pruning on the task\nvectors, only keeping the pre-trained model parameters at the corresponding\nlayer in merged model. Owing to its flexibility, our method can be seamlessly\nintegrated with most of existing model merging methods to improve their\nperformance on OOD tasks. Extensive experiments demonstrate that the\napplication of our method results in substantial enhancements in OOD\nperformance while preserving the ability on ID tasks.", "AI": {"tldr": "LwPTV (Layer-wise Pruning Task Vector) improves multi-task learning by pruning redundant parameters in task vectors, enhancing out-of-domain performance while maintaining in-domain effectiveness.", "motivation": "Current model merging methods for multi-task learning focus on in-domain performance, neglecting out-of-domain efficacy.", "method": "Proposes LwPTV, which uses a saliency score to measure parameter redundancy, enabling layer-wise pruning of task vectors.", "result": "LwPTV significantly improves out-of-domain performance without compromising in-domain results.", "conclusion": "LwPTV is a flexible and effective method for enhancing multi-task learning across diverse domains."}}
{"id": "2506.09106", "pdf": "https://arxiv.org/pdf/2506.09106", "abs": "https://arxiv.org/abs/2506.09106", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "title": "Bias Analysis in Unconditional Image Generative Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "AI": {"tldr": "The paper investigates bias in unconditional generative AI models, finding small attribute shifts but highlighting classifier sensitivity in bias evaluation.", "motivation": "Addressing concerns about representational harm and discriminatory outcomes in generative AI, especially in unconditional generation where bias mechanisms are unclear.", "method": "Training unconditional image generative models and using a bias evaluation framework to study shifts between training and generated distributions.", "result": "Detected attribute shifts are small but sensitive to the attribute classifier, especially for attributes on a spectrum.", "conclusion": "Emphasizes the need for better labeling practices, scrutiny of evaluation frameworks, and understanding the social complexity of attributes in bias assessment."}}
{"id": "2502.04388", "pdf": "https://arxiv.org/pdf/2502.04388", "abs": "https://arxiv.org/abs/2502.04388", "authors": ["Hepeng Li", "Yuhong Liu", "Jun Yan", "Jie Gao", "Xiaoou Yang", "Mohamed Naili"], "title": "Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) agents capable of autonomous learning and\nindependent decision-making hold great promise for addressing complex\nchallenges across various critical infrastructure domains, including\ntransportation, energy systems, and manufacturing. However, the surge in the\ndesign and deployment of AI systems, driven by various stakeholders with\ndistinct and unaligned objectives, introduces a crucial challenge: How can\nuncoordinated AI systems coexist and evolve harmoniously in shared environments\nwithout creating chaos or compromising safety? To address this, we advocate for\na fundamental rethinking of existing multi-agent frameworks, such as\nmulti-agent systems and game theory, which are largely limited to predefined\nrules and static objective structures. We posit that AI agents should be\nempowered to adjust their objectives dynamically, make compromises, form\ncoalitions, and safely compete or cooperate through evolving relationships and\nsocial feedback. Through two case studies in critical infrastructure\napplications, we call for a shift toward the emergent, self-organizing, and\ncontext-aware nature of these multi-agentic AI systems.", "AI": {"tldr": "The paper advocates for dynamic, self-organizing AI systems to address challenges of uncoordinated AI agents in shared environments.", "motivation": "The rise of AI systems with unaligned objectives in critical infrastructure domains poses risks of chaos and safety compromises.", "method": "Proposes rethinking multi-agent frameworks to enable dynamic objective adjustment, compromise, coalition formation, and social feedback.", "result": "Case studies highlight the need for emergent, context-aware AI systems in critical infrastructure.", "conclusion": "A shift toward self-organizing, adaptive AI systems is essential for harmonious coexistence of AI agents."}}
{"id": "2309.09652", "pdf": "https://arxiv.org/pdf/2309.09652", "abs": "https://arxiv.org/abs/2309.09652", "authors": ["Peter Ochieng"], "title": "Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "10 pages", "summary": "This work introduces UDPNet, a novel architecture designed to accelerate the\nreverse diffusion process in speech synthesis. Unlike traditional diffusion\nmodels that rely on timestep embeddings and shared network parameters, UDPNet\nunrolls the reverse diffusion process directly into the network architecture,\nwith successive layers corresponding to equally spaced steps in the diffusion\nschedule. Each layer progressively refines the noisy input, culminating in a\nhigh-fidelity estimation of the original data, \\(x_0\\). Additionally, we\nredefine the learning target by predicting latent variables instead of the\nconventional \\(x_0\\) or noise \\(\\epsilon_0\\). This shift addresses the common\nissue of large prediction errors in early denoising stages, effectively\nreducing speech distortion. Extensive evaluations on single- and multi-speaker\ndatasets demonstrate that UDPNet consistently outperforms state-of-the-art\nmethods in both quality and efficiency, while generalizing effectively to\nunseen speech. These results position UDPNet as a robust solution for real-time\nspeech synthesis applications. Sample audio is available at\nhttps://onexpeters.github.io/UDPNet.", "AI": {"tldr": "UDPNet accelerates speech synthesis by unrolling the reverse diffusion process into its architecture, predicting latent variables to reduce distortion, and outperforming state-of-the-art methods.", "motivation": "Traditional diffusion models in speech synthesis suffer from timestep embeddings and shared parameters, leading to inefficiencies and distortion. UDPNet aims to address these issues.", "method": "UDPNet unrolls the reverse diffusion process into its architecture, with layers corresponding to diffusion steps. It predicts latent variables instead of conventional targets like noise or original data.", "result": "UDPNet outperforms state-of-the-art methods in quality and efficiency, reducing speech distortion and generalizing well to unseen speech.", "conclusion": "UDPNet is a robust solution for real-time speech synthesis, offering improved performance and generalization."}}
{"id": "2506.09063", "pdf": "https://arxiv.org/pdf/2506.09063", "abs": "https://arxiv.org/abs/2506.09063", "authors": ["Shayan Shekarforoush", "David B. Lindell", "Marcus A. Brubaker", "David J. Fleet"], "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "eess.IV"], "comment": "21 pages, 14 figures, Project Webpage:\n  https://shekshaa.github.io/CryoSPIRE", "summary": "Cryo-EM is a transformational paradigm in molecular biology where\ncomputational methods are used to infer 3D molecular structure at atomic\nresolution from extremely noisy 2D electron microscope images. At the forefront\nof research is how to model the structure when the imaged particles exhibit\nnon-rigid conformational flexibility and compositional variation where parts\nare sometimes missing. We introduce a novel 3D reconstruction framework with a\nhierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for\n4D scene reconstruction. In particular, the structure of the model is grounded\nin an initial process that infers a part-based segmentation of the particle,\nproviding essential inductive bias in order to handle both conformational and\ncompositional variability. The framework, called CryoSPIRE, is shown to reveal\nbiologically meaningful structures on complex experimental datasets, and\nestablishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM\nheterogeneity methods.", "AI": {"tldr": "CryoSPIRE is a new 3D reconstruction framework for cryo-EM that handles conformational and compositional variability using a hierarchical Gaussian mixture model, achieving state-of-the-art results.", "motivation": "Addressing the challenge of modeling non-rigid conformational flexibility and compositional variation in cryo-EM images.", "method": "A hierarchical Gaussian mixture model inspired by Gaussian Splatting, with part-based segmentation for inductive bias.", "result": "CryoSPIRE reveals biologically meaningful structures and sets a new benchmark on CryoBench.", "conclusion": "The framework advances cryo-EM reconstruction by effectively handling variability and improving accuracy."}}
{"id": "2409.09396", "pdf": "https://arxiv.org/pdf/2409.09396", "abs": "https://arxiv.org/abs/2409.09396", "authors": ["Wenhao Yang", "Jianguo Wei", "Wenhuan Lu", "Lei Li", "Xugang Lu"], "title": "Channel Adaptation for Speaker Verification Using Optimal Transport with Pseudo Label", "categories": ["eess.AS", "cs.SD"], "comment": "5 pages, 3 figures", "summary": "Domain gap often degrades the performance of speaker verification (SV)\nsystems when the statistical distributions of training data and real-world test\nspeech are mismatched. Channel variation, a primary factor causing this gap, is\nless addressed than other issues (e.g., noise). Although various domain\nadaptation algorithms could be applied to handle this domain gap problem, most\nalgorithms could not take the complex distribution structure in domain\nalignment with discriminative learning. In this paper, we propose a novel\nunsupervised domain adaptation method, i.e., Joint Partial Optimal Transport\nwith Pseudo Label (JPOT-PL), to alleviate the channel mismatch problem.\nLeveraging the geometric-aware distance metric of optimal transport in\ndistribution alignment, we further design a pseudo label-based discriminative\nlearning where the pseudo label can be regarded as a new type of soft speaker\nlabel derived from the optimal coupling. With the JPOT-PL, we carry out\nexperiments on the SV channel adaptation task with VoxCeleb as the basis\ncorpus. Experiments show our method reduces EER by over 10% compared with\nseveral state-of-the-art channel adaptation algorithms.", "AI": {"tldr": "The paper proposes JPOT-PL, an unsupervised domain adaptation method to address channel mismatch in speaker verification, reducing EER by over 10%.", "motivation": "Domain gap, especially from channel variation, degrades speaker verification performance, but existing methods lack effective alignment and discriminative learning.", "method": "JPOT-PL combines optimal transport for distribution alignment with pseudo label-based discriminative learning.", "result": "Experiments show a 10%+ reduction in EER compared to state-of-the-art methods.", "conclusion": "JPOT-PL effectively mitigates channel mismatch in speaker verification, outperforming existing approaches."}}
{"id": "2506.09340", "pdf": "https://arxiv.org/pdf/2506.09340", "abs": "https://arxiv.org/abs/2506.09340", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "title": "RePO: Replay-Enhanced Policy Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "RePO improves RL for LLMs by using replay strategies to enhance policy optimization, outperforming GRPO with higher efficiency and performance gains.", "motivation": "Address the high computational costs and low data efficiency of GRPO in optimizing LLMs via RL.", "method": "Introduces RePO, leveraging diverse replay strategies to retrieve off-policy samples from a replay buffer for broader policy optimization.", "result": "RePO achieves significant performance gains (18.4 and 4.1 points for two models) and increases effective optimization steps by 48% with a 15% computational cost rise.", "conclusion": "RePO is a more efficient and effective method for RL-based optimization of LLMs compared to GRPO."}}
{"id": "2506.09977", "pdf": "https://arxiv.org/pdf/2506.09977", "abs": "https://arxiv.org/abs/2506.09977", "authors": ["Stylianos Loukas Vasileiou", "Antonio Rago", "Maria Vanina Martinez", "William Yeoh"], "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies", "categories": ["cs.AI"], "comment": null, "summary": "Understanding how humans revise their beliefs in light of new information is\ncrucial for developing AI systems which can effectively model, and thus align\nwith, human reasoning. While theoretical belief revision frameworks rely on a\nset of principles that establish how these operations are performed, empirical\nevidence from cognitive psychology suggests that people may follow different\npatterns when presented with conflicting information. In this paper, we present\nthree comprehensive user studies showing that people consistently prefer\nexplanation-based revisions, i.e., those which are guided by explanations, that\nresult in changes to their belief systems that are not necessarily captured by\nclassical belief change theory. Our experiments systematically investigate how\npeople revise their beliefs with explanations for inconsistencies, whether they\nare provided with them or left to formulate them themselves, demonstrating a\nrobust preference for what may seem non-minimal revisions across different\ntypes of scenarios. These findings have implications for AI systems designed to\nmodel human reasoning or interact with humans, suggesting that such systems\nshould accommodate explanation-based, potentially non-minimal belief revision\noperators to better align with human cognitive processes.", "AI": {"tldr": "People prefer explanation-based belief revisions, often non-minimal, challenging classical belief change theory. AI systems should adapt to align with human reasoning.", "motivation": "To understand human belief revision patterns and improve AI systems' alignment with human reasoning.", "method": "Three user studies analyzing how people revise beliefs with explanations, whether provided or self-formulated.", "result": "Consistent preference for explanation-based, non-minimal belief revisions across scenarios.", "conclusion": "AI systems should incorporate explanation-based belief revision operators to better model human cognition."}}
{"id": "2506.09096", "pdf": "https://arxiv.org/pdf/2506.09096", "abs": "https://arxiv.org/abs/2506.09096", "authors": ["Chaoyang Zhou", "Shunyu Liu", "Zengmao Wang", "Di Wang", "Rong-Cheng Tu", "Bo Du", "Dacheng Tao"], "title": "Intra-Trajectory Consistency for Reward Modeling", "categories": ["cs.LG", "cs.AI"], "comment": "Under review", "summary": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.", "AI": {"tldr": "The paper proposes using generation probabilities to improve reward models for LLMs by ensuring reward consistency across response processes, enhancing generalization and performance.", "motivation": "Current reward models rely on coarse-grained response-level scores, which struggle to identify specific components correlating with rewards, leading to poor generalization.", "method": "The paper introduces intra-trajectory consistency regularization, leveraging generation probabilities to propagate fine-grained signals for reward learning.", "result": "The proposed method improves performance on RewardBench, enhances DPO-aligned policies, and achieves better best-of-N inference-time verification results.", "conclusion": "The approach effectively refines reward models by incorporating fine-grained signals, demonstrating improved generalization and performance in LLMs."}}
{"id": "2506.09109", "pdf": "https://arxiv.org/pdf/2506.09109", "abs": "https://arxiv.org/abs/2506.09109", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "categories": ["cs.CV", "cs.CL"], "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "AI": {"tldr": "CAIRe is a new metric for evaluating cultural relevance in text-to-image models, outperforming baselines by 28% F1 and aligning well with human judgments.", "motivation": "Addressing cross-cultural biases in text-to-image models, which are hindered by performance trade-offs and lack of reliable bias measurement.", "method": "Introduces CAIRe, a framework that grounds image entities in a knowledge base for graded cultural relevance judgments.", "result": "CAIRe surpasses baselines by 28% F1 and achieves Pearson's correlations of 0.56 and 0.66 with human ratings on culturally universal datasets.", "conclusion": "CAIRe effectively measures cultural relevance, demonstrating strong alignment with human judgment across diverse image sources."}}
{"id": "2504.07138", "pdf": "https://arxiv.org/pdf/2504.07138", "abs": "https://arxiv.org/abs/2504.07138", "authors": ["Claudio Novelli", "Javier Argota S\u00e1nchez-Vaquerizo", "Dirk Helbing", "Antonino Rotolo", "Luciano Floridi"], "title": "A Replica for our Democracies? On Using Digital Twins to Enhance Deliberative Democracy", "categories": ["cs.MA", "cs.CY", "cs.ET"], "comment": null, "summary": "Deliberative democracy depends on carefully designed institutional\nframeworks, such as participant selection, facilitation methods, and\ndecision-making mechanisms, that shape how deliberation performs. However,\nidentifying optimal institutional designs for specific contexts remains\nchallenging when relying solely on real-world observations or laboratory\nexperiments: they can be expensive, ethically and methodologically tricky, or\ntoo limited in scale to give us clear answers. Computational experiments offer\na complementary approach, enabling researchers to conduct large-scale\ninvestigations while systematically analyzing complex dynamics, emergent and\nunexpected collective behavior, and risks or opportunities associated with\nnovel democratic designs. Therefore, this paper explores Digital Twin (DT)\ntechnology as a computational testing ground for deliberative systems (with\npotential applicability to broader institutional analysis). By constructing\ndynamic models that simulate real-world deliberation, DTs allow researchers and\npolicymakers to rigorously test \"what-if\" scenarios across diverse\ninstitutional configurations in a controlled virtual environment. This approach\nfacilitates evidence-based assessment of novel designs using synthetically\ngenerated data, bypassing the constraints of real-world or lab-based\nexperimentation, and without societal disruption. The paper also discusses the\nlimitations of this new methodological approach and suggests where future\nresearch should focus.", "AI": {"tldr": "The paper proposes using Digital Twin (DT) technology as a computational tool to simulate and test deliberative democracy designs, overcoming limitations of real-world and lab experiments.", "motivation": "Identifying optimal institutional designs for deliberative democracy is challenging due to the cost, ethical issues, and scale limitations of traditional methods.", "method": "The paper explores DT technology to create dynamic models simulating real-world deliberation, enabling systematic testing of diverse institutional configurations in a virtual environment.", "result": "DTs provide a controlled, scalable, and ethical way to assess novel democratic designs using synthetic data, avoiding societal disruption.", "conclusion": "While promising, the approach has limitations; future research should address these gaps to refine the methodology."}}
{"id": "2410.16785", "pdf": "https://arxiv.org/pdf/2410.16785", "abs": "https://arxiv.org/abs/2410.16785", "authors": ["Osamu Take", "Taketo Akama"], "title": "Annotation-Free MIDI-to-Audio Synthesis via Concatenative Synthesis and Generative Refinement", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Work in progress; 7 pages, 4 figures, 3 tables", "summary": "Recent MIDI-to-audio synthesis methods using deep neural networks have\nsuccessfully generated high-quality, expressive instrumental tracks. However,\nthese methods require MIDI annotations for supervised training, limiting the\ndiversity of instrument timbres and expression styles in the output. We propose\nCoSaRef, a MIDI-to-audio synthesis method that does not require MIDI-audio\npaired datasets. CoSaRef first generates a synthetic audio track using\nconcatenative synthesis based on MIDI input, then refines it with a\ndiffusion-based deep generative model trained on datasets without MIDI\nannotations. This approach improves the diversity of timbres and expression\nstyles. Additionally, it allows detailed control over timbres and expression\nthrough audio sample selection and extra MIDI design, similar to traditional\nfunctions in digital audio workstations. Experiments showed that CoSaRef could\ngenerate realistic tracks while preserving fine-grained timbre control via\none-shot samples. Moreover, despite not being supervised on MIDI annotation,\nCoSaRef outperformed the state-of-the-art timbre-controllable method based on\nMIDI supervision in both objective and subjective evaluation.", "AI": {"tldr": "CoSaRef is a MIDI-to-audio synthesis method that avoids MIDI-audio paired datasets, using concatenative synthesis and diffusion models for diverse, high-quality outputs with fine-grained control.", "motivation": "Existing methods rely on MIDI annotations, limiting timbre and expression diversity. CoSaRef aims to overcome this by leveraging unannotated datasets.", "method": "CoSaRef combines concatenative synthesis (for initial audio generation) with a diffusion-based model (for refinement), trained without MIDI annotations.", "result": "CoSaRef produces realistic tracks with diverse timbres and outperforms MIDI-supervised methods in evaluations.", "conclusion": "CoSaRef offers a flexible, high-quality alternative to MIDI-dependent synthesis, enabling detailed control without annotation constraints."}}
{"id": "2506.09510", "pdf": "https://arxiv.org/pdf/2506.09510", "abs": "https://arxiv.org/abs/2506.09510", "authors": ["Changhao Peng", "Yuqi Ye", "Wei Gao"], "title": "Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Gaussian and Laplacian entropy models are proved effective in learned point\ncloud attribute compression, as they assist in arithmetic coding of latents.\nHowever, we demonstrate through experiments that there is still unutilized\ninformation in entropy parameters estimated by neural networks in current\nmethods, which can be used for more accurate probability estimation. Thus we\nintroduce generalized Gaussian entropy model, which controls the tail shape\nthrough shape parameter to more accurately estimate the probability of latents.\nMeanwhile, to the best of our knowledge, existing methods use fixed likelihood\nintervals for each integer during arithmetic coding, which limits model\nperformance. We propose Mean Error Discriminator (MED) to determine whether the\nentropy parameter estimation is accurate and then dynamically adjust likelihood\nintervals. Experiments show that our method significantly improves\nrate-distortion (RD) performance on three VAE-based models for point cloud\nattribute compression, and our method can be applied to other compression\ntasks, such as image and video compression.", "AI": {"tldr": "The paper introduces a generalized Gaussian entropy model and a Mean Error Discriminator (MED) to improve probability estimation and dynamic likelihood interval adjustment in point cloud attribute compression, enhancing rate-distortion performance.", "motivation": "Current entropy models in point cloud compression underutilize information in entropy parameters, limiting accuracy and performance.", "method": "Proposes a generalized Gaussian entropy model with a shape parameter for better probability estimation and MED for dynamic likelihood interval adjustment.", "result": "Significantly improves rate-distortion performance on VAE-based models and is applicable to other compression tasks.", "conclusion": "The new model and MED enhance compression efficiency and accuracy, with broader applicability beyond point clouds."}}
{"id": "2410.12359", "pdf": "https://arxiv.org/pdf/2410.12359", "abs": "https://arxiv.org/abs/2410.12359", "authors": ["Rui-Chen Zheng", "Hui-Peng Du", "Xiao-Hang Jiang", "Yang Ai", "Zhen-Hua Ling"], "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs", "categories": ["eess.AS"], "comment": null, "summary": "Current neural audio codecs typically use residual vector quantization (RVQ)\nto discretize speech signals. However, they often experience codebook collapse,\nwhich reduces the effective codebook size and leads to suboptimal performance.\nTo address this problem, we introduce ERVQ, Enhanced Residual Vector\nQuantization, a novel enhancement strategy for the RVQ framework in neural\naudio codecs. ERVQ mitigates codebook collapse and boosts codec performance\nthrough both intra- and inter-codebook optimization. Intra-codebook\noptimization incorporates an online clustering strategy and a code balancing\nloss to ensure balanced and efficient codebook utilization. Inter-codebook\noptimization improves the diversity of quantized features by minimizing the\nsimilarity between successive quantizations. Our experiments show that ERVQ\nsignificantly enhances audio codec performance across different models,\nsampling rates, and bitrates, achieving superior quality and generalization\ncapabilities. It also achieves 100% codebook utilization on one of the most\nadvanced neural audio codecs. Further experiments indicate that audio codecs\nimproved by the ERVQ strategy can improve unified speech-and-text large\nlanguage models (LLMs). Specifically, there is a notable improvement in the\nnaturalness of generated speech in downstream zero-shot text-to-speech tasks.\nAudio samples are available here.", "AI": {"tldr": "ERVQ enhances RVQ in neural audio codecs by optimizing intra- and inter-codebook usage, mitigating codebook collapse and improving performance.", "motivation": "Current RVQ-based audio codecs suffer from codebook collapse, reducing effective codebook size and performance.", "method": "ERVQ introduces intra-codebook optimization (online clustering, code balancing loss) and inter-codebook optimization (minimizing similarity between quantizations).", "result": "ERVQ boosts codec performance across models, sampling rates, and bitrates, achieving 100% codebook utilization and improving LLM-generated speech naturalness.", "conclusion": "ERVQ effectively addresses codebook collapse, enhancing audio codec performance and downstream applications like text-to-speech."}}
{"id": "2506.09342", "pdf": "https://arxiv.org/pdf/2506.09342", "abs": "https://arxiv.org/abs/2506.09342", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Latent Multi-Head Attention for Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "AI": {"tldr": "The study explores latent multi-head attention (MLA) for small language models, showing MLA+RoPE with half-rank dimensions reduces memory by 45% with minimal quality loss, outperforming standard attention.", "motivation": "To investigate efficiency-quality trade-offs in small language models using MLA variants, aiming for memory-efficient deployment without compromising performance.", "method": "Training 30M-parameter GPT models on synthetic stories, comparing MHA, MLA, and MLA+RoPE with half-rank latent dimensions.", "result": "MLA+RoPE (r=d/2) reduces KV-cache memory by 45% with only a 0.3% validation loss increase, outperforming vanilla attention by 2% with RoPE. Inference speeds up 1.4x over full-rank MLA.", "conclusion": "MLA+RoPE offers a Pareto improvement for memory-constrained models, with RoPE being crucial for performance. The approach balances efficiency and quality effectively."}}
{"id": "2506.09985", "pdf": "https://arxiv.org/pdf/2506.09985", "abs": "https://arxiv.org/abs/2506.09985", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "48 pages, 19 figures", "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "AI": {"tldr": "The paper presents V-JEPA 2, a self-supervised model trained on internet-scale video and robot data, achieving state-of-the-art performance in motion understanding, action anticipation, and video QA tasks. It also demonstrates zero-shot robotic planning.", "motivation": "To develop AI models that understand and act in the physical world by leveraging large-scale video data and minimal interaction data.", "method": "Pre-train V-JEPA 2 on 1M+ hours of video, align it with a language model, and post-train V-JEPA 2-AC on 62 hours of robot videos for planning.", "result": "State-of-the-art results in motion understanding (77.3 top-1), action anticipation (39.7 recall-at-5), and video QA (84.0 on PerceptionTest). Zero-shot robotic planning achieved.", "conclusion": "Self-supervised learning from web-scale data and minimal robot interaction enables effective world modeling and planning in the physical world."}}
{"id": "2506.09099", "pdf": "https://arxiv.org/pdf/2506.09099", "abs": "https://arxiv.org/abs/2506.09099", "authors": ["Joshua Barron", "Devin White"], "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.", "AI": {"tldr": "The paper explores the trade-off between memorization and generalization in LLMs, showing that model size influences whether a model excels at memorization or generalization, with no model succeeding at both when tasks are combined.", "motivation": "To understand the relationship between memorization and generalization in LLMs, as their interplay remains unclear despite growing evidence of their connection.", "method": "Pre-training capacity-limited Transformer models on synthetic tasks: one for generalization (arithmetic extrapolation) and one for memorization (factual recall).", "result": "Small models generalize but fail to memorize; larger models memorize but fail to generalize. Intermediate models shift toward memorization. No model succeeds at both tasks when combined.", "conclusion": "Pre-training may inherently favor one learning mode over the other, with implications for designing small language models."}}
{"id": "2506.09113", "pdf": "https://arxiv.org/pdf/2506.09113", "abs": "https://arxiv.org/abs/2506.09113", "authors": ["Yu Gao", "Haoyuan Guo", "Tuyen Hoang", "Weilin Huang", "Lu Jiang", "Fangyuan Kong", "Huixia Li", "Jiashi Li", "Liang Li", "Xiaojie Li", "Xunsong Li", "Yifu Li", "Shanchuan Lin", "Zhijie Lin", "Jiawei Liu", "Shu Liu", "Xiaonan Nie", "Zhiwu Qing", "Yuxi Ren", "Li Sun", "Zhi Tian", "Rui Wang", "Sen Wang", "Guoqiang Wei", "Guohong Wu", "Jie Wu", "Ruiqi Xia", "Fei Xiao", "Xuefeng Xiao", "Jiangqiao Yan", "Ceyuan Yang", "Jianchao Yang", "Runkai Yang", "Tao Yang", "Yihang Yang", "Zilyu Ye", "Xuejiao Zeng", "Yan Zeng", "Heng Zhang", "Yang Zhao", "Xiaozheng Zheng", "Peihao Zhu", "Jiaxin Zou", "Feilong Zuo"], "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "categories": ["cs.CV"], "comment": "Seedance 1.0 Technical Report", "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.", "AI": {"tldr": "Seedance 1.0 is a high-performance video generation model addressing prompt adherence, motion plausibility, and visual quality through multi-source data, efficient architecture, and optimized post-training.", "motivation": "Current video generation models struggle to balance prompt following, motion plausibility, and visual quality.", "method": "Combines multi-source data curation, efficient architecture, post-training optimization, and model acceleration techniques.", "result": "Achieves high-quality, fast video generation with superior spatiotemporal fluidity and precise instruction adherence.", "conclusion": "Seedance 1.0 outperforms state-of-the-art models in quality and speed, excelling in complex scenarios."}}
{"id": "2505.21298", "pdf": "https://arxiv.org/pdf/2505.21298", "abs": "https://arxiv.org/abs/2505.21298", "authors": ["Emanuele La Malfa", "Gabriele La Malfa", "Samuele Marro", "Jie M. Zhang", "Elizabeth Black", "Michael Luck", "Philip Torr", "Michael Wooldridge"], "title": "Large Language Models Miss the Multi-Agent Mark", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities.", "AI": {"tldr": "The paper critiques current MAS LLMs for lacking foundational MAS principles, identifies discrepancies in agency, environment, coordination, and behavior measurement, and advocates for better integration of MAS concepts.", "motivation": "To address the gap between MAS theory and current MAS LLM implementations, highlighting missed opportunities and mischaracterizations.", "method": "Systematic analysis of discrepancies in four key areas: agency, environment design, coordination, and emergent behavior measurement.", "result": "Identifies oversimplified, LLM-centric architectures lacking MAS characteristics like autonomy and social interaction.", "conclusion": "Advocates for integrating established MAS concepts and precise terminology to avoid mischaracterization and leverage research opportunities."}}
{"id": "2502.16794", "pdf": "https://arxiv.org/pdf/2502.16794", "abs": "https://arxiv.org/abs/2502.16794", "authors": ["Xilin Jiang", "Sukru Samet Dindar", "Vishal Choudhari", "Stephan Bickel", "Ashesh Mehta", "Guy M McKhann", "Daniel Friedman", "Adeen Flinker", "Nima Mesgarani"], "title": "AAD-LLM: Neural Attention-Driven Auditory Scene Understanding", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "eess.AS"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Auditory foundation models, including auditory large language models (LLMs),\nprocess all sound inputs equally, independent of listener perception. However,\nhuman auditory perception is inherently selective: listeners focus on specific\nspeakers while ignoring others in complex auditory scenes. Existing models do\nnot incorporate this selectivity, limiting their ability to generate\nperception-aligned responses. To address this, we introduce Intention-Informed\nAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM\n(AAD-LLM), a prototype system that integrates brain signals to infer listener\nattention. AAD-LLM extends an auditory LLM by incorporating intracranial\nelectroencephalography (iEEG) recordings to decode which speaker a listener is\nattending to and refine responses accordingly. The model first predicts the\nattended speaker from neural activity, then conditions response generation on\nthis inferred attentional state. We evaluate AAD-LLM on speaker description,\nspeech transcription and extraction, and question answering in multitalker\nscenarios, with both objective and subjective ratings showing improved\nalignment with listener intention. By taking a first step toward\nintention-aware auditory AI, this work explores a new paradigm where listener\nperception informs machine listening, paving the way for future\nlistener-centered auditory systems. Demo and code available:\nhttps://aad-llm.github.io.", "AI": {"tldr": "The paper introduces Intention-Informed Auditory Scene Understanding (II-ASU) and Auditory Attention-Driven LLM (AAD-LLM), a system that uses brain signals to align AI responses with human auditory attention.", "motivation": "Current auditory models lack human-like selective attention, limiting their ability to generate perception-aligned responses.", "method": "AAD-LLM integrates intracranial EEG (iEEG) to decode listener attention and refines responses based on inferred attentional states.", "result": "AAD-LLM improves performance in speaker description, transcription, and question answering, aligning better with listener intention.", "conclusion": "This work pioneers intention-aware auditory AI, paving the way for listener-centered auditory systems."}}
{"id": "2208.07552", "pdf": "https://arxiv.org/pdf/2208.07552", "abs": "https://arxiv.org/abs/2208.07552", "authors": ["Juhyung Park", "Dongwon Park", "Sooyeon Ji", "Hyeong-Geol Shin", "Se Young Chun", "Jongho Lee"], "title": "Coil2Coil: Self-supervised MR image denoising using phased-array coil images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "9 pages, 5figures", "summary": "Denoising of magnetic resonance images is beneficial in improving the quality\nof low signal-to-noise ratio images. Recently, denoising using deep neural\nnetworks has demonstrated promising results. Most of these networks, however,\nutilize supervised learning, which requires large training images of\nnoise-corrupted and clean image pairs. Obtaining training images, particularly\nclean images, is expensive and time-consuming. Hence, methods such as\nNoise2Noise (N2N) that require only pairs of noise-corrupted images have been\ndeveloped to reduce the burden of obtaining training datasets. In this study,\nwe propose a new self-supervised denoising method, Coil2Coil (C2C), that does\nnot require the acquisition of clean images or paired noise-corrupted images\nfor training. Instead, the method utilizes multichannel data from phased-array\ncoils to generate training images. First, it divides and combines multichannel\ncoil images into two images, one for input and the other for label. Then, they\nare processed to impose noise independence and sensitivity normalization such\nthat they can be used for the training images of N2N. For inference, the method\ninputs a coil-combined image (e.g., DICOM image), enabling a wide application\nof the method. When evaluated using synthetic noise-added images, C2C shows the\nbest performance against several self-supervised methods, reporting comparable\noutcomes to supervised methods. When testing the DICOM images, C2C successfully\ndenoised real noise without showing structure-dependent residuals in the error\nmaps. Because of the significant advantage of not requiring additional scans\nfor clean or paired images, the method can be easily utilized for various\nclinical applications.", "AI": {"tldr": "Proposes Coil2Coil (C2C), a self-supervised MRI denoising method using multichannel coil data, eliminating the need for clean or paired training images.", "motivation": "Supervised denoising methods require costly clean image pairs; C2C avoids this by leveraging multichannel data.", "method": "Divides multichannel coil images into input-label pairs, processes for noise independence, and trains using Noise2Noise principles.", "result": "Outperforms other self-supervised methods, matches supervised results, and denoises real DICOM images effectively.", "conclusion": "C2C offers a practical, no-additional-scan solution for clinical MRI denoising."}}
{"id": "2410.23323", "pdf": "https://arxiv.org/pdf/2410.23323", "abs": "https://arxiv.org/abs/2410.23323", "authors": ["Peter Ochieng", "Dennis Kaburu"], "title": "Phonology-Guided Speech-to-Speech Translation for African Languages", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "We present a prosody-guided framework for speech-to-speech translation (S2ST)\nthat aligns and translates speech \\emph{without} transcripts by leveraging\ncross-linguistic pause synchrony. Analyzing a 6{,}000-hour East African news\ncorpus spanning five languages, we show that \\emph{within-phylum} language\npairs exhibit 30--40\\% lower pause variance and over 3$\\times$ higher\nonset/offset correlation compared to cross-phylum pairs. These findings\nmotivate \\textbf{SPaDA}, a dynamic-programming alignment algorithm that\nintegrates silence consistency, rate synchrony, and semantic similarity. SPaDA\nimproves alignment $F_1$ by +3--4 points and eliminates up to 38\\% of spurious\nmatches relative to greedy VAD baselines. Using SPaDA-aligned segments, we\ntrain \\textbf{SegUniDiff}, a diffusion-based S2ST model guided by\n\\emph{external gradients} from frozen semantic and speaker encoders. SegUniDiff\nmatches an enhanced cascade in BLEU (30.3 on CVSS-C vs.\\ 28.9 for UnitY),\nreduces speaker error rate (EER) from 12.5\\% to 5.3\\%, and runs at an RTF of\n1.02. To support evaluation in low-resource settings, we also release a\nthree-tier, transcript-free BLEU suite (M1--M3) that correlates strongly with\nhuman judgments. Together, our results show that prosodic cues in multilingual\nspeech provide a reliable scaffold for scalable, non-autoregressive S2ST.", "AI": {"tldr": "A prosody-guided framework for speech-to-speech translation (S2ST) leverages cross-linguistic pause synchrony to align and translate speech without transcripts, improving alignment and translation quality.", "motivation": "The study is motivated by the observation that within-phylum language pairs exhibit lower pause variance and higher onset/offset correlation, suggesting prosodic cues can enhance S2ST.", "method": "The framework includes SPaDA, a dynamic-programming alignment algorithm integrating silence consistency, rate synchrony, and semantic similarity, and SegUniDiff, a diffusion-based S2ST model guided by external gradients.", "result": "SPaDA improves alignment by 3-4 points and reduces spurious matches by 38%. SegUniDiff matches cascade BLEU scores, reduces speaker error rate, and operates efficiently.", "conclusion": "Prosodic cues in multilingual speech provide a reliable scaffold for scalable, non-autoregressive S2ST, supported by a new transcript-free BLEU suite for low-resource evaluation."}}
{"id": "2506.09349", "pdf": "https://arxiv.org/pdf/2506.09349", "abs": "https://arxiv.org/abs/2506.09349", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "AI": {"tldr": "OmniDRCA is a parallel speech-text foundation model using joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment, achieving SOTA performance in speech-text tasks.", "motivation": "Existing methods either lack modality awareness in speech-text generation or rely on interleaved modeling. OmniDRCA aims to improve parallel joint modeling for better mutual modality awareness.", "method": "OmniDRCA employs joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment to process speech and text in parallel.", "result": "OmniDRCA achieves SOTA performance in parallel joint speech-text modeling and is competitive with interleaved models on Spoken Question Answering benchmarks.", "conclusion": "OmniDRCA demonstrates the effectiveness of parallel joint modeling and contrastive alignment, with potential for extension to full-duplex conversational scenarios."}}
{"id": "2410.16222", "pdf": "https://arxiv.org/pdf/2410.16222", "abs": "https://arxiv.org/abs/2410.16222", "authors": ["Valentyn Boreiko", "Alexander Panfilov", "Vaclav Voracek", "Matthias Hein", "Jonas Geiping"], "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. These methods largely succeed in coercing the\ntarget output in their original settings, but their attacks vary substantially\nin fluency and computational effort. In this work, we propose a unified threat\nmodel for the principled comparison of these methods. Our threat model checks\nif a given jailbreak is likely to occur in the distribution of text. For this,\nwe build an N-gram language model on 1T tokens, which, unlike model-based\nperplexity, allows for an LLM-agnostic, nonparametric, and inherently\ninterpretable evaluation. We adapt popular attacks to this threat model, and,\nfor the first time, benchmark these attacks on equal footing with it. After an\nextensive comparison, we find attack success rates against safety-tuned modern\nmodels to be lower than previously presented and that attacks based on discrete\noptimization significantly outperform recent LLM-based attacks. Being\ninherently interpretable, our threat model allows for a comprehensive analysis\nand comparison of jailbreak attacks. We find that effective attacks exploit and\nabuse infrequent bigrams, either selecting the ones absent from real-world text\nor rare ones, e.g., specific to Reddit or code datasets.", "AI": {"tldr": "A unified threat model is proposed to evaluate jailbreaking attacks on LLMs, revealing lower success rates than previously thought and highlighting the superiority of discrete optimization-based attacks.", "motivation": "To provide a principled comparison of jailbreaking attacks on safety-tuned LLMs, addressing variability in fluency and computational effort.", "method": "Develop an N-gram language model on 1T tokens for LLM-agnostic, nonparametric, and interpretable evaluation. Adapt and benchmark popular attacks under this model.", "result": "Attack success rates are lower than previously reported; discrete optimization-based attacks outperform LLM-based ones. Effective attacks exploit rare or infrequent bigrams.", "conclusion": "The threat model enables interpretable analysis, showing that successful attacks rely on exploiting uncommon text patterns."}}
{"id": "2506.09101", "pdf": "https://arxiv.org/pdf/2506.09101", "abs": "https://arxiv.org/abs/2506.09101", "authors": ["M\u00edriam Barrab\u00e9s", "Daniel Mas Montserrat", "Kapal Dev", "Alexander G. Ioannidis"], "title": "Feature Shift Localization Network", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "9 pages, 2 figures, 4 tables", "summary": "Feature shifts between data sources are present in many applications\ninvolving healthcare, biomedical, socioeconomic, financial, survey, and\nmulti-sensor data, among others, where unharmonized heterogeneous data sources,\nnoisy data measurements, or inconsistent processing and standardization\npipelines can lead to erroneous features. Localizing shifted features is\nimportant to address the underlying cause of the shift and correct or filter\nthe data to avoid degrading downstream analysis. While many techniques can\ndetect distribution shifts, localizing the features originating them is still\nchallenging, with current solutions being either inaccurate or not scalable to\nlarge and high-dimensional datasets. In this work, we introduce the Feature\nShift Localization Network (FSL-Net), a neural network that can localize\nfeature shifts in large and high-dimensional datasets in a fast and accurate\nmanner. The network, trained with a large number of datasets, learns to extract\nthe statistical properties of the datasets and can localize feature shifts from\npreviously unseen datasets and shifts without the need for re-training. The\ncode and ready-to-use trained model are available at\nhttps://github.com/AI-sandbox/FSL-Net.", "AI": {"tldr": "FSL-Net is a neural network designed to accurately and scalably localize feature shifts in large, high-dimensional datasets without requiring re-training.", "motivation": "Feature shifts in heterogeneous data sources degrade downstream analysis, but existing solutions for localizing these shifts are either inaccurate or unscalable.", "method": "FSL-Net is trained on diverse datasets to learn statistical properties, enabling it to localize shifts in unseen datasets without re-training.", "result": "FSL-Net provides fast and accurate feature shift localization in large, high-dimensional datasets.", "conclusion": "FSL-Net offers a scalable and efficient solution for localizing feature shifts, addressing a critical gap in data analysis."}}
{"id": "2506.09229", "pdf": "https://arxiv.org/pdf/2506.09229", "abs": "https://arxiv.org/abs/2506.09229", "authors": ["Sungwon Hwang", "Hyojin Jang", "Kinam Kim", "Minho Park", "Jaegul choo"], "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models", "categories": ["cs.CV"], "comment": "24 pages, 25 figures", "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io", "AI": {"tldr": "The paper introduces CREPA, a method to improve video diffusion model fine-tuning by aligning hidden states across frames for better semantic consistency.", "motivation": "Fine-tuning video diffusion models for specific attributes is challenging and underexplored, despite its practical importance.", "method": "Proposes Cross-frame Representation Alignment (CREPA), aligning hidden states of a frame with external features from neighboring frames.", "result": "CREPA improves visual fidelity and semantic coherence in large-scale VDMs like CogVideoX-5B and Hunyuan Video.", "conclusion": "CREPA is broadly applicable and effective for enhancing video diffusion model fine-tuning."}}
{"id": "2506.07400", "pdf": "https://arxiv.org/pdf/2506.07400", "abs": "https://arxiv.org/abs/2506.07400", "authors": ["Philip R. Liu", "Sparsh Bansal", "Jimmy Dinh", "Aditya Pawar", "Ramani Satishkumar", "Shail Desai", "Neeraj Gupta", "Xin Wang", "Shu Hu"], "title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "comment": "7 pages, 6 figures. Accepted to the 2025 IEEE 8th International\n  Conference on Multimedia Information Processing and Retrieval (MIPR)", "summary": "The integration of deep learning-based glaucoma detection with large language\nmodels (LLMs) presents an automated strategy to mitigate ophthalmologist\nshortages and improve clinical reporting efficiency. However, applying general\nLLMs to medical imaging remains challenging due to hallucinations, limited\ninterpretability, and insufficient domain-specific medical knowledge, which can\npotentially reduce clinical accuracy. Although recent approaches combining\nimaging models with LLM reasoning have improved reporting, they typically rely\non a single generalist agent, restricting their capacity to emulate the diverse\nand complex reasoning found in multidisciplinary medical teams. To address\nthese limitations, we propose MedChat, a multi-agent diagnostic framework and\nplatform that combines specialized vision models with multiple role-specific\nLLM agents, all coordinated by a director agent. This design enhances\nreliability, reduces hallucination risk, and enables interactive diagnostic\nreporting through an interface tailored for clinical review and educational\nuse. Code available at https://github.com/Purdue-M2/MedChat.", "AI": {"tldr": "MedChat proposes a multi-agent framework combining vision models and role-specific LLMs to improve glaucoma detection and reporting, addressing limitations of single-agent systems.", "motivation": "To mitigate ophthalmologist shortages and enhance clinical reporting efficiency by overcoming challenges like hallucinations and limited interpretability in general LLMs.", "method": "Uses a multi-agent diagnostic framework with specialized vision models and role-specific LLM agents, coordinated by a director agent.", "result": "Enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting.", "conclusion": "MedChat offers a promising solution for automated glaucoma detection and reporting, improving clinical accuracy and efficiency."}}
{"id": "2502.20838", "pdf": "https://arxiv.org/pdf/2502.20838", "abs": "https://arxiv.org/abs/2502.20838", "authors": ["Ragib Amin Nihal", "Benjamin Yen", "Runwu Shi", "Kazuhiro Nakadai"], "title": "Weakly Supervised Multiple Instance Learning for Whale Call Detection and Temporal Localization in Long-Duration Passive Acoustic Monitoring", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": null, "summary": "Marine ecosystem monitoring via Passive Acoustic Monitoring (PAM) generates\nvast data, but deep learning often requires precise annotations and short\nsegments. We introduce DSMIL-LocNet, a Multiple Instance Learning framework for\nwhale call detection and localization using only bag-level labels. Our\ndual-stream model processes 2-30 minute audio segments, leveraging spectral and\ntemporal features with attention-based instance selection. Tests on Antarctic\nwhale data show longer contexts improve classification (F1: 0.8-0.9) while\nmedium instances ensure localization precision (0.65-0.70). This suggests MIL\ncan enhance scalable marine monitoring. Code:\nhttps://github.com/Ragib-Amin-Nihal/DSMIL-Loc", "AI": {"tldr": "DSMIL-LocNet uses Multiple Instance Learning for whale call detection and localization with bag-level labels, improving classification and localization in marine monitoring.", "motivation": "Deep learning for marine monitoring often needs precise annotations, which DSMIL-LocNet avoids by using bag-level labels.", "method": "A dual-stream model processes 2-30 minute audio segments, combining spectral and temporal features with attention-based instance selection.", "result": "Tests show improved classification (F1: 0.8-0.9) and localization precision (0.65-0.70) with longer contexts and medium instances.", "conclusion": "MIL frameworks like DSMIL-LocNet can enhance scalable marine ecosystem monitoring."}}
{"id": "2402.01779", "pdf": "https://arxiv.org/pdf/2402.01779", "abs": "https://arxiv.org/abs/2402.01779", "authors": ["Marien Renaud", "Jean Prost", "Arthur Leclaire", "Nicolas Papadakis"], "title": "Plug-and-Play image restoration with Stochastic deNOising REgularization", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Plug-and-Play (PnP) algorithms are a class of iterative algorithms that\naddress image inverse problems by combining a physical model and a deep neural\nnetwork for regularization. Even if they produce impressive image restoration\nresults, these algorithms rely on a non-standard use of a denoiser on images\nthat are less and less noisy along the iterations, which contrasts with recent\nalgorithms based on Diffusion Models (DM), where the denoiser is applied only\non re-noised images. We propose a new PnP framework, called Stochastic\ndeNOising REgularization (SNORE), which applies the denoiser only on images\nwith noise of the adequate level. It is based on an explicit stochastic\nregularization, which leads to a stochastic gradient descent algorithm to solve\nill-posed inverse problems. A convergence analysis of this algorithm and its\nannealing extension is provided. Experimentally, we prove that SNORE is\ncompetitive with respect to state-of-the-art methods on deblurring and\ninpainting tasks, both quantitatively and qualitatively.", "AI": {"tldr": "SNORE introduces a PnP framework using stochastic regularization, applying denoisers only on adequately noisy images, outperforming state-of-the-art methods in deblurring and inpainting.", "motivation": "Address the non-standard use of denoisers in PnP algorithms by aligning with DM principles, ensuring denoisers operate on correctly noised images.", "method": "Proposes SNORE, a stochastic regularization framework, using stochastic gradient descent for inverse problems, with convergence analysis.", "result": "Competes with state-of-the-art methods in deblurring and inpainting, both quantitatively and qualitatively.", "conclusion": "SNORE effectively bridges PnP and DM principles, offering a robust solution for image restoration tasks."}}
{"id": "2411.14013", "pdf": "https://arxiv.org/pdf/2411.14013", "abs": "https://arxiv.org/abs/2411.14013", "authors": ["Mat\u00edas Pizarro", "Mike Laszkiewicz", "Shawkat Hesso", "Dorothea Kolossa", "Asja Fischer"], "title": "Model Attribution and Detection of Synthetic Speech via Vocoder Fingerprints", "categories": ["eess.AS", "cs.CR", "cs.LG"], "comment": null, "summary": "As speech generation technology advances, so do the potential threats of\nmisusing synthetic speech signals. This work tackles three tasks: (1)\nsingle-model attribution in an open-world setting corresponding to the task of\nidentifying whether synthetic speech signals originate from a specific vocoder\n(which requires only target vocoder data), (2) model attribution in a\nclosed-world setting that corresponds to selecting the specific model that\ngenerated a sample from a given set of models, and (3) distinguishing synthetic\nfrom real speech. We show that standardized average residuals between audio\nsignals and their low-pass or EnCodec filtered versions serve as powerful\nvocoder fingerprints that can be leveraged for all tasks achieving an average\nAUROC of over 99% on LJSpeech and JSUT in most settings. The accompanying\nrobustness study shows that it is also resilient to noise levels up to a\ncertain degree.", "AI": {"tldr": "The paper addresses synthetic speech misuse by tackling three tasks: single-model attribution (open-world), model attribution (closed-world), and synthetic vs. real speech distinction, achieving high accuracy (99% AUROC) using standardized residuals as vocoder fingerprints.", "motivation": "Advancements in speech generation raise misuse concerns, necessitating methods to attribute synthetic speech to specific models and distinguish it from real speech.", "method": "Uses standardized average residuals from low-pass or EnCodec filtered audio signals as vocoder fingerprints for attribution and detection.", "result": "Achieves over 99% AUROC on LJSpeech and JSUT datasets, with robustness to noise up to a certain level.", "conclusion": "Standardized residuals are effective fingerprints for synthetic speech attribution and detection, offering high accuracy and noise resilience."}}
{"id": "2506.09351", "pdf": "https://arxiv.org/pdf/2506.09351", "abs": "https://arxiv.org/abs/2506.09351", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters.", "AI": {"tldr": "DIVE is a Diversity-Enhanced reconstruction method for MoE LLMs, improving training efficiency by leveraging expert diversity from pruned models.", "motivation": "Existing MoE LLM reconstruction methods lack expert diversity, causing redundancy. DIVE addresses this by using pruned models to enhance diversity.", "method": "DIVE involves domain affinity mining, pruning-based expert reconstruction, and efficient retraining of routers, experts, and normalization modules.", "result": "DIVE achieves higher training efficiency with minimal accuracy loss, outperforming other methods with the same activated parameters.", "conclusion": "DIVE effectively enhances expert diversity and training efficiency in MoE LLMs, offering a practical solution for reconstruction."}}
{"id": "2506.08672", "pdf": "https://arxiv.org/pdf/2506.08672", "abs": "https://arxiv.org/abs/2506.08672", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "AI": {"tldr": "RuleReasoner, a reinforced rule-based reasoning method, outperforms large reasoning models (LRMs) in both in-distribution and out-of-distribution tasks, offering computational efficiency and robust generalization.", "motivation": "Address the challenge of whether small reasoning models (SRMs) can effectively learn rule-based reasoning with robust generalization across diverse tasks and domains.", "method": "Introduces RuleReasoner, which uses a domain-aware dynamic sampling approach to resample training batches based on historical rewards, enabling flexible online learning for reinforcement learning (RL).", "result": "RuleReasoner outperforms frontier LRMs by 4.1% on ID tasks and 10.4% on OOD tasks, with higher computational efficiency.", "conclusion": "RuleReasoner demonstrates that SRMs can achieve robust rule-based reasoning, offering a simpler and more efficient alternative to LRMs."}}
{"id": "2506.09104", "pdf": "https://arxiv.org/pdf/2506.09104", "abs": "https://arxiv.org/abs/2506.09104", "authors": ["Jung Hyun Lee", "Seungjae Shin", "Vinnam Kim", "Jaeseong You", "An Chen"], "title": "Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "As the rapid scaling of large language models (LLMs) poses significant\nchallenges for deployment on resource-constrained devices, there is growing\ninterest in extremely low-bit quantization, such as 2-bit. Although prior works\nhave shown that 2-bit large models are pareto-optimal over their 4-bit smaller\ncounterparts in both accuracy and latency, these advancements have been limited\nto pre-trained LLMs and have not yet been extended to instruction-tuned models.\nTo bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel\nprogressive quantization framework (FP16$\\rightarrow$INT4$\\rightarrow$INT2)\nthat unifies block-wise post-training quantization (PTQ) with\ndistillation-based quantization-aware training (Distill-QAT) for INT2\ninstruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned\nmodels to INT4 using block-wise PTQ to significantly reduce the quantization\nerror introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT\nto enable INT2 instruction-tuned LLMs to generate responses consistent with\ntheir original FP16 counterparts by minimizing the generalized Jensen-Shannon\ndivergence (JSD) between the two. To the best of our knowledge, we are the\nfirst to demonstrate that UPQ can quantize open-source instruction-tuned LLMs\nto INT2 without relying on proprietary post-training data, while achieving\nstate-of-the-art performances on MMLU and IFEval$-$two of the most\nrepresentative benchmarks for evaluating instruction-tuned LLMs.", "AI": {"tldr": "UPQ is a novel framework for quantizing instruction-tuned LLMs to INT2, combining block-wise PTQ and Distill-QAT, achieving state-of-the-art results on benchmarks.", "motivation": "Address the gap in extending low-bit quantization to instruction-tuned LLMs, enabling deployment on resource-constrained devices.", "method": "Progressive quantization (FP16\u2192INT4\u2192INT2) with block-wise PTQ and Distill-QAT, minimizing JSD divergence.", "result": "Achieves state-of-the-art performance on MMLU and IFEval benchmarks without proprietary data.", "conclusion": "UPQ successfully quantizes instruction-tuned LLMs to INT2, offering a practical solution for resource-limited deployments."}}
{"id": "2506.09237", "pdf": "https://arxiv.org/pdf/2506.09237", "abs": "https://arxiv.org/abs/2506.09237", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "AI": {"tldr": "PatchGuard introduces an adversarially robust AD and AL method using pseudo anomalies and a ViT-based architecture, outperforming previous methods in adversarial settings.", "motivation": "Current AD and AL methods are vulnerable to adversarial attacks due to limited training data (only normal samples). PatchGuard aims to address this by incorporating pseudo anomalies and localization masks.", "method": "Uses Foreground-Aware Pseudo-Anomalies in a ViT-based framework with adversarial training guided by a novel loss function.", "result": "Achieves performance gains of 53.2% in AD and 68.5% in AL in adversarial settings, while maintaining competitive accuracy in non-adversarial scenarios.", "conclusion": "PatchGuard significantly improves robustness in AD and AL, validated by experiments on industrial and medical datasets."}}
{"id": "2312.04540", "pdf": "https://arxiv.org/pdf/2312.04540", "abs": "https://arxiv.org/abs/2312.04540", "authors": ["Ahmad Rahimi", "Po-Chien Luan", "Yuejiang Liu", "Frano Raji\u010d", "Alexandre Alahi"], "title": "Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "comment": "CVPR 2025", "summary": "Modeling spatial-temporal interactions among neighboring agents is at the\nheart of multi-agent problems such as motion forecasting and crowd navigation.\nDespite notable progress, it remains unclear to which extent modern\nrepresentations can capture the causal relationships behind agent interactions.\nIn this work, we take an in-depth look at the causal awareness of these\nrepresentations, from computational formalism to real-world practice. First, we\ncast doubt on the notion of non-causal robustness studied in the recent\nCausalAgents benchmark. We show that recent representations are already\npartially resilient to perturbations of non-causal agents, and yet modeling\nindirect causal effects involving mediator agents remains challenging. To\naddress this challenge, we introduce a metric learning approach that\nregularizes latent representations with causal annotations. Our controlled\nexperiments show that this approach not only leads to higher degrees of causal\nawareness but also yields stronger out-of-distribution robustness. To further\noperationalize it in practice, we propose a sim-to-real causal transfer method\nvia cross-domain multi-task learning. Experiments on pedestrian datasets show\nthat our method can substantially boost generalization, even in the absence of\nreal-world causal annotations. We hope our work provides a new perspective on\nthe challenges and pathways towards causally-aware representations of\nmulti-agent interactions. Our code is available at\nhttps://github.com/vita-epfl/CausalSim2Real.", "AI": {"tldr": "The paper critiques current representations of spatial-temporal agent interactions, introduces a metric learning approach for causal awareness, and proposes a sim-to-real transfer method, showing improved robustness and generalization.", "motivation": "To understand and improve the causal awareness of modern representations in modeling spatial-temporal agent interactions, addressing gaps in resilience to perturbations and indirect causal effects.", "method": "Introduces a metric learning approach to regularize latent representations with causal annotations and a sim-to-real transfer method via cross-domain multi-task learning.", "result": "The approach enhances causal awareness and out-of-distribution robustness, with experiments on pedestrian datasets demonstrating improved generalization even without real-world causal annotations.", "conclusion": "The work offers insights into achieving causally-aware representations for multi-agent interactions, with practical implications for motion forecasting and crowd navigation."}}
{"id": "2506.07473", "pdf": "https://arxiv.org/pdf/2506.07473", "abs": "https://arxiv.org/abs/2506.07473", "authors": ["Emmanuel Deruty"], "title": "An introduction to pitch strength in contemporary popular music analysis and production", "categories": ["cs.SD", "eess.AS", "00A65", "J.5"], "comment": "In Music 2024, Innovation in Music Conference, 14-16 June, 2024,\n  Kristiania University College, Oslo, Norway", "summary": "Music information retrieval distinguishes between low- and high-level\ndescriptions of music. Current generative AI models rely on text descriptions\nthat are higher level than the controls familiar to studio musicians. Pitch\nstrength, a low-level perceptual parameter of contemporary popular music, may\nbe one feature that could make such AI models more suited to music production.\nSignal and perceptual analyses suggest that pitch strength (1) varies\nsignificantly across and inside songs; (2) contributes to both small- and\nlarge-scale structure; (3) contributes to the handling of polyphonic\ndissonance; and (4) may be a feature of upper harmonics made audible in a\nperspective of perceptual richness.", "AI": {"tldr": "The paper explores pitch strength as a low-level feature to enhance generative AI models for music production, highlighting its variability and structural impact in songs.", "motivation": "To bridge the gap between high-level text descriptions in AI models and the low-level controls used by studio musicians, focusing on pitch strength as a key perceptual parameter.", "method": "Signal and perceptual analyses were conducted to examine pitch strength's variability, structural contributions, and role in polyphonic dissonance.", "result": "Pitch strength varies significantly in songs, influences structure, aids in handling dissonance, and may relate to upper harmonics in perceptual richness.", "conclusion": "Pitch strength is a promising low-level feature for improving AI music models, aligning them better with practical music production needs."}}
{"id": "2405.18410", "pdf": "https://arxiv.org/pdf/2405.18410", "abs": "https://arxiv.org/abs/2405.18410", "authors": ["Mahrokh Najaf", "Gregory Ongie"], "title": "Towards a Sampling Theory for Implicit Neural Representations", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE Asilomar 2024", "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\nsolving inverse problems in computer vision and computational imaging. INRs\nrepresent images as continuous domain functions realized by a neural network\ntaking spatial coordinates as inputs. However, unlike traditional pixel\nrepresentations, little is known about the sample complexity of estimating\nimages using INRs in the context of linear inverse problems. Towards this end,\nwe study the sampling requirements for recovery of a continuous domain image\nfrom its low-pass Fourier coefficients by fitting a single hidden-layer INR\nwith ReLU activation and a Fourier features layer using a generalized form of\nweight decay regularization. Our key insight is to relate minimizers of this\nnon-convex parameter space optimization problem to minimizers of a convex\npenalty defined over an infinite-dimensional space of measures. We identify a\nsufficient number of samples for which an image realized by a width-1 INR is\nexactly recoverable by solving the INR training problem, and give a conjecture\nfor the general width-$W$ case. To validate our theory, we empirically assess\nthe probability of achieving exact recovery of images realized by low-width\nsingle hidden-layer INRs, and illustrate the performance of INR on\nsuper-resolution recovery of more realistic continuous domain phantom images.", "AI": {"tldr": "The paper explores the sample complexity of estimating images using implicit neural representations (INRs) in linear inverse problems, focusing on recovery from low-pass Fourier coefficients with a specific INR architecture.", "motivation": "Little is known about the sample complexity of INRs for image estimation in inverse problems, motivating a study of their recovery capabilities.", "method": "The study uses a single hidden-layer INR with ReLU activation and Fourier features, employing generalized weight decay regularization to relate non-convex optimization to convex penalty minimizers.", "result": "The paper identifies sufficient sample conditions for exact recovery of images realized by width-1 INRs and conjectures for general width-$W$ cases, supported by empirical validation.", "conclusion": "INRs show promise for exact recovery in inverse problems, with theoretical and empirical evidence supporting their effectiveness, particularly in super-resolution tasks."}}
{"id": "2503.23004", "pdf": "https://arxiv.org/pdf/2503.23004", "abs": "https://arxiv.org/abs/2503.23004", "authors": ["Stefano Damiano", "Kathleen MacWilliam", "Valerio Lorenzoni", "Thomas Dietzen", "Toon van Waterschoot"], "title": "The trajectoRIR Database: Room Acoustic Recordings Along a Trajectory of Moving Microphones", "categories": ["eess.AS"], "comment": "17 pages, 7 figures", "summary": "Data availability is essential to develop acoustic signal processing\nalgorithms, especially when it comes to data-driven approaches that demand\nlarge and diverse training datasets. For this reason, an increasing number of\ndatabases have been published in recent years, including either room impulse\nresponses (RIRs) or audio recordings during motion. In this paper we introduce\nthe trajectoRIR database, an extensive, multi-array collection of both dynamic\nand stationary acoustic recordings along a controlled trajectory in a room.\nSpecifically, the database features recordings using moving microphones and\nstationary RIRs spatially sampling the room acoustics along an L-shaped\ntrajectory. This combination makes trajectoRIR unique and applicable in various\ntasks ranging from sound source localization and tracking to spatially dynamic\nsound field reconstruction, auralization and system identification. The\nrecording room has a reverberation time of 0.5 seconds, and the three different\nmicrophone configurations employed include a dummy head, with additional\nreference microphones located next to the ears, 3 first-order Ambisonics\nmicrophones, two circular arrays of 16 and 4 channels, and a 12-channel linear\narray. The motion of the microphones was achieved using a robotic cart\ntraversing a 4.62 meter-long rail at three speeds: [0.2, 0.4, 0.8] m/s. Audio\nsignals were reproduced using two stationary loudspeakers. The collected\ndatabase features 8648 stationary RIRs, as well as perfect sweeps, speech,\nmusic, and stationary noise recorded during motion. Python functions are\nincluded to access the recorded audio as well as to retrieve geometrical\ninformation.", "AI": {"tldr": "The paper introduces trajectoRIR, a database combining dynamic and stationary acoustic recordings for diverse signal processing tasks.", "motivation": "To address the need for large, diverse datasets in acoustic signal processing, especially for data-driven approaches.", "method": "Recordings include moving microphones and stationary RIRs along an L-shaped trajectory, using various microphone configurations and robotic motion.", "result": "The database features 8648 stationary RIRs and dynamic recordings, with Python tools for access.", "conclusion": "trajectoRIR is a unique, versatile resource for tasks like sound localization, tracking, and auralization."}}
{"id": "2506.09359", "pdf": "https://arxiv.org/pdf/2506.09359", "abs": "https://arxiv.org/abs/2506.09359", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "categories": ["cs.CL"], "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation.", "AI": {"tldr": "The paper explores using LLMs to evaluate semantic and weak semantic equivalence in Text-to-SQL systems, addressing challenges in ambiguous queries and multiple valid SQL interpretations.", "motivation": "The rise of LLMs has advanced NL2SQL systems, but evaluating semantic equivalence of generated SQL remains challenging due to ambiguous queries and multiple valid interpretations.", "method": "The study uses LLMs to assess semantic and weak semantic equivalence, analyzing common patterns of SQL equivalence and inequivalence.", "result": "The paper identifies challenges in LLM-based evaluation of SQL equivalence, highlighting practical and semantic issues.", "conclusion": "LLMs show promise for evaluating SQL equivalence but face challenges in handling ambiguity and multiple valid interpretations."}}
{"id": "2506.09061", "pdf": "https://arxiv.org/pdf/2506.09061", "abs": "https://arxiv.org/abs/2506.09061", "authors": ["Alyssa Pinnock", "Shakya Jayakody", "Kawsher A Roxy", "Md Rubel Ahmed"], "title": "EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model", "categories": ["cs.DC", "cs.AI", "cs.PF"], "comment": "4 figures, 7 pages, IEEE conference template", "summary": "This paper introduces EdgeProfiler, a fast profiling framework designed for\nevaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs\noffer remarkable capabilities in natural language understanding and generation,\ntheir high computational, memory, and power requirements often confine them to\ncloud environments. EdgeProfiler addresses these challenges by providing a\nsystematic methodology for assessing LLM performance in resource-constrained\nedge settings. The framework profiles compact LLMs, including TinyLLaMA,\nGemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization\ntechniques and strict memory constraints. Analytical modeling is used to\nestimate latency, FLOPs, and energy consumption. The profiling reveals that\n4-bit quantization reduces model memory usage by approximately 60-70%, while\nmaintaining accuracy within 2-5% of full-precision baselines. Inference speeds\nare observed to improve by 2-3x compared to FP16 baselines across various edge\ndevices. Power modeling estimates a 35-50% reduction in energy consumption for\nINT4 configurations, enabling practical deployment on hardware such as\nRaspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the\nimportance of efficient profiling tailored to lightweight LLMs in edge\nenvironments, balancing accuracy, energy efficiency, and computational\nfeasibility.", "AI": {"tldr": "EdgeProfiler is a profiling framework for lightweight LLMs on edge systems, using quantization to reduce memory and energy usage while maintaining accuracy.", "motivation": "LLMs are typically resource-heavy, limiting their use to cloud environments. EdgeProfiler aims to enable their deployment on resource-constrained edge devices.", "method": "The framework profiles compact LLMs (e.g., TinyLLaMA, Gemma3.1B) using aggressive quantization and analytical modeling for latency, FLOPs, and energy consumption.", "result": "4-bit quantization cuts memory usage by 60-70%, maintains accuracy within 2-5%, improves inference speed by 2-3x, and reduces energy consumption by 35-50%.", "conclusion": "EdgeProfiler demonstrates the feasibility of deploying lightweight LLMs on edge devices, balancing accuracy, efficiency, and computational constraints."}}
{"id": "2506.09105", "pdf": "https://arxiv.org/pdf/2506.09105", "abs": "https://arxiv.org/abs/2506.09105", "authors": ["Javier Lopez-Piqueres", "Pranav Deshpande", "Archan Ray", "Mattia J. Villani", "Marco Pistoia", "Niraj Kumar"], "title": "MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "We present MetaTT, a unified Tensor Train (TT) adapter framework for global\nlow-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes\neach weight matrix independently, MetaTT uses a single shared TT to factorize\nall transformer sub-modules -- query, key, value, projection, and feed-forward\nlayers -- by indexing the structural axes like layer and matrix type, and\noptionally heads and tasks. For a given rank, while LoRA adds parameters\nproportional to the product across modes, MetaTT only adds parameters\nproportional to the sum across modes leading to a significantly compressed\nfinal adapter. Our benchmarks compare MetaTT with LoRA along with recent\nstate-of-the-art matrix and tensor decomposition based fine-tuning schemes. We\nobserve that when tested on standard language modeling benchmarks, MetaTT leads\nto the most reduction in the parameters while maintaining similar accuracy to\nLoRA and even outperforming other tensor-based methods. Unlike CP or other\nrank-factorizations, the TT ansatz benefits from mature optimization routines\n-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we\nfind simplifies training. Because new modes can be appended cheaply, MetaTT\nnaturally extends to shared adapters across many tasks without redesigning the\ncore tensor.", "AI": {"tldr": "MetaTT is a Tensor Train adapter framework for efficient low-rank fine-tuning of transformers, outperforming LoRA and other methods in parameter reduction while maintaining accuracy.", "motivation": "To address the inefficiency of independent fine-tuning (like LoRA) by unifying transformer sub-modules into a single shared Tensor Train structure.", "method": "MetaTT factorizes all transformer sub-modules using a shared Tensor Train, indexing structural axes like layer and matrix type, and optionally heads and tasks.", "result": "MetaTT reduces parameters significantly compared to LoRA while maintaining similar accuracy and outperforming other tensor-based methods.", "conclusion": "MetaTT offers a scalable, efficient fine-tuning solution with mature optimization routines and natural extensibility to multi-task scenarios."}}
{"id": "2506.09278", "pdf": "https://arxiv.org/pdf/2506.09278", "abs": "https://arxiv.org/abs/2506.09278", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Project Page: https://uniflowmatch.github.io/", "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "AI": {"tldr": "UFM introduces a unified model for dense image correspondence, outperforming specialized methods in both flow and wide-baseline matching with higher accuracy, lower error, and faster speed.", "motivation": "Dense image correspondence is crucial for applications like visual odometry and 3D reconstruction, but existing methods treat wide-baseline and optical flow tasks separately. UFM aims to unify these approaches.", "method": "UFM uses a generic transformer architecture to regress (u,v) flow directly, trained on unified data for co-visible pixels. It avoids coarse-to-fine cost volumes, simplifying training and improving accuracy for large flows.", "result": "UFM outperforms state-of-the-art methods: 28% more accurate than Unimatch, 62% less error and 6.7x faster than RoMa.", "conclusion": "UFM demonstrates unified training can surpass specialized methods, enabling fast, general-purpose correspondence and opening new research directions."}}
{"id": "2505.08319", "pdf": "https://arxiv.org/pdf/2505.08319", "abs": "https://arxiv.org/abs/2505.08319", "authors": ["Egil Diau"], "title": "Reciprocity as the Foundational Substrate of Society: How Reciprocal Dynamics Scale into Social Systems", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": "Position paper extending arXiv:2505.02945. Clarifies scope and\n  rewrites for clarity. No changes to core framework, theoretical claims, or\n  simulation direction. The framing remains within the scope of cs.CY and cs.MA", "summary": "Prevailing accounts in both multi-agent AI and the social sciences explain\nsocial structure through top-down abstractions-such as institutions, norms, or\ntrust-yet lack simulateable models of how such structures emerge from\nindividual behavior. Ethnographic and archaeological evidence suggests that\nreciprocity served as the foundational mechanism of early human societies,\nenabling economic circulation, social cohesion, and interpersonal obligation\nlong before the rise of formal institutions. Modern financial systems such as\ncredit and currency can likewise be viewed as scalable extensions of\nreciprocity, formalizing exchange across time and anonymity. Building on this\ninsight, we argue that reciprocity is not merely a local or primitive exchange\nheuristic, but the scalable substrate from which large-scale social structures\ncan emerge. We propose a three-stage framework to model this emergence:\nreciprocal dynamics at the individual level, norm stabilization through shared\nexpectations, and the construction of durable institutional patterns. This\napproach offers a cognitively minimal, behaviorally grounded foundation for\nsimulating how large-scale social systems can emerge from decentralized\nreciprocal interaction.", "AI": {"tldr": "The paper argues that reciprocity is the foundational mechanism for social structures, proposing a three-stage framework to model its emergence from individual behavior to large-scale institutions.", "motivation": "To address the lack of simulateable models explaining how social structures emerge from individual behavior, using reciprocity as the key mechanism.", "method": "A three-stage framework: reciprocal dynamics at the individual level, norm stabilization through shared expectations, and construction of durable institutional patterns.", "result": "Reciprocity is shown as a scalable substrate for large-scale social structures, bridging individual behavior and formal institutions.", "conclusion": "Reciprocity provides a behaviorally grounded foundation for simulating the emergence of large-scale social systems."}}
{"id": "2506.07494", "pdf": "https://arxiv.org/pdf/2506.07494", "abs": "https://arxiv.org/abs/2506.07494", "authors": ["Peng Huang", "Imdad Ullah", "Xiaotong Wei", "Tariq Ahamed Ahanger", "Najm Hassan", "Zawar Hussain Shah"], "title": "Towards Energy-Efficient and Low-Latency Voice-Controlled Smart Homes: A Proposal for Offline Speech Recognition and IoT Integration", "categories": ["cs.SD", "cs.CY", "eess.AS"], "comment": null, "summary": "The smart home systems, based on AI speech recognition and IoT technology,\nenable people to control devices through verbal commands and make people's\nlives more efficient. However, existing AI speech recognition services are\nprimarily deployed on cloud platforms on the Internet. When users issue a\ncommand, speech recognition devices like ``Amazon Echo'' will post a recording\nthrough numerous network nodes, reach multiple servers, and then receive\nresponses through the Internet. This mechanism presents several issues,\nincluding unnecessary energy consumption, communication latency, and the risk\nof a single-point failure. In this position paper, we propose a smart home\nconcept based on offline speech recognition and IoT technology: 1) integrating\noffline keyword spotting (KWS) technologies into household appliances with\nlimited resource hardware to enable them to understand user voice commands; 2)\ndesigning a local IoT network with decentralized architecture to manage and\nconnect various devices, enhancing the robustness and scalability of the\nsystem. This proposal of a smart home based on offline speech recognition and\nIoT technology will allow users to use low-latency voice control anywhere in\nthe home without depending on the Internet and provide better scalability and\nenergy sustainability.", "AI": {"tldr": "Proposes a smart home system using offline speech recognition and IoT to reduce latency, energy use, and dependency on cloud services.", "motivation": "Existing cloud-based AI speech recognition in smart homes causes energy waste, latency, and single-point failures.", "method": "Integrates offline keyword spotting (KWS) into appliances and designs a decentralized local IoT network.", "result": "Enables low-latency, internet-independent voice control with improved scalability and energy efficiency.", "conclusion": "Offline speech recognition and IoT enhance smart home robustness and sustainability."}}
{"id": "2408.00273", "pdf": "https://arxiv.org/pdf/2408.00273", "abs": "https://arxiv.org/abs/2408.00273", "authors": ["Yanbing Chen", "Tianze Tang", "Taehyo Kim", "Hai Shu"], "title": "UKAN-EP: Enhancing U-KAN with Efficient Attention and Pyramid Aggregation for 3D Multi-Modal MRI Brain Tumor Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Gliomas are among the most common malignant brain tumors and are\ncharacterized by considerable heterogeneity, which complicates accurate\ndetection and segmentation. Multi-modal MRI is the clinical standard for glioma\nimaging, but variability across modalities and high computational complexity\nhinder effective automated segmentation. In this paper, we propose UKAN-EP, a\nnovel 3D extension of the original 2D U-KAN model for multi-modal MRI brain\ntumor segmentation. While U-KAN integrates Kolmogorov-Arnold Network (KAN)\nlayers into a U-Net backbone, UKAN-EP further incorporates Efficient Channel\nAttention (ECA) and Pyramid Feature Aggregation (PFA) modules to enhance\ninter-modality feature fusion and multi-scale feature representation. We also\nintroduce a dynamic loss weighting strategy that adaptively balances the\nCross-Entropy and Dice losses during training. We evaluate UKAN-EP on the 2024\nBraTS-GLI dataset and compare it against strong baselines including U-Net,\nAttention U-Net, and Swin UNETR. Results show that UKAN-EP achieves superior\nsegmentation performance while requiring substantially fewer computational\nresources. An extensive ablation study further demonstrates the effectiveness\nof ECA and PFA, as well as the limited utility of self-attention and spatial\nattention alternatives. Code is available at\nhttps://github.com/TianzeTang0504/UKAN-EP.", "AI": {"tldr": "UKAN-EP, a 3D extension of U-KAN, improves glioma segmentation in multi-modal MRI by integrating ECA and PFA modules and a dynamic loss strategy, outperforming baselines with fewer resources.", "motivation": "Glioma segmentation is challenging due to tumor heterogeneity and multi-modal MRI variability, requiring efficient and accurate automated methods.", "method": "UKAN-EP extends U-KAN with ECA and PFA for better feature fusion and multi-scale representation, plus dynamic loss weighting.", "result": "UKAN-EP outperforms U-Net, Attention U-Net, and Swin UNETR on the BraTS-GLI dataset with lower computational costs.", "conclusion": "UKAN-EP is effective for glioma segmentation, with ECA and PFA proving crucial, while self-attention alternatives show limited utility."}}
{"id": "2505.03071", "pdf": "https://arxiv.org/pdf/2505.03071", "abs": "https://arxiv.org/abs/2505.03071", "authors": ["Vincent Dumoulin", "Otilia Stretcu", "Jenny Hamer", "Lauren Harrell", "Rob Laber", "Hugo Larochelle", "Bart van Merri\u00ebnboer", "Amanda Navine", "Patrick Hart", "Ben Williams", "Timothy A. C. Lamont", "Tries B. Razak", "Mars Coral Restoration Team", "Sheryn Brodie", "Brendan Doohan", "Phil Eichinski", "Paul Roe", "Lin Schwarzkopf", "Tom Denton"], "title": "The Search for Squawk: Agile Modeling in Bioacoustics", "categories": ["eess.AS"], "comment": null, "summary": "Passive acoustic monitoring (PAM) has shown great promise in helping\necologists understand the health of animal populations and ecosystems. However,\nextracting insights from millions of hours of audio recordings requires the\ndevelopment of specialized recognizers. This is typically a challenging task,\nnecessitating large amounts of training data and machine learning expertise. In\nthis work, we introduce a general, scalable and data-efficient system for\ndeveloping recognizers for novel bioacoustic problems in under an hour. Our\nsystem consists of several key components that tackle problems in previous\nbioacoustic workflows: 1) highly generalizable acoustic embeddings pre-trained\nfor birdsong classification minimize data hunger; 2) indexed audio search\nallows the efficient creation of classifier training datasets, and 3)\nprecomputation of embeddings enables an efficient active learning loop,\nimproving classifier quality iteratively with minimal wait time. Ecologists\nemployed our system in three novel case studies: analyzing coral reef health\nthrough unidentified sounds; identifying juvenile Hawaiian bird calls to\nquantify breeding success and improve endangered species monitoring; and\nChristmas Island bird occupancy modeling. We augment the case studies with\nsimulated experiments which explore the range of design decisions in a\nstructured way and help establish best practices. Altogether these experiments\nshowcase our system's scalability, efficiency, and generalizability, enabling\nscientists to quickly address new bioacoustic challenges.", "AI": {"tldr": "A scalable system for bioacoustic recognizers reduces data and time requirements, enabling ecologists to address new challenges quickly.", "motivation": "Passive acoustic monitoring (PAM) aids ecosystem health assessment but faces challenges in developing recognizers due to data and expertise needs.", "method": "The system uses pre-trained acoustic embeddings, indexed audio search, and active learning to create efficient classifiers.", "result": "Tested in real-world case studies (coral reefs, Hawaiian birds, Christmas Island birds) and simulations, the system proved scalable and generalizable.", "conclusion": "The system enables rapid development of bioacoustic recognizers, making PAM more accessible for ecological research."}}
{"id": "2506.09367", "pdf": "https://arxiv.org/pdf/2506.09367", "abs": "https://arxiv.org/abs/2506.09367", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "categories": ["cs.CL", "cs.AI"], "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "AI": {"tldr": "COGENT is a curriculum-oriented framework for generating grade-appropriate educational content, addressing challenges in AI-generated STEM education materials.", "motivation": "Generative AI struggles to align with curriculum standards and maintain readability in STEM education, especially for younger students.", "method": "COGENT integrates science concepts, core ideas, and learning objectives, controls readability, and uses a \"wonder-based\" approach for engagement.", "result": "COGENT produces grade-appropriate content comparable or superior to human references, validated by LLM and human evaluation.", "conclusion": "COGENT offers a scalable solution for adaptive, high-quality educational resources."}}
{"id": "2506.09070", "pdf": "https://arxiv.org/pdf/2506.09070", "abs": "https://arxiv.org/abs/2506.09070", "authors": ["Chenqi Zhang", "Yu Feng", "Jieru Zhao", "Guangda Liu", "Wenchao Ding", "Chentao Wu", "Minyi Guo"], "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and\nsparse Gaussian-based representation. However, 3DGS struggles to meet the\nreal-time requirement of 90 frames per second (FPS) on resource-constrained\nmobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on\ncompute efficiency but overlook memory efficiency, leading to redundant DRAM\ntraffic. We introduce STREAMINGGS, a fully streaming 3DGS\nalgorithm-architecture co-design that achieves fine-grained pipelining and\nreduces DRAM traffic by transforming from a tile-centric rendering to a\nmemory-centric rendering. Results show that our design achieves up to 45.7\n$\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.", "AI": {"tldr": "STREAMINGGS improves 3D Gaussian Splatting (3DGS) performance on mobile devices by addressing memory inefficiency, achieving significant speedup and energy savings.", "motivation": "3DGS struggles with real-time performance on mobile devices due to inefficient memory usage, despite its compute efficiency.", "method": "STREAMINGGS introduces a co-design of algorithm and architecture, shifting from tile-centric to memory-centric rendering to reduce DRAM traffic and enable fine-grained pipelining.", "result": "The design achieves up to 45.7\u00d7 speedup and 62.9\u00d7 energy savings compared to mobile Ampere GPUs.", "conclusion": "STREAMINGGS effectively addresses the memory inefficiency of 3DGS, enabling real-time performance on resource-constrained devices."}}
{"id": "2506.09108", "pdf": "https://arxiv.org/pdf/2506.09108", "abs": "https://arxiv.org/abs/2506.09108", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "title": "SensorLM: Learning the Language of Wearable Sensors", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks.", "AI": {"tldr": "SensorLM is a sensor-language foundation model for interpreting wearable sensor data with natural language, achieving superior performance in tasks like activity analysis and healthcare.", "motivation": "Aligning and interpreting sensor data with language is challenging due to the lack of annotated datasets in real-world wearable data.", "method": "A hierarchical caption generation pipeline captures statistical, structural, and semantic information, creating a large sensor-language dataset. SensorLM extends multimodal pretraining architectures like CLIP and CoCa.", "result": "SensorLM outperforms state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval, showing capabilities like scaling and label efficiency.", "conclusion": "SensorLM advances sensor-language understanding, demonstrating strong performance and generalization to unseen tasks."}}
{"id": "2506.09299", "pdf": "https://arxiv.org/pdf/2506.09299", "abs": "https://arxiv.org/abs/2506.09299", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "categories": ["cs.CV", "cs.LG"], "comment": "6 Pages, 3 figures", "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "AI": {"tldr": "A lightweight, energy-efficient YOLOv4-Tiny model is optimized via INT8 quantization for aerial emergency object detection, achieving comparable performance to YOLOv5-small while significantly reducing model size and improving speed.", "motivation": "Addressing the lack of publicly available drone-view emergency imagery, the paper aims to provide a real-time, efficient solution for emergency response using edge devices.", "method": "The YOLOv4-Tiny model is trained on a custom aerial emergency dataset (10,820 images) and optimized through post-training INT8 quantization.", "result": "The quantized model reduces size by 71% (22.5 MB to 6.4 MB) and improves inference speed by 44%, with comparable detection performance.", "conclusion": "The quantized YOLOv4-Tiny is highly suitable for real-time emergency detection on low-power edge devices."}}
{"id": "2506.06366", "pdf": "https://arxiv.org/pdf/2506.06366", "abs": "https://arxiv.org/abs/2506.06366", "authors": ["Lin Chen", "Yunke Zhang", "Jie Feng", "Haoye Chai", "Honglin Zhang", "Bingbing Fan", "Yibo Ma", "Shiyuan Zhang", "Nian Li", "Tianhui Liu", "Nicholas Sukiennik", "Keyu Zhao", "Yu Li", "Ziyi Liu", "Fengli Xu", "Yong Li"], "title": "AI Agent Behavioral Science", "categories": ["q-bio.NC", "cs.CY", "cs.MA"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled the development\nof AI agents that exhibit increasingly human-like behaviors, including\nplanning, adaptation, and social dynamics across diverse, interactive, and\nopen-ended scenarios. These behaviors are not solely the product of the\ninternal architectures of the underlying models, but emerge from their\nintegration into agentic systems operating within specific contexts, where\nenvironmental factors, social cues, and interaction feedbacks shape behavior\nover time. This evolution necessitates a new scientific perspective: AI Agent\nBehavioral Science. Rather than focusing only on internal mechanisms, this\nperspective emphasizes the systematic observation of behavior, design of\ninterventions to test hypotheses, and theory-guided interpretation of how AI\nagents act, adapt, and interact over time. We systematize a growing body of\nresearch across individual agent, multi-agent, and human-agent interaction\nsettings, and further demonstrate how this perspective informs responsible AI\nby treating fairness, safety, interpretability, accountability, and privacy as\nbehavioral properties. By unifying recent findings and laying out future\ndirections, we position AI Agent Behavioral Science as a necessary complement\nto traditional model-centric approaches, providing essential tools for\nunderstanding, evaluating, and governing the real-world behavior of\nincreasingly autonomous AI systems.", "AI": {"tldr": "The paper introduces AI Agent Behavioral Science, a new perspective focusing on observing and interpreting AI agent behavior in real-world contexts, complementing traditional model-centric approaches.", "motivation": "Advances in LLMs have led to AI agents exhibiting human-like behaviors, necessitating a shift from internal model mechanisms to studying behavior in context.", "method": "The paper systematizes research across individual, multi-agent, and human-agent interactions, emphasizing behavioral observation, intervention design, and theory-guided interpretation.", "result": "AI Agent Behavioral Science is proposed as a framework for understanding and governing AI behavior, addressing fairness, safety, interpretability, accountability, and privacy as behavioral properties.", "conclusion": "This perspective is essential for evaluating and managing the real-world behavior of autonomous AI systems, complementing existing model-centric approaches."}}
{"id": "2506.08570", "pdf": "https://arxiv.org/pdf/2506.08570", "abs": "https://arxiv.org/abs/2506.08570", "authors": ["Or Tal", "Felix Kreuk", "Yossi Adi"], "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": null, "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly across many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and pinpoint which design choices most\ninfluence performance. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\nFlow-Matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM", "AI": {"tldr": "The paper compares Auto-Regressive decoding and Conditional Flow-Matching paradigms in text-to-music generation, isolating their effects to guide future system designs.", "motivation": "To address the challenge of evaluating diverse SOTA systems in text-to-music generation and identify the impact of modeling paradigms on performance.", "method": "A controlled comparison of Auto-Regressive decoding and Conditional Flow-Matching using identical datasets, training configurations, and similar architectures.", "result": "The study highlights distinct strengths and limitations of each paradigm, evaluating generation quality, robustness, scalability, conditioning adherence, and editing capabilities.", "conclusion": "The findings provide actionable insights for future text-to-music generation systems, emphasizing trade-offs and emergent behaviors of the two paradigms."}}
{"id": "2408.16355", "pdf": "https://arxiv.org/pdf/2408.16355", "abs": "https://arxiv.org/abs/2408.16355", "authors": ["Kirsten W. H. Maas", "Danny Ruijters", "Anna Vilanova", "Nicola Pezzotti"], "title": "NeRF-CA: Dynamic Reconstruction of X-ray Coronary Angiography with Extremely Sparse-views", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Dynamic three-dimensional (4D) reconstruction from two-dimensional X-ray\ncoronary angiography (CA) remains a significant clinical problem. Existing CA\nreconstruction methods often require extensive user interaction or large\ntraining datasets. Recently, Neural Radiance Field (NeRF) has successfully\nreconstructed high-fidelity scenes in natural and medical contexts without\nthese requirements. However, challenges such as sparse-views, intra-scan\nmotion, and complex vessel morphology hinder its direct application to CA data.\nWe introduce NeRF-CA, a first step toward a fully automatic 4D CA\nreconstruction that achieves reconstructions from sparse coronary angiograms.\nTo the best of our knowledge, we are the first to address the challenges of\nsparse-views and cardiac motion by decoupling the scene into the moving\ncoronary artery and the static background, effectively translating the problem\nof motion into a strength. NeRF-CA serves as a first stepping stone for solving\nthe 4D CA reconstruction problem, achieving adequate 4D reconstructions from as\nfew as four angiograms, as required by clinical practice, while significantly\noutperforming state-of-the-art sparse-view X-ray NeRF. We validate our approach\nquantitatively and qualitatively using representative 4D phantom datasets and\nablation studies. To accelerate research in this domain, we made our codebase\npublic: https://github.com/kirstenmaas/NeRF-CA.", "AI": {"tldr": "NeRF-CA introduces a method for 4D reconstruction of coronary arteries from sparse X-ray angiograms, addressing challenges like sparse views and motion by decoupling static and moving components.", "motivation": "Dynamic 4D reconstruction from 2D X-ray coronary angiography is clinically significant but hindered by sparse views, motion, and vessel complexity. Existing methods require extensive interaction or large datasets.", "method": "NeRF-CA decouples the scene into static background and moving coronary arteries, leveraging Neural Radiance Fields (NeRF) for reconstruction from sparse angiograms.", "result": "NeRF-CA achieves adequate 4D reconstructions from as few as four angiograms, outperforming state-of-the-art sparse-view X-ray NeRF methods.", "conclusion": "NeRF-CA is a promising step toward fully automatic 4D CA reconstruction, validated by phantom datasets and ablation studies, with code made publicly available."}}
{"id": "2506.05802", "pdf": "https://arxiv.org/pdf/2506.05802", "abs": "https://arxiv.org/abs/2506.05802", "authors": ["Adriana Stan", "David Combei", "Dan Oneata", "Horia Cucu"], "title": "TADA: Training-free Attribution and Out-of-Domain Detection of Audio Deepfakes", "categories": ["eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Deepfake detection has gained significant attention across audio, text, and\nimage modalities, with high accuracy in distinguishing real from fake. However,\nidentifying the exact source--such as the system or model behind a\ndeepfake--remains a less studied problem. In this paper, we take a significant\nstep forward in audio deepfake model attribution or source tracing by proposing\na training-free, green AI approach based entirely on k-Nearest Neighbors (kNN).\nLeveraging a pre-trained self-supervised learning (SSL) model, we show that\ngrouping samples from the same generator is straightforward--we obtain an 0.93\nF1-score across five deepfake datasets. The method also demonstrates strong\nout-of-domain (OOD) detection, effectively identifying samples from unseen\nmodels at an F1-score of 0.84.\n  We further analyse these results in a multi-dimensional approach and provide\nadditional insights. All code and data protocols used in this work are\navailable in our open repository: https://github.com/adrianastan/tada/.", "AI": {"tldr": "The paper proposes a training-free, green AI method for audio deepfake model attribution using k-Nearest Neighbors (kNN) and a pre-trained SSL model, achieving high F1-scores for in-domain and out-of-domain detection.", "motivation": "While deepfake detection is well-studied, identifying the exact source (e.g., the model behind a deepfake) is less explored. This paper addresses this gap in audio deepfake model attribution.", "method": "The approach uses k-Nearest Neighbors (kNN) with a pre-trained self-supervised learning (SSL) model, requiring no additional training.", "result": "Achieves an F1-score of 0.93 for in-domain detection and 0.84 for out-of-domain detection across five datasets.", "conclusion": "The method is effective for source tracing in audio deepfakes, with strong performance and open-source availability."}}
{"id": "2506.09381", "pdf": "https://arxiv.org/pdf/2506.09381", "abs": "https://arxiv.org/abs/2506.09381", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time.", "AI": {"tldr": "The paper explores automatic differentiation of perceived low-quality vs. high-quality news headlines/links using ML models, achieving up to 90.3% accuracy with DistilBERT.", "motivation": "The rise of online news necessitates tools to distinguish low-quality content, prompting an investigation into automated methods.", "method": "Twelve ML models were tested on a balanced dataset of 57.5M news links/headlines (2018-2024) with 115 linguistic features, using expert-derived labels.", "result": "Bagging classifier performed well (88.1% accuracy), while fine-tuned DistilBERT achieved the highest accuracy (90.3%) but required more training time.", "conclusion": "Both traditional NLP features and deep learning models can effectively classify news quality, with trade-offs in performance and training time."}}
{"id": "2506.09110", "pdf": "https://arxiv.org/pdf/2506.09110", "abs": "https://arxiv.org/abs/2506.09110", "authors": ["Jingying Ma", "Feng Wu", "Qika Lin", "Yucheng Xing", "Chenyu Liu", "Ziyu Jia", "Mengling Feng"], "title": "CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model", "categories": ["cs.LG"], "comment": null, "summary": "Electroencephalography (EEG) provides real-time insights into brain activity\nand is widely used in neuroscience. However, variations in channel\nconfigurations, sequence lengths, and task objectives limit the transferability\nof traditional task-specific models. Although recent EEG foundation models\n(EFMs) aim to learn generalizable representations, they struggle with limited\nheterogeneous representation capacity and inefficiency in capturing multi-scale\nbrain dependencies. To address these challenges, we propose CodeBrain, an\nefficient EFM structurally aligned with brain organization, trained in two\nstages. (1) We introduce a TFDual-Tokenizer that independently tokenizes\nheterogeneous temporal and frequency components, enabling a quadratic expansion\nof the discrete representation space. This also offers a degree of\ninterpretability through cross-domain token analysis. (2) We propose the\nEEGSSM, which combines a structured global convolution architecture and a\nsliding window attention mechanism to jointly model sparse long-range and local\ndependencies. Unlike fully connected Transformer models, EEGSSM better reflects\nthe brain's small-world topology and efficiently captures EEG's inherent\nmulti-scale structure. EEGSSM is trained with a masked self-supervised learning\nobjective to predict token indices obtained in TFDual-Tokenizer. Comprehensive\nexperiments on 10 public EEG datasets demonstrate the generalizability of\nCodeBrain with linear probing. By offering biologically informed and\ninterpretable EEG modeling, CodeBrain lays the foundation for future\nneuroscience research. Both code and pretraining weights will be released in\nthe future version.", "AI": {"tldr": "CodeBrain is an EEG foundation model addressing limitations in generalizability and multi-scale dependency capture by using a TFDual-Tokenizer and EEGSSM, achieving strong performance across datasets.", "motivation": "Traditional EEG models lack transferability due to configuration variations. EFMs struggle with heterogeneous representation and multi-scale dependencies.", "method": "Two-stage training: (1) TFDual-Tokenizer for temporal/frequency tokenization, (2) EEGSSM combining global convolution and sliding window attention for multi-scale dependency modeling.", "result": "CodeBrain demonstrates generalizability on 10 EEG datasets via linear probing, outperforming traditional models.", "conclusion": "CodeBrain provides biologically informed, interpretable EEG modeling, advancing neuroscience research."}}
{"id": "2506.09300", "pdf": "https://arxiv.org/pdf/2506.09300", "abs": "https://arxiv.org/abs/2506.09300", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents the deployment and performance evaluation of a quantized\nYOLOv4-Tiny model for real-time object detection in aerial emergency imagery on\na resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model\nwas quantized to INT8 precision using TensorFlow Lite post-training\nquantization techniques and evaluated for detection speed, power consumption,\nand thermal feasibility under embedded deployment conditions. The quantized\nmodel achieved an inference time of 28.2 ms per image with an average power\nconsumption of 13.85 W, demonstrating a significant reduction in power usage\ncompared to its FP32 counterpart. Detection accuracy remained robust across key\nemergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These\nresults highlight the potential of low-power embedded AI systems for real-time\ndeployment in safety-critical emergency response applications.", "AI": {"tldr": "A quantized YOLOv4-Tiny model was deployed on a Raspberry Pi 5 for real-time aerial emergency object detection, showing reduced power usage and robust accuracy.", "motivation": "To enable efficient real-time object detection in emergency scenarios using resource-constrained edge devices.", "method": "YOLOv4-Tiny was quantized to INT8 using TensorFlow Lite post-training quantization and evaluated for speed, power, and thermal performance.", "result": "Achieved 28.2 ms inference time, 13.85 W power consumption, and maintained accuracy for emergency classes.", "conclusion": "Quantized models on edge devices are viable for safety-critical emergency applications."}}
{"id": "2506.00267", "pdf": "https://arxiv.org/pdf/2506.00267", "abs": "https://arxiv.org/abs/2506.00267", "authors": ["Cihan Xiao", "Ruixing Liang", "Xiangyu Zhang", "Mehmet Emre Tiryaki", "Veronica Bae", "Lavanya Shankar", "Rong Yang", "Ethan Poon", "Emmanuel Dupoux", "Sanjeev Khudanpur", "Leibny Paola Garcia Perera"], "title": "CASPER: A Large Scale Spontaneous Speech Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The success of large language models has driven interest in developing\nsimilar speech processing capabilities. However, a key challenge is the\nscarcity of high-quality spontaneous speech data, as most existing datasets\ncontain scripted dialogues. To address this, we present a novel pipeline for\neliciting and recording natural dialogues and release our dataset with 100+\nhours of spontaneous speech. Our approach fosters fluid, natural conversations\nwhile encouraging a diverse range of topics and interactive exchanges. Unlike\ntraditional methods, it facilitates genuine interactions, providing a\nreproducible framework for future data collection. This paper introduces our\ndataset and methodology, laying the groundwork for addressing the shortage of\nspontaneous speech data. We plan to expand this dataset in future stages,\noffering a growing resource for the research community.", "AI": {"tldr": "A novel pipeline for collecting spontaneous speech data is introduced, addressing the scarcity of high-quality natural dialogue datasets.", "motivation": "The lack of spontaneous speech data in existing datasets, which mostly contain scripted dialogues, hinders the development of advanced speech processing models.", "method": "A pipeline for eliciting and recording natural dialogues is developed, resulting in a dataset of 100+ hours of spontaneous speech.", "result": "The dataset fosters fluid, natural conversations and diverse topics, providing a reproducible framework for future data collection.", "conclusion": "The paper lays groundwork for addressing the data shortage and plans to expand the dataset, benefiting the research community."}}
{"id": "2504.07904", "pdf": "https://arxiv.org/pdf/2504.07904", "abs": "https://arxiv.org/abs/2504.07904", "authors": ["Blake VanBerlo", "Alexander Wong", "Jesse Hoey", "Robert Arntfield"], "title": "The Efficacy of Semantics-Preserving Transformations in Self-Supervised Learning for Medical Ultrasound", "categories": ["eess.IV", "cs.CV", "cs.LG", "I.2.10; I.4.9; J.3"], "comment": "17 pages, 12 figures, 18 tables, Submitted to Medical Image Analysis", "summary": "Data augmentation is a central component of joint embedding self-supervised\nlearning (SSL). Approaches that work for natural images may not always be\neffective in medical imaging tasks. This study systematically investigated the\nimpact of data augmentation and preprocessing strategies in SSL for lung\nultrasound. Three data augmentation pipelines were assessed: (1) a baseline\npipeline commonly used across imaging domains, (2) a novel semantic-preserving\npipeline designed for ultrasound, and (3) a distilled set of the most effective\ntransformations from both pipelines. Pretrained models were evaluated on\nmultiple classification tasks: B-line detection, pleural effusion detection,\nand COVID-19 classification. Experiments revealed that semantics-preserving\ndata augmentation resulted in the greatest performance for COVID-19\nclassification - a diagnostic task requiring global image context.\nCropping-based methods yielded the greatest performance on the B-line and\npleural effusion object classification tasks, which require strong local\npattern recognition. Lastly, semantics-preserving ultrasound image\npreprocessing resulted in increased downstream performance for multiple tasks.\nGuidance regarding data augmentation and preprocessing strategies was\nsynthesized for practitioners working with SSL in ultrasound.", "AI": {"tldr": "The study evaluates data augmentation and preprocessing strategies for self-supervised learning in lung ultrasound, finding that semantic-preserving methods excel in COVID-19 classification, while cropping-based methods work best for local tasks like B-line detection.", "motivation": "To determine effective data augmentation and preprocessing strategies for self-supervised learning in medical imaging, specifically lung ultrasound, where natural image methods may not suffice.", "method": "Three augmentation pipelines were tested: a baseline, a novel semantic-preserving pipeline for ultrasound, and a distilled set of effective transformations. Pretrained models were evaluated on B-line detection, pleural effusion detection, and COVID-19 classification.", "result": "Semantic-preserving augmentation performed best for COVID-19 classification, while cropping-based methods excelled in local tasks like B-line detection. Preprocessing improved performance across tasks.", "conclusion": "The study provides practical guidance for data augmentation and preprocessing in self-supervised learning for ultrasound, highlighting task-specific strategies."}}
{"id": "2506.07237", "pdf": "https://arxiv.org/pdf/2506.07237", "abs": "https://arxiv.org/abs/2506.07237", "authors": ["Jui-Chiang Wei", "Yi-Cheng Lin", "Fabian Ritter-Gutierrez", "Hung-yi Lee"], "title": "Multi-Distillation from Speech and Music Representation Models", "categories": ["eess.AS"], "comment": "8 pages, 1 figures", "summary": "Real-world audio often mixes speech and music, yet models typically handle\nonly one domain. This paper introduces a multi-teacher distillation framework\nthat unifies speech and music models into a single one while significantly\nreducing model size. Our approach leverages the strengths of domain-specific\nteacher models, such as HuBERT for speech and MERT for music, and explores\nvarious strategies to balance both domains. Experiments across diverse tasks\ndemonstrate that our model matches the performance of domain-specific models,\nshowing the effectiveness of cross-domain distillation. Additionally, we\nconduct few-shot learning experiments, highlighting the need for general models\nin real-world scenarios where labeled data is limited. Our results show that\nour model not only performs on par with specialized models but also outperforms\nthem in few-shot scenarios, proving that a cross-domain approach is essential\nand effective for diverse tasks with limited data.", "AI": {"tldr": "A multi-teacher distillation framework unifies speech and music models into a single compact model, matching domain-specific performance and excelling in few-shot learning.", "motivation": "Real-world audio often mixes speech and music, but existing models handle only one domain, limiting their practicality.", "method": "The framework uses domain-specific teachers (HuBERT for speech, MERT for music) and explores strategies to balance both domains.", "result": "The unified model matches domain-specific models' performance and outperforms them in few-shot scenarios.", "conclusion": "Cross-domain distillation is effective for diverse tasks, especially with limited data, proving the need for general models."}}
{"id": "2506.09391", "pdf": "https://arxiv.org/pdf/2506.09391", "abs": "https://arxiv.org/abs/2506.09391", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "title": "Comparing human and LLM politeness strategies in free production", "categories": ["cs.CL"], "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.", "AI": {"tldr": "Large language models (LLMs) show politeness strategies but overuse negative politeness, raising concerns about pragmatic alignment.", "motivation": "To investigate if LLMs use context-sensitive politeness strategies like humans, balancing informational and social goals.", "method": "Compare human and LLM responses in constrained and open-ended tasks, analyzing politeness strategies.", "result": "Larger models replicate human politeness preferences but overuse negative strategies, even in positive contexts. Human evaluators prefer LLM responses in open-ended tasks.", "conclusion": "LLMs handle politeness well but misalign in strategy use, highlighting challenges for pragmatic alignment in AI."}}
{"id": "2506.09089", "pdf": "https://arxiv.org/pdf/2506.09089", "abs": "https://arxiv.org/abs/2506.09089", "authors": ["Xia Li"], "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "in French language", "summary": "In developing the teaching program for a course in Oral Expression in\nTeaching Chinese as a Foreign Language at the university level, the teacher\ndesigns communicative tasks based on conflicts to encourage learners to engage\nin interactive dynamics and develop their oral interaction skills. During the\ndesign of these tasks, the teacher uses ChatGPT to assist in finalizing the\nprogram. This article aims to present the key characteristics of the\ninteractions between the teacher and ChatGPT during this program development\nprocess, as well as to examine the use of ChatGPT and its impacts in this\nspecific context.", "AI": {"tldr": "A university teacher uses ChatGPT to design communicative conflict-based tasks for a Chinese oral expression course, analyzing the interaction dynamics and ChatGPT's impact.", "motivation": "To enhance learners' oral interaction skills through conflict-based tasks and explore ChatGPT's role in program development.", "method": "Designing communicative tasks with ChatGPT assistance and analyzing teacher-ChatGPT interactions.", "result": "Insights into the characteristics of teacher-ChatGPT interactions and ChatGPT's impact on task design.", "conclusion": "ChatGPT is a valuable tool for designing interactive tasks, aiding in program development for language teaching."}}
{"id": "2506.09114", "pdf": "https://arxiv.org/pdf/2506.09114", "abs": "https://arxiv.org/abs/2506.09114", "authors": ["Jialin Chen", "Ziyu Zhao", "Gaukhar Nurbek", "Aosong Feng", "Ali Maatouk", "Leandros Tassiulas", "Yifeng Gao", "Rex Ying"], "title": "TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval", "categories": ["cs.LG"], "comment": null, "summary": "The ubiquity of dynamic data in domains such as weather, healthcare, and\nenergy underscores a growing need for effective interpretation and retrieval of\ntime-series data. These data are inherently tied to domain-specific contexts,\nsuch as clinical notes or weather narratives, making cross-modal retrieval\nessential not only for downstream tasks but also for developing robust\ntime-series foundation models by retrieval-augmented generation (RAG). Despite\nthe increasing demand, time-series retrieval remains largely underexplored.\nExisting methods often lack semantic grounding, struggle to align heterogeneous\nmodalities, and have limited capacity for handling multi-channel signals. To\naddress this gap, we propose TRACE, a generic multimodal retriever that grounds\ntime-series embeddings in aligned textual context. TRACE enables fine-grained\nchannel-level alignment and employs hard negative mining to facilitate\nsemantically meaningful retrieval. It supports flexible cross-modal retrieval\nmodes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking\nlinguistic descriptions with complex temporal patterns. By retrieving\nsemantically relevant pairs, TRACE enriches downstream models with informative\ncontext, leading to improved predictive accuracy and interpretability. Beyond a\nstatic retrieval engine, TRACE also serves as a powerful standalone encoder,\nwith lightweight task-specific tuning that refines context-aware\nrepresentations while maintaining strong cross-modal alignment. These\nrepresentations achieve state-of-the-art performance on downstream forecasting\nand classification tasks. Extensive experiments across multiple domains\nhighlight its dual utility, as both an effective encoder for downstream\napplications and a general-purpose retriever to enhance time-series models.", "AI": {"tldr": "TRACE is a multimodal retriever for time-series data, enabling cross-modal retrieval and improving downstream tasks with semantic grounding and alignment.", "motivation": "The need for effective interpretation and retrieval of time-series data, which is often tied to domain-specific contexts, is growing. Existing methods lack semantic grounding and struggle with heterogeneous modalities.", "method": "TRACE grounds time-series embeddings in aligned textual context, uses hard negative mining, and supports flexible cross-modal retrieval modes (Text-to-Timeseries and Timeseries-to-Text).", "result": "TRACE improves predictive accuracy and interpretability, achieving state-of-the-art performance on forecasting and classification tasks.", "conclusion": "TRACE serves as both a powerful encoder for downstream tasks and a general-purpose retriever, enhancing time-series models."}}
{"id": "2506.09327", "pdf": "https://arxiv.org/pdf/2506.09327", "abs": "https://arxiv.org/abs/2506.09327", "authors": ["Tong Wang", "Guanzhou Chen", "Xiaodong Zhang", "Chenxi Liu", "Jiaqi Wang", "Xiaoliang Tan", "Wenchao Guo", "Qingyuan Yang", "Kaiqi Zhang"], "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image interpretation plays a critical role in environmental\nmonitoring, urban planning, and disaster assessment. However, acquiring\nhigh-quality labeled data is often costly and time-consuming. To address this\nchallenge, we proposes a multi-modal self-supervised learning framework that\nleverages high-resolution RGB images, multi-spectral data, and digital surface\nmodels (DSM) for pre-training. By designing an information-aware adaptive\nmasking strategy, cross-modal masking mechanism, and multi-task self-supervised\nobjectives, the framework effectively captures both the correlations across\ndifferent modalities and the unique feature structures within each modality. We\nevaluated the proposed method on multiple downstream tasks, covering typical\nremote sensing applications such as scene classification, semantic\nsegmentation, change detection, object detection, and depth estimation.\nExperiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.\nThe results demonstrate that the proposed method outperforms existing\npretraining approaches in most tasks. Specifically, on the Potsdam and\nVaihingen semantic segmentation tasks, our method achieved mIoU scores of\n78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation\ntask, the RMSE error is reduced to 0.182, and for the binary change detection\ntask in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing\nthe second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and\nHR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.", "AI": {"tldr": "A multi-modal self-supervised learning framework for remote sensing image interpretation, leveraging RGB, multi-spectral data, and DSM, outperforms existing methods across 26 tasks.", "motivation": "High-quality labeled data is costly and time-consuming to acquire, necessitating an efficient pre-training method.", "method": "Proposes a framework with adaptive masking, cross-modal masking, and multi-task self-supervised objectives to capture inter-modal correlations and intra-modal features.", "result": "Achieves superior performance on 15 datasets, e.g., 78.30% mIoU for Potsdam segmentation and 0.182 RMSE for US3D depth estimation.", "conclusion": "The framework effectively addresses the labeled data challenge and advances remote sensing tasks."}}
{"id": "2506.00628", "pdf": "https://arxiv.org/pdf/2506.00628", "abs": "https://arxiv.org/abs/2506.00628", "authors": ["Niyati Bafna", "Matthew Wiesner"], "title": "LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Prior research indicates that LID model performance significantly declines on\naccented speech; however, the specific causes, extent, and characterization of\nthese errors remain under-explored. (i) We identify a common failure mode on\naccented speech whereby LID systems often misclassify L2 accented speech as the\nspeaker's native language or a related language. (ii) We present evidence\nsuggesting that state-of-the-art models are invariant to permutations of short\nspans of speech, implying they classify on the basis of short phonotactic\nfeatures indicative of accent rather than language. Our analysis reveals a\nsimple method to enhance model robustness to accents through input chunking.\n(iii) We present an approach that integrates sequence-level information into\nour model without relying on monolingual ASR systems; this reduces\naccent-language confusion and significantly enhances performance on accented\nspeech while maintaining comparable results on standard LID.", "AI": {"tldr": "The paper explores why LID models fail on accented speech, identifies misclassification patterns, and proposes methods to improve robustness.", "motivation": "Prior research shows LID models perform poorly on accented speech, but the causes and solutions are not well understood.", "method": "Analyzes failure modes, tests model invariance, and introduces input chunking and sequence-level integration to improve performance.", "result": "Identifies misclassification patterns, enhances robustness to accents, and improves LID performance on accented speech.", "conclusion": "Simple methods like input chunking and sequence-level integration can significantly improve LID model performance on accented speech."}}
{"id": "2505.22685", "pdf": "https://arxiv.org/pdf/2505.22685", "abs": "https://arxiv.org/abs/2505.22685", "authors": ["Marcus J. Vroemen", "Yuqian Chen", "Yui Lo", "Tengfei Xue", "Weidong Cai", "Fan Zhang", "Josien P. W. Pluim", "Lauren J. O'Donnell"], "title": "DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "15 pages, 5 figures", "summary": "Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural\nconnections, but traditional connectome generation is time-consuming and\nrequires gray matter parcellation, posing challenges for large-scale studies.\nWe introduce DeepMultiConnectome, a deep-learning model that predicts\nstructural connectomes directly from tractography, bypassing the need for gray\nmatter parcellation while supporting multiple parcellation schemes. Using a\npoint-cloud-based neural network with multi-task learning, the model classifies\nstreamlines according to their connected regions across two parcellation\nschemes, sharing a learned representation. We train and validate\nDeepMultiConnectome on tractography from the Human Connectome Project Young\nAdult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter\nparcellation scheme. DeepMultiConnectome predicts multiple structural\nconnectomes from a whole-brain tractogram containing 3 million streamlines in\napproximately 40 seconds. DeepMultiConnectome is evaluated by comparing\npredicted connectomes with traditional connectomes generated using the\nconventional method of labeling streamlines using a gray matter parcellation.\nThe predicted connectomes are highly correlated with traditionally generated\nconnectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region\nscheme) and largely preserve network properties. A test-retest analysis of\nDeepMultiConnectome demonstrates reproducibility comparable to traditionally\ngenerated connectomes. The predicted connectomes perform similarly to\ntraditionally generated connectomes in predicting age and cognitive function.\nOverall, DeepMultiConnectome provides a scalable, fast model for generating\nsubject-specific connectomes across multiple parcellation schemes.", "AI": {"tldr": "DeepMultiConnectome is a deep-learning model that predicts structural connectomes directly from tractography, bypassing gray matter parcellation and supporting multiple schemes. It is fast, scalable, and highly correlated with traditional methods.", "motivation": "Traditional connectome generation is time-consuming and requires gray matter parcellation, posing challenges for large-scale studies.", "method": "Uses a point-cloud-based neural network with multi-task learning to classify streamlines across two parcellation schemes, sharing a learned representation.", "result": "Predicts connectomes in ~40 seconds with high correlation to traditional methods (r=0.992 for 84-region, r=0.986 for 164-region) and preserves network properties.", "conclusion": "DeepMultiConnectome offers a scalable, fast solution for generating subject-specific connectomes across multiple parcellation schemes."}}
{"id": "2506.00975", "pdf": "https://arxiv.org/pdf/2506.00975", "abs": "https://arxiv.org/abs/2506.00975", "authors": ["Qichao Wang", "Ziqiao Meng", "Wenqian Cui", "Yifei Zhang", "Pengcheng Wu", "Bingzhe Wu", "Irwin King", "Liang Chen", "Peilin Zhao"], "title": "NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by ICML 2025", "summary": "Inspired by the impressive capabilities of GPT-4o, there is growing interest\nin enabling speech language models (SLMs) to engage in natural, fluid spoken\ninteractions with humans. Recent advancements have led to the development of\nseveral SLMs that demonstrate promising results in this area. However, current\napproaches have yet to fully exploit dual-channel speech data, which inherently\ncaptures the structure and dynamics of human conversation. In this work, we\nsystematically explore the use of dual-channel speech data in the context of\nmodern large language models, and introduce a novel generative modeling\nparadigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent\ndual-channel spoken dialogue learning using decoder-only architectures for the\nfirst time. We evaluate our approach on standard benchmarks, and empirical\nresults show that our proposed method, NTPP, significantly improves the\nconversational abilities of SLMs in terms of turn-taking prediction, response\ncoherence, and naturalness. Moreover, compared to existing methods, NTPP\nachieves substantially lower inference latency, highlighting its practical\nefficiency for real-time applications.", "AI": {"tldr": "The paper introduces Next-Token-Pair Prediction (NTPP), a novel method for improving speech language models (SLMs) using dual-channel speech data, enhancing conversational abilities and reducing latency.", "motivation": "Current SLMs lack full utilization of dual-channel speech data, which captures human conversation dynamics. The goal is to improve natural, fluid spoken interactions.", "method": "Proposes NTPP, a generative modeling paradigm for speaker-independent dual-channel spoken dialogue learning using decoder-only architectures.", "result": "NTPP significantly improves turn-taking prediction, response coherence, naturalness, and reduces inference latency compared to existing methods.", "conclusion": "NTPP advances SLMs by effectively leveraging dual-channel data, offering practical efficiency for real-time applications."}}
{"id": "2506.09393", "pdf": "https://arxiv.org/pdf/2506.09393", "abs": "https://arxiv.org/abs/2506.09393", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "categories": ["cs.CL"], "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings.", "AI": {"tldr": "KT$^2$ is a probabilistic knowledge tracing framework using a tree-structured hierarchy of concepts to improve performance in low-resource settings.", "motivation": "Existing KT methods struggle in low-resource, online classroom settings where data is sparse and updates are frequent.", "method": "KT$^2$ employs a Hidden Markov Tree Model to track student mastery hierarchically, using an EM algorithm and incremental updates.", "result": "KT$^2$ outperforms baselines in online, low-resource scenarios.", "conclusion": "The hierarchical approach of KT$^2$ effectively addresses low-resource challenges in knowledge tracing."}}
{"id": "2506.09102", "pdf": "https://arxiv.org/pdf/2506.09102", "abs": "https://arxiv.org/abs/2506.09102", "authors": ["Mihaela van der Schaar", "Richard Peck", "Eoin McKinney", "Jim Weatherall", "Stuart Bailey", "Justine Rochon", "Chris Anagnostopoulos", "Pierre Marquet", "Anthony Wood", "Nicky Best", "Harry Amad", "Julianna Piskorz", "Krzysztof Kacprzyk", "Rafik Salama", "Christina Gunther", "Francesca Frau", "Antoine Pugeat", "Ramon Hernandez"], "title": "Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "This manifesto represents a collaborative vision forged by leaders in\npharmaceuticals, consulting firms, clinical research, and AI. It outlines a\nroadmap for two AI technologies - causal inference and digital twins - to\ntransform clinical trials, delivering faster, safer, and more personalized\noutcomes for patients. By focusing on actionable integration within existing\nregulatory frameworks, we propose a way forward to revolutionize clinical\nresearch and redefine the gold standard for clinical trials using AI.", "AI": {"tldr": "A collaborative vision for using AI (causal inference and digital twins) to revolutionize clinical trials, ensuring faster, safer, and more personalized outcomes.", "motivation": "To transform clinical trials by integrating AI technologies within existing regulatory frameworks, improving efficiency and patient outcomes.", "method": "Focus on actionable integration of causal inference and digital twins into clinical trials.", "result": "Proposes a roadmap to redefine the gold standard for clinical trials using AI.", "conclusion": "AI can revolutionize clinical research by enhancing trial speed, safety, and personalization."}}
{"id": "2506.09163", "pdf": "https://arxiv.org/pdf/2506.09163", "abs": "https://arxiv.org/abs/2506.09163", "authors": ["Daniel Jenson", "Jhonathan Navott", "Piotr Grynfelder", "Mengyan Zhang", "Makkunda Sharma", "Elizaveta Semenova", "Seth Flaxman"], "title": "Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nWhile early architectures were developed primarily as a scalable alternative to\nGaussian Processes (GPs), modern NPs tackle far more complex and data hungry\napplications spanning geology, epidemiology, climate, and robotics. These\napplications have placed increasing pressure on the scalability of these\nmodels, with many architectures compromising accuracy for scalability. In this\npaper, we demonstrate that this tradeoff is often unnecessary, particularly\nwhen modeling fully or partially translation invariant processes. We propose a\nversatile new architecture, the Biased Scan Attention Transformer Neural\nProcess (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),\ngroup-invariant attention biases, and memory-efficient Biased Scan Attention\n(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models\nwhile often training in a fraction of the time, (2) exhibit translation\ninvariance, enabling learning at multiple resolutions simultaneously, (3)\ntransparently model processes that evolve in both space and time, (4) support\nhigh dimensional fixed effects, and (5) scale gracefully -- running inference\nwith over 1M test points with 100K context points in under a minute on a single\n24GB GPU.", "AI": {"tldr": "BSA-TNP is a scalable and accurate Neural Process model with translation invariance, efficient training, and high-dimensional support.", "motivation": "Address the tradeoff between accuracy and scalability in Neural Processes, especially for translation-invariant processes.", "method": "Introduces BSA-TNP with Kernel Regression Blocks, group-invariant attention biases, and Biased Scan Attention.", "result": "Matches/exceeds accuracy of top models, trains faster, handles high dimensions, and scales to 1M test points.", "conclusion": "BSA-TNP eliminates unnecessary tradeoffs, offering versatility and efficiency for complex applications."}}
{"id": "2506.09343", "pdf": "https://arxiv.org/pdf/2506.09343", "abs": "https://arxiv.org/abs/2506.09343", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025 Highlight", "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "AI": {"tldr": "The paper introduces CheckManual, a benchmark for manual-based appliance manipulation, addressing gaps in prior research by leveraging manuals for robot task planning.", "motivation": "Existing research lacks focus on using appliance manuals for robot manipulation, either limiting manuals to QA tasks or ignoring them entirely. This paper aims to bridge this gap.", "method": "The authors propose a data generation pipeline using CAD models to create manuals, then design challenges, metrics, and simulator environments for evaluation. They also introduce ManualPlan, a manipulation planning model.", "result": "CheckManual is established as a benchmark, and ManualPlan serves as a baseline model for manual-based appliance manipulation tasks.", "conclusion": "The work highlights the importance of manuals for robot manipulation and provides a foundational benchmark and model for future research."}}
{"id": "2501.03874", "pdf": "https://arxiv.org/pdf/2501.03874", "abs": "https://arxiv.org/abs/2501.03874", "authors": ["Ning Zhang", "Timothy Shea", "Arto Nurmikko"], "title": "Neuromorphic Optical Tracking and Imaging of Randomly Moving Targets through Strongly Scattering Media", "categories": ["cs.NE", "cs.CV", "cs.LG", "eess.IV"], "comment": "26 pages, 6 figures", "summary": "Tracking and acquiring simultaneous optical images of randomly moving targets\nobscured by scattering media remains a challenging problem of importance to\nmany applications that require precise object localization and identification.\nIn this work we develop an end-to-end neuromorphic optical engineering and\ncomputational approach to demonstrate how to track and image normally invisible\nobjects by combining an event detecting camera with a multistage neuromorphic\ndeep learning strategy. Photons emerging from dense scattering media are\ndetected by the event camera and converted to pixel-wise asynchronized spike\ntrains - a first step in isolating object-specific information from the\ndominant uninformative background. Spiking data is fed into a deep spiking\nneural network (SNN) engine where object tracking and image reconstruction are\nperformed by two separate yet interconnected modules running in parallel in\ndiscrete time steps over the event duration. Through benchtop experiments we\ndemonstrate tracking and imaging randomly moving objects in dense turbid media\nas well as image reconstruction of spatially stationary but optically dynamic\nobjects. Standardized character sets serve as representative proxies for\ngeometrically complex objects, underscoring the method's generality. The\nresults highlight the advantages of a fully neuromorphic approach in meeting a\nmajor imaging technology with high computational efficiency and low power\nconsumption.", "AI": {"tldr": "A neuromorphic approach using an event camera and deep spiking neural network (SNN) enables tracking and imaging of moving targets in scattering media with high efficiency and low power.", "motivation": "Tracking and imaging moving targets in scattering media is challenging but crucial for precise localization and identification in various applications.", "method": "Combines an event camera with a multistage neuromorphic deep learning strategy, using pixel-wise spike trains and a deep SNN for tracking and reconstruction.", "result": "Successfully demonstrated tracking and imaging of moving objects in dense turbid media and reconstruction of dynamic stationary objects.", "conclusion": "The neuromorphic approach offers high computational efficiency and low power consumption, addressing a major imaging challenge."}}
{"id": "2506.09408", "pdf": "https://arxiv.org/pdf/2506.09408", "abs": "https://arxiv.org/abs/2506.09408", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "AI": {"tldr": "Token Constraint Decoding (TCD) improves LLM robustness to input noise, boosting performance by up to 39% for weaker models.", "motivation": "LLMs are vulnerable to minor input perturbations despite strong MCQA performance.", "method": "Introduces TCD, an inference-time algorithm enforcing token-level alignment, paired with prompt engineering.", "result": "TCD restores degraded performance, with up to +39% gains for weaker models, and regularizes overconfident outputs.", "conclusion": "TCD is a practical, model-agnostic method for enhancing LLM robustness in real-world applications."}}
{"id": "2506.09107", "pdf": "https://arxiv.org/pdf/2506.09107", "abs": "https://arxiv.org/abs/2506.09107", "authors": ["Athena Vakali", "Ilias Dimitriadis"], "title": "FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines", "categories": ["cs.CY", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "AI models have become active decision makers, often acting without human\nsupervision. The rapid advancement of AI technology has already caused harmful\nincidents that have hurt individuals and societies and AI unfairness in heavily\ncriticized. It is urgent to disrupt AI pipelines which largely neglect human\nprinciples and focus on computational biases exploration at the data (pre),\nmodel(in), and deployment (post) processing stages. We claim that by exploiting\nthe advances of agents technology, we will introduce cautious, prompt, and\nongoing fairness watch schemes, under realistic, systematic, and human-centric\nfairness expectations. We envision agents as fairness guardians, since agents\nlearn from their environment, adapt to new information, and solve complex\nproblems by interacting with external tools and other systems. To set the\nproper fairness guardrails in the overall AI pipeline, we introduce a\nfairness-by-design approach which embeds multi-role agents in an end-to-end\n(human to AI) synergetic scheme. Our position is that we may design adaptive\nand realistic AI fairness frameworks, and we introduce a generalized algorithm\nwhich can be customized to the requirements and goals of each AI decision\nmaking scenario. Our proposed, so called FAIRTOPIA framework, is structured\nover a three-layered architecture, which encapsulates the AI pipeline inside an\nagentic guardian and a knowledge-based, self-refining layered scheme. Based on\nour proposition, we enact fairness watch in all of the AI pipeline stages,\nunder robust multi-agent workflows, which will inspire new fairness research\nhypothesis, heuristics, and methods grounded in human-centric, systematic,\ninterdisciplinary, socio-technical principles.", "AI": {"tldr": "The paper proposes FAIRTOPIA, a framework using multi-role agents to ensure fairness in AI pipelines by embedding human-centric principles at all stages.", "motivation": "Addressing AI unfairness and harmful incidents caused by unsupervised AI decision-making, the paper emphasizes the need for human-centric fairness in AI pipelines.", "method": "Introduces a fairness-by-design approach with multi-role agents in an end-to-end scheme, featuring a three-layered architecture (FAIRTOPIA) for adaptive fairness frameworks.", "result": "Proposes a generalized algorithm for customizable fairness in AI scenarios, ensuring fairness watch across all pipeline stages.", "conclusion": "FAIRTOPIA aims to inspire new fairness research grounded in human-centric, interdisciplinary principles, ensuring robust AI fairness."}}
{"id": "2506.09171", "pdf": "https://arxiv.org/pdf/2506.09171", "abs": "https://arxiv.org/abs/2506.09171", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 68T20, 68T30, 93E35", "I.2.6; I.2.7; I.2.8"], "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld.", "AI": {"tldr": "A novel LLM agent framework enhances planning via in-context learning, atomic fact augmentation, and recursive lookahead search, improving adaptability and performance in interactive tasks.", "motivation": "Existing LLMs struggle with adapting to new information and multi-step reasoning without fine-tuning, requiring a more efficient method for leveraging past experiences.", "method": "The framework uses atomic fact augmentation and recursive lookahead search to dynamically improve prompts for action proposal, world simulation, and state-value estimation.", "result": "The agent shows improved performance and adaptability in tasks like TextFrozenLake and ALFWorld, refining behavior without weight updates.", "conclusion": "The proposed framework effectively enhances LLM planning and decision-making in interactive environments by leveraging experience and dynamic fact-based augmentation."}}
{"id": "2506.09345", "pdf": "https://arxiv.org/pdf/2506.09345", "abs": "https://arxiv.org/abs/2506.09345", "authors": ["Songping Wang", "Xiantao Hu", "Yueming Lyu", "Caifeng Shan"], "title": "An Effective End-to-End Solution for Multimodal Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, multimodal tasks have strongly advanced the field of action\nrecognition with their rich multimodal information. However, due to the\nscarcity of tri-modal data, research on tri-modal action recognition tasks\nfaces many challenges. To this end, we have proposed a comprehensive multimodal\naction recognition solution that effectively utilizes multimodal information.\nFirst, the existing data are transformed and expanded by optimizing data\nenhancement techniques to enlarge the training scale. At the same time, more\nRGB datasets are used to pre-train the backbone network, which is better\nadapted to the new task by means of transfer learning. Secondly, multimodal\nspatial features are extracted with the help of 2D CNNs and combined with the\nTemporal Shift Module (TSM) to achieve multimodal spatial-temporal feature\nextraction comparable to 3D CNNs and improve the computational efficiency. In\naddition, common prediction enhancement methods, such as Stochastic Weight\nAveraging (SWA), Ensemble and Test-Time augmentation (TTA), are used to\nintegrate the knowledge of models from different training periods of the same\narchitecture and different architectures, so as to predict the actions from\ndifferent perspectives and fully exploit the target information. Ultimately, we\nachieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the\ncompetition leaderboard, demonstrating the superiority of our solution.", "AI": {"tldr": "A tri-modal action recognition solution using data enhancement, transfer learning, and multimodal spatial-temporal feature extraction achieves top accuracy.", "motivation": "Addressing challenges in tri-modal action recognition due to scarce data by leveraging multimodal information.", "method": "Data enhancement, transfer learning with RGB datasets, 2D CNNs with TSM for spatial-temporal features, and prediction enhancement techniques like SWA, Ensemble, and TTA.", "result": "Achieved Top-1 accuracy of 99% and Top-5 accuracy of 100% on the competition leaderboard.", "conclusion": "The proposed solution demonstrates superiority in tri-modal action recognition by effectively utilizing multimodal information and advanced techniques."}}
{"id": "2506.01234", "pdf": "https://arxiv.org/pdf/2506.01234", "abs": "https://arxiv.org/abs/2506.01234", "authors": ["Woojin Cho", "Steve Andreas Immanuel", "Junhyuk Heo", "Darongsae Kwon"], "title": "Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Accepted to IGARSS 2025 (Oral)", "summary": "Multispectral satellite images play a vital role in agriculture, fisheries,\nand environmental monitoring. However, their high dimensionality, large data\nvolumes, and diverse spatial resolutions across multiple channels pose\nsignificant challenges for data compression and analysis. This paper presents\nImpliSat, a unified framework specifically designed to address these challenges\nthrough efficient compression and reconstruction of multispectral satellite\ndata. ImpliSat leverages Implicit Neural Representations (INR) to model\nsatellite images as continuous functions over coordinate space, capturing fine\nspatial details across varying spatial resolutions. Furthermore, we introduce a\nFourier modulation algorithm that dynamically adjusts to the spectral and\nspatial characteristics of each band, ensuring optimal compression while\npreserving critical image details.", "AI": {"tldr": "ImpliSat is a framework using Implicit Neural Representations (INR) and Fourier modulation for efficient compression and reconstruction of multispectral satellite data.", "motivation": "High dimensionality, large data volumes, and diverse spatial resolutions in multispectral satellite images pose challenges for compression and analysis.", "method": "Leverages INR to model images as continuous functions and introduces a Fourier modulation algorithm for dynamic adjustment to spectral and spatial characteristics.", "result": "Efficient compression and reconstruction while preserving critical image details.", "conclusion": "ImpliSat effectively addresses challenges in multispectral satellite data handling."}}
{"id": "2506.09414", "pdf": "https://arxiv.org/pdf/2506.09414", "abs": "https://arxiv.org/abs/2506.09414", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "AI": {"tldr": "PGDA-KGQA is a prompt-guided generative framework for KGQA that enhances training data diversity and multi-hop reasoning through innovative augmentation strategies, outperforming existing methods.", "motivation": "Addressing the limitations of current KGQA methods, such as data scarcity, lack of multi-hop reasoning samples, and semantic distortion, to improve model generalization and accuracy.", "method": "PGDA-KGQA uses a unified prompt-design paradigm with LLMs to generate diverse (question, logical form) pairs. It includes single-hop pseudo questions, semantic-preserving rewriting, and answer-guided reverse path exploration for multi-hop questions.", "result": "Achieves improvements on WebQSP (2.8% F1, 1.2% Hits@1, 3.1% Accuracy) and ComplexWebQuestions (1.8% F1, 1.1% Hits@1, 2.4% Accuracy).", "conclusion": "PGDA-KGQA effectively addresses data diversity and multi-hop reasoning challenges, enhancing KGQA performance."}}
{"id": "2506.09160", "pdf": "https://arxiv.org/pdf/2506.09160", "abs": "https://arxiv.org/abs/2506.09160", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "title": "Understanding Human-AI Trust in Education", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "As AI chatbots become increasingly integrated in education, students are\nturning to these systems for guidance, feedback, and information. However, the\nanthropomorphic characteristics of these chatbots create ambiguity regarding\nwhether students develop trust toward them as they would a human peer or\ninstructor, based in interpersonal trust, or as they would any other piece of\ntechnology, based in technology trust. This ambiguity presents theoretical\nchallenges, as interpersonal trust models may inappropriately ascribe human\nintentionality and morality to AI, while technology trust models were developed\nfor non-social technologies, leaving their applicability to anthropomorphic\nsystems unclear. To address this gap, we investigate how human-like and\nsystem-like trusting beliefs comparatively influence students' perceived\nenjoyment, trusting intention, behavioral intention to use, and perceived\nusefulness of an AI chatbot - factors associated with students' engagement and\nlearning outcomes. Through partial least squares structural equation modeling,\nwe found that human-like and system-like trust significantly influenced student\nperceptions, with varied effects. Human-like trust more strongly predicted\ntrusting intention, while system-like trust better predicted behavioral\nintention and perceived usefulness. Both had similar effects on perceived\nenjoyment. Given the partial explanatory power of each type of trust, we\npropose that students develop a distinct form of trust with AI chatbots\n(human-AI trust) that differs from human-human and human-technology models of\ntrust. Our findings highlight the need for new theoretical frameworks specific\nto human-AI trust and offer practical insights for fostering appropriately\ncalibrated trust, which is critical for the effective adoption and pedagogical\nimpact of AI in education.", "AI": {"tldr": "The paper explores how students' trust in AI chatbots (human-like vs. system-like) affects their engagement and learning outcomes, proposing a distinct 'human-AI trust' model.", "motivation": "To address ambiguity in whether students trust AI chatbots as humans or technology, given the anthropomorphic traits of chatbots and their growing role in education.", "method": "Partial least squares structural equation modeling to analyze how human-like and system-like trust influence student perceptions (enjoyment, intention, usefulness).", "result": "Human-like trust predicts trusting intention, while system-like trust predicts behavioral intention and usefulness. Both similarly affect enjoyment, suggesting a unique 'human-AI trust' model.", "conclusion": "A new theoretical framework for human-AI trust is needed, with practical implications for AI's effective use in education."}}
{"id": "2506.09172", "pdf": "https://arxiv.org/pdf/2506.09172", "abs": "https://arxiv.org/abs/2506.09172", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Harshvardhan Sikka"], "title": "MultiNet: An Open-Source Software Toolkit \\& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models", "categories": ["cs.LG", "cs.CV"], "comment": "ICML CodeML Workshop, 13 Pages, 6 Figures, 2 Tables", "summary": "Recent innovations in multimodal action models represent a promising\ndirection for developing general-purpose agentic systems, combining visual\nunderstanding, language comprehension, and action generation. We introduce\nMultiNet - a novel, fully open-source benchmark and surrounding software\necosystem designed to rigorously evaluate and adapt models across vision,\nlanguage, and action domains. We establish standardized evaluation protocols\nfor assessing vision-language models (VLMs) and vision-language-action models\n(VLAs), and provide open source software to download relevant data, models, and\nevaluations. Additionally, we provide a composite dataset with over 1.3\ntrillion tokens of image captioning, visual question answering, commonsense\nreasoning, robotic control, digital game-play, simulated\nlocomotion/manipulation, and many more tasks. The MultiNet benchmark,\nframework, toolkit, and evaluation harness have been used in downstream\nresearch on the limitations of VLA generalization.", "AI": {"tldr": "MultiNet is an open-source benchmark for evaluating multimodal action models (VLMs/VLAs) with standardized protocols, a large dataset, and tools for research.", "motivation": "To advance general-purpose agentic systems by rigorously evaluating and adapting models across vision, language, and action domains.", "method": "Introduces MultiNet, a benchmark with standardized evaluation protocols, open-source software, and a composite dataset (1.3T tokens) for diverse tasks.", "result": "MultiNet supports downstream research on VLA generalization limitations.", "conclusion": "MultiNet provides a comprehensive framework for evaluating and improving multimodal action models."}}
{"id": "2506.09350", "pdf": "https://arxiv.org/pdf/2506.09350", "abs": "https://arxiv.org/abs/2506.09350", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "AI": {"tldr": "AAPT transforms a pre-trained latent video diffusion model into a real-time, interactive video generator using autoregressive adversarial post-training.", "motivation": "Existing video generation models are too slow for real-time use, limiting adoption in interactive applications.", "method": "Proposes autoregressive adversarial post-training (AAPT) for one-step latent frame generation, leveraging adversarial training and KV cache efficiency.", "result": "Achieves real-time 24fps video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 for up to a minute.", "conclusion": "AAPT enables efficient, real-time, and interactive video generation with reduced error accumulation."}}
{"id": "2506.09424", "pdf": "https://arxiv.org/pdf/2506.09424", "abs": "https://arxiv.org/abs/2506.09424", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications.", "AI": {"tldr": "The paper evaluates LLMs and LMMs for automated deception detection across diverse datasets, showing fine-tuned LLMs excel in textual tasks while LMMs struggle with cross-modal cues.", "motivation": "Detecting deception digitally is critical but challenging, necessitating evaluation of advanced models like LLMs and LMMs.", "method": "Assessed open-source and commercial LLMs on datasets (RLTD, MU3D, OpSpam) using zero-shot, few-shot, and fine-tuned approaches. Analyzed auxiliary features and prompting strategies.", "result": "Fine-tuned LLMs achieve state-of-the-art performance in textual deception detection; LMMs underperform with cross-modal cues.", "conclusion": "LLMs show promise for real-world deception detection but have limitations, especially in multimodal contexts."}}
{"id": "2506.09167", "pdf": "https://arxiv.org/pdf/2506.09167", "abs": "https://arxiv.org/abs/2506.09167", "authors": ["James R. Williamson", "Andrew Alini", "Brian A. Telfer", "Adam W. Potter", "Karl E. Friedl"], "title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry", "categories": ["eess.SP", "cs.AI", "q-bio.QM"], "comment": "13 pages", "summary": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks.", "AI": {"tldr": "The paper explores the relationship between visceral adipose tissue (VAT) and physical activity (PA), using NHANES data and advanced modeling techniques to estimate VAT from PA, achieving high accuracy.", "motivation": "Excess VAT is linked to metabolic health risks like type 2 diabetes. Understanding its relationship with PA could improve health monitoring.", "method": "Two approaches: 1) Engineered features and ridge regression, 2) Deep neural networks and transformers, both enhanced with demographic covariates.", "result": "Combined methods achieved high accuracy (r=0.86) in estimating VAT from PA, showing a strong PA-VAT relationship.", "conclusion": "PA strongly correlates with VAT, highlighting its role in metabolic health risks and potential for non-invasive monitoring."}}
{"id": "2506.09173", "pdf": "https://arxiv.org/pdf/2506.09173", "abs": "https://arxiv.org/abs/2506.09173", "authors": ["Michael Cooper", "Rohan Wadhawan", "John Michael Giorgi", "Chenhao Tan", "Davis Liang"], "title": "The Curious Language Model: Strategic Test-Time Information Acquisition", "categories": ["cs.LG", "cs.CL"], "comment": "39 pages", "summary": "Decision-makers often possess insufficient information to render a confident\ndecision. In these cases, the decision-maker can often undertake actions to\nacquire the necessary information about the problem at hand, e.g., by\nconsulting knowledgeable authorities or by conducting experiments. Importantly,\ndifferent levers of information acquisition come with different costs, posing\nthe challenge of selecting the actions that are both informative and\ncost-effective. In this work, we propose CuriosiTree, a heuristic-based,\ntest-time policy for zero-shot information acquisition in large language models\n(LLMs). CuriosiTree employs a greedy tree search to estimate the expected\ninformation gain of each action and strategically chooses actions based on a\nbalance of anticipated information gain and associated cost. Empirical\nvalidation in a clinical diagnosis simulation shows that CuriosiTree enables\ncost-effective integration of heterogenous sources of information, and\noutperforms baseline action selection strategies in selecting action sequences\nthat enable accurate diagnosis.", "AI": {"tldr": "CuriosiTree is a heuristic-based policy for zero-shot information acquisition in LLMs, using greedy tree search to balance information gain and cost, outperforming baselines in clinical diagnosis.", "motivation": "Decision-makers often lack sufficient information and need cost-effective ways to acquire it, especially in scenarios like clinical diagnosis.", "method": "CuriosiTree employs a greedy tree search to estimate expected information gain and strategically selects actions based on gain-cost balance.", "result": "Empirical validation shows CuriosiTree outperforms baselines in selecting cost-effective action sequences for accurate diagnosis.", "conclusion": "CuriosiTree effectively integrates diverse information sources, enhancing decision-making in information-scarce scenarios."}}
{"id": "2506.09357", "pdf": "https://arxiv.org/pdf/2506.09357", "abs": "https://arxiv.org/abs/2506.09357", "authors": ["Junchao Zhou"], "title": "A new approach for image segmentation based on diffeomorphic registration and gradient fields", "categories": ["cs.CV"], "comment": null, "summary": "Image segmentation is a fundamental task in computer vision aimed at\ndelineating object boundaries within images. Traditional approaches, such as\nedge detection and variational methods, have been widely explored, while recent\nadvances in deep learning have shown promising results but often require\nextensive training data. In this work, we propose a novel variational framework\nfor 2D image segmentation that integrates concepts from shape analysis and\ndiffeomorphic transformations. Our method models segmentation as the\ndeformation of a template curve via a diffeomorphic transformation of the image\ndomain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. The curve evolution is guided by a loss function that compares the\ndeformed curve to the image gradient field, formulated through the varifold\nrepresentation of geometric shapes. The approach is implemented in Python with\nGPU acceleration using the PyKeops library. This framework allows for accurate\nsegmentation with a flexible and theoretically grounded methodology that does\nnot rely on large datasets.", "AI": {"tldr": "A novel variational framework for 2D image segmentation using shape analysis and diffeomorphic transformations, requiring minimal training data.", "motivation": "Traditional methods and deep learning approaches have limitations; the former lack flexibility, while the latter need large datasets.", "method": "Uses LDDMM framework for curve deformation guided by a loss function comparing the curve to the image gradient field, implemented in Python with GPU acceleration.", "result": "Accurate segmentation with a flexible, theoretically grounded methodology.", "conclusion": "The proposed framework offers a data-efficient and theoretically sound alternative for image segmentation."}}
{"id": "2506.09428", "pdf": "https://arxiv.org/pdf/2506.09428", "abs": "https://arxiv.org/abs/2506.09428", "authors": ["Fei Ding", "Baiqiao Wang"], "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "AI": {"tldr": "A novel SFT method reduces catastrophic forgetting in LLMs without needing original pre-training data, preserving generalization while improving task-specific performance.", "motivation": "Address the issue of catastrophic forgetting and diminished general capabilities in LLMs during SFT, especially when original data is inaccessible.", "method": "Reconstructs the likely SFT instruction distribution, uses multi-model screening to select optimal data, and mixes it with new data for SFT.", "result": "Preserves generalization capabilities in general domains while enhancing task-specific performance.", "conclusion": "The proposed method offers a cost-effective solution to mitigate catastrophic forgetting in SFT without relying on original data."}}
{"id": "2506.09183", "pdf": "https://arxiv.org/pdf/2506.09183", "abs": "https://arxiv.org/abs/2506.09183", "authors": ["Mingkang Wu", "Devin White", "Evelyn Rose", "Vernon Lawhern", "Nicholas R Waytowich", "Yongcan Cao"], "title": "Multi-Task Reward Learning from Human Ratings", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the workshop on Models of Human Feedback for AI Alignment\n  at the 42nd International Conference on Machine Learning", "summary": "Reinforcement learning from human feeback (RLHF) has become a key factor in\naligning model behavior with users' goals. However, while humans integrate\nmultiple strategies when making decisions, current RLHF approaches often\nsimplify this process by modeling human reasoning through isolated tasks such\nas classification or regression. In this paper, we propose a novel\nreinforcement learning (RL) method that mimics human decision-making by jointly\nconsidering multiple tasks. Specifically, we leverage human ratings in\nreward-free environments to infer a reward function, introducing learnable\nweights that balance the contributions of both classification and regression\nmodels. This design captures the inherent uncertainty in human decision-making\nand allows the model to adaptively emphasize different strategies. We conduct\nseveral experiments using synthetic human ratings to validate the effectiveness\nof the proposed approach. Results show that our method consistently outperforms\nexisting rating-based RL methods, and in some cases, even surpasses traditional\nRL approaches.", "AI": {"tldr": "A novel RL method mimics human decision-making by jointly considering multiple tasks, outperforming existing rating-based RL methods.", "motivation": "Current RLHF approaches oversimplify human reasoning by isolating tasks, missing the complexity of human decision-making.", "method": "Proposes a reward-free RL method using human ratings, with learnable weights balancing classification and regression models.", "result": "Outperforms existing rating-based RL methods and sometimes traditional RL approaches.", "conclusion": "The method effectively captures human decision-making uncertainty and adapts strategies, improving alignment with user goals."}}
{"id": "2506.09174", "pdf": "https://arxiv.org/pdf/2506.09174", "abs": "https://arxiv.org/abs/2506.09174", "authors": ["Chenheng Xu", "Dan Wu", "Yixin Zhu", "Ying Nian Wu"], "title": "Multivariate Long-term Time Series Forecasting with Fourier Neural Filter", "categories": ["cs.LG"], "comment": null, "summary": "Multivariate long-term time series forecasting has been suffering from the\nchallenge of capturing both temporal dependencies within variables and spatial\ncorrelations across variables simultaneously. Current approaches predominantly\nrepurpose backbones from natural language processing or computer vision (e.g.,\nTransformers), which fail to adequately address the unique properties of time\nseries (e.g., periodicity). The research community lacks a dedicated backbone\nwith temporal-specific inductive biases, instead relying on domain-agnostic\nbackbones supplemented with auxiliary techniques (e.g., signal decomposition).\nWe introduce FNF as the backbone and DBD as the architecture to provide\nexcellent learning capabilities and optimal learning pathways for\nspatio-temporal modeling, respectively. Our theoretical analysis proves that\nFNF unifies local time-domain and global frequency-domain information\nprocessing within a single backbone that extends naturally to spatial modeling,\nwhile information bottleneck theory demonstrates that DBD provides superior\ngradient flow and representation capacity compared to existing unified or\nsequential architectures. Our empirical evaluation across 11 public benchmark\ndatasets spanning five domains (energy, meteorology, transportation,\nenvironment, and nature) confirms state-of-the-art performance with consistent\nhyperparameter settings. Notably, our approach achieves these results without\nany auxiliary techniques, suggesting that properly designed neural\narchitectures can capture the inherent properties of time series, potentially\ntransforming time series modeling in scientific and industrial applications.", "AI": {"tldr": "The paper introduces FNF and DBD as dedicated backbones for spatio-temporal modeling in multivariate time series forecasting, addressing limitations of current domain-agnostic approaches.", "motivation": "Current methods repurpose NLP/CV backbones (e.g., Transformers) without addressing time series-specific properties like periodicity, lacking dedicated architectures.", "method": "Proposes FNF (unifying local/global time-frequency processing) and DBD (optimizing gradient flow/representation) for spatio-temporal modeling.", "result": "Achieves state-of-the-art performance on 11 datasets across five domains without auxiliary techniques.", "conclusion": "Properly designed architectures (FNF/DBD) can inherently capture time series properties, potentially transforming forecasting applications."}}
{"id": "2506.09363", "pdf": "https://arxiv.org/pdf/2506.09363", "abs": "https://arxiv.org/abs/2506.09363", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "AI": {"tldr": "SAGE introduces semantic-augment erasing and a global-local retention mechanism to improve concept erasure in diffusion models, ensuring safer text-to-image generation.", "motivation": "To address safety risks like unsafe content and copyright infringement in diffusion models by escaping the 'word concept abyss' and enabling generalized concept erasure.", "method": "Semantic-augment erasing transforms word erasure into domain erasure via cyclic self-check and self-erasure, while a global-local retention mechanism preserves irrelevant concepts.", "result": "SAGE outperforms existing methods in safe generation, demonstrating comprehensive superiority.", "conclusion": "SAGE effectively unlearns unsafe concepts while retaining irrelevant ones, enhancing the safety of diffusion models."}}
{"id": "2506.09440", "pdf": "https://arxiv.org/pdf/2506.09440", "abs": "https://arxiv.org/abs/2506.09440", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "categories": ["cs.CL", "cs.AI"], "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "AI": {"tldr": "The paper introduces the GigaChat family of Russian LLMs, detailing their architecture, training, and performance on benchmarks, with open-source models released for broader use.", "motivation": "Address the lack of foundational Russian LLMs due to high computational costs, aiming to expand NLP research and industrial applications.", "method": "Developed GigaChat models in various sizes, including base and instruction-tuned versions, with detailed architecture and training processes.", "result": "Evaluated performance on Russian and English benchmarks, compared with multilingual models, and demonstrated top models via API, Telegram bot, and Web interface.", "conclusion": "Released open-source GigaChat models to support Russian NLP research and industrial solutions, filling a critical gap in the field."}}
{"id": "2506.09194", "pdf": "https://arxiv.org/pdf/2506.09194", "abs": "https://arxiv.org/abs/2506.09194", "authors": ["Emirhan Bilgi\u00e7", "Neslihan Serap \u015eeng\u00f6r", "Nam\u0131k Berk Yalab\u0131k", "Yavuz Selim \u0130\u015fler", "Aykut G\u00f6rkem Gelen", "Rahmi Elibol"], "title": "Integration of Contrastive Predictive Coding and Spiking Neural Networks", "categories": ["eess.SP", "cs.AI"], "comment": "4 pages, 5 figures, 1 table. Accepted at the 2025 33rd Signal\n  Processing and Communications Applications Conference (SIU)", "summary": "This study examines the integration of Contrastive Predictive Coding (CPC)\nwith Spiking Neural Networks (SNN). While CPC learns the predictive structure\nof data to generate meaningful representations, SNN mimics the computational\nprocesses of biological neural systems over time. In this study, the goal is to\ndevelop a predictive coding model with greater biological plausibility by\nprocessing inputs and outputs in a spike-based system. The proposed model was\ntested on the MNIST dataset and achieved a high classification rate in\ndistinguishing positive sequential samples from non-sequential negative\nsamples. The study demonstrates that CPC can be effectively combined with SNN,\nshowing that an SNN trained for classification tasks can also function as an\nencoding mechanism. Project codes and detailed results can be accessed on our\nGitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN", "AI": {"tldr": "The study integrates Contrastive Predictive Coding (CPC) with Spiking Neural Networks (SNN) to create a biologically plausible predictive coding model, achieving high classification accuracy on MNIST.", "motivation": "To develop a predictive coding model with greater biological plausibility by combining CPC's predictive learning with SNN's spike-based processing.", "method": "Integration of CPC with SNN, tested on the MNIST dataset to classify sequential vs. non-sequential samples.", "result": "High classification rate, demonstrating effective combination of CPC and SNN for both classification and encoding.", "conclusion": "CPC can be successfully integrated with SNN, enhancing biological plausibility and functionality in predictive tasks."}}
{"id": "2506.09193", "pdf": "https://arxiv.org/pdf/2506.09193", "abs": "https://arxiv.org/abs/2506.09193", "authors": ["Yilin Zhuang", "Karthik Duraisamy"], "title": "LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Accurate probabilistic weather forecasting demands both high accuracy and\nefficient uncertainty quantification, challenges that overburden both ensemble\nnumerical weather prediction (NWP) and recent machine-learning methods. We\nintroduce LaDCast, the first global latent-diffusion framework for medium-range\nensemble forecasting, which generates hourly ensemble forecasts entirely in a\nlearned latent space. An autoencoder compresses high-dimensional ERA5\nreanalysis fields into a compact representation, and a transformer-based\ndiffusion model produces sequential latent updates with arbitrary hour\ninitialization. The model incorporates Geometric Rotary Position Embedding\n(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream\nattention mechanism for efficient conditioning, and sinusoidal temporal\nembeddings to capture seasonal patterns. LaDCast achieves deterministic and\nprobabilistic skill close to that of the European Centre for Medium-Range\nForecast IFS-ENS, without any explicit perturbations. Notably, LaDCast\ndemonstrates superior performance in tracking rare extreme events such as\ncyclones, capturing their trajectories more accurately than established models.\nBy operating in latent space, LaDCast reduces storage and compute by orders of\nmagnitude, demonstrating a practical path toward forecasting at kilometer-scale\nresolution in real time. We open-source our code and models and provide the\ntraining and evaluation pipelines at: https://github.com/tonyzyl/ladcast.", "AI": {"tldr": "LaDCast is a global latent-diffusion framework for medium-range ensemble weather forecasting, achieving accuracy and efficiency by operating in a learned latent space.", "motivation": "Addressing the challenges of high accuracy and efficient uncertainty quantification in weather forecasting, which overburden traditional methods like ensemble NWP and machine-learning approaches.", "method": "Uses an autoencoder to compress ERA5 reanalysis fields into a latent space, a transformer-based diffusion model for sequential updates, and incorporates GeoRoPE, dual-stream attention, and temporal embeddings.", "result": "Matches the deterministic and probabilistic skill of IFS-ENS, excels in tracking extreme events like cyclones, and reduces storage/compute requirements significantly.", "conclusion": "LaDCast offers a practical solution for high-resolution real-time forecasting, with open-sourced code and models for broader use."}}
{"id": "2506.09369", "pdf": "https://arxiv.org/pdf/2506.09369", "abs": "https://arxiv.org/abs/2506.09369", "authors": ["Zeran Ke", "Bin Tan", "Xianwei Zheng", "Yujun Shen", "Tianfu Wu", "Nan Xue"], "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined", "categories": ["cs.CV"], "comment": "accepted to CVPR 2025; 17 pages, appendices included", "summary": "This paper studies the problem of Line Segment Detection (LSD) for the\ncharacterization of line geometry in images, with the aim of learning a\ndomain-agnostic robust LSD model that works well for any natural images. With\nthe focus of scalable self-supervised learning of LSD, we revisit and\nstreamline the fundamental designs of (deep and non-deep) LSD approaches to\nhave a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the\ncuration of line geometry at scale from over 10M unlabeled real-world images.\nOur ScaleLSD works very well to detect much more number of line segments from\nany natural images even than the pioneered non-deep LSD approach, having a more\ncomplete and accurate geometric characterization of images using line segments.\nExperimentally, our proposed ScaleLSD is comprehensively testified under\nzero-shot protocols in detection performance, single-view 3D geometry\nestimation, two-view line segment matching, and multiview 3D line mapping, all\nwith excellent performance obtained. Based on the thorough evaluation, our\nScaleLSD is observed to be the first deep approach that outperforms the\npioneered non-deep LSD in all aspects we have tested, significantly expanding\nand reinforcing the versatility of the line geometry of images. Code and Models\nare available at https://github.com/ant-research/scalelsd", "AI": {"tldr": "ScaleLSD is a scalable self-supervised learning model for Line Segment Detection (LSD) that outperforms non-deep LSD methods in accuracy and versatility.", "motivation": "To develop a domain-agnostic, robust LSD model that works well for any natural images by leveraging scalable self-supervised learning.", "method": "Revisits and streamlines fundamental LSD designs to create ScaleLSD, a high-performing and efficient learner trained on over 10M unlabeled images.", "result": "ScaleLSD detects more line segments accurately and outperforms non-deep LSD in zero-shot detection, 3D geometry estimation, and multiview mapping.", "conclusion": "ScaleLSD is the first deep LSD approach to surpass non-deep methods in all tested aspects, enhancing image line geometry versatility."}}
{"id": "2506.09450", "pdf": "https://arxiv.org/pdf/2506.09450", "abs": "https://arxiv.org/abs/2506.09450", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "AI": {"tldr": "UniToMBench is a unified benchmark for evaluating Theory of Mind (ToM) in LLMs, combining SimToM and TOMBENCH strengths with multi-interaction tasks and evolving scenarios. It shows GPT-4o models excel in emotional/belief tasks but vary in knowledge-based tasks.", "motivation": "To address the challenge of LLMs accurately predicting human mental states (ToM) by creating a systematic benchmark.", "method": "Developed UniToMBench with 1,000 hand-written scenarios, integrating perspective-taking techniques and diverse metrics.", "result": "GPT-4o models achieve >80% accuracy in emotional/belief tasks but show variability in knowledge-based tasks.", "conclusion": "UniToMBench is a valuable tool for assessing and improving ToM capabilities in LLMs, revealing current strengths and limitations."}}
{"id": "2506.09199", "pdf": "https://arxiv.org/pdf/2506.09199", "abs": "https://arxiv.org/abs/2506.09199", "authors": ["Hariharan Ramesh", "Jyotikrishna Dass"], "title": "FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "21 pages, 12 figures", "summary": "Integrating Low-Rank Adaptation (LoRA) into federated learning offers a\npromising solution for parameter-efficient fine-tuning of Large Language Models\n(LLMs) without sharing local data. However, several methods designed for\nfederated LoRA present significant challenges in balancing communication\nefficiency, model accuracy, and computational cost, particularly among\nheterogeneous clients. These methods either rely on simplistic averaging of\nlocal adapters, which introduces aggregation noise, require transmitting large\nstacked local adapters, leading to poor communication efficiency, or\nnecessitate reconstructing memory-dense global weight-update matrix and\nperforming computationally expensive decomposition to design client-specific\nlow-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning\nframework that achieves mathematically accurate aggregation without incurring\nhigh communication or computational overhead. Instead of constructing the full\nglobal weight-update matrix at the server, FLoRIST employs an efficient\ndecomposition pipeline by performing singular value decomposition on stacked\nlocal adapters separately. This approach operates within a compact intermediate\nspace to represent the accumulated information from local LoRAs. We introduce\ntunable singular value thresholding for server-side optimal rank selection to\nconstruct a pair of global low-rank adapters shared by all clients. Extensive\nempirical evaluations across multiple datasets and LLMs demonstrate that\nFLoRIST consistently strikes the best balance between superior communication\nefficiency and competitive performance in both homogeneous and heterogeneous\nsetups.", "AI": {"tldr": "FLoRIST is a federated fine-tuning framework for LLMs using LoRA, addressing challenges in communication efficiency, accuracy, and computational cost by avoiding full global weight-update matrix construction and using efficient decomposition.", "motivation": "Existing federated LoRA methods struggle with balancing communication efficiency, model accuracy, and computational cost, especially in heterogeneous client setups.", "method": "FLoRIST employs an efficient decomposition pipeline via singular value decomposition on stacked local adapters separately, avoiding full global weight-update matrix construction.", "result": "FLoRIST achieves superior communication efficiency and competitive performance across datasets and LLMs in homogeneous and heterogeneous setups.", "conclusion": "FLoRIST effectively balances communication efficiency and performance, making it a robust solution for federated fine-tuning of LLMs."}}
{"id": "2506.09200", "pdf": "https://arxiv.org/pdf/2506.09200", "abs": "https://arxiv.org/abs/2506.09200", "authors": ["Val Andrei Fajardo", "David B. Emerson", "Amandeep Singh", "Veronica Chatrath", "Marcelo Lotif", "Ravi Theja", "Alex Cheung", "Izuki Matsubi"], "title": "FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems", "categories": ["cs.LG", "cs.CL"], "comment": "9 pages, 4 figures, 2 tables. Accepted for the CODEML Workshop at\n  ICML 2025. Framework code available at\n  https://github.com/VectorInstitute/fed-rag", "summary": "Retrieval-augmented generation (RAG) systems have been shown to be effective\nin addressing many of the drawbacks of relying solely on the parametric memory\nof large language models. Recent work has demonstrated that RAG systems can be\nimproved via fine-tuning of their retriever and generator models. In this work,\nwe introduce FedRAG, a framework for fine-tuning RAG systems across centralized\nand federated architectures. FedRAG supports state-of-the-art fine-tuning\nmethods, offering a simple and intuitive interface and a seamless conversion\nfrom centralized to federated training tasks. FedRAG is also deeply integrated\nwith the modern RAG ecosystem, filling a critical gap in available tools.", "AI": {"tldr": "FedRAG introduces a framework for fine-tuning RAG systems across centralized and federated architectures, improving retrieval-augmented generation.", "motivation": "To address the limitations of relying solely on parametric memory in large language models by enhancing RAG systems through fine-tuning.", "method": "FedRAG supports state-of-the-art fine-tuning methods for retriever and generator models, offering a simple interface and seamless transition between centralized and federated training.", "result": "FedRAG fills a critical gap in the RAG ecosystem by providing a tool for fine-tuning in both centralized and federated settings.", "conclusion": "FedRAG is a valuable addition to the RAG ecosystem, enabling improved performance through flexible fine-tuning across architectures."}}
{"id": "2506.09378", "pdf": "https://arxiv.org/pdf/2506.09378", "abs": "https://arxiv.org/abs/2506.09378", "authors": ["Qijian Tian", "Xin Tan", "Jingyu Gong", "Yuan Xie", "Lizhuang Ma"], "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images", "categories": ["cs.CV"], "comment": null, "summary": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and\nsemantic field reconstruction. Combining 3D scenes with semantic fields\nfacilitates the perception and understanding of the surrounding environment.\nHowever, key challenges include embedding semantics into 3D representations,\nachieving generalizable real-time reconstruction, and ensuring practical\napplicability by using only images as input without camera parameters or ground\ntruth depth. To this end, we propose UniForward, a feed-forward model to\npredict 3D Gaussians with anisotropic semantic features from only uncalibrated\nand unposed sparse-view images. To enable the unified representation of the 3D\nscene and semantic field, we embed semantic features into 3D Gaussians and\npredict them through a dual-branch decoupled decoder. During training, we\npropose a loss-guided view sampler to sample views from easy to hard,\neliminating the need for ground truth depth or masks required by previous\nmethods and stabilizing the training process. The whole model can be trained\nend-to-end using a photometric loss and a distillation loss that leverages\nsemantic features from a pre-trained 2D semantic model. At the inference stage,\nour UniForward can reconstruct 3D scenes and the corresponding semantic fields\nin real time from only sparse-view images. The reconstructed 3D scenes achieve\nhigh-quality rendering, and the reconstructed 3D semantic field enables the\nrendering of view-consistent semantic features from arbitrary views, which can\nbe further decoded into dense segmentation masks in an open-vocabulary manner.\nExperiments on novel view synthesis and novel view segmentation demonstrate\nthat our method achieves state-of-the-art performances for unifying 3D scene\nand semantic field reconstruction.", "AI": {"tldr": "UniForward is a feed-forward model for unified 3D scene and semantic field reconstruction from sparse-view images, achieving real-time performance and high-quality results without needing camera parameters or ground truth depth.", "motivation": "To combine 3D scenes with semantic fields for better environment perception, addressing challenges like embedding semantics into 3D representations and generalizable real-time reconstruction.", "method": "Proposes UniForward, a model predicting 3D Gaussians with semantic features from uncalibrated images, using a dual-branch decoder and loss-guided view sampling.", "result": "Achieves state-of-the-art performance in novel view synthesis and segmentation, enabling real-time reconstruction and view-consistent semantic rendering.", "conclusion": "UniForward successfully unifies 3D scene and semantic field reconstruction, offering practical applicability and high-quality results."}}
{"id": "2506.09457", "pdf": "https://arxiv.org/pdf/2506.09457", "abs": "https://arxiv.org/abs/2506.09457", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "POET addresses the reward-generation gap in DAAs by equal-length truncation of responses, improving performance in alignment tasks.", "motivation": "The misalignment between training objectives and generation performance in DAAs, particularly the neglect of prefix tokens, motivates the need for POET.", "method": "POET truncates preferred and dispreferred responses to equal lengths, ensuring attention to prefix tokens during optimization.", "result": "POET improves DPO and SimPO, achieving up to 15.6 points in AlpacaEval 2 and better downstream task performance.", "conclusion": "Addressing the reward-generation gap via POET enhances DAAs' effectiveness in aligning LLMs with human preferences."}}
{"id": "2506.09202", "pdf": "https://arxiv.org/pdf/2506.09202", "abs": "https://arxiv.org/abs/2506.09202", "authors": ["Hao Hu", "Xinqi Wang", "Simon Shaolei Du"], "title": "Policy-Based Trajectory Clustering in Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a novel task of clustering trajectories from offline\nreinforcement learning (RL) datasets, where each cluster center represents the\npolicy that generated its trajectories. By leveraging the connection between\nthe KL-divergence of offline trajectory distributions and a mixture of\npolicy-induced distributions, we formulate a natural clustering objective. To\nsolve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted\nAutoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies\nand assigns trajectories based on policy generation probabilities, while CAAE\nresembles the VQ-VAE framework by guiding the latent representations of\ntrajectories toward the vicinity of specific codebook entries to achieve\nclustering. Theoretically, we prove the finite-step convergence of PG-Kmeans\nand identify a key challenge in offline trajectory clustering: the inherent\nambiguity of optimal solutions due to policy-induced conflicts, which can\nresult in multiple equally valid but structurally distinct clusterings.\nExperimentally, we validate our methods on the widely used D4RL dataset and\ncustom GridWorld environments. Our results show that both PG-Kmeans and CAAE\neffectively partition trajectories into meaningful clusters. They offer a\npromising framework for policy-based trajectory clustering, with broad\napplications in offline RL and beyond.", "AI": {"tldr": "The paper introduces a novel task of clustering trajectories from offline RL datasets, proposing two methods (PG-Kmeans and CAAE) and validating their effectiveness on D4RL and GridWorld datasets.", "motivation": "To address the challenge of clustering trajectories in offline RL datasets, where each cluster represents a policy, leveraging KL-divergence and policy-induced distributions.", "method": "Proposes PG-Kmeans (iterative BC policy training and trajectory assignment) and CAAE (VQ-VAE-like framework for clustering).", "result": "Both methods effectively partition trajectories into meaningful clusters, demonstrating convergence and handling inherent ambiguities.", "conclusion": "PG-Kmeans and CAAE provide a promising framework for policy-based trajectory clustering, with broad applications in offline RL."}}
{"id": "2506.09207", "pdf": "https://arxiv.org/pdf/2506.09207", "abs": "https://arxiv.org/abs/2506.09207", "authors": ["William Anderson", "Kevin Chung", "Youngsoo Choi"], "title": "mLaSDI: Multi-stage latent space dynamics identification", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Determining accurate numerical solutions of partial differential equations\n(PDEs) is an important task in many scientific disciplines. However, solvers\ncan be computationally expensive, leading to the development of reduced-order\nmodels (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was\nproposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the\ntraining data using an autoencoder and learns a system of user-chosen ordinary\ndifferential equations (ODEs), which govern the latent space dynamics. This\nallows for rapid predictions by interpolating and evolving the low-dimensional\nODEs in the latent space. While LaSDI has produced effective ROMs for numerous\nproblems, the autoencoder can have difficulty accurately reconstructing\ntraining data while also satisfying the imposed dynamics in the latent space,\nparticularly in complex or high-frequency regimes. To address this, we propose\nmulti-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several\nautoencoders are trained sequentially in stages, where each autoencoder learns\nto correct the error of the previous stages. We find that applying mLaSDI with\nsmall autoencoders results in lower prediction and reconstruction errors, while\nalso reducing training time compared to LaSDI.", "AI": {"tldr": "The paper introduces mLaSDI, a multi-stage version of LaSDI, to improve reduced-order models (ROMs) for PDEs by training autoencoders sequentially to correct errors, achieving better accuracy and efficiency.", "motivation": "Accurate numerical solutions of PDEs are computationally expensive, and existing ROMs like LaSDI struggle with complex or high-frequency regimes due to autoencoder limitations.", "method": "mLaSDI trains multiple autoencoders sequentially, each correcting the errors of the previous stages, to enhance reconstruction and prediction accuracy.", "result": "mLaSDI reduces prediction and reconstruction errors and training time compared to LaSDI, especially with small autoencoders.", "conclusion": "mLaSDI improves upon LaSDI by addressing autoencoder limitations, offering a more efficient and accurate ROM framework for PDEs."}}
{"id": "2506.09385", "pdf": "https://arxiv.org/pdf/2506.09385", "abs": "https://arxiv.org/abs/2506.09385", "authors": ["Jialong Zuo", "Yongtai Deng", "Mengdan Tan", "Rui Jin", "Dongyue Wu", "Nong Sang", "Liang Pan", "Changxin Gao"], "title": "ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model", "categories": ["cs.CV"], "comment": null, "summary": "In real-word scenarios, person re-identification (ReID) expects to identify a\nperson-of-interest via the descriptive query, regardless of whether the query\nis a single modality or a combination of multiple modalities. However, existing\nmethods and datasets remain constrained to limited modalities, failing to meet\nthis requirement. Therefore, we investigate a new challenging problem called\nOmni Multi-modal Person Re-identification (OM-ReID), which aims to achieve\neffective retrieval with varying multi-modal queries. To address dataset\nscarcity, we construct ORBench, the first high-quality multi-modal dataset\ncomprising 1,000 unique identities across five modalities: RGB, infrared, color\npencil, sketch, and textual description. This dataset also has significant\nsuperiority in terms of diversity, such as the painting perspectives and\ntextual information. It could serve as an ideal platform for follow-up\ninvestigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal\nlearning framework for person ReID. It enables synergistic fusion and\ncross-modal alignment of arbitrary modality combinations in a single model,\nwith a unified encoding and multi-expert routing mechanism proposed. Extensive\nexperiments verify the advancement and practicality of our ORBench. A wide\nrange of possible models have been evaluated and compared on it, and our\nproposed ReID5o model gives the best performance. The dataset and code will be\nmade publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.", "AI": {"tldr": "The paper introduces OM-ReID, a multi-modal person re-identification problem, and proposes ORBench dataset and ReID5o framework for effective retrieval across five modalities.", "motivation": "Existing ReID methods and datasets are limited to few modalities, failing to address real-world scenarios requiring diverse multi-modal queries.", "method": "Constructed ORBench dataset with 1,000 identities across five modalities (RGB, infrared, sketch, color pencil, text). Proposed ReID5o, a framework for unified encoding and cross-modal alignment.", "result": "ReID5o outperforms other models on ORBench, demonstrating its effectiveness in multi-modal ReID.", "conclusion": "ORBench and ReID5o provide a robust platform and solution for OM-ReID, addressing dataset scarcity and enabling multi-modal retrieval."}}
{"id": "2506.09495", "pdf": "https://arxiv.org/pdf/2506.09495", "abs": "https://arxiv.org/abs/2506.09495", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "The study explores suicidal behaviors on YouTube using computational and expert-driven methods, identifying unique digital indicators like YouTube Engagement and Mental Health Struggles, and differences in motivation between pre-attempt and post-attempt video uploaders.", "motivation": "Suicide is a major cause of death, and social media data can provide new insights into suicidal behavior, bridging the gap between digital footprints and clinical knowledge.", "method": "Three approaches were used: computational bottom-up (LLM-based topic modeling), hybrid (expert review of topics), and top-down (psychological assessment of narratives). The study analyzed 181 YouTube channels of individuals with suicide attempts and 134 control channels.", "result": "Five topics were linked to suicide attempts, with two showing temporal changes. YouTube Engagement, a platform-specific indicator, was missed by experts. Motivational differences were found between pre-attempt and post-attempt uploaders.", "conclusion": "The study highlights the value of combining digital behavior analysis with clinical insights to better understand suicidality, revealing unique indicators and motivational shifts."}}
{"id": "2506.09204", "pdf": "https://arxiv.org/pdf/2506.09204", "abs": "https://arxiv.org/abs/2506.09204", "authors": ["Xiaotian Chen", "Hongyun Liu", "Seyed Sahand Mohammadi Ziabari"], "title": "A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Deep Neural Networks (DNNs) have been proven to be exceptionally effective\nand have been applied across diverse domains within deep learning. However, as\nDNN models increase in complexity, the demand for reduced computational costs\nand memory overheads has become increasingly urgent. Sparsity has emerged as a\nleading approach in this area. The robustness of sparse Multi-layer Perceptrons\n(MLPs) for supervised feature selection, along with the application of Sparse\nEvolutionary Training (SET), illustrates the feasibility of reducing\ncomputational costs without compromising accuracy. Moreover, it is believed\nthat the SET algorithm can still be improved through a structural optimization\nmethod called motif-based optimization, with potential efficiency gains\nexceeding 40% and a performance decline of under 4%. This research investigates\nwhether the structural optimization of Sparse Evolutionary Training applied to\nMulti-layer Perceptrons (SET-MLP) can enhance performance and to what extent\nthis improvement can be achieved.", "AI": {"tldr": "The paper explores structural optimization of Sparse Evolutionary Training (SET) for Multi-layer Perceptrons (SET-MLP) to enhance performance while reducing computational costs.", "motivation": "As DNNs grow in complexity, reducing computational costs and memory overheads is critical. Sparsity, particularly via SET, shows promise but may be further improved.", "method": "The study investigates motif-based optimization for SET-MLP to boost efficiency and performance.", "result": "Potential efficiency gains exceed 40% with a performance decline under 4%.", "conclusion": "Structural optimization of SET-MLP can significantly improve performance, balancing efficiency and accuracy."}}
{"id": "2506.09215", "pdf": "https://arxiv.org/pdf/2506.09215", "abs": "https://arxiv.org/abs/2506.09215", "authors": ["Greyson Brothers"], "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs", "categories": ["cs.LG", "cs.AI", "68T07 (Primary), 68P30, 68T45 (Secondary)", "E.4; I.2.6; I.2.10"], "comment": "[ICML 2025 Spotlight Poster] To be published in the Forty-Second\n  International Conference on Machine Learning (ICML) Proceedings", "summary": "We investigate the design of pooling methods used to summarize the outputs of\ntransformer embedding models, primarily motivated by reinforcement learning and\nvision applications. This work considers problems where a subset of the input\nvectors contains requisite information for a downstream task (signal) while the\nrest are distractors (noise). By framing pooling as vector quantization with\nthe goal of minimizing signal loss, we demonstrate that the standard methods\nused to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are\nvulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs\nfluctuates. We then show that an attention-based adaptive pooling method can\napproximate the signal-optimal vector quantizer within derived error bounds for\nany SNR. Our theoretical results are first validated by supervised experiments\non a synthetic dataset designed to isolate the SNR problem, then generalized to\nstandard relational reasoning, multi-agent reinforcement learning, and vision\nbenchmarks with noisy observations, where transformers with adaptive pooling\ndisplay superior robustness across tasks.", "AI": {"tldr": "The paper explores pooling methods for transformer embeddings, showing standard methods (AvgPool, MaxPool, ClsToken) fail under fluctuating signal-to-noise ratios (SNR). An attention-based adaptive pooling method is proposed, outperforming others in robustness across tasks.", "motivation": "Address the vulnerability of standard pooling methods to SNR fluctuations in transformer embeddings, particularly for reinforcement learning and vision applications.", "method": "Frames pooling as vector quantization to minimize signal loss, introduces an attention-based adaptive pooling method, and validates it theoretically and experimentally.", "result": "Adaptive pooling approximates the signal-optimal quantizer within error bounds, outperforming standard methods in synthetic and real-world benchmarks (relational reasoning, multi-agent RL, vision).", "conclusion": "Attention-based adaptive pooling is robust to SNR variations, offering superior performance over traditional methods in noisy environments."}}
{"id": "2506.09399", "pdf": "https://arxiv.org/pdf/2506.09399", "abs": "https://arxiv.org/abs/2506.09399", "authors": ["Kaiyu Guo", "Zijian Wang", "Brian C. Lovell", "Mahsa Baktashmotlagh"], "title": "Improving Out-of-Distribution Detection via Dynamic Covariance Calibration", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-Distribution (OOD) detection is essential for the trustworthiness of\nAI systems. Methods using prior information (i.e., subspace-based methods) have\nshown effective performance by extracting information geometry to detect OOD\ndata with a more appropriate distance metric. However, these methods fail to\naddress the geometry distorted by ill-distributed samples, due to the\nlimitation of statically extracting information geometry from the training\ndistribution. In this paper, we argue that the influence of ill-distributed\nsamples can be corrected by dynamically adjusting the prior geometry in\nresponse to new data. Based on this insight, we propose a novel approach that\ndynamically updates the prior covariance matrix using real-time input features,\nrefining its information. Specifically, we reduce the covariance along the\ndirection of real-time input features and constrain adjustments to the residual\nspace, thus preserving essential data characteristics and avoiding effects on\nunintended directions in the principal space. We evaluate our method on two\npre-trained models for the CIFAR dataset and five pre-trained models for\nImageNet-1k, including the self-supervised DINO model. Extensive experiments\ndemonstrate that our approach significantly enhances OOD detection across\nvarious models. The code is released at https://github.com/workerbcd/ooddcc.", "AI": {"tldr": "The paper proposes a dynamic method to improve Out-of-Distribution (OOD) detection by adjusting prior geometry in real-time, addressing distortions from ill-distributed samples.", "motivation": "Prior subspace-based OOD detection methods fail to handle geometry distortions caused by ill-distributed samples due to static prior extraction.", "method": "The approach dynamically updates the prior covariance matrix using real-time input features, refining information while preserving essential data characteristics.", "result": "The method significantly enhances OOD detection across multiple pre-trained models, including CIFAR and ImageNet-1k datasets.", "conclusion": "Dynamic adjustment of prior geometry effectively improves OOD detection, outperforming static methods."}}
{"id": "2506.09501", "pdf": "https://arxiv.org/pdf/2506.09501", "abs": "https://arxiv.org/abs/2506.09501", "authors": ["Jiayi Yuan", "Hao Li", "Xinheng Ding", "Wenya Xie", "Yu-Jhe Li", "Wentian Zhao", "Kun Wan", "Jing Shi", "Xia Hu", "Zirui Liu"], "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility.", "AI": {"tldr": "The paper highlights the fragility of LLM performance reproducibility due to system configuration changes, tracing the issue to floating-point arithmetic. It introduces LayerCast for stable inference.", "motivation": "Benchmark scores for LLMs assume accuracy and reproducibility, but system configurations like GPU settings can cause significant performance variations, especially in reasoning models.", "method": "The study conducts controlled experiments across hardware, software, and precision settings to quantify output divergence. It also develops LayerCast, a pipeline using 16-bit weights and FP32 computations.", "result": "Experiments show up to 9% accuracy variation and 9,000 token length differences in responses due to GPU and batch size changes. Floating-point precision is identified as a key factor.", "conclusion": "Floating-point precision impacts LLM reproducibility but is often overlooked. LayerCast offers a solution by balancing memory efficiency and numerical stability."}}
{"id": "2506.09268", "pdf": "https://arxiv.org/pdf/2506.09268", "abs": "https://arxiv.org/abs/2506.09268", "authors": ["Henri Alam", "Antonio de Domenico", "Tareq Si Salem", "Florian Kaltenberger"], "title": "A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks", "categories": ["cs.NI", "cs.AI"], "comment": "To be published in 2025 IEEE International Workshop on Signal\n  Processing and Artificial Intelligence in Wireless Communications (IEEE SPAWC\n  2025)", "summary": "Integrated terrestrial and non-terrestrial network (TN-NTN) architectures\noffer a promising solution for expanding coverage and improving capacity for\nthe network. While non-terrestrial networks (NTNs) are primarily exploited for\nthese specific reasons, their role in alleviating terrestrial network (TN) load\nand enabling energy-efficient operation has received comparatively less\nattention. In light of growing concerns associated with the densification of\nterrestrial deployments, this work aims to explore the potential of NTNs in\nsupporting a more sustainable network. In this paper, we propose a novel online\noptimisation framework for integrated TN-NTN architectures, built on a\nmulti-armed bandit (MAB) formulation and leveraging the Bandit-feedback\nConstrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively\noptimises key system parameters--including bandwidth allocation, user equipment\n(UE) association, and macro base station (MBS) shutdown--to balance network\ncapacity and energy efficiency in real time. Extensive system-level simulations\nover a 24-hour period show that our framework significantly reduces the\nproportion of unsatisfied UEs during peak hours and achieves up to 19%\nthroughput gains and 5% energy savings in low-traffic periods, outperforming\nstandard network settings following 3GPP recommendations.", "AI": {"tldr": "The paper proposes an online optimization framework for integrated terrestrial and non-terrestrial networks (TN-NTN) to balance capacity and energy efficiency, achieving significant performance gains.", "motivation": "To address the under-explored role of NTNs in alleviating terrestrial network load and enabling sustainable operations amid densification concerns.", "method": "A novel online optimization framework using multi-armed bandit (MAB) formulation and the BCOMD algorithm to adaptively optimize bandwidth allocation, UE association, and MBS shutdown.", "result": "Simulations show 19% throughput gains, 5% energy savings, and reduced unsatisfied UEs during peak hours, outperforming 3GPP standards.", "conclusion": "The framework demonstrates NTNs' potential for sustainable network operations, balancing capacity and energy efficiency effectively."}}
{"id": "2506.09227", "pdf": "https://arxiv.org/pdf/2506.09227", "abs": "https://arxiv.org/abs/2506.09227", "authors": ["Jie Ren", "Yue Xing", "Yingqian Cui", "Charu C. Aggarwal", "Hui Liu"], "title": "SoK: Machine Unlearning for Large Language Models", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Large language model (LLM) unlearning has become a critical topic in machine\nlearning, aiming to eliminate the influence of specific training data or\nknowledge without retraining the model from scratch. A variety of techniques\nhave been proposed, including Gradient Ascent, model editing, and re-steering\nhidden representations. While existing surveys often organize these methods by\ntheir technical characteristics, such classifications tend to overlook a more\nfundamental dimension: the underlying intention of unlearning--whether it seeks\nto truly remove internal knowledge or merely suppress its behavioral effects.\nIn this SoK paper, we propose a new taxonomy based on this intention-oriented\nperspective. Building on this taxonomy, we make three key contributions. First,\nwe revisit recent findings suggesting that many removal methods may\nfunctionally behave like suppression, and explore whether true removal is\nnecessary or achievable. Second, we survey existing evaluation strategies,\nidentify limitations in current metrics and benchmarks, and suggest directions\nfor developing more reliable and intention-aligned evaluations. Third, we\nhighlight practical challenges--such as scalability and support for sequential\nunlearning--that currently hinder the broader deployment of unlearning methods.\nIn summary, this work offers a comprehensive framework for understanding and\nadvancing unlearning in generative AI, aiming to support future research and\nguide policy decisions around data removal and privacy.", "AI": {"tldr": "This paper proposes a new taxonomy for LLM unlearning based on the underlying intention (removal vs. suppression), revisits the effectiveness of removal methods, critiques evaluation strategies, and highlights practical challenges.", "motivation": "To address the overlooked dimension of intention in unlearning (removal vs. suppression) and provide a clearer framework for advancing unlearning in generative AI.", "method": "Proposes an intention-oriented taxonomy, revisits removal methods, surveys evaluation strategies, and identifies practical challenges.", "result": "Finds that many removal methods may functionally behave like suppression, critiques current evaluation metrics, and highlights scalability and sequential unlearning as key challenges.", "conclusion": "Offers a comprehensive framework to guide future research and policy decisions on unlearning in generative AI."}}
{"id": "2506.09403", "pdf": "https://arxiv.org/pdf/2506.09403", "abs": "https://arxiv.org/abs/2506.09403", "authors": ["Xinya Liu", "Jianghao Wu", "Tao Lu", "Shaoting Zhang", "Guotai Wang"], "title": "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation", "categories": ["cs.CV", "I.2.6; I.5.1"], "comment": "18 pages, 4 figures. Accepted for publication in Neurocomputing", "summary": "Domain Adaptation (DA) is crucial for robust deployment of medical image\nsegmentation models when applied to new clinical centers with significant\ndomain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal\nwith privacy concerns and access constraints on source-domain data during\nadaptation to target-domain data. However, SFDA faces challenges such as\ninsufficient supervision in the target domain with unlabeled images. In this\nwork, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels\nmethod for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch\nIntensity Enhancement (T3IE) that not only improves quality of raw\npseudo-labels in the target domain, but also leads to SAM-compatible inputs\nwith three channels to better leverage SAM's zero-shot inference ability for\nrefining the pseudo-labels; 2) A reliable pseudo-label selection module that\nrejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs\n(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training\nprocedure in the unlabeled target domain where reliable pseudo-labels are used\nfor supervision and unreliable parts are regularized by entropy minimization.\nExperiments conducted on two multi-domain medical image segmentation datasets\nfor fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA\neffectively enhances pseudo-label quality in the unlabeled target domain, and\nimproves SFDA performance by leveraging the reliability-aware training; 2)\nSRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is\nclose to that of supervised training in the target domain. The code of this\nwork is available online: https://github.com/HiLab-git/SRPL-SFDA.", "AI": {"tldr": "Proposes SRPL-SFDA, a SAM-guided method for Source-Free Domain Adaptation (SFDA) in medical image segmentation, improving pseudo-label quality and outperforming state-of-the-art SFDA methods.", "motivation": "Addresses challenges in SFDA for medical image segmentation, such as insufficient supervision in the target domain and privacy concerns.", "method": "Uses SAM-guided reliable pseudo-labels with Test-Time Tri-branch Intensity Enhancement (T3IE), a selection module for reliable pseudo-labels, and reliability-aware training.", "result": "Enhances pseudo-label quality, improves SFDA performance, and outperforms state-of-the-art methods, nearing supervised training performance.", "conclusion": "SRPL-SFDA is effective for SFDA in medical image segmentation, leveraging SAM's zero-shot ability and reliability-aware training."}}
{"id": "2506.09507", "pdf": "https://arxiv.org/pdf/2506.09507", "abs": "https://arxiv.org/abs/2506.09507", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling.", "AI": {"tldr": "The paper proposes a unified rotary position embedding (RoPE) method to integrate Transformers and State Space Models (SSMs), addressing positional encoding incompatibility. The resulting hybrid model, \\model, outperforms standard Transformers in speed and accuracy.", "motivation": "The integration of Transformers and SSMs is hindered by incompatible positional encoding mechanisms (explicit RoPE in Transformers vs. implicit convolutions in SSMs), leading to performance issues.", "method": "A unified rotary position embedding (\\ourRoPE) is introduced to align positional encoding for both self-attention and state-space components, enabling the hybrid architecture \\model.", "result": "\\model achieves 42.3% faster training and 29.5% faster inference at 4K sequence length, with over 4% higher accuracy on language modeling benchmarks. It also scales better, with larger versions showing greater gains.", "conclusion": "Unified positional encoding resolves incompatibility in hybrid models, enabling efficient and high-performance long-context modeling."}}
{"id": "2506.09276", "pdf": "https://arxiv.org/pdf/2506.09276", "abs": "https://arxiv.org/abs/2506.09276", "authors": ["Lorenzo Steccanella", "Joshua B. Evans", "\u00d6zg\u00fcr \u015eim\u015fek", "Anders Jonsson"], "title": "Learning The Minimum Action Distance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents a state representation framework for Markov decision\nprocesses (MDPs) that can be learned solely from state trajectories, requiring\nneither reward signals nor the actions executed by the agent. We propose\nlearning the minimum action distance (MAD), defined as the minimum number of\nactions required to transition between states, as a fundamental metric that\ncaptures the underlying structure of an environment. MAD naturally enables\ncritical downstream tasks such as goal-conditioned reinforcement learning and\nreward shaping by providing a dense, geometrically meaningful measure of\nprogress. Our self-supervised learning approach constructs an embedding space\nwhere the distances between embedded state pairs correspond to their MAD,\naccommodating both symmetric and asymmetric approximations. We evaluate the\nframework on a comprehensive suite of environments with known MAD values,\nencompassing both deterministic and stochastic dynamics, as well as discrete\nand continuous state spaces, and environments with noisy observations.\nEmpirical results demonstrate that the proposed approach not only efficiently\nlearns accurate MAD representations across these diverse settings but also\nsignificantly outperforms existing state representation methods in terms of\nrepresentation quality.", "AI": {"tldr": "A self-supervised framework learns minimum action distance (MAD) from state trajectories, enabling tasks like goal-conditioned RL without rewards or actions.", "motivation": "To capture environment structure without relying on rewards or actions, enabling downstream tasks like reward shaping.", "method": "Learn MAD as a metric in an embedding space, accommodating symmetric/asymmetric approximations, evaluated on diverse environments.", "result": "Accurate MAD representations learned efficiently, outperforming existing methods in quality.", "conclusion": "The framework effectively learns MAD, enhancing state representation for diverse RL tasks."}}
{"id": "2506.09247", "pdf": "https://arxiv.org/pdf/2506.09247", "abs": "https://arxiv.org/abs/2506.09247", "authors": ["Karl L\u00f6wenmark", "Daniel Str\u00f6mbergsson", "Chang Liu", "Marcus Liwicki", "Fredrik Sandin"], "title": "Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation", "categories": ["cs.LG"], "comment": null, "summary": "Condition monitoring (CM) plays a crucial role in ensuring reliability and\nefficiency in the process industry. Although computerised maintenance systems\neffectively detect and classify faults, tasks like fault severity estimation,\nand maintenance decisions still largely depend on human expert analysis. The\nanalysis and decision making automatically performed by current systems\ntypically exhibit considerable uncertainty and high false alarm rates, leading\nto increased workload and reduced efficiency.\n  This work integrates large language model (LLM)-based reasoning agents with\nCM workflows to address analyst and industry needs, namely reducing false\nalarms, enhancing fault severity estimation, improving decision support, and\noffering explainable interfaces. We propose MindRAG, a modular framework\ncombining multimodal retrieval-augmented generation (RAG) with novel vector\nstore structures designed specifically for CM data. The framework leverages\nexisting annotations and maintenance work orders as surrogates for labels in a\nsupervised learning protocol, addressing the common challenge of training\npredictive models on unlabelled and noisy real-world datasets.\n  The primary contributions include: (1) an approach for structuring industry\nCM data into a semi-structured multimodal vector store compatible with\nLLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM\ndata; (3) developing practical reasoning agents capable of addressing\nreal-world CM queries; and (4) presenting an experimental framework for\nintegrating and evaluating such agents in realistic industrial scenarios.\nPreliminary results, evaluated with the help of an experienced analyst,\nindicate that MindRAG provide meaningful decision support for more efficient\nmanagement of alarms, thereby improving the interpretability of CM systems.", "AI": {"tldr": "MindRAG integrates LLM-based reasoning with CM workflows to reduce false alarms, improve fault severity estimation, and enhance decision support using multimodal RAG techniques.", "motivation": "Current CM systems rely heavily on human experts, suffer from uncertainty, and high false alarm rates, reducing efficiency.", "method": "Proposes MindRAG, a modular framework combining multimodal RAG with novel vector store structures for CM data, leveraging existing annotations and work orders.", "result": "Preliminary results show MindRAG improves alarm management and system interpretability.", "conclusion": "MindRAG offers a practical solution for automating CM tasks, enhancing efficiency and decision-making in industrial settings."}}
{"id": "2506.09411", "pdf": "https://arxiv.org/pdf/2506.09411", "abs": "https://arxiv.org/abs/2506.09411", "authors": ["Vaclav Knapp", "Matyas Bohacek"], "title": "Synthetic Human Action Video Data Generation with Pose Transfer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In video understanding tasks, particularly those involving human motion,\nsynthetic data generation often suffers from uncanny features, diminishing its\neffectiveness for training. Tasks such as sign language translation, gesture\nrecognition, and human motion understanding in autonomous driving have thus\nbeen unable to exploit the full potential of synthetic data. This paper\nproposes a method for generating synthetic human action video data using pose\ntransfer (specifically, controllable 3D Gaussian avatar models). We evaluate\nthis method on the Toyota Smarthome and NTU RGB+D datasets and show that it\nimproves performance in action recognition tasks. Moreover, we demonstrate that\nthe method can effectively scale few-shot datasets, making up for groups\nunderrepresented in the real training data and adding diverse backgrounds. We\nopen-source the method along with RANDOM People, a dataset with videos and\navatars of novel human identities for pose transfer crowd-sourced from the\ninternet.", "AI": {"tldr": "Proposes a method for synthetic human action video generation using pose transfer to address uncanny features in synthetic data, improving action recognition performance and scaling few-shot datasets.", "motivation": "Synthetic data for video understanding tasks often has uncanny features, limiting its effectiveness in tasks like sign language translation and gesture recognition.", "method": "Uses controllable 3D Gaussian avatar models for pose transfer to generate synthetic human action videos.", "result": "Improves performance on action recognition tasks (tested on Toyota Smarthome and NTU RGB+D datasets) and scales few-shot datasets by adding diversity.", "conclusion": "The method enhances synthetic data utility, addresses underrepresented groups, and is open-sourced with the RANDOM People dataset."}}
{"id": "2506.09542", "pdf": "https://arxiv.org/pdf/2506.09542", "abs": "https://arxiv.org/abs/2506.09542", "authors": ["Dingjun Wu", "Yukun Yan", "Zhenghao Liu", "Zhiyuan Liu", "Maosong Sun"], "title": "KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding\nresponses in external knowledge. However, existing methods typically rely on a\nsingle source, either unstructured text or structured knowledge. Moreover, they\nlack cognitively inspired mechanisms for activating relevant knowledge. To\naddress these issues, we propose KG-Infused RAG, a framework that integrates\nKGs into RAG systems to implement spreading activation, a cognitive process\nthat enables concept association and inference. KG-Infused RAG retrieves KG\nfacts, expands the query accordingly, and enhances generation by combining\ncorpus passages with structured facts, enabling interpretable, multi-source\nretrieval grounded in semantic structure. We further improve KG-Infused RAG via\npreference learning on sampled key stages in the pipeline. Experiments on five\nQA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by\n3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG\nbrings further performance gains, demonstrating its effectiveness and\nversatility as a plug-and-play enhancement module for corpus-based RAG methods.", "AI": {"tldr": "KG-Infused RAG enhances RAG by integrating knowledge graphs (KGs) and spreading activation, improving factual accuracy and interpretability. It outperforms vanilla RAG by 3.8% to 13.8%.", "motivation": "Existing RAG methods rely on single knowledge sources and lack cognitive mechanisms for activating relevant knowledge.", "method": "Proposes KG-Infused RAG, which retrieves KG facts, expands queries, and combines corpus passages with structured facts. Preference learning is used to improve key pipeline stages.", "result": "Outperforms vanilla RAG by 3.8% to 13.8% on five QA benchmarks and enhances Self-RAG when integrated.", "conclusion": "KG-Infused RAG is an effective, versatile plug-and-play enhancement for corpus-based RAG methods."}}
{"id": "2506.09284", "pdf": "https://arxiv.org/pdf/2506.09284", "abs": "https://arxiv.org/abs/2506.09284", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "AI": {"tldr": "UAD (Unsupervised Affordance Distillation) distills affordance knowledge from foundation models into a task-conditioned affordance model without manual annotations, enabling generalization to real-world robotic and human activities.", "motivation": "Existing methods rely on manual annotations or predefined tasks, limiting their applicability in unstructured environments.", "method": "UAD leverages large vision and vision-language models to auto-annotate a dataset with instruction-affordance pairs, training a lightweight decoder on frozen features.", "result": "UAD generalizes well to real-world scenes and human activities, and its affordance-based imitation learning policy generalizes to unseen objects and tasks with minimal demonstrations.", "conclusion": "UAD offers a scalable, annotation-free approach for affordance learning, enhancing robotic manipulation in open-ended tasks."}}
{"id": "2506.09258", "pdf": "https://arxiv.org/pdf/2506.09258", "abs": "https://arxiv.org/abs/2506.09258", "authors": ["Vaidotas Simkus", "Michael U. Gutmann"], "title": "CFMI: Flow Matching for Missing Data Imputation", "categories": ["cs.LG", "stat.ML", "62D10", "I.5.1"], "comment": null, "summary": "We introduce conditional flow matching for imputation (CFMI), a new\ngeneral-purpose method to impute missing data. The method combines continuous\nnormalising flows, flow-matching, and shared conditional modelling to deal with\nintractabilities of traditional multiple imputation. Our comparison with nine\nclassical and state-of-the-art imputation methods on 24 small to\nmoderate-dimensional tabular data sets shows that CFMI matches or outperforms\nboth traditional and modern techniques across a wide range of metrics. Applying\nthe method to zero-shot imputation of time-series data, we find that it matches\nthe accuracy of a related diffusion-based method while outperforming it in\nterms of computational efficiency. Overall, CFMI performs at least as well as\ntraditional methods on lower-dimensional data while remaining scalable to\nhigh-dimensional settings, matching or exceeding the performance of other deep\nlearning-based approaches, making it a go-to imputation method for a wide range\nof data types and dimensionalities.", "AI": {"tldr": "CFMI is a new imputation method combining continuous normalising flows, flow-matching, and shared conditional modelling, outperforming traditional and modern techniques across various metrics.", "motivation": "To address intractabilities in traditional multiple imputation methods and provide a scalable solution for diverse data types.", "method": "Combines continuous normalising flows, flow-matching, and shared conditional modelling.", "result": "CFMI matches or outperforms nine classical and state-of-the-art methods on 24 datasets and excels in computational efficiency for time-series data.", "conclusion": "CFMI is a versatile, high-performing imputation method suitable for a wide range of data types and dimensionalities."}}
{"id": "2506.09416", "pdf": "https://arxiv.org/pdf/2506.09416", "abs": "https://arxiv.org/abs/2506.09416", "authors": ["Xinyu Peng", "Ziyang Zheng", "Yaoming Wang", "Han Li", "Nuowen Kan", "Wenrui Dai", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "title": "Noise Conditional Variational Score Distillation", "categories": ["cs.CV"], "comment": null, "summary": "We propose Noise Conditional Variational Score Distillation (NCVSD), a novel\nmethod for distilling pretrained diffusion models into generative denoisers. We\nachieve this by revealing that the unconditional score function implicitly\ncharacterizes the score function of denoising posterior distributions. By\nintegrating this insight into the Variational Score Distillation (VSD)\nframework, we enable scalable learning of generative denoisers capable of\napproximating samples from the denoising posterior distribution across a wide\nrange of noise levels. The proposed generative denoisers exhibit desirable\nproperties that allow fast generation while preserve the benefit of iterative\nrefinement: (1) fast one-step generation through sampling from pure Gaussian\nnoise at high noise levels; (2) improved sample quality by scaling the\ntest-time compute with multi-step sampling; and (3) zero-shot probabilistic\ninference for flexible and controllable sampling. We evaluate NCVSD through\nextensive experiments, including class-conditional image generation and inverse\nproblem solving. By scaling the test-time compute, our method outperforms\nteacher diffusion models and is on par with consistency models of larger sizes.\nAdditionally, with significantly fewer NFEs than diffusion-based methods, we\nachieve record-breaking LPIPS on inverse problems.", "AI": {"tldr": "NCVSD distills diffusion models into generative denoisers by leveraging unconditional score functions, enabling fast generation and iterative refinement.", "motivation": "To bridge the gap between diffusion models and efficient generative denoisers while preserving iterative refinement benefits.", "method": "Integrates unconditional score functions into Variational Score Distillation (VSD) to learn denoisers for various noise levels.", "result": "Outperforms teacher diffusion models, matches larger consistency models, and achieves record LPIPS in inverse problems with fewer NFEs.", "conclusion": "NCVSD offers scalable, efficient generative denoising with flexible sampling and improved performance."}}
{"id": "2506.09556", "pdf": "https://arxiv.org/pdf/2506.09556", "abs": "https://arxiv.org/abs/2506.09556", "authors": ["Georgios Chatzichristodoulou", "Despoina Kosmopoulou", "Antonios Kritikos", "Anastasia Poulopoulou", "Efthymios Georgiou", "Athanasios Katsamanis", "Vassilis Katsouros", "Alexandros Potamianos"], "title": "MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions", "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "SER is a challenging task due to the subjective nature of human emotions and\ntheir uneven representation under naturalistic conditions. We propose MEDUSA, a\nmultimodal framework with a four-stage training pipeline, which effectively\nhandles class imbalance and emotion ambiguity. The first two stages train an\nensemble of classifiers that utilize DeepSER, a novel extension of a deep\ncross-modal transformer fusion mechanism from pretrained self-supervised\nacoustic and linguistic representations. Manifold MixUp is employed for further\nregularization. The last two stages optimize a trainable meta-classifier that\ncombines the ensemble predictions. Our training approach incorporates human\nannotation scores as soft targets, coupled with balanced data sampling and\nmultitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion\nRecognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic\nConditions Challenge.", "AI": {"tldr": "MEDUSA, a multimodal framework with a four-stage pipeline, addresses SER challenges like class imbalance and emotion ambiguity, achieving top performance in a 2025 challenge.", "motivation": "SER is difficult due to subjective emotions and uneven data representation in naturalistic conditions.", "method": "Uses a four-stage pipeline: ensemble training with DeepSER (cross-modal transformer fusion), Manifold MixUp, and a meta-classifier. Incorporates soft targets, balanced sampling, and multitask learning.", "result": "Ranked 1st in the Interspeech 2025 Challenge for Categorical Emotion Recognition.", "conclusion": "MEDUSA effectively tackles SER challenges through multimodal fusion and advanced training techniques."}}
{"id": "2506.09286", "pdf": "https://arxiv.org/pdf/2506.09286", "abs": "https://arxiv.org/abs/2506.09286", "authors": ["Mohammadsajad Abavisani", "Kseniya Solovyeva", "David Danks", "Vince Calhoun", "Sergey Plis"], "title": "Causal Graph Recovery in Neuroimaging through Answer Set Programming", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ME"], "comment": null, "summary": "Learning graphical causal structures from time series data presents\nsignificant challenges, especially when the measurement frequency does not\nmatch the causal timescale of the system. This often leads to a set of equally\npossible underlying causal graphs due to information loss from sub-sampling\n(i.e., not observing all possible states of the system throughout time). Our\nresearch addresses this challenge by incorporating the effects of sub-sampling\nin the derivation of causal graphs, resulting in more accurate and intuitive\noutcomes. We use a constraint optimization approach, specifically answer set\nprogramming (ASP), to find the optimal set of answers. ASP not only identifies\nthe most probable underlying graph, but also provides an equivalence class of\npossible graphs for expert selection. In addition, using ASP allows us to\nleverage graph theory to further prune the set of possible solutions, yielding\na smaller, more accurate answer set significantly faster than traditional\napproaches. We validate our approach on both simulated data and empirical\nstructural brain connectivity, and demonstrate its superiority over established\nmethods in these experiments. We further show how our method can be used as a\nmeta-approach on top of established methods to obtain, on average, 12%\nimprovement in F1 score. In addition, we achieved state of the art results in\nterms of precision and recall of reconstructing causal graph from sub-sampled\ntime series data. Finally, our method shows robustness to varying degrees of\nsub-sampling on realistic simulations, whereas other methods perform worse for\nhigher rates of sub-sampling.", "AI": {"tldr": "The paper introduces a method using answer set programming (ASP) to derive causal graphs from sub-sampled time series data, improving accuracy and efficiency over traditional approaches.", "motivation": "Addressing the challenge of learning causal structures from time series data with mismatched measurement frequencies, which leads to ambiguous causal graphs due to sub-sampling.", "method": "Uses constraint optimization via ASP to identify optimal causal graphs and equivalence classes, leveraging graph theory to prune solutions.", "result": "Validated on simulated and empirical data, showing superior performance (12% F1 improvement) and robustness to sub-sampling.", "conclusion": "The ASP-based method outperforms existing approaches, offering more accurate and faster causal graph reconstruction from sub-sampled time series."}}
{"id": "2506.09270", "pdf": "https://arxiv.org/pdf/2506.09270", "abs": "https://arxiv.org/abs/2506.09270", "authors": ["Rodrigo Carrasco-Davis", "Sebastian Lee", "Claudia Clopath", "Will Dabney"], "title": "Uncertainty Prioritized Experience Replay", "categories": ["cs.LG"], "comment": "Accepted at Reinforcement Learning Conference", "summary": "Prioritized experience replay, which improves sample efficiency by selecting\nrelevant transitions to update parameter estimates, is a crucial component of\ncontemporary value-based deep reinforcement learning models. Typically,\ntransitions are prioritized based on their temporal difference error. However,\nthis approach is prone to favoring noisy transitions, even when the value\nestimation closely approximates the target mean. This phenomenon resembles the\nnoisy TV problem postulated in the exploration literature, in which\nexploration-guided agents get stuck by mistaking noise for novelty. To mitigate\nthe disruptive effects of noise in value estimation, we propose using epistemic\nuncertainty estimation to guide the prioritization of transitions from the\nreplay buffer. Epistemic uncertainty quantifies the uncertainty that can be\nreduced by learning, hence reducing transitions sampled from the buffer\ngenerated by unpredictable random processes. We first illustrate the benefits\nof epistemic uncertainty prioritized replay in two tabular toy models: a simple\nmulti-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our\nprioritization scheme on the Atari suite, outperforming quantile regression\ndeep Q-learning benchmarks; thus forging a path for the use of uncertainty\nprioritized replay in reinforcement learning agents.", "AI": {"tldr": "The paper proposes using epistemic uncertainty to prioritize transitions in replay buffers, improving reinforcement learning by reducing noise impact.", "motivation": "Traditional prioritization based on temporal difference errors favors noisy transitions, resembling the noisy TV problem, which disrupts learning.", "method": "The authors introduce epistemic uncertainty estimation to guide transition prioritization, tested in toy models and the Atari suite.", "result": "The method outperforms benchmarks like quantile regression deep Q-learning, demonstrating effectiveness.", "conclusion": "Epistemic uncertainty prioritization offers a promising approach to enhance reinforcement learning agents by mitigating noise effects."}}
{"id": "2506.09417", "pdf": "https://arxiv.org/pdf/2506.09417", "abs": "https://arxiv.org/abs/2506.09417", "authors": ["Yunxiao Shi", "Yinhao Zhu", "Shizhong Han", "Jisoo Jeong", "Amin Ansari", "Hong Cai", "Fatih Porikli"], "title": "ODG: Occupancy Prediction Using Dual Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, ODG, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG\nalso delivers competitive inference speed when compared to the latest efficient\napproaches.", "AI": {"tldr": "ODG combines BEV and sparse points representations for efficient 3D occupancy prediction, addressing limitations of both methods.", "motivation": "Existing methods for 3D occupancy prediction are either computationally expensive or suffer from information loss (BEV) or inefficiency (sparse points).", "method": "ODG uses a dual-branch design: a query-based sparse points branch and a BEV branch, with cross-attention to share 3D information.", "result": "ODG outperforms on Occ3D-nuScenes and Occ3D-Waymo benchmarks and offers competitive inference speed.", "conclusion": "ODG effectively balances accuracy and efficiency for 3D occupancy prediction in autonomous driving."}}
{"id": "2506.09558", "pdf": "https://arxiv.org/pdf/2506.09558", "abs": "https://arxiv.org/abs/2506.09558", "authors": ["Eleni Gkovedarou", "Joke Daems", "Luna De Bruyne"], "title": "Gender Bias in English-to-Greek Machine Translation", "categories": ["cs.CL"], "comment": "Accepted at GITT 2025 (MT Summit)", "summary": "As the demand for inclusive language increases, concern has grown over the\nsusceptibility of machine translation (MT) systems to reinforce gender\nstereotypes. This study investigates gender bias in two commercial MT systems,\nGoogle Translate and DeepL, focusing on the understudied English-to-Greek\nlanguage pair. We address three aspects of gender bias: i) male bias, ii)\noccupational stereotyping, and iii) errors in anti-stereotypical translations.\nAdditionally, we explore the potential of prompted GPT-4o as a bias mitigation\ntool that provides both gender-explicit and gender-neutral alternatives when\nnecessary. To achieve this, we introduce GendEL, a manually crafted bilingual\ndataset of 240 gender-ambiguous and unambiguous sentences that feature\nstereotypical occupational nouns and adjectives. We find persistent gender bias\nin translations by both MT systems; while they perform well in cases where\ngender is explicitly defined, with DeepL outperforming both Google Translate\nand GPT-4o in feminine gender-unambiguous sentences, they are far from\nproducing gender-inclusive or neutral translations when the gender is\nunspecified. GPT-4o shows promise, generating appropriate gendered and neutral\nalternatives for most ambiguous cases, though residual biases remain evident.", "AI": {"tldr": "The study examines gender bias in Google Translate and DeepL for English-to-Greek translations, introducing GendEL dataset. GPT-4o shows potential for bias mitigation but isn't flawless.", "motivation": "Addressing growing concerns about gender bias in MT systems, especially in understudied language pairs like English-to-Greek.", "method": "Analyzed gender bias in two MT systems using GendEL, a dataset of 240 gender-ambiguous and unambiguous sentences, and tested GPT-4o for bias mitigation.", "result": "Persistent gender bias found in MT systems; GPT-4o performed better in providing gendered/neutral alternatives but still had biases.", "conclusion": "MT systems struggle with gender inclusivity; GPT-4o offers promise but requires further refinement to fully mitigate bias."}}
{"id": "2506.09338", "pdf": "https://arxiv.org/pdf/2506.09338", "abs": "https://arxiv.org/abs/2506.09338", "authors": ["Young-Jin Park", "Kristjan Greenewald", "Kaveh Alim", "Hao Wang", "Navid Azizan"], "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated and often overestimate\nsuccess probabilities. To address this, we present a calibration approach,\nperformed via quantile regression, that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the inference budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach successfully adapts to each\ninstance and reasoning step when using our calibrated PRMs. Experiments on\nmathematical reasoning benchmarks show that (i) our PRM calibration method\nsuccessfully achieves small calibration error, outperforming the baseline\nmethods, (ii) calibration is crucial for enabling effective adaptive scaling,\nand (iii) the proposed IAS strategy reduces inference costs while maintaining\nfinal answer accuracy, utilizing less compute on more confident problems as\ndesired.", "AI": {"tldr": "The paper introduces a calibration method for Process Reward Models (PRMs) using quantile regression to improve alignment with true success probabilities. It also proposes an instance-adaptive scaling (IAS) framework to dynamically adjust inference budgets, reducing costs while maintaining accuracy.", "motivation": "State-of-the-art PRMs are poorly calibrated and often overestimate success probabilities, necessitating a method to align PRM outputs with true success rates.", "method": "The approach involves calibrating PRMs via quantile regression and using these calibrated estimates to dynamically adjust inference budgets in the IAS framework.", "result": "Experiments show the calibration method reduces error, outperforming baselines, and IAS reduces inference costs without sacrificing accuracy.", "conclusion": "Calibration is key for effective adaptive scaling, and the IAS framework successfully balances cost and accuracy."}}
{"id": "2506.09272", "pdf": "https://arxiv.org/pdf/2506.09272", "abs": "https://arxiv.org/abs/2506.09272", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Antonin Berthon", "Mihaela van der Schaar"], "title": "G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration", "categories": ["cs.LG", "stat.ML", "68T05, 68U20, 62F15", "I.2.6; I.6.5; G.3"], "comment": "Accepted at the 42nd International Conference on Machine Learning\n  (ICML 2025). 9 pages, 3 figures", "summary": "Constructing robust simulators is essential for asking \"what if?\" questions\nand guiding policy in critical domains like healthcare and logistics. However,\nexisting methods often struggle, either failing to generalize beyond historical\ndata or, when using Large Language Models (LLMs), suffering from inaccuracies\nand poor empirical alignment. We introduce G-Sim, a hybrid framework that\nautomates simulator construction by synergizing LLM-driven structural design\nwith rigorous empirical calibration. G-Sim employs an LLM in an iterative loop\nto propose and refine a simulator's core components and causal relationships,\nguided by domain knowledge. This structure is then grounded in reality by\nestimating its parameters using flexible calibration techniques. Specifically,\nG-Sim can leverage methods that are both likelihood-free and gradient-free with\nrespect to the simulator, such as gradient-free optimization for direct\nparameter estimation or simulation-based inference for obtaining a posterior\ndistribution over parameters. This allows it to handle non-differentiable and\nstochastic simulators. By integrating domain priors with empirical evidence,\nG-Sim produces reliable, causally-informed simulators, mitigating\ndata-inefficiency and enabling robust system-level interventions for complex\ndecision-making.", "AI": {"tldr": "G-Sim is a hybrid framework combining LLM-driven structural design with empirical calibration to build reliable simulators for complex decision-making.", "motivation": "Existing simulator methods often fail to generalize or align empirically, especially when using LLMs. G-Sim addresses these limitations by integrating domain knowledge and flexible calibration techniques.", "method": "G-Sim uses an LLM in an iterative loop to design and refine simulator components and causal relationships. It then grounds these structures in reality using likelihood-free and gradient-free calibration methods.", "result": "G-Sim produces causally-informed, reliable simulators that mitigate data inefficiency and support robust system-level interventions.", "conclusion": "G-Sim offers a scalable solution for constructing accurate simulators, enhancing decision-making in critical domains like healthcare and logistics."}}
{"id": "2506.09427", "pdf": "https://arxiv.org/pdf/2506.09427", "abs": "https://arxiv.org/abs/2506.09427", "authors": ["Yukang Feng", "Jianwen Sun", "Chuanhao Li", "Zizhen Li", "Jiaxin Ai", "Fanrui Zhang", "Yifan Chang", "Sizhuo Zhou", "Shenglin Zhang", "Yu Dai", "Kaipeng Zhang"], "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.", "AI": {"tldr": "The paper introduces InterSyn, a large-scale multimodal dataset, and SynJudge, an evaluation tool, to improve LMMs' ability to generate tightly interleaved image-text outputs.", "motivation": "Current LMMs struggle with generating tightly interleaved image-text outputs due to limited training datasets.", "method": "The SEIR method constructs InterSyn, a dataset with multi-turn, instruction-driven dialogues and automated quality refinement. SynJudge evaluates multimodal outputs.", "result": "SEIR improves dataset quality, and LMMs trained on InterSyn show uniform performance gains.", "conclusion": "InterSyn and SynJudge advance multimodal systems by addressing dataset and evaluation limitations."}}
{"id": "2506.09560", "pdf": "https://arxiv.org/pdf/2506.09560", "abs": "https://arxiv.org/abs/2506.09560", "authors": ["Stefan Krsteski", "Matea Tashkovska", "Borjan Sazdov", "Hristijan Gjoreski", "Branislav Gerazov"], "title": "Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language", "categories": ["cs.CL"], "comment": "Camera-ready version accepted at SlavNLP-2025@ACL", "summary": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data.", "AI": {"tldr": "The paper introduces resources to advance LLMs for Macedonian, including a large corpus, an instruction dataset, and an evaluation suite. They train an 8B-parameter model (domestic-yak) that outperforms baselines and rivals larger models.", "motivation": "To address the limited capabilities of LLMs for low-resource languages like Macedonian, the authors aim to provide tools and datasets to support research and adoption.", "method": "They compile a 40GB Macedonian corpus, a 106k-instruction dataset, and a benchmark suite. They train domestic-yak, an 8B-parameter model, and evaluate it against baselines.", "result": "domestic-yak outperforms all 8B-parameter models and matches larger models. Native speakers prefer it for grammatical and cultural accuracy.", "conclusion": "The released resources and model set a foundation for advancing LLMs in underrepresented languages, with all materials publicly available."}}
{"id": "2506.09347", "pdf": "https://arxiv.org/pdf/2506.09347", "abs": "https://arxiv.org/abs/2506.09347", "authors": ["Xuemei Cao", "Hanlin Gu", "Xin Yang", "Bingjun Wei", "Haoyang Liang", "Xiangkun Wang", "Tianrui Li"], "title": "ErrorEraser: Unlearning Data Bias for Improved Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages", "summary": "Continual Learning (CL) primarily aims to retain knowledge to prevent\ncatastrophic forgetting and transfer knowledge to facilitate learning new\ntasks. Unlike traditional methods, we propose a novel perspective: CL not only\nneeds to prevent forgetting, but also requires intentional forgetting.This\narises from existing CL methods ignoring biases in real-world data, leading the\nmodel to learn spurious correlations that transfer and amplify across tasks.\nFrom feature extraction and prediction results, we find that data biases\nsimultaneously reduce CL's ability to retain and transfer knowledge. To address\nthis, we propose ErrorEraser, a universal plugin that removes erroneous\nmemories caused by biases in CL, enhancing performance in both new and old\ntasks. ErrorEraser consists of two modules: Error Identification and Error\nErasure. The former learns the probability density distribution of task data in\nthe feature space without prior knowledge, enabling accurate identification of\npotentially biased samples. The latter ensures only erroneous knowledge is\nerased by shifting the decision space of representative outlier samples.\nAdditionally, an incremental feature distribution learning strategy is designed\nto reduce the resource overhead during error identification in downstream\ntasks. Extensive experimental results show that ErrorEraser significantly\nmitigates the negative impact of data biases, achieving higher accuracy and\nlower forgetting rates across three types of CL methods. The code is available\nat https://github.com/diadai/ErrorEraser.", "AI": {"tldr": "ErrorEraser is a plugin for Continual Learning (CL) that addresses data biases by identifying and erasing erroneous memories, improving performance in both new and old tasks.", "motivation": "Existing CL methods ignore biases in real-world data, leading to spurious correlations that hinder knowledge retention and transfer.", "method": "ErrorEraser consists of Error Identification (learning data distribution to spot biases) and Error Erasure (shifting decision space to remove errors). It also uses incremental feature distribution learning to reduce overhead.", "result": "ErrorEraser significantly reduces the impact of data biases, achieving higher accuracy and lower forgetting rates across CL methods.", "conclusion": "The proposed method effectively mitigates bias-induced errors in CL, enhancing overall performance."}}
{"id": "2506.09279", "pdf": "https://arxiv.org/pdf/2506.09279", "abs": "https://arxiv.org/abs/2506.09279", "authors": ["Ziyi Chen", "Yiyang Liu", "Mattia Prosperi", "Krishna Vaddiparti", "Robert L Cook", "Jiang Bian", "Yi Guo", "Yonghui Wu"], "title": "A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Objective: To characterize stigma dimensions, social, and related behavioral\ncircumstances in people living with HIV (PLWHs) seeking care, using natural\nlanguage processing methods applied to a large collection of electronic health\nrecord (EHR) clinical notes from a large integrated health system in the\nsoutheast United States. Methods: We identified 9,140 cohort of PLWHs from the\nUF Health IDR and performed topic modeling analysis using Latent Dirichlet\nAllocation (LDA) to uncover stigma dimensions, social, and related behavioral\ncircumstances. Domain experts created a seed list of HIV-related stigma\nkeywords, then applied a snowball strategy to iteratively review notes for\nadditional terms until saturation was reached. To identify more target topics,\nwe tested three keyword-based filtering strategies. Domain experts manually\nreviewed the detected topics using the prevalent terms and key discussion\ntopics. Word frequency analysis was used to highlight the prevalent terms\nassociated with each topic. In addition, we conducted topic variation analysis\namong subgroups to examine differences across age and sex-specific\ndemographics. Results and Conclusion: Topic modeling on sentences containing at\nleast one keyword uncovered a wide range of topic themes associated with\nHIV-related stigma, social, and related behaviors circumstances, including\n\"Mental Health Concern and Stigma\", \"Social Support and Engagement\", \"Limited\nHealthcare Access and Severe Illness\", \"Treatment Refusal and Isolation\" and so\non. Topic variation analysis across age subgroups revealed differences.\nExtracting and understanding the HIV-related stigma dimensions, social, and\nrelated behavioral circumstances from EHR clinical notes enables scalable,\ntime-efficient assessment, overcoming the limitations of traditional\nquestionnaires and improving patient outcomes.", "AI": {"tldr": "The study uses NLP and topic modeling on EHR notes to analyze HIV-related stigma, social, and behavioral factors in PLWHs, revealing key themes and demographic variations.", "motivation": "To overcome limitations of traditional questionnaires by leveraging EHR data for scalable, efficient assessment of HIV-related stigma and social factors.", "method": "Identified 9,140 PLWHs, applied LDA topic modeling with keyword filtering, and manually reviewed topics. Analyzed word frequency and demographic variations.", "result": "Uncovered themes like 'Mental Health Concern and Stigma' and 'Limited Healthcare Access.' Demographic differences were noted.", "conclusion": "EHR-based NLP provides scalable stigma assessment, improving patient outcomes beyond traditional methods."}}
{"id": "2506.09429", "pdf": "https://arxiv.org/pdf/2506.09429", "abs": "https://arxiv.org/abs/2506.09429", "authors": ["Swadhin Das", "Divyansh Mundra", "Priyanshu Dayal", "Raksha Sharma"], "title": "A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based models have achieved strong performance in remote sensing\nimage captioning by capturing long-range dependencies and contextual\ninformation. However, their practical deployment is hindered by high\ncomputational costs, especially in multi-modal frameworks that employ separate\ntransformer-based encoders and decoders. In addition, existing remote sensing\nimage captioning models primarily focus on high-level semantic extraction while\noften overlooking fine-grained structural features such as edges, contours, and\nobject boundaries. To address these challenges, a lightweight transformer\narchitecture is proposed by reducing the dimensionality of the encoder layers\nand employing a distilled version of GPT-2 as the decoder. A knowledge\ndistillation strategy is used to transfer knowledge from a more complex teacher\nmodel to improve the performance of the lightweight network. Furthermore, an\nedge-aware enhancement strategy is incorporated to enhance image representation\nand object boundary understanding, enabling the model to capture fine-grained\nspatial details in remote sensing images. Experimental results demonstrate that\nthe proposed approach significantly improves caption quality compared to\nstate-of-the-art methods.", "AI": {"tldr": "A lightweight transformer architecture with knowledge distillation and edge-aware enhancement improves remote sensing image captioning by reducing computational costs and capturing fine-grained details.", "motivation": "Transformer-based models for remote sensing image captioning face high computational costs and neglect fine-grained structural features.", "method": "Proposes a lightweight transformer with reduced encoder dimensionality, a distilled GPT-2 decoder, knowledge distillation, and edge-aware enhancement.", "result": "Significantly improves caption quality compared to state-of-the-art methods.", "conclusion": "The approach effectively balances performance and efficiency while capturing fine-grained spatial details."}}
{"id": "2506.09566", "pdf": "https://arxiv.org/pdf/2506.09566", "abs": "https://arxiv.org/abs/2506.09566", "authors": ["Bla\u017e \u0160krlj", "Boshko Koloski", "Senja Pollak", "Nada Lavra\u010d"], "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To-appear as a book chapter", "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "AI": {"tldr": "A survey on integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) to enhance reasoning and knowledge tasks, highlighting gaps and future directions.", "motivation": "To explore how structured knowledge from KGs can improve LLMs' factual grounding and reasoning, and vice versa.", "method": "Systematic categorization into KG-enhanced LLMs and LLM-augmented KGs, with analysis of scalability, efficiency, and data quality.", "result": "Identified mutual benefits and gaps, emphasizing the need for scalable, efficient, and high-quality integration.", "conclusion": "Proposes future research in neuro-symbolic integration, dynamic KG updating, data reliability, and ethics for advanced knowledge systems."}}
{"id": "2506.09354", "pdf": "https://arxiv.org/pdf/2506.09354", "abs": "https://arxiv.org/abs/2506.09354", "authors": ["Kellie Yu Hui Sim", "Roy Ka-Wei Lee", "Kenny Tsu Wei Choo"], "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions", "categories": ["cs.HC", "cs.AI", "H.5.0"], "comment": null, "summary": "Mental health is a growing global concern, prompting interest in AI-driven\nsolutions to expand access to psychosocial support. Peer support, grounded in\nlived experience, offers a valuable complement to professional care. However,\nvariability in training, effectiveness, and definitions raises concerns about\nquality, consistency, and safety. Large Language Models (LLMs) present new\nopportunities to enhance peer support interactions, particularly in real-time,\ntext-based interactions. We present and evaluate an AI-supported system with an\nLLM-simulated distressed client, context-sensitive LLM-generated suggestions,\nand real-time emotion visualisations. 2 mixed-methods studies with 12 peer\nsupporters and 5 mental health professionals (i.e., experts) examined the\nsystem's effectiveness and implications for practice. Both groups recognised\nits potential to enhance training and improve interaction quality. However, we\nfound a key tension emerged: while peer supporters engaged meaningfully,\nexperts consistently flagged critical issues in peer supporter responses, such\nas missed distress cues and premature advice-giving. This misalignment\nhighlights potential limitations in current peer support training, especially\nin emotionally charged contexts where safety and fidelity to best practices are\nessential. Our findings underscore the need for standardised, psychologically\ngrounded training, especially as peer support scales globally. They also\ndemonstrate how LLM-supported systems can scaffold this development--if\ndesigned with care and guided by expert oversight. This work contributes to\nemerging conversations on responsible AI integration in mental health and the\nevolving role of LLMs in augmenting peer-delivered care.", "AI": {"tldr": "AI-driven peer support system using LLMs shows promise but reveals gaps in training and alignment between peer supporters and experts.", "motivation": "Addressing global mental health concerns by leveraging AI to enhance peer support interactions, ensuring quality and safety.", "method": "Developed an AI-supported system with LLM-simulated clients, context-sensitive suggestions, and emotion visualizations. Evaluated through mixed-methods studies with peer supporters and experts.", "result": "Peer supporters and experts recognized the system's potential for training and interaction improvement, but experts identified critical response issues like missed distress cues.", "conclusion": "Highlights the need for standardized, expert-guided training in peer support and careful AI integration to ensure safety and effectiveness."}}
{"id": "2506.09316", "pdf": "https://arxiv.org/pdf/2506.09316", "abs": "https://arxiv.org/abs/2506.09316", "authors": ["Yeonju Ro", "Zhenyu Zhang", "Souvik Kundu", "Zhangyang Wang", "Aditya Akella"], "title": "On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at capturing global token dependencies via\nself-attention but face prohibitive compute and memory costs on lengthy inputs.\nWhile sub-quadratic methods (e.g., linear attention) can reduce these costs,\nthey often degrade accuracy due to overemphasizing recent tokens. In this work,\nwe first propose \\textit{dual-state linear attention} (\\textbf{\\dsla}), a novel\ndesign that maintains two specialized hidden states-one for preserving\nhistorical context and one for tracking recency-thereby mitigating the\nshort-range bias typical of linear-attention architectures. To further balance\nefficiency and accuracy under dynamic workload conditions, we introduce\n\\textbf{\\serve}, an online \\textit{adaptive distillation} framework that\nprogressively replaces Transformer layers with DSLA layers at inference time,\nguided by a sensitivity-based layer ordering. \\serve\\ uses a chained\nfine-tuning strategy to ensure that each newly converted DSLA layer remains\nconsistent with previously replaced layers, preserving the overall quality.\nExtensive evaluations on commonsense reasoning, long-context QA, and text\nsummarization demonstrate that \\serve\\ yields \\textbf{2.3x} faster inference\nthan Llama2-7B and \\textbf{3.0x} faster than the hybrid Zamba-7B, while\nretaining comparable performance across downstream tasks. Our ablation studies\nshow that DSLA's dual states capture both global and local dependencies,\naddressing the historical-token underrepresentation seen in prior linear\nattentions. Codes are available at https://github.com/utnslab/DSLA-Serve.", "AI": {"tldr": "The paper introduces DSLA (dual-state linear attention) and Serve, an adaptive distillation framework, to improve efficiency and accuracy in large language models by balancing historical and recent token dependencies.", "motivation": "Address the prohibitive compute and memory costs of LLMs on lengthy inputs and mitigate the short-range bias in sub-quadratic methods like linear attention.", "method": "Propose DSLA with dual hidden states for historical context and recency, and Serve for adaptive distillation, replacing Transformer layers with DSLA layers at inference.", "result": "Serve achieves 2.3x faster inference than Llama2-7B and 3.0x faster than Zamba-7B while maintaining comparable performance.", "conclusion": "DSLA and Serve effectively balance efficiency and accuracy, capturing both global and local dependencies in LLMs."}}
{"id": "2506.09445", "pdf": "https://arxiv.org/pdf/2506.09445", "abs": "https://arxiv.org/abs/2506.09445", "authors": ["Ayush Gupta", "Anirban Roy", "Rama Chellappa", "Nathaniel D. Bastian", "Alvaro Velasquez", "Susmit Jha"], "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We address the problem of video question answering (video QA) with temporal\ngrounding in a weakly supervised setup, without any temporal annotations. Given\na video and a question, we generate an open-ended answer grounded with the\nstart and end time. For this task, we propose TOGA: a vision-language model for\nTemporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune\nTOGA to jointly generate the answer and the temporal grounding. We operate in a\nweakly supervised setup where the temporal grounding annotations are not\navailable. We generate pseudo labels for temporal grounding and ensure the\nvalidity of these labels by imposing a consistency constraint between the\nquestion of a grounding response and the response generated by a question\nreferring to the same temporal segment. We notice that jointly generating the\nanswers with the grounding improves performance on question answering as well\nas grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For\ngrounded QA, we consider the NExT-GQA benchmark which is designed to evaluate\nweakly supervised grounded question answering. For open-ended QA, we consider\nthe MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art\nperformance for both tasks on these benchmarks.", "AI": {"tldr": "TOGA is a vision-language model for weakly supervised video QA with temporal grounding, achieving state-of-the-art results.", "motivation": "Addressing video QA with temporal grounding without temporal annotations, leveraging weak supervision.", "method": "TOGA is instruct-tuned to jointly generate answers and temporal grounding using pseudo labels and consistency constraints.", "result": "State-of-the-art performance on NExT-GQA, MSVD-QA, and ActivityNet-QA benchmarks.", "conclusion": "Jointly generating answers and grounding improves performance, validating TOGA's effectiveness in weakly supervised setups."}}
{"id": "2506.09591", "pdf": "https://arxiv.org/pdf/2506.09591", "abs": "https://arxiv.org/abs/2506.09591", "authors": ["Stefan Arnold"], "title": "Memorization in Language Models through the Lens of Intrinsic Dimension", "categories": ["cs.CL"], "comment": null, "summary": "Language Models (LMs) are prone to memorizing parts of their data during\ntraining and unintentionally emitting them at generation time, raising concerns\nabout privacy leakage and disclosure of intellectual property. While previous\nresearch has identified properties such as context length, parameter size, and\nduplication frequency, as key drivers of unintended memorization, little is\nknown about how the latent structure modulates this rate of memorization. We\ninvestigate the role of Intrinsic Dimension (ID), a geometric proxy for the\nstructural complexity of a sequence in latent space, in modulating\nmemorization. Our findings suggest that ID acts as a suppressive signal for\nmemorization: compared to low-ID sequences, high-ID sequences are less likely\nto be memorized, particularly in overparameterized models and under sparse\nexposure. These findings highlight the interaction between scale, exposure, and\ncomplexity in shaping memorization.", "AI": {"tldr": "High Intrinsic Dimension (ID) sequences are less likely to be memorized by language models, especially in overparameterized models and sparse exposure scenarios.", "motivation": "To understand how latent structure (measured by ID) affects memorization in language models, addressing privacy and intellectual property concerns.", "method": "Investigates the role of Intrinsic Dimension (ID) as a geometric proxy for sequence complexity in modulating memorization.", "result": "High-ID sequences suppress memorization compared to low-ID ones, particularly in overparameterized models and sparse exposure.", "conclusion": "Memorization is shaped by the interplay of model scale, exposure frequency, and sequence complexity (ID)."}}
{"id": "2506.09362", "pdf": "https://arxiv.org/pdf/2506.09362", "abs": "https://arxiv.org/abs/2506.09362", "authors": ["Kellie Yu Hui Sim", "Kenny Tsu Wei Choo"], "title": "\"I Said Things I Needed to Hear Myself\": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore", "categories": ["cs.HC", "cs.AI", "H.5.0"], "comment": null, "summary": "Peer support plays a vital role in expanding access to mental health care by\nproviding empathetic, community-based support outside formal clinical systems.\nAs digital platforms increasingly mediate such support, the design and impact\nof these technologies remain under-examined, particularly in Asian contexts.\nThis paper presents findings from an interview study with 20 peer supporters in\nSingapore, who operate across diverse online, offline, and hybrid environments.\nThrough a thematic analysis, we unpack how participants start, conduct, and\nsustain peer support, highlighting their motivations, emotional labour, and the\nsociocultural dimensions shaping their practices. Building on this grounded\nunderstanding, we surface design directions for culturally responsive digital\ntools that scaffold rather than supplant relational care. Drawing insights from\nqualitative accounts, we offer a situated perspective on how AI might\nresponsibly augment peer support. This research contributes to human-centred\ncomputing by articulating the lived realities of peer supporters and proposing\ndesign implications for trustworthy and context-sensitive AI in mental health.", "AI": {"tldr": "The paper explores peer support in mental health, focusing on digital platforms in Singapore, and suggests culturally responsive AI design.", "motivation": "To understand the role and impact of digital peer support in mental health, especially in under-examined Asian contexts like Singapore.", "method": "Conducted interviews with 20 peer supporters in Singapore, analyzing their practices through thematic analysis.", "result": "Revealed motivations, emotional labor, and sociocultural factors in peer support, leading to design directions for AI tools.", "conclusion": "Proposes AI should augment peer support responsibly, with culturally sensitive design, based on qualitative insights."}}
{"id": "2506.09332", "pdf": "https://arxiv.org/pdf/2506.09332", "abs": "https://arxiv.org/abs/2506.09332", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "title": "Natural Language Guided Ligand-Binding Protein Design", "categories": ["cs.LG", "cs.CE", "cs.CL"], "comment": null, "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications.", "AI": {"tldr": "InstructPro is a protein generative model that uses natural language instructions and ligand formulas to design ligand-binding proteins, outperforming existing baselines.", "motivation": "Designing proteins that bind to specific ligands is crucial in biology and chemistry, but AI models are limited by scarce protein-ligand complex data. Human-curated text descriptions offer an alternative resource.", "method": "Proposes InstructPro, a model trained on a large dataset (InstructProBench) with triples of function descriptions, ligand formulas, and protein sequences. Two variants (1B and 3B parameters) are developed.", "result": "InstructPro-1B achieves an 81.52% docking success rate and low RMSD (4.026\u00c5). InstructPro-3B further reduces RMSD to 2.527\u00c5, outperforming ProGen2, ESM3, and Pinal.", "conclusion": "InstructPro demonstrates the ability to generate ligand-binding proteins from textual instructions, offering a promising approach for protein design."}}
{"id": "2506.09446", "pdf": "https://arxiv.org/pdf/2506.09446", "abs": "https://arxiv.org/abs/2506.09446", "authors": ["Yuhe Ding", "Jian Liang", "Bo Jiang", "Zi Wang", "Aihua Zheng", "Bin Luo"], "title": "Harmonizing and Merging Source Models for CLIP-based Domain Generalization", "categories": ["cs.CV"], "comment": null, "summary": "CLIP-based domain generalization aims to improve model generalization to\nunseen domains by leveraging the powerful zero-shot classification capabilities\nof CLIP and multiple source datasets. Existing methods typically train a single\nmodel across multiple source domains to capture domain-shared information.\nHowever, this paradigm inherently suffers from two types of conflicts: 1)\nsample conflicts, arising from noisy samples and extreme domain shifts among\nsources; and 2) optimization conflicts, stemming from competition and\ntrade-offs during multi-source training. Both hinder the generalization and\nlead to suboptimal solutions. Recent studies have shown that model merging can\neffectively mitigate the competition of multi-objective optimization and\nimprove generalization performance. Inspired by these findings, we propose\nHarmonizing and Merging (HAM), a novel source model merging framework for\nCLIP-based domain generalization. During the training process of the source\nmodels, HAM enriches the source samples without conflicting samples, and\nharmonizes the update directions of all models. Then, a redundancy-aware\nhistorical model merging method is introduced to effectively integrate\nknowledge across all source models. HAM comprehensively consolidates source\ndomain information while enabling mutual enhancement among source models,\nultimately yielding a final model with optimal generalization capabilities.\nExtensive experiments on five widely used benchmark datasets demonstrate the\neffectiveness of our approach, achieving state-of-the-art performance.", "AI": {"tldr": "HAM improves CLIP-based domain generalization by harmonizing and merging source models to avoid conflicts and enhance performance.", "motivation": "Existing methods suffer from sample and optimization conflicts during multi-source training, hindering generalization.", "method": "HAM enriches source samples, harmonizes model updates, and merges models redundantly-aware.", "result": "Achieves state-of-the-art performance on five benchmark datasets.", "conclusion": "HAM effectively consolidates domain information and enhances generalization."}}
{"id": "2506.09627", "pdf": "https://arxiv.org/pdf/2506.09627", "abs": "https://arxiv.org/abs/2506.09627", "authors": ["Nicolas Audinet de Pieuchon", "Adel Daoud", "Connor T. Jerzak", "Moa Johansson", "Richard Johansson"], "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.", "AI": {"tldr": "The paper compares debiasing methods (DSL and PPI) for LLM annotations, finding DSL often outperforms PPI in bias reduction but lacks consistency across datasets, highlighting a bias-variance tradeoff.", "motivation": "LLMs provide cost-effective text annotation but introduce bias compared to experts, affecting downstream analyses. Debiasing methods like DSL and PPI aim to mitigate this, but their finite-sample performance is unclear.", "method": "The study evaluates DSL and PPI by analyzing their performance scaling with expert annotations and comparing them across tasks.", "result": "DSL often reduces bias and improves efficiency more than PPI, but its performance varies across datasets. Both methods work well with large datasets.", "conclusion": "The findings reveal a bias-variance tradeoff in debiasing methods, emphasizing the need for metrics to assess their efficiency in finite samples."}}
{"id": "2506.09368", "pdf": "https://arxiv.org/pdf/2506.09368", "abs": "https://arxiv.org/abs/2506.09368", "authors": ["Yang Liu", "Jing Liu", "Chengfang Li", "Rui Xi", "Wenchao Li", "Liang Cao", "Jin Wang", "Laurence T. Yang", "Junsong Yuan", "Wei Zhou"], "title": "Anomaly Detection and Generation with Diffusion Models: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 11 figures, 13 tables", "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including\ncybersecurity, finance, healthcare, and industrial manufacturing, by\nidentifying unexpected patterns that deviate from established norms in\nreal-world data. Recent advancements in deep learning, specifically diffusion\nmodels (DMs), have sparked significant interest due to their ability to learn\ncomplex data distributions and generate high-fidelity samples, offering a\nrobust framework for unsupervised AD. In this survey, we comprehensively review\nanomaly detection and generation with diffusion models (ADGDM), presenting a\ntutorial-style analysis of the theoretical foundations and practical\nimplementations and spanning images, videos, time series, tabular, and\nmultimodal data. Crucially, unlike existing surveys that often treat anomaly\ndetection and generation as separate problems, we highlight their inherent\nsynergistic relationship. We reveal how DMs enable a reinforcing cycle where\ngeneration techniques directly address the fundamental challenge of anomaly\ndata scarcity, while detection methods provide critical feedback to improve\ngeneration fidelity and relevance, advancing both capabilities beyond their\nindividual potential. A detailed taxonomy categorizes ADGDM methods based on\nanomaly scoring mechanisms, conditioning strategies, and architectural designs,\nanalyzing their strengths and limitations. We final discuss key challenges\nincluding scalability and computational efficiency, and outline promising\nfuture directions such as efficient architectures, conditioning strategies, and\nintegration with foundation models (e.g., visual-language models and large\nlanguage models). By synthesizing recent advances and outlining open research\nquestions, this survey aims to guide researchers and practitioners in\nleveraging DMs for innovative AD solutions across diverse applications.", "AI": {"tldr": "A survey on anomaly detection and generation using diffusion models (ADGDM), highlighting their synergistic relationship and applications across diverse data types.", "motivation": "To address the challenge of anomaly data scarcity and improve detection and generation capabilities by leveraging diffusion models.", "method": "Comprehensive review and taxonomy of ADGDM methods, analyzing anomaly scoring, conditioning strategies, and architectural designs.", "result": "Reveals the reinforcing cycle between anomaly detection and generation, advancing both beyond individual potential.", "conclusion": "Outlines challenges like scalability and future directions, guiding researchers in leveraging DMs for innovative AD solutions."}}
{"id": "2506.09348", "pdf": "https://arxiv.org/pdf/2506.09348", "abs": "https://arxiv.org/abs/2506.09348", "authors": ["Natalie S. Frank"], "title": "Adversarial Surrogate Risk Bounds for Binary Classification", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "37 pages, 2 figures", "summary": "A central concern in classification is the vulnerability of machine learning\nmodels to adversarial attacks. Adversarial training is one of the most popular\ntechniques for training robust classifiers, which involves minimizing an\nadversarial surrogate risk. Recent work characterized when a minimizing\nsequence of an adversarial surrogate risk is also a minimizing sequence of the\nadversarial classification risk for binary classification -- a property known\nas adversarial consistency. However, these results do not address the rate at\nwhich the adversarial classification risk converges to its optimal value for\nsuch a sequence of functions that minimize the adversarial surrogate. This\npaper provides surrogate risk bounds that quantify that convergence rate.\nAdditionally, we derive distribution-dependent surrogate risk bounds in the\nstandard (non-adversarial) learning setting, that may be of independent\ninterest.", "AI": {"tldr": "The paper analyzes the convergence rate of adversarial classification risk for robust classifiers, providing surrogate risk bounds and extending results to standard learning settings.", "motivation": "To address the gap in understanding the convergence rate of adversarial classification risk for robust classifiers trained via adversarial training.", "method": "Derives surrogate risk bounds to quantify the convergence rate and extends these bounds to standard learning settings.", "result": "Provides surrogate risk bounds for adversarial classification risk convergence and distribution-dependent bounds for standard learning.", "conclusion": "The paper advances understanding of adversarial consistency and offers insights applicable to both adversarial and standard learning scenarios."}}
{"id": "2506.09460", "pdf": "https://arxiv.org/pdf/2506.09460", "abs": "https://arxiv.org/abs/2506.09460", "authors": ["Amirreza Khoshbakht", "Erchan Aptoula"], "title": "Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization", "categories": ["cs.CV"], "comment": null, "summary": "Open-set domain generalization(OSDG) for hyperspectral image classification\npresents significant challenges due to the presence of unknown classes in\ntarget domains and the need for models to generalize across multiple unseen\ndomains without target-specific adaptation. Existing domain adaptation methods\nassume access to target domain data during training and fail to address the\nfundamental issue of domain shift when unknown classes are present, leading to\nnegative transfer and reduced classification performance. To address these\nlimitations, we propose a novel open-set domain generalization framework that\ncombines four key components: Spectrum-Invariant Frequency Disentanglement\n(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network\n(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning\n(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty\nDisentanglement (SSUD) for reliable open-set classification. The SIFD module\nextracts domain-invariant spectral features in the frequency domain through\nattention-weighted frequency analysis and domain-agnostic regularization, while\nDCRN captures complementary spectral and spatial information via parallel\npathways with adaptive fusion. EDL provides principled uncertainty estimation\nusing Dirichlet distributions, enabling the SSUD module to make reliable\nopen-set decisions through uncertainty-aware pathway weighting and adaptive\nrejection thresholding. Experimental results on three cross-scene hyperspectral\nclassification tasks show that our approach achieves performance comparable to\nstate-of-the-art domain adaptation methods while requiring no access to the\ntarget domain during training. The implementation will be made available at\nhttps://github.com/amir-khb/SSUDOSDG upon acceptance.", "AI": {"tldr": "Proposes a novel open-set domain generalization framework for hyperspectral image classification, addressing unknown classes and domain shifts without target domain data.", "motivation": "Existing methods fail in open-set scenarios with unknown classes and require target domain data, leading to negative transfer and poor performance.", "method": "Combines Spectrum-Invariant Frequency Disentanglement (SIFD), Dual-Channel Residual Network (DCRN), Evidential Deep Learning (EDL), and Spectral-Spatial Uncertainty Disentanglement (SSUD) for domain-agnostic feature extraction, robust learning, uncertainty quantification, and reliable open-set classification.", "result": "Achieves performance comparable to state-of-the-art domain adaptation methods without target domain data during training.", "conclusion": "The framework effectively addresses open-set domain generalization challenges in hyperspectral image classification."}}
{"id": "2506.09641", "pdf": "https://arxiv.org/pdf/2506.09641", "abs": "https://arxiv.org/abs/2506.09641", "authors": ["Anna Stein", "Kevin Tang"], "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": "Submitted to Interspeech 2025", "summary": "This study compares probabilistic predictors based on information theory with\nNaive Discriminative Learning (NDL) predictors in modeling acoustic word\nduration, focusing on probabilistic reduction. We examine three models using\nthe Buckeye corpus: one with NDL-derived predictors using information-theoretic\nformulas, one with traditional NDL predictors, and one with N-gram\nprobabilistic predictors. Results show that the N-gram model outperforms both\nNDL models, challenging the assumption that NDL is more effective due to its\ncognitive motivation. However, incorporating information-theoretic formulas\ninto NDL improves model performance over the traditional model. This research\nhighlights a) the need to incorporate not only frequency and contextual\npredictability but also average contextual predictability, and b) the\nimportance of combining information-theoretic metrics of predictability and\ninformation derived from discriminative learning in modeling acoustic\nreduction.", "AI": {"tldr": "The study compares NDL and N-gram models for acoustic word duration, finding N-grams outperform NDL but information-theoretic enhancements improve NDL.", "motivation": "To evaluate the effectiveness of NDL and N-gram models in predicting acoustic word duration, focusing on probabilistic reduction.", "method": "Three models were tested: NDL with information-theoretic formulas, traditional NDL, and N-gram predictors, using the Buckeye corpus.", "result": "N-gram model outperformed both NDL models, but information-theoretic enhancements improved NDL performance.", "conclusion": "Highlights the need for combining information-theoretic metrics and discriminative learning in modeling acoustic reduction."}}
{"id": "2506.09373", "pdf": "https://arxiv.org/pdf/2506.09373", "abs": "https://arxiv.org/abs/2506.09373", "authors": ["Jiaqi Tang", "Yu Xia", "Yi-Feng Wu", "Yuwei Hu", "Yuhui Chen", "Qing-Guo Chen", "Xiaogang Xu", "Xiangyu Wu", "Hao Lu", "Yanqing Ma", "Shiyin Lu", "Qifeng Chen"], "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The advent of autonomous agents is transforming interactions with Graphical\nUser Interfaces (GUIs) by employing natural language as a powerful\nintermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods\nin current GUI agents for achieving spatial localization, these methods face\nsubstantial challenges due to their limited capacity to accurately perceive\npositional data. Existing strategies, such as reinforcement learning, often\nfail to assess positional accuracy effectively, thereby restricting their\nutility. In response, we introduce Location Preference Optimization (LPO), a\nnovel approach that leverages locational data to optimize interaction\npreferences. LPO uses information entropy to predict interaction positions by\nfocusing on zones rich in information. Besides, it further introduces a dynamic\nlocation reward function based on physical distance, reflecting the varying\nimportance of interaction positions. Supported by Group Relative Preference\nOptimization (GRPO), LPO facilitates an extensive exploration of GUI\nenvironments and significantly enhances interaction precision. Comprehensive\nexperiments demonstrate LPO's superior performance, achieving SOTA results\nacross both offline benchmarks and real-world online evaluations. Our code will\nbe made publicly available soon, at https://github.com/AIDC-AI/LPO.", "AI": {"tldr": "LPO improves GUI agent interactions by optimizing location preferences using information entropy and dynamic rewards, outperforming existing methods.", "motivation": "Current GUI agents struggle with positional accuracy due to limitations in SFT and reinforcement learning methods.", "method": "LPO leverages locational data and information entropy to predict interaction zones, supported by GRPO for exploration.", "result": "LPO achieves state-of-the-art performance in offline benchmarks and real-world evaluations.", "conclusion": "LPO enhances interaction precision and is a promising advancement for GUI agents."}}
{"id": "2506.09376", "pdf": "https://arxiv.org/pdf/2506.09376", "abs": "https://arxiv.org/abs/2506.09376", "authors": ["Bowen Zheng", "Tianming Yang"], "title": "Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Diffusion distillation is a widely used technique to reduce the sampling cost\nof diffusion models, yet it often requires extensive training, and the student\nperformance tends to be degraded. Recent studies show that incorporating a GAN\nobjective may alleviate these issues, yet the underlying mechanism remains\nunclear. In this work, we first identify a key limitation of distillation:\nmismatched step sizes and parameter numbers between the teacher and the student\nmodel lead them to converge to different local minima, rendering direct\nimitation suboptimal. We further demonstrate that a standalone GAN objective,\nwithout relying a distillation loss, overcomes this limitation and is\nsufficient to convert diffusion models into efficient one-step generators.\nBased on this finding, we propose that diffusion training may be viewed as a\nform of generative pre-training, equipping models with capabilities that can be\nunlocked through lightweight GAN fine-tuning. Supporting this view, we create a\none-step generation model by fine-tuning a pre-trained model with 85% of\nparameters frozen, achieving strong performance with only 0.2M images and\nnear-SOTA results with 5M images. We further present a frequency-domain\nanalysis that may explain the one-step generative capability gained in\ndiffusion training. Overall, our work provides a new perspective for diffusion\ntraining, highlighting its role as a powerful generative pre-training process,\nwhich can be the basis for building efficient one-step generation models.", "AI": {"tldr": "The paper identifies limitations in diffusion distillation and proposes using a GAN objective to convert diffusion models into efficient one-step generators, achieving strong performance with minimal fine-tuning.", "motivation": "To address the inefficiency and performance degradation in diffusion distillation by exploring the role of GAN objectives and redefining diffusion training as generative pre-training.", "method": "The study replaces distillation loss with a standalone GAN objective, fine-tunes pre-trained models with most parameters frozen, and analyzes results in the frequency domain.", "result": "The approach achieves strong performance with only 0.2M images and near-SOTA results with 5M images, demonstrating the effectiveness of GAN fine-tuning.", "conclusion": "Diffusion training serves as powerful generative pre-training, enabling efficient one-step generation models through lightweight GAN fine-tuning."}}
{"id": "2506.09469", "pdf": "https://arxiv.org/pdf/2506.09469", "abs": "https://arxiv.org/abs/2506.09469", "authors": ["Maria Damanaki", "Nikos Piperigkos", "Alexandros Gkillas", "Aris S. Lalos"], "title": "Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing", "categories": ["cs.CV"], "comment": "2025 IEEE International Conference on Multimedia and Expo Workshops,\n  3DMM - 3D Multimedia Analytics, Search and Generation", "summary": "Multi-Object Tracking (MOT) plays a crucial role in autonomous driving\nsystems, as it lays the foundations for advanced perception and precise path\nplanning modules. Nonetheless, single agent based MOT lacks in sensing\nsurroundings due to occlusions, sensors failures, etc. Hence, the integration\nof multiagent information is essential for comprehensive understanding of the\nenvironment. This paper proposes a novel Cooperative MOT framework for tracking\nobjects in 3D LiDAR scene by formulating and solving a graph topology-aware\noptimization problem so as to fuse information coming from multiple vehicles.\nBy exploiting a fully connected graph topology defined by the detected bounding\nboxes, we employ the Graph Laplacian processing optimization technique to\nsmooth the position error of bounding boxes and effectively combine them. In\nthat manner, we reveal and leverage inherent coherences of diverse multi-agent\ndetections, and associate the refined bounding boxes to tracked objects at two\nstages, optimizing localization and tracking accuracies. An extensive\nevaluation study has been conducted, using the real-world V2V4Real dataset,\nwhere the proposed method significantly outperforms the baseline frameworks,\nincluding the state-of-the-art deep-learning DMSTrack and V2V4Real, in various\ntesting sequences.", "AI": {"tldr": "A novel Cooperative MOT framework for 3D LiDAR scenes improves tracking by fusing multi-agent data via graph topology-aware optimization.", "motivation": "Single-agent MOT struggles with occlusions and sensor failures, necessitating multi-agent collaboration for better environmental understanding.", "method": "Uses a fully connected graph topology and Graph Laplacian optimization to refine bounding box positions and associate them with tracked objects.", "result": "Outperforms baseline frameworks like DMSTrack and V2V4Real on the V2V4Real dataset.", "conclusion": "The proposed method enhances localization and tracking accuracy by leveraging multi-agent coherence."}}
{"id": "2506.09643", "pdf": "https://arxiv.org/pdf/2506.09643", "abs": "https://arxiv.org/abs/2506.09643", "authors": ["Harry Walsh", "Maksym Ivashechkin", "Richard Bowden"], "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Machine learning models fundamentally rely on large quantities of\nhigh-quality data. Collecting the necessary data for these models can be\nchallenging due to cost, scarcity, and privacy restrictions. Signed languages\nare visual languages used by the deaf community and are considered low-resource\nlanguages. Sign language datasets are often orders of magnitude smaller than\ntheir spoken language counterparts. Sign Language Production is the task of\ngenerating sign language videos from spoken language sentences, while Sign\nLanguage Translation is the reverse translation task. Here, we propose\nleveraging recent advancements in Sign Language Production to augment existing\nsign language datasets and enhance the performance of Sign Language Translation\nmodels. For this, we utilize three techniques: a skeleton-based approach to\nproduction, sign stitching, and two photo-realistic generative models, SignGAN\nand SignSplat. We evaluate the effectiveness of these techniques in enhancing\nthe performance of Sign Language Translation models by generating variation in\nthe signer's appearance and the motion of the skeletal data. Our results\ndemonstrate that the proposed methods can effectively augment existing datasets\nand enhance the performance of Sign Language Translation models by up to 19%,\npaving the way for more robust and accurate Sign Language Translation systems,\neven in resource-constrained environments.", "AI": {"tldr": "The paper proposes using Sign Language Production techniques to augment small sign language datasets, improving Sign Language Translation model performance by up to 19%.", "motivation": "Sign languages are low-resource, making dataset collection challenging. Augmenting existing datasets can enhance translation models.", "method": "Uses skeleton-based production, sign stitching, and generative models (SignGAN, SignSplat) to create dataset variations.", "result": "Performance of Sign Language Translation models improved by up to 19%.", "conclusion": "The methods enable robust translation systems in resource-limited settings."}}
{"id": "2506.09383", "pdf": "https://arxiv.org/pdf/2506.09383", "abs": "https://arxiv.org/abs/2506.09383", "authors": ["Chengtian Ma", "Yunyue Wei", "Chenhui Zuo", "Chen Zhang", "Yanan Sui"], "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Balance control is important for human and bipedal robotic systems. While\ndynamic balance during locomotion has received considerable attention,\nquantitative understanding of static balance and falling remains limited. This\nwork presents a hierarchical control pipeline for simulating human balance via\na comprehensive whole-body musculoskeletal system. We identified spatiotemporal\ndynamics of balancing during stable standing, revealed the impact of muscle\ninjury on balancing behavior, and generated fall contact patterns that aligned\nwith clinical data. Furthermore, our simulated hip exoskeleton assistance\ndemonstrated improvement in balance maintenance and reduced muscle effort under\nperturbation. This work offers unique muscle-level insights into human balance\ndynamics that are challenging to capture experimentally. It could provide a\nfoundation for developing targeted interventions for individuals with balance\nimpairments and support the advancement of humanoid robotic systems.", "AI": {"tldr": "A hierarchical control pipeline simulates human balance using a musculoskeletal system, revealing balance dynamics, effects of muscle injury, and validating fall patterns with clinical data. Hip exoskeleton assistance improved balance and reduced muscle effort.", "motivation": "To address the limited quantitative understanding of static balance and falling, and to provide muscle-level insights for balance impairments and robotic systems.", "method": "A hierarchical control pipeline applied to a whole-body musculoskeletal system, including simulation of stable standing, muscle injury effects, and fall contact patterns. Hip exoskeleton assistance was also tested.", "result": "Identified balance dynamics, validated fall patterns with clinical data, and showed improved balance and reduced muscle effort with exoskeleton assistance.", "conclusion": "The study offers valuable muscle-level insights into balance dynamics, aiding interventions for balance impairments and advancing humanoid robotics."}}
{"id": "2506.09398", "pdf": "https://arxiv.org/pdf/2506.09398", "abs": "https://arxiv.org/abs/2506.09398", "authors": ["Haiyang Yu", "Yuchao Lin", "Xuan Zhang", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames", "categories": ["cs.LG", "physics.comp-ph"], "comment": "Code available at: https://github.com/divelab/AIRS", "summary": "We consider the task of predicting Hamiltonian matrices to accelerate\nelectronic structure calculations, which plays an important role in physics,\nchemistry, and materials science. Motivated by the inherent relationship\nbetween the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local\nframe, we propose a novel and efficient network, called QHNetV2, that achieves\nglobal SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor\nproducts. This is achieved by introducing a set of new efficient and powerful\nSO(2)-equivariant operations and performing all off-diagonal feature updates\nand message passing within SO(2) local frames, thereby eliminating the need of\nSO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed\nwithin the SO(2) local frame at each node to fuse node features, mimicking the\nsymmetric contraction operation. Extensive experiments on the large QH9 and\nMD17 datasets demonstrate that our model achieves superior performance across a\nwide range of molecular structures and trajectories, highlighting its strong\ngeneralization capability. The proposed SO(2) operations on SO(2) local frames\noffer a promising direction for scalable and symmetry-aware learning of\nelectronic structures. Our code will be released as part of the AIRS library\nhttps://github.com/divelab/AIRS.", "AI": {"tldr": "QHNetV2, a novel network, predicts Hamiltonian matrices efficiently using SO(2)-equivariant operations, avoiding costly SO(3) tensor products, and shows strong performance on molecular datasets.", "motivation": "Accelerating electronic structure calculations in physics, chemistry, and materials science by leveraging the relationship between Hamiltonian matrices and SO(2) local frames.", "method": "Introduces SO(2)-equivariant operations and performs feature updates within SO(2) local frames, eliminating SO(3) tensor products. Uses continuous SO(2) tensor products for node feature fusion.", "result": "Superior performance on QH9 and MD17 datasets, demonstrating strong generalization across molecular structures and trajectories.", "conclusion": "SO(2) operations on local frames provide a scalable, symmetry-aware approach for electronic structure learning, with code released in the AIRS library."}}
{"id": "2506.09473", "pdf": "https://arxiv.org/pdf/2506.09473", "abs": "https://arxiv.org/abs/2506.09473", "authors": ["Cheng Chen", "Yunpeng Zhai", "Yifan Zhao", "Jinyang Gao", "Bolin Ding", "Jia Li"], "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning", "categories": ["cs.CV"], "comment": "10 pages, 6 figures, CVPR 2025", "summary": "In-context learning (ICL), a predominant trend in instruction learning, aims\nat enhancing the performance of large language models by providing clear task\nguidance and examples, improving their capability in task understanding and\nexecution. This paper investigates ICL on Large Vision-Language Models (LVLMs)\nand explores the policies of multi-modal demonstration selection. Existing\nresearch efforts in ICL face significant challenges: First, they rely on\npre-defined demonstrations or heuristic selecting strategies based on human\nintuition, which are usually inadequate for covering diverse task requirements,\nleading to sub-optimal solutions; Second, individually selecting each\ndemonstration fails in modeling the interactions between them, resulting in\ninformation redundancy. Unlike these prevailing efforts, we propose a new\nexploration-exploitation reinforcement learning framework, which explores\npolicies to fuse multi-modal information and adaptively select adequate\ndemonstrations as an integrated whole. The framework allows LVLMs to optimize\nthemselves by continually refining their demonstrations through\nself-exploration, enabling the ability to autonomously identify and generate\nthe most effective selection policies for in-context learning. Experimental\nresults verify the superior performance of our approach on four Visual\nQuestion-Answering (VQA) datasets, demonstrating its effectiveness in enhancing\nthe generalization capability of few-shot LVLMs.", "AI": {"tldr": "The paper introduces a reinforcement learning framework for multi-modal demonstration selection in Large Vision-Language Models (LVLMs) to improve in-context learning (ICL).", "motivation": "Existing ICL methods rely on pre-defined or heuristic demonstration selection, which is inadequate for diverse tasks and ignores interactions between demonstrations.", "method": "A reinforcement learning framework is proposed to explore and exploit policies for adaptive multi-modal demonstration selection.", "result": "The approach outperforms existing methods on four Visual Question-Answering datasets, enhancing few-shot LVLM generalization.", "conclusion": "The framework autonomously refines demonstration selection, improving ICL effectiveness in LVLMs."}}
{"id": "2506.09645", "pdf": "https://arxiv.org/pdf/2506.09645", "abs": "https://arxiv.org/abs/2506.09645", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.6"], "comment": "32 pages, 28 figures", "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "AI": {"tldr": "RAPL is a novel framework for graph retrieval in KGQA, addressing limitations of existing methods with a two-stage labeling strategy, model-agnostic graph transformation, and path-based reasoning, achieving superior performance and generalizability.", "motivation": "Existing retrieval-augmented generation pipelines rely on unstructured text, limiting interpretability and structured reasoning. Knowledge graphs offer a structured alternative, but current graph-based retrievers struggle with generalization.", "method": "RAPL introduces (1) a two-stage labeling strategy combining heuristic signals and parametric models, (2) a model-agnostic graph transformation for enhanced representation, and (3) a path-based reasoning strategy.", "result": "RAPL outperforms state-of-the-art methods by 2.66%-20.34%, reduces performance gaps between LLMs, and shows strong generalizability.", "conclusion": "RAPL improves graph retrieval for KGQA, offering better performance and generalizability, with potential for broader applications."}}
{"id": "2506.09396", "pdf": "https://arxiv.org/pdf/2506.09396", "abs": "https://arxiv.org/abs/2506.09396", "authors": ["Zongjie Li", "Shuai Wang"], "title": "Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This position paper proposes a fundamental shift in designing code generation\nmodels: treating reasoning depth as a controllable resource. Rather than being\nan incidental byproduct of prompting, we argue that the trade-off between\nrapid, direct answers (\"fast thinking\") and elaborate, chain-of-thought\ndeliberation (\"slow thinking\") must be explicitly managed. We contend that\noptimizing reasoning budgets across the entire model lifecycle - from synthetic\ndata creation and benchmarking to real-world deploymen - can unlock superior\ntrade-offs among accuracy, latency, and cost. This paper outlines how adaptive\ncontrol over reasoning can enrich supervision signals, motivate new\nmulti-dimensional benchmarks, and inform cost-aware, security-conscious\ndeployment policies. By viewing fast and slow thinking as complementary modes\nto be scheduled, we envision coding agents that think deep when necessary and\nact fast when possible.", "AI": {"tldr": "Proposes treating reasoning depth as a controllable resource in code generation models to balance speed and accuracy.", "motivation": "To optimize trade-offs between fast, direct answers and slow, deliberate reasoning in code generation.", "method": "Advocates for adaptive control over reasoning depth across the model lifecycle, from data creation to deployment.", "result": "Envisions coding agents that dynamically switch between fast and deep thinking for better performance.", "conclusion": "Managing reasoning budgets can improve accuracy, latency, and cost in code generation models."}}
{"id": "2506.09404", "pdf": "https://arxiv.org/pdf/2506.09404", "abs": "https://arxiv.org/abs/2506.09404", "authors": ["Shengda Gu", "Kai Li", "Junliang Xing", "Yifan Zhang", "Jian Cheng"], "title": "Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Combinatorial optimization problems are notoriously challenging due to their\ndiscrete structure and exponentially large solution space. Recent advances in\ndeep reinforcement learning (DRL) have enabled the learning heuristics directly\nfrom data. However, DRL methods often suffer from limited exploration and\nsusceptibility to local optima. On the other hand, evolutionary algorithms such\nas Genetic Algorithms (GAs) exhibit strong global exploration capabilities but\nare typically sample inefficient and computationally intensive. In this work,\nwe propose the Evolutionary Augmentation Mechanism (EAM), a general and\nplug-and-play framework that synergizes the learning efficiency of DRL with the\nglobal search power of GAs. EAM operates by generating solutions from a learned\npolicy and refining them through domain-specific genetic operations such as\ncrossover and mutation. These evolved solutions are then selectively reinjected\ninto the policy training loop, thereby enhancing exploration and accelerating\nconvergence. We further provide a theoretical analysis that establishes an\nupper bound on the KL divergence between the evolved solution distribution and\nthe policy distribution, ensuring stable and effective policy updates. EAM is\nmodel-agnostic and can be seamlessly integrated with state-of-the-art DRL\nsolvers such as the Attention Model, POMO, and SymNCO. Extensive results on\nbenchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM\nsignificantly improves both solution quality and training efficiency over\ncompetitive baselines.", "AI": {"tldr": "EAM combines DRL and GAs to enhance exploration and efficiency in solving combinatorial optimization problems.", "motivation": "DRL lacks exploration, while GAs are inefficient. EAM aims to merge their strengths.", "method": "EAM refines DRL-generated solutions with GA operations (crossover, mutation) and reinjects them into training.", "result": "EAM improves solution quality and training efficiency on benchmarks like TSP, CVRP, PCTSP, and OP.", "conclusion": "EAM is a versatile, effective framework for combinatorial optimization, outperforming baselines."}}
{"id": "2506.09476", "pdf": "https://arxiv.org/pdf/2506.09476", "abs": "https://arxiv.org/abs/2506.09476", "authors": ["Tianxiang Hao", "Lixian Zhang", "Yingjia Zhang", "Mengxuan Chen", "Jinxiao Zhang", "Haohuan Fu"], "title": "Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries", "categories": ["cs.CV"], "comment": null, "summary": "Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,\noffers rare insights into understanding early urban development and long-term\ntransformation. However, severe quality degradation (e.g., distortion,\nmisalignment, and spectral scarcity) and annotation absence have long hindered\nsemantic segmentation on such historical RS imagery. To bridge this gap and\nenhance understanding of urban development, we introduce\n$\\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on\nhistorical satellite imagery with the earliest observation time among all\nexisting segmentation datasets, along with a benchmark framework for\nunsupervised segmentation tasks, $\\textbf{Urban1960SatUSM}$. First,\n$\\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic\nsegmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering\n1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the\nearliest segmentation dataset of its kind, it provides a pioneering benchmark\nfor historical urban understanding. Second,\n$\\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel\nunsupervised semantic segmentation framework for historical RS imagery. It\nemploys a confidence-aware alignment mechanism and focal-confidence loss based\non a self-supervised learning architecture, which generates robust\npseudo-labels and adaptively prioritizes prediction difficulty and label\nreliability to improve unsupervised segmentation on noisy historical data\nwithout manual supervision. Experiments show Urban1960SatUSM significantly\noutperforms existing unsupervised segmentation methods on Urban1960SatSeg for\nsegmenting historical urban scenes, promising in paving the way for\nquantitative studies of long-term urban change using modern computer vision.\nOur benchmark and supplementary material are available at\nhttps://github.com/Tianxiang-Hao/Urban1960SatSeg.", "AI": {"tldr": "The paper introduces Urban1960SatBench, a novel annotated segmentation dataset for historical satellite imagery, and Urban1960SatUSM, an unsupervised segmentation framework to address quality degradation and annotation absence in such data.", "motivation": "Historical satellite imagery provides insights into early urban development but suffers from quality issues and lack of annotations, hindering semantic segmentation.", "method": "The authors create Urban1960SatBench, an annotated dataset, and propose Urban1960SatUSM, an unsupervised framework using confidence-aware alignment and focal-confidence loss for robust segmentation.", "result": "Urban1960SatUSM outperforms existing methods on Urban1960SatBench, enabling better segmentation of historical urban scenes.", "conclusion": "The work advances quantitative studies of long-term urban change by addressing challenges in historical satellite imagery segmentation."}}
{"id": "2506.09657", "pdf": "https://arxiv.org/pdf/2506.09657", "abs": "https://arxiv.org/abs/2506.09657", "authors": ["Nikolas Evkarpidi", "Elena Tutubalina"], "title": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA", "categories": ["cs.CL"], "comment": "Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures", "summary": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.", "AI": {"tldr": "A system for QA over tabular data integrates text-to-SQL, text-to-code, self-correction, RAG, and an E2E module, achieving 80% accuracy and top-13 ranking in SemEval 2025 Task 8.", "motivation": "To improve QA accuracy over tabular data by combining multiple modules and leveraging LLMs.", "method": "Integration of text-to-SQL, text-to-code, self-correction, RAG, and an E2E module orchestrated by an LLM.", "result": "80% accuracy, top-13 ranking among 38 teams, comparable performance to proprietary LLMs.", "conclusion": "The pipeline significantly improves accuracy for open-source models and addresses challenges in QA over tables."}}
{"id": "2506.09397", "pdf": "https://arxiv.org/pdf/2506.09397", "abs": "https://arxiv.org/abs/2506.09397", "authors": ["Xiangchen Li", "Dimitrios Spatharakis", "Saeid Ghafouri", "Jiakun Fan", "Dimitrios Nikolopoulos"], "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI", "68T07, 68M14", "I.2.6; C.2.4; C.1.4"], "comment": "6 pages, 9 figures, 2 tables", "summary": "Regardless the advancements in device capabilities, efficient inferencing\nadvanced large language models (LLMs) at the edge remains challenging due to\nlimited device memory and power constraints. Existing strategies, such as\naggressive quantization, pruning, or remote inference, trade accuracy for\nefficiency or lead to substantial cost burdens. This position paper introduces\na new approach that leverages speculative decoding, previously viewed primarily\nas a decoding acceleration technique for autoregressive generation of LLMs, as\na promising approach specifically adapted for edge computing by orchestrating\ncomputation across heterogeneous devices. We propose SLED, a method that allows\nlightweight edge devices to draft multiple candidate tokens locally using\ndiverse draft models, while a single, shared edge server efficiently batches\nand verifies the tokens utilizing a more precise target model. This approach\nsupports device heterogeneity and reduces server-side memory footprint by\navoiding the need to deploy multiple target models. Our initial experiments\nwith Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate\nsubstantial benefits: significantly reduced latency, improved energy\nefficiency, and increased concurrent inference sessions, all without\nsacrificing model accuracy.", "AI": {"tldr": "SLED leverages speculative decoding for efficient LLM inferencing at the edge, reducing latency and energy use without accuracy loss.", "motivation": "Efficient LLM inferencing at the edge is hindered by memory and power limits; existing methods compromise accuracy or cost.", "method": "SLED uses lightweight edge devices to draft tokens locally, verified by a shared edge server, avoiding multiple target models.", "result": "Experiments show reduced latency, better energy efficiency, and more concurrent sessions without accuracy loss.", "conclusion": "SLED is a promising approach for edge computing, balancing efficiency and accuracy."}}
{"id": "2506.09433", "pdf": "https://arxiv.org/pdf/2506.09433", "abs": "https://arxiv.org/abs/2506.09433", "authors": ["Shurui Gui", "Shuiwang Ji"], "title": "Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training", "categories": ["cs.LG"], "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin language modeling, recent studies reveal that they often fail on\nout-of-distribution (OOD) samples due to spurious correlations acquired during\npre-training. Here, we aim to mitigate such spurious correlations through\ncausality-aware post-training (CAPT). By decomposing a biased prediction into\ntwo unbiased steps, known as \\textit{event estimation} and \\textit{event\nintervention}, we reduce LLMs' pre-training biases without incurring additional\nfine-tuning biases, thus enhancing the model's generalization ability.\nExperiments on the formal causal inference benchmark CLadder and the logical\nreasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with\nCAPT can outperform both traditional SFT and larger LLMs on in-distribution\n(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the\neffectiveness and sample efficiency of CAPT.", "AI": {"tldr": "CAPT reduces spurious correlations in LLMs by decomposing predictions into unbiased steps, improving generalization without extra fine-tuning biases.", "motivation": "Address LLMs' failure on OOD samples due to spurious correlations from pre-training.", "method": "Causality-aware post-training (CAPT) decomposes predictions into event estimation and intervention steps.", "result": "3B-scale models with CAPT outperform SFT and larger LLMs on ID and OOD tasks with minimal fine-tuning samples.", "conclusion": "CAPT is effective and sample-efficient for enhancing LLM generalization."}}
{"id": "2506.09479", "pdf": "https://arxiv.org/pdf/2506.09479", "abs": "https://arxiv.org/abs/2506.09479", "authors": ["Zetian Song", "Jiaye Fu", "Jiaqi Zhang", "Xiaohan Lu", "Chuanmin Jia", "Siwei Ma", "Wen Gao"], "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation", "categories": ["cs.CV"], "comment": null, "summary": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a\nnew paradigm to reconstruct 3D scenes. Using neural networks trained on\nlarge-scale multi-view datasets, it can directly infer 3DGS representations\nfrom sparse input views. Although the feedforward approach achieves high\nreconstruction speed, it still suffers from the substantial storage cost of 3D\nGaussians. Existing 3DGS compression methods relying on scene-wise optimization\nare not applicable due to architectural incompatibilities. To overcome this\nlimitation, we propose TinySplat, a complete feedforward approach for\ngenerating compact 3D scene representations. Built upon standard feedforward\n3DGS methods, TinySplat integrates a training-free compression framework that\nsystematically eliminates key sources of redundancy. Specifically, we introduce\nView-Projection Transformation (VPT) to reduce geometric redundancy by\nprojecting geometric parameters into a more compact space. We further present\nVisibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy\nby aligning feature energy along dominant viewing directions via basis\ntransformation. Lastly, spatial redundancy is addressed through an\noff-the-shelf video codec. Comprehensive experimental results on multiple\nbenchmark datasets demonstrate that TinySplat achieves over 100x compression\nfor 3D Gaussian data generated by feedforward methods. Compared to the\nstate-of-the-art compression approach, we achieve comparable quality with only\n6% of the storage size. Meanwhile, our compression framework requires only 25%\nof the encoding time and 1% of the decoding time.", "AI": {"tldr": "TinySplat is a feedforward method for compressing 3D Gaussian Splatting (3DGS) representations, reducing storage costs by 100x while maintaining quality.", "motivation": "Existing 3DGS compression methods are incompatible with feedforward approaches, leading to high storage costs. TinySplat addresses this by eliminating redundancy without scene-wise optimization.", "method": "TinySplat integrates View-Projection Transformation (VPT) for geometric redundancy, Visibility-Aware Basis Reduction (VABR) for perceptual redundancy, and a video codec for spatial redundancy.", "result": "Achieves 100x compression, comparable quality at 6% storage size, and significantly faster encoding (25%) and decoding (1%) times.", "conclusion": "TinySplat offers an efficient, training-free solution for compressing 3DGS representations, enabling practical deployment of feedforward methods."}}
{"id": "2506.09669", "pdf": "https://arxiv.org/pdf/2506.09669", "abs": "https://arxiv.org/abs/2506.09669", "authors": ["Lihu Chen", "Ga\u00ebl Varoquaux"], "title": "Query-Level Uncertainty in Large Language Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.", "AI": {"tldr": "A method for detecting knowledge boundaries in LLMs using Query-Level Uncertainty, improving adaptive inference and efficiency.", "motivation": "Enhancing LLMs' awareness of their knowledge limits to enable adaptive strategies like RAG or abstention, fostering efficient and trustworthy AI.", "method": "Introduces a training-free approach, Internal Confidence, leveraging self-evaluations across layers and tokens to detect knowledge boundaries without token generation.", "result": "Outperforms baselines in factual QA and mathematical reasoning; enables efficient RAG and model cascading, reducing costs while maintaining performance.", "conclusion": "Internal Confidence effectively identifies knowledge boundaries, aiding adaptive inference and cost-efficient AI deployment."}}
{"id": "2506.09438", "pdf": "https://arxiv.org/pdf/2506.09438", "abs": "https://arxiv.org/abs/2506.09438", "authors": ["Haoxiang Ye", "Tao Sun", "Qing Ling"], "title": "Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Decentralized learning, which facilitates joint model training across\ngeographically scattered agents, has gained significant attention in the field\nof signal and information processing in recent years. While the optimization\nerrors of decentralized learning algorithms have been extensively studied,\ntheir generalization errors remain relatively under-explored. As the\ngeneralization errors reflect the scalability of trained models on unseen data\nand are crucial in determining the performance of trained models in real-world\napplications, understanding the generalization errors of decentralized learning\nis of paramount importance. In this paper, we present fine-grained\ngeneralization error analysis for both attack-free and Byzantine-resilient\ndecentralized learning with heterogeneous data as well as under mild\nassumptions, in contrast to prior studies that consider homogeneous data and/or\nrely on a stringent bounded stochastic gradient assumption. Our results shed\nlight on the impact of data heterogeneity, model initialization and stochastic\ngradient noise -- factors that have not been closely investigated before -- on\nthe generalization error of decentralized learning. We also reveal that\nByzantine attacks performed by malicious agents largely affect the\ngeneralization error, and their negative impact is inherently linked to the\ndata heterogeneity while remaining independent on the sample size. Numerical\nexperiments on both convex and non-convex tasks are conducted to validate our\ntheoretical findings.", "AI": {"tldr": "The paper analyzes generalization errors in decentralized learning, focusing on data heterogeneity, model initialization, and stochastic gradient noise, while also examining the impact of Byzantine attacks.", "motivation": "Understanding generalization errors in decentralized learning is crucial for real-world scalability, as prior studies overlooked data heterogeneity and relied on stringent assumptions.", "method": "The study conducts fine-grained generalization error analysis for attack-free and Byzantine-resilient decentralized learning under mild assumptions, using numerical experiments on convex and non-convex tasks.", "result": "Findings highlight the impact of data heterogeneity, model initialization, and stochastic gradient noise on generalization errors, with Byzantine attacks significantly affecting performance.", "conclusion": "The study provides insights into factors influencing generalization errors in decentralized learning, emphasizing the need to address data heterogeneity and Byzantine resilience."}}
{"id": "2506.09482", "pdf": "https://arxiv.org/pdf/2506.09482", "abs": "https://arxiv.org/abs/2506.09482", "authors": ["Dingcheng Zhen", "Qian Qiao", "Tan Yu", "Kangxi Wu", "Ziwei Zhang", "Siyuan Liu", "Shunshun Yin", "Ming Tao"], "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression", "categories": ["cs.CV"], "comment": null, "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.", "AI": {"tldr": "TransDiff combines AR Transformer and diffusion models for image generation, achieving superior performance on ImageNet 256x256 with faster inference and introducing MRAR for further improvements.", "motivation": "To bridge the gap between AR Transformer and diffusion models for better image generation performance and efficiency.", "method": "Jointly encodes labels and images into semantic features using AR Transformer and diffusion models, and introduces MRAR for multi-reference autoregression.", "result": "Achieves FID of 1.61, IS of 293.4, and faster inference (x2 vs. AR Transformer, x112 vs. diffusion-only). MRAR further reduces FID to 1.42.", "conclusion": "TransDiff sets a new benchmark in image generation, with MRAR enhancing diversity and quality."}}
{"id": "2506.09672", "pdf": "https://arxiv.org/pdf/2506.09672", "abs": "https://arxiv.org/abs/2506.09672", "authors": ["Hao Xiong", "Chuanyuan Tan", "Wenliang Chen"], "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%", "AI": {"tldr": "The paper addresses issues in Unstructured Knowledge Editing (UKE) by introducing datasets for locality evaluation and optimizing fine-tuning methods, achieving superior performance.", "motivation": "To improve UKE by addressing the lack of locality evaluation and the failure of fine-tuning methods in unstructured knowledge updates.", "method": "Constructed datasets (UnKEBench-Loc and AKEW-Loc) for locality evaluation and identified factors affecting fine-tuning performance, leading to an optimized method (FT-UKE).", "result": "FT-UKE outperforms existing SOTA methods, with performance gains increasing in batch editing scenarios.", "conclusion": "The study provides a robust training recipe for UKE, demonstrating the effectiveness of optimized fine-tuning methods."}}
{"id": "2506.09455", "pdf": "https://arxiv.org/pdf/2506.09455", "abs": "https://arxiv.org/abs/2506.09455", "authors": ["Yizhak Yisrael Elboher", "Omri Isac", "Guy Katz", "Tobias Ladner", "Haoze Wu"], "title": "Abstraction-Based Proof Production in Formal Verification of Neural Networks", "categories": ["cs.LO", "cs.AI"], "comment": "To appear in SAIV 2025", "summary": "Modern verification tools for deep neural networks (DNNs) increasingly rely\non abstraction to scale to realistic architectures. In parallel, proof\nproduction is becoming a critical requirement for increasing the reliability of\nDNN verification results. However, current proofproducing verifiers do not\nsupport abstraction-based reasoning, creating a gap between scalability and\nprovable guarantees. We address this gap by introducing a novel framework for\nproof-producing abstraction-based DNN verification. Our approach modularly\nseparates the verification task into two components: (i) proving the\ncorrectness of an abstract network, and (ii) proving the soundness of the\nabstraction with respect to the original DNN. The former can be handled by\nexisting proof-producing verifiers, whereas we propose the first method for\ngenerating formal proofs for the latter. This preliminary work aims to enable\nscalable and trustworthy verification by supporting common abstraction\ntechniques within a formal proof framework.", "AI": {"tldr": "A framework for proof-producing abstraction-based DNN verification to bridge scalability and provable guarantees.", "motivation": "Current proof-producing verifiers lack support for abstraction-based reasoning, limiting scalability and reliability in DNN verification.", "method": "Modularly separates verification into proving abstract network correctness and abstraction soundness, leveraging existing verifiers for the former and introducing a new method for the latter.", "result": "Enables scalable and trustworthy DNN verification by integrating common abstraction techniques with formal proofs.", "conclusion": "The framework advances DNN verification by combining scalability through abstraction with formal proof guarantees."}}
{"id": "2506.09451", "pdf": "https://arxiv.org/pdf/2506.09451", "abs": "https://arxiv.org/abs/2506.09451", "authors": ["Runxue Bao", "Quanchao Lu", "Yanfu Zhang"], "title": "Safe Screening Rules for Group SLOPE", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by ECML PKDD 2025", "summary": "Variable selection is a challenging problem in high-dimensional sparse\nlearning, especially when group structures exist. Group SLOPE performs well for\nthe adaptive selection of groups of predictors. However, the block\nnon-separable group effects in Group SLOPE make existing methods either invalid\nor inefficient. Consequently, Group SLOPE tends to incur significant\ncomputational costs and memory usage in practical high-dimensional scenarios.\nTo overcome this issue, we introduce a safe screening rule tailored for the\nGroup SLOPE model, which efficiently identifies inactive groups with zero\ncoefficients by addressing the block non-separable group effects. By excluding\nthese inactive groups during training, we achieve considerable gains in\ncomputational efficiency and memory usage. Importantly, the proposed screening\nrule can be seamlessly integrated into existing solvers for both batch and\nstochastic algorithms. Theoretically, we establish that our screening rule can\nbe safely employed with existing optimization algorithms, ensuring the same\nresults as the original approaches. Experimental results confirm that our\nmethod effectively detects inactive feature groups and significantly boosts\ncomputational efficiency without compromising accuracy.", "AI": {"tldr": "A safe screening rule for Group SLOPE is introduced to improve computational efficiency and memory usage by identifying inactive groups with zero coefficients.", "motivation": "Group SLOPE struggles with block non-separable group effects, leading to high computational costs and memory usage in high-dimensional sparse learning.", "method": "A tailored safe screening rule is proposed to detect inactive groups, which can be integrated into existing solvers for batch and stochastic algorithms.", "result": "The method efficiently identifies inactive groups, reducing computational costs and memory usage without sacrificing accuracy.", "conclusion": "The proposed screening rule enhances Group SLOPE's practicality in high-dimensional scenarios while maintaining theoretical guarantees."}}
{"id": "2506.09518", "pdf": "https://arxiv.org/pdf/2506.09518", "abs": "https://arxiv.org/abs/2506.09518", "authors": ["Jianing Chen", "Zehao Li", "Yujun Cai", "Hao Jiang", "Chengxuan Qian", "Juyuan Kang", "Shuqin Gao", "Honglong Zhao", "Tianlu Mao", "Yucheng Zhang"], "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppresses\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.", "AI": {"tldr": "HAIF-GS improves dynamic 3D scene reconstruction from monocular videos by addressing redundant updates, insufficient motion supervision, and non-rigid deformations using anchor-driven deformation.", "motivation": "Dynamic 3D scene reconstruction from monocular videos is challenging due to inconsistent motion representations in existing methods, leading to inefficiencies and poor quality.", "method": "HAIF-GS uses sparse anchor-driven deformation with an Anchor Filter, Induced Flow-Guided Deformation, and Hierarchical Anchor Propagation to model motion coherently.", "result": "HAIF-GS outperforms prior methods in rendering quality, temporal coherence, and efficiency on synthetic and real-world benchmarks.", "conclusion": "HAIF-GS provides a robust solution for dynamic 3D reconstruction, addressing key limitations of existing approaches."}}
{"id": "2506.09684", "pdf": "https://arxiv.org/pdf/2506.09684", "abs": "https://arxiv.org/abs/2506.09684", "authors": ["Haoyi Song", "Ruihan Ji", "Naichen Shi", "Fan Lai", "Raed Al Kontar"], "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.", "AI": {"tldr": "The paper proposes a probabilistic framework for uncertainty quantification (UQ) in large language models (LLMs), introducing Inv-Entropy as a new measure and GAAP for perturbation diversity.", "motivation": "Existing UQ methods for LLMs lack a probabilistic foundation and are often heuristic, necessitating a more robust approach.", "method": "The paper introduces a dual random walk perspective, models input-output pairs as Markov chains, and proposes a probabilistic framework with Inv-Entropy. GAAP, a genetic algorithm-based perturbation method, is also introduced.", "result": "Inv-Entropy outperforms existing semantic UQ methods, and the framework supports flexible uncertainty measures and metrics.", "conclusion": "The proposed framework provides a robust, flexible, and theoretically grounded approach to UQ in LLMs, validated by extensive experiments."}}
{"id": "2506.09485", "pdf": "https://arxiv.org/pdf/2506.09485", "abs": "https://arxiv.org/abs/2506.09485", "authors": ["Yuxin Liu", "Zhenghao Peng", "Xuanhao Cui", "Bolei Zhou"], "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": null, "summary": "Scenario-based testing is essential for validating the performance of\nautonomous driving (AD) systems. However, such testing is limited by the\nscarcity of long-tailed, safety-critical scenarios in existing datasets\ncollected in the real world. To tackle the data issue, we propose the Adv-BMT\nframework, which augments real-world scenarios with diverse and realistic\nadversarial interactions. The core component of Adv-BMT is a bidirectional\nmotion transformer (BMT) model to perform inverse traffic motion predictions,\nwhich takes agent information in the last time step of the scenario as input,\nand reconstruct the traffic in the inverse of chronological order until the\ninitial time step. The Adv-BMT framework is a two-staged pipeline: it first\nconducts adversarial initializations and then inverse motion predictions.\nDifferent from previous work, we do not need any collision data for\npretraining, and are able to generate realistic and diverse collision\ninteractions. Our experimental results validate the quality of generated\ncollision scenarios by Adv-BMT: training in our augmented dataset would reduce\nepisode collision rates by 20\\% compared to previous work.", "AI": {"tldr": "The paper introduces Adv-BMT, a framework to generate diverse and realistic adversarial scenarios for autonomous driving testing, improving collision avoidance by 20%.", "motivation": "Existing datasets lack long-tailed, safety-critical scenarios, limiting the effectiveness of autonomous driving system testing.", "method": "Adv-BMT uses a bidirectional motion transformer (BMT) to reconstruct traffic in reverse chronological order, followed by adversarial initializations and inverse motion predictions.", "result": "The framework generates realistic collision scenarios without pretraining on collision data, reducing episode collision rates by 20%.", "conclusion": "Adv-BMT effectively augments real-world datasets, enhancing the validation of autonomous driving systems."}}
{"id": "2506.09452", "pdf": "https://arxiv.org/pdf/2506.09452", "abs": "https://arxiv.org/abs/2506.09452", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.IT", "math.IT", "I.2.7; I.2.m"], "comment": "Submitted to IEEE S&P 2026", "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "AI": {"tldr": "The paper introduces the Stained Glass Transform, a method to provide privacy for LLM inputs while preserving model utility, addressing concerns over plaintext data in shared or multi-tenant AI infrastructures.", "motivation": "High costs and privacy concerns in shared AI infrastructures limit data usage, especially for sensitive data.", "method": "Proposes the Stained Glass Transform, a learned, stochastic transformation of word embeddings to ensure privacy without losing utility.", "result": "The method theoretically connects to Gaussian Mixture Models' mutual information and shows practical privacy and utility in benchmarks.", "conclusion": "The Stained Glass Transform effectively balances privacy and utility for LLM deployments in shared environments."}}
{"id": "2506.09522", "pdf": "https://arxiv.org/pdf/2506.09522", "abs": "https://arxiv.org/abs/2506.09522", "authors": ["Beomsik Cho", "Jaehyung Kim"], "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code available at https://github.com/bscho333/ReVisiT", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$.", "AI": {"tldr": "ReVisiT is a decoding method for LVLMs that improves visual grounding by referencing vision tokens during text generation, outperforming baselines with lower computational costs.", "motivation": "Conventional LVLM decoding strategies often fail to utilize visual information effectively, leading to ungrounded responses. Existing solutions require extra training or resources.", "method": "ReVisiT projects vision tokens into text token space, dynamically selects relevant tokens via divergence minimization, and refines output distributions for better visual semantics.", "result": "ReVisiT enhances visual grounding on benchmarks, outperforming baselines with up to 2\u00d7 lower computational costs.", "conclusion": "ReVisiT offers a simple, effective solution for improving LVLM visual grounding without additional training or resources."}}
{"id": "2506.09790", "pdf": "https://arxiv.org/pdf/2506.09790", "abs": "https://arxiv.org/abs/2506.09790", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "categories": ["cs.CL", "cs.CV", "cs.SE"], "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "AI": {"tldr": "ComfyUI-R1 is a large reasoning model for automated workflow generation in AI art, trained with a two-stage framework and outperforming state-of-the-art methods.", "motivation": "The steep learning curve for crafting effective AI workflows on platforms like ComfyUI necessitates an automated solution.", "method": "ComfyUI-R1 uses a two-stage training framework: CoT fine-tuning for domain adaptation and reinforcement learning for reasoning capability, guided by hybrid rewards.", "result": "The 7B-parameter model achieves 97% format validity and high F1 scores, surpassing GPT-4o and Claude series.", "conclusion": "Long chain-of-thought reasoning and code-based workflow transformation are critical for synthesizing intricate AI art workflows."}}
{"id": "2506.09496", "pdf": "https://arxiv.org/pdf/2506.09496", "abs": "https://arxiv.org/abs/2506.09496", "authors": ["Dingyi Rong", "Haotian Lu", "Wenzhuo Zheng", "Fan Zhang", "Shuangjia Zheng", "Ning Liu"], "title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Designing protein sequences with optimal energetic stability is a key\nchallenge in protein inverse folding, as current deep learning methods are\nprimarily trained by maximizing sequence recovery rates, often neglecting the\nenergy of the generated sequences. This work aims to overcome this limitation\nby developing a model that directly generates low-energy, stable protein\nsequences. We propose EnerBridge-DPO, a novel inverse folding framework focused\non generating low-energy, high-stability protein sequences. Our core innovation\nlies in: First, integrating Markov Bridges with Direct Preference Optimization\n(DPO), where energy-based preferences are used to fine-tune the Markov Bridge\nmodel. The Markov Bridge initiates optimization from an information-rich prior\nsequence, providing DPO with a pool of structurally plausible sequence\ncandidates. Second, an explicit energy constraint loss is introduced, which\nenhances the energy-driven nature of DPO based on prior sequences, enabling the\nmodel to effectively learn energy representations from a wealth of prior\nknowledge and directly predict sequence energy values, thereby capturing\nquantitative features of the energy landscape. Our evaluations demonstrate that\nEnerBridge-DPO can design protein complex sequences with lower energy while\nmaintaining sequence recovery rates comparable to state-of-the-art models, and\naccurately predicts $\\Delta \\Delta G$ values between various sequences.", "AI": {"tldr": "EnerBridge-DPO is a novel framework for generating low-energy, stable protein sequences by integrating Markov Bridges with Direct Preference Optimization (DPO) and an energy constraint loss.", "motivation": "Current deep learning methods for protein inverse folding focus on sequence recovery rates, often ignoring sequence energy. This work addresses the gap by prioritizing low-energy, stable sequences.", "method": "The model combines Markov Bridges and DPO, using energy-based preferences to fine-tune sequences. An energy constraint loss is added to enhance energy-driven learning and predict sequence energy values.", "result": "EnerBridge-DPO designs sequences with lower energy while matching state-of-the-art recovery rates and accurately predicts \u0394\u0394G values.", "conclusion": "The framework successfully bridges the gap between sequence recovery and energy stability, offering a robust solution for protein inverse folding."}}
{"id": "2506.09454", "pdf": "https://arxiv.org/pdf/2506.09454", "abs": "https://arxiv.org/abs/2506.09454", "authors": ["Yuanhao Pu", "Defu Lian", "Xiaolong Chen", "Xu Huang", "Jin Chen", "Enhong Chen"], "title": "NDCG-Consistent Softmax Approximation with Accelerated Convergence", "categories": ["cs.LG"], "comment": "35 pages", "summary": "Ranking tasks constitute fundamental components of extreme similarity\nlearning frameworks, where extremely large corpora of objects are modeled\nthrough relative similarity relationships adhering to predefined ordinal\nstructures. Among various ranking surrogates, Softmax (SM) Loss has been widely\nadopted due to its natural capability to handle listwise ranking via global\nnegative comparisons, along with its flexibility across diverse application\nscenarios. However, despite its effectiveness, SM Loss often suffers from\nsignificant computational overhead and scalability limitations when applied to\nlarge-scale object spaces. To address this challenge, we propose novel loss\nformulations that align directly with ranking metrics: the\nRanking-Generalizable \\textbf{squared} (RG$^2$) Loss and the\nRanking-Generalizable interactive (RG$^\\times$) Loss, both derived through\nTaylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic\nmechanisms underlying weighted squared losses (WSL) in ranking methods and\nuncovers fundamental connections between sampling-based and non-sampling-based\nloss paradigms. Furthermore, we integrate the proposed RG losses with the\nhighly efficient Alternating Least Squares (ALS) optimization method, providing\nboth generalization guarantees and convergence rate analyses. Empirical\nevaluations on real-world datasets demonstrate that our approach achieves\ncomparable or superior ranking performance relative to SM Loss, while\nsignificantly accelerating convergence. This framework offers the similarity\nlearning community both theoretical insights and practically efficient tools,\nwith methodologies applicable to a broad range of tasks where balancing ranking\nquality and computational efficiency is essential.", "AI": {"tldr": "The paper proposes RG$^2$ and RG$^\times$ losses as efficient alternatives to Softmax Loss for ranking tasks, offering theoretical insights and faster convergence.", "motivation": "Softmax Loss, while effective, faces computational and scalability issues in large-scale ranking tasks.", "method": "Derived RG$^2$ and RG$^\times$ losses via Taylor expansions of Softmax Loss, integrated with ALS optimization.", "result": "Empirical results show comparable/superior performance to Softmax Loss with faster convergence.", "conclusion": "The framework provides efficient tools and theoretical insights for similarity learning, balancing ranking quality and computational efficiency."}}
{"id": "2506.09534", "pdf": "https://arxiv.org/pdf/2506.09534", "abs": "https://arxiv.org/abs/2506.09534", "authors": ["Tao Wang", "Mengyu Li", "Geduo Zeng", "Cheng Meng", "Qiong Zhang"], "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS", "categories": ["cs.CV", "I.4.5"], "comment": "18 pages, 8 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance\nfield rendering, but it typically requires millions of redundant Gaussian\nprimitives, overwhelming memory and rendering budgets. Existing compaction\napproaches address this by pruning Gaussians based on heuristic importance\nscores, without global fidelity guarantee. To bridge this gap, we propose a\nnovel optimal transport perspective that casts 3DGS compaction as global\nGaussian mixture reduction. Specifically, we first minimize the composite\ntransport divergence over a KD-tree partition to produce a compact geometric\nrepresentation, and then decouple appearance from geometry by fine-tuning color\nand opacity attributes with far fewer Gaussian primitives. Experiments on\nbenchmark datasets show that our method (i) yields negligible loss in rendering\nquality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;\nand (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.\nNotably, our method is applicable to any stage of vanilla or accelerated 3DGS\npipelines, providing an efficient and agnostic pathway to lightweight neural\nrendering.", "AI": {"tldr": "A novel optimal transport-based method for compacting 3D Gaussian Splatting (3DGS) reduces redundancy while maintaining rendering quality, outperforming existing techniques.", "motivation": "3DGS often uses millions of redundant Gaussian primitives, straining memory and rendering resources. Existing compaction methods lack global fidelity guarantees.", "method": "Proposes an optimal transport perspective to cast 3DGS compaction as Gaussian mixture reduction, minimizing transport divergence and decoupling appearance from geometry.", "result": "Achieves negligible quality loss with only 10% of Gaussians and outperforms state-of-the-art compaction techniques.", "conclusion": "The method offers an efficient, agnostic solution for lightweight neural rendering, applicable to various 3DGS pipelines."}}
{"id": "2506.09796", "pdf": "https://arxiv.org/pdf/2506.09796", "abs": "https://arxiv.org/abs/2506.09796", "authors": ["Andreas S\u00e4uberli", "Diego Frassinelli", "Barbara Plank"], "title": "Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?", "categories": ["cs.CL"], "comment": "Accepted for publication at the 20th Workshop on Innovative Use of\n  NLP for Building Educational Applications (BEA) at ACL 2025", "summary": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting.", "AI": {"tldr": "LLMs' human-like response behavior in educational assessments is evaluated, showing potential but limited suitability for zero-shot piloting.", "motivation": "To assess if LLMs can replace human participants in test development by evaluating their psychometric plausibility.", "method": "Evaluates 18 LLMs using classical test theory and item response theory on multiple-choice datasets across reading, U.S. history, and economics.", "result": "Larger models are overly confident but more human-like when calibrated; LLMs correlate better with humans in reading but not strongly enough for zero-shot use.", "conclusion": "LLMs are not yet suitable for zero-shot piloting in educational assessments due to weak human-like correlations."}}
{"id": "2506.09499", "pdf": "https://arxiv.org/pdf/2506.09499", "abs": "https://arxiv.org/abs/2506.09499", "authors": ["Thomas J. Ringstrom", "Paul R. Schrater"], "title": "A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes", "categories": ["cs.LG", "cs.AI"], "comment": "12 Pages", "summary": "We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free\nMarkov Decision Process. Rather than a value function, OKBEs directly construct\nand optimize a predictive map called a state-time option kernel (STOK) to\nmaximize the probability of completing a goal while avoiding constraint\nviolations. STOKs are compositional, modular, and interpretable\ninitiation-to-termination transition kernels for policies in the Options\nFramework of Reinforcement Learning. This means: 1) STOKs can be composed using\nChapman-Kolmogorov equations to make spatiotemporal predictions for multiple\npolicies over long horizons, 2) high-dimensional STOKs can be represented and\ncomputed efficiently in a factorized and reconfigurable form, and 3) STOKs\nrecord the probabilities of semantically interpretable goal-success and\nconstraint-violation events, needed for formal verification. Given a\nhigh-dimensional state-transition model for an intractable planning problem, we\ncan decompose it with local STOKs and goal-conditioned policies that are\naggregated into a factorized goal kernel, making it possible to forward-plan at\nthe level of goals in high-dimensions to solve the problem. These properties\nlead to highly flexible agents that can rapidly synthesize meta-policies, reuse\nplanning representations across many tasks, and justify goals using\nempowerment, an intrinsic motivation function. We argue that\nreward-maximization is in conflict with the properties of compositionality,\nmodularity, and interpretability. Alternatively, OKBEs facilitate these\nproperties to support verifiable long-horizon planning and intrinsic motivation\nthat scales to dynamic high-dimensional world-models.", "AI": {"tldr": "OKBEs introduce state-time option kernels (STOKs) for reward-free MDPs, optimizing goal completion while avoiding constraints. STOKs are modular, interpretable, and support long-horizon planning.", "motivation": "Address the conflict between reward-maximization and properties like compositionality, modularity, and interpretability in reinforcement learning.", "method": "Construct and optimize STOKs, compositional transition kernels, using Chapman-Kolmogorov equations for spatiotemporal predictions and efficient computation.", "result": "Enables flexible agents for meta-policies, reusable planning, and verifiable long-horizon planning in high-dimensional models.", "conclusion": "OKBEs support scalable, interpretable, and verifiable planning, aligning with intrinsic motivation and dynamic world-models."}}
{"id": "2506.09477", "pdf": "https://arxiv.org/pdf/2506.09477", "abs": "https://arxiv.org/abs/2506.09477", "authors": ["Yunhao Tang", "R\u00e9mi Munos"], "title": "On a few pitfalls in KL divergence gradient estimation for RL", "categories": ["cs.LG"], "comment": null, "summary": "We point out a few pitfalls in implementing gradient estimation for KL\ndivergence in RL training for LLM, as seen in a number of open source projects\nand papers. The first major pitfall is to differentiate through the KL estimate\nas loss functions to minimize KL divergence. We show that such implementations\nare generally incorrect and do not produce the desired KL gradient. Secondly,\nwe show that some implementations do not account for the sequential nature of\nthe estimation problem and produce a partial gradient at best. We demonstrate\nthe impact of such issues with illustrative tabular and LLM experiments, and\nshow the correct way to implement the KL gradient.", "AI": {"tldr": "The paper identifies pitfalls in gradient estimation for KL divergence in RL training for LLMs, showing incorrect implementations and their impacts, and provides correct methods.", "motivation": "To address common errors in gradient estimation for KL divergence in RL training for LLMs, as seen in open-source projects and papers.", "method": "Analyzes incorrect implementations, demonstrates their flaws with tabular and LLM experiments, and presents the correct gradient estimation method.", "result": "Incorrect implementations fail to produce desired KL gradients, while the correct method is validated through experiments.", "conclusion": "The paper clarifies proper gradient estimation for KL divergence in RL training, improving implementation accuracy."}}
{"id": "2506.09538", "pdf": "https://arxiv.org/pdf/2506.09538", "abs": "https://arxiv.org/abs/2506.09538", "authors": ["Wenjun Ji", "Yuxiang Fu", "Luyang Ying", "Deng-Ping Fan", "Yuyi Wang", "Ming-Ming Cheng", "Ivor Tsang", "Qing Guo"], "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches", "categories": ["cs.CV"], "comment": null, "summary": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents.", "AI": {"tldr": "The paper introduces Angle-Robust Concept Learning (AngleRoCL) to enhance the angle robustness of text-to-image adversarial patches, improving attack effectiveness across multiple views.", "motivation": "Existing methods neglect the angle robustness of adversarial patches, limiting their real-world effectiveness. This study aims to address this gap.", "method": "Proposes AngleRoCL, which learns generalizable text embeddings to guide T2I models in generating angle-robust patches.", "result": "AngleRoCL significantly improves angle robustness, with over 50% average relative improvement in attack effectiveness across multiple angles.", "conclusion": "The research advances understanding of angle-robust patches and explores the link between textual concepts and physical properties in T2I-generated content."}}
{"id": "2506.09820", "pdf": "https://arxiv.org/pdf/2506.09820", "abs": "https://arxiv.org/abs/2506.09820", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "title": "CoRT: Code-integrated Reasoning within Thinking", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "AI": {"tldr": "CoRT is a post-training framework to improve LRMs' efficiency and accuracy in mathematical reasoning by integrating Code Interpreters (CI) through Hint-Engineering.", "motivation": "LRMs struggle with complex math operations, and direct CI integration is inefficient. CoRT aims to optimize LRM-CI interaction.", "method": "Synthesizes code-integrated reasoning data via Hint-Engineering, post-trains models (1.5B to 32B) with fine-tuning and reinforcement learning.", "result": "Achieves 4-8% absolute improvements on math reasoning tasks and reduces token usage by 30-50%.", "conclusion": "CoRT effectively enhances LRMs' mathematical reasoning by leveraging CI, with significant performance gains and efficiency improvements."}}
{"id": "2506.09508", "pdf": "https://arxiv.org/pdf/2506.09508", "abs": "https://arxiv.org/abs/2506.09508", "authors": ["Andreas Schlaginhaufen", "Reda Ouhamma", "Maryam Kamgarpour"], "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "comment": null, "summary": "We study reinforcement learning from human feedback in general Markov\ndecision processes, where agents learn from trajectory-level preference\ncomparisons. A central challenge in this setting is to design algorithms that\nselect informative preference queries to identify the underlying reward while\nensuring theoretical guarantees. We propose a meta-algorithm based on\nrandomized exploration, which avoids the computational challenges associated\nwith optimistic approaches and remains tractable. We establish both regret and\nlast-iterate guarantees under mild reinforcement learning oracle assumptions.\nTo improve query complexity, we introduce and analyze an improved algorithm\nthat collects batches of trajectory pairs and applies optimal experimental\ndesign to select informative comparison queries. The batch structure also\nenables parallelization of preference queries, which is relevant in practical\ndeployment as feedback can be gathered concurrently. Empirical evaluation\nconfirms that the proposed method is competitive with reward-based\nreinforcement learning while requiring a small number of preference queries.", "AI": {"tldr": "A meta-algorithm for reinforcement learning from human feedback using trajectory-level preference comparisons, ensuring tractability and theoretical guarantees.", "motivation": "To address the challenge of designing algorithms that efficiently learn from human feedback while maintaining theoretical guarantees in Markov decision processes.", "method": "Proposes a randomized exploration meta-algorithm and an improved batch-based algorithm using optimal experimental design for informative queries.", "result": "Achieves regret and last-iterate guarantees, with empirical results showing competitiveness with reward-based RL using fewer queries.", "conclusion": "The method is efficient, tractable, and reduces query complexity, making it practical for real-world deployment."}}
{"id": "2506.09526", "pdf": "https://arxiv.org/pdf/2506.09526", "abs": "https://arxiv.org/abs/2506.09526", "authors": ["Woojin Cho", "Minju Jo", "Kookjin Lee", "Noseong Park"], "title": "Neural Functions for Learning Periodic Signal", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As function approximators, deep neural networks have served as an effective\ntool to represent various signal types. Recent approaches utilize multi-layer\nperceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its\ncorresponding signal, facilitating the learning of continuous neural\nrepresentations from discrete data points. Despite notable successes in\nlearning diverse signal types, coordinate-based MLPs often face issues of\noverfitting and limited generalizability beyond the training region, resulting\nin subpar extrapolation performance. This study addresses scenarios where the\nunderlying true signals exhibit periodic properties, either spatially or\ntemporally. We propose a novel network architecture, which extracts periodic\npatterns from measurements and leverages this information to represent the\nsignal, thereby enhancing generalization and improving extrapolation\nperformance. We demonstrate the efficacy of the proposed method through\ncomprehensive experiments, including the learning of the periodic solutions for\ndifferential equations, and time series imputation (interpolation) and\nforecasting (extrapolation) on real-world datasets.", "AI": {"tldr": "A novel neural network architecture improves generalization and extrapolation for periodic signals by leveraging periodic patterns.", "motivation": "Coordinate-based MLPs struggle with overfitting and poor extrapolation, especially for periodic signals.", "method": "Proposes a network architecture that extracts and utilizes periodic patterns from data.", "result": "Enhanced generalization and better extrapolation performance, validated on differential equations and real-world datasets.", "conclusion": "The method effectively addresses limitations of MLPs for periodic signals, improving performance in interpolation and forecasting."}}
{"id": "2506.09541", "pdf": "https://arxiv.org/pdf/2506.09541", "abs": "https://arxiv.org/abs/2506.09541", "authors": ["Yi Zhang", "Yi Wang", "Yawen Cui", "Lap-Pui Chau"], "title": "3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection\napproach that effectively handles single- and multi-view RGB images in indoor\nand outdoor environments, showcasing its general-purpose applicability. The key\nchallenge for image-based 3D object detection tasks is the lack of 3D geometric\ncues, which leads to ambiguity in establishing correspondences between images\nand 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D\ngeometric representations in both explicit and implicit manners based on\npredicted depth information. Specifically, we utilize the predicted depth to\nlearn voxel occupancy and optimize the voxelized 3D feature volume explicitly\nthrough the proposed voxel occupancy attention. To further enhance 3D\nawareness, the feature volume is integrated with an implicit 3D representation,\nthe truncated signed distance function (TSDF). Without requiring supervision\nfrom 3D signals, we significantly improve the model's comprehension of 3D\ngeometry by leveraging intermediate 3D representations and achieve end-to-end\ntraining. Our approach surpasses the performance of state-of-the-art\nimage-based methods on both single- and multi-view benchmark datasets across\ndiverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D\ndataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19\nAP3D@0.7 improvement on the KITTI dataset. The project page is available at:\nhttps://cindy0725.github.io/3DGeoDet/.", "AI": {"tldr": "3DGeoDet is a geometry-aware 3D object detection method using RGB images, improving performance via explicit and implicit 3D representations without 3D supervision.", "motivation": "Addressing the lack of 3D geometric cues in image-based 3D object detection, which causes ambiguity in image-to-3D correspondences.", "method": "Generates 3D geometric representations using predicted depth, combining explicit voxel occupancy attention and implicit TSDF for enhanced 3D awareness.", "result": "Outperforms state-of-the-art methods, with significant mAP improvements on SUN RGB-D, ScanNetV2, and KITTI datasets.", "conclusion": "3DGeoDet effectively improves 3D geometry comprehension and detection performance, demonstrating general-purpose applicability."}}
{"id": "2506.09827", "pdf": "https://arxiv.org/pdf/2506.09827", "abs": "https://arxiv.org/abs/2506.09827", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "S\u00f6ren Auer"], "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.", "AI": {"tldr": "The paper introduces EmoNet-Voice, a new benchmark for evaluating AI's emotional understanding in speech, featuring large-scale datasets and synthetic audio with expert validation.", "motivation": "Current speech emotion recognition datasets lack granularity, privacy, or rely on acted portrayals, necessitating a robust benchmark.", "method": "EmoNet-Voice includes a pre-training dataset (EmoNet-Voice Big) and a benchmark dataset (EmoNet-Voice Bench) with synthetic audio and expert annotations.", "result": "Empathic Insight Voice models achieve high agreement with human experts, with findings like anger being easier to detect than low-arousal states.", "conclusion": "EmoNet-Voice provides a privacy-preserving, fine-grained benchmark for SER, advancing AI's emotional understanding capabilities."}}
{"id": "2506.09520", "pdf": "https://arxiv.org/pdf/2506.09520", "abs": "https://arxiv.org/abs/2506.09520", "authors": ["Jason da Silva Castanheira", "Nicholas Shea", "Stephen M. Fleming"], "title": "How attention simplifies mental representations for planning", "categories": ["q-bio.NC", "cs.AI", "cs.RO"], "comment": null, "summary": "Human planning is efficient -- it frugally deploys limited cognitive\nresources to accomplish difficult tasks -- and flexible -- adapting to novel\nproblems and environments. Computational approaches suggest that people\nconstruct simplified mental representations of their environment, balancing the\ncomplexity of a task representation with its utility. These models imply a\nnested optimisation in which planning shapes perception, and perception shapes\nplanning -- but the perceptual and attentional mechanisms governing how this\ninteraction unfolds remain unknown. Here, we harness virtual maze navigation to\ncharacterise how spatial attention controls which aspects of a task\nrepresentation enter subjective awareness and are available for planning. We\nfind that spatial proximity governs which aspects of a maze are available for\nplanning, and that when task-relevant information follows natural (lateralised)\ncontours of attention, people can more easily construct simplified and useful\nmaze representations. This influence of attention varies considerably across\nindividuals, explaining differences in people's task representations and\nbehaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the\neffects of visuospatial attention into existing computational accounts of\nvalue-guided construal. Together, our work bridges computational perspectives\non perception and decision-making to better understand how individuals\nrepresent their environments in aid of planning.", "AI": {"tldr": "The paper explores how spatial attention influences task representation and planning in humans, using virtual maze navigation to show proximity and natural attention contours simplify representations, with individual variations affecting behavior.", "motivation": "To understand the perceptual and attentional mechanisms that govern how humans construct simplified mental representations for efficient and flexible planning.", "method": "Virtual maze navigation experiments to characterize how spatial attention controls task representation and planning.", "result": "Spatial proximity and natural attention contours simplify maze representations, with individual variations in attention affecting behavior.", "conclusion": "The study bridges computational models of perception and decision-making, showing how visuospatial attention shapes task representations for planning."}}
{"id": "2506.09532", "pdf": "https://arxiv.org/pdf/2506.09532", "abs": "https://arxiv.org/abs/2506.09532", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "AI": {"tldr": "Athena-PRM is a multimodal process reward model for evaluating reasoning steps, using weak-strong completer consistency for high-quality labels. It achieves strong performance with minimal data and outperforms benchmarks.", "motivation": "High-performance PRMs require costly step-level annotations. Existing methods are noisy and expensive, prompting the need for efficient, high-quality labeling.", "method": "Uses prediction consistency between weak and strong completers for reliable labels, with ORM initialization and negative data up-sampling.", "result": "Achieves superior performance with 5,000 samples, improving benchmarks by up to 10.2 points and setting SoTA in VisualProcessBench.", "conclusion": "Athena-PRM is effective for reasoning step evaluation and enhances model performance, as demonstrated by Athena-7B's benchmark outperformance."}}
{"id": "2506.09553", "pdf": "https://arxiv.org/pdf/2506.09553", "abs": "https://arxiv.org/abs/2506.09553", "authors": ["Ligao Deng", "Yupeng Deng", "Yu Meng", "Jingbo Chen", "Zhihao Xi", "Diyou Liu", "Qifeng Chu"], "title": "GLD-Road:A global-local decoding road network extraction model for remote sensing images", "categories": ["cs.CV"], "comment": null, "summary": "Road networks are crucial for mapping, autonomous driving, and disaster\nresponse. While manual annotation is costly, deep learning offers efficient\nextraction. Current methods include postprocessing (prone to errors), global\nparallel (fast but misses nodes), and local iterative (accurate but slow). We\npropose GLD-Road, a two-stage model combining global efficiency and local\nprecision. First, it detects road nodes and connects them via a Connect Module.\nThen, it iteratively refines broken roads using local searches, drastically\nreducing computation. Experiments show GLD-Road outperforms state-of-the-art\nmethods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also\nreduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++\n(local). The experimental results are available at\nhttps://github.com/ucas-dlg/GLD-Road.", "AI": {"tldr": "GLD-Road is a two-stage model for road network extraction, combining global efficiency and local precision, outperforming existing methods in accuracy and speed.", "motivation": "Manual annotation of road networks is costly, and current deep learning methods either sacrifice accuracy for speed or vice versa.", "method": "GLD-Road uses a two-stage approach: global detection of road nodes and connections, followed by local iterative refinement of broken roads.", "result": "GLD-Road improves APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3), and reduces retrieval time by 40% vs. Sat2Graph and 92% vs. RNGDet++.", "conclusion": "GLD-Road effectively balances speed and accuracy, making it a superior solution for road network extraction."}}
{"id": "2506.09833", "pdf": "https://arxiv.org/pdf/2506.09833", "abs": "https://arxiv.org/abs/2506.09833", "authors": ["Omar Sherif", "Ali Hamdi"], "title": "Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation", "categories": ["cs.CL", "I.2.1"], "comment": "6 pages, 1 figure. To appear in Intelligent Methods, Systems, and\n  Applications 2025", "summary": "Effective rehabilitation assessment is essential for monitoring patient\nprogress, particularly in home-based settings. Existing systems often face\nchallenges such as data imbalance and difficulty detecting subtle movement\nerrors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method\nthat generates synthetic skeleton data by simulating clinically relevant\nmovement mistakes. Unlike standard augmentation techniques, EGPA targets\nbiomechanical errors observed in rehabilitation. Combined with an\nattention-based graph convolutional network, EGPA improves performance across\nmultiple evaluation metrics. Experiments demonstrate reductions in mean\nabsolute error of up to 27.6 percent and gains in error classification accuracy\nof 45.8 percent. Attention visualizations show that the model learns to focus\non clinically significant joints and movement phases, enhancing both accuracy\nand interpretability. EGPA offers a promising approach for improving automated\nmovement quality assessment in both clinical and home-based rehabilitation\ncontexts.", "AI": {"tldr": "EGPA introduces synthetic skeleton data to address data imbalance and detect subtle movement errors in rehabilitation, improving accuracy and interpretability.", "motivation": "Existing rehabilitation systems struggle with data imbalance and detecting subtle movement errors, especially in home-based settings.", "method": "EGPA generates synthetic skeleton data by simulating clinically relevant movement mistakes and uses an attention-based graph convolutional network.", "result": "Experiments show a 27.6% reduction in mean absolute error and a 45.8% gain in error classification accuracy.", "conclusion": "EGPA enhances automated movement quality assessment in rehabilitation, offering clinical and home-based applications."}}
{"id": "2506.09548", "pdf": "https://arxiv.org/pdf/2506.09548", "abs": "https://arxiv.org/abs/2506.09548", "authors": ["Taku Okawara", "Kenji Koide", "Aoki Takanose", "Shuji Oishi", "Masashi Yokozuka", "Kentaro Uno", "Kazuya Yoshida"], "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Robotics and Automation Letters", "summary": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is\nrobust to challenging conditions such as featureless environments and\ndeformable terrains. We developed an online learning-based leg kinematics model\nnamed the neural leg kinematics model, which incorporates tactile information\n(foot reaction force) to implicitly express the nonlinear dynamics between\nrobot feet and the ground. Online training of this model enhances its\nadaptability to weight load changes of a robot (e.g., assuming delivery or\ntransportation tasks) and terrain conditions. According to the \\textit{neural\nadaptive leg odometry factor} and online uncertainty estimation of the leg\nkinematics model-based motion predictions, we jointly solve online training of\nthis kinematics model and odometry estimation on a unified factor graph to\nretain the consistency of both. The proposed method was verified through real\nexperiments using a quadruped robot in two challenging situations: 1) a sandy\nbeach, representing an extremely featureless area with a deformable terrain,\nand 2) a campus, including multiple featureless areas and terrain types of\nasphalt, gravel (deformable terrain), and grass. Experimental results showed\nthat our odometry estimation incorporating the \\textit{neural leg kinematics\nmodel} outperforms state-of-the-art works. Our project page is available for\nfurther details: https://takuokawara.github.io/RAL2025_project_page/", "AI": {"tldr": "A LiDAR-IMU-leg odometry system with an online learning-based leg kinematics model improves robustness in featureless and deformable terrains.", "motivation": "Addressing challenges like featureless environments and deformable terrains for reliable robot odometry.", "method": "Developed a neural leg kinematics model incorporating tactile data, integrated with odometry estimation on a factor graph.", "result": "Outperformed state-of-the-art methods in real-world experiments on sandy beaches and varied campus terrains.", "conclusion": "The proposed system enhances adaptability and accuracy in challenging conditions."}}
{"id": "2506.09544", "pdf": "https://arxiv.org/pdf/2506.09544", "abs": "https://arxiv.org/abs/2506.09544", "authors": ["Yang Yang", "Du Yin", "Hao Xue", "Flora Salim"], "title": "STOAT: Spatial-Temporal Probabilistic Causal Inference Network", "categories": ["cs.LG"], "comment": null, "summary": "Spatial-temporal causal time series (STC-TS) involve region-specific temporal\nobservations driven by causally relevant covariates and interconnected across\ngeographic or network-based spaces. Existing methods often model spatial and\ntemporal dynamics independently and overlook causality-driven probabilistic\nforecasting, limiting their predictive power. To address this, we propose STOAT\n(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework\nfor probabilistic forecasting in STC-TS. The proposed method extends a causal\ninference approach by incorporating a spatial relation matrix that encodes\ninterregional dependencies (e.g. proximity or connectivity), enabling spatially\ninformed causal effect estimation. The resulting latent series are processed by\ndeep probabilistic models to estimate the parameters of the distributions,\nenabling calibrated uncertainty modeling. We further explore multiple output\ndistributions (e.g., Gaussian, Student's-$t$, Laplace) to capture\nregion-specific variability. Experiments on COVID-19 data across six countries\ndemonstrate that STOAT outperforms state-of-the-art probabilistic forecasting\nmodels (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,\nparticularly in regions with strong spatial dependencies. By bridging causal\ninference and geospatial probabilistic forecasting, STOAT offers a\ngeneralizable framework for complex spatial-temporal tasks, such as epidemic\nmanagement.", "AI": {"tldr": "STOAT is a novel framework for probabilistic forecasting in spatial-temporal causal time series, combining causal inference with spatial dependencies and outperforming existing models.", "motivation": "Existing methods often model spatial and temporal dynamics separately and ignore causality-driven probabilistic forecasting, limiting predictive power.", "method": "STOAT incorporates a spatial relation matrix for interregional dependencies and uses deep probabilistic models to estimate distribution parameters, enabling calibrated uncertainty modeling.", "result": "STOAT outperforms state-of-the-art models like DeepAR and DeepVAR, especially in regions with strong spatial dependencies, as shown in COVID-19 data experiments.", "conclusion": "STOAT bridges causal inference and geospatial probabilistic forecasting, offering a generalizable framework for tasks like epidemic management."}}
{"id": "2506.09557", "pdf": "https://arxiv.org/pdf/2506.09557", "abs": "https://arxiv.org/abs/2506.09557", "authors": ["Zhaoyang Wei", "Chenhui Qiang", "Bowen Jiang", "Xumeng Han", "Xuehui Yu", "Zhenjun Han"], "title": "AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to\nenhance the structured, multi-step decision-making capabilities of Multi-Modal\nLarge Models (MLLMs), is particularly crucial for autonomous driving with\nadverse weather conditions and complex traffic environments. However, existing\nbenchmarks have largely overlooked the need for rigorous evaluation of CoT\nprocesses in these specific and challenging scenarios. To address this critical\ngap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically\ndesigned for autonomous driving with adverse weather and complex scenes.\nAD^2-Bench is meticulously constructed to fulfill three key criteria:\ncomprehensive data coverage across diverse adverse environments, fine-grained\nannotations that support multi-step reasoning, and a dedicated evaluation\nframework tailored for assessing CoT performance. The core contribution of\nAD^2-Bench is its extensive collection of over 5.4k high-quality, manually\nannotated CoT instances. Each intermediate reasoning step in these annotations\nis treated as an atomic unit with explicit ground truth, enabling unprecedented\nfine-grained analysis of MLLMs' inferential processes under text-level,\npoint-level, and region-level visual prompts. Our comprehensive evaluation of\nstate-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting\nthe benchmark's difficulty and the need to advance robust, interpretable\nend-to-end autonomous driving systems. AD^2-Bench thus provides a standardized\nevaluation platform, driving research forward by improving MLLMs' reasoning in\nautonomous driving, making it an invaluable resource.", "AI": {"tldr": "AD^2-Bench is the first Chain-of-Thought benchmark for autonomous driving in adverse weather and complex scenes, featuring 5.4k annotated CoT instances and revealing MLLMs' accuracy below 60%.", "motivation": "Existing benchmarks lack rigorous evaluation of CoT reasoning in challenging autonomous driving scenarios, necessitating a dedicated benchmark.", "method": "AD^2-Bench is designed with comprehensive data coverage, fine-grained annotations for multi-step reasoning, and a tailored evaluation framework.", "result": "State-of-the-art MLLMs achieve below 60% accuracy on AD^2-Bench, underscoring its difficulty and the need for improvement.", "conclusion": "AD^2-Bench serves as a standardized platform to advance MLLMs' reasoning in autonomous driving, addressing critical gaps in current benchmarks."}}
{"id": "2506.09847", "pdf": "https://arxiv.org/pdf/2506.09847", "abs": "https://arxiv.org/abs/2506.09847", "authors": ["Tomas Peterka", "Matyas Bohacek"], "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "AI": {"tldr": "The paper introduces a dataset for detecting misattributed imagery in news, focusing on location and date relevance, and evaluates LLMs for these tasks.", "motivation": "Addressing the gap in detecting out-of-context and misattributed imagery in misinformation by focusing on provenance (location and date) rather than just semantics.", "method": "Created the News Media Provenance Dataset with provenance-tagged images and evaluated six LLMs on location (LOR) and date/time (DTOR) relevance tasks.", "result": "Zero-shot performance on LOR is promising, but DTOR performance is lacking, suggesting need for specialized solutions.", "conclusion": "The dataset and tasks highlight challenges in provenance-based detection, calling for future work to improve DTOR performance."}}
{"id": "2506.09634", "pdf": "https://arxiv.org/pdf/2506.09634", "abs": "https://arxiv.org/abs/2506.09634", "authors": ["Yanzhao Shi", "Xiaodan Zhang", "Junzhong Ji", "Haoning Jiang", "Chengxin Zheng", "Yinong Wang", "Liangqiong Qu"], "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors", "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.", "AI": {"tldr": "HSENet improves 3D medical image analysis by combining dual-3D vision encoders and Spatial Packer for better visual-language understanding, outperforming existing methods in retrieval, report generation, and question answering.", "motivation": "Existing multimodal large language models (MLLMs) focus on 2D medical images, limiting their ability to capture 3D anatomical structures and leading to diagnostic errors.", "method": "HSENet uses dual-3D vision encoders for global and fine-grained details, pre-trained with diagnostic reports, and Spatial Packer to compress 3D regions into visual tokens for LLM integration.", "result": "Achieves state-of-the-art performance in 3D language-visual retrieval (+5.96% gain), report generation (+8.01% gain), and visual question answering (+1.99% gain).", "conclusion": "HSENet effectively bridges 3D medical imaging and language understanding, enhancing diagnostic accuracy and workflow efficiency."}}
{"id": "2506.09574", "pdf": "https://arxiv.org/pdf/2506.09574", "abs": "https://arxiv.org/abs/2506.09574", "authors": ["Gaurav Chaudhary", "Wassim Uddin Mondal", "Laxmidhar Behera"], "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Sample efficiency and exploration remain critical challenges in Deep\nReinforcement Learning (DRL), particularly in complex domains. Offline RL,\nwhich enables agents to learn optimal policies from static, pre-collected\ndatasets, has emerged as a promising alternative. However, offline RL is\nconstrained by issues such as out-of-distribution (OOD) actions that limit\npolicy performance and generalization. To overcome these limitations, we\npropose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework\nthat unifies offline and online RL for efficient and scalable learning. While\nprevious hybrid methods rely on extensive design components and added\ncomputational complexity to utilize offline data effectively, MOORL introduces\na meta-policy that seamlessly adapts across offline and online trajectories.\nThis enables the agent to leverage offline data for robust initialization while\nutilizing online interactions to drive efficient exploration. Our theoretical\nanalysis demonstrates that the hybrid approach enhances exploration by\neffectively combining the complementary strengths of offline and online data.\nFurthermore, we demonstrate that MOORL learns a stable Q-function without added\ncomplexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL\nbenchmarks validate its effectiveness, showing consistent improvements over\nstate-of-the-art offline and hybrid RL baselines. With minimal computational\noverhead, MOORL achieves strong performance, underscoring its potential for\npractical applications in real-world scenarios.", "AI": {"tldr": "MOORL is a hybrid offline-online RL framework that combines offline data for initialization and online interactions for exploration, outperforming existing methods with minimal computational overhead.", "motivation": "Addressing the challenges of sample efficiency and exploration in DRL, particularly in complex domains, and overcoming the limitations of offline RL like OOD actions.", "method": "Introduces a meta-policy that adapts across offline and online trajectories, leveraging offline data for initialization and online interactions for exploration.", "result": "Theoretical analysis shows enhanced exploration, stable Q-function learning, and consistent improvements over baselines on 28 tasks from D4RL and V-D4RL benchmarks.", "conclusion": "MOORL is effective, scalable, and practical for real-world applications, achieving strong performance with minimal computational overhead."}}
{"id": "2506.09565", "pdf": "https://arxiv.org/pdf/2506.09565", "abs": "https://arxiv.org/abs/2506.09565", "authors": ["Qijing Li", "Jingxiang Sun", "Liang An", "Zhaoqi Su", "Hongwen Zhang", "Yebin Liu"], "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields", "categories": ["cs.CV"], "comment": null, "summary": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io.", "AI": {"tldr": "SemanticSplat is a feed-forward method for holistic 3D scene understanding, unifying 3D Gaussians with semantic attributes to improve geometry, appearance, and semantics modeling.", "motivation": "Existing methods either lack holistic comprehension or require dense input views, limiting practicality.", "method": "SemanticSplat fuses feature fields (e.g., LSeg, SAM) with a cost volume representation and uses a two-stage distillation framework for sparse-view reconstruction.", "result": "The method achieves coherent and accurate scene comprehension, excelling in tasks like promptable and open-vocabulary segmentation.", "conclusion": "SemanticSplat advances 3D scene understanding by unifying multi-modal semantics with geometry and appearance, offering practical deployment benefits."}}
{"id": "2506.09853", "pdf": "https://arxiv.org/pdf/2506.09853", "abs": "https://arxiv.org/abs/2506.09853", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.", "AI": {"tldr": "The paper addresses challenges in Chain-of-Thought (CoT) prompting for LLMs\u2014sufficiency and necessity of reasoning steps\u2014using a causal framework to improve efficiency and reduce redundancy.", "motivation": "To enhance the reasoning capabilities of LLMs by ensuring CoT steps are both sufficient and necessary, addressing inefficiencies in current methods.", "method": "Proposes a causal framework incorporating Probability of Sufficiency and Necessity to quantify step influence, enabling automated step addition/pruning.", "result": "Experiments show improved reasoning efficiency and reduced token usage without accuracy loss on math and commonsense benchmarks.", "conclusion": "The framework offers a cost-effective way to enhance LLM reasoning performance by optimizing CoT steps."}}
{"id": "2506.09644", "pdf": "https://arxiv.org/pdf/2506.09644", "abs": "https://arxiv.org/abs/2506.09644", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "AI": {"tldr": "DGAE improves autoencoder performance under high compression by using a diffusion model to guide the decoder, achieving smaller latent spaces and better results.", "motivation": "Addressing training instability and performance degradation in autoencoders under high compression, while minimizing latent space dimensionality.", "method": "Proposes DGAE, which employs a diffusion model to guide the decoder for better signal recovery from compressed representations.", "result": "DGAE mitigates performance degradation, achieves state-of-the-art results with a 2x smaller latent space, and improves diffusion model convergence.", "conclusion": "DGAE offers a compact and efficient solution for high-compression autoencoders, enhancing generative model performance."}}
{"id": "2506.09593", "pdf": "https://arxiv.org/pdf/2506.09593", "abs": "https://arxiv.org/abs/2506.09593", "authors": ["Achim Hekler", "Lukas Kuhn", "Florian Buettner"], "title": "Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Reliable uncertainty calibration is essential for safely deploying deep\nneural networks in high-stakes applications. Deep neural networks are known to\nexhibit systematic overconfidence, especially under distribution shifts.\nAlthough foundation models such as ConvNeXt, EVA and BEiT have demonstrated\nsignificant improvements in predictive performance, their calibration\nproperties remain underexplored. This paper presents a comprehensive\ninvestigation into the calibration behavior of foundation models, revealing\ninsights that challenge established paradigms. Our empirical analysis shows\nthat these models tend to be underconfident in in-distribution predictions,\nresulting in higher calibration errors, while demonstrating improved\ncalibration under distribution shifts. Furthermore, we demonstrate that\nfoundation models are highly responsive to post-hoc calibration techniques in\nthe in-distribution setting, enabling practitioners to effectively mitigate\nunderconfidence bias. However, these methods become progressively less reliable\nunder severe distribution shifts and can occasionally produce counterproductive\nresults. Our findings highlight the complex, non-monotonic effects of\narchitectural and training innovations on calibration, challenging established\nnarratives of continuous improvement.", "AI": {"tldr": "Foundation models like ConvNeXt, EVA, and BEiT show underconfidence in in-distribution predictions but better calibration under shifts. Post-hoc calibration works well in-distribution but fails under severe shifts.", "motivation": "To investigate the calibration behavior of foundation models, which is crucial for safe deployment in high-stakes applications.", "method": "Empirical analysis of foundation models' calibration under in-distribution and distribution shift scenarios, including post-hoc calibration techniques.", "result": "Foundation models are underconfident in-distribution but better calibrated under shifts. Post-hoc calibration is effective in-distribution but unreliable under severe shifts.", "conclusion": "Calibration behavior of foundation models is complex and non-monotonic, challenging the notion of continuous improvement with architectural innovations."}}
{"id": "2506.09612", "pdf": "https://arxiv.org/pdf/2506.09612", "abs": "https://arxiv.org/abs/2506.09612", "authors": ["Mingxiao LI", "mang ning", "Marie-Francine Moens"], "title": "Consistent Story Generation with Asymmetry Zigzag Sampling", "categories": ["cs.CV"], "comment": "17 pages, 9. figures", "summary": "Text-to-image generation models have made significant progress in producing\nhigh-quality images from textual descriptions, yet they continue to struggle\nwith maintaining subject consistency across multiple images, a fundamental\nrequirement for visual storytelling. Existing methods attempt to address this\nby either fine-tuning models on large-scale story visualization datasets, which\nis resource-intensive, or by using training-free techniques that share\ninformation across generations, which still yield limited success. In this\npaper, we introduce a novel training-free sampling strategy called Zigzag\nSampling with Asymmetric Prompts and Visual Sharing to enhance subject\nconsistency in visual story generation. Our approach proposes a zigzag sampling\nmechanism that alternates between asymmetric prompting to retain subject\ncharacteristics, while a visual sharing module transfers visual cues across\ngenerated images to %further enforce consistency. Experimental results, based\non both quantitative metrics and qualitative evaluations, demonstrate that our\nmethod significantly outperforms previous approaches in generating coherent and\nconsistent visual stories. The code is available at\nhttps://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.", "AI": {"tldr": "A novel training-free sampling strategy, Zigzag Sampling with Asymmetric Prompts and Visual Sharing, improves subject consistency in text-to-image generation for visual storytelling.", "motivation": "Existing methods for maintaining subject consistency in visual storytelling are either resource-intensive or ineffective.", "method": "Proposes zigzag sampling with asymmetric prompts and a visual sharing module to enhance consistency without training.", "result": "Outperforms previous methods in generating coherent and consistent visual stories, validated by metrics and evaluations.", "conclusion": "The approach offers a resource-efficient and effective solution for consistent visual story generation."}}
{"id": "2506.09886", "pdf": "https://arxiv.org/pdf/2506.09886", "abs": "https://arxiv.org/abs/2506.09886", "authors": ["Rodion Oblovatny", "Alexandra Bazarova", "Alexey Zaytsev"], "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.", "AI": {"tldr": "A novel method detects hallucinations in LLMs by analyzing probabilistic divergence between prompt and response hidden-state distributions, outperforming existing baselines.", "motivation": "Hallucinations in LLMs are problematic, and current detection methods often rely on external knowledge or auxiliary models. This work aims to provide a model-intrinsic solution.", "method": "The approach measures distributional distances between prompt and response hidden states, using deep learnable kernels for enhanced sensitivity.", "result": "The method achieves state-of-the-art performance on benchmarks and remains robust even without kernel training.", "conclusion": "The proposed solution is scalable and effective for detecting hallucinations in LLMs without external dependencies."}}
{"id": "2506.09662", "pdf": "https://arxiv.org/pdf/2506.09662", "abs": "https://arxiv.org/abs/2506.09662", "authors": ["Bianca Perasso", "Ludovico Lozza", "Andrea Ponte", "Luca Demetrio", "Luca Oneto", "Fabio Roli"], "title": "Empirical Quantification of Spurious Correlations in Malware Detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "End-to-end deep learning exhibits unmatched performance for detecting\nmalware, but such an achievement is reached by exploiting spurious correlations\n-- features with high relevance at inference time, but known to be useless\nthrough domain knowledge. While previous work highlighted that deep networks\nmainly focus on metadata, none investigated the phenomenon further, without\nquantifying their impact on the decision. In this work, we deepen our\nunderstanding of how spurious correlation affects deep learning for malware\ndetection by highlighting how much models rely on empty spaces left by the\ncompiler, which diminishes the relevance of the compiled code. Through our\nseminal analysis on a small-scale balanced dataset, we introduce a ranking of\ntwo end-to-end models to better understand which is more suitable to be put in\nproduction.", "AI": {"tldr": "The paper investigates how spurious correlations in deep learning models for malware detection affect decision-making, focusing on reliance on compiler-empty spaces over compiled code. It ranks two end-to-end models for production suitability.", "motivation": "To understand the impact of spurious correlations (like metadata) on deep learning models in malware detection and quantify their influence on decisions.", "method": "Seminal analysis on a small-scale balanced dataset, evaluating reliance on compiler-empty spaces versus compiled code.", "result": "Identified that models heavily rely on empty spaces left by compilers, reducing the relevance of compiled code. Introduced a ranking of two models for production suitability.", "conclusion": "Deep learning models for malware detection exploit spurious correlations, necessitating careful evaluation before deployment. The study provides insights for selecting production-ready models."}}
{"id": "2506.09594", "pdf": "https://arxiv.org/pdf/2506.09594", "abs": "https://arxiv.org/abs/2506.09594", "authors": ["Wenjin Qin", "Hailin Wang", "Jingyao Hou", "Jianjun Wang"], "title": "Accelerating Large-Scale Regularized High-Order Tensor Recovery", "categories": ["cs.LG"], "comment": null, "summary": "Currently, existing tensor recovery methods fail to recognize the impact of\ntensor scale variations on their structural characteristics. Furthermore,\nexisting studies face prohibitive computational costs when dealing with\nlarge-scale high-order tensor data. To alleviate these issue, assisted by the\nKrylov subspace iteration, block Lanczos bidiagonalization process, and random\nprojection strategies, this article first devises two fast and accurate\nrandomized algorithms for low-rank tensor approximation (LRTA) problem.\nTheoretical bounds on the accuracy of the approximation error estimate are\nestablished. Next, we develop a novel generalized nonconvex modeling framework\ntailored to large-scale tensor recovery, in which a new regularization paradigm\nis exploited to achieve insightful prior representation for large-scale\ntensors. On the basis of the above, we further investigate new unified\nnonconvex models and efficient optimization algorithms, respectively, for\nseveral typical high-order tensor recovery tasks in unquantized and quantized\nsituations. To render the proposed algorithms practical and efficient for\nlarge-scale tensor data, the proposed randomized LRTA schemes are integrated\ninto their central and time-intensive computations. Finally, we conduct\nextensive experiments on various large-scale tensors, whose results demonstrate\nthe practicability, effectiveness and superiority of the proposed method in\ncomparison with some state-of-the-art approaches.", "AI": {"tldr": "The paper introduces fast randomized algorithms for low-rank tensor approximation (LRTA) and a nonconvex modeling framework for large-scale tensor recovery, addressing computational challenges and outperforming existing methods.", "motivation": "Existing tensor recovery methods ignore tensor scale variations and face high computational costs with large-scale data.", "method": "Uses Krylov subspace iteration, block Lanczos bidiagonalization, and random projection for LRTA, and develops a nonconvex framework with new regularization.", "result": "Theoretical bounds on approximation error are established, and experiments show the method's superiority over state-of-the-art approaches.", "conclusion": "The proposed algorithms and framework are practical, effective, and efficient for large-scale tensor recovery tasks."}}
{"id": "2506.09626", "pdf": "https://arxiv.org/pdf/2506.09626", "abs": "https://arxiv.org/abs/2506.09626", "authors": ["Giacomo Rosin", "Muhammad Rameez Ur Rahman", "Sebastiano Vascon"], "title": "ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting", "categories": ["cs.CV"], "comment": "IJCNN 2025", "summary": "Human trajectory forecasting is crucial in applications such as autonomous\ndriving, robotics and surveillance. Accurate forecasting requires models to\nconsider various factors, including social interactions, multi-modal\npredictions, pedestrian intention and environmental context. While existing\nmethods account for these factors, they often overlook the impact of the\nenvironment, which leads to collisions with obstacles. This paper introduces\nECAM (Environmental Collision Avoidance Module), a contrastive learning-based\nmodule to enhance collision avoidance ability with the environment. The\nproposed module can be integrated into existing trajectory forecasting models,\nimproving their ability to generate collision-free predictions. We evaluate our\nmethod on the ETH/UCY dataset and quantitatively and qualitatively demonstrate\nits collision avoidance capabilities. Our experiments show that\nstate-of-the-art methods significantly reduce (-40/50%) the collision rate when\nintegrated with the proposed module. The code is available at\nhttps://github.com/CVML-CFU/ECAM.", "AI": {"tldr": "The paper introduces ECAM, a contrastive learning-based module to improve collision avoidance in human trajectory forecasting by integrating environmental context.", "motivation": "Existing trajectory forecasting methods often neglect environmental impact, leading to collisions. ECAM addresses this gap.", "method": "ECAM uses contrastive learning to enhance collision avoidance and integrates with existing models.", "result": "Experiments on ETH/UCY dataset show a 40-50% reduction in collision rates when ECAM is added to state-of-the-art methods.", "conclusion": "ECAM effectively improves collision avoidance in trajectory forecasting, as demonstrated by significant collision rate reductions."}}
{"id": "2506.09890", "pdf": "https://arxiv.org/pdf/2506.09890", "abs": "https://arxiv.org/abs/2506.09890", "authors": ["Yuxin Chen", "Yiran Zhao", "Yang Zhang", "An Zhang", "Kenji Kawaguchi", "Shafiq Joty", "Junnan Li", "Tat-Seng Chua", "Michael Qizhe Shieh", "Wenxuan Zhang"], "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.", "AI": {"tldr": "LLMs develop a core language-agnostic parameter space, enabling abstract thought beyond specific languages, supported by shared neurons. Neuron-specific training strategies are proposed for enhanced multilingual performance.", "motivation": "Challenge the assumption that LLMs 'think' in English by identifying a language-agnostic parameter space and shared neurons that support multilingual generalization.", "method": "Identify language-related neurons (shared or exclusive) and analyze their evolution in LLMs. Propose neuron-specific training strategies for different development stages.", "result": "Shared neurons increase in importance, forming a core language-agnostic space, while exclusive neurons diminish. Proposed training strategies improve multilingual performance.", "conclusion": "LLMs develop abstract thought through shared neurons, and tailored training strategies can enhance their multilingual capabilities."}}
{"id": "2506.09677", "pdf": "https://arxiv.org/pdf/2506.09677", "abs": "https://arxiv.org/abs/2506.09677", "authors": ["Bin Zhu", "Hailong Yin", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Reasoning Models Are More Easily Gaslighted Than You Think", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in reasoning-centric models promise improved robustness\nthrough mechanisms such as chain-of-thought prompting and test-time scaling.\nHowever, their ability to withstand misleading user input remains\nunderexplored. In this paper, we conduct a systematic evaluation of three\nstate-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet\nand Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and\nCharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)\nfollowing gaslighting negation prompts, indicating that even top-tier reasoning\nmodels struggle to preserve correct answers under manipulative user feedback.\nBuilt upon the insights of the evaluation and to further probe this\nvulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark\nspecifically designed to evaluate reasoning models' susceptibility to defend\ntheir belief under gaslighting negation prompt. Constructed by filtering and\ncurating 1,025 challenging samples from the existing benchmarks,\nGaslightingBench-R induces even more dramatic failures, with accuracy drops\nexceeding 53% on average. Our findings reveal fundamental limitations in the\nrobustness of reasoning models, highlighting the gap between step-by-step\nreasoning and belief persistence.", "AI": {"tldr": "The paper evaluates reasoning models' vulnerability to gaslighting negation prompts, revealing significant accuracy drops, and introduces GaslightingBench-R to further test this weakness.", "motivation": "To assess the robustness of reasoning models against misleading user input, an underexplored area.", "method": "Systematic evaluation of three state-of-the-art reasoning models (OpenAI's o4-mini, Claude-3.7-Sonnet, Gemini-2.5-Flash) on multimodal benchmarks (MMMU, MathVista, CharXiv), followed by the creation of GaslightingBench-R.", "result": "Accuracy drops of 25-29% under gaslighting prompts, worsening to over 53% on GaslightingBench-R.", "conclusion": "Reasoning models have fundamental robustness limitations, showing a gap between step-by-step reasoning and belief persistence."}}
{"id": "2506.09613", "pdf": "https://arxiv.org/pdf/2506.09613", "abs": "https://arxiv.org/abs/2506.09613", "authors": ["Kaiwen Tuo", "Huan Wang"], "title": "SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot", "categories": ["cs.LG"], "comment": null, "summary": "State-space language models such as Mamba match Transformer quality while\npermitting linear complexity inference, yet still comprise billions of\nparameters that hinder deployment. Existing one-shot pruning methods are\ntailored to attention blocks and fail to account for the time-shared and\ndiscretized state-transition matrix at the heart of the selective state-space\nmodule (SSM). In this paper, we introduce SparseSSM, the first training-free\npruning framework that extends the classic optimal brain surgeon (OBS)\nframework to state space architectures. Our layer-wise algorithm (i) derives an\napproximate second-order saliency score that aggregates Hessian-trace\ninformation across time steps, (ii) incorporates a component sensitivity\nanalysis to guide feed-forward network (FFN) pruning, which also sheds light on\nwhere redundancy resides in mamba architecture, (iii) can be easily extended to\nsemi-structured and structured sparsity. Empirically, we prune 50% of SSM\nweights without fine-tuning and observe no zero-shot accuracy loss, achieving\nthe current state-of-the-art pruning algorithm for Mamba-based LLMs.", "AI": {"tldr": "SparseSSM is a training-free pruning framework for state-space models like Mamba, achieving 50% weight pruning without accuracy loss.", "motivation": "Existing pruning methods fail for state-space models due to their unique architecture.", "method": "Extends optimal brain surgeon (OBS) with layer-wise pruning, Hessian-trace aggregation, and sensitivity analysis.", "result": "Prunes 50% of SSM weights without fine-tuning, maintaining zero-shot accuracy.", "conclusion": "SparseSSM is state-of-the-art for pruning Mamba-based LLMs."}}
{"id": "2506.09663", "pdf": "https://arxiv.org/pdf/2506.09663", "abs": "https://arxiv.org/abs/2506.09663", "authors": ["Haowen Wang", "Xiaoping Yuan", "Zhao Jin", "Zhen Zhao", "Zhengping Che", "Yousong Xue", "Jin Tian", "Yakun Huang", "Jian Tang"], "title": "Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Articulated objects are ubiquitous in everyday life, and accurate 3D\nrepresentations of their geometry and motion are critical for numerous\napplications. However, in the absence of human annotation, existing approaches\nstill struggle to build a unified representation for objects that contain\nmultiple movable parts. We introduce DeGSS, a unified framework that encodes\narticulated objects as deformable 3D Gaussian fields, embedding geometry,\nappearance, and motion in one compact representation. Each interaction state is\nmodeled as a smooth deformation of a shared field, and the resulting\ndeformation trajectories guide a progressive coarse-to-fine part segmentation\nthat identifies distinct rigid components, all in an unsupervised manner. The\nrefined field provides a spatially continuous, fully decoupled description of\nevery part, supporting part-level reconstruction and precise modeling of their\nkinematic relationships. To evaluate generalization and realism, we enlarge the\nsynthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset\nthat pairs RGB captures with accurately reverse-engineered 3D models. Extensive\nexperiments demonstrate that our method outperforms existing methods in both\naccuracy and stability.", "AI": {"tldr": "DeGSS introduces a unified framework for 3D representation of articulated objects using deformable Gaussian fields, enabling unsupervised part segmentation and precise kinematic modeling.", "motivation": "Existing methods struggle with unified 3D representations of articulated objects without human annotation, limiting applications.", "method": "DeGSS encodes objects as deformable 3D Gaussian fields, modeling interaction states as smooth deformations and enabling unsupervised part segmentation.", "result": "The method outperforms existing approaches in accuracy and stability, validated on synthetic and real-world datasets.", "conclusion": "DeGSS provides a robust, unsupervised solution for 3D representation of articulated objects, advancing part-level reconstruction and kinematic modeling."}}
{"id": "2506.09902", "pdf": "https://arxiv.org/pdf/2506.09902", "abs": "https://arxiv.org/abs/2506.09902", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "AI": {"tldr": "PersonaLens is a benchmark for evaluating personalization in task-oriented AI assistants, addressing gaps in existing benchmarks by using diverse user profiles and LLM-based agents for assessment.", "motivation": "Existing benchmarks for personalization in AI assistants are limited to chit-chat or narrow domains, failing to capture task-oriented complexities.", "method": "Introduces PersonaLens with diverse user profiles, a user agent for dialogues, and a judge agent using LLM-as-a-Judge to evaluate personalization, response quality, and task success.", "result": "Experiments show significant variability in personalization capabilities of current LLM assistants.", "conclusion": "PersonaLens provides crucial insights for advancing conversational AI systems by addressing the challenges of personalized task-oriented assistance."}}
{"id": "2506.09695", "pdf": "https://arxiv.org/pdf/2506.09695", "abs": "https://arxiv.org/abs/2506.09695", "authors": ["Changwei Wu", "Yifei Chen", "Yuxin Du", "Jinying Zong", "Jie Dong", "Mingxuan Liu", "Yong Peng", "Jin Fan", "Feiwei Qin", "Changmiao Wang"], "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.", "AI": {"tldr": "FasterSNN, a hybrid spiking neural network, improves Alzheimer's Disease diagnosis by combining LIF neurons, region-adaptive convolution, and multi-scale spiking attention for efficient and stable 3D MRI processing.", "motivation": "Early AD diagnosis is challenging due to subjective assessments and costly imaging. Deep learning methods are inefficient, while SNNs, though promising, suffer from weak expressiveness and unstable training.", "method": "FasterSNN integrates LIF neurons with region-adaptive convolution and multi-scale spiking attention to process 3D MRI efficiently.", "result": "Benchmark tests show FasterSNN achieves competitive performance with improved efficiency and stability.", "conclusion": "FasterSNN is a practical solution for AD screening, balancing accuracy and computational efficiency."}}
{"id": "2506.09625", "pdf": "https://arxiv.org/pdf/2506.09625", "abs": "https://arxiv.org/abs/2506.09625", "authors": ["Ekaterina Filimoshina", "Dmitry Shirokov"], "title": "GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras", "categories": ["cs.LG", "68T07, 15A66"], "comment": "Accepted to ICML 2025", "summary": "We propose, implement, and compare with competitors a new architecture of\nequivariant neural networks based on geometric (Clifford) algebras: Generalized\nLipschitz Group Equivariant Neural Networks (GLGENN). These networks are\nequivariant to all pseudo-orthogonal transformations, including rotations and\nreflections, of a vector space with any non-degenerate or degenerate symmetric\nbilinear form. We propose a weight-sharing parametrization technique that takes\ninto account the fundamental structures and operations of geometric algebras.\nDue to this technique, GLGENN architecture is parameter-light and has less\ntendency to overfitting than baseline equivariant models. GLGENN outperforms or\nmatches competitors on several benchmarking equivariant tasks, including\nestimation of an equivariant function and a convex hull experiment, while using\nsignificantly fewer optimizable parameters.", "AI": {"tldr": "GLGENN is a new equivariant neural network architecture using geometric algebras, outperforming competitors with fewer parameters and less overfitting.", "motivation": "To create a neural network equivariant to all pseudo-orthogonal transformations, including rotations and reflections, for vector spaces with symmetric bilinear forms.", "method": "Uses geometric (Clifford) algebras and a weight-sharing parametrization technique to reduce parameters and overfitting.", "result": "GLGENN matches or outperforms competitors on equivariant tasks like function estimation and convex hull experiments, using fewer parameters.", "conclusion": "GLGENN is a parameter-efficient, high-performing architecture for equivariant tasks."}}
{"id": "2506.09668", "pdf": "https://arxiv.org/pdf/2506.09668", "abs": "https://arxiv.org/abs/2506.09668", "authors": ["Maik Dannecker", "Vasiliki Sideri-Lampretsa", "Sophie Starck", "Angeline Mihailov", "Mathieu Milh", "Nadine Girard", "Guillaume Auzias", "Daniel Rueckert"], "title": "CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain", "categories": ["cs.CV", "cs.LG"], "comment": "Work currently under revision for IEEE TMI", "summary": "Magnetic resonance imaging of fetal and neonatal brains reveals rapid\nneurodevelopment marked by substantial anatomical changes unfolding within\ndays. Studying this critical stage of the developing human brain, therefore,\nrequires accurate brain models-referred to as atlases-of high spatial and\ntemporal resolution. To meet these demands, established traditional atlases and\nrecently proposed deep learning-based methods rely on large and comprehensive\ndatasets. This poses a major challenge for studying brains in the presence of\npathologies for which data remains scarce. We address this limitation with\nCINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for\ncreating high-resolution, spatio-temporal, multimodal brain atlases, suitable\nfor low-data settings. Unlike established methods, CINeMA operates in latent\nspace, avoiding compute-intensive image registration and reducing atlas\nconstruction times from days to minutes. Furthermore, it enables flexible\nconditioning on anatomical features including GA, birth age, and pathologies\nlike ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA\nsupports downstream tasks such as tissue segmentation and age prediction\nwhereas its generative properties enable synthetic data creation and\nanatomically informed data augmentation. Surpassing state-of-the-art methods in\naccuracy, efficiency, and versatility, CINeMA represents a powerful tool for\nadvancing brain research. We release the code and atlases at\nhttps://github.com/m-dannecker/CINeMA.", "AI": {"tldr": "CINeMA is a novel framework for creating high-resolution, multimodal brain atlases in low-data settings, outperforming traditional methods in speed and flexibility.", "motivation": "Studying rapid neurodevelopment in fetal and neonatal brains requires accurate atlases, but existing methods depend on large datasets, which are scarce for pathological cases.", "method": "CINeMA operates in latent space, avoiding intensive image registration and enabling flexible conditioning on anatomical features like age and pathologies.", "result": "CINeMA reduces atlas construction time from days to minutes and supports tasks like segmentation, age prediction, and synthetic data creation.", "conclusion": "CINeMA is a versatile and efficient tool for brain research, with applications in segmentation, prediction, and data augmentation."}}
{"id": "2506.09917", "pdf": "https://arxiv.org/pdf/2506.09917", "abs": "https://arxiv.org/abs/2506.09917", "authors": ["Wendi Zhou", "Ameer Saadat-Yazd", "Nadin Kokciyan"], "title": "Aspect-Based Opinion Summarization with Argumentation Schemes", "categories": ["cs.CL"], "comment": "Accepted by ArgMining 2025", "summary": "Reviews are valuable resources for customers making purchase decisions in\nonline shopping. However, it is impractical for customers to go over the vast\nnumber of reviews and manually conclude the prominent opinions, which prompts\nthe need for automated opinion summarization systems. Previous approaches,\neither extractive or abstractive, face challenges in automatically producing\ngrounded aspect-centric summaries. In this paper, we propose a novel\nsummarization system that not only captures predominant opinions from an aspect\nperspective with supporting evidence, but also adapts to varying domains\nwithout relying on a pre-defined set of aspects. Our proposed framework,\nASESUM, summarizes viewpoints relevant to the critical aspects of a product by\nextracting aspect-centric arguments and measuring their salience and validity.\nWe conduct experiments on a real-world dataset to demonstrate the superiority\nof our approach in capturing diverse perspectives of the original reviews\ncompared to new and existing methods.", "AI": {"tldr": "The paper introduces ASESUM, a novel summarization system for online reviews that captures aspect-centric opinions without relying on predefined aspects, outperforming existing methods.", "motivation": "The impracticality of manually summarizing vast online reviews necessitates automated systems that can extract and summarize key opinions effectively.", "method": "The proposed ASESUM framework extracts aspect-centric arguments, measures their salience and validity, and adapts to varying domains without predefined aspects.", "result": "Experiments on a real-world dataset show ASESUM's superiority in capturing diverse perspectives compared to existing methods.", "conclusion": "ASESUM effectively automates opinion summarization by focusing on critical aspects and providing grounded summaries, advancing the field beyond extractive and abstractive approaches."}}
{"id": "2506.09701", "pdf": "https://arxiv.org/pdf/2506.09701", "abs": "https://arxiv.org/abs/2506.09701", "authors": ["Vincenzo Collura", "Karim Tit", "Laura Bussi", "Eleonora Giunchiglia", "Maxime Cordy"], "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.", "AI": {"tldr": "TRIDENT is a model-agnostic algorithm ensuring LLM outputs comply with temporal constraints (LTLf) without retraining, using DFA-guided beam search.", "motivation": "LLMs lack inherent capability to enforce temporal constraints, necessitating a solution like TRIDENT.", "method": "Compiles LTLf formulas into a DFA to guide constrained beam search, masking violations and re-ranking paths.", "result": "Guarantees constraint satisfaction and improves output quality, validated on image-stream classification and text generation.", "conclusion": "TRIDENT effectively enforces temporal constraints while maintaining or enhancing output quality and efficiency."}}
{"id": "2506.09630", "pdf": "https://arxiv.org/pdf/2506.09630", "abs": "https://arxiv.org/abs/2506.09630", "authors": ["Pol G. Recasens", "Alberto Gutierrez", "Jordi Torres", "Josep. Ll Berral", "Anisa Halimi", "Kieran Fraser"], "title": "In-Context Bias Propagation in LLM-Based Tabular Data Generation", "categories": ["cs.LG"], "comment": "Paper accepted at ICML 2025 workshop DIG-BUG", "summary": "Large Language Models (LLMs) are increasingly used for synthetic tabular data\ngeneration through in-context learning (ICL), offering a practical solution for\ndata augmentation in data scarce scenarios. While prior work has shown the\npotential of LLMs to improve downstream task performance through augmenting\nunderrepresented groups, these benefits often assume access to a subset of\nunbiased in-context examples, representative of the real dataset. In real-world\nsettings, however, data is frequently noisy and demographically skewed. In this\npaper, we systematically study how statistical biases within in-context\nexamples propagate to the distribution of synthetic tabular data, showing that\neven mild in-context biases lead to global statistical distortions. We further\nintroduce an adversarial scenario where a malicious contributor can inject bias\ninto the synthetic dataset via a subset of in-context examples, ultimately\ncompromising the fairness of downstream classifiers for a targeted and\nprotected subgroup. Our findings demonstrate a new vulnerability associated\nwith LLM-based data generation pipelines that rely on in-context prompts with\nin sensitive domains.", "AI": {"tldr": "LLMs for synthetic tabular data generation can propagate biases from in-context examples, leading to unfair downstream outcomes.", "motivation": "To investigate how biases in in-context examples affect synthetic data and downstream fairness, especially in adversarial scenarios.", "method": "Systematic study of bias propagation in LLM-generated synthetic data and adversarial bias injection.", "result": "Mild in-context biases cause global statistical distortions; adversarial bias injection compromises fairness for protected subgroups.", "conclusion": "LLM-based data generation is vulnerable to bias propagation, posing risks in sensitive domains."}}
{"id": "2506.09691", "pdf": "https://arxiv.org/pdf/2506.09691", "abs": "https://arxiv.org/abs/2506.09691", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "title": "Adding simple structure at inference improves Vision-Language Compositionality", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "AI": {"tldr": "The paper proposes an inference-time technique to improve compositionality in dual encoder Vision-Language Models (VLMs) by structuring image and text inputs into smaller segments and aggregating their similarities, achieving performance gains without training.", "motivation": "Dual encoder VLMs like CLIP struggle with compositionality, limiting retrieval performance. While training approaches have been explored, inference-time techniques are understudied.", "method": "The method involves: i) dividing images into crops, ii) extracting text segments (objects, attributes, relations), iii) aligning crops with text segments using a VLM, and iv) aggregating similarities for the final score.", "result": "The approach consistently improves VLM performance on compositionality tasks, especially for attribute-object binding, without requiring training. Image crop processing is key to the gains.", "conclusion": "Inference-time techniques hold promise for enhancing VLMs, with further improvements possible in specific areas."}}
{"id": "2506.09942", "pdf": "https://arxiv.org/pdf/2506.09942", "abs": "https://arxiv.org/abs/2506.09942", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.", "AI": {"tldr": "The paper introduces VerIF, a verification method combining rule-based and LLM-based verification for reinforcement learning in instruction-following tasks, achieving state-of-the-art results.", "motivation": "To address the underexplored best practices for reinforcement learning in instruction following, particularly the verification challenge.", "method": "Proposes VerIF, integrating rule-based code verification and LLM-based verification (e.g., QwQ-32B), supported by the VerInstruct dataset (22,000 instances).", "result": "Significant improvements in benchmarks, state-of-the-art performance for comparable models, and unaffected general capabilities.", "conclusion": "VerIF can enhance RL recipes for instruction-following tasks; datasets and models are released for future research."}}
{"id": "2506.09718", "pdf": "https://arxiv.org/pdf/2506.09718", "abs": "https://arxiv.org/abs/2506.09718", "authors": ["Xulin Ma", "Jiankai Tang", "Zhang Jiang", "Songqin Cheng", "Yuanchun Shi", "Dong LI", "Xin Liu", "Daniel McDuff", "Xiaojing Liu", "Yuntao Wang"], "title": "Non-Contact Health Monitoring During Daily Personal Care Routines", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring\nof physiological signals and offers a practical alternative to traditional\nhealth sensing methods. Although rPPG is promising for daily health monitoring,\nits application in long-term personal care scenarios, such as mirror-facing\nroutines in high-altitude environments, remains challenging due to ambient\nlighting variations, frequent occlusions from hand movements, and dynamic\nfacial postures. To address these challenges, we present LADH (Long-term\nAltitude Daily Health), the first long-term rPPG dataset containing 240\nsynchronized RGB and infrared (IR) facial videos from 21 participants across\nfive common personal care scenarios, along with ground-truth PPG, respiration,\nand blood oxygen signals. Our experiments demonstrate that combining RGB and IR\nvideo inputs improves the accuracy and robustness of non-contact physiological\nmonitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate\nestimation. Furthermore, we find that multi-task learning enhances performance\nacross multiple physiological indicators simultaneously. Dataset and code are\nopen at https://github.com/McJackTang/FusionVitals.", "AI": {"tldr": "The paper introduces LADH, a dataset for long-term rPPG monitoring in challenging environments, showing RGB+IR fusion improves accuracy.", "motivation": "To address challenges like lighting variations and occlusions in long-term rPPG for personal care, especially in high-altitude settings.", "method": "Uses synchronized RGB and IR facial videos from 21 participants across five scenarios, with ground-truth physiological signals.", "result": "RGB+IR fusion reduces heart rate estimation MAE to 4.99 BPM; multi-task learning boosts performance for multiple indicators.", "conclusion": "LADH dataset and RGB+IR fusion enhance rPPG robustness, with open-source data and code for further research."}}
{"id": "2506.09638", "pdf": "https://arxiv.org/pdf/2506.09638", "abs": "https://arxiv.org/abs/2506.09638", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "AI": {"tldr": "FedVLMBench is the first systematic benchmark for federated fine-tuning of Vision-Language Models (VLMs), addressing privacy concerns in domains like healthcare. It evaluates architectures, strategies, and task generalization, revealing key insights and optimal configurations.", "motivation": "Existing VLM fine-tuning relies on centralized training, which is unsuitable for privacy-sensitive domains. Federated Learning (FL) is introduced to address this, but lacks comprehensive benchmarks.", "method": "FedVLMBench integrates two VLM architectures, four fine-tuning strategies, five FL algorithms, and six multimodal datasets across various task scenarios.", "result": "A 2-layer MLP connector with concurrent tuning is optimal for encoder-based VLMs in FL. FL methods are more sensitive to data heterogeneity in vision-centric tasks.", "conclusion": "FedVLMBench provides tools, datasets, and guidance for advancing privacy-preserving, federated training of multimodal models."}}
{"id": "2506.09699", "pdf": "https://arxiv.org/pdf/2506.09699", "abs": "https://arxiv.org/abs/2506.09699", "authors": ["Mattia Nardon", "Mikel Mujika Agirre", "Ander Gonz\u00e1lez Tom\u00e9", "Daniel Sedano Algarabel", "Josep Rueda Collell", "Ana Paola Caro", "Andrea Caraffa", "Fabio Poiesi", "Paul Ian Chippendale", "Davide Boscaini"], "title": "CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Accurate 6D pose estimation of complex objects in 3D environments is\nessential for effective robotic manipulation. Yet, existing benchmarks fall\nshort in evaluating 6D pose estimation methods under realistic industrial\nconditions, as most datasets focus on household objects in domestic settings,\nwhile the few available industrial datasets are limited to artificial setups\nwith objects placed on tables. To bridge this gap, we introduce CHIP, the first\ndataset designed for 6D pose estimation of chairs manipulated by a robotic arm\nin a real-world industrial environment. CHIP includes seven distinct chairs\ncaptured using three different RGBD sensing technologies and presents unique\nchallenges, such as distractor objects with fine-grained differences and severe\nocclusions caused by the robotic arm and human operators. CHIP comprises 77,811\nRGBD images annotated with ground-truth 6D poses automatically derived from the\nrobot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP\nusing three zero-shot 6D pose estimation methods, assessing performance across\ndifferent sensor types, localization priors, and occlusion levels. Results show\nsubstantial room for improvement, highlighting the unique challenges posed by\nthe dataset. CHIP will be publicly released.", "AI": {"tldr": "CHIP is a new dataset for 6D pose estimation of chairs in industrial settings, addressing gaps in existing benchmarks by including real-world challenges like occlusions and distractors.", "motivation": "Existing datasets lack realism for industrial applications, focusing on household objects or artificial setups. CHIP fills this gap by providing a dataset for chairs manipulated by robots in real-world industrial environments.", "method": "CHIP includes 77,811 RGBD images of seven chairs, annotated with ground-truth 6D poses derived from robot kinematics. It uses three RGBD sensing technologies and introduces challenges like occlusions and fine-grained distractors.", "result": "Benchmarking with three zero-shot 6D pose estimation methods reveals significant room for improvement, emphasizing the dataset's unique challenges.", "conclusion": "CHIP addresses a critical need for realistic industrial 6D pose estimation benchmarks and will be publicly released to advance research in this area."}}
{"id": "2506.09944", "pdf": "https://arxiv.org/pdf/2506.09944", "abs": "https://arxiv.org/abs/2506.09944", "authors": ["Wuwei Zhang", "Fangcong Yin", "Howard Yen", "Danqi Chen", "Xi Ye"], "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking", "categories": ["cs.CL"], "comment": null, "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs.", "AI": {"tldr": "The paper introduces QRHEAD and QR-RETRIEVER, improving retrieval in long-context LMs, achieving significant performance gains and strong zero-shot results.", "motivation": "Enhancing retrieval capabilities in long-context language models by identifying and utilizing query-focused attention heads.", "method": "Identify QRHEAD via query-focused attention aggregation and develop QR-RETRIEVER for efficient retrieval using attention mass.", "result": "10%+ performance gains on multi-hop reasoning tasks and strong zero-shot results on BEIR, outperforming competitors.", "conclusion": "QRHEAD and QR-RETRIEVER advance retrieval in LMs, offering interpretability and practical utility."}}
{"id": "2506.09733", "pdf": "https://arxiv.org/pdf/2506.09733", "abs": "https://arxiv.org/abs/2506.09733", "authors": ["Minjong Cheon"], "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.", "AI": {"tldr": "AtmosMJ, a deep convolutional network, achieves stable long-range weather forecasts on standard latitude-longitude grids without spherical remapping, challenging the need for non-standard data representations.", "motivation": "To investigate if stable long-range weather forecasts can be achieved on standard grids, challenging the assumption that non-standard spatial domains are necessary.", "method": "Introduces AtmosMJ with a Gated Residual Fusion (GRF) mechanism to prevent error accumulation in recursive simulations, operating directly on ERA5 data.", "result": "AtmosMJ produces stable forecasts for ~500 days, matches 10-day accuracy of leading models, and requires only 5.7 days of GPU training.", "conclusion": "Efficient architectural design, not non-standard data representation, is key to stable and computationally efficient long-range weather prediction."}}
{"id": "2506.09660", "pdf": "https://arxiv.org/pdf/2506.09660", "abs": "https://arxiv.org/abs/2506.09660", "authors": ["Baran Can G\u00fcl", "Stefanos Tziampazis", "Nasser Jazdi", "Michael Weyrich"], "title": "SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization", "categories": ["cs.LG", "cs.DC"], "comment": "Preprint version. Accepted for publication at IEEE ETFA 2025", "summary": "As Federated Learning (FL) expands to larger and more distributed\nenvironments, consistency in training is challenged by network-induced delays,\nclock unsynchronicity, and variability in client updates. This combination of\nfactors may contribute to misaligned contributions that undermine model\nreliability and convergence. Existing methods like staleness-aware aggregation\nand model versioning address lagging updates heuristically, yet lack mechanisms\nto quantify staleness, especially in latency-sensitive and cross-regional\ndeployments. In light of these considerations, we introduce \\emph{SyncFed}, a\ntime-aware FL framework that employs explicit synchronization and timestamping\nto establish a common temporal reference across the system. Staleness is\nquantified numerically based on exchanged timestamps under the Network Time\nProtocol (NTP), enabling the server to reason about the relative freshness of\nclient updates and apply temporally informed weighting during aggregation. Our\nempirical evaluation on a geographically distributed testbed shows that, under\n\\emph{SyncFed}, the global model evolves within a stable temporal context,\nresulting in improved accuracy and information freshness compared to\nround-based baselines devoid of temporal semantics.", "AI": {"tldr": "SyncFed is a time-aware Federated Learning framework that uses synchronization and timestamping to improve model accuracy and freshness by quantifying staleness in client updates.", "motivation": "Challenges like network delays and unsynchronicity in Federated Learning can misalign client contributions, undermining model reliability and convergence. Existing methods lack mechanisms to quantify staleness.", "method": "SyncFed employs explicit synchronization and timestamping under NTP to establish a common temporal reference, numerically quantifying staleness for informed weighting during aggregation.", "result": "Empirical evaluation shows SyncFed improves model accuracy and information freshness compared to round-based baselines.", "conclusion": "SyncFed effectively addresses temporal misalignment in FL, enhancing model reliability and convergence through time-aware synchronization."}}
{"id": "2506.09724", "pdf": "https://arxiv.org/pdf/2506.09724", "abs": "https://arxiv.org/abs/2506.09724", "authors": ["Ye Zhang", "Yu Zhou", "Yifeng Wang", "Jun Xiao", "Ziyue Wang", "Yongbing Zhang", "Jianxu Chen"], "title": "The Four Color Theorem for Cell Instance Segmentation", "categories": ["cs.CV"], "comment": "Accepted at ICML 2025", "summary": "Cell instance segmentation is critical to analyzing biomedical images, yet\naccurately distinguishing tightly touching cells remains a persistent\nchallenge. Existing instance segmentation frameworks, including\ndetection-based, contour-based, and distance mapping-based approaches, have\nmade significant progress, but balancing model performance with computational\nefficiency remains an open problem. In this paper, we propose a novel cell\ninstance segmentation method inspired by the four-color theorem. By\nconceptualizing cells as countries and tissues as oceans, we introduce a\nfour-color encoding scheme that ensures adjacent instances receive distinct\nlabels. This reformulation transforms instance segmentation into a constrained\nsemantic segmentation problem with only four predicted classes, substantially\nsimplifying the instance differentiation process. To solve the training\ninstability caused by the non-uniqueness of four-color encoding, we design an\nasymptotic training strategy and encoding transformation method. Extensive\nexperiments on various modes demonstrate our approach achieves state-of-the-art\nperformance. The code is available at https://github.com/zhangye-zoe/FCIS.", "AI": {"tldr": "A novel cell instance segmentation method using a four-color encoding scheme inspired by the four-color theorem, simplifying instance differentiation and achieving state-of-the-art performance.", "motivation": "Accurately distinguishing tightly touching cells in biomedical images is challenging; existing methods struggle to balance performance and computational efficiency.", "method": "Proposes a four-color encoding scheme (cells as countries, tissues as oceans) to transform instance segmentation into a constrained semantic segmentation problem with four classes. Includes an asymptotic training strategy and encoding transformation to address training instability.", "result": "Achieves state-of-the-art performance across various modes, demonstrating effectiveness.", "conclusion": "The four-color encoding approach simplifies cell instance segmentation while maintaining high performance, offering a promising solution for biomedical image analysis."}}
{"id": "2506.09967", "pdf": "https://arxiv.org/pdf/2506.09967", "abs": "https://arxiv.org/abs/2506.09967", "authors": ["Shangshang Wang", "Julian Asilis", "\u00d6mer Faruk Akg\u00fcl", "Enes Burak Bilgin", "Ollie Liu", "Deqing Fu", "Willie Neiswanger"], "title": "Resa: Transparent Reasoning Models via SAEs", "categories": ["cs.CL"], "comment": null, "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\$1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround \\$1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.", "AI": {"tldr": "Resa introduces SAE-Tuning, a cost-effective method to enhance reasoning in language models, achieving near-RL performance at a fraction of the cost and time.", "motivation": "To efficiently elicit strong reasoning abilities in language models without expensive RL training.", "method": "Uses sparse autoencoder tuning (SAE-Tuning) to extract reasoning abilities from a source model and transfer them to a target model via supervised fine-tuning.", "result": "Achieves >97% of RL-trained performance at >2000x lower cost (~$1) and >450x faster (~20 minutes). Also shows generalizable and modular reasoning abilities.", "conclusion": "SAE-Tuning is a scalable, efficient alternative to RL for enhancing reasoning in language models, with potential for broader application."}}
{"id": "2506.09736", "pdf": "https://arxiv.org/pdf/2506.09736", "abs": "https://arxiv.org/abs/2506.09736", "authors": ["Yuting Li", "Lai Wei", "Kaipeng Zheng", "Jingyuan Huang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report", "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.", "AI": {"tldr": "Language-only models with image captions can outperform MLLMs, suggesting poor visual integration. A visual perturbation framework improves robustness without extra training, enhancing reasoning performance.", "motivation": "Current MLLMs generate accurate visual descriptions but struggle to integrate them in reasoning, as shown by language-only models outperforming them.", "method": "Proposes a visual perturbation framework with three strategies: distractor concatenation, dominance-preserving mixup, and random rotation, integrated into post-training pipelines.", "result": "Consistent improvements in mathematical reasoning across datasets, with gains comparable to algorithmic changes. Competitive performance achieved with Qwen2.5-VL-7B.", "conclusion": "Visual perturbation is crucial for multimodal reasoning, with each strategy uniquely enhancing different aspects. Better reasoning stems from better visual processing."}}
{"id": "2506.09674", "pdf": "https://arxiv.org/pdf/2506.09674", "abs": "https://arxiv.org/abs/2506.09674", "authors": ["Alessandro Licciardi", "Davide Leo", "Davide Carbone"], "title": "Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) enables the training of machine learning models\nacross decentralized clients while preserving data privacy. However, the\npresence of anomalous or corrupted clients - such as those with faulty sensors\nor non representative data distributions - can significantly degrade model\nperformance. Detecting such clients without accessing raw data remains a key\nchallenge. We propose WAFFLE (Wavelet and Fourier representations for Federated\nLearning) a detection algorithm that labels malicious clients {\\it before\ntraining}, using locally computed compressed representations derived from\neither the Wavelet Scattering Transform (WST) or the Fourier Transform. Both\napproaches provide low-dimensional, task-agnostic embeddings suitable for\nunsupervised client separation. A lightweight detector, trained on a\ndistillated public dataset, performs the labeling with minimal communication\nand computational overhead. While both transforms enable effective detection,\nWST offers theoretical advantages, such as non-invertibility and stability to\nlocal deformations, that make it particularly well-suited to federated\nscenarios. Experiments on benchmark datasets show that our method improves\ndetection accuracy and downstream classification performance compared to\nexisting FL anomaly detection algorithms, validating its effectiveness as a\npre-training alternative to online detection strategies.", "AI": {"tldr": "WAFFLE detects malicious clients in Federated Learning (FL) before training using compressed representations (Wavelet or Fourier transforms), improving accuracy and performance.", "motivation": "Anomalous clients in FL degrade model performance, but detecting them without raw data access is challenging.", "method": "Uses Wavelet Scattering Transform (WST) or Fourier Transform for task-agnostic embeddings, with a lightweight detector trained on public data.", "result": "Improves detection accuracy and downstream classification over existing FL anomaly detection methods.", "conclusion": "WAFFLE is effective as a pre-training alternative to online detection, with WST offering theoretical advantages."}}
{"id": "2506.09735", "pdf": "https://arxiv.org/pdf/2506.09735", "abs": "https://arxiv.org/abs/2506.09735", "authors": ["Chuang Ma", "Shaokai Zhao", "Dongdong Zhou", "Yu Pei", "Zhiguo Luo", "Liang Xie", "Ye Yan", "Erwei Yin"], "title": "MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expression recognition (MER), a critical subfield of affective\ncomputing, presents greater challenges than macro-expression recognition due to\nits brief duration and low intensity. While incorporating prior knowledge has\nbeen shown to enhance MER performance, existing methods predominantly rely on\nsimplistic, singular sources of prior knowledge, failing to fully exploit\nmulti-source information. This paper introduces the Multi-Prior Fusion Network\n(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We\npropose two complementary encoders: the Generic Feature Encoder (GFE) and the\nAdvanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with\nCoordinate Attention (CA) mechanisms, to improve the model's ability to capture\nspatiotemporal and channel-specific features. Inspired by developmental\npsychology, we present two variants of MPFNet--MPFNet-P and\nMPFNet-C--corresponding to two fundamental modes of infant cognitive\ndevelopment: parallel and hierarchical processing. These variants enable the\nevaluation of different strategies for integrating prior knowledge. Extensive\nexperiments demonstrate that MPFNet significantly improves MER accuracy while\nmaintaining balanced performance across categories, achieving accuracies of\n0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.\nTo the best of our knowledge, our approach achieves state-of-the-art\nperformance on the SMIC and SAMM datasets.", "AI": {"tldr": "The paper introduces MPFNet, a Multi-Prior Fusion Network, to improve micro-expression recognition by leveraging multi-source prior knowledge and progressive training. It achieves state-of-the-art results on key datasets.", "motivation": "Micro-expression recognition (MER) is challenging due to brief duration and low intensity. Existing methods use simplistic prior knowledge, missing multi-source potential.", "method": "Proposes MPFNet with two encoders (GFE and AFE) based on I3D and CA mechanisms. Introduces variants MPFNet-P and MPFNet-C inspired by infant cognitive development.", "result": "Achieves accuracies of 0.811, 0.924, and 0.857 on SMIC, CASME II, and SAMM datasets, setting state-of-the-art on SMIC and SAMM.", "conclusion": "MPFNet effectively integrates multi-source prior knowledge, significantly improving MER accuracy and balancing performance across categories."}}
{"id": "2506.09975", "pdf": "https://arxiv.org/pdf/2506.09975", "abs": "https://arxiv.org/abs/2506.09975", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "categories": ["cs.CL"], "comment": "to appear in ACL Findings", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "AI": {"tldr": "Detecting AI-generated social media posts is challenging due to short, informal text. A dataset of 505,159 AI-generated posts shows detection drops when attackers don't share their fine-tuned models, confirmed by human studies. Fine-tuning vulnerabilities impact all detection domains.", "motivation": "Social media is a key attack vector for influence campaigns using AI-generated posts, making detection crucial despite challenges like short text and informal language.", "method": "Created a dataset of 505,159 AI-generated posts from various LLMs across 11 topics, testing detection under realistic assumptions (no model sharing). Conducted human studies and ablation experiments.", "result": "Detection is effective with known models but drops significantly when attackers withhold fine-tuned models. Human studies confirm this, and ablation experiments reveal detection vulnerabilities.", "conclusion": "Fine-tuning LLMs poses a major challenge for detection, impacting all domains. Realistic threat scenarios require improved detection methods."}}
{"id": "2506.09740", "pdf": "https://arxiv.org/pdf/2506.09740", "abs": "https://arxiv.org/abs/2506.09740", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "AI": {"tldr": "The paper evaluates text-image alignment in diffusion models using zero-shot referring image segmentation, identifies misalignment causes, and proposes ELBO-T2IAlign for calibration.", "motivation": "Current methods assume perfect text-image alignment in diffusion models, which is often incorrect, leading to misalignment in tasks like segmentation and image editing.", "method": "Uses zero-shot referring image segmentation to assess alignment, identifies causes of misalignment (e.g., small or rare objects), and introduces ELBO-T2IAlign for calibration.", "result": "Misalignment occurs with small, occluded, or rare objects; ELBO-T2IAlign effectively calibrates alignment without training.", "conclusion": "ELBO-T2IAlign is a generic, training-free solution for improving text-image alignment in diffusion models, validated by experiments."}}
{"id": "2506.09682", "pdf": "https://arxiv.org/pdf/2506.09682", "abs": "https://arxiv.org/abs/2506.09682", "authors": ["Iulia Duta", "Pietro Li\u00f2"], "title": "Wasserstein Hypergraph Neural Network", "categories": ["cs.LG"], "comment": null, "summary": "The ability to model relational information using machine learning has driven\nadvancements across various domains, from medicine to social science. While\ngraph representation learning has become mainstream over the past decade,\nrepresenting higher-order relationships through hypergraphs is rapidly gaining\nmomentum. In the last few years, numerous hypergraph neural networks have\nemerged, most of them falling under a two-stage, set-based framework. The\nmessages are sent from nodes to edges and then from edges to nodes. However,\nmost of the advancement still takes inspiration from the graph counterpart,\noften simplifying the aggregations to basic pooling operations. In this paper\nwe are introducing Wasserstein Hypergraph Neural Network, a model that treats\nthe nodes and hyperedge neighbourhood as distributions and aggregate the\ninformation using Sliced Wasserstein Pooling. Unlike conventional aggregators\nsuch as mean or sum, which only capture first-order statistics, our approach\nhas the ability to preserve geometric properties like the shape and spread of\ndistributions. This enables the learned embeddings to reflect how easily one\nhyperedge distribution can be transformed into another, following principles of\noptimal transport. Experimental results demonstrate that applying Wasserstein\npooling in a hypergraph setting significantly benefits node classification\ntasks, achieving top performance on several real-world datasets.", "AI": {"tldr": "The paper introduces Wasserstein Hypergraph Neural Network (WHGNN), using Sliced Wasserstein Pooling for higher-order relational modeling, outperforming traditional methods in node classification.", "motivation": "Current hypergraph neural networks simplify aggregations to basic pooling, missing higher-order geometric properties. WHGNN aims to preserve these by treating nodes and hyperedges as distributions.", "method": "WHGNN models nodes and hyperedges as distributions and aggregates information using Sliced Wasserstein Pooling, capturing geometric properties like shape and spread.", "result": "WHGNN achieves top performance in node classification tasks on real-world datasets, demonstrating the benefits of Wasserstein pooling.", "conclusion": "WHGNN's distribution-based approach and Wasserstein pooling effectively model higher-order relationships, advancing hypergraph neural networks."}}
{"id": "2506.09745", "pdf": "https://arxiv.org/pdf/2506.09745", "abs": "https://arxiv.org/abs/2506.09745", "authors": ["Yangrui Zhu", "Junhua Bao", "Yipan Wei", "Yapeng Li", "Bo Du"], "title": "Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets", "categories": ["cs.CV"], "comment": null, "summary": "Existing multimodal methods typically assume that different modalities share\nthe same category set. However, in real-world applications, the category\ndistributions in multimodal data exhibit inconsistencies, which can hinder the\nmodel's ability to effectively utilize cross-modal information for recognizing\nall categories. In this work, we propose the practical setting termed\nMulti-Modal Heterogeneous Category-set Learning (MMHCL), where models are\ntrained in heterogeneous category sets of multi-modal data and aim to recognize\ncomplete classes set of all modalities during test. To effectively address this\ntask, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).\nSpecifically, CSCF aligns modality-specific features to a shared semantic space\nto enable knowledge transfer between seen and unseen classes. It then selects\nthe most discriminative modality for decision fusion through uncertainty\nestimation. Finally, it integrates cross-modal information based on class\nsimilarity, where the auxiliary modality refines the prediction of the dominant\none. Experimental results show that our method significantly outperforms\nexisting state-of-the-art (SOTA) approaches on multiple benchmark datasets,\neffectively addressing the MMHCL task.", "AI": {"tldr": "The paper introduces Multi-Modal Heterogeneous Category-set Learning (MMHCL) to address inconsistencies in category distributions across modalities. It proposes CSCF, a model that aligns features, selects discriminative modalities, and refines predictions using class similarity, outperforming SOTA methods.", "motivation": "Real-world multimodal data often has inconsistent category sets, hindering cross-modal information utilization for recognition.", "method": "Proposes CSCF: aligns modality features to a shared space, selects discriminative modalities via uncertainty estimation, and refines predictions using class similarity.", "result": "CSCF outperforms SOTA methods on benchmark datasets, effectively solving MMHCL.", "conclusion": "CSCF successfully addresses MMHCL by leveraging cross-modal information and class similarity, improving recognition across heterogeneous category sets."}}
{"id": "2506.09983", "pdf": "https://arxiv.org/pdf/2506.09983", "abs": "https://arxiv.org/abs/2506.09983", "authors": ["Hiroshi Matsuda", "Chunpeng Ma", "Masayuki Asahara"], "title": "Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs", "categories": ["cs.CL"], "comment": "9 pages, 2 figures, accepted for SyntaxFest 2025", "summary": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.", "AI": {"tldr": "A novel step-by-step instruction strategy for LLMs improves dependency parsing accuracy across 17 languages, with multilingual fine-tuning enhancing cross-language generalization.", "motivation": "Standard prompting in LLMs often fails to produce structurally valid outputs in dependency parsing, prompting the need for a more effective method.", "method": "Proposes a step-by-step instruction strategy: universal POS tagging first, followed by syntactic head and dependency label prediction, using a simplified CoNLL-U format.", "result": "Achieves state-of-the-art accuracy on Universal Dependencies datasets for 17 languages, with no hallucination or contamination. Multilingual fine-tuning boosts cross-language generalization.", "conclusion": "Explicit reasoning steps in LLM-based parsing are effective, offering a scalable, format-consistent alternative to bracket-based approaches."}}
{"id": "2506.09742", "pdf": "https://arxiv.org/pdf/2506.09742", "abs": "https://arxiv.org/abs/2506.09742", "authors": ["Gusseppe Bravo-Rocca", "Peini Liu", "Jordi Guitart", "Rodrigo M Carrillo-Larco", "Ajay Dholakia", "David Ellison"], "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at AAMAS 2025", "summary": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.", "AI": {"tldr": "A cognitive architecture for ML monitoring enhances interpretability by applying feature engineering principles to LLMs, improving decision-making.", "motivation": "Traditional ML monitoring outputs are verbose and lack interpretability, hindering effective decision-making.", "method": "Proposes a Decision Procedure module with three steps: Refactor (improves data representation), Break Down (decomposes complex info), and Compile (integrates insights). Reduces reliance on LLM-generated planning.", "result": "Experiments show higher accuracy and interpretability compared to baselines across domains.", "conclusion": "The approach provides robust, interpretable, and actionable insights for ML monitoring."}}
{"id": "2506.09714", "pdf": "https://arxiv.org/pdf/2506.09714", "abs": "https://arxiv.org/abs/2506.09714", "authors": ["Vaggelis Dorovatas", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "title": "Auto-Compressing Networks", "categories": ["cs.LG"], "comment": null, "summary": "Deep neural networks with short residual connections have demonstrated\nremarkable success across domains, but increasing depth often introduces\ncomputational redundancy without corresponding improvements in representation\nquality. In this work, we introduce Auto-Compressing Networks (ACNs), an\narchitectural variant where additive long feedforward connections from each\nlayer to the output replace traditional short residual connections. ACNs\nshowcase a unique property we coin as \"auto-compression\", the ability of a\nnetwork to organically compress information during training with gradient\ndescent, through architectural design alone. Through auto-compression,\ninformation is dynamically \"pushed\" into early layers during training,\nenhancing their representational quality and revealing potential redundancy in\ndeeper ones. We theoretically show that this property emerges from layer-wise\ntraining patterns present in ACNs, where layers are dynamically utilized during\ntraining based on task requirements. We also find that ACNs exhibit enhanced\nnoise robustness compared to residual networks, superior performance in\nlow-data settings, improved transfer learning capabilities, and mitigate\ncatastrophic forgetting suggesting that they learn representations that\ngeneralize better despite using fewer parameters. Our results demonstrate up to\n18% reduction in catastrophic forgetting and 30-80% architectural compression\nwhile maintaining accuracy across vision transformers, MLP-mixers, and BERT\narchitectures. Furthermore, we demonstrate that coupling ACNs with traditional\npruning techniques, enables significantly better sparsity-performance\ntrade-offs compared to conventional architectures. These findings establish\nACNs as a practical approach to developing efficient neural architectures that\nautomatically adapt their computational footprint to task complexity, while\nlearning robust representations.", "AI": {"tldr": "Auto-Compressing Networks (ACNs) replace short residual connections with long feedforward connections, enabling auto-compression for efficient training and improved representation quality.", "motivation": "Addressing computational redundancy in deep networks without sacrificing representation quality.", "method": "ACNs use additive long feedforward connections to dynamically compress information during training.", "result": "ACNs reduce catastrophic forgetting by 18%, achieve 30-80% compression, and improve noise robustness, low-data performance, and transfer learning.", "conclusion": "ACNs offer a practical, efficient architecture that adapts to task complexity and learns robust representations."}}
{"id": "2506.09748", "pdf": "https://arxiv.org/pdf/2506.09748", "abs": "https://arxiv.org/abs/2506.09748", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 6 figures", "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "AI": {"tldr": "A hierarchical cross-source image matching method for UAV absolute localization integrates semantic-aware coarse matching with fine-grained matching, achieving high accuracy without GNSS.", "motivation": "Absolute localization for UAVs is challenging without GNSS. Existing vision-based methods struggle with cross-source discrepancies and temporal variations.", "method": "Combines semantic-aware coarse matching (using vision foundation models) with lightweight fine-grained matching for pixel-level accuracy.", "result": "Superior accuracy and robustness demonstrated on benchmark datasets and a new CS-UAV dataset.", "conclusion": "The proposed method effectively overcomes limitations of traditional image matching for UAV localization in GNSS-denied scenarios."}}
{"id": "2506.09992", "pdf": "https://arxiv.org/pdf/2506.09992", "abs": "https://arxiv.org/abs/2506.09992", "authors": ["Amel Muminovic", "Amela Kadric Muminovic"], "title": "Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities.", "AI": {"tldr": "The study evaluates large language models for detecting toxic comments in Serbian, Croatian, and Bosnian, showing that context-augmented prompts improve performance, with Gemini 1.5 Pro achieving the best balance.", "motivation": "Online toxic language harms communities, especially in regions with limited moderation tools and labeled data.", "method": "Tested four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, Claude 3 Opus) in zero-shot and context-augmented modes on a manually labeled dataset of 4,500 YouTube/TikTok comments.", "result": "Context-augmented mode improved recall and F1 score, with Gemini 1.5 Pro achieving F1=0.82 and accuracy=0.82. GPT-4.1 had the highest precision and lowest false positives.", "conclusion": "Adding minimal context and optimizing prompts can enhance toxic language detection in low-resource Balkan languages."}}
{"id": "2506.09749", "pdf": "https://arxiv.org/pdf/2506.09749", "abs": "https://arxiv.org/abs/2506.09749", "authors": ["Shuo Jiang", "Min Xie", "Jianxi Luo"], "title": "Large Language Models for Design Structure Matrix Optimization", "categories": ["cs.CE", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization.", "AI": {"tldr": "The paper explores using Large Language Models (LLMs) to optimize Design Structure Matrix (DSM) element sequencing, a combinatorial optimization problem in engineering. The proposed LLM-based framework outperforms traditional methods by integrating contextual domain knowledge and network topology.", "motivation": "Traditional optimization methods struggle with large, intricate DSM problems due to lack of contextual understanding. LLMs offer advanced reasoning capabilities to address this gap.", "method": "A novel LLM-based framework combines network topology and domain knowledge for iterative DSM optimization.", "result": "The method achieves faster convergence and better solutions than stochastic and deterministic baselines, with domain knowledge significantly boosting performance.", "conclusion": "LLMs can effectively solve complex engineering optimization problems by blending semantic and mathematical reasoning, opening new avenues for LLM-based design optimization."}}
{"id": "2506.09738", "pdf": "https://arxiv.org/pdf/2506.09738", "abs": "https://arxiv.org/abs/2506.09738", "authors": ["Xin Wang", "Zeyang Zhang", "Linxin Xiao", "Haibo Chen", "Chendi Ge", "Wenwu Zhu"], "title": "Towards Multi-modal Graph Large Language Model", "categories": ["cs.LG"], "comment": null, "summary": "Multi-modal graphs, which integrate diverse multi-modal features and\nrelations, are ubiquitous in real-world applications. However, existing\nmulti-modal graph learning methods are typically trained from scratch for\nspecific graph data and tasks, failing to generalize across various multi-modal\ngraph data and tasks. To bridge this gap, we explore the potential of\nMulti-modal Graph Large Language Models (MG-LLM) to unify and generalize across\ndiverse multi-modal graph data and tasks. We propose a unified framework of\nmulti-modal graph data, task, and model, discovering the inherent\nmulti-granularity and multi-scale characteristics in multi-modal graphs.\nSpecifically, we present five key desired characteristics for MG-LLM: 1)\nunified space for multi-modal structures and attributes, 2) capability of\nhandling diverse multi-modal graph tasks, 3) multi-modal graph in-context\nlearning, 4) multi-modal graph interaction with natural language, and 5)\nmulti-modal graph reasoning. We then elaborate on the key challenges, review\nrelated works, and highlight promising future research directions towards\nrealizing these ambitious characteristics. Finally, we summarize existing\nmulti-modal graph datasets pertinent for model training. We believe this paper\ncan contribute to the ongoing advancement of the research towards MG-LLM for\ngeneralization across multi-modal graph data and tasks.", "AI": {"tldr": "The paper explores Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across diverse multi-modal graph data and tasks, proposing a framework with five key characteristics.", "motivation": "Existing multi-modal graph learning methods lack generalization across diverse data and tasks, prompting the need for a unified approach like MG-LLM.", "method": "The paper proposes a unified framework for multi-modal graph data, tasks, and models, focusing on multi-granularity and multi-scale characteristics. It outlines five desired traits for MG-LLM.", "result": "The paper identifies key challenges, reviews related works, and highlights future research directions for MG-LLM. It also summarizes relevant datasets.", "conclusion": "The paper aims to advance research on MG-LLM for generalization across multi-modal graph data and tasks."}}
{"id": "2506.09777", "pdf": "https://arxiv.org/pdf/2506.09777", "abs": "https://arxiv.org/abs/2506.09777", "authors": ["Anton Razzhigaev", "Matvey Mikhalchuk", "Klim Kireev", "Igor Udovichenko", "Andrey Kuznetsov", "Aleksandr Petiushko"], "title": "Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing facial images from black-box recognition models poses a\nsignificant privacy threat. While many methods require access to embeddings, we\naddress the more challenging scenario of model inversion using only similarity\nscores. This paper introduces DarkerBB, a novel approach that reconstructs\ncolor faces by performing zero-order optimization within a PCA-derived\neigenface space. Despite this highly limited information, experiments on LFW,\nAgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves\nstate-of-the-art verification accuracies in the similarity-only setting, with\ncompetitive query efficiency.", "AI": {"tldr": "DarkerBB reconstructs facial images from black-box models using only similarity scores, achieving state-of-the-art results.", "motivation": "Addressing the privacy threat of facial image reconstruction from black-box models, especially when only similarity scores are available.", "method": "Zero-order optimization within a PCA-derived eigenface space.", "result": "Achieves state-of-the-art verification accuracies on LFW, AgeDB-30, and CFP-FP benchmarks with competitive query efficiency.", "conclusion": "DarkerBB is effective for model inversion using only similarity scores, posing a significant privacy concern."}}
{"id": "2506.09996", "pdf": "https://arxiv.org/pdf/2506.09996", "abs": "https://arxiv.org/abs/2506.09996", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring", "categories": ["cs.CL", "cs.CY"], "comment": "22 pages, 7 figures, and 9 tables", "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.", "AI": {"tldr": "The paper introduces a streaming content monitor (SCM) for partial detection of harmful content in LLM outputs, reducing latency while maintaining high accuracy.", "motivation": "Existing moderation methods for LLMs either cause high latency (full detection) or suffer from performance gaps (partial detection with full-detection-trained moderators).", "method": "Constructs FineHarm dataset with fine-grained annotations and proposes SCM, trained with dual supervision for token-level harmfulness judgment.", "result": "SCM achieves 0.95+ macro F1 score by viewing only 18% of tokens, comparable to full detection, and improves safety alignment.", "conclusion": "SCM effectively balances latency and accuracy in moderation and enhances LLM safety alignment."}}
{"id": "2506.09755", "pdf": "https://arxiv.org/pdf/2506.09755", "abs": "https://arxiv.org/abs/2506.09755", "authors": ["Shuo Jiang", "Min Xie", "Frank Youhua Chen", "Jian Ma", "Jianxi Luo"], "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era", "categories": ["cs.CE", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.", "AI": {"tldr": "The paper introduces Intelligent Design 4.0 (ID 4.0), an AI-driven paradigm for engineering design, leveraging multi-agent systems and foundation models to automate and enhance design processes.", "motivation": "To explore how AI, especially foundation models and multi-agent systems, can transform engineering design by improving innovation, efficiency, and automation.", "method": "The paper reviews the historical evolution of Intelligent Design, proposes a conceptual framework for ID 4.0, and discusses its potential for end-to-end automation.", "result": "ID 4.0 offers a framework for autonomous, adaptive, and efficient engineering design, with future potential in complex scenarios and human-aligned goal-setting.", "conclusion": "ID 4.0 represents a significant advancement in engineering design, with promising directions for further research and practical implementation."}}
{"id": "2506.09769", "pdf": "https://arxiv.org/pdf/2506.09769", "abs": "https://arxiv.org/abs/2506.09769", "authors": ["Haruki Kainuma", "Takayuki Nishio"], "title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "summary": "This paper proposes Load-aware Tram-FL, an extension of Tram-FL that\nintroduces a training scheduling mechanism to minimize total training time in\ndecentralized federated learning by accounting for both computational and\ncommunication loads. The scheduling problem is formulated as a global\noptimization task, which-though intractable in its original form-is made\nsolvable by decomposing it into node-wise subproblems. To promote balanced data\nutilization under non-IID distributions, a variance constraint is introduced,\nwhile the overall training latency, including both computation and\ncommunication costs, is minimized through the objective function. Simulation\nresults on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly\nreduces training time and accelerates convergence compared to baseline methods.", "AI": {"tldr": "Load-aware Tram-FL extends Tram-FL with a scheduling mechanism to minimize training time in decentralized federated learning by optimizing computational and communication loads.", "motivation": "To address the challenge of minimizing total training time in decentralized federated learning while balancing computational and communication loads.", "method": "Formulates the scheduling problem as a global optimization task, decomposes it into node-wise subproblems, and introduces a variance constraint for balanced data utilization under non-IID distributions.", "result": "Simulations on MNIST and CIFAR-10 show significant reductions in training time and faster convergence compared to baselines.", "conclusion": "Load-aware Tram-FL effectively optimizes training scheduling, reducing latency and improving performance in decentralized federated learning."}}
{"id": "2506.09782", "pdf": "https://arxiv.org/pdf/2506.09782", "abs": "https://arxiv.org/abs/2506.09782", "authors": ["Nicola Farronato", "Florian Scheidegger", "Mattia Rigotti", "Cristiano Malossi", "Michele Magno", "Haotong Qin"], "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.", "AI": {"tldr": "Q-SAM2 introduces a low-bit quantization method for SAM2, improving efficiency while maintaining accuracy through calibration and QAT.", "motivation": "SAM2's high computational and memory costs limit its use in resource-constrained scenarios.", "method": "Q-SAM2 uses linear layer calibration for initialization and a QAT pipeline with clipping to adapt to quantization.", "result": "Q-SAM2 achieves high accuracy and efficiency, outperforming existing methods, especially in 2-bit quantization.", "conclusion": "Q-SAM2 is effective for both quantization-aware training and post-training quantization, with significant accuracy improvements."}}
{"id": "2506.09148", "pdf": "https://arxiv.org/pdf/2506.09148", "abs": "https://arxiv.org/abs/2506.09148", "authors": ["Hetvi Waghela", "Jaydip Sen", "Sneha Rakshit", "Subhasis Dasgupta"], "title": "Adversarial Text Generation with Dynamic Contextual Perturbation", "categories": ["cs.CR", "cs.CL"], "comment": "This is the accepted version of the paper, which was presented at\n  IEEE CALCON. The conference was organized at Jadavpur University, Kolkata,\n  from December 14 to 15, 2025. The paper is six pages long, and it consists of\n  six tables and six figures. This is not the final camera-ready version of the\n  paper", "summary": "Adversarial attacks on Natural Language Processing (NLP) models expose\nvulnerabilities by introducing subtle perturbations to input text, often\nleading to misclassification while maintaining human readability. Existing\nmethods typically focus on word-level or local text segment alterations,\noverlooking the broader context, which results in detectable or semantically\ninconsistent perturbations. We propose a novel adversarial text attack scheme\nnamed Dynamic Contextual Perturbation (DCP). DCP dynamically generates\ncontext-aware perturbations across sentences, paragraphs, and documents,\nensuring semantic fidelity and fluency. Leveraging the capabilities of\npre-trained language models, DCP iteratively refines perturbations through an\nadversarial objective function that balances the dual objectives of inducing\nmodel misclassification and preserving the naturalness of the text. This\ncomprehensive approach allows DCP to produce more sophisticated and effective\nadversarial examples that better mimic natural language patterns. Our\nexperimental results, conducted on various NLP models and datasets, demonstrate\nthe efficacy of DCP in challenging the robustness of state-of-the-art NLP\nsystems. By integrating dynamic contextual analysis, DCP significantly enhances\nthe subtlety and impact of adversarial attacks. This study highlights the\ncritical role of context in adversarial attacks and lays the groundwork for\ncreating more robust NLP systems capable of withstanding sophisticated\nadversarial strategies.", "AI": {"tldr": "Proposes Dynamic Contextual Perturbation (DCP), a novel adversarial text attack method that generates context-aware perturbations for NLP models, ensuring semantic fidelity and fluency while challenging model robustness.", "motivation": "Existing adversarial attack methods focus on local text alterations, often resulting in detectable or inconsistent perturbations. DCP addresses this by considering broader context.", "method": "DCP dynamically generates context-aware perturbations across sentences, paragraphs, and documents using pre-trained language models, refining them iteratively with an adversarial objective function.", "result": "DCP produces sophisticated adversarial examples that mimic natural language, effectively challenging state-of-the-art NLP models in experiments.", "conclusion": "DCP highlights the importance of context in adversarial attacks and advances the development of more robust NLP systems."}}
{"id": "2506.09785", "pdf": "https://arxiv.org/pdf/2506.09785", "abs": "https://arxiv.org/abs/2506.09785", "authors": ["Alexander Marusov", "Alexander Yuhay", "Alexey Zaytsev"], "title": "A theoretical framework for self-supervised contrastive learning for continuous dependent data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.", "AI": {"tldr": "A novel contrastive SSL framework for dependent data, outperforming TS2Vec on benchmarks with accuracy improvements of 4.17% and 2.08%, and 7% higher ROC-AUC on drought classification.", "motivation": "Traditional SSL methods assume semantic independence between samples, which fails for dependent data like temporal and spatio-temporal domains.", "method": "Proposes a theoretical framework with hard and soft closeness measures, deriving dependency-aware loss functions for SSL.", "result": "Outperforms TS2Vec on UEA and UCR benchmarks (4.17% and 2.08% accuracy gains) and achieves 7% higher ROC-AUC on drought classification.", "conclusion": "The proposed framework effectively captures spatio-temporal dependencies, demonstrating superior performance on dependent data tasks."}}
{"id": "2506.09781", "pdf": "https://arxiv.org/pdf/2506.09781", "abs": "https://arxiv.org/abs/2506.09781", "authors": ["Chungpa Lee", "Sehee Lim", "Kibok Lee", "Jy-yong Sohn"], "title": "On the Similarities of Embeddings in Contrastive Learning", "categories": ["cs.LG", "stat.ML"], "comment": "contrastive learning, representation learning, embedding, similarity,\n  negative pair, positive pair", "summary": "Contrastive learning (CL) operates on a simple yet effective principle:\nembeddings of positive pairs are pulled together, while those of negative pairs\nare pushed apart. Although various forms of contrastive loss have been proposed\nand analyzed from different perspectives, prior works lack a comprehensive\nframework that systematically explains a broad class of these objectives. In\nthis paper, we present a unified framework for understanding CL, which is based\non analyzing the cosine similarity between embeddings of positive and negative\npairs. In full-batch settings, we show that perfect alignment of positive pairs\nis unattainable when similarities of negative pairs fall below a certain\nthreshold, and that this misalignment can be alleviated by incorporating\nwithin-view negative pairs. In mini-batch settings, we demonstrate that smaller\nbatch sizes incur stronger separation among negative pairs within batches,\nwhich leads to higher variance in similarities of negative pairs. To address\nthis limitation of mini-batch CL, we introduce an auxiliary loss term that\nreduces the variance of similarities of negative pairs in CL. Empirical results\ndemonstrate that incorporating the proposed loss consistently improves the\nperformance of CL methods in small-batch training.", "AI": {"tldr": "A unified framework for contrastive learning (CL) explains various objectives, highlighting challenges in full-batch and mini-batch settings and proposing solutions.", "motivation": "Prior works lack a comprehensive framework for understanding contrastive learning objectives.", "method": "Analyzes cosine similarity between embeddings of positive and negative pairs, introduces an auxiliary loss for mini-batch CL.", "result": "Shows misalignment in full-batch settings and variance in mini-batch settings; proposed loss improves small-batch performance.", "conclusion": "The framework and auxiliary loss enhance CL performance, especially in small-batch training."}}
{"id": "2506.09784", "pdf": "https://arxiv.org/pdf/2506.09784", "abs": "https://arxiv.org/abs/2506.09784", "authors": ["Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Accurate and efficient zero-shot 6D pose estimation with frozen foundation models", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.", "AI": {"tldr": "FreeZeV2 is a training-free method for 6D pose estimation of unseen objects, improving accuracy and efficiency over its predecessor through sparse feature extraction, feature-aware scoring, and modular design.", "motivation": "Addressing the challenge of generalizing 6D pose estimation to novel objects without task-specific training, which demands significant computational resources.", "method": "Leverages pre-trained geometric and vision foundation models, introduces sparse feature extraction, feature-aware scoring, and a modular design supporting segmentation model ensembles.", "result": "Achieves state-of-the-art performance on the BOP Benchmark, with 8x speedup and 5% accuracy improvement over FreeZe, and additional 8% accuracy gain with segmentation ensembles.", "conclusion": "FreeZeV2 demonstrates that task-specific training is unnecessary for accurate and efficient 6D pose estimation, setting a new benchmark in the field."}}
{"id": "2506.09260", "pdf": "https://arxiv.org/pdf/2506.09260", "abs": "https://arxiv.org/abs/2506.09260", "authors": ["Yibin Lei", "Tao Shen", "Andrew Yates"], "title": "ThinkQE: Query Expansion via an Evolving Thinking Process", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Effective query expansion for web search benefits from promoting both\nexploration and result diversity to capture multiple interpretations and facets\nof a query. While recent LLM-based methods have improved retrieval performance\nand demonstrate strong domain generalization without additional training, they\noften generate narrowly focused expansions that overlook these desiderata. We\npropose ThinkQE, a test-time query expansion framework addressing this\nlimitation through two key components: a thinking-based expansion process that\nencourages deeper and comprehensive semantic exploration, and a\ncorpus-interaction strategy that iteratively refines expansions using retrieval\nfeedback from the corpus. Experiments on diverse web search benchmarks (DL19,\nDL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,\nincluding training-intensive dense retrievers and rerankers.", "AI": {"tldr": "ThinkQE improves query expansion for web search by promoting exploration and diversity, outperforming existing methods.", "motivation": "Current LLM-based query expansion methods often produce narrowly focused results, missing the need for diverse interpretations and facets of a query.", "method": "ThinkQE uses a thinking-based expansion process and corpus-interaction strategy to refine expansions iteratively with retrieval feedback.", "result": "ThinkQE outperforms prior methods on benchmarks (DL19, DL20, BRIGHT), including training-intensive models.", "conclusion": "ThinkQE effectively addresses the limitations of existing query expansion methods by enhancing exploration and diversity."}}
{"id": "2506.09822", "pdf": "https://arxiv.org/pdf/2506.09822", "abs": "https://arxiv.org/abs/2506.09822", "authors": ["Rebecca Loubet", "Pascal Zittlau", "Marco Hoffmann", "Luisa Vollmer", "Sophie Fellenz", "Heike Leitte", "Fabian Jirasek", "Johannes Lenhard", "Hans Hasse"], "title": "Superstudent intelligence in thermodynamics", "categories": ["cs.CE", "cs.AI"], "comment": "This document is the unedited Author's version of a yet to be\n  Submitted Work to Physical Review Physics Education Research. 15 pages, 2\n  figures, Graphical Abstract, Highlights and SI available (12 pages)", "summary": "In this short note, we report and analyze a striking event: OpenAI's large\nlanguage model o3 has outwitted all students in a university exam on\nthermodynamics. The thermodynamics exam is a difficult hurdle for most\nstudents, where they must show that they have mastered the fundamentals of this\nimportant topic. Consequently, the failure rates are very high, A-grades are\nrare - and they are considered proof of the students' exceptional intellectual\nabilities. This is because pattern learning does not help in the exam. The\nproblems can only be solved by knowledgeably and creatively combining\nprinciples of thermodynamics. We have given our latest thermodynamics exam not\nonly to the students but also to OpenAI's most powerful reasoning model, o3,\nand have assessed the answers of o3 exactly the same way as those of the\nstudents. In zero-shot mode, the model o3 solved all problems correctly, better\nthan all students who took the exam; its overall score was in the range of the\nbest scores we have seen in more than 10,000 similar exams since 1985. This is\na turning point: machines now excel in complex tasks, usually taken as proof of\nhuman intellectual capabilities. We discuss the consequences this has for the\nwork of engineers and the education of future engineers.", "AI": {"tldr": "OpenAI's model o3 outperformed all students in a thermodynamics exam, solving all problems correctly in zero-shot mode, marking a turning point in machine capabilities.", "motivation": "To evaluate the performance of AI in complex tasks traditionally seen as proof of human intellect, specifically in thermodynamics exams.", "method": "Administered the same thermodynamics exam to students and OpenAI's o3 model, assessing answers identically.", "result": "o3 solved all problems correctly, scoring better than all students and matching the best historical scores.", "conclusion": "This demonstrates AI's ability to excel in tasks requiring deep understanding and creativity, raising implications for engineering education and practice."}}
{"id": "2506.09803", "pdf": "https://arxiv.org/pdf/2506.09803", "abs": "https://arxiv.org/abs/2506.09803", "authors": ["Longzhu He", "Chaozhuo Li", "Peng Tang", "Litian Zhang", "Sen Su"], "title": "Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Graph neural networks (GNNs) have achieved significant success in graph\nrepresentation learning and have been applied to various domains. However, many\nreal-world graphs contain sensitive personal information, such as user profiles\nin social networks, raising serious privacy concerns when graph learning is\nperformed using GNNs. To address this issue, locally private graph learning\nprotocols have gained considerable attention. These protocols leverage the\nprivacy advantages of local differential privacy (LDP) and the effectiveness of\nGNN's message-passing in calibrating noisy data, offering strict privacy\nguarantees for users' local data while maintaining high utility (e.g., node\nclassification accuracy) for graph learning. Despite these advantages, such\nprotocols may be vulnerable to data poisoning attacks, a threat that has not\nbeen considered in previous research. Identifying and addressing these threats\nis crucial for ensuring the robustness and security of privacy-preserving graph\nlearning frameworks. This work introduces the first data poisoning attack\ntargeting locally private graph learning protocols. The attacker injects fake\nusers into the protocol, manipulates these fake users to establish links with\ngenuine users, and sends carefully crafted data to the server, ultimately\ncompromising the utility of private graph learning. The effectiveness of the\nattack is demonstrated both theoretically and empirically. In addition, several\ndefense strategies have also been explored, but their limited effectiveness\nhighlights the need for more robust defenses.", "AI": {"tldr": "The paper introduces a data poisoning attack on locally private graph learning protocols, demonstrating its effectiveness and exploring limited defense strategies.", "motivation": "Privacy concerns in graph neural networks (GNNs) due to sensitive data, and the overlooked threat of data poisoning in locally private protocols.", "method": "The attacker injects fake users, manipulates links, and sends crafted data to compromise the protocol's utility.", "result": "The attack is proven effective theoretically and empirically, with defenses found inadequate.", "conclusion": "Highlights the need for stronger defenses against data poisoning in privacy-preserving graph learning."}}
{"id": "2506.09814", "pdf": "https://arxiv.org/pdf/2506.09814", "abs": "https://arxiv.org/abs/2506.09814", "authors": ["Xiandong Zou", "Ruihao Xia", "Hongsong Wang", "Pan Zhou"], "title": "DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision", "categories": ["cs.CV"], "comment": null, "summary": "While text-to-3D generation has attracted growing interest, existing methods\noften struggle to produce 3D assets that align well with human preferences.\nCurrent preference alignment techniques for 3D content typically rely on\nhardly-collected preference-paired multi-view 2D images to train 2D reward\nmodels, when then guide 3D generation -- leading to geometric artifacts due to\ntheir inherent 2D bias. To address these limitations, we construct 3D-MeshPref,\nthe first large-scale unpaired 3D preference dataset, featuring diverse 3D\nmeshes annotated by a large language model and refined by human evaluators. We\nthen develop RewardCS, the first reward model trained directly on unpaired\n3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling\neffective learning of human-aligned 3D geometric preferences without requiring\npaired comparisons. Building on this, we propose DreamCS, a unified framework\nthat integrates RewardCS into text-to-3D pipelines -- enhancing both implicit\nand explicit 3D generation with human preference feedback. Extensive\nexperiments show DreamCS outperforms prior methods, producing 3D assets that\nare both geometrically faithful and human-preferred. Code and models will be\nreleased publicly.", "AI": {"tldr": "The paper introduces 3D-MeshPref, a large-scale unpaired 3D preference dataset, and RewardCS, a reward model trained on it. DreamCS, their framework, integrates RewardCS into text-to-3D pipelines, improving alignment with human preferences and reducing geometric artifacts.", "motivation": "Existing text-to-3D methods struggle with human preference alignment due to reliance on 2D reward models, causing geometric artifacts. The authors aim to address this by directly learning 3D preferences.", "method": "They create 3D-MeshPref (a 3D preference dataset) and train RewardCS using a Cauchy-Schwarz divergence objective. DreamCS integrates RewardCS into text-to-3D pipelines.", "result": "DreamCS outperforms prior methods, producing geometrically faithful and human-preferred 3D assets.", "conclusion": "The proposed approach effectively aligns 3D generation with human preferences, overcoming limitations of 2D-based methods."}}
{"id": "2506.09289", "pdf": "https://arxiv.org/pdf/2506.09289", "abs": "https://arxiv.org/abs/2506.09289", "authors": ["Boxi Yu", "Yuxuan Zhu", "Pinjia He", "Daniel Kang"], "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench", "categories": ["cs.SE", "cs.CL", "D.0; I.2"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has spurred the development of\ncoding agents for real-world code generation. As a widely used benchmark for\nevaluating the code generation capabilities of these agents, SWE-Bench uses\nreal-world problems based on GitHub issues and their corresponding pull\nrequests. However, the manually written test cases included in these pull\nrequests are often insufficient, allowing generated patches to pass the tests\nwithout resolving the underlying issue. To address this challenge, we introduce\nUTGenerator, an LLM-driven test case generator that automatically analyzes\ncodebases and dependencies to generate test cases for real-world Python\nprojects. Building on UTGenerator, we propose UTBoost, a comprehensive\nframework for test case augmentation. In our evaluation, we identified 36 task\ninstances with insufficient test cases and uncovered 345 erroneous patches\nincorrectly labeled as passed in the original SWE Bench. These corrections,\nimpacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard\nentries, yield 18 and 11 ranking changes, respectively.", "AI": {"tldr": "UTGenerator and UTBoost address insufficient test cases in SWE-Bench by generating and augmenting test cases, uncovering 345 erroneous patches and improving rankings.", "motivation": "Manually written test cases in SWE-Bench are often inadequate, allowing incorrect patches to pass.", "method": "Introduces UTGenerator (LLM-driven test case generator) and UTBoost (test case augmentation framework).", "result": "Identified 36 task instances with insufficient tests, uncovered 345 erroneous patches, and improved leaderboard rankings.", "conclusion": "UTGenerator and UTBoost enhance code evaluation by addressing test case limitations in real-world projects."}}
{"id": "2506.09836", "pdf": "https://arxiv.org/pdf/2506.09836", "abs": "https://arxiv.org/abs/2506.09836", "authors": ["Junli Deng", "Ping Shi", "Qipei Li", "Jinyang Guo"], "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing intricate, ever-changing environments remains a central\nambition in computer vision, yet existing solutions often crumble before the\ncomplexity of real-world dynamics. We present DynaSplat, an approach that\nextends Gaussian Splatting to dynamic scenes by integrating dynamic-static\nseparation and hierarchical motion modeling. First, we classify scene elements\nas static or dynamic through a novel fusion of deformation offset statistics\nand 2D motion flow consistency, refining our spatial representation to focus\nprecisely where motion matters. We then introduce a hierarchical motion\nmodeling strategy that captures both coarse global transformations and\nfine-grained local movements, enabling accurate handling of intricate,\nnon-rigid motions. Finally, we integrate physically-based opacity estimation to\nensure visually coherent reconstructions, even under challenging occlusions and\nperspective shifts. Extensive experiments on challenging datasets reveal that\nDynaSplat not only surpasses state-of-the-art alternatives in accuracy and\nrealism but also provides a more intuitive, compact, and efficient route to\ndynamic scene reconstruction.", "AI": {"tldr": "DynaSplat enhances Gaussian Splatting for dynamic scenes with dynamic-static separation, hierarchical motion modeling, and opacity estimation, outperforming existing methods in accuracy and realism.", "motivation": "Existing methods struggle with real-world dynamic scene complexity, prompting the need for a more robust solution.", "method": "Classifies static/dynamic elements using deformation offset statistics and 2D motion flow, employs hierarchical motion modeling, and integrates opacity estimation.", "result": "Outperforms state-of-the-art methods in accuracy, realism, and efficiency on challenging datasets.", "conclusion": "DynaSplat offers a superior, intuitive, and compact approach to dynamic scene reconstruction."}}
{"id": "2506.09810", "pdf": "https://arxiv.org/pdf/2506.09810", "abs": "https://arxiv.org/abs/2506.09810", "authors": ["Minoh Jeong", "Alfred Hero"], "title": "Generalizing Supervised Contrastive learning: A Projection Perspective", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "Self-supervised contrastive learning (SSCL) has emerged as a powerful\nparadigm for representation learning and has been studied from multiple\nperspectives, including mutual information and geometric viewpoints. However,\nsupervised contrastive (SupCon) approaches have received comparatively little\nattention in this context: for instance, while InfoNCE used in SSCL is known to\nform a lower bound on mutual information (MI), the relationship between SupCon\nand MI remains unexplored. To address this gap, we introduce ProjNCE, a\ngeneralization of the InfoNCE loss that unifies supervised and self-supervised\ncontrastive objectives by incorporating projection functions and an adjustment\nterm for negative pairs. We prove that ProjNCE constitutes a valid MI bound and\naffords greater flexibility in selecting projection strategies for class\nembeddings. Building on this flexibility, we further explore the centroid-based\nclass embeddings in SupCon by exploring a variety of projection methods.\nExtensive experiments on multiple datasets and settings demonstrate that\nProjNCE consistently outperforms both SupCon and standard cross-entropy\ntraining. Our work thus refines SupCon along two complementary\nperspective--mutual information interpretation and projection design--and\noffers broadly applicable improvements whenever SupCon serves as the\nfoundational contrastive objective.", "AI": {"tldr": "ProjNCE, a generalization of InfoNCE, unifies supervised and self-supervised contrastive learning, offering a mutual information bound and flexible projection methods, outperforming existing approaches.", "motivation": "The gap in understanding the relationship between supervised contrastive learning (SupCon) and mutual information (MI) motivates the introduction of ProjNCE.", "method": "ProjNCE extends InfoNCE with projection functions and an adjustment term for negative pairs, enabling flexible class embedding strategies.", "result": "ProjNCE consistently outperforms SupCon and cross-entropy training across datasets and settings.", "conclusion": "ProjNCE refines SupCon by providing MI interpretation and projection design improvements, enhancing its foundational contrastive learning role."}}
{"id": "2506.09834", "pdf": "https://arxiv.org/pdf/2506.09834", "abs": "https://arxiv.org/abs/2506.09834", "authors": ["Chuang Maa", "Yu Peia", "Jianhang Zhanga", "Shaokai Zhaoa", "Bowen Jib", "Liang Xiea", "Ye Yana", "Erwei Yin"], "title": "MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an\nindividual's genuine emotional state. Their analysis has attracted considerable\ninterest due to its promising applications in fields such as healthcare,\ncriminal investigation, and human-computer interaction. However, existing ME\nresearch is limited to single visual modality, overlooking the rich emotional\ninformation conveyed by other physiological modalities, resulting in ME\nrecognition and spotting performance far below practical application needs.\nTherefore, exploring the cross-modal association mechanism between ME visual\nfeatures and physiological signals (PS), and developing a multimodal fusion\nframework, represents a pivotal step toward advancing ME analysis. This study\nintroduces a novel ME dataset, MMME, which, for the first time, enables\nsynchronized collection of facial action signals (MEs), central nervous system\nsignals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming\nthe constraints of existing ME corpora, MMME comprises 634 MEs, 2,841\nmacro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,\nestablishing a robust foundation for investigating ME neural mechanisms and\nconducting multimodal fusion-based analyses. Extensive experiments validate the\ndataset's reliability and provide benchmarks for ME analysis, demonstrating\nthat integrating MEs with PS significantly enhances recognition and spotting\nperformance. To the best of our knowledge, MMME is the most comprehensive ME\ndataset to date in terms of modality diversity. It provides critical data\nsupport for exploring the neural mechanisms of MEs and uncovering the\nvisual-physiological synergistic effects, driving a paradigm shift in ME\nresearch from single-modality visual analysis to multimodal fusion. The dataset\nwill be publicly available upon acceptance of this paper.", "AI": {"tldr": "The paper introduces MMME, a novel multimodal dataset for micro-expression (ME) analysis, combining visual and physiological signals to improve recognition and spotting performance.", "motivation": "Existing ME research relies on single visual modality, missing emotional insights from physiological signals. A multimodal approach is needed for practical applications.", "method": "The study develops the MMME dataset, synchronizing facial action signals (MEs), EEG, and peripheral physiological signals (PPG, RSP, SKT, EDA, ECG). It includes 634 MEs, 2,841 macro-expressions, and 2,890 multimodal trials.", "result": "Experiments show integrating MEs with physiological signals significantly enhances recognition and spotting performance. MMME is the most comprehensive ME dataset in modality diversity.", "conclusion": "MMME enables exploration of ME neural mechanisms and visual-physiological synergy, shifting ME research from single-modality to multimodal fusion."}}
{"id": "2506.09839", "pdf": "https://arxiv.org/pdf/2506.09839", "abs": "https://arxiv.org/abs/2506.09839", "authors": ["Chen Gao", "Liankai Jin", "Xingyu Peng", "Jiazhao Zhang", "Yue Deng", "Annan Li", "He Wang", "Si Liu"], "title": "OctoNav: Towards Generalist Embodied Navigation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "31 pages, 25 figures", "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit\nof embodied AI. However, previous navigation research is divided into different\ntasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task\nobjectives and modalities, making datasets and methods are designed\nindividually. In this work, we take steps toward generalist navigation agents,\nwhich can follow free-form instructions that include arbitrary compounds of\nmulti-modal and multi-capability. To achieve this, we propose a large-scale\nbenchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.\nSpecifically, OctoNav-Bench features continuous environments and is constructed\nvia a designed annotation pipeline. We thoroughly craft instruction-trajectory\npairs, where instructions are diverse in free-form with arbitrary modality and\ncapability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within\nOctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,\nwe build it upon MLLMs and adapt it to a VLA-type model, which can produce\nlow-level actions solely based on 2D visual observations. Moreover, we design a\nHybrid Training Paradigm (HTP) that consists of three stages, i.e.,\nAction-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains\nspecifically designed learning policies and rewards. Importantly, for TBA-SFT\nand Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which\nshow impressive reasoning ability via thinking-before-answer. Thus, we aim to\ninvestigate how to achieve thinking-before-action in the embodied navigation\nfield, to improve model's reasoning ability toward generalists. Specifically,\nwe propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a\ncold-start phrase and then leverage Nav-GPRO to improve its thinking ability.\nFinally, OctoNav-R1 shows superior performance compared with previous methods.", "AI": {"tldr": "The paper introduces OctoNav-Bench and OctoNav-R1, aiming to develop generalist navigation agents capable of following free-form, multi-modal instructions. It includes a large-scale benchmark and a method combining MLLMs with a hybrid training paradigm.", "motivation": "To address the fragmentation in navigation research by creating a unified approach for generalist agents that handle diverse instructions and modalities.", "method": "Proposes OctoNav-Bench (a benchmark with diverse instruction-trajectory pairs) and OctoNav-R1 (a VLA-type model trained via a Hybrid Training Paradigm, including TBA-SFT and Nav-GPRO stages).", "result": "OctoNav-R1 outperforms previous methods, demonstrating superior performance in embodied navigation.", "conclusion": "The work advances generalist navigation agents by integrating multi-modal capabilities and reasoning, validated by the success of OctoNav-R1."}}
{"id": "2506.09813", "pdf": "https://arxiv.org/pdf/2506.09813", "abs": "https://arxiv.org/abs/2506.09813", "authors": ["Ariel Procaccia", "Benjamin Schiffer", "Serena Wang", "Shirley Zhang"], "title": "Metritocracy: Representative Metrics for Lite Benchmarks", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.", "AI": {"tldr": "The paper formalizes two notions of representation for selecting subsets of evaluation metrics using social choice theory, proving bounds on required metrics and applying them to real-world cases.", "motivation": "Addressing the unclear definition of 'representative' in subset selection of evaluation metrics for efficiency or interpretability.", "method": "Introduces positional representation and positional proportionality, proving bounds on metrics needed and generalizing these properties.", "result": "Upper and lower bounds on metrics for representation properties, with case studies on LLM and hospital quality evaluation.", "conclusion": "The formalized properties provide practical frameworks for representative subset selection in evaluation metrics."}}
{"id": "2506.09846", "pdf": "https://arxiv.org/pdf/2506.09846", "abs": "https://arxiv.org/abs/2506.09846", "authors": ["Panagiotis Kaliosis", "John Pavlopoulos"], "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 10 figures, Under Review", "summary": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.", "AI": {"tldr": "Proposes a novel loss function using Wasserstein distance to align character frequency distributions, improving handwritten text recognition accuracy and robustness.", "motivation": "Handwritten text recognition is challenging due to evolving and context-dependent handwriting, causing models to underperform on specific subsets.", "method": "Introduces a loss function incorporating Wasserstein distance to penalize divergence from target character frequency distributions, and integrates it into guided decoding.", "result": "Enhances accuracy and robustness under intra-dataset shifts, improving generalization and performance across datasets and architectures.", "conclusion": "The method effectively addresses challenges in handwritten text recognition and can improve existing models without retraining."}}
{"id": "2506.09851", "pdf": "https://arxiv.org/pdf/2506.09851", "abs": "https://arxiv.org/abs/2506.09851", "authors": ["Md. Yeasin Rahat", "Rajan Das Gupta", "Nur Raisa Rahman", "Sudipto Roy Pritom", "Samiur Rahman Shakir", "Md Imrul Hasan Showmick", "Md. Jakir Hossen"], "title": "Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets", "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "comment": "Accepted in MECON 2025", "summary": "The prediction of foreign exchange rates, such as the US Dollar (USD) to\nBangladeshi Taka (BDT), plays a pivotal role in global financial markets,\ninfluencing trade, investments, and economic stability. This study leverages\nhistorical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo\nFinance, to develop advanced machine learning models for accurate forecasting.\nA Long Short-Term Memory (LSTM) neural network is employed, achieving an\nexceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and\na test loss of 0.8523, significantly outperforming traditional methods like\nARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is\napplied for directional prediction, with backtesting on a $10,000 initial\ncapital revealing a 40.82% profitable trade rate, though resulting in a net\nloss of $20,653.25 over 49 trades. The study analyzes historical trends,\nshowing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates\nnormalized daily returns to capture volatility. These findings highlight the\npotential of deep learning in forex forecasting, offering traders and\npolicymakers robust tools to mitigate risks. Future work could integrate\nsentiment analysis and real-time economic indicators to further enhance model\nadaptability in volatile markets.", "AI": {"tldr": "The study uses LSTM and GBC models to forecast USD/BDT exchange rates, achieving high accuracy but mixed profitability in trading.", "motivation": "Accurate forex prediction is crucial for trade, investments, and economic stability.", "method": "LSTM neural network and Gradient Boosting Classifier (GBC) applied to historical USD/BDT data (2018-2023).", "result": "LSTM achieved 99.449% accuracy (RMSE 0.9858), outperforming ARIMA. GBC showed 40.82% profitable trades but net loss.", "conclusion": "Deep learning shows promise in forex forecasting; future work could integrate sentiment analysis and real-time data."}}
{"id": "2506.09862", "pdf": "https://arxiv.org/pdf/2506.09862", "abs": "https://arxiv.org/abs/2506.09862", "authors": ["Mikel Casals", "Vasilis Belis", "Elias F. Combarro", "Eduard Alarc\u00f3n", "Sofia Vallecorsa", "Michele Grossi"], "title": "Guided Graph Compression for Quantum Graph Neural Networks", "categories": ["cs.LG", "cs.AI", "hep-ex", "quant-ph"], "comment": null, "summary": "Graph Neural Networks (GNNs) are effective for processing graph-structured\ndata but face challenges with large graphs due to high memory requirements and\ninefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a\npromising avenue to address these issues and inspires new algorithmic\napproaches. In particular, Quantum Graph Neural Networks (QGNNs) have been\nexplored in recent literature. However, current quantum hardware limits the\ndimension of the data that can be effectively encoded. Existing approaches\neither simplify datasets manually or use artificial graph datasets. This work\nintroduces the Guided Graph Compression (GGC) framework, which uses a graph\nautoencoder to reduce both the number of nodes and the dimensionality of node\nfeatures. The compression is guided to enhance the performance of a downstream\nclassification task, which can be applied either with a quantum or a classical\nclassifier. The framework is evaluated on the Jet Tagging task, a\nclassification problem of fundamental importance in high energy physics that\ninvolves distinguishing particle jets initiated by quarks from those by gluons.\nThe GGC is compared against using the autoencoder as a standalone preprocessing\nstep and against a baseline classical GNN classifier. Our numerical results\ndemonstrate that GGC outperforms both alternatives, while also facilitating the\ntesting of novel QGNN ansatzes on realistic datasets.", "AI": {"tldr": "The paper introduces Guided Graph Compression (GGC), a framework using a graph autoencoder to compress graph data for improved performance in downstream tasks, including quantum or classical classifiers.", "motivation": "Addressing the challenges of large graph processing in GNNs and leveraging quantum computing potential, the paper aims to enable realistic QGNN testing by compressing graph data effectively.", "method": "The GGC framework employs a graph autoencoder to reduce node count and feature dimensionality, guided by downstream task performance, and is evaluated on the Jet Tagging task.", "result": "GGC outperforms standalone autoencoder preprocessing and classical GNN baselines, enhancing performance and enabling QGNN testing on realistic datasets.", "conclusion": "GGC provides a scalable solution for graph compression, bridging the gap between quantum and classical approaches for graph-structured data."}}
{"id": "2506.09816", "pdf": "https://arxiv.org/pdf/2506.09816", "abs": "https://arxiv.org/abs/2506.09816", "authors": ["Cecilia Casolo", "S\u00f6ren Becker", "Niki Kilbertus"], "title": "Identifiability Challenges in Sparse Linear Ordinary Differential Equations", "categories": ["cs.LG"], "comment": "9 pages, 4 figures", "summary": "Dynamical systems modeling is a core pillar of scientific inquiry across\nnatural and life sciences. Increasingly, dynamical system models are learned\nfrom data, rendering identifiability a paramount concept. For systems that are\nnot identifiable from data, no guarantees can be given about their behavior\nunder new conditions and inputs, or about possible control mechanisms to steer\nthe system. It is known in the community that \"linear ordinary differential\nequations (ODE) are almost surely identifiable from a single trajectory.\"\nHowever, this only holds for dense matrices. The sparse regime remains\nunderexplored, despite its practical relevance with sparsity arising naturally\nin many biological, social, and physical systems. In this work, we address this\ngap by characterizing the identifiability of sparse linear ODEs. Contrary to\nthe dense case, we show that sparse systems are unidentifiable with a positive\nprobability in practically relevant sparsity regimes and provide lower bounds\nfor this probability. We further study empirically how this theoretical\nunidentifiability manifests in state-of-the-art methods to estimate linear ODEs\nfrom data. Our results corroborate that sparse systems are also practically\nunidentifiable. Theoretical limitations are not resolved through inductive\nbiases or optimization dynamics. Our findings call for rethinking what can be\nexpected from data-driven dynamical system modeling and allows for quantitative\nassessments of how much to trust a learned linear ODE.", "AI": {"tldr": "The paper explores identifiability in sparse linear ODEs, showing they are unidentifiable with positive probability, unlike dense systems, and discusses practical implications.", "motivation": "To address the underexplored sparse regime in linear ODE identifiability, which is practically relevant in biological, social, and physical systems.", "method": "Characterizes identifiability of sparse linear ODEs, provides lower bounds for unidentifiability probability, and empirically tests state-of-the-art estimation methods.", "result": "Sparse systems are unidentifiable with positive probability, and theoretical unidentifiability manifests in practical estimation methods.", "conclusion": "The findings urge rethinking expectations in data-driven dynamical system modeling and enable quantitative trust assessment of learned linear ODEs."}}
{"id": "2506.09849", "pdf": "https://arxiv.org/pdf/2506.09849", "abs": "https://arxiv.org/abs/2506.09849", "authors": ["Florian Bordes", "Quentin Garrido", "Justine T Kao", "Adina Williams", "Michael Rabbat", "Emmanuel Dupoux"], "title": "IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments", "categories": ["cs.CV"], "comment": null, "summary": "We present IntPhys 2, a video benchmark designed to evaluate the intuitive\nphysics understanding of deep learning models. Building on the original IntPhys\nbenchmark, IntPhys 2 focuses on four core principles related to macroscopic\nobjects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.\nThese conditions are inspired by research into intuitive physical understanding\nemerging during early childhood. IntPhys 2 offers a comprehensive suite of\ntests, based on the violation of expectation framework, that challenge models\nto differentiate between possible and impossible events within controlled and\ndiverse virtual environments. Alongside the benchmark, we provide performance\nevaluations of several state-of-the-art models. Our findings indicate that\nwhile these models demonstrate basic visual understanding, they face\nsignificant challenges in grasping intuitive physics across the four principles\nin complex scenes, with most models performing at chance levels (50%), in stark\ncontrast to human performance, which achieves near-perfect accuracy. This\nunderscores the gap between current models and human-like intuitive physics\nunderstanding, highlighting the need for advancements in model architectures\nand training methodologies.", "AI": {"tldr": "IntPhys 2 is a video benchmark testing deep learning models' intuitive physics understanding, focusing on four core principles. State-of-the-art models struggle, performing at chance levels, unlike humans.", "motivation": "To evaluate and improve deep learning models' intuitive physics understanding, inspired by early childhood development.", "method": "Uses a violation of expectation framework in controlled virtual environments to test four principles: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.", "result": "Models perform poorly (50% accuracy), far below human performance, revealing a gap in intuitive physics understanding.", "conclusion": "Highlights the need for better model architectures and training methods to achieve human-like intuitive physics understanding."}}
{"id": "2506.09953", "pdf": "https://arxiv.org/pdf/2506.09953", "abs": "https://arxiv.org/abs/2506.09953", "authors": ["Benjamin Reichman", "Constantin Patsch", "Jack Truxal", "Atishay Jain", "Larry Heck"], "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.", "AI": {"tldr": "The paper introduces a dataset for visually grounded dialogue tasks requiring external knowledge, extending OK-VQA to videos, and provides baselines for future challenges.", "motivation": "To explore the challenge of combining visual recognition over time with external knowledge in a dialogue setting, where questions rely on information not visually present.", "method": "A dataset of 2,017 videos with 5,986 human-annotated dialogues (40,954 turns) is created, where questions require external knowledge beyond the video content.", "result": "Baselines are provided, highlighting the complexity of integrating visual and external knowledge in dialogues.", "conclusion": "The dataset and baselines pave the way for future research in visually grounded dialogue systems requiring external knowledge."}}
{"id": "2506.09873", "pdf": "https://arxiv.org/pdf/2506.09873", "abs": "https://arxiv.org/abs/2506.09873", "authors": ["Emma Kallina", "Thomas Bohn\u00e9", "Jat Singh"], "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Published at the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency FAccT'25", "summary": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement\n(SHI) during AI development. At the same time, SHI is already common in\ncommercial software development, but with potentially different foci. This\nstudy clarifies the extent to which established SHI practices are able to\ncontribute to rAI efforts as well as potential disconnects -- essential\ninsights to inform and tailor future interventions that further shift industry\npractice towards rAI efforts. First, we analysed 56 rAI guidance documents to\nidentify why SHI is recommended (i.e. its expected benefits for rAI) and\nuncovered goals such as redistributing power, improving socio-technical\nunderstandings, anticipating risks, and enhancing public oversight. To\nunderstand why and how SHI is currently practised in commercial settings, we\nthen conducted an online survey (n=130) and semi-structured interviews (n=10)\nwith AI practitioners. Our findings reveal that SHI in practice is primarily\ndriven by commercial priorities (e.g. customer value, compliance) and several\nfactors currently discourage more rAI-aligned SHI practices. This suggests that\nestablished SHI practices are largely not contributing to rAI efforts. To\naddress this disconnect, we propose interventions and research opportunities to\nadvance rAI development in practice.", "AI": {"tldr": "The study examines how stakeholder involvement (SHI) in commercial AI development aligns with Responsible AI (rAI) goals, finding a disconnect due to commercial priorities.", "motivation": "To clarify if established SHI practices contribute to rAI efforts and identify gaps to inform future interventions.", "method": "Analysis of 56 rAI guidance documents, an online survey (n=130), and semi-structured interviews (n=10) with AI practitioners.", "result": "SHI in practice is driven by commercial priorities, not rAI goals, revealing a disconnect.", "conclusion": "Proposes interventions to align SHI with rAI efforts for better industry practice."}}
{"id": "2506.09824", "pdf": "https://arxiv.org/pdf/2506.09824", "abs": "https://arxiv.org/abs/2506.09824", "authors": ["Johan Erbani", "Sonia Ben Mokhtar", "Pierre-Edouard Portier", "Elod Egyed-Zsigmond", "Diana Nurbakova"], "title": "Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) is a machine learning paradigm that enables multiple\ndata holders to collaboratively train a machine learning model without sharing\ntheir training data with external parties. In this paradigm, workers locally\nupdate a model and share with a central server their updated gradients (or\nmodel parameters). While FL seems appealing from a privacy perspective, it\nopens a number of threats from a security perspective as (Byzantine)\nparticipants can contribute poisonous gradients (or model parameters) harming\nmodel convergence. Byzantine-resilient FL addresses this issue by ensuring that\nthe training proceeds as if Byzantine participants were absent. Towards this\npurpose, common strategies ignore outlier gradients during model aggregation,\nassuming that Byzantine gradients deviate more from honest gradients than\nhonest gradients do from each other. However, in heterogeneous settings, honest\ngradients may differ significantly, making it difficult to distinguish honest\noutliers from Byzantine ones. In this paper, we introduce the Worker Label\nAlignement Loss (WoLA), a weighted loss that aligns honest worker gradients\ndespite data heterogeneity, which facilitates the identification of Byzantines'\ngradients. This approach significantly outperforms state-of-the-art methods in\nheterogeneous settings. In this paper, we provide both theoretical insights and\nempirical evidence of its effectiveness.", "AI": {"tldr": "The paper introduces WoLA, a weighted loss method for Byzantine-resilient federated learning, improving gradient alignment in heterogeneous settings.", "motivation": "Federated learning (FL) faces security threats from Byzantine participants who submit poisonous gradients. Current methods struggle in heterogeneous settings where honest gradients vary widely.", "method": "Proposes Worker Label Alignment Loss (WoLA), a weighted loss to align honest gradients and better identify Byzantine ones.", "result": "WoLA outperforms state-of-the-art methods in heterogeneous settings, supported by theory and experiments.", "conclusion": "WoLA effectively addresses Byzantine threats in FL, especially in heterogeneous environments, enhancing model convergence."}}
{"id": "2506.09881", "pdf": "https://arxiv.org/pdf/2506.09881", "abs": "https://arxiv.org/abs/2506.09881", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "AI": {"tldr": "Vireo is a novel framework for Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS), unifying OVSS and DGSS. It leverages frozen Visual Foundation Models and depth-based features, introducing GeoText Prompts, CMPE, and DOV-VEH for robust performance.", "motivation": "Addressing the need for pixel-level segmentation of unseen categories and robustness across unseen domains, crucial for real-world applications like autonomous driving.", "method": "Vireo uses frozen Visual Foundation Models and depth-based features, with GeoText Prompts, CMPE, and DOV-VEH to align visual-textual modalities and enhance robustness.", "result": "Vireo achieves state-of-the-art performance, significantly outperforming existing methods in domain generalization and open-vocabulary recognition.", "conclusion": "Vireo provides a unified, scalable solution for robust visual understanding in diverse environments, with code publicly available."}}
{"id": "2506.09998", "pdf": "https://arxiv.org/pdf/2506.09998", "abs": "https://arxiv.org/abs/2506.09998", "authors": ["Tim Z. Xiao", "Johannes Zenn", "Zhen Liu", "Weiyang Liu", "Robert Bamler", "Bernhard Sch\u00f6lkopf"], "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling", "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report v1 (21 pages, 14 figures)", "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.", "AI": {"tldr": "The paper introduces Verbalized Rejection Sampling (VRS) to improve LLMs' ability to generate faithful samples from Bernoulli distributions, reducing bias and enhancing reliability without modifying model internals.", "motivation": "LLMs can describe probability distributions but struggle to generate accurate samples, limiting their use in stochastic tasks. This gap is investigated for Bernoulli distributions.", "method": "The authors propose VRS, a natural-language adaptation of rejection sampling, prompting LLMs to reason about and accept/reject samples.", "result": "VRS significantly reduces sampling bias across models, with theoretical analysis confirming its improvement over direct sampling.", "conclusion": "VRS demonstrates how classical probabilistic tools can be verbalized to enhance LLM reliability in sampling tasks without heavy engineering."}}
{"id": "2506.09883", "pdf": "https://arxiv.org/pdf/2506.09883", "abs": "https://arxiv.org/abs/2506.09883", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "AI": {"tldr": "A lightweight framework, Geometric Distillation, enhances 2D-trained VLMs with 3D spatial understanding using geometric cues from 3D models, improving performance on 3D tasks without architectural changes.", "motivation": "VLMs lack 3D spatial understanding, limiting their effectiveness in spatially grounded tasks. This work aims to bridge this gap efficiently.", "method": "Proposes Geometric Distillation, distilling sparse correspondences, relative depth relations, and dense cost volumes from 3D models into VLMs without altering their architecture.", "result": "Outperforms prior methods on 3D vision-language reasoning and perception benchmarks, achieving better 3D reasoning with lower computational cost.", "conclusion": "Demonstrates a scalable way to equip 2D-trained VLMs with 3D understanding, expanding their applicability in multimodal tasks."}}
{"id": "2506.09867", "pdf": "https://arxiv.org/pdf/2506.09867", "abs": "https://arxiv.org/abs/2506.09867", "authors": ["Amit Baran Dey", "Wasim Arif", "Rakhesh Singh Kshetrimayum"], "title": "Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing", "categories": ["cs.LG"], "comment": "6 pages, 11 figures, Accepted to IEEE INDISCON 2025", "summary": "This paper proposes a machine learning-based methodology for the\nclassification of various oil samples based on their dielectric properties,\nutilizing a microwave resonant sensor. The dielectric behaviour of oils,\ngoverned by their molecular composition, induces distinct shifts in the\nsensor's resonant frequency and amplitude response. These variations are\nsystematically captured and processed to extract salient features, which serve\nas inputs for multiple machine learning classifiers. The microwave resonant\nsensor operates in a non-destructive, low-power manner, making it particularly\nwell-suited for real-time industrial applications. A comprehensive dataset is\ndeveloped by varying the permittivity of oil samples and acquiring the\ncorresponding sensor responses. Several classifiers are trained and evaluated\nusing the extracted resonant features to assess their capability in\ndistinguishing between oil types. Experimental results demonstrate that the\nproposed approach achieves a high classification accuracy of 99.41% with the\nrandom forest classifier, highlighting its strong potential for automated oil\nidentification. The system's compact form factor, efficiency, and high\nperformance underscore its viability for fast and reliable oil characterization\nin industrial environments.", "AI": {"tldr": "A machine learning method classifies oil samples using dielectric properties and a microwave sensor, achieving 99.41% accuracy with random forest.", "motivation": "To enable non-destructive, real-time oil classification for industrial applications using dielectric properties.", "method": "Uses a microwave resonant sensor to capture dielectric variations, extracts features, and applies machine learning classifiers.", "result": "Achieves 99.41% classification accuracy with random forest.", "conclusion": "The system is efficient, compact, and suitable for industrial oil characterization."}}
{"id": "2506.09885", "pdf": "https://arxiv.org/pdf/2506.09885", "abs": "https://arxiv.org/abs/2506.09885", "authors": ["Haoru Wang", "Kai Ye", "Yangyan Li", "Wenzheng Chen", "Baoquan Chen"], "title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge", "categories": ["cs.CV"], "comment": null, "summary": "We consider the problem of generalizable novel view synthesis (NVS), which\naims to generate photorealistic novel views from sparse or even unposed 2D\nimages without per-scene optimization. This task remains fundamentally\nchallenging, as it requires inferring 3D structure from incomplete and\nambiguous 2D observations. Early approaches typically rely on strong 3D\nknowledge, including architectural 3D inductive biases (e.g., embedding\nexplicit 3D representations, such as NeRF or 3DGS, into network design) and\nground-truth camera poses for both input and target views. While recent efforts\nhave sought to reduce the 3D inductive bias or the dependence on known camera\nposes of input views, critical questions regarding the role of 3D knowledge and\nthe necessity of circumventing its use remain under-explored. In this work, we\nconduct a systematic analysis on the 3D knowledge and uncover a critical trend:\nthe performance of methods that requires less 3D knowledge accelerates more as\ndata scales, eventually achieving performance on par with their 3D\nknowledge-driven counterparts, which highlights the increasing importance of\nreducing dependence on 3D knowledge in the era of large-scale data. Motivated\nby and following this trend, we propose a novel NVS framework that minimizes 3D\ninductive bias and pose dependence for both input and target views. By\neliminating this 3D knowledge, our method fully leverages data scaling and\nlearns implicit 3D awareness directly from sparse 2D images, without any 3D\ninductive bias or pose annotation during training. Extensive experiments\ndemonstrate that our model generates photorealistic and 3D-consistent novel\nviews, achieving even comparable performance with methods that rely on posed\ninputs, thereby validating the feasibility and effectiveness of our\ndata-centric paradigm. Project page:\nhttps://pku-vcl-geometry.github.io/Less3Depend/ .", "AI": {"tldr": "The paper explores reducing 3D knowledge dependence in novel view synthesis (NVS), showing that methods with less 3D bias scale better with data. It proposes a framework minimizing 3D inductive bias and pose dependence, achieving results comparable to pose-dependent methods.", "motivation": "The challenge of inferring 3D structure from sparse or unposed 2D images without per-scene optimization, and the under-explored role of 3D knowledge in NVS.", "method": "A novel NVS framework that eliminates 3D inductive bias and pose dependence, learning implicit 3D awareness directly from sparse 2D images.", "result": "The method generates photorealistic, 3D-consistent novel views, matching performance of pose-dependent methods.", "conclusion": "Reducing 3D knowledge dependence is feasible and effective, especially with large-scale data, as demonstrated by the proposed framework."}}
{"id": "2305.14725", "pdf": "https://arxiv.org/pdf/2305.14725", "abs": "https://arxiv.org/abs/2305.14725", "authors": ["Barry Menglong Yao", "Sijia Wang", "Yu Chen", "Qifan Wang", "Minqian Liu", "Zhiyang Xu", "Licheng Yu", "Lifu Huang"], "title": "AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes", "categories": ["cs.CL", "I.2.7"], "comment": "19 pages, 7 figures", "summary": "We propose attribute-aware multimodal entity linking, where the input\nconsists of a mention described with a text paragraph and images, and the goal\nis to predict the corresponding target entity from a multimodal knowledge base\n(KB) where each entity is also accompanied by a text description, visual\nimages, and a collection of attributes that present the meta-information of the\nentity in a structured format. To facilitate this research endeavor, we\nconstruct AMELI, encompassing a new multimodal entity linking benchmark dataset\nthat contains 16,735 mentions described in text and associated with 30,472\nimages, and a multimodal knowledge base that covers 34,690 entities along with\n177,873 entity images and 798,216 attributes. To establish baseline performance\non AMELI, we experiment with several state-of-the-art architectures for\nmultimodal entity linking and further propose a new approach that incorporates\nattributes of entities into disambiguation. Experimental results and extensive\nqualitative analysis demonstrate that extracting and understanding the\nattributes of mentions from their text descriptions and visual images play a\nvital role in multimodal entity linking. To the best of our knowledge, we are\nthe first to integrate attributes in the multimodal entity linking task. The\nprograms, model checkpoints, and the dataset are publicly available at\nhttps://github.com/VT-NLP/Ameli.", "AI": {"tldr": "The paper introduces attribute-aware multimodal entity linking, proposing AMELI, a benchmark dataset and knowledge base, and a new method incorporating entity attributes for improved disambiguation.", "motivation": "To enhance multimodal entity linking by leveraging structured attributes of entities, addressing gaps in existing methods that overlook such meta-information.", "method": "Constructs AMELI dataset and KB, tests state-of-the-art architectures, and introduces a new approach integrating entity attributes from text and images.", "result": "Attributes significantly improve disambiguation, with the proposed method outperforming existing models.", "conclusion": "Attributes are crucial for multimodal entity linking, and the new approach sets a benchmark for future research."}}
{"id": "2506.09891", "pdf": "https://arxiv.org/pdf/2506.09891", "abs": "https://arxiv.org/abs/2506.09891", "authors": ["Sebastian Hickman", "Ilija Trajkovic", "Julia Kaltenborn", "Francis Pelletier", "Alex Archibald", "Yaniv Gurwicz", "Peer Nowack", "David Rolnick", "Julien Boussard"], "title": "Causal Climate Emulation with Bayesian Filtering", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.ao-ph"], "comment": "32 pages, 21 figures", "summary": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.", "AI": {"tldr": "A physics-informed causal machine learning model is developed to emulate climate dynamics efficiently, outperforming traditional methods.", "motivation": "Traditional climate models are computationally expensive, and current machine learning approaches lack physics-informed causal relationships.", "method": "Develops an interpretable climate model emulator using causal representation learning and a Bayesian filter for stable long-term autoregressive emulation.", "result": "The emulator accurately learns climate dynamics, validated on synthetic and real climate model data.", "conclusion": "The proposed method offers an efficient and interpretable alternative to traditional climate modeling."}}
{"id": "2506.09870", "pdf": "https://arxiv.org/pdf/2506.09870", "abs": "https://arxiv.org/abs/2506.09870", "authors": ["Maximilian Egger", "Rawad Bitar"], "title": "Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning", "categories": ["cs.LG", "cs.DC", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "Ensuring resilience to Byzantine clients while maintaining the privacy of the\nclients' data is a fundamental challenge in federated learning (FL). When the\nclients' data is homogeneous, suitable countermeasures were studied from an\ninformation-theoretic perspective utilizing secure aggregation techniques while\nensuring robust aggregation of the clients' gradients. However, the\ncountermeasures used fail when the clients' data is heterogeneous. Suitable\npre-processing techniques, such as nearest neighbor mixing, were recently shown\nto enhance the performance of those countermeasures in the heterogeneous\nsetting. Nevertheless, those pre-processing techniques cannot be applied with\nthe introduced privacy-preserving mechanisms.\n  We propose a multi-stage method encompassing a careful co-design of\nverifiable secret sharing, secure aggregation, and a tailored symmetric private\ninformation retrieval scheme to achieve information-theoretic privacy\nguarantees and Byzantine resilience under data heterogeneity. We evaluate the\neffectiveness of our scheme on a variety of attacks and show how it outperforms\nthe previously known techniques. Since the communication overhead of secure\naggregation is non-negligible, we investigate the interplay with zero-order\nestimation methods that reduce the communication cost in state-of-the-art FL\ntasks and thereby make private aggregation scalable.", "AI": {"tldr": "A multi-stage method combining verifiable secret sharing, secure aggregation, and symmetric private information retrieval is proposed to ensure privacy and Byzantine resilience in federated learning with heterogeneous data.", "motivation": "Addressing the challenge of maintaining privacy and resilience against Byzantine clients in federated learning, especially when data is heterogeneous.", "method": "A co-design of verifiable secret sharing, secure aggregation, and symmetric private information retrieval, evaluated on various attacks.", "result": "Outperforms previous techniques in privacy and resilience, with reduced communication overhead via zero-order estimation.", "conclusion": "The proposed method effectively balances privacy, resilience, and scalability in heterogeneous federated learning."}}
{"id": "2506.09895", "pdf": "https://arxiv.org/pdf/2506.09895", "abs": "https://arxiv.org/abs/2506.09895", "authors": ["Athinoulla Konstantinou", "Georgios Leontidis", "Mamatha Thota", "Aiden Durrant"], "title": "EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks", "categories": ["cs.CV"], "comment": "19 pages, 11 Figures, 13 Tables", "summary": "Learning self-supervised representations that are invariant and equivariant\nto transformations is crucial for advancing beyond traditional visual\nclassification tasks. However, many methods rely on predictor architectures to\nencode equivariance, despite evidence that architectural choices, such as\ncapsule networks, inherently excel at learning interpretable pose-aware\nrepresentations. To explore this, we introduce EquiCaps (Equivariant Capsule\nNetwork), a capsule-based approach to pose-aware self-supervision that\neliminates the need for a specialised predictor for enforcing equivariance.\nInstead, we leverage the intrinsic pose-awareness capabilities of capsules to\nimprove performance in pose estimation tasks. To further challenge our\nassumptions, we increase task complexity via multi-geometric transformations to\nenable a more thorough evaluation of invariance and equivariance by introducing\n3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical\nresults demonstrate that EquiCaps outperforms prior state-of-the-art\nequivariant methods on rotation prediction, achieving a supervised-level $R^2$\nof 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE\nand CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to\nnon-capsule-based equivariant approaches, EquiCaps maintains robust equivariant\nperformance under combined geometric transformations, underscoring its\ngeneralisation capabilities and the promise of predictor-free capsule\narchitectures.", "AI": {"tldr": "EquiCaps, a capsule-based method, leverages intrinsic pose-awareness for self-supervised learning, outperforming state-of-the-art equivariant methods in pose estimation tasks.", "motivation": "To explore capsule networks' inherent ability for pose-aware representations without needing specialized predictors for equivariance.", "method": "Introduces EquiCaps, a capsule-based approach, and tests it with multi-geometric transformations using the 3DIEBench-T dataset.", "result": "Achieves a supervised-level R\u00b2 of 0.78 in rotation prediction, outperforming SIE and CapsIE by 0.05 and 0.04 R\u00b2, respectively.", "conclusion": "EquiCaps demonstrates robust equivariant performance under complex transformations, highlighting the potential of predictor-free capsule architectures."}}
{"id": "2402.16733", "pdf": "https://arxiv.org/pdf/2402.16733", "abs": "https://arxiv.org/abs/2402.16733", "authors": ["Haneul Yoo", "Jieun Han", "So-Yeon Ahn", "Alice Oh"], "title": "DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing", "categories": ["cs.CL", "cs.AI"], "comment": "To appear in ACL 2025. arXiv admin note: text overlap with\n  arXiv:2310.05191", "summary": "Automated essay scoring (AES) is a useful tool in English as a Foreign\nLanguage (EFL) writing education, offering real-time essay scores for students\nand instructors. However, previous AES models were trained on essays and scores\nirrelevant to the practical scenarios of EFL writing education and usually\nprovided a single holistic score due to the lack of appropriate datasets. In\nthis paper, we release DREsS, a large-scale, standard dataset for rubric-based\nautomated essay scoring with 48.9K samples in total. DREsS comprises three\nsub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a\nreal-classroom dataset with 2.3K essays authored by EFL undergraduate students\nand scored by English education experts. We also standardize existing\nrubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a\ncorruption-based augmentation strategy for essays, which generates 40.1K\nsynthetic samples of DREsS_CASE and improves the baseline results by 45.44%.\nDREsS will enable further research to provide a more accurate and practical AES\nsystem for EFL writing education.", "AI": {"tldr": "The paper introduces DREsS, a large-scale dataset for rubric-based automated essay scoring (AES) in EFL education, addressing gaps in previous models by including real-classroom data and synthetic samples for improved accuracy.", "motivation": "Previous AES models lacked relevance to EFL education and provided only holistic scores due to inadequate datasets.", "method": "The authors release DREsS, comprising three sub-datasets: real-classroom essays (DREsS_New), standardized existing datasets (DREsS_Std.), and synthetic samples (DREsS_CASE) generated via CASE, a corruption-based augmentation strategy.", "result": "DREsS includes 48.9K samples, with synthetic data improving baseline results by 45.44%.", "conclusion": "DREsS enables more accurate and practical AES systems for EFL writing education."}}
{"id": "2506.09932", "pdf": "https://arxiv.org/pdf/2506.09932", "abs": "https://arxiv.org/abs/2506.09932", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "categories": ["cs.CV", "cs.AI"], "comment": "4 Pages, 5 Figures", "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "AI": {"tldr": "HadaNorm, a novel linear transformation, improves quantization for diffusion models by normalizing activations and using Hadamard transformations, achieving better efficiency-performance trade-offs.", "motivation": "Diffusion models face high memory and computational demands, limiting deployment on resource-constrained devices. Standard PTQ methods struggle with outliers and require transformations for higher compression.", "method": "HadaNorm normalizes activations feature channels and applies Hadamard transformations before quantization, mitigating outliers and enabling aggressive activation quantization.", "result": "HadaNorm reduces quantization error across transformer blocks and outperforms state-of-the-art methods in efficiency-performance trade-offs.", "conclusion": "HadaNorm effectively addresses quantization challenges for diffusion models, offering superior performance and efficiency."}}
{"id": "2506.09887", "pdf": "https://arxiv.org/pdf/2506.09887", "abs": "https://arxiv.org/abs/2506.09887", "authors": ["Nirmit Joshi", "Hugo Koubbi", "Theodor Misiakiewicz", "Nathan Srebro"], "title": "Learning single-index models via harmonic decomposition", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "80 pages", "summary": "We study the problem of learning single-index models, where the label $y \\in\n\\mathbb{R}$ depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through\nan unknown one-dimensional projection $\\langle\n\\boldsymbol{w}_*,\\boldsymbol{x}\\rangle$. Prior work has shown that under\nGaussian inputs, the statistical and computational complexity of recovering\n$\\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.\nIn this paper, we propose a new perspective: we argue that \"spherical\nharmonics\" -- rather than \"Hermite polynomials\" -- provide the natural basis\nfor this problem, as they capture its intrinsic \"rotational symmetry\". Building\non this insight, we characterize the complexity of learning single-index models\nunder arbitrary spherically symmetric input distributions. We introduce two\nfamilies of estimators -- based on tensor unfolding and online SGD -- that\nrespectively achieve either optimal sample complexity or optimal runtime, and\nargue that estimators achieving both may not exist in general. When specialized\nto Gaussian inputs, our theory not only recovers and clarifies existing results\nbut also reveals new phenomena that had previously been overlooked.", "AI": {"tldr": "The paper explores learning single-index models using spherical harmonics instead of Hermite polynomials, achieving optimal sample complexity or runtime with new estimators.", "motivation": "The study aims to address the limitations of prior work by leveraging spherical harmonics to capture the rotational symmetry of single-index models under spherically symmetric inputs.", "method": "The authors propose two estimators: one based on tensor unfolding for optimal sample complexity and another using online SGD for optimal runtime.", "result": "The approach recovers and clarifies existing results for Gaussian inputs while uncovering new phenomena previously overlooked.", "conclusion": "Spherical harmonics provide a natural basis for learning single-index models, with trade-offs between sample complexity and runtime."}}
{"id": "2506.09897", "pdf": "https://arxiv.org/pdf/2506.09897", "abs": "https://arxiv.org/abs/2506.09897", "authors": ["Tao Liu", "Zhenchao Cui"], "title": "CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects", "categories": ["cs.CV"], "comment": null, "summary": "Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid\nnetworks: high-level features (P5-P6) frequently receive zero positive anchors\nunder standard label assignment protocols, leaving their semantic\nrepresentations untrained due to exclusion from loss computation. This creates\ndual deficiencies: (1) Stranded high-level features become semantic dead-ends\nwithout gradient updates, while (2) low-level features lack essential semantic\ncontext for robust classification. We propose E-FPN-BS that systematically\nconverts wasted high-level semantics into low-level feature enhancements. To\naddress these issues, we propose E-FPN-BS, a novel architecture integrating\nmulti-scale feature enhancement and adaptive optimization. First, our Context\nEnhancement Module(CEM) employs dual-branch processing to align and compress\nhigh-level features for effective global-local fusion. Second, the\nForeground-Background Separation Module (FBSM) generates spatial gating masks\nthat dynamically amplify discriminative regions. To address gradient imbalance\nacross object scales, we further propose a Dynamic Gradient-Balanced Loss\n(DCLoss) that automatically modulates loss contributions via scale-aware\ngradient equilibrium. Extensive experiments across multiple benchmark datasets\ndemonstrate the outstanding performance and generalization ability of our\napproach.", "AI": {"tldr": "E-FPN-BS addresses TOD issues by enhancing high-level features for low-level use, introducing CEM and FBSM for feature fusion and dynamic amplification, and DCLoss for balanced gradients.", "motivation": "Standard label assignment in TOD leaves high-level features untrained, causing semantic dead-ends and weak low-level context.", "method": "Proposes E-FPN-BS with CEM for feature fusion, FBSM for dynamic amplification, and DCLoss for gradient balance.", "result": "Outperforms benchmarks, demonstrating strong generalization.", "conclusion": "E-FPN-BS effectively resolves feature misuse and gradient imbalance in TOD."}}
{"id": "2404.01129", "pdf": "https://arxiv.org/pdf/2404.01129", "abs": "https://arxiv.org/abs/2404.01129", "authors": ["Bohao Yang", "Kun Zhao", "Dong Liu", "Liang Zhan", "Chenghua Lin"], "title": "Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Automatic open-domain dialogue evaluation has attracted increasing attention,\nyet remains challenging due to the complexity of assessing response\nappropriateness. Traditional evaluation metrics, typically trained with true\npositive and randomly selected negative responses, tend to assign higher scores\nto responses that share greater content similarity with contexts. However,\nadversarial negative responses, despite possessing high lexical overlap with\ncontexts, can be semantically incongruous. Consequently, existing metrics\nstruggle to effectively evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in handling adversarial negative examples. We propose a novel\nevaluation framework that integrates Abstract Meaning Representation (AMR)\nenhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly\nincorporate AMR graph information through a gating mechanism for enhanced\nsemantic representation learning, while both SLM predictions and AMR knowledge\nare integrated into LLM prompts for robust evaluation. Extensive experiments on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to state-of-the-art baselines. Our comprehensive ablation studies\nreveal that AMR graph information contributes substantially more to performance\nimprovements. Our framework achieves strong correlations with human judgments\nacross multiple datasets, establishing a new benchmark for dialogue evaluation.\nOur code and data are publicly available.", "AI": {"tldr": "A novel framework combining AMR-enhanced SLMs and LLMs improves open-domain dialogue evaluation by better handling adversarial negative responses and achieving high correlation with human judgments.", "motivation": "Traditional metrics fail to evaluate adversarial negative responses effectively due to high lexical overlap but semantic incongruity, leading to low correlation with human judgments.", "method": "Integrates AMR-enhanced SLMs with LLMs, using AMR graphs for semantic representation and combining SLM predictions with AMR knowledge in LLM prompts.", "result": "Superior performance over baselines, with AMR graphs significantly boosting results. Strong correlation with human judgments across datasets.", "conclusion": "The proposed framework sets a new benchmark for dialogue evaluation, demonstrating the value of AMR and hybrid SLM-LLM approaches."}}
{"id": "2506.09937", "pdf": "https://arxiv.org/pdf/2506.09937", "abs": "https://arxiv.org/abs/2506.09937", "authors": ["Qiao Gu", "Yuanliang Ju", "Shengxiang Sun", "Igor Gilitschenski", "Haruki Nishimura", "Masha Itkina", "Florian Shkurti"], "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI"], "comment": "Project Page: https://vla-safe.github.io/", "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.", "AI": {"tldr": "SAFE is a failure detector for vision-language-action models (VLAs) that generalizes to unseen tasks and environments, outperforming existing methods in accuracy and detection time.", "motivation": "VLAs show limited success on novel tasks, requiring a failure detector that works across diverse tasks and environments.", "method": "SAFE learns from VLA internal features to predict task failure likelihood, trained on both successful and failed rollouts.", "result": "SAFE achieves state-of-the-art performance and optimal accuracy-detection time trade-off, tested on OpenVLA, \u03c00, and \u03c00-FAST in simulations and real-world.", "conclusion": "SAFE effectively generalizes failure detection for VLAs, enhancing their safety and reliability in novel scenarios."}}
{"id": "2506.09896", "pdf": "https://arxiv.org/pdf/2506.09896", "abs": "https://arxiv.org/abs/2506.09896", "authors": ["Attanasia Garuso", "Silvija Kokalj-Filipovic", "Yagna Kaasaragadda"], "title": "A look at adversarial attacks on radio waveforms from discrete latent space", "categories": ["cs.LG"], "comment": null, "summary": "Having designed a VQVAE that maps digital radio waveforms into discrete\nlatent space, and yields a perfectly classifiable reconstruction of the\noriginal data, we here analyze the attack suppressing properties of VQVAE when\nan adversarial attack is performed on high-SNR radio-frequency (RF)\ndata-points. To target amplitude modulations from a subset of digitally\nmodulated waveform classes, we first create adversarial attacks that preserve\nthe phase between the in-phase and quadrature component whose values are\nadversarially changed. We compare them with adversarial attacks of the same\nintensity where phase is not preserved. We test the classification accuracy of\nsuch adversarial examples on a classifier trained to deliver 100% accuracy on\nthe original data. To assess the ability of VQVAE to suppress the strength of\nthe attack, we evaluate the classifier accuracy on the reconstructions by VQVAE\nof the adversarial datapoints and show that VQVAE substantially decreases the\neffectiveness of the attack. We also compare the I/Q plane diagram of the\nattacked data, their reconstructions and the original data. Finally, using\nmultiple methods and metrics, we compare the probability distribution of the\nVQVAE latent space with and without attack. Varying the attack strength, we\nobserve interesting properties of the discrete space, which may help detect the\nattacks.", "AI": {"tldr": "The paper analyzes how VQVAE suppresses adversarial attacks on high-SNR RF data, showing it reduces attack effectiveness and explores latent space properties for attack detection.", "motivation": "To evaluate VQVAE's ability to mitigate adversarial attacks on digitally modulated waveforms, preserving phase and amplitude modulations.", "method": "Created adversarial attacks (phase-preserving and non-preserving), tested classifier accuracy on VQVAE reconstructions, and analyzed latent space distributions.", "result": "VQVAE significantly reduces attack effectiveness and reveals latent space properties useful for attack detection.", "conclusion": "VQVAE is effective in suppressing adversarial attacks and offers insights into latent space behavior under varying attack strengths."}}
{"id": "2506.09916", "pdf": "https://arxiv.org/pdf/2506.09916", "abs": "https://arxiv.org/abs/2506.09916", "authors": ["Tilemachos Aravanis", "Panagiotis Filntisis", "Petros Maragos", "George Retsinas"], "title": "Only-Style: Stylistic Consistency in Image Generation without Content Leakage", "categories": ["cs.CV"], "comment": null, "summary": "Generating images in a consistent reference visual style remains a\nchallenging computer vision task. State-of-the-art methods aiming for\nstyle-consistent generation struggle to effectively separate semantic content\nfrom stylistic elements, leading to content leakage from the image provided as\na reference to the targets. To address this challenge, we propose Only-Style: a\nmethod designed to mitigate content leakage in a semantically coherent manner\nwhile preserving stylistic consistency. Only-Style works by localizing content\nleakage during inference, allowing the adaptive tuning of a parameter that\ncontrols the style alignment process, specifically within the image patches\ncontaining the subject in the reference image. This adaptive process best\nbalances stylistic consistency with leakage elimination. Moreover, the\nlocalization of content leakage can function as a standalone component, given a\nreference-target image pair, allowing the adaptive tuning of any\nmethod-specific parameter that provides control over the impact of the\nstylistic reference. In addition, we propose a novel evaluation framework to\nquantify the success of style-consistent generations in avoiding undesired\ncontent leakage. Our approach demonstrates a significant improvement over\nstate-of-the-art methods through extensive evaluation across diverse instances,\nconsistently achieving robust stylistic consistency without undesired content\nleakage.", "AI": {"tldr": "Only-Style mitigates content leakage in style-consistent image generation by localizing leakage and adaptively tuning style alignment, achieving robust results.", "motivation": "Addressing the challenge of separating semantic content from stylistic elements in image generation to prevent content leakage.", "method": "Localizes content leakage during inference and adaptively tunes style alignment parameters in affected patches.", "result": "Significant improvement over state-of-the-art methods, achieving stylistic consistency without content leakage.", "conclusion": "Only-Style effectively balances style consistency and leakage elimination, with a novel evaluation framework for quantifying success."}}
{"id": "2405.14259", "pdf": "https://arxiv.org/pdf/2405.14259", "abs": "https://arxiv.org/abs/2405.14259", "authors": ["Chan-Jan Hsu", "Yi-Chang Chen", "Feng-Ting Liao", "Pei-Chen Ho", "Yu-Hsiang Wang", "Po-Chun Hsu", "Da-shan Shiu"], "title": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Robust and Instruction-Aware ASR and OCR", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework designed to integrate large language models (LLMs) into cross-modal\ntext recognition systems for automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the necessary formulations to enable GFD\nto operate across mismatched token spaces of different models by calculating\nlikelihood at the byte level, thereby enabling seamless fusion and synchronous\nprogression during the decoding process. GFD is plug-and-play by design, making\nit readily compatible with various auto-regressive models without the need for\nany re-training. GFD proves effective for general ASR and OCR tasks through\nintermediate and frequent interactions with LLMs, surpassing cascaded methods\nin English and Mandarin benchmarks. In addition, GFD transfers in-context\nlearning abilities of LLMs and allows for adaptive ASR in instruction-aware and\nlong-context settings, yielding significant WER reductions of up to 17.7\\%.", "AI": {"tldr": "GFD is a novel shallow fusion framework for integrating LLMs into ASR and OCR systems, enabling seamless fusion across mismatched token spaces without re-training. It outperforms cascaded methods and adapts to in-context learning.", "motivation": "To enhance cross-modal text recognition systems (ASR and OCR) by integrating LLMs efficiently, overcoming token space mismatches and enabling adaptive learning.", "method": "GFD operates at the byte level for likelihood calculation, allowing plug-and-play fusion with auto-regressive models. It leverages intermediate and frequent LLM interactions.", "result": "GFD surpasses cascaded methods in English and Mandarin benchmarks, achieving up to 17.7% WER reduction in adaptive ASR settings.", "conclusion": "GFD is an effective, plug-and-play solution for integrating LLMs into ASR and OCR, improving performance and enabling adaptive learning."}}
{"id": "2506.09940", "pdf": "https://arxiv.org/pdf/2506.09940", "abs": "https://arxiv.org/abs/2506.09940", "authors": ["Jiachen Hu", "Rui Ai", "Han Zhong", "Xiaoyu Chen", "Liwei Wang", "Zhaoran Wang", "Zhuoran Yang"], "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at ICML 2025", "summary": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.09901", "pdf": "https://arxiv.org/pdf/2506.09901", "abs": "https://arxiv.org/abs/2506.09901", "authors": ["Noel Brindise", "Vijeth Hebbar", "Riya Shah", "Cedric Langbort"], "title": "\"What are my options?\": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we provide an extended discussion of a new approach to\nexplainable Reinforcement Learning called Diverse Near-Optimal Alternatives\n(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable \"options\" for\ntrajectory-planning agents, optimizing policies to produce qualitatively\ndiverse trajectories in Euclidean space. In the spirit of explainability, these\ndistinct policies are used to \"explain\" an agent's options in terms of\navailable trajectory shapes from which a human user may choose. In particular,\nDNA applies to value function-based policies on Markov decision processes where\nagents are limited to continuous trajectories. Here, we describe DNA, which\nuses reward shaping in local, modified Q-learning problems to solve for\ndistinct policies with guaranteed epsilon-optimality. We show that it\nsuccessfully returns qualitatively different policies that constitute\nmeaningfully different \"options\" in simulation, including a brief comparison to\nrelated approaches in the stochastic optimization field of Quality Diversity.\nBeyond the explanatory motivation, this work opens new possibilities for\nexploration and adaptive planning in RL.", "AI": {"tldr": "DNA introduces a method for explainable Reinforcement Learning by generating diverse, near-optimal trajectories for human interpretability and choice.", "motivation": "The goal is to enhance explainability in RL by providing diverse policy options for trajectory-planning agents, aiding human understanding and decision-making.", "method": "DNA uses reward shaping in modified Q-learning to produce distinct, epsilon-optimal policies for continuous trajectories in Markov decision processes.", "result": "The method successfully generates qualitatively diverse policies, offering meaningful options in simulations, and compares favorably to Quality Diversity approaches.", "conclusion": "DNA advances explainable RL and opens new avenues for exploration and adaptive planning in reinforcement learning."}}
{"id": "2506.09919", "pdf": "https://arxiv.org/pdf/2506.09919", "abs": "https://arxiv.org/abs/2506.09919", "authors": ["He Zhang", "Chentao Song", "Hongwen Zhang", "Tao Yu"], "title": "MetricHMR: Metric Human Mesh Recovery from Monocular Images", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric\nhuman mesh recovery with accurate global translation from monocular images. In\ncontrast to existing HMR methods that suffer from severe scale and depth\nambiguity, MetricHMR is able to produce geometrically reasonable body shape and\nglobal translation in the reconstruction results. To this end, we first\nsystematically analyze previous HMR methods on camera models to emphasize the\ncritical role of the standard perspective projection model in enabling\nmetric-scale HMR. We then validate the acceptable ambiguity range of metric HMR\nunder the standard perspective projection model. Finally, we contribute a novel\napproach that introduces a ray map based on the standard perspective projection\nto jointly encode bounding-box information, camera parameters, and geometric\ncues for End2End metric HMR without any additional metric-regularization\nmodules. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, even compared with sequential HMR methods, in\nmetric pose, shape, and global translation estimation across both indoor and\nin-the-wild scenarios.", "AI": {"tldr": "MetricHMR introduces a method for accurate metric human mesh recovery from monocular images, addressing scale and depth ambiguity with a novel ray-map approach.", "motivation": "Existing HMR methods struggle with scale and depth ambiguity, limiting their accuracy in global translation and body shape reconstruction.", "method": "The approach uses a standard perspective projection model and introduces a ray map to encode bounding-box info, camera parameters, and geometric cues for End2End metric HMR.", "result": "MetricHMR achieves state-of-the-art performance in metric pose, shape, and global translation estimation across diverse scenarios.", "conclusion": "The method demonstrates the effectiveness of the standard perspective projection model and ray-map encoding for accurate metric HMR."}}
{"id": "2406.06144", "pdf": "https://arxiv.org/pdf/2406.06144", "abs": "https://arxiv.org/abs/2406.06144", "authors": ["Jiaming Ji", "Kaile Wang", "Tianyi Qiu", "Boyuan Chen", "Jiayi Zhou", "Changye Li", "Hantao Lou", "Juntao Dai", "Yunhuai Liu", "Yaodong Yang"], "title": "Language Models Resist Alignment: Evidence From Data Compression", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main", "summary": "Large language models (LLMs) may exhibit unintended or undesirable behaviors.\nRecent works have concentrated on aligning LLMs to mitigate harmful outputs.\nDespite these efforts, some anomalies indicate that even a well-conducted\nalignment process can be easily circumvented, whether intentionally or\naccidentally. Does alignment fine-tuning yield have robust effects on models,\nor are its impacts merely superficial? In this work, we make the first\nexploration of this phenomenon from both theoretical and empirical\nperspectives. Empirically, we demonstrate the $\\mathbf{elasticity}$ of\npost-alignment models, i.e., the tendency to revert to the behavior\ndistribution formed during the pre-training phase upon further fine-tuning.\nLeveraging compression theory, we formally deduce that fine-tuning\ndisproportionately undermines alignment relative to pre-training, potentially\nby orders of magnitude. We validate the presence of elasticity through\nexperiments on models of varying types and scales. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. Furthermore,\nwe further reveal that elasticity positively correlates with the increased\nmodel size and the expansion of pre-training data. Our findings underscore the\nneed to address the inherent elasticity of LLMs to mitigate their resistance to\nalignment. The model weight and code are available at\npku-lm-resist-alignment.github.io.", "AI": {"tldr": "The paper explores the 'elasticity' of post-alignment LLMs, showing they revert to pre-training behaviors upon further fine-tuning, undermining alignment efforts.", "motivation": "To investigate whether alignment fine-tuning has robust effects or superficial impacts, given anomalies in LLM behavior.", "method": "Combines theoretical analysis (compression theory) and empirical experiments on models of varying scales to demonstrate elasticity.", "result": "Alignment is disproportionately undermined by fine-tuning; elasticity correlates with model size and pre-training data.", "conclusion": "Addressing LLM elasticity is crucial to mitigate resistance to alignment."}}
{"id": "2506.09943", "pdf": "https://arxiv.org/pdf/2506.09943", "abs": "https://arxiv.org/abs/2506.09943", "authors": ["Aaron Foss", "Chloe Evans", "Sasha Mitts", "Koustuv Sinha", "Ammar Rizvi", "Justine T. Kao"], "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models", "categories": ["cs.CV", "cs.AI", "I.2.10; I.4.8"], "comment": "35 pages, 3 figures, Submitted to NeurIPS2025 benchmark track", "summary": "We introduce CausalVQA, a benchmark dataset for video question answering\n(VQA) composed of question-answer pairs that probe models' understanding of\ncausality in the physical world. Existing VQA benchmarks either tend to focus\non surface perceptual understanding of real-world videos, or on narrow physical\nreasoning questions created using simulation environments. CausalVQA fills an\nimportant gap by presenting challenging questions that are grounded in\nreal-world scenarios, while focusing on models' ability to predict the likely\noutcomes of different actions and events through five question types:\ncounterfactual, hypothetical, anticipation, planning and descriptive. We\ndesigned quality control mechanisms that prevent models from exploiting trivial\nshortcuts, requiring models to base their answers on deep visual understanding\ninstead of linguistic cues. We find that current frontier multimodal models\nfall substantially below human performance on the benchmark, especially on\nanticipation and hypothetical questions. This highlights a challenge for\ncurrent systems to leverage spatial-temporal reasoning, understanding of\nphysical principles, and comprehension of possible alternatives to make\naccurate predictions in real-world settings.", "AI": {"tldr": "CausalVQA is a new VQA benchmark dataset focusing on causality in real-world videos, challenging models with five question types and revealing gaps in current multimodal models.", "motivation": "Existing VQA benchmarks lack focus on causality in real-world scenarios, limiting models' ability to predict outcomes of actions and events.", "method": "CausalVQA introduces five question types (counterfactual, hypothetical, anticipation, planning, descriptive) with quality controls to prevent shortcuts.", "result": "Current multimodal models perform significantly below humans, especially on anticipation and hypothetical questions.", "conclusion": "CausalVQA highlights the need for improved spatial-temporal reasoning and physical understanding in models for real-world predictions."}}
{"id": "2506.09923", "pdf": "https://arxiv.org/pdf/2506.09923", "abs": "https://arxiv.org/abs/2506.09923", "authors": ["Liou Tang", "James Joshi", "Ashish Kundu"], "title": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning", "categories": ["cs.LG"], "comment": null, "summary": "Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples.", "AI": {"tldr": "The paper introduces Apollo, a label-only membership inference attack for Machine Unlearning (MU), which infers unlearned samples under a strict threat model.", "motivation": "Existing MU privacy attacks rely on weaker threat models, limiting real-world applicability. Apollo addresses this by requiring only label-output access.", "method": "Proposes Apollo, a posteriori label-only attack, to infer unlearned samples without needing the original model.", "result": "Apollo achieves high precision in identifying unlearned samples despite stricter access constraints.", "conclusion": "Apollo demonstrates effective privacy threats under realistic conditions, highlighting vulnerabilities in MU systems."}}
{"id": "2506.09920", "pdf": "https://arxiv.org/pdf/2506.09920", "abs": "https://arxiv.org/abs/2506.09920", "authors": ["Jianhan Qi", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) clustering assigns similar pixels to the same class\nwithout any annotations, which is an important yet challenging task. For\nlarge-scale HSIs, most methods rely on superpixel segmentation and perform\nsuperpixel-level clustering based on graph neural networks (GNNs). However,\nexisting GNNs cannot fully exploit the spectral information of the input HSI,\nand the inaccurate superpixel topological graph may lead to the confusion of\ndifferent class semantics during information aggregation. To address these\nchallenges, we first propose a structural-spectral graph convolutional operator\n(SSGCO) tailored for graph-structured HSI superpixels to improve their\nrepresentation quality through the co-extraction of spatial and spectral\nfeatures. Second, we propose an evidence-guided adaptive edge learning (EGAEL)\nmodule that adaptively predicts and refines edge weights in the superpixel\ntopological graph. We integrate the proposed method into a contrastive learning\nframework to achieve clustering, where representation learning and clustering\nare simultaneously conducted. Experiments demonstrate that the proposed method\nimproves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best\ncompared methods on four HSI datasets. Our code is available at\nhttps://github.com/jhqi/SSGCO-EGAEL.", "AI": {"tldr": "The paper introduces a method for hyperspectral image clustering using a structural-spectral graph convolutional operator and evidence-guided adaptive edge learning to improve accuracy.", "motivation": "Existing methods for hyperspectral image clustering struggle with exploiting spectral information and inaccurate superpixel graphs, leading to semantic confusion.", "method": "Proposes SSGCO for spatial-spectral feature extraction and EGAEL for adaptive edge refinement, integrated into a contrastive learning framework.", "result": "Achieves 2.61% to 6.06% higher clustering accuracy on four datasets compared to existing methods.", "conclusion": "The proposed SSGCO and EGAEL effectively enhance clustering performance by better utilizing spectral and spatial features."}}
{"id": "2406.08726", "pdf": "https://arxiv.org/pdf/2406.08726", "abs": "https://arxiv.org/abs/2406.08726", "authors": ["Genevieve Smith", "Eve Fleisig", "Madeline Bossi", "Ishita Rustagi", "Xavier Yin"], "title": "Standard Language Ideology in AI-Generated Language", "categories": ["cs.CL"], "comment": null, "summary": "Standard language ideology is reflected and reinforced in language generated\nby large language models (LLMs). We present a faceted taxonomy of open problems\nthat illustrate how standard language ideology manifests in AI-generated\nlanguage, alongside implications for minoritized language communities and\nsociety more broadly. We introduce the concept of standard AI-generated\nlanguage ideology, a process through which LLMs position \"standard\"\nlanguages--particularly Standard American English (SAE)--as the linguistic\ndefault, reinforcing the perception that SAE is the most \"appropriate\"\nlanguage. We then discuss ongoing tensions around what constitutes desirable\nsystem behavior, as well as advantages and drawbacks of generative AI tools\nattempting, or refusing, to imitate different English language varieties.\nRather than prescribing narrow technical fixes, we offer three recommendations\nfor researchers, practitioners, and funders that focus on shifting structural\nconditions and supporting more emancipatory outcomes for diverse language\ncommunities.", "AI": {"tldr": "The paper explores how large language models (LLMs) reinforce standard language ideology, particularly Standard American English (SAE), and its impact on minoritized communities. It proposes a taxonomy of issues and recommendations for more inclusive AI language generation.", "motivation": "To highlight how LLMs perpetuate standard language ideology, disadvantaging minoritized language communities, and to advocate for structural changes in AI development.", "method": "The authors present a faceted taxonomy of problems and introduce the concept of 'standard AI-generated language ideology,' analyzing its implications.", "result": "LLMs reinforce SAE as the linguistic default, creating tensions around desirable system behavior and the imitation of diverse English varieties.", "conclusion": "The paper recommends structural shifts in AI research and funding to support more inclusive and emancipatory outcomes for diverse language communities."}}
{"id": "2506.09952", "pdf": "https://arxiv.org/pdf/2506.09952", "abs": "https://arxiv.org/abs/2506.09952", "authors": ["Ziyi Wang", "Yanran Zhang", "Jie Zhou", "Jiwen Lu"], "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2025", "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", "AI": {"tldr": "UniPre3D is the first unified pre-training method for point clouds of any scale and 3D models of any architecture, using Gaussian primitives and differentiable rendering for pixel-level supervision.", "motivation": "Addressing the lack of unified 3D models and pre-training methods effective for both object- and scene-level point clouds.", "method": "Predicts Gaussian primitives, uses differentiable Gaussian splatting for rendering, and integrates 2D features from pre-trained image models.", "result": "Validated through extensive experiments across diverse object- and scene-level tasks.", "conclusion": "UniPre3D offers a universal and effective pre-training solution for 3D vision tasks."}}
{"id": "2506.09928", "pdf": "https://arxiv.org/pdf/2506.09928", "abs": "https://arxiv.org/abs/2506.09928", "authors": ["Ruixuan Xu", "Xiangxiang Weng"], "title": "Bayesian Probabilistic Matrix Factorization", "categories": ["cs.LG", "stat.ML"], "comment": "11 pages, 4 figures", "summary": "Matrix factorization is a widely used technique in recommendation systems.\nProbabilistic Matrix Factorization (PMF) [1] extends traditional matrix\nfactorization by incorporating probability distributions over latent factors,\nallowing for uncertainty quantification. However, computing the posterior\ndistribution is intractable due to the high-dimensional integral. To address\nthis, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)\n[2] and Variational Inference (VI) [3] to approximate the posterior. We\nevaluate their performance on MovieLens dataset and compare their convergence\nspeed, predictive accuracy, and computational efficiency. Experimental results\ndemonstrate that VI offers faster convergence, while MCMC provides more\naccurate posterior estimates.", "AI": {"tldr": "The paper compares MCMC and VI for approximating the posterior in Probabilistic Matrix Factorization, finding VI faster but MCMC more accurate.", "motivation": "Address the intractability of computing the posterior distribution in PMF by comparing Bayesian inference methods.", "method": "Uses MCMC and VI to approximate the posterior in PMF, evaluated on the MovieLens dataset.", "result": "VI converges faster, but MCMC provides more accurate posterior estimates.", "conclusion": "VI is efficient for speed, while MCMC is preferable for accuracy in PMF."}}
{"id": "2506.09935", "pdf": "https://arxiv.org/pdf/2506.09935", "abs": "https://arxiv.org/abs/2506.09935", "authors": ["Jiangyong Huang", "Xiaojian Ma", "Xiongkun Linghu", "Yue Fan", "Junchao He", "Wenxin Tan", "Qing Li", "Song-Chun Zhu", "Yixin Chen", "Baoxiong Jia", "Siyuan Huang"], "title": "LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation", "categories": ["cs.CV"], "comment": "Project page: https://leo-vl.github.io", "summary": "Developing 3D-VL generalists capable of understanding 3D scenes and following\nnatural language instructions to perform a wide range of tasks has been a\nlong-standing goal in the 3D-VL community. Despite recent progress, 3D-VL\nmodels still lag behind their 2D counterparts in capability and robustness,\nfalling short of the generalist standard. A key obstacle to developing 3D-VL\ngeneralists lies in data scalability, hindered by the lack of an efficient\nscene representation. We propose LEO-VL, a 3D-VL model built upon condensed\nfeature grid (CFG), an efficient scene representation that bridges 2D\nperception and 3D spatial structure while significantly reducing token\noverhead. This efficiency unlocks large-scale training towards 3D-VL\ngeneralist, for which we curate over 700k high-quality 3D-VL data spanning four\ndomains of real-world indoor scenes and five tasks such as captioning and\ndialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA\nbenchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the\nefficiency of our representation, the importance of task and scene diversity,\nand the validity of our data curation principle. Furthermore, we introduce\nSceneDPO, a novel post-training objective that enhances the robustness of 3D-VL\nmodels. We hope our findings contribute to the advancement of scalable and\nrobust 3D-VL generalists.", "AI": {"tldr": "LEO-VL, a 3D-VL model using condensed feature grids (CFG), improves efficiency and scalability for 3D vision-language tasks, achieving SOTA on benchmarks like SQA3D.", "motivation": "Addressing the gap in 3D-VL models' capability and robustness compared to 2D models, hindered by inefficient scene representation and data scalability.", "method": "Proposes LEO-VL with CFG for efficient scene representation, enabling large-scale training with 700k 3D-VL data across tasks like captioning and dialogue. Introduces SceneDPO for robustness.", "result": "Achieves state-of-the-art on benchmarks (SQA3D, MSQA, Beacon3D). Ablations confirm CFG efficiency, task diversity importance, and data curation validity.", "conclusion": "LEO-VL advances scalable and robust 3D-VL generalists, with SceneDPO enhancing model robustness."}}
{"id": "2406.14230", "pdf": "https://arxiv.org/pdf/2406.14230", "abs": "https://arxiv.org/abs/2406.14230", "authors": ["Han Jiang", "Xiaoyuan Yi", "Zhihua Wei", "Ziang Xiao", "Shu Wang", "Xing Xie"], "title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "ICML 2025", "summary": "Warning: Contains harmful model outputs. Despite significant advancements,\nthe propensity of Large Language Models (LLMs) to generate harmful and\nunethical content poses critical challenges. Measuring value alignment of LLMs\nbecomes crucial for their regulation and responsible deployment. Although\nnumerous benchmarks have been constructed to assess social bias, toxicity, and\nethical issues in LLMs, those static benchmarks suffer from evaluation\nchronoeffect, in which, as models rapidly evolve, existing benchmarks may leak\ninto training data or become saturated, overestimating ever-developing LLMs. To\ntackle this problem, we propose GETA, a novel generative evolving testing\napproach based on adaptive testing methods in measurement theory. Unlike\ntraditional adaptive testing methods that rely on a static test item pool, GETA\nprobes the underlying moral boundaries of LLMs by dynamically generating test\nitems tailored to model capability. GETA co-evolves with LLMs by learning a\njoint distribution of item difficulty and model value conformity, thus\neffectively addressing evaluation chronoeffect. We evaluated various popular\nLLMs with GETA and demonstrated that 1) GETA can dynamically create\ndifficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms.", "AI": {"tldr": "GETA is a generative evolving testing approach to dynamically assess LLMs' value alignment, addressing the limitations of static benchmarks.", "motivation": "Static benchmarks for evaluating LLMs' ethical alignment suffer from evaluation chronoeffect, leading to outdated or saturated assessments as models evolve.", "method": "GETA uses adaptive testing to dynamically generate test items tailored to model capabilities, learning a joint distribution of item difficulty and model conformity.", "result": "GETA effectively creates difficulty-tailored test items and provides evaluations consistent with models' performance on unseen data.", "conclusion": "GETA offers a robust, evolving framework for assessing LLMs' value alignment, addressing the shortcomings of static benchmarks."}}
{"id": "2506.09954", "pdf": "https://arxiv.org/pdf/2506.09954", "abs": "https://arxiv.org/abs/2506.09954", "authors": ["Ziyi Wang", "Yongming Rao", "Shuofeng Sun", "Xinrun Liu", "Yi Wei", "Xumin Yu", "Zuyan Liu", "Yanbo Wang", "Hongmin Liu", "Jie Zhou", "Jiwen Lu"], "title": "Vision Generalist Model: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by International Journal of Computer Vision (IJCV)", "summary": "Recently, we have witnessed the great success of the generalist model in\nnatural language processing. The generalist model is a general framework\ntrained with massive data and is able to process various downstream tasks\nsimultaneously. Encouraged by their impressive performance, an increasing\nnumber of researchers are venturing into the realm of applying these models to\ncomputer vision tasks. However, the inputs and outputs of vision tasks are more\ndiverse, and it is difficult to summarize them as a unified representation. In\nthis paper, we provide a comprehensive overview of the vision generalist\nmodels, delving into their characteristics and capabilities within the field.\nFirst, we review the background, including the datasets, tasks, and benchmarks.\nThen, we dig into the design of frameworks that have been proposed in existing\nresearch, while also introducing the techniques employed to enhance their\nperformance. To better help the researchers comprehend the area, we take a\nbrief excursion into related domains, shedding light on their interconnections\nand potential synergies. To conclude, we provide some real-world application\nscenarios, undertake a thorough examination of the persistent challenges, and\noffer insights into possible directions for future research endeavors.", "AI": {"tldr": "The paper reviews vision generalist models, their design, techniques, and applications, while addressing challenges and future directions.", "motivation": "The success of generalist models in NLP inspires their application to diverse vision tasks, despite challenges in unifying representations.", "method": "The paper reviews datasets, tasks, benchmarks, framework designs, and performance-enhancing techniques, with insights from related domains.", "result": "A comprehensive overview of vision generalist models, their capabilities, and real-world applications is provided.", "conclusion": "Challenges in vision generalist models are examined, with suggestions for future research directions."}}
{"id": "2506.09955", "pdf": "https://arxiv.org/pdf/2506.09955", "abs": "https://arxiv.org/abs/2506.09955", "authors": ["Yitao Xu", "Tong Zhang", "Ehsan Pajouheshgar", "Sabine S\u00fcsstrunk"], "title": "Canonical Latent Representations in Conditional Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "45 pages,41 figures", "summary": "Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.", "AI": {"tldr": "The paper introduces CLAReps, latent codes in CDMs that preserve class-defining features while discarding irrelevant context, enabling robust and interpretable representations. A distillation method, CaDistill, uses CLAReps to transfer core class knowledge efficiently, improving student model robustness and generalization.", "motivation": "CDMs entangle class-defining features with irrelevant context, making it hard to extract robust and interpretable representations. The goal is to disentangle these features for better discriminative learning.", "method": "Proposes CLAReps (Canonical LAtent Representations) to extract essential categorical information from CDMs. Develops CaDistill, a diffusion-based feature-distillation paradigm using CLAReps as a compact teacher signal.", "result": "CLAReps produce interpretable and compact class summaries. CaDistill achieves strong adversarial robustness and generalization, focusing on class signals rather than spurious cues.", "conclusion": "CDMs can serve as compact, interpretable teachers for robust representation learning, not just as image generators."}}
{"id": "2506.09958", "pdf": "https://arxiv.org/pdf/2506.09958", "abs": "https://arxiv.org/abs/2506.09958", "authors": ["Sushant Gautam", "Michael A. Riegler", "P\u00e5l Halvorsen"], "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "categories": ["cs.CV", "cs.LG", "68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing) 68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing)", "I.2.10; I.2.6; J.3"], "comment": null, "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "AI": {"tldr": "Kvasir-VQA-x1 is a new, large-scale dataset for GI endoscopy, enhancing clinical complexity and visual diversity to improve MedVQA systems.", "motivation": "Addressing limitations in existing datasets by providing deeper clinical reasoning and visual diversity for better clinical decision support.", "method": "Systematic use of large language models to generate 159,549 question-answer pairs, stratified by complexity, and visual augmentations for robustness testing.", "result": "A dataset supporting two evaluation tracks: standard VQA performance and robustness against visual perturbations.", "conclusion": "Kvasir-VQA-x1 aims to advance reliable multimodal AI for clinical use, adhering to FAIR principles and being openly accessible."}}
{"id": "2406.17761", "pdf": "https://arxiv.org/pdf/2406.17761", "abs": "https://arxiv.org/abs/2406.17761", "authors": ["Shane Arora", "Marzena Karpinska", "Hung-Ting Chen", "Ipsita Bhattacharjee", "Mohit Iyyer", "Eunsol Choi"], "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "46 pages, 26 figures. Accepted as a main conference paper at ACL\n  2025. Code and data available at https://github.com/2015aroras/CaLMQA .\n  Dataset expanded to 51.7K questions", "summary": "Despite rising global usage of large language models (LLMs), their ability to\ngenerate long-form answers to culturally specific questions remains unexplored\nin many languages. To fill this gap, we perform the first study of textual\nmultilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally\nspecific questions across 23 different languages. We define culturally specific\nquestions as those that refer to concepts unique to one or a few cultures, or\nhave different answers depending on the cultural or regional context. We obtain\nthese questions by crawling naturally-occurring questions from community web\nforums in high-resource languages, and by hiring native speakers to write\nquestions in under-resourced, rarely-studied languages such as Fijian and\nKirundi. Our data collection methodologies are translation-free, enabling the\ncollection of culturally unique questions like \"Kuber iki umwami wa mbere\nw'uburundi yitwa Ntare?\" (Kirundi; English translation: \"Why was the first king\nof Burundi called Ntare (Lion)?\"). We evaluate factuality, relevance and\nsurface-level quality of LLM-generated long-form answers, finding that (1) for\nmany languages, even the best models make critical surface-level errors (e.g.,\nanswering in the wrong language, repetition), especially for low-resource\nlanguages; and (2) answers to culturally specific questions contain more\nfactual errors than answers to culturally agnostic questions -- questions that\nhave consistent meaning and answer across many cultures. We release CaLMQA to\nfacilitate future research in cultural and multilingual long-form QA.", "AI": {"tldr": "The paper introduces CaLMQA, a multilingual dataset for culturally specific long-form QA, highlighting LLMs' limitations in handling such questions, especially in low-resource languages.", "motivation": "To address the unexplored ability of LLMs in generating long-form answers to culturally specific questions across diverse languages.", "method": "Created CaLMQA (51.7K questions in 23 languages) by crawling community forums and hiring native speakers for under-resourced languages. Evaluated LLM answers for factuality, relevance, and quality.", "result": "LLMs exhibit critical errors (e.g., wrong language, repetition) in low-resource languages and more factual errors in culturally specific questions.", "conclusion": "CaLMQA enables future research in multilingual and culturally aware QA, revealing LLMs' current limitations."}}
{"id": "2506.09956", "pdf": "https://arxiv.org/pdf/2506.09956", "abs": "https://arxiv.org/abs/2506.09956", "authors": ["Sahar Abdelnabi", "Aideen Fay", "Ahmed Salem", "Egor Zverev", "Kai-Chieh Liao", "Chi-Huang Liu", "Chun-Chih Kuo", "Jannis Weigend", "Danyael Manlangit", "Alex Apostolov", "Haris Umair", "Jo\u00e3o Donato", "Masayuki Kawakita", "Athar Mahboob", "Tran Huu Bach", "Tsun-Han Chiang", "Myeongjin Cho", "Hajin Choi", "Byeonghyeon Kim", "Hyeonjin Lee", "Benjamin Pannell", "Conor McCauley", "Mark Russinovich", "Andrew Paverd", "Giovanni Cherubin"], "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge", "categories": ["cs.CR", "cs.AI"], "comment": "Dataset at:\n  https://huggingface.co/datasets/microsoft/llmail-inject-challenge", "summary": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection.", "AI": {"tldr": "The paper evaluates indirect prompt injection attacks on LLMs, highlighting vulnerabilities in real-world applications. It presents LLMail-Inject, a public challenge dataset, to study adaptive attacks and defense strategies.", "motivation": "To address the lack of systematic evaluation of LLM defenses against adaptive prompt injection attacks, which pose significant security and privacy risks.", "method": "Conducted LLMail-Inject, a public challenge simulating email-based attacks on LLM assistants, involving diverse defenses, architectures, and retrieval configurations.", "result": "Collected 208,095 unique attack submissions from 839 participants, providing insights into instruction-data separation vulnerabilities.", "conclusion": "The dataset and analysis aim to guide future research for practical solutions to prompt injection, emphasizing the need for structural defenses."}}
{"id": "2506.09991", "pdf": "https://arxiv.org/pdf/2506.09991", "abs": "https://arxiv.org/abs/2506.09991", "authors": ["Xinyu Yang", "Yuwei An", "Hongyi Liu", "Tianqi Chen", "Beidi Chen"], "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation", "categories": ["cs.LG"], "comment": null, "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.", "AI": {"tldr": "Multiverse is a parallel generative model inspired by AR-LLMs, using a MapReduce paradigm for efficient, scalable, and high-performance generation without human annotations.", "motivation": "To overcome the sequential limitations of AR-LLMs by enabling native parallelism in generation, improving efficiency and performance.", "method": "Uses a three-stage MapReduce approach (Map, Process, Reduce) and co-designs data, algorithm (Multiverse Attention), and system (Multiverse Engine) for parallel inference.", "result": "Multiverse-32B matches leading AR-LLMs in performance (AIME24 & 25 scores of 54% and 46%) and achieves 2x speedup with superior scaling.", "conclusion": "Multiverse offers a scalable, efficient alternative to AR-LLMs, with open-sourced tools and data for broader adoption."}}
{"id": "2506.09965", "pdf": "https://arxiv.org/pdf/2506.09965", "abs": "https://arxiv.org/abs/2506.09965", "authors": ["Junfei Wu", "Jian Guan", "Kaituo Feng", "Qiang Liu", "Shu Wu", "Liang Wang", "Wei Wu", "Tieniu Tan"], "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "categories": ["cs.CV", "cs.AI", "I.2"], "comment": null, "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "AI": {"tldr": "The paper introduces 'drawing to reason in space,' a novel paradigm for LVLMs to enhance spatial reasoning by using visual drawing operations, achieving significant performance improvements.", "motivation": "Existing text-centric multimodal reasoning methods lack precise geometric understanding and spatial tracking, limiting performance in spatial tasks.", "method": "The proposed method equips LVLMs with drawing operations (e.g., bounding boxes, auxiliary lines) and uses a three-stage training framework: cold-start training, reflective rejection sampling, and reinforcement learning.", "result": "The model, VILASR, outperforms existing methods by 18.4% on average across diverse spatial reasoning benchmarks.", "conclusion": "Visual drawing operations significantly enhance LVLMs' spatial reasoning, overcoming limitations of text-centric approaches."}}
{"id": "2407.13329", "pdf": "https://arxiv.org/pdf/2407.13329", "abs": "https://arxiv.org/abs/2407.13329", "authors": ["Lorenzo Paolini", "Sahar Vahdati", "Angelo Di Iorio", "Robert Wardenga", "Ivan Heibi", "Silvio Peroni"], "title": "CiteFusion: An Ensemble Framework for Citation Intent Classification Harnessing Dual-Model Binary Couples and SHAP Analyses", "categories": ["cs.CL"], "comment": "Submitted to Scientometrics Journal", "summary": "Understanding the motivations underlying scholarly citations is essential to\nevaluate research impact and pro-mote transparent scholarly communication. This\nstudy introduces CiteFusion, an ensemble framework designed to address the\nmulti-class Citation Intent Classification task on two benchmark datasets:\nSciCite and ACL-ARC. The framework employs a one-vs-all decomposition of the\nmulti-class task into class-specific binary sub-tasks, leveraging complementary\npairs of SciBERT and XLNet models, independently tuned, for each citation\nintent. The outputs of these base models are aggregated through a feedforward\nneural network meta-classifier to reconstruct the original classification task.\nTo enhance interpretability, SHAP (SHapley Additive exPlanations) is employed\nto analyze token-level contributions, and interactions among base models,\nproviding transparency into the classification dynamics of CiteFusion, and\ninsights about the kind of misclassifications of the ensem-ble. In addition,\nthis work investigates the semantic role of structural context by incorporating\nsection titles, as framing devices, into input sentences, assessing their\npositive impact on classification accuracy. CiteFusion ul-timately demonstrates\nrobust performance in imbalanced and data-scarce scenarios: experimental\nresults show that CiteFusion achieves state-of-the-art performance, with\nMacro-F1 scores of 89.60% on SciCite, and 76.24% on ACL-ARC. Furthermore, to\nensure interoperability and reusability, citation intents from both datasets\nsche-mas are mapped to Citation Typing Ontology (CiTO) object properties,\nhighlighting some overlaps. Finally, we describe and release a web-based\napplication that classifies citation intents leveraging the CiteFusion models\ndeveloped on SciCite.", "AI": {"tldr": "CiteFusion is an ensemble framework for multi-class Citation Intent Classification, using SciBERT and XLNet models, achieving state-of-the-art performance on SciCite and ACL-ARC datasets. It enhances interpretability with SHAP and investigates the role of structural context.", "motivation": "To evaluate research impact and promote transparent scholarly communication by understanding citation motivations.", "method": "One-vs-all decomposition into binary sub-tasks, leveraging SciBERT and XLNet models, aggregated via a neural network meta-classifier. SHAP is used for interpretability, and section titles are incorporated for context.", "result": "Achieves Macro-F1 scores of 89.60% on SciCite and 76.24% on ACL-ARC, demonstrating robustness in imbalanced and data-scarce scenarios.", "conclusion": "CiteFusion provides a high-performance, interpretable solution for citation intent classification, with practical applications like a released web-based tool."}}
{"id": "2506.09988", "pdf": "https://arxiv.org/pdf/2506.09988", "abs": "https://arxiv.org/abs/2506.09988", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "AI": {"tldr": "EditInspector is a benchmark for evaluating text-guided image edits, revealing current models' limitations and proposing improved methods.", "motivation": "The rise of text-guided image editing necessitates a framework to verify and assess edit quality.", "method": "Introduces EditInspector, a benchmark based on human annotations, and evaluates SoTA models across multiple dimensions.", "result": "Current models perform poorly in comprehensive edit evaluation and hallucinate in change descriptions. Proposed methods outperform SoTA in artifact detection and difference captioning.", "conclusion": "EditInspector highlights gaps in current models and offers better solutions for evaluating text-guided edits."}}
{"id": "2505.14156", "pdf": "https://arxiv.org/pdf/2505.14156", "abs": "https://arxiv.org/abs/2505.14156", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "I.2; H.3.3"], "comment": null, "summary": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.", "AI": {"tldr": "SGR combines text-based and graph-based approaches using LLMs for session search, outperforming benchmarks.", "motivation": "Current methods lack integration of graph structures and word-level semantics in session search.", "method": "SGR converts session graphs to text using symbolic grammar, then uses LLMs with self-supervised tasks for graph understanding.", "result": "Superior performance on AOL and Tiangong-ST datasets.", "conclusion": "SGR bridges traditional search and modern LLMs, offering a novel methodology."}}
{"id": "2506.09969", "pdf": "https://arxiv.org/pdf/2506.09969", "abs": "https://arxiv.org/abs/2506.09969", "authors": ["Jeripothula Prudviraj", "Vikram Jamwal"], "title": "Vectorized Region Based Brush Strokes for Artistic Rendering", "categories": ["cs.CV", "I.3.3; I.5"], "comment": null, "summary": "Creating a stroke-by-stroke evolution process of a visual artwork tries to\nbridge the emotional and educational gap between the finished static artwork\nand its creation process. Recent stroke-based painting systems focus on\ncapturing stroke details by predicting and iteratively refining stroke\nparameters to maximize the similarity between the input image and the rendered\noutput. However, these methods often struggle to produce stroke compositions\nthat align with artistic principles and intent. To address this, we explore an\nimage-to-painting method that (i) facilitates semantic guidance for brush\nstrokes in targeted regions, (ii) computes the brush stroke parameters, and\n(iii) establishes a sequence among segments and strokes to sequentially render\nthe final painting. Experimental results on various input image types, such as\nface images, paintings, and photographic images, show that our method aligns\nwith a region-based painting strategy while rendering a painting with high\nfidelity and superior stroke quality.", "AI": {"tldr": "The paper introduces an image-to-painting method that improves stroke quality and alignment with artistic principles by incorporating semantic guidance, stroke parameter computation, and sequential rendering.", "motivation": "To bridge the gap between static artwork and its creation process, addressing limitations of current stroke-based painting systems that often fail to align with artistic intent.", "method": "The method uses semantic guidance for targeted regions, computes brush stroke parameters, and establishes a sequential rendering process for strokes.", "result": "Experiments show the method produces high-fidelity paintings with superior stroke quality across various image types (faces, paintings, photos).", "conclusion": "The proposed method effectively aligns with artistic principles and enhances the painting process through structured stroke rendering."}}
{"id": "2408.04211", "pdf": "https://arxiv.org/pdf/2408.04211", "abs": "https://arxiv.org/abs/2408.04211", "authors": ["Jiahao Tian", "Jinman Zhao", "Zhenkai Wang", "Zhicheng Ding"], "title": "MMREC: LLM Based Multi-Modal Recommender System", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The importance of recommender systems is growing rapidly due to the\nexponential increase in the volume of content generated daily. This surge in\ncontent presents unique challenges for designing effective recommender systems.\nKey among these challenges is the need to effectively leverage the vast amounts\nof natural language data and images that represent user preferences. This paper\npresents a novel approach to enhancing recommender systems by leveraging Large\nLanguage Models (LLMs) and deep learning techniques. The proposed framework\naims to improve the accuracy and relevance of recommendations by incorporating\nmulti-modal information processing and by the use of unified latent space\nrepresentation. The study explores the potential of LLMs to better understand\nand utilize natural language data in recommendation contexts, addressing the\nlimitations of previous methods. The framework efficiently extracts and\nintegrates text and image information through LLMs, unifying diverse modalities\nin a latent space to simplify the learning process for the ranking model.\nExperimental results demonstrate the enhanced discriminative power of the model\nwhen utilizing multi-modal information. This research contributes to the\nevolving field of recommender systems by showcasing the potential of LLMs and\nmulti-modal data integration to create more personalized and contextually\nrelevant recommendations.", "AI": {"tldr": "A novel framework using LLMs and deep learning improves recommender systems by integrating multi-modal data (text and images) into a unified latent space, enhancing recommendation accuracy and relevance.", "motivation": "The exponential growth of content and the need to leverage natural language and image data for better user preference understanding drive this research.", "method": "The proposed framework uses LLMs and deep learning to process multi-modal data, unifying it in a latent space for simplified learning in the ranking model.", "result": "Experiments show the model's improved discriminative power with multi-modal data, leading to more accurate and relevant recommendations.", "conclusion": "The study highlights the potential of LLMs and multi-modal integration to advance recommender systems, offering personalized and context-aware recommendations."}}
{"id": "2506.09993", "pdf": "https://arxiv.org/pdf/2506.09993", "abs": "https://arxiv.org/abs/2506.09993", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "title": "Text-Aware Image Restoration with Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://cvlab-kaist.github.io/TAIR/", "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "AI": {"tldr": "TAIR introduces a novel image restoration task focusing on textual fidelity, proposing TeReDiff, a multi-task diffusion framework, and SA-Text, a large-scale benchmark. It outperforms existing methods in text recognition accuracy.", "motivation": "Existing diffusion-based methods struggle with textual fidelity in degraded images, often generating incorrect text-like patterns (text-image hallucination).", "method": "Proposes TeReDiff, a multi-task diffusion framework integrating diffusion model features with a text-spotting module for joint training and rich text representation extraction.", "result": "Outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy.", "conclusion": "TAIR and TeReDiff effectively address text-image hallucination, improving both visual and textual restoration."}}
{"id": "2506.06905", "pdf": "https://arxiv.org/pdf/2506.06905", "abs": "https://arxiv.org/abs/2506.06905", "authors": ["Akash Gupta", "Amos Storkey", "Mirella Lapata"], "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\nperform new tasks with minimal supervision. However, ICL performance,\nespecially in smaller LMMs, is inconsistent and does not always improve\nmonotonically with increasing examples. We hypothesize that this occurs due to\nthe LMM being overwhelmed by additional information present in the image\nembeddings, which is not required for the downstream task. To address this, we\npropose a meta-learning approach that provides an alternative for inducing\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\ndistilled from task-relevant image features and can be adapted at test time\nusing a few examples. To facilitate this distillation, we introduce an\nattention-mapper module that can be easily integrated with the popular LLaVA\nv1.5 architecture and is jointly learned with soft prompts, enabling task\nadaptation in LMMs under low-data regimes with just a few gradient steps.\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\nICL and related prompt-tuning approaches, even under image perturbations,\nimproving task induction and reasoning across visual question answering tasks.", "AI": {"tldr": "A meta-learning approach improves few-shot learning in LMMs by distilling task-relevant image features into soft prompts, outperforming ICL and prompt-tuning methods.", "motivation": "Inconsistent ICL performance in smaller LMMs due to irrelevant image embedding information.", "method": "Proposes a meta-learning approach with soft prompts distilled from task-relevant image features, integrated via an attention-mapper module.", "result": "Outperforms ICL and prompt-tuning on VL-ICL Bench, even under image perturbations.", "conclusion": "The method enhances task induction and reasoning in LMMs under low-data regimes."}}
{"id": "2506.09980", "pdf": "https://arxiv.org/pdf/2506.09980", "abs": "https://arxiv.org/abs/2506.09980", "authors": ["Jiaxiang Tang", "Ruijie Lu", "Zhaoshuo Li", "Zekun Hao", "Xuan Li", "Fangyin Wei", "Shuran Song", "Gang Zeng", "Ming-Yu Liu", "Tsung-Yi Lin"], "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing", "categories": ["cs.CV"], "comment": "Code: https://github.com/NVlabs/PartPacker Project Page:\n  https://research.nvidia.com/labs/dir/partpacker/", "summary": "Recent progress in 3D object generation has greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce a dual volume packing strategy that organizes\nall parts into two complementary volumes, allowing for the creation of complete\nand interleaved parts that assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-based part-level generation methods.", "AI": {"tldr": "Proposes a part-level 3D object generation framework for editable and semantically meaningful parts, improving quality and diversity.", "motivation": "Existing methods generate fused meshes, limiting part-level editing. Addressing the challenge of varying part counts in objects.", "method": "Uses a dual volume packing strategy to organize parts into complementary volumes for complete and interleaved assembly.", "result": "Achieves better quality, diversity, and generalization compared to previous image-based part-level generation methods.", "conclusion": "The framework enables high-quality, part-level 3D object generation from a single image, enhancing editability and semantic meaning."}}
{"id": "2408.14352", "pdf": "https://arxiv.org/pdf/2408.14352", "abs": "https://arxiv.org/abs/2408.14352", "authors": ["Nicolas Yax", "Pierre-Yves Oudeyer", "Stefano Palminteri"], "title": "LogProber: Disentangling confidence from contamination in LLM responses", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical.In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm.", "AI": {"tldr": "LogProber is a new algorithm for detecting contamination in LLMs by focusing on question familiarity, addressing limitations of existing methods.", "motivation": "Contamination in LLMs, where test data leaks into training, hampers fair performance evaluation. Current detection methods for short text sequences are limited.", "method": "LogProber detects contamination in a black-box setting by analyzing familiarity with questions rather than answers.", "result": "The method is efficient and tackles drawbacks of concurrent approaches, though some contamination forms may still evade detection.", "conclusion": "LogProber offers a practical solution for contamination detection, but algorithm design impacts its effectiveness against certain contamination types."}}
{"id": "2506.09994", "pdf": "https://arxiv.org/pdf/2506.09994", "abs": "https://arxiv.org/abs/2506.09994", "authors": ["Venkatesh Pattabiraman", "Zizhou Huang", "Daniele Panozzo", "Denis Zorin", "Lerrel Pinto", "Raunaq Bhirangi"], "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "If human experience is any guide, operating effectively in unstructured\nenvironments -- like homes and offices -- requires robots to sense the forces\nduring physical interaction. Yet, the lack of a versatile, accessible, and\neasily customizable tactile sensor has led to fragmented, sensor-specific\nsolutions in robotic manipulation -- and in many cases, to force-unaware,\nsensorless approaches. With eFlesh, we bridge this gap by introducing a\nmagnetic tactile sensor that is low-cost, easy to fabricate, and highly\ncustomizable. Building an eFlesh sensor requires only four components: a\nhobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired\nshape, and a magnetometer circuit board. The sensor is constructed from tiled,\nparameterized microstructures, which allow for tuning the sensor's geometry and\nits mechanical response. We provide an open-source design tool that converts\nconvex OBJ/STL files into 3D-printable STLs for fabrication. This modular\ndesign framework enables users to create application-specific sensors, and to\nadjust sensitivity depending on the task. Our sensor characterization\nexperiments demonstrate the capabilities of eFlesh: contact localization RMSE\nof 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for\nshear force. We also present a learned slip detection model that generalizes to\nunseen objects with 95% accuracy, and visuotactile control policies that\nimprove manipulation performance by 40% over vision-only baselines -- achieving\n91% average success rate for four precise tasks that require sub-mm accuracy\nfor successful completion. All design files, code and the CAD-to-eFlesh STL\nconversion tool are open-sourced and available on https://e-flesh.com.", "AI": {"tldr": "eFlesh is a low-cost, customizable magnetic tactile sensor for robots, enabling precise force sensing and manipulation in unstructured environments.", "motivation": "The lack of versatile, accessible tactile sensors has led to fragmented or force-unaware robotic solutions, limiting effective interaction in unstructured settings like homes and offices.", "method": "eFlesh uses 3D-printed microstructures with magnets and a magnetometer, offering tunable geometry and mechanical response. An open-source tool converts CAD models into printable designs.", "result": "Achieves 0.5 mm contact localization RMSE, 0.27 N normal force RMSE, and 95% slip detection accuracy. Visuotactile policies improve manipulation by 40%.", "conclusion": "eFlesh bridges the gap in tactile sensing, providing a customizable, open-source solution for precise robotic manipulation."}}
{"id": "2506.09069", "pdf": "https://arxiv.org/pdf/2506.09069", "abs": "https://arxiv.org/abs/2506.09069", "authors": ["Sahaj Raj Malla"], "title": "Devanagari Digit Recognition using Quantum Machine Learning", "categories": ["quant-ph", "cs.CV", "cs.LG", "68T05, 68Q10, 68T07", "I.2.6; I.4.8; F.1.2"], "comment": "9 pages, 4 figures, arXiv preprint, code available upon request", "summary": "Handwritten digit recognition in regional scripts, such as Devanagari, is\ncrucial for multilingual document digitization, educational tools, and the\npreservation of cultural heritage. The script's complex structure and limited\nannotated datasets pose significant challenges to conventional models. This\npaper introduces the first hybrid quantum-classical architecture for Devanagari\nhandwritten digit recognition, combining a convolutional neural network (CNN)\nfor spatial feature extraction with a 10-qubit variational quantum circuit\n(VQC) for quantum-enhanced classification. Trained and evaluated on the\nDevanagari Handwritten Character Dataset (DHCD), the proposed model achieves a\nstate-of-the-art test accuracy for quantum implementation of 99.80% and a test\nloss of 0.2893, with an average per-class F1-score of 0.9980. Compared to\nequivalent classical CNNs, our model demonstrates superior accuracy with\nsignificantly fewer parameters and enhanced robustness. By leveraging quantum\nprinciples such as superposition and entanglement, this work establishes a\nnovel benchmark for regional script recognition, highlighting the promise of\nquantum machine learning (QML) in real-world, low-resource language settings.", "AI": {"tldr": "A hybrid quantum-classical model for Devanagari handwritten digit recognition achieves state-of-the-art accuracy, outperforming classical CNNs with fewer parameters.", "motivation": "Addressing challenges in recognizing complex regional scripts like Devanagari due to limited datasets and structural complexity, while exploring quantum-enhanced solutions.", "method": "Combines a CNN for spatial feature extraction with a 10-qubit variational quantum circuit (VQC) for classification, trained on the Devanagari Handwritten Character Dataset (DHCD).", "result": "Achieves 99.80% test accuracy, 0.2893 test loss, and 0.9980 average F1-score, surpassing classical CNNs in accuracy and robustness.", "conclusion": "Demonstrates quantum machine learning's potential for regional script recognition, offering a novel benchmark for low-resource language applications."}}
{"id": "2506.09981", "pdf": "https://arxiv.org/pdf/2506.09981", "abs": "https://arxiv.org/abs/2506.09981", "authors": ["Jiazhi Yang", "Kashyap Chitta", "Shenyuan Gao", "Long Chen", "Yuqian Shao", "Xiaosong Jia", "Hongyang Li", "Andreas Geiger", "Xiangyu Yue", "Li Chen"], "title": "ReSim: Reliable World Simulation for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://opendrivelab.com/ReSim", "summary": "How can we reliably simulate future driving scenarios under a wide range of\nego driving behaviors? Recent driving world models, developed exclusively on\nreal-world driving data composed mainly of safe expert trajectories, struggle\nto follow hazardous or non-expert behaviors, which are rare in such data. This\nlimitation restricts their applicability to tasks such as policy evaluation. In\nthis work, we address this challenge by enriching real-world human\ndemonstrations with diverse non-expert data collected from a driving simulator\n(e.g., CARLA), and building a controllable world model trained on this\nheterogeneous corpus. Starting with a video generator featuring a diffusion\ntransformer architecture, we devise several strategies to effectively integrate\nconditioning signals and improve prediction controllability and fidelity. The\nresulting model, ReSim, enables Reliable Simulation of diverse open-world\ndriving scenarios under various actions, including hazardous non-expert ones.\nTo close the gap between high-fidelity simulation and applications that require\nreward signals to judge different actions, we introduce a Video2Reward module\nthat estimates a reward from ReSim's simulated future. Our ReSim paradigm\nachieves up to 44% higher visual fidelity, improves controllability for both\nexpert and non-expert actions by over 50%, and boosts planning and policy\nselection performance on NAVSIM by 2% and 25%, respectively.", "AI": {"tldr": "ReSim enhances driving scenario simulation by integrating diverse non-expert data with real-world trajectories, improving fidelity and controllability for hazardous behaviors.", "motivation": "Existing driving world models, trained on safe expert data, fail to simulate hazardous or non-expert behaviors, limiting their use in policy evaluation.", "method": "ReSim combines real-world and simulator data (e.g., CARLA) using a diffusion transformer video generator, with strategies to improve controllability and fidelity.", "result": "ReSim achieves 44% higher visual fidelity, over 50% better controllability, and boosts planning/policy selection by 2% and 25% on NAVSIM.", "conclusion": "ReSim reliably simulates diverse driving scenarios, including hazardous behaviors, and integrates a Video2Reward module for practical applications."}}
{"id": "2408.16326", "pdf": "https://arxiv.org/pdf/2408.16326", "abs": "https://arxiv.org/abs/2408.16326", "authors": ["Xin Zheng", "Jie Lou", "Boxi Cao", "Xueru Wen", "Yuqiu Ji", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Debing Zhang", "Le Sun"], "title": "Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Findings", "summary": "Self-critic has become a crucial mechanism for enhancing the reasoning\nperformance of LLMs. However, current approaches mainly involve basic prompts\nfor intuitive instance-level feedback, which resembles System-1 processes and\nlimits the reasoning capabilities. Moreover, there is a lack of in-depth\ninvestigations into the relationship between LLM's ability to criticize and its\ntask-solving performance. To address these issues, we propose Critic-CoT, a\nnovel framework that pushes LLMs toward System-2-like critic capability.\nThrough a step-wise CoT reasoning paradigm and the automatic construction of\ndistant-supervision data without human annotation, Critic-CoT enables LLMs to\nengage in slow, analytic self-critique and refinement, thereby improving their\nreasoning abilities. Experiments on GSM8K and MATH demonstrate that our\nenhanced model significantly boosts task-solving performance by filtering out\ninvalid solutions or iterative refinement. Furthermore, we investigate the\nintrinsic correlation between critique and task-solving abilities within LLMs,\ndiscovering that these abilities can mutually reinforce each other rather than\nconflict.", "AI": {"tldr": "Critic-CoT enhances LLM reasoning by enabling System-2-like self-critique through step-wise CoT and distant-supervision, improving performance on tasks like GSM8K and MATH.", "motivation": "Current self-critic methods in LLMs rely on basic prompts (System-1-like), limiting reasoning. The relationship between critique and task-solving abilities is underexplored.", "method": "Critic-CoT uses step-wise Chain-of-Thought (CoT) reasoning and distant-supervision data to enable analytic self-critique and refinement.", "result": "Experiments show improved task-solving performance on GSM8K and MATH by filtering invalid solutions and iterative refinement.", "conclusion": "Critique and task-solving abilities in LLMs can mutually reinforce each other, enhancing overall reasoning performance."}}
{"id": "2506.09997", "pdf": "https://arxiv.org/pdf/2506.09997", "abs": "https://arxiv.org/abs/2506.09997", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page: https://hubert0527.github.io/dgslrm/", "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "AI": {"tldr": "DGS-LRM is a feed-forward method for predicting deformable 3D Gaussian splats from monocular videos of dynamic scenes, outperforming existing methods in reconstruction quality and tracking.", "motivation": "Existing feed-forward models are limited to static scenes, lacking the ability to reconstruct dynamic objects. Challenges include scarce training data and the need for suitable 3D representations.", "method": "Key contributions: a synthetic dataset with ground-truth multi-view videos and 3D scene flow, a deformable 3D Gaussian representation, and a transformer network for real-time reconstruction.", "result": "DGS-LRM matches optimization-based methods in quality and outperforms predictive dynamic reconstruction methods, with accurate 3D deformation and tracking performance.", "conclusion": "DGS-LRM advances dynamic scene reconstruction, offering high-quality synthesis and tracking, bridging the gap between feed-forward and optimization-based approaches."}}
{"id": "2506.09075", "pdf": "https://arxiv.org/pdf/2506.09075", "abs": "https://arxiv.org/abs/2506.09075", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "AI": {"tldr": "A simple Transformer-based framework for motion in-betweening challenges the need for complex models, emphasizing data modeling choices like volume, pose representation, and velocity features.", "motivation": "To simplify motion in-betweening by reducing reliance on complex models and highlighting the importance of data-centric approaches.", "method": "Uses a single Transformer encoder, focusing on data volume, pose representation, and velocity input features.", "result": "Demonstrates that data choices (volume, pose, velocity) can outperform complex models in motion quality.", "conclusion": "Model complexity isn't key; data-centric approaches like volume, pose, and velocity features are crucial for high-quality motion in-betweening."}}
{"id": "2506.09982", "pdf": "https://arxiv.org/pdf/2506.09982", "abs": "https://arxiv.org/abs/2506.09982", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "categories": ["cs.CV"], "comment": "Project Page: https://animateanymesh.github.io/AnimateAnyMesh/", "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "AI": {"tldr": "AnimateAnyMesh is a feed-forward framework for text-driven animation of 3D meshes, using DyMeshVAE and Rectified Flow for efficient, high-quality generation.", "motivation": "Creating high-quality animated 3D models is challenging due to spatio-temporal complexity and lack of 4D training data.", "method": "Leverages DyMeshVAE to compress/reconstruct dynamic mesh sequences and Rectified Flow for text-conditional generation.", "result": "Generates semantically accurate, temporally coherent animations in seconds, outperforming existing methods.", "conclusion": "Advances 4D content creation accessibility; data, code, and models will be open-released."}}
{"id": "2409.00598", "pdf": "https://arxiv.org/pdf/2409.00598", "abs": "https://arxiv.org/abs/2409.00598", "authors": ["Bang An", "Sicheng Zhu", "Ruiyi Zhang", "Michael-Andrei Panaitescu-Liess", "Yuancheng Xu", "Furong Huang"], "title": "Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models", "categories": ["cs.CL", "cs.CR", "cs.CY", "cs.LG"], "comment": null, "summary": "Safety-aligned large language models (LLMs) sometimes falsely refuse\npseudo-harmful prompts, like \"how to kill a mosquito,\" which are actually\nharmless. Frequent false refusals not only frustrate users but also provoke a\npublic backlash against the very values alignment seeks to protect. In this\npaper, we propose the first method to auto-generate diverse,\ncontent-controlled, and model-dependent pseudo-harmful prompts. Using this\nmethod, we construct an evaluation dataset called PHTest, which is ten times\nlarger than existing datasets, covers more false refusal patterns, and\nseparately labels controversial prompts. We evaluate 20 LLMs on PHTest,\nuncovering new insights due to its scale and labeling. Our findings reveal a\ntrade-off between minimizing false refusals and improving safety against\njailbreak attacks. Moreover, we show that many jailbreak defenses significantly\nincrease the false refusal rates, thereby undermining usability. Our method and\ndataset can help developers evaluate and fine-tune safer and more usable LLMs.\nOur code and dataset are available at\nhttps://github.com/umd-huang-lab/FalseRefusal", "AI": {"tldr": "A method to auto-generate pseudo-harmful prompts for evaluating LLMs, revealing a trade-off between false refusals and safety, and showing jailbreak defenses increase false refusals.", "motivation": "Addressing false refusals of harmless prompts by LLMs, which frustrate users and risk public backlash against safety alignment.", "method": "Proposes a method to auto-generate diverse, content-controlled pseudo-harmful prompts, creating the PHTest dataset for evaluation.", "result": "Evaluation of 20 LLMs shows a trade-off between false refusals and safety, with jailbreak defenses increasing false refusals.", "conclusion": "The method and PHTest dataset aid in developing safer, more usable LLMs by better evaluating and fine-tuning their responses."}}
{"id": "2007.02527", "pdf": "https://arxiv.org/pdf/2007.02527", "abs": "https://arxiv.org/abs/2007.02527", "authors": ["Thomas J. Ringstrom", "Mohammadhosein Hasanbeig", "Alessandro Abate"], "title": "Goal Kernel Planning: Linearly-Solvable Non-Markovian Policies for Logical Tasks with Goal-Conditioned Options", "categories": ["cs.AI"], "comment": "52 Pages total. This is an update to a paper we submitted to a\n  Journal and received reviewer feedback for improvement", "summary": "In the domain of hierarchical planning, compositionality, abstraction, and\ntask transfer are crucial for designing algorithms that can efficiently solve a\nvariety of problems with maximal representational reuse. Many real-world\nproblems require non-Markovian policies to handle complex structured tasks with\nlogical conditions, often leading to prohibitively large state representations;\nthis requires efficient methods for breaking these problems down and reusing\nstructure between tasks. To this end, we introduce a compositional framework\ncalled Linearly-Solvable Goal Kernel Dynamic Programming (LS-GKDP) to address\nthe complexity of solving non-Markovian Boolean sub-goal tasks with ordering\nconstraints. LS-GKDP combines the Linearly-Solvable Markov Decision Process\n(LMDP) formalism with the Options Framework of Reinforcement Learning. LMDPs\ncan be efficiently solved as a principal eigenvector problem, and options are\npolicies with termination conditions used as temporally extended actions; with\nLS-GKDP we expand LMDPs to control over options for logical tasks. This\ninvolves decomposing a high-dimensional problem down into a set of\ngoal-condition options for each goal and constructing a goal kernel, which is\nan abstract transition kernel that jumps from an option's initial-states to its\ntermination-states along with an update of the higher-level task-state. We show\nhow an LMDP with a goal kernel enables the efficient optimization of\nmeta-policies in a lower-dimensional subspace defined by the task grounding.\nOptions can also be remapped to new problems within a super-exponential space\nof tasks without significant recomputation, and we identify cases where the\nsolution is invariant to the task grounding, permitting zero-shot task\ntransfer.", "AI": {"tldr": "LS-GKDP is a compositional framework combining LMDPs and the Options Framework to efficiently solve non-Markovian Boolean sub-goal tasks with ordering constraints.", "motivation": "Addressing the complexity of hierarchical planning for tasks requiring non-Markovian policies and logical conditions, aiming for representational reuse and task transfer.", "method": "Combines Linearly-Solvable Markov Decision Processes (LMDPs) with the Options Framework, decomposing problems into goal-conditioned options and constructing a goal kernel for abstract transitions.", "result": "Enables efficient optimization of meta-policies in lower-dimensional subspaces and supports zero-shot task transfer in certain cases.", "conclusion": "LS-GKDP provides a scalable and reusable solution for hierarchical planning in complex, structured tasks."}}
{"id": "2506.09076", "pdf": "https://arxiv.org/pdf/2506.09076", "abs": "https://arxiv.org/abs/2506.09076", "authors": ["Haley Stone", "Jing Du", "Hao Xue", "Matthew Scotch", "David Heslop", "Andreas Z\u00fcfle", "Chandini Raina MacIntyre", "Flora Salim"], "title": "A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models", "categories": ["q-bio.GN", "cs.LG", "q-bio.PE"], "comment": "9 pages, 3 figures", "summary": "Pathogen genome data offers valuable structure for spatial models, but its\nutility is limited by incomplete sequencing coverage. We propose a\nprobabilistic framework for inferring genetic distances between unsequenced\ncases and known sequences within defined transmission chains, using time-aware\nevolutionary distance modeling. The method estimates pairwise divergence from\ncollection dates and observed genetic distances, enabling biologically\nplausible imputation grounded in observed divergence patterns, without\nrequiring sequence alignment or known transmission chains. Applied to highly\npathogenic avian influenza A/H5 cases in wild birds in the United States, this\napproach supports scalable, uncertainty-aware augmentation of genomic datasets\nand enhances the integration of evolutionary information into spatiotemporal\nmodeling workflows.", "AI": {"tldr": "A probabilistic framework infers genetic distances between unsequenced and known pathogen cases using time-aware evolutionary distance modeling, enhancing genomic dataset utility.", "motivation": "Incomplete sequencing coverage limits the utility of pathogen genome data for spatial models.", "method": "Proposes a probabilistic framework for inferring genetic distances, using time-aware evolutionary distance modeling without requiring sequence alignment or known transmission chains.", "result": "Applied to avian influenza cases, the method enables scalable, uncertainty-aware augmentation of genomic datasets.", "conclusion": "The approach enhances the integration of evolutionary information into spatiotemporal modeling workflows."}}
{"id": "2506.09987", "pdf": "https://arxiv.org/pdf/2506.09987", "abs": "https://arxiv.org/abs/2506.09987", "authors": ["Benno Krojer", "Mojtaba Komeili", "Candace Ross", "Quentin Garrido", "Koustuv Sinha", "Nicolas Ballas", "Mahmoud Assran"], "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing benchmarks for assessing the spatio-temporal understanding and\nreasoning abilities of video language models are susceptible to score inflation\ndue to the presence of shortcut solutions based on superficial visual or\ntextual cues. This paper mitigates the challenges in accurately assessing model\nperformance by introducing the Minimal Video Pairs (MVP) benchmark, a simple\nshortcut-aware video QA benchmark for assessing the physical understanding of\nvideo language models. The benchmark is comprised of 55K high-quality\nmultiple-choice video QA examples focusing on physical world understanding.\nExamples are curated from nine video data sources, spanning first-person\negocentric and exocentric videos, robotic interaction data, and cognitive\nscience intuitive physics benchmarks. To mitigate shortcut solutions that rely\non superficial visual or textual cues and biases, each sample in MVP has a\nminimal-change pair -- a visually similar video accompanied by an identical\nquestion but an opposing answer. To answer a question correctly, a model must\nprovide correct answers for both examples in the minimal-change pair; as such,\nmodels that solely rely on visual or textual biases would achieve below random\nperformance. Human performance on MVP is 92.9\\%, while the best open-source\nstate-of-the-art video-language model achieves 40.2\\% compared to random\nperformance at 25\\%.", "AI": {"tldr": "The paper introduces the Minimal Video Pairs (MVP) benchmark to address shortcut solutions in video QA tasks, ensuring accurate assessment of video-language models' physical understanding.", "motivation": "Existing benchmarks are prone to score inflation due to superficial cues, necessitating a more robust evaluation method.", "method": "MVP includes 55K multiple-choice QA pairs with minimal-change pairs to counter shortcuts, sourced from diverse video datasets.", "result": "Human performance is 92.9%, while the best model scores 40.2% (random: 25%), highlighting the benchmark's rigor.", "conclusion": "MVP effectively mitigates shortcuts, providing a reliable measure of true spatio-temporal reasoning in video-language models."}}
{"id": "2409.07615", "pdf": "https://arxiv.org/pdf/2409.07615", "abs": "https://arxiv.org/abs/2409.07615", "authors": ["Matthieu Dubois", "Fran\u00e7ois Yvon", "Pablo Piantanida"], "title": "MOSAIC: Multiple Observers Spotting AI Content", "categories": ["cs.CL"], "comment": "ACL 2025 Findings, code can be found at\n  https://github.com/BaggerOfWords/MOSAIC", "summary": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities, has made it easier for all to\nproduce harmful, toxic, faked or forged content. In response, various proposals\nhave been made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a binary classification\nproblem. Early approaches evaluate an input document with a well-chosen\ndetector LLM, assuming that low-perplexity scores reliably signal machine-made\ncontent. More recent systems instead consider two LLMs and compare their\nprobability distributions over the document to further discriminate when\nperplexity alone cannot. However, using a fixed pair of models can induce\nbrittleness in performance. We extend these approaches to the ensembling of\nseveral LLMs and derive a new, theoretically grounded approach to combine their\nrespective strengths. Our experiments, conducted with various generator LLMs,\nindicate that this approach effectively leverages the strengths of each model,\nresulting in robust detection performance across multiple domains. Our code and\ndata are available at https://github.com/BaggerOfWords/MOSAIC .", "AI": {"tldr": "The paper proposes an ensemble-based method to detect AI-generated text by leveraging multiple LLMs, improving robustness over single or fixed-pair models.", "motivation": "The rise of LLMs has increased the spread of harmful or fake content, necessitating better tools to distinguish AI-generated from human-written text.", "method": "The approach ensembles several LLMs, combining their probability distributions to detect AI-generated content more robustly than perplexity-based or fixed-pair methods.", "result": "Experiments show the ensemble method outperforms single or fixed-pair models, achieving robust detection across domains.", "conclusion": "The proposed ensemble approach effectively combines the strengths of multiple LLMs, enhancing detection performance and generalizability."}}
{"id": "2205.07537", "pdf": "https://arxiv.org/pdf/2205.07537", "abs": "https://arxiv.org/abs/2205.07537", "authors": ["Mohammed M. S. El-Kholany", "Martin Gebser", "Konstantin Schekotihin"], "title": "Decomposition Strategies and Multi-shot ASP Solving for Job-shop Scheduling", "categories": ["cs.AI", "cs.LO"], "comment": "This paper is an extended version of our papers presented at the 38th\n  International Conference on Logic Programming (ICLP 2022) and the 24th\n  International Symposium on Practical Aspects of Declarative Languages (PADL\n  2022), accepted for publication in Logical Methods in Computer Science\n  journal", "summary": "The Job-shop Scheduling Problem (JSP) is a well-known and challenging\ncombinatorial optimization problem in which tasks sharing a machine are to be\narranged in a sequence such that encompassing jobs can be completed as early as\npossible. In this paper, we investigate problem decomposition into time windows\nwhose operations can be successively scheduled and optimized by means of\nmulti-shot Answer Set Programming (ASP) solving. From a computational\nperspective, decomposition aims to split highly complex scheduling tasks into\nbetter manageable subproblems with a balanced number of operations such that\ngood-quality or even optimal partial solutions can be reliably found in a small\nfraction of runtime. We devise and investigate a variety of decomposition\nstrategies in terms of the number and size of time windows as well as\nheuristics for choosing their operations. Moreover, we incorporate time window\noverlapping and compression techniques into the iterative scheduling process to\ncounteract optimization limitations due to the restriction to window-wise\npartial schedules. Our experiments on different JSP benchmark sets show that\nsuccessive optimization by multi-shot ASP solving leads to substantially better\nschedules within tight runtime limits than single-shot optimization on the full\nproblem. In particular, we find that decomposing initial solutions obtained\nwith proficient heuristic methods into time windows leads to improved solution\nquality.", "AI": {"tldr": "The paper explores decomposing the Job-shop Scheduling Problem (JSP) into time windows for multi-shot ASP solving, improving solution quality within tight runtime limits.", "motivation": "JSP is a complex combinatorial optimization problem; decomposition aims to manage subproblems efficiently and find optimal partial solutions quickly.", "method": "Decomposes JSP into time windows, uses multi-shot ASP solving, and incorporates overlapping/compression techniques for iterative scheduling.", "result": "Multi-shot ASP solving outperforms single-shot optimization, especially when decomposing heuristic-based initial solutions.", "conclusion": "Time window decomposition with multi-shot ASP solving enhances JSP scheduling efficiency and solution quality."}}
{"id": "2506.09097", "pdf": "https://arxiv.org/pdf/2506.09097", "abs": "https://arxiv.org/abs/2506.09097", "authors": ["R\u00e9mi Vaucher", "St\u00e9phane Chr\u00e9tien"], "title": "Detecting malignant dynamics on very few blood sample using signature coefficients", "categories": ["q-bio.QM", "cs.LG", "stat.ML"], "comment": "Under review", "summary": "Recent discoveries have suggested that the promising avenue of using\ncirculating tumor DNA (ctDNA) levels in blood samples provides reasonable\naccuracy for cancer monitoring, with extremely low burden on the patient's\nside. It is known that the presence of ctDNA can result from various mechanisms\nleading to DNA release from cells, such as apoptosis, necrosis or active\nsecretion. One key idea in recent cancer monitoring studies is that monitoring\nthe dynamics of ctDNA levels might be sufficient for early multi-cancer\ndetection. This interesting idea has been turned into commercial products, e.g.\nin the company named GRAIL.\n  In the present work, we propose to explore the use of Signature theory for\ndetecting aggressive cancer tumors based on the analysis of blood samples. Our\napproach combines tools from continuous time Markov modelling for the dynamics\nof ctDNA levels in the blood, with Signature theory for building efficient\ntesting procedures. Signature theory is a topic of growing interest in the\nMachine Learning community (see Chevyrev2016 and Fermanian2021), which is now\nrecognised as a powerful feature extraction tool for irregularly sampled\nsignals. The method proposed in the present paper is shown to correctly address\nthe challenging problem of overcoming the inherent data scarsity due to the\nextremely small number of blood samples per patient. The relevance of our\napproach is illustrated with extensive numerical experiments that confirm the\nefficiency of the proposed pipeline.", "AI": {"tldr": "The paper explores using Signature theory and continuous time Markov modeling to detect aggressive cancers via ctDNA analysis in blood samples, addressing data scarcity challenges.", "motivation": "Recent studies show ctDNA levels in blood can monitor cancer with low patient burden. The paper aims to leverage Signature theory for early multi-cancer detection.", "method": "Combines continuous time Markov modeling for ctDNA dynamics with Signature theory for feature extraction and efficient testing.", "result": "The method effectively handles data scarcity and shows promise in numerical experiments.", "conclusion": "The proposed pipeline is efficient for aggressive cancer detection using ctDNA analysis."}}
{"id": "2506.09989", "pdf": "https://arxiv.org/pdf/2506.09989", "abs": "https://arxiv.org/abs/2506.09989", "authors": ["Yiming Dou", "Wonseok Oh", "Yuqing Luo", "Antonio Loquercio", "Andrew Owens"], "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page: https://www.yimingdou.com/hearing_hands/ ,\n  Code: https://github.com/Dou-Yiming/hearing_hands/", "summary": "We study the problem of making 3D scene reconstructions interactive by asking\nthe following question: can we predict the sounds of human hands physically\ninteracting with a scene? First, we record a video of a human manipulating\nobjects within a 3D scene using their hands. We then use these action-sound\npairs to train a rectified flow model to map 3D hand trajectories to their\ncorresponding audio. At test time, a user can query the model for other\nactions, parameterized as sequences of hand poses, to estimate their\ncorresponding sounds. In our experiments, we find that our generated sounds\naccurately convey material properties and actions, and that they are often\nindistinguishable to human observers from real sounds. Project page:\nhttps://www.yimingdou.com/hearing_hands/", "AI": {"tldr": "A method to predict sounds of human hands interacting with 3D scenes using rectified flow models trained on action-sound pairs.", "motivation": "To enable interactive 3D scene reconstructions by predicting sounds of physical interactions.", "method": "Record videos of hand manipulations, train a rectified flow model on action-sound pairs, and query the model for sound predictions based on hand poses.", "result": "Generated sounds accurately reflect material properties and actions, often indistinguishable from real sounds.", "conclusion": "The approach successfully predicts realistic sounds for hand interactions in 3D scenes, enhancing interactivity."}}
{"id": "2409.15912", "pdf": "https://arxiv.org/pdf/2409.15912", "abs": "https://arxiv.org/abs/2409.15912", "authors": ["Lucie Dvorackova", "Marcin P. Joachimiak", "Michal Cerny", "Adriana Kubecova", "Vilem Sklenak", "Tomas Kliegr"], "title": "Explaining word embeddings with perfect fidelity: Case study in research impact prediction", "categories": ["cs.CL"], "comment": null, "summary": "Best performing approaches for scholarly document quality prediction are\nbased on embedding models, which do not allow direct explanation of classifiers\nas distinct words no longer correspond to the input features for model\ntraining. Although model-agnostic explanation methods such as Local\ninterpretable model-agnostic explanations (LIME) can be applied, these produce\nresults with questionable correspondence to the ML model. We introduce a new\nfeature importance method, Self-model Rated Entities (SMER), for logistic\nregression-based classification models trained on word embeddings. We show that\nSMER has theoretically perfect fidelity with the explained model, as its\nprediction corresponds exactly to the average of predictions for individual\nwords in the text. SMER allows us to reliably determine which words or entities\npositively contribute to predicting impactful articles. Quantitative and\nqualitative evaluation is performed through five diverse experiments conducted\non 50.000 research papers from the CORD-19 corpus. Through an AOPC curve\nanalysis, we experimentally demonstrate that SMER produces better explanations\nthan LIME for logistic regression.", "AI": {"tldr": "SMER is a new feature importance method for logistic regression models using word embeddings, offering perfect fidelity and better explanations than LIME.", "motivation": "Existing embedding-based models lack direct explainability, and methods like LIME have questionable fidelity.", "method": "Introduces SMER, a feature importance method for logistic regression models trained on word embeddings, ensuring exact correspondence to the model's predictions.", "result": "SMER outperforms LIME in explanation quality, demonstrated through experiments on 50,000 CORD-19 papers.", "conclusion": "SMER provides reliable and theoretically perfect explanations for word embedding-based models, improving interpretability."}}
{"id": "2306.13956", "pdf": "https://arxiv.org/pdf/2306.13956", "abs": "https://arxiv.org/abs/2306.13956", "authors": ["Noel Brindise", "Cedric Langbort"], "title": "Pointwise-in-Time Explanation for Linear Temporal Logic Rules", "categories": ["cs.AI"], "comment": "See related publication in Conference on Decision and Control (CDC)\n  2023", "summary": "The new field of Explainable Planning (XAIP) has produced a variety of\napproaches to explain and describe the behavior of autonomous agents to human\nobservers. Many summarize agent behavior in terms of the constraints, or\n''rules,'' which the agent adheres to during its trajectories. In this work, we\nnarrow the focus from summary to specific moments in individual trajectories,\noffering a ''pointwise-in-time'' view. Our novel framework, which we define on\nLinear Temporal Logic (LTL) rules, assigns an intuitive status to any rule in\norder to describe the trajectory progress at individual time steps; here, a\nrule is classified as active, satisfied, inactive, or violated. Given a\ntrajectory, a user may query for status of specific LTL rules at individual\ntrajectory time steps. In this paper, we present this novel framework, named\nRule Status Assessment (RSA), and provide an example of its implementation. We\nfind that pointwise-in-time status assessment is useful as a post-hoc\ndiagnostic, enabling a user to systematically track the agent's behavior with\nrespect to a set of rules.", "AI": {"tldr": "The paper introduces Rule Status Assessment (RSA), a framework for pointwise-in-time status classification of Linear Temporal Logic (LTL) rules in agent trajectories, aiding post-hoc diagnostics.", "motivation": "To provide a detailed, moment-specific view of agent behavior by assessing rule statuses (active, satisfied, inactive, violated) at individual time steps, enhancing explainability.", "method": "Defines RSA on LTL rules, classifying rule statuses at each trajectory time step, allowing user queries for specific rules and times.", "result": "Demonstrates RSA's utility as a post-hoc diagnostic tool for systematically tracking agent behavior against rules.", "conclusion": "RSA offers a practical, intuitive approach to explain agent behavior by focusing on rule statuses at specific moments, improving transparency."}}
{"id": "2506.09209", "pdf": "https://arxiv.org/pdf/2506.09209", "abs": "https://arxiv.org/abs/2506.09209", "authors": ["Leandro Anghinoni", "Pablo Zivic", "Jorge Adrian Sanchez"], "title": "Revisiting Graph Projections for Effective Complementary Product Recommendation", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Complementary product recommendation is a powerful strategy to improve\ncustomer experience and retail sales. However, recommending the right product\nis not a simple task because of the noisy and sparse nature of user-item\ninteractions. In this work, we propose a simple yet effective method to predict\na list of complementary products given a query item, based on the structure of\na directed weighted graph projected from the user-item bipartite graph. We\nrevisit bipartite graph projections for recommender systems and propose a novel\napproach for inferring complementarity relationships from historical user-item\ninteractions. We compare our model with recent methods from the literature and\nshow, despite the simplicity of our approach, an average improvement of +43%\nand +38% over sequential and graph-based recommenders, respectively, over\ndifferent benchmarks.", "AI": {"tldr": "A novel method for complementary product recommendation using a directed weighted graph from user-item interactions, outperforming existing methods by 43% and 38%.", "motivation": "Improving customer experience and retail sales by addressing the noisy and sparse nature of user-item interactions.", "method": "Projects a directed weighted graph from user-item bipartite graph to infer complementarity relationships.", "result": "Average improvements of +43% over sequential and +38% over graph-based recommenders.", "conclusion": "The proposed simple yet effective method significantly enhances complementary product recommendations."}}
{"id": "2506.09995", "pdf": "https://arxiv.org/pdf/2506.09995", "abs": "https://arxiv.org/abs/2506.09995", "authors": ["Yuanpeng Tu", "Hao Luo", "Xi Chen", "Xiang Bai", "Fan Wang", "Hengshuang Zhao"], "title": "PlayerOne: Egocentric World Simulator", "categories": ["cs.CV"], "comment": "Project page: https://playerone-hku.github.io/", "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.", "AI": {"tldr": "PlayerOne is the first egocentric realistic world simulator, enabling immersive exploration and dynamic video generation aligned with real human motion.", "motivation": "To create a simulator for immersive, unrestricted exploration in dynamic environments, aligning virtual scenes with real human motion.", "method": "Uses a coarse-to-fine pipeline: pretraining on text-video pairs for coarse understanding, then finetuning on motion-video data. Includes part-disentangled motion injection and joint reconstruction for scene consistency.", "result": "Demonstrates precise control of human movements and world-consistent modeling in diverse scenarios.", "conclusion": "PlayerOne pioneers egocentric real-world simulation, opening new frontiers in world modeling and applications."}}
{"id": "2410.08193", "pdf": "https://arxiv.org/pdf/2410.08193", "abs": "https://arxiv.org/abs/2410.08193", "authors": ["Yuancheng Xu", "Udari Madhushani Sehwag", "Alec Koppel", "Sicheng Zhu", "Bang An", "Furong Huang", "Sumitra Ganesh"], "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment", "categories": ["cs.CL"], "comment": "Published at the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining. Our project page is\navailable at: https://genarm.github.io.", "AI": {"tldr": "GenARM introduces an autoregressive reward model for test-time alignment of LLMs, outperforming prior methods and matching training-time performance.", "motivation": "Traditional alignment methods for LLMs are costly and inflexible, requiring repeated training for diverse preferences. Test-time methods using trajectory-level RMs are inefficient for autoregressive generation.", "method": "GenARM uses an autoregressive reward model (ARM) to predict next-token rewards, enabling efficient alignment of frozen LLMs without retraining.", "result": "GenARM outperforms test-time baselines, matches training-time performance, supports weak-to-strong guidance, and enables multi-objective alignment.", "conclusion": "GenARM provides a cost-effective, flexible solution for aligning LLMs with human preferences, addressing limitations of existing methods."}}
{"id": "2401.14086", "pdf": "https://arxiv.org/pdf/2401.14086", "abs": "https://arxiv.org/abs/2401.14086", "authors": ["Jiri Nemecek", "Tomas Pevny", "Jakub Marecek"], "title": "Generating Likely Counterfactuals Using Sum-Product Networks", "categories": ["cs.AI", "cs.LG", "math.OC"], "comment": "32 pages total", "summary": "The need to explain decisions made by AI systems is driven by both recent\nregulation and user demand. The decisions are often explainable only post hoc.\nIn counterfactual explanations, one may ask what constitutes the best\ncounterfactual explanation. Clearly, multiple criteria must be taken into\naccount, although \"distance from the sample\" is a key criterion. Recent methods\nthat consider the plausibility of a counterfactual seem to sacrifice this\noriginal objective. Here, we present a system that provides high-likelihood\nexplanations that are, at the same time, close and sparse. We show that the\nsearch for the most likely explanations satisfying many common desiderata for\ncounterfactual explanations can be modeled using Mixed-Integer Optimization\n(MIO). We use a Sum-Product Network (SPN) to estimate the likelihood of a\ncounterfactual. To achieve that, we propose an MIO formulation of an SPN, which\ncan be of independent interest. The source code with examples is available at\nhttps://github.com/Epanemu/LiCE.", "AI": {"tldr": "The paper proposes a system using Mixed-Integer Optimization (MIO) and Sum-Product Networks (SPNs) to generate high-likelihood, close, and sparse counterfactual explanations for AI decisions.", "motivation": "The need for explainable AI decisions is driven by regulation and user demand, but current methods often sacrifice key criteria like distance from the sample.", "method": "The system models the search for likely counterfactual explanations using MIO and employs SPNs to estimate likelihoods, with an MIO formulation for SPNs.", "result": "The method provides counterfactual explanations that are both high-likelihood and satisfy common desiderata like closeness and sparsity.", "conclusion": "The proposed system effectively balances plausibility and key criteria for counterfactual explanations, with potential broader applications for SPNs."}}
{"id": "2506.09255", "pdf": "https://arxiv.org/pdf/2506.09255", "abs": "https://arxiv.org/abs/2506.09255", "authors": ["Saeed Hashemi", "Genchang Peng", "Mehrdad Nourani", "Omar Nofal", "Jay Harvey"], "title": "AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization", "categories": ["eess.SP", "cs.LG"], "comment": "Accepted to be presented at the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC 2025). This\n  version is submitted to arXiv prior to final IEEE formatting and publication", "summary": "Stereo-electroencephalography (SEEG) is an invasive technique to implant\ndepth electrodes and collect data for pre-surgery evaluation. Visual inspection\nof signals recorded from hundreds of channels is time consuming and\ninefficient. We propose a machine learning approach to rank the impactful\nchannels by incorporating clinician's selection and computational finding. A\nclassification model using XGBoost is trained to learn the discriminative\nfeatures of each channel during ictal periods. Then, the SHapley Additive\nexPlanations (SHAP) scoring is utilized to rank SEEG channels based on their\ncontribution to seizures. A channel extension strategy is also incorporated to\nexpand the search space and identify suspicious epileptogenic zones beyond\nthose selected by clinicians. For validation, SEEG data for five patients were\nanalyzed showing promising results in terms of accuracy, consistency, and\nexplainability.", "AI": {"tldr": "A machine learning approach using XGBoost and SHAP scoring is proposed to rank impactful SEEG channels for epilepsy evaluation, improving efficiency and accuracy.", "motivation": "Visual inspection of SEEG signals is time-consuming and inefficient, prompting the need for an automated method to identify impactful channels.", "method": "XGBoost is trained to classify channels during ictal periods, and SHAP scoring ranks them by seizure contribution. A channel extension strategy expands the search for epileptogenic zones.", "result": "Validation on five patients' SEEG data showed promising accuracy, consistency, and explainability.", "conclusion": "The proposed method efficiently identifies impactful SEEG channels, aiding in pre-surgery evaluation for epilepsy."}}
{"id": "2506.09098", "pdf": "https://arxiv.org/pdf/2506.09098", "abs": "https://arxiv.org/abs/2506.09098", "authors": ["Yangjie Cui", "Boyang Gao", "Yiwei Zhang", "Xin Dong", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "title": "WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras", "categories": ["cs.RO", "cs.CV"], "comment": "https://youtu.be/AQAgVdrx1DE", "summary": "Previous studies on event camera sensing have demonstrated certain detection\nperformance using dense event representations. However, the accumulated noise\nin such dense representations has received insufficient attention, which\ndegrades the representation quality and increases the likelihood of missed\ndetections. To address this challenge, we propose the Wavelet\nDenoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event\ncameras. In particular, a dense event representation is presented first, which\nenables real-time reconstruction of events as tensors. Then, a wavelet\ntransform method is designed to filter noise in the event representations. Such\na method is integrated into the backbone for feature extraction. The extracted\nfeatures are subsequently fed into a transformer-based network for object\nprediction. To further reduce inference time, we incorporate the Dynamic\nReorganization Convolution Block (DRCB) as a fusion module within the hybrid\nencoder. The proposed method has been evaluated on three event-based object\ndetection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that\nWD-DETR outperforms tested state-of-the-art methods. Additionally, we implement\nour approach on a common onboard computer for robots, the NVIDIA Jetson Orin\nNX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,\nwhich is exceptionally well-suited for real-time perception of onboard robotic\nsystems.", "AI": {"tldr": "WD-DETR improves event camera detection by denoising dense event representations with wavelet transforms and a transformer-based network, achieving high performance and real-time speeds.", "motivation": "Addressing noise in dense event representations that degrade detection quality and cause missed detections.", "method": "Proposes WD-DETR: wavelet denoising for event representations, integrated with a transformer-based network and Dynamic Reorganization Convolution Block for efficiency.", "result": "Outperforms state-of-the-art methods on DSEC, Gen1, and 1Mpx datasets, achieving ~35 FPS on NVIDIA Jetson Orin NX.", "conclusion": "WD-DETR is effective for real-time robotic perception, combining noise reduction and high-speed performance."}}
{"id": "2410.08674", "pdf": "https://arxiv.org/pdf/2410.08674", "abs": "https://arxiv.org/abs/2410.08674", "authors": ["Nizar Habash", "Hanada Taha-Thomure", "Khalid N. Elmadani", "Zeina Zeino", "Abdallah Abushmaes"], "title": "Guidelines for Fine-grained Sentence-level Arabic Readability Annotation", "categories": ["cs.CL"], "comment": "Accepted at LAW-XIX at ACL 2025", "summary": "This paper presents the annotation guidelines of the Balanced Arabic\nReadability Evaluation Corpus (BAREC), a large-scale resource for fine-grained\nsentence-level readability assessment in Arabic. BAREC includes 69,441\nsentences (1M+ words) labeled across 19 levels, from kindergarten to\npostgraduate. Based on the Taha/Arabi21 framework, the guidelines were refined\nthrough iterative training with native Arabic-speaking educators. We highlight\nkey linguistic, pedagogical, and cognitive factors in determining readability\nand report high inter-annotator agreement: Quadratic Weighted Kappa 81.8%\n(substantial/excellent agreement) in the last annotation phase. We also\nbenchmark automatic readability models across multiple classification\ngranularities (19-, 7-, 5-, and 3-level). The corpus and guidelines are\npublicly available.", "AI": {"tldr": "The paper introduces BAREC, a large-scale Arabic readability corpus with 69,441 sentences labeled across 19 levels, refined through educator feedback, and reports high annotator agreement (81.8%).", "motivation": "To create a fine-grained, sentence-level readability assessment resource for Arabic, addressing linguistic, pedagogical, and cognitive factors.", "method": "Refined annotation guidelines based on the Taha/Arabi21 framework through iterative training with native Arabic educators.", "result": "High inter-annotator agreement (81.8% Quadratic Weighted Kappa) and benchmarking of automatic readability models across multiple granularities.", "conclusion": "BAREC and its guidelines are publicly available, offering a valuable resource for Arabic readability research."}}
{"id": "2405.20421", "pdf": "https://arxiv.org/pdf/2405.20421", "abs": "https://arxiv.org/abs/2405.20421", "authors": ["Qianqi Yan", "Xuehai He", "Xiang Yue", "Xin Eric Wang"], "title": "Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA", "categories": ["cs.AI"], "comment": null, "summary": "Large Multimodal Models (LMMs) have shown remarkable progress in medical\nVisual Question Answering (Med-VQA), achieving high accuracy on existing\nbenchmarks. However, their reliability under robust evaluation is questionable.\nThis study reveals that when subjected to simple probing evaluation,\nstate-of-the-art models perform worse than random guessing on medical diagnosis\nquestions. To address this critical evaluation problem, we introduce the\nProbing Evaluation for Medical Diagnosis (ProbMed) dataset to rigorously assess\nLMM performance in medical imaging through probing evaluation and procedural\ndiagnosis. Particularly, probing evaluation features pairing original questions\nwith negation questions with hallucinated attributes, while procedural\ndiagnosis requires reasoning across various diagnostic dimensions for each\nimage, including modality recognition, organ identification, clinical findings,\nabnormalities, and positional grounding. Our evaluation reveals that\ntop-performing models like GPT-4o, GPT-4V, and Gemini Pro perform worse than\nrandom guessing on specialized diagnostic questions, indicating significant\nlimitations in handling fine-grained medical inquiries. Besides, models like\nLLaVA-Med struggle even with more general questions, and results from CheXagent\ndemonstrate the transferability of expertise across different modalities of the\nsame organ, showing that specialized domain knowledge is still crucial for\nimproving performance. This study underscores the urgent need for more robust\nevaluation to ensure the reliability of LMMs in critical fields like medical\ndiagnosis, and current LMMs are still far from applicable to those fields.", "AI": {"tldr": "State-of-the-art Large Multimodal Models (LMMs) perform poorly on medical diagnosis questions under probing evaluation, highlighting reliability issues. The ProbMed dataset is introduced to rigorously assess LMMs, revealing significant limitations in fine-grained medical tasks.", "motivation": "To address the questionable reliability of LMMs in medical Visual Question Answering (Med-VQA) under robust evaluation, especially in specialized diagnostic tasks.", "method": "Introduces the Probing Evaluation for Medical Diagnosis (ProbMed) dataset, featuring probing evaluation (negation questions with hallucinated attributes) and procedural diagnosis (reasoning across diagnostic dimensions).", "result": "Top models like GPT-4o, GPT-4V, and Gemini Pro perform worse than random guessing on specialized diagnostic questions, while others like LLaVA-Med struggle even with general questions.", "conclusion": "Current LMMs are unreliable for fine-grained medical diagnosis, emphasizing the need for more robust evaluation and specialized domain knowledge."}}
{"id": "2506.09280", "pdf": "https://arxiv.org/pdf/2506.09280", "abs": "https://arxiv.org/abs/2506.09280", "authors": ["Haitian Jiang", "Shaowei Zhu", "Zhen Zhang", "Zhenyu Song", "Xinwei Fu", "Zhen Jia", "Yida Wang", "Jinyang Li"], "title": "TTrace: Lightweight Error Checking and Diagnosis for Distributed Training", "categories": ["cs.DC", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Distributed training is essential for scaling the training of large neural\nnetwork models, such as large language models (LLMs), across thousands of GPUs.\nHowever, the complexity of distributed training programs makes them\nparticularly prone to silent bugs, which do not produce explicit error signal\nbut lead to incorrect training outcome. Effectively detecting and localizing\nsuch silent bugs in distributed training is challenging. Common debugging\npractice using metrics like training loss or gradient norm curves can be\ninefficient and ineffective. Additionally, obtaining intermediate tensor values\nand determining whether they are correct during silent bug localization is\ndifficult, particularly in the context of low-precision training.\n  To address those challenges, we design and implement TTrace, the first system\ncapable of detecting and localizing silent bugs in distributed training. TTrace\ncollects intermediate tensors from distributing training in a fine-grained\nmanner and compares them against those from a trusted single-device reference\nimplementation. To properly compare the floating-point values in the tensors,\nwe propose novel mathematical analysis that provides a guideline for setting\nthresholds, enabling TTrace to distinguish bug-induced errors from\nfloating-point round-off errors. Experimental results demonstrate that TTrace\neffectively detects 11 existing bugs and 3 new bugs in the widely used\nMegatron-LM framework, while requiring fewer than 10 lines of code change.\nTTrace is effective in various training recipes, including low-precision\nrecipes involving BF16 and FP8.", "AI": {"tldr": "TTrace is a system designed to detect and localize silent bugs in distributed training by comparing intermediate tensors against a trusted reference, using novel mathematical analysis for accurate threshold setting.", "motivation": "Silent bugs in distributed training of large neural networks are hard to detect and localize, as traditional debugging methods are inefficient and ineffective.", "method": "TTrace collects and compares intermediate tensors from distributed training with a single-device reference, using mathematical analysis to set thresholds for distinguishing bugs from floating-point errors.", "result": "TTrace successfully detected 11 existing and 3 new bugs in Megatron-LM with minimal code changes and worked effectively in low-precision training scenarios.", "conclusion": "TTrace provides an efficient and effective solution for detecting silent bugs in distributed training, especially in low-precision settings."}}
{"id": "2506.09217", "pdf": "https://arxiv.org/pdf/2506.09217", "abs": "https://arxiv.org/abs/2506.09217", "authors": ["Boyu Jiang", "Liang Shi", "Zhengzhi Lin", "Loren Stowe", "Feng Guo"], "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule", "categories": ["cs.RO", "cs.CV", "stat.AP"], "comment": null, "summary": "The performance of perception systems in autonomous driving systems (ADS) is\nstrongly influenced by object distance, scene dynamics, and environmental\nconditions such as weather. AI-based perception outputs are inherently\nstochastic, with variability driven by these external factors, while\ntraditional evaluation metrics remain static and event-independent, failing to\ncapture fluctuations in confidence over time. In this work, we introduce the\nPerception Characteristics Distance (PCD) -- a novel evaluation metric that\nquantifies the farthest distance at which an object can be reliably detected,\nincorporating uncertainty in model outputs. To support this, we present the\nSensorRainFall dataset, collected on the Virginia Smart Road using a\nsensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear\nand daylight-rain scenarios, with precise ground-truth distances to the target\nobjects. Statistical analysis reveals the presence of change points in the\nvariance of detection confidence score with distance. By averaging the PCD\nvalues across a range of detection quality thresholds and probabilistic\nthresholds, we compute the mean PCD (mPCD), which captures the overall\nperception characteristics of a system with respect to detection distance.\nApplying state-of-the-art perception models shows that mPCD captures meaningful\nreliability differences under varying weather conditions -- differences that\nstatic metrics overlook. PCD provides a principled, distribution-aware measure\nof perception performance, supporting safer and more robust ADS operation,\nwhile the SensorRainFall dataset offers a valuable benchmark for evaluation.\nThe SensorRainFall dataset is publicly available at\nhttps://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the\nevaluation code is open-sourced at\nhttps://github.com/datadrivenwheels/PCD_Python.", "AI": {"tldr": "The paper introduces Perception Characteristics Distance (PCD), a new metric for evaluating autonomous driving perception systems, accounting for uncertainty and variability. It also presents the SensorRainFall dataset for benchmarking.", "motivation": "Current evaluation metrics for perception systems are static and fail to capture variability due to factors like distance and weather.", "method": "Proposes PCD to quantify reliable detection distance, supported by the SensorRainFall dataset collected under controlled conditions.", "result": "PCD reveals meaningful reliability differences under varying conditions, which static metrics miss.", "conclusion": "PCD offers a robust, distribution-aware evaluation method, enhancing ADS safety and performance."}}
{"id": "2410.14387", "pdf": "https://arxiv.org/pdf/2410.14387", "abs": "https://arxiv.org/abs/2410.14387", "authors": ["Constanza Fierro", "Negar Foroutan", "Desmond Elliott", "Anders S\u00f8gaard"], "title": "How Do Multilingual Language Models Remember Facts?", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has only focused on\nEnglish monolingual models. The question of how these mechanisms generalize to\nnon-English languages and multilingual LLMs remains unexplored. In this paper,\nwe address this gap by conducting a comprehensive analysis of three\nmultilingual LLMs. First, we show that previously identified recall mechanisms\nin English largely apply to multilingual contexts, with nuances based on\nlanguage and architecture. Next, through patching intermediate representations,\nwe localize the role of language during recall, finding that subject enrichment\nis language-independent, while object extraction is language-dependent.\nAdditionally, we discover that the last token representation acts as a Function\nVector (FV), encoding both the language of the query and the content to be\nextracted from the subject. Furthermore, in decoder-only LLMs, FVs compose\nthese two pieces of information in two separate stages. These insights reveal\nunique mechanisms in multilingual LLMs for recalling information, highlighting\nthe need for new methodologies -- such as knowledge evaluation, fact editing,\nand knowledge acquisition -- that are specifically tailored for multilingual\nLLMs.", "AI": {"tldr": "The paper explores how knowledge recall mechanisms in multilingual LLMs differ from English-only models, revealing language-dependent and independent aspects, and introduces the concept of Function Vectors (FVs).", "motivation": "To understand how knowledge recall mechanisms in LLMs generalize to non-English languages and multilingual models, addressing a gap in prior research focused on English monolingual models.", "method": "Analyzed three multilingual LLMs, applying patching to intermediate representations to localize language roles in recall, identifying Function Vectors (FVs) in the last token representation.", "result": "Recall mechanisms in English largely apply to multilingual contexts, with subject enrichment being language-independent and object extraction language-dependent. FVs encode query language and content.", "conclusion": "Multilingual LLMs have unique recall mechanisms, necessitating tailored methodologies for knowledge evaluation, fact editing, and acquisition in multilingual contexts."}}
{"id": "2406.14917", "pdf": "https://arxiv.org/pdf/2406.14917", "abs": "https://arxiv.org/abs/2406.14917", "authors": ["Melvin Wong", "Jiao Liu", "Thiago Rios", "Stefan Menzel", "Yew Soon Ong"], "title": "LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative Evolutionary Multitasking", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE"], "comment": "This work has been submitted to the IEEE for review", "summary": "In this paper, we introduce LLM-driven MultiTask Evolutionary Algorithm\n(LLM2TEA), the first agentic AI designer within a generative evolutionary\nmultitasking (GEM) framework that promotes the crossover and synergy of designs\nfrom multiple domains, leading to innovative solutions that transcend\nindividual disciplines. Of particular interest is the discovery of objects that\nare not only innovative but also conform to the physical specifications of the\nreal world in science and engineering. LLM2TEA comprises a large language model\nto initialize a population of genotypes (defined by text prompts) describing\nthe objects of interest, a text-to-3D generative model to produce phenotypes\nfrom these prompts, a classifier to interpret the semantic representations of\nthe objects, and a physics simulation model to assess their physical\nproperties. We propose several novel LLM-based multitask evolutionary operators\nto guide the search toward the discovery of high-performing practical objects.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, revealing from 97\\% to 174\\% improvement in the\ndiversity of innovative objects compared to the present text-to-3D generative\nmodel baseline. In addition, more than 73\\% of the generated designs have\nbetter physical performance than the top 1\\% percentile of the designs\ngenerated in the baseline. Moreover, LLM2TEA generates designs that are not\nonly aesthetically creative but also functional in real-world applications.\nSeveral of these designs have been successfully 3D-printed, emphasizing the\nproposed approach's capacity to transform AI-generated outputs into tangible\nphysical objects. The designs produced by LLM2TEA meets practical requirements\nwhile showcasing creative and innovative features, underscoring its potential\napplications in complex design optimization and discovery.", "AI": {"tldr": "LLM2TEA is an AI-driven evolutionary algorithm that combines large language models, text-to-3D generation, and physics simulation to create innovative, functional designs across domains.", "motivation": "To discover interdisciplinary, real-world-compliant designs by leveraging AI for generative multitasking.", "method": "Uses LLM for genotype initialization, text-to-3D for phenotype generation, classifiers for semantics, and physics simulation for validation. Introduces novel LLM-based evolutionary operators.", "result": "Achieves 97-174% improvement in design diversity and 73% of designs outperform the baseline's top 1%. Designs are functional and 3D-printable.", "conclusion": "LLM2TEA effectively bridges AI creativity with practical engineering, enabling tangible, innovative solutions."}}
{"id": "2506.09282", "pdf": "https://arxiv.org/pdf/2506.09282", "abs": "https://arxiv.org/abs/2506.09282", "authors": ["Dhruv Parikh", "Viktor Prasanna"], "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs", "categories": ["cs.DC", "cs.LG"], "comment": "IC3", "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.", "AI": {"tldr": "ScalableHD is a method for efficient Hyperdimensional Computing (HDC) inference on multi-core CPUs, achieving up to 10x speedup over baselines while maintaining accuracy.", "motivation": "Traditional HDC methods suffer from low accuracy and lack efficient inference on general-purpose CPUs.", "method": "ScalableHD uses a two-stage pipelined execution model, parallelized across cores, with memory tiling and NUMA-aware optimizations.", "result": "Achieves up to 10x speedup in throughput over baselines like TorchHD, with robust scalability across tasks.", "conclusion": "ScalableHD bridges the gap for efficient HDC inference on CPUs, offering high performance and scalability."}}
{"id": "2506.09353", "pdf": "https://arxiv.org/pdf/2506.09353", "abs": "https://arxiv.org/abs/2506.09353", "authors": ["Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "title": "DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt", "categories": ["cs.CR", "cs.CV"], "comment": "16 pages", "summary": "Large Vision-Language Models (LVLMs) have achieved impressive progress across\nvarious applications but remain vulnerable to malicious queries that exploit\nthe visual modality. Existing alignment approaches typically fail to resist\nmalicious queries while preserving utility on benign ones effectively. To\naddress these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),\nwhich is built upon two key innovations. First, we introduce the Visual Safety\nPrompt, which appends a trainable padding region around the input image. It\npreserves visual features and expands the optimization space. Second, we\npropose Deep Alignment, a novel approach to train the visual safety prompt\nthrough supervision in the model's activation space. It enhances the inherent\nability of LVLMs to perceive malicious queries, achieving deeper alignment than\nprior works. Extensive experiments across five benchmarks on two representative\nLVLMs demonstrate that DAVSP effectively resists malicious queries while\npreserving benign input utility. Furthermore, DAVSP exhibits great cross-model\ngeneration ability. Ablation studies further reveal that both the Visual Safety\nPrompt and Deep Alignment are essential components, jointly contributing to its\noverall effectiveness. The code is publicly available at\nhttps://github.com/zhangyitonggg/DAVSP.", "AI": {"tldr": "DAVSP introduces Visual Safety Prompt and Deep Alignment to protect LVLMs from malicious queries while maintaining utility on benign inputs.", "motivation": "LVLMs are vulnerable to malicious visual queries; existing alignment methods fail to balance safety and utility.", "method": "Proposes Visual Safety Prompt (trainable padding) and Deep Alignment (supervision in activation space).", "result": "Effective resistance to malicious queries and preserved utility on benign inputs across benchmarks.", "conclusion": "DAVSP's innovations are essential for robust LVLM alignment, with cross-model generalization potential."}}
{"id": "2410.17131", "pdf": "https://arxiv.org/pdf/2410.17131", "abs": "https://arxiv.org/abs/2410.17131", "authors": ["Hao Xiang", "Bowen Yu", "Hongyu Lin", "Keming Lu", "Yaojie Lu", "Xianpei Han", "Ben He", "Le Sun", "Jingren Zhou", "Junyang Lin"], "title": "Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The key to effective alignment lies in high-quality preference data. Recent\nresearch has focused on automated alignment, which involves developing\nalignment systems with minimal human intervention. However, prior research has\npredominantly focused on developing data generation methods, while insufficient\nattention has been paid to quality control mechanisms, which often produce\ninaccurate and unhelpful data, leading to unpredictable benefits during\niterative optimization. In this paper, we present Self-Steering Optimization\n($SSO$), an algorithm that autonomously generates high-quality preference data,\neliminating manual annotation requirements. $SSO$ employs a specialized\noptimization objective to build a data generator from the policy model itself,\nwhich is used to produce accurate and on-policy data. We demonstrate $SSO$'s\neffectiveness through comprehensive experiments on two series of models: Llama\n3 and Qwen 2. Our evaluation across diverse benchmarks shows that $SSO$\nconsistently outperforms baselines in human preference alignment and reward\noptimization. Further analysis validates $SSO$ as a scalable framework for\npreference optimization, benefiting the advancement in automated alignment\ntechniques.", "AI": {"tldr": "The paper introduces Self-Steering Optimization (SSO), an algorithm for autonomously generating high-quality preference data to improve alignment systems without manual annotation.", "motivation": "Prior research lacks quality control in automated alignment, leading to inaccurate data. SSO addresses this gap by focusing on high-quality data generation.", "method": "SSO uses a specialized optimization objective to create a data generator from the policy model, producing accurate and on-policy preference data.", "result": "SSO outperforms baselines in human preference alignment and reward optimization, validated on Llama 3 and Qwen 2 models.", "conclusion": "SSO is a scalable framework for preference optimization, advancing automated alignment techniques."}}
{"id": "2406.15481", "pdf": "https://arxiv.org/pdf/2406.15481", "abs": "https://arxiv.org/abs/2406.15481", "authors": ["Haneul Yoo", "Yongjin Yang", "Hwaran Lee"], "title": "Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding", "categories": ["cs.AI", "cs.CL"], "comment": "To appear in ACL 2025", "summary": "As large language models (LLMs) have advanced rapidly, concerns regarding\ntheir safety have become prominent. In this paper, we discover that\ncode-switching in red-teaming queries can effectively elicit undesirable\nbehaviors of LLMs, which are common practices in natural language. We introduce\na simple yet effective framework, CSRT, to synthesize codeswitching red-teaming\nqueries and investigate the safety and multilingual understanding of LLMs\ncomprehensively. Through extensive experiments with ten state-of-the-art LLMs\nand code-switching queries combining up to 10 languages, we demonstrate that\nthe CSRT significantly outperforms existing multilingual red-teaming\ntechniques, achieving 46.7% more attacks than standard attacks in English and\nbeing effective in conventional safety domains. We also examine the\nmultilingual ability of those LLMs to generate and understand codeswitching\ntexts. Additionally, we validate the extensibility of the CSRT by generating\ncodeswitching attack prompts with monolingual data. We finally conduct detailed\nablation studies exploring code-switching and propound unintended correlation\nbetween resource availability of languages and safety alignment in existing\nmultilingual LLMs.", "AI": {"tldr": "The paper introduces CSRT, a framework for code-switching red-teaming queries to test LLM safety and multilingual understanding, showing it outperforms existing methods and reveals safety alignment issues.", "motivation": "Addressing safety concerns in LLMs by exploring how code-switching in queries can expose undesirable behaviors.", "method": "Developed CSRT to synthesize code-switching queries, tested on 10 LLMs with up to 10 languages.", "result": "CSRT achieved 46.7% more attacks than standard methods, revealing multilingual safety gaps.", "conclusion": "Code-switching is effective for red-teaming, and LLM safety alignment correlates with language resource availability."}}
{"id": "2506.09312", "pdf": "https://arxiv.org/pdf/2506.09312", "abs": "https://arxiv.org/abs/2506.09312", "authors": ["Erik Buchholz", "Natasha Fernandes", "David D. Nguyen", "Alsharif Abuadbba", "Surya Nepal", "Salil S. Kanhere"], "title": "What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "While location trajectories offer valuable insights, they also reveal\nsensitive personal information. Differential Privacy (DP) offers formal\nprotection, but achieving a favourable utility-privacy trade-off remains\nchallenging. Recent works explore deep learning-based generative models to\nproduce synthetic trajectories. However, current models lack formal privacy\nguarantees and rely on conditional information derived from real data during\ngeneration. This work investigates the utility cost of enforcing DP in such\nmodels, addressing three research questions across two datasets and eleven\nutility metrics. (1) We evaluate how DP-SGD, the standard DP training method\nfor deep learning, affects the utility of state-of-the-art generative models.\n(2) Since DP-SGD is limited to unconditional models, we propose a novel DP\nmechanism for conditional generation that provides formal guarantees and assess\nits impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN\n- affect the utility-privacy trade-off. Our results show that DP-SGD\nsignificantly impacts performance, although some utility remains if the\ndatasets is sufficiently large. The proposed DP mechanism improves training\nstability, particularly when combined with DP-SGD, for unstable models such as\nGANs and on smaller datasets. Diffusion models yield the best utility without\nguarantees, but with DP-SGD, GANs perform best, indicating that the best\nnon-private model is not necessarily optimal when targeting formal guarantees.\nIn conclusion, DP trajectory generation remains a challenging task, and formal\nguarantees are currently only feasible with large datasets and in constrained\nuse cases.", "AI": {"tldr": "The paper explores the utility-privacy trade-off in differentially private (DP) trajectory generation, evaluating DP-SGD's impact, proposing a novel DP mechanism for conditional generation, and comparing model types (Diffusion, VAE, GAN).", "motivation": "To address the lack of formal privacy guarantees in deep learning-based generative models for trajectory data and improve the utility-privacy balance.", "method": "Evaluates DP-SGD's effect on generative models, introduces a new DP mechanism for conditional generation, and compares performance across Diffusion, VAE, and GAN models.", "result": "DP-SGD reduces utility but retains some with large datasets. The proposed DP mechanism enhances stability, especially for GANs. Diffusion models perform best without guarantees, while GANs excel with DP-SGD.", "conclusion": "DP trajectory generation is challenging, with formal guarantees feasible only for large datasets and constrained use cases."}}
{"id": "2506.09491", "pdf": "https://arxiv.org/pdf/2506.09491", "abs": "https://arxiv.org/abs/2506.09491", "authors": ["Guanghu Xie", "Zhiduo Jiang", "Yonglong Zhang", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "title": "DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Transparent and reflective objects in everyday environments pose significant\nchallenges for depth sensors due to their unique visual properties, such as\nspecular reflections and light transmission. These characteristics often lead\nto incomplete or inaccurate depth estimation, which severely impacts downstream\ngeometry-based vision tasks, including object recognition, scene\nreconstruction, and robotic manipulation. To address the issue of missing depth\ninformation in transparent and reflective objects, we propose DCIRNet, a novel\nmultimodal depth completion network that effectively integrates RGB images and\ndepth maps to enhance depth estimation quality. Our approach incorporates an\ninnovative multimodal feature fusion module designed to extract complementary\ninformation between RGB images and incomplete depth maps. Furthermore, we\nintroduce a multi-stage supervision and depth refinement strategy that\nprogressively improves depth completion and effectively mitigates the issue of\nblurred object boundaries. We integrate our depth completion model into\ndexterous grasping frameworks and achieve a $44\\%$ improvement in the grasp\nsuccess rate for transparent and reflective objects. We conduct extensive\nexperiments on public datasets, where DCIRNet demonstrates superior\nperformance. The experimental results validate the effectiveness of our\napproach and confirm its strong generalization capability across various\ntransparent and reflective objects.", "AI": {"tldr": "DCIRNet, a multimodal depth completion network, improves depth estimation for transparent and reflective objects by fusing RGB and depth data, enhancing grasp success rates by 44%.", "motivation": "Transparent and reflective objects challenge depth sensors due to specular reflections and light transmission, leading to incomplete depth data and impacting vision tasks.", "method": "DCIRNet integrates RGB images and depth maps with a multimodal feature fusion module and multi-stage supervision for refined depth completion.", "result": "The model achieves a 44% improvement in grasp success rates and outperforms on public datasets.", "conclusion": "DCIRNet effectively addresses depth estimation challenges for transparent and reflective objects, demonstrating strong generalization."}}
{"id": "2411.02460", "pdf": "https://arxiv.org/pdf/2411.02460", "abs": "https://arxiv.org/abs/2411.02460", "authors": ["Haneul Yoo", "Cheonbok Park", "Sangdoo Yun", "Alice Oh", "Hwaran Lee"], "title": "Code-Switching Curriculum Learning for Multilingual Transfer in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in Findings of ACL 2025", "summary": "Large language models (LLMs) now exhibit near human-level performance in\nvarious tasks, but their performance drops drastically after a handful of\nhigh-resource languages due to the imbalance in pre-training data. Inspired by\nthe human process of second language acquisition, particularly\ncode-switching$\\unicode{x2014}$the practice of language alternation in a\nconversation$\\unicode{x2014}$we propose code-switching curriculum learning\n(CSCL) to enhance cross-lingual transfer for LLMs. CSCL mimics the stages of\nhuman language learning by progressively training models with a curriculum\nconsisting of 1) token-level code-switching, 2) sentence-level code-switching,\nand 3) monolingual corpora. Using Qwen 2 as our underlying model, we\ndemonstrate the efficacy of the CSCL in improving language transfer to Korean,\nachieving significant performance gains compared to monolingual continual\npre-training methods. Ablation studies reveal that both token- and\nsentence-level code-switching significantly enhance cross-lingual transfer and\nthat curriculum learning amplifies these effects. We also extend our findings\ninto various languages, including Japanese (high-resource) and Indonesian\n(low-resource), and using two additional models (Gemma 2 and Phi 3.5). We\nfurther show that CSCL mitigates spurious correlations between language\nresources and safety alignment, presenting a robust, efficient framework for\nmore equitable language transfer in LLMs. We observe that CSCL is effective for\nlow-resource settings where high-quality, monolingual corpora for language\ntransfer are hardly available.", "AI": {"tldr": "The paper proposes Code-Switching Curriculum Learning (CSCL) to improve cross-lingual transfer in LLMs by mimicking human second language acquisition stages, showing significant gains in performance for languages like Korean, Japanese, and Indonesian.", "motivation": "Address the performance drop of LLMs in low-resource languages due to imbalanced pre-training data by leveraging human-like code-switching techniques.", "method": "CSCL progressively trains models with a curriculum: token-level code-switching, sentence-level code-switching, and monolingual corpora, tested on Qwen 2, Gemma 2, and Phi 3.5.", "result": "CSCL significantly improves cross-lingual transfer, especially for low-resource languages, and mitigates spurious correlations between language resources and safety alignment.", "conclusion": "CSCL offers a robust, efficient framework for equitable language transfer in LLMs, particularly beneficial for low-resource settings."}}
{"id": "2407.01067", "pdf": "https://arxiv.org/pdf/2407.01067", "abs": "https://arxiv.org/abs/2407.01067", "authors": ["Changde Du", "Kaicheng Fu", "Bincheng Wen", "Yi Sun", "Jie Peng", "Wei Wei", "Ying Gao", "Shengpei Wang", "Chuncheng Zhang", "Jinpeng Li", "Shuang Qiu", "Le Chang", "Huiguang He"], "title": "Human-like object concept representations emerge naturally in multimodal large language models", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.LG"], "comment": "Published on Nature Machine Intelligence", "summary": "Understanding how humans conceptualize and categorize natural objects offers\ncritical insights into perception and cognition. With the advent of Large\nLanguage Models (LLMs), a key question arises: can these models develop\nhuman-like object representations from linguistic and multimodal data? In this\nstudy, we combined behavioral and neuroimaging analyses to explore the\nrelationship between object concept representations in LLMs and human\ncognition. We collected 4.7 million triplet judgments from LLMs and Multimodal\nLLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity\nstructure of 1,854 natural objects. The resulting 66-dimensional embeddings\nwere stable, predictive, and exhibited semantic clustering similar to human\nmental representations. Remarkably, the dimensions underlying these embeddings\nwere interpretable, suggesting that LLMs and MLLMs develop human-like\nconceptual representations of objects. Further analysis showed strong alignment\nbetween model embeddings and neural activity patterns in brain regions such as\nEBA, PPA, RSC, and FFA. This provides compelling evidence that the object\nrepresentations in LLMs, while not identical to human ones, share fundamental\nsimilarities that reflect key aspects of human conceptual knowledge. Our\nfindings advance the understanding of machine intelligence and inform the\ndevelopment of more human-like artificial cognitive systems.", "AI": {"tldr": "The study explores if LLMs develop human-like object representations, finding strong alignment between model embeddings and human cognition.", "motivation": "To understand if LLMs can form human-like object concepts from linguistic and multimodal data.", "method": "Combined behavioral and neuroimaging analyses, using 4.7M triplet judgments to derive 66D embeddings for 1,854 objects.", "result": "Model embeddings showed semantic clustering like human cognition and aligned with neural activity in key brain regions.", "conclusion": "LLMs develop human-like object representations, advancing machine intelligence and human-like AI systems."}}
{"id": "2506.09313", "pdf": "https://arxiv.org/pdf/2506.09313", "abs": "https://arxiv.org/abs/2506.09313", "authors": ["Angel Yanguas-Gil", "Jeffrey W. Elam"], "title": "Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.plasm-ph"], "comment": null, "summary": "In this work we explore surrogate models to optimize plasma enhanced atomic\nlayer deposition (PEALD) in high aspect ratio features. In plasma-based\nprocesses such as PEALD and atomic layer etching, surface recombination can\ndominate the reactivity of plasma species with the surface, which can lead to\nunfeasibly long exposure times to achieve full conformality inside\nnanostructures like high aspect ratio vias. Using a synthetic dataset based on\nsimulations of PEALD, we train artificial neural networks to predict saturation\ntimes based on cross section thickness data obtained for partially coated\nconditions. The results obtained show that just two experiments in\nundersaturated conditions contain enough information to predict saturation\ntimes within 10% of the ground truth. A surrogate model trained to determine\nwhether surface recombination dominates the plasma-surface interactions in a\nPEALD process achieves 99% accuracy. This demonstrates that machine learning\ncan provide a new pathway to accelerate the optimization of PEALD processes in\nareas such as microelectronics. Our approach can be easily extended to atomic\nlayer etching and more complex structures.", "AI": {"tldr": "Machine learning accelerates PEALD optimization by predicting saturation times and identifying dominant surface recombination with high accuracy.", "motivation": "Surface recombination in PEALD can lead to impractical exposure times for conformality in nanostructures, necessitating faster optimization methods.", "method": "Artificial neural networks trained on synthetic PEALD simulation data predict saturation times and analyze plasma-surface interactions.", "result": "Two undersaturated experiments predict saturation times within 10% accuracy; a surrogate model achieves 99% accuracy in identifying dominant surface recombination.", "conclusion": "Machine learning offers a fast, accurate way to optimize PEALD and can be extended to other processes like atomic layer etching."}}
{"id": "2506.09552", "pdf": "https://arxiv.org/pdf/2506.09552", "abs": "https://arxiv.org/abs/2506.09552", "authors": ["Fatemeh Mohammadi Amin", "Darwin G. Caldwell", "Hans Wernher van de Venn"], "title": "Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments", "categories": ["cs.RO", "cs.CV"], "comment": "Preprint, Journal of Intelligent & Robotic Systems", "summary": "The robust interpretation of 3D environments is crucial for human-robot\ncollaboration (HRC) applications, where safety and operational efficiency are\nparamount. Semantic segmentation plays a key role in this context by enabling a\nprecise and detailed understanding of the environment. Considering the intense\ndata hunger for real-world industrial annotated data essential for effective\nsemantic segmentation, this paper introduces a pioneering approach in the\nSim2Real domain adaptation for semantic segmentation of 3D point cloud data,\nspecifically tailored for HRC. Our focus is on developing a network that\nrobustly transitions from simulated environments to real-world applications,\nthereby enhancing its practical utility and impact on a safe HRC.\n  In this work, we propose a dual-stream network architecture (FUSION)\ncombining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional\nNeural Networks (CNN) augmented with residual layers as a Sim2Real domain\nadaptation algorithm for an industrial environment. The proposed model was\nevaluated on real-world HRC setups and simulation industrial point clouds, it\nshowed increased state-of-the-art performance, achieving a segmentation\naccuracy of 97.76%, and superior robustness compared to existing methods.", "AI": {"tldr": "The paper introduces FUSION, a dual-stream network combining DGCNN and CNN for Sim2Real domain adaptation in 3D point cloud semantic segmentation, achieving 97.76% accuracy in HRC applications.", "motivation": "To address the lack of annotated real-world industrial data for semantic segmentation in HRC, ensuring safety and efficiency.", "method": "Proposes FUSION, a dual-stream network with DGCNN and CNN (augmented with residual layers) for Sim2Real adaptation.", "result": "Achieved 97.76% segmentation accuracy and superior robustness in real-world HRC setups.", "conclusion": "FUSION effectively bridges the Sim2Real gap, enhancing semantic segmentation for safer HRC."}}
{"id": "2411.12768", "pdf": "https://arxiv.org/pdf/2411.12768", "abs": "https://arxiv.org/abs/2411.12768", "authors": ["Nay Myat Min", "Long H. Pham", "Yige Li", "Jun Sun"], "title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025, 20 pages", "summary": "Large Language Models (LLMs) are vulnerable to backdoor attacks that\nmanipulate outputs via hidden triggers. Existing defense methods--designed for\nvision/text classification tasks--fail for text generation. We propose Internal\nConsistency Regularization (CROW), a defense leveraging the observation that\nbackdoored models exhibit unstable layer-wise hidden representations when\ntriggered, while clean models show smooth transitions. CROW enforces\nconsistency across layers via adversarial perturbations and regularization\nduring finetuning, neutralizing backdoors without requiring clean reference\nmodels or trigger knowledge--only a small clean dataset. Experiments across\nLlama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's\neffectiveness: it achieves significant reductions in attack success rates\nacross diverse backdoor strategies (sentiment steering, targeted refusal, code\ninjection) while preserving generative performance. CROW's\narchitecture-agnostic design enables practical deployment.", "AI": {"tldr": "CROW defends LLMs against backdoor attacks by enforcing internal consistency in hidden representations during finetuning, outperforming existing methods without needing clean models or trigger knowledge.", "motivation": "LLMs are vulnerable to backdoor attacks, and current defenses fail for text generation tasks.", "method": "Proposes Internal Consistency Regularization (CROW), which enforces layer-wise consistency via adversarial perturbations and regularization during finetuning.", "result": "CROW significantly reduces attack success rates across diverse backdoor strategies while maintaining generative performance.", "conclusion": "CROW is an effective, architecture-agnostic defense for LLMs against backdoor attacks."}}
{"id": "2408.00241", "pdf": "https://arxiv.org/pdf/2408.00241", "abs": "https://arxiv.org/abs/2408.00241", "authors": ["Minheng Xiao", "Zhizhong Wu"], "title": "Multiple Greedy Quasi-Newton Methods for Saddle Point Problems", "categories": ["cs.AI"], "comment": "Accepted by DOCS 2024", "summary": "This paper introduces the Multiple Greedy Quasi-Newton (MGSR1-SP) method, a\nnovel approach to solving strongly-convex-strongly-concave (SCSC) saddle point\nproblems. Our method enhances the approximation of the squared indefinite\nHessian matrix inherent in these problems, significantly improving both\nstability and efficiency through iterative greedy updates. We provide a\nthorough theoretical analysis of MGSR1-SP, demonstrating its linear-quadratic\nconvergence rate. Numerical experiments conducted on AUC maximization and\nadversarial debiasing problems, compared with state-of-the-art algorithms,\nunderscore our method's enhanced convergence rate. These results affirm the\npotential of MGSR1-SP to improve performance across a broad spectrum of machine\nlearning applications where efficient and accurate Hessian approximations are\ncrucial.", "AI": {"tldr": "The paper introduces MGSR1-SP, a method for solving SCSC saddle point problems, improving Hessian approximation for better stability and efficiency, with proven linear-quadratic convergence.", "motivation": "To address the challenge of efficiently and accurately approximating the squared indefinite Hessian matrix in SCSC saddle point problems, which is crucial for stability and performance in machine learning applications.", "method": "MGSR1-SP uses iterative greedy updates to enhance Hessian approximation, with a theoretical analysis of its convergence properties.", "result": "Numerical experiments on AUC maximization and adversarial debiasing show MGSR1-SP outperforms state-of-the-art methods in convergence rate.", "conclusion": "MGSR1-SP is a promising method for improving performance in machine learning tasks requiring efficient Hessian approximations."}}
{"id": "2506.09366", "pdf": "https://arxiv.org/pdf/2506.09366", "abs": "https://arxiv.org/abs/2506.09366", "authors": ["Yuxuan Kuang", "Haoran Geng", "Amine Elhafsi", "Tan-Dzung Do", "Pieter Abbeel", "Jitendra Malik", "Marco Pavone", "Yue Wang"], "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.", "AI": {"tldr": "SkillBlender is a hierarchical reinforcement learning framework for versatile humanoid loco-manipulation, blending pretrained primitive skills to minimize task-specific tuning.", "motivation": "Current methods for humanoid robot control require extensive task-specific tuning, limiting versatility and scalability for diverse daily tasks.", "method": "SkillBlender pretrains task-agnostic primitive skills and dynamically blends them for complex tasks, reducing reward engineering. SkillBench, a benchmark, evaluates performance.", "result": "SkillBlender outperforms baselines, avoids reward hacking, and achieves accurate, feasible movements for diverse tasks.", "conclusion": "SkillBlender enhances versatility and scalability in humanoid loco-manipulation, with open-sourced code and benchmark for community use."}}
{"id": "2506.09665", "pdf": "https://arxiv.org/pdf/2506.09665", "abs": "https://arxiv.org/abs/2506.09665", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "AI": {"tldr": "A method for generating high-quality 3D model materials using video diffusion models, intrinsic decomposition, and differentiable rendering, conditioned on text or image inputs.", "motivation": "To create high-quality, physically accurate materials for 3D models efficiently, leveraging modern generative and decomposition techniques.", "method": "1. Use a finetuned video diffusion model to generate coherent multi-view videos of 3D models. 2. Extract intrinsic properties (color, roughness, metallic) from the video. 3. Apply differentiable rendering to produce PBR materials.", "result": "Robust extraction of PBR materials compatible with standard content creation tools.", "conclusion": "The approach effectively bridges generative models and physical rendering for material synthesis."}}
{"id": "2411.17304", "pdf": "https://arxiv.org/pdf/2411.17304", "abs": "https://arxiv.org/abs/2411.17304", "authors": ["Milena Chadimov\u00e1", "Eduard Jur\u00e1\u0161ek", "Tom\u00e1\u0161 Kliegr"], "title": "Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent.", "AI": {"tldr": "The paper introduces 'hashing,' a method to mask bias-inducing words in LLMs, reducing cognitive biases and improving performance across various tasks and models.", "motivation": "To address cognitive biases and reliance on external knowledge in LLMs by masking problematic terms.", "method": "Masking bias-inducing words with hash-like identifiers, tested across 490 prompts and multiple LLMs (LLama, ChatGPT, etc.).", "result": "Significant improvements in bias reduction and task performance, though hallucination rates varied by model.", "conclusion": "Hashing effectively reduces biases and improves LLM performance, but its impact depends on the model and task."}}
{"id": "2408.05860", "pdf": "https://arxiv.org/pdf/2408.05860", "abs": "https://arxiv.org/abs/2408.05860", "authors": ["Minheng Xiao"], "title": "Root Cause Attribution of Delivery Risks via Causal Discovery with Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents a novel approach to root cause attribution of delivery\nrisks within supply chains by integrating causal discovery with reinforcement\nlearning. As supply chains become increasingly complex, traditional methods of\nroot cause analysis struggle to capture the intricate interrelationships\nbetween various factors, often leading to spurious correlations and suboptimal\ndecision-making. Our approach addresses these challenges by leveraging causal\ndiscovery to identify the true causal relationships between operational\nvariables, and reinforcement learning to iteratively refine the causal graph.\nThis method enables the accurate identification of key drivers of late\ndeliveries, such as shipping mode and delivery status, and provides actionable\ninsights for optimizing supply chain performance. We apply our approach to a\nreal-world supply chain dataset, demonstrating its effectiveness in uncovering\nthe underlying causes of delivery delays and offering strategies for mitigating\nthese risks. The findings have significant implications for improving\noperational efficiency, customer satisfaction, and overall profitability within\nsupply chains.", "AI": {"tldr": "A novel method combines causal discovery and reinforcement learning to identify root causes of supply chain delivery risks, outperforming traditional analysis.", "motivation": "Traditional root cause analysis fails in complex supply chains due to spurious correlations, necessitating a more accurate approach.", "method": "Integrates causal discovery to identify true causal relationships and reinforcement learning to refine the causal graph.", "result": "Effectively identifies key drivers of late deliveries (e.g., shipping mode, delivery status) and offers actionable insights.", "conclusion": "The approach improves supply chain efficiency, customer satisfaction, and profitability by accurately mitigating delivery risks."}}
{"id": "2506.09401", "pdf": "https://arxiv.org/pdf/2506.09401", "abs": "https://arxiv.org/abs/2506.09401", "authors": ["Vivek Shripad Borkar"], "title": "A theoretical basis for model collapse in recursive training", "categories": ["math.PR", "cs.LG", "68T01"], "comment": null, "summary": "It is known that recursive training from generative models can lead to the so\ncalled `collapse' of the simulated probability distribution. This note shows\nthat one in fact gets two different asymptotic behaviours depending on whether\nan external source, howsoever minor, is also contributing samples.", "AI": {"tldr": "Recursive training in generative models can lead to distribution collapse, but the presence of an external source alters the asymptotic behavior.", "motivation": "To understand the impact of external sources on the asymptotic behavior of generative models during recursive training.", "method": "Analyzes the asymptotic outcomes of recursive training with and without an external source contributing samples.", "result": "Two distinct asymptotic behaviors emerge: collapse without an external source and altered behavior with one.", "conclusion": "The presence of an external source significantly influences the asymptotic behavior of generative models, preventing collapse."}}
{"id": "2506.09930", "pdf": "https://arxiv.org/pdf/2506.09930", "abs": "https://arxiv.org/abs/2506.09930", "authors": ["Irving Fang", "Juexiao Zhang", "Shengbang Tong", "Chen Feng"], "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models", "categories": ["cs.RO", "cs.CV"], "comment": "Under review", "summary": "One promise that Vision-Language-Action (VLA) models hold over traditional\nimitation learning for robotics is to leverage the broad generalization\ncapabilities of large Vision-Language Models (VLMs) to produce versatile,\n\"generalist\" robot policies. However, current evaluations of VLAs remain\ninsufficient. Traditional imitation learning benchmarks are unsuitable due to\nthe lack of language instructions. Emerging benchmarks for VLAs that\nincorporate language often come with limited evaluation tasks and do not intend\nto investigate how much VLM pretraining truly contributes to the generalization\ncapabilities of the downstream robotic policy. Meanwhile, much research relies\non real-world robot setups designed in isolation by different institutions,\nwhich creates a barrier for reproducibility and accessibility. To address this\ngap, we introduce a unified probing suite of 50 simulation-based tasks across\n10 subcategories spanning language instruction, vision, and objects. We\nsystematically evaluate several state-of-the-art VLA architectures on this\nsuite to understand their generalization capability. Our results show that\nwhile VLM backbones endow VLAs with robust perceptual understanding and high\nlevel planning, which we refer to as good intentions, this does not reliably\ntranslate into precise motor execution: when faced with out-of-distribution\nobservations, policies often exhibit coherent intentions, but falter in action\nexecution. Moreover, finetuning on action data can erode the original VLM's\ngeneralist reasoning abilities. We release our task suite and evaluation code\nto serve as a standardized benchmark for future VLAs and to drive research on\nclosing the perception-to-action gap. More information, including the source\ncode, can be found at https://ai4ce.github.io/INT-ACT/", "AI": {"tldr": "The paper introduces a simulation-based task suite to evaluate Vision-Language-Action (VLA) models, revealing gaps between perceptual understanding and precise motor execution.", "motivation": "Current evaluations of VLAs are insufficient, lacking standardized benchmarks to assess generalization and the impact of VLM pretraining.", "method": "A unified suite of 50 simulation tasks across 10 subcategories is developed to systematically evaluate VLA architectures.", "result": "VLM backbones provide robust perception and planning but struggle with precise execution on out-of-distribution tasks; finetuning can degrade reasoning.", "conclusion": "The task suite serves as a benchmark to address the perception-to-action gap in VLAs, with released code for reproducibility."}}
{"id": "2411.18553", "pdf": "https://arxiv.org/pdf/2411.18553", "abs": "https://arxiv.org/abs/2411.18553", "authors": ["Darius Feher", "Ivan Vuli\u0107", "Benjamin Minixhofer"], "title": "Retrofitting Large Language Models with Dynamic Tokenization", "categories": ["cs.CL"], "comment": null, "summary": "Current language models (LMs) use a fixed, static subword tokenizer. This\ndefault choice typically results in degraded efficiency and language\ncapabilities, especially in languages other than English. To address this\nissue, we challenge the static design and propose retrofitting LMs with dynamic\ntokenization: a way to dynamically decide on token boundaries based on the\ninput text via a subword-merging algorithm inspired by byte-pair encoding. We\nmerge frequent subword sequences in a batch, then apply a pre-trained\nembedding-prediction hypernetwork to compute the token embeddings on-the-fly.\nFor encoder-style models (e.g., XLM-R), this on average reduces token sequence\nlengths by >20% across 14 languages while degrading performance by less than\n2%. The same method applied to pre-filling and scoring in decoder-style models\n(e.g., Mistral-7B) results in minimal performance degradation at up to 17%\nreduction in sequence length. Overall, we find that dynamic tokenization can\nmitigate the limitations of static tokenization by substantially improving\ninference speed and promoting fairness across languages, enabling more\nequitable and adaptable LMs.", "AI": {"tldr": "Dynamic tokenization improves LM efficiency and fairness by reducing token sequence lengths by >20% with minimal performance loss.", "motivation": "Static tokenizers degrade efficiency and language capabilities, especially in non-English languages.", "method": "Propose dynamic tokenization using a subword-merging algorithm and pre-trained embedding-prediction hypernetwork.", "result": "Reduces token sequence lengths by >20% in 14 languages with <2% performance degradation.", "conclusion": "Dynamic tokenization enhances inference speed and fairness, making LMs more adaptable and equitable."}}
{"id": "2408.11526", "pdf": "https://arxiv.org/pdf/2408.11526", "abs": "https://arxiv.org/abs/2408.11526", "authors": ["Mayank Kharbanda", "Rajiv Ratn Shah", "Raghava Mutharaju"], "title": "RConE: Rough Cone Embedding for Multi-Hop Logical Query Answering on Multi-Modal Knowledge Graphs", "categories": ["cs.AI"], "comment": "Accepted in TKDE (June 2025) as regular paper", "summary": "Multi-hop query answering over a Knowledge Graph (KG) involves traversing one\nor more hops from the start node to answer a query. Path-based and logic-based\nmethods are state-of-the-art for multi-hop question answering. The former is\nused in link prediction tasks. The latter is for answering complex logical\nqueries. The logical multi-hop querying technique embeds the KG and queries in\nthe same embedding space. The existing work incorporates First Order Logic\n(FOL) operators, such as conjunction ($\\wedge$), disjunction ($\\vee$), and\nnegation ($\\neg$), in queries. Though current models have most of the building\nblocks to execute the FOL queries, they cannot use the dense information of\nmulti-modal entities in the case of Multi-Modal Knowledge Graphs (MMKGs). We\npropose RConE, an embedding method to capture the multi-modal information\nneeded to answer a query. The model first shortlists candidate (multi-modal)\nentities containing the answer. It then finds the solution (sub-entities)\nwithin those entities. Several existing works tackle path-based\nquestion-answering in MMKGs. However, to our knowledge, we are the first to\nintroduce logical constructs in querying MMKGs and to answer queries that\ninvolve sub-entities of multi-modal entities as the answer. Extensive\nevaluation of four publicly available MMKGs indicates that RConE outperforms\nthe current state-of-the-art.", "AI": {"tldr": "RConE is a novel embedding method for answering logical multi-hop queries in Multi-Modal Knowledge Graphs (MMKGs), outperforming existing methods.", "motivation": "Existing models lack the ability to utilize dense multi-modal information in MMKGs for logical querying.", "method": "RConE embeds KG and queries in the same space, shortlists candidate multi-modal entities, and finds sub-entity solutions.", "result": "RConE outperforms state-of-the-art methods on four MMKGs.", "conclusion": "RConE is the first to introduce logical constructs in MMKG querying, addressing sub-entity answers effectively."}}
{"id": "2506.09406", "pdf": "https://arxiv.org/pdf/2506.09406", "abs": "https://arxiv.org/abs/2506.09406", "authors": ["Minji Kang", "Chanwoo Baek", "Yoonsang Lee"], "title": "Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Quadruped robots have made significant advances in locomotion, extending\ntheir capabilities from controlled environments to real-world applications.\nBeyond movement, recent work has explored loco-manipulation using the legs to\nperform tasks such as pressing buttons or opening doors. While these efforts\ndemonstrate the feasibility of leg-based manipulation, most have focused on\nrelatively static tasks. In this work, we propose a framework that enables\nquadruped robots to collect objects without additional actuators by leveraging\nthe agility of their legs. By attaching a simple scoop-like add-on to one leg,\nthe robot can scoop objects and toss them into a collection tray mounted on its\nback. Our method employs a hierarchical policy structure comprising two expert\npolicies-one for scooping and tossing, and one for approaching object\npositions-and a meta-policy that dynamically switches between them. The expert\npolicies are trained separately, followed by meta-policy training for\ncoordinated multi-object collection. This approach demonstrates how quadruped\nlegs can be effectively utilized for dynamic object manipulation, expanding\ntheir role beyond locomotion.", "AI": {"tldr": "A framework enables quadruped robots to dynamically collect objects using leg agility and a scoop-like add-on, trained via hierarchical policies.", "motivation": "Extend quadruped robots' capabilities beyond locomotion to dynamic object manipulation without additional actuators.", "method": "Hierarchical policy structure with expert policies for scooping/tossing and approaching objects, plus a meta-policy for coordination.", "result": "Demonstrates effective dynamic object manipulation using legs, expanding their utility.", "conclusion": "Quadruped legs can be repurposed for dynamic tasks, enhancing their versatility beyond locomotion."}}
{"id": "2506.09934", "pdf": "https://arxiv.org/pdf/2506.09934", "abs": "https://arxiv.org/abs/2506.09934", "authors": ["Jared Lawson", "Rohan Chitale", "Nabil Simaan"], "title": "Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 5 figures, accepted in Robotics and Automation Letters", "summary": "Safe navigation of steerable and robotic catheters in the cerebral\nvasculature requires awareness of the catheters shape and pose. Currently, a\nsignificant perception burden is placed on interventionalists to mentally\nreconstruct and predict catheter motions from biplane fluoroscopy images.\nEfforts to track these catheters are limited to planar segmentation or bulky\nsensing instrumentation, which are incompatible with microcatheters used in\nneurointervention. In this work, a catheter is equipped with custom radiopaque\nmarkers arranged to enable simultaneous shape and pose estimation under biplane\nfluoroscopy. A design measure is proposed to guide the arrangement of these\nmarkers to minimize sensitivity to marker tracking uncertainty. This approach\nwas deployed for microcatheters smaller than 2mm OD navigating phantom\nvasculature with shape tracking errors less than 1mm and catheter roll errors\nbelow 40 degrees. This work can enable steerable catheters to autonomously\nnavigate under biplane imaging.", "AI": {"tldr": "A method for tracking steerable catheters in cerebral vasculature using custom radiopaque markers, achieving sub-1mm shape errors and under 40-degree roll errors.", "motivation": "Current catheter tracking methods are either limited to planar segmentation or involve bulky sensors, making them unsuitable for microcatheters used in neurointervention.", "method": "Equipping catheters with custom radiopaque markers arranged to enable shape and pose estimation under biplane fluoroscopy, with a design measure to minimize tracking uncertainty.", "result": "Successful deployment in microcatheters (<2mm OD) with shape errors <1mm and roll errors <40 degrees in phantom vasculature.", "conclusion": "This approach enables autonomous navigation of steerable catheters under biplane imaging, reducing reliance on manual mental reconstruction."}}
{"id": "2412.05023", "pdf": "https://arxiv.org/pdf/2412.05023", "abs": "https://arxiv.org/abs/2412.05023", "authors": ["Krishnasai Addala", "Kabir Dev Paul Baghel", "Navya Gupta", "Rishitej Reddy Vyalla", "Chhavi Kirtani", "Avinash Anand", "Rajiv Ratn Shah"], "title": "Steps are all you need: Rethinking STEM Education with Prompt Engineering", "categories": ["cs.CL"], "comment": null, "summary": "Few shot and Chain-of-Thought prompting have shown promise when applied to\nPhysics Question Answering Tasks, but are limited by the lack of mathematical\nability inherent to LLMs, and are prone to hallucination. By utilizing a\nMixture of Experts (MoE) Model, along with analogical prompting, we are able to\nshow improved model performance when compared to the baseline on standard LLMs.\nWe also survey the limits of these prompting techniques and the effects they\nhave on model performance. Additionally, we propose Analogical CoT prompting, a\nprompting technique designed to allow smaller, open source models to leverage\nAnalogical prompting, something they have struggled with, possibly due to a\nlack of specialist training data.", "AI": {"tldr": "The paper explores improving Physics QA tasks using MoE models and analogical prompting, addressing LLM limitations like math ability and hallucination. It introduces Analogical CoT prompting for smaller models.", "motivation": "Address limitations of few-shot and Chain-of-Thought prompting in LLMs for Physics QA, such as poor math ability and hallucination.", "method": "Uses a Mixture of Experts (MoE) model and analogical prompting, and proposes Analogical CoT prompting for smaller models.", "result": "Shows improved performance over baseline LLMs and surveys the limits of prompting techniques.", "conclusion": "Proposes Analogical CoT prompting to help smaller models leverage analogical prompting, addressing their lack of specialist training data."}}
{"id": "2409.07507", "pdf": "https://arxiv.org/pdf/2409.07507", "abs": "https://arxiv.org/abs/2409.07507", "authors": ["Daniel Adam", "Tom\u00e1\u0161 Kliegr"], "title": "Traceable LLM-based validation of statements in knowledge graphs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This article presents a method for verifying RDF triples using LLMs, with an\nemphasis on providing traceable arguments. Because the LLMs cannot currently\nreliably identify the origin of the information used to construct the response\nto the user prompt, our approach is to avoid using internal LLM factual\nknowledge altogether. Instead, verified RDF statements are compared to chunks\nof external documents retrieved through a web search or Wikipedia. To assess\nthe possible application of this retrieval augmented generation (RAG) workflow\non biosciences content, we evaluated 1,719 positive statements from the BioRED\ndataset and the same number of newly generated negative statements. The\nresulting precision is 88 %, and recall is 44 %. This indicates that the method\nrequires human oversight. We also evaluated the method on the SNLI dataset,\nwhich allowed us to compare our approach with models specifically tuned for the\nnatural language inference task. We demonstrate the method on Wikidata, where a\nSPARQL query is used to automatically retrieve statements needing verification.\nOverall, the results suggest that LLMs could be used for large-scale\nverification of statements in KGs, a task previously unfeasible due to human\nannotation costs.", "AI": {"tldr": "A method for verifying RDF triples using LLMs avoids relying on LLM internal knowledge, instead comparing statements to external documents. Evaluated on BioRED and SNLI datasets, it shows 88% precision and 44% recall, requiring human oversight. Demonstrated on Wikidata, it suggests feasibility for large-scale KG verification.", "motivation": "LLMs lack traceability for information origins, so the method avoids using their internal knowledge to ensure verifiable arguments.", "method": "Compares verified RDF statements to external documents retrieved via web search or Wikipedia, using a retrieval augmented generation (RAG) workflow.", "result": "88% precision and 44% recall on BioRED dataset; comparison with SNLI dataset shows potential for natural language inference tasks.", "conclusion": "LLMs can enable large-scale KG verification, though human oversight is needed due to current limitations."}}
{"id": "2506.09422", "pdf": "https://arxiv.org/pdf/2506.09422", "abs": "https://arxiv.org/abs/2506.09422", "authors": ["Ye Niu", "Sanping Zhou", "Yizhe Li", "Ye Den", "Le Wang"], "title": "Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "In many complex scenarios, robotic manipulation relies on generative models\nto estimate the distribution of multiple successful actions. As the diffusion\nmodel has better training robustness than other generative models, it performs\nwell in imitation learning through successful robot demonstrations. However,\nthe diffusion-based policy methods typically require significant time to\niteratively denoise robot actions, which hinders real-time responses in robotic\nmanipulation. Moreover, existing diffusion policies model a time-varying action\ndenoising process, whose temporal complexity increases the difficulty of model\ntraining and leads to suboptimal action accuracy. To generate robot actions\nefficiently and accurately, we present the Time-Unified Diffusion Policy\n(TUDP), which utilizes action recognition capabilities to build a time-unified\ndenoising process. On the one hand, we build a time-unified velocity field in\naction space with additional action discrimination information. By unifying all\ntimesteps of action denoising, our velocity field reduces the difficulty of\npolicy learning and speeds up action generation. On the other hand, we propose\nan action-wise training method, which introduces an action discrimination\nbranch to supply additional action discrimination information. Through\naction-wise training, the TUDP implicitly learns the ability to discern\nsuccessful actions to better denoising accuracy. Our method achieves\nstate-of-the-art performance on RLBench with the highest success rate of 82.6%\non a multi-view setup and 83.8% on a single-view setup. In particular, when\nusing fewer denoising iterations, TUDP achieves a more significant improvement\nin success rate. Additionally, TUDP can produce accurate actions for a wide\nrange of real-world tasks.", "AI": {"tldr": "The paper introduces Time-Unified Diffusion Policy (TUDP) to improve robotic manipulation by unifying the denoising process and enhancing action accuracy, achieving state-of-the-art performance.", "motivation": "Existing diffusion-based policies for robotic manipulation are slow and complex, hindering real-time responses and accuracy.", "method": "TUDP unifies the denoising process with a time-unified velocity field and introduces action-wise training for better discrimination.", "result": "TUDP achieves 82.6% success rate (multi-view) and 83.8% (single-view) on RLBench, with notable improvements in fewer iterations.", "conclusion": "TUDP efficiently generates accurate actions, advancing robotic manipulation performance."}}
{"id": "2506.09990", "pdf": "https://arxiv.org/pdf/2506.09990", "abs": "https://arxiv.org/abs/2506.09990", "authors": ["Wenbo Zhang", "Tianrun Hu", "Yanyuan Qiao", "Hanbo Zhang", "Yuchu Qin", "Yang Li", "Jiajun Liu", "Tao Kong", "Lingqiao Liu", "Xiao Ma"], "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built\nupon Trajectory Autoregressive Modeling. Unlike conventional approaches that\npredict next step action(s) forward, CoA generates an entire trajectory by\nexplicit backward reasoning with task-specific goals through an action-level\nChain-of-Thought (CoT) process. This process is unified within a single\nautoregressive structure: (1) the first token corresponds to a stable keyframe\naction that encodes the task-specific goals; and (2) subsequent action tokens\nare generated autoregressively, conditioned on the initial keyframe and\npreviously predicted actions. This backward action reasoning enforces a\nglobal-to-local structure, allowing each local action to be tightly constrained\nby the final goal. To further realize the action reasoning structure, CoA\nincorporates four complementary designs: continuous action token\nrepresentation; dynamic stopping for variable-length trajectory generation;\nreverse temporal ensemble; and multi-token prediction to balance action chunk\nmodeling with global structure. As a result, CoA gives strong spatial\ngeneralization capabilities while preserving the flexibility and simplicity of\na visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art\nperformance across 60 RLBench tasks and 8 real-world manipulation tasks.", "AI": {"tldr": "CoA introduces a backward-reasoning visuo-motor policy for trajectory generation, outperforming state-of-the-art methods in both simulated and real-world tasks.", "motivation": "To improve trajectory generation by enforcing global-to-local action constraints through backward reasoning, enhancing spatial generalization and task performance.", "method": "CoA uses backward reasoning with task-specific goals, unified in an autoregressive structure, and incorporates four designs for efficient trajectory generation.", "result": "Achieves state-of-the-art performance on 60 RLBench tasks and 8 real-world manipulation tasks.", "conclusion": "CoA's backward-reasoning paradigm and complementary designs enable superior performance and generalization in visuo-motor policies."}}
{"id": "2412.05342", "pdf": "https://arxiv.org/pdf/2412.05342", "abs": "https://arxiv.org/abs/2412.05342", "authors": ["Xiaoyu Wang", "Ningyuan Xi", "Teng Chen", "Qingqing Gu", "Yue Zhao", "Xiaokai Chen", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "title": "Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCNN 2025", "summary": "Large Language Models (LLM) are usually fine-tuned to participate in dyadic\nor two-party dialogues, which can not adapt well to multi-party dialogues\n(MPD), which hinders their applications in such scenarios including\nmulti-personal meetings, discussions and daily communication. Previous\nLLM-based researches mainly focus on the multi-agent framework, while their\nbase LLMs are still pairwisely fine-tuned. In this work, we design a\nmulti-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue\ndatasets, and prove such a straightforward framework can let the LLM align with\nthe multi-party conversation style efficiently and effectively. We also design\ntwo training strategies which can convert MuPaS into the MPD simulator.\nSubstantial experiments show that MuPaS can achieve state-of-the-art\nmulti-party response, higher accuracy of the-next-speaker prediction, higher\nhuman and automatic evaluated utterance qualities, and can even generate\nreasonably with out-of-distribution scene, topic and role descriptions. The\nMuPaS framework bridges the LLM training with more complicated multi-party\napplications, such as conversation generation, virtual rehearsal or\nmeta-universe.", "AI": {"tldr": "MuPaS is a multi-party fine-tuning framework for LLMs, improving their performance in multi-party dialogues (MPD) over traditional dyadic fine-tuning.", "motivation": "Current LLMs are fine-tuned for dyadic dialogues, limiting their effectiveness in multi-party scenarios like meetings or discussions.", "method": "Introduces MuPaS, a framework for fine-tuning LLMs on multi-party dialogue datasets, with two training strategies for MPD simulation.", "result": "MuPaS achieves state-of-the-art multi-party responses, better next-speaker prediction, and high-quality utterances, even in out-of-distribution scenarios.", "conclusion": "MuPaS bridges LLM training with complex multi-party applications, enabling broader use in conversation generation and virtual environments."}}
{"id": "2410.02197", "pdf": "https://arxiv.org/pdf/2410.02197", "abs": "https://arxiv.org/abs/2410.02197", "authors": ["Yifan Zhang", "Ge Zhang", "Yue Wu", "Kangping Xu", "Quanquan Gu"], "title": "Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "Modeling human preferences is crucial for aligning foundation models with\nhuman values. Traditional reward modeling methods, such as the Bradley-Terry\n(BT) reward model, fall short in expressiveness, particularly in addressing\nintransitive preferences. In this paper, we introduce preference embedding, an\napproach that embeds responses into a latent space to capture intricate\npreference structures efficiently, achieving linear query complexity.\nAdditionally, we propose preference score-based General Preference Optimization\n(GPO), which generalizes reward-based reinforcement learning from human\nfeedback (RLHF). Experimental results show that our General Preference\nembedding Model (GPM) consistently outperforms the BT reward model on the\nRewardBench benchmark and effectively models cyclic preferences where any BT\nreward model behaves like a random guess. Furthermore, evaluations on\ndownstream tasks such as AlpacaEval2.0, following the language model\npost-training with GPO and our general preference model, reveal performance\nimprovements over BT models. These findings indicate that our method may\nenhance the alignment of foundation models with nuanced human values. The code\nis available at https://github.com/general-preference/general-preference-model.", "AI": {"tldr": "The paper introduces preference embedding and General Preference Optimization (GPO) to better model human preferences, outperforming traditional Bradley-Terry (BT) models in expressiveness and alignment with human values.", "motivation": "Traditional reward models like BT lack expressiveness for intransitive preferences, limiting alignment of foundation models with nuanced human values.", "method": "Proposes preference embedding for efficient latent space modeling and GPO to generalize RLHF, achieving linear query complexity.", "result": "GPM outperforms BT on RewardBench, handles cyclic preferences, and improves performance on tasks like AlpacaEval2.0.", "conclusion": "The method enhances alignment of foundation models with human values, demonstrated by superior performance over BT models."}}
{"id": "2506.09441", "pdf": "https://arxiv.org/pdf/2506.09441", "abs": "https://arxiv.org/abs/2506.09441", "authors": ["Piyush Mishra", "Philippe Roudot"], "title": "Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Tracking multiple particles in noisy and cluttered scenes remains challenging\ndue to a combinatorial explosion of trajectory hypotheses, which scales\nsuper-exponentially with the number of particles and frames. The transformer\narchitecture has shown a significant improvement in robustness against this\nhigh combinatorial load. However, its performance still falls short of the\nconventional Bayesian filtering approaches in scenarios presenting a reduced\nset of trajectory hypothesis. This suggests that while transformers excel at\nnarrowing down possible associations, they may not be able to reach the\noptimality of the Bayesian approach in locally sparse scenario. Hence, we\nintroduce a hybrid tracking framework that combines the ability of\nself-attention to learn the underlying representation of particle behavior with\nthe reliability and interpretability of Bayesian filtering. We perform\ntrajectory-to-detection association by solving a label prediction problem,\nusing a transformer encoder to infer soft associations between detections\nacross frames. This prunes the hypothesis set, enabling efficient\nmultiple-particle tracking in Bayesian filtering framework. Our approach\ndemonstrates improved tracking accuracy and robustness against spurious\ndetections, offering a solution for high clutter multiple particle tracking\nscenarios.", "AI": {"tldr": "A hybrid framework combining transformers and Bayesian filtering improves particle tracking in noisy, cluttered scenes by leveraging self-attention for hypothesis pruning and Bayesian methods for optimality.", "motivation": "Existing transformer-based tracking struggles with optimality in sparse scenarios, while Bayesian filtering excels but faces combinatorial challenges. A hybrid approach is proposed to bridge this gap.", "method": "The framework uses a transformer encoder for soft association predictions to prune trajectory hypotheses, followed by Bayesian filtering for reliable tracking.", "result": "The hybrid method enhances tracking accuracy and robustness against spurious detections in high-clutter scenarios.", "conclusion": "Combining transformers and Bayesian filtering offers a superior solution for multiple-particle tracking in noisy, cluttered environments."}}
{"id": "2302.07944", "pdf": "https://arxiv.org/pdf/2302.07944", "abs": "https://arxiv.org/abs/2302.07944", "authors": ["Brandon Trabucco", "Kyle Doherty", "Max Gurinas", "Ruslan Salakhutdinov"], "title": "Effective Data Augmentation With Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Update to ICLR 2024 manuscript\n  (https://openreview.net/forum?id=ZWzUA9zeAg), add leafy spurge citations", "summary": "Data augmentation is one of the most prevalent tools in deep learning,\nunderpinning many recent advances, including those from classification,\ngenerative models, and representation learning. The standard approach to data\naugmentation combines simple transformations like rotations and flips to\ngenerate new images from existing ones. However, these new images lack\ndiversity along key semantic axes present in the data. Current augmentations\ncannot alter the high-level semantic attributes, such as animal species present\nin a scene, to enhance the diversity of data. We address the lack of diversity\nin data augmentation with image-to-image transformations parameterized by\npre-trained text-to-image diffusion models. Our method edits images to change\ntheir semantics using an off-the-shelf diffusion model, and generalizes to\nnovel visual concepts from a few labelled examples. We evaluate our approach on\nfew-shot image classification tasks, and on a real-world weed recognition task,\nand observe an improvement in accuracy in tested domains.", "AI": {"tldr": "The paper introduces a method using text-to-image diffusion models for data augmentation to enhance semantic diversity in images, improving few-shot classification and real-world tasks like weed recognition.", "motivation": "Standard data augmentation lacks diversity in high-level semantic attributes, limiting its effectiveness in tasks requiring varied visual concepts.", "method": "Leverages pre-trained text-to-image diffusion models to edit images and alter their semantics, generalizing from few labeled examples.", "result": "Shows improved accuracy in few-shot image classification and a real-world weed recognition task.", "conclusion": "The proposed method addresses the diversity gap in data augmentation, enhancing performance in practical applications."}}
{"id": "2412.05453", "pdf": "https://arxiv.org/pdf/2412.05453", "abs": "https://arxiv.org/abs/2412.05453", "authors": ["Krishnasai Addala", "Kabir Dev Paul Baghel", "Dhruv Jain", "Navya Gupta", "Rishitej Reddy Vyalla", "Chhavi Kirtani", "Avinash Anand", "Rajiv Ratn Shah"], "title": "Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "This study explores the effectiveness of using knowledge graphs generated by\nlarge language models to decompose high school-level physics questions into\nsub-questions. We introduce a pipeline aimed at enhancing model response\nquality for Question Answering tasks. By employing LLMs to construct knowledge\ngraphs that capture the internal logic of the questions, these graphs then\nguide the generation of subquestions. We hypothesize that this method yields\nsub-questions that are more logically consistent with the original questions\ncompared to traditional decomposition techniques. Our results show that\nsub-questions derived from knowledge graphs exhibit significantly improved\nfidelity to the original question's logic. This approach not only enhances the\nlearning experience by providing clearer and more contextually appropriate\nsub-questions but also highlights the potential of LLMs to transform\neducational methodologies. The findings indicate a promising direction for\napplying AI to improve the quality and effectiveness of educational content.", "AI": {"tldr": "The study uses LLM-generated knowledge graphs to decompose physics questions into sub-questions, improving fidelity and educational quality.", "motivation": "To enhance model response quality in Question Answering tasks by leveraging knowledge graphs for logically consistent sub-question generation.", "method": "Employing LLMs to construct knowledge graphs that guide sub-question generation, comparing results to traditional techniques.", "result": "Sub-questions from knowledge graphs show significantly improved logical consistency with the original questions.", "conclusion": "The approach enhances educational content quality and demonstrates LLMs' potential in transforming educational methodologies."}}
{"id": "2410.18374", "pdf": "https://arxiv.org/pdf/2410.18374", "abs": "https://arxiv.org/abs/2410.18374", "authors": ["Zi-Rui Wang"], "title": "Improving Handwritten Text Recognition via 3D Attention and Multi-Scale Training", "categories": ["cs.AI"], "comment": null, "summary": "The segmentation-free research efforts for addressing handwritten text\nrecognition can be divided into three categories: connectionist temporal\nclassification (CTC), hidden Markov model and encoder-decoder methods. In this\npaper, inspired by the above three modeling methods, we propose a new\nrecognition network by using a novel three-dimensional (3D) attention module\nand global-local context information. Based on the feature maps of the last\nconvolutional layer, a series of 3D blocks with different resolutions are\nsplit. Then, these 3D blocks are fed into the 3D attention module to generate\nsequential visual features. Finally, by integrating the visual features and the\ncorresponding global-local context features, a well-designed representation can\nbe obtained. Main canonical neural units including attention mechanisms,\nfully-connected layer, recurrent unit and convolutional layer are efficiently\norganized into a network and can be jointly trained by the CTC loss and the\ncross-entropy loss. Experiments on the latest Chinese handwritten text datasets\n(the SCUT-HCCDoc and the SCUT-EPT) and one English handwritten text dataset\n(the IAM) show that the proposed method can achieve comparable results with the\nstate-of-the-art methods. The code is available at\nhttps://github.com/Wukong90/3DAttention-MultiScaleTraining-for-HTR.", "AI": {"tldr": "The paper proposes a new handwritten text recognition network using a 3D attention module and global-local context, achieving state-of-the-art results on Chinese and English datasets.", "motivation": "To improve handwritten text recognition by integrating CTC, hidden Markov models, and encoder-decoder methods with a novel 3D attention module and context features.", "method": "Uses 3D blocks from convolutional layers, processes them with a 3D attention module, and combines visual and global-local context features. Trained with CTC and cross-entropy losses.", "result": "Achieves comparable performance to state-of-the-art methods on SCUT-HCCDoc, SCUT-EPT, and IAM datasets.", "conclusion": "The proposed method effectively integrates attention and context features for robust handwritten text recognition."}}
{"id": "2506.09512", "pdf": "https://arxiv.org/pdf/2506.09512", "abs": "https://arxiv.org/abs/2506.09512", "authors": ["Donglin Wang", "Anjie Qiu", "Qiuheng Zhou", "Hans D. Schotten"], "title": "A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "7 pages, 1 figure", "summary": "The rapid advancement of Vehicle-to-Everything (V2X) communication is\ntransforming Intelligent Transportation Systems (ITS), with 6G networks\nexpected to provide ultra-reliable, low-latency, and high-capacity connectivity\nfor Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and\nMachine Learning (ML) have emerged as key enablers in optimizing V2X\ncommunication by enhancing network management, predictive analytics, security,\nand cooperative driving due to their outstanding performance across various\ndomains, such as natural language processing and computer vision. This survey\ncomprehensively reviews recent advances in AI and ML models applied to 6G-V2X\ncommunication. It focuses on state-of-the-art techniques, including Deep\nLearning (DL), Reinforcement Learning (RL), Generative Learning (GL), and\nFederated Learning (FL), with particular emphasis on developments from the past\ntwo years. Notably, AI, especially GL, has shown remarkable progress and\nemerging potential in enhancing the performance, adaptability, and intelligence\nof 6G-V2X systems. Despite these advances, a systematic summary of recent\nresearch efforts in this area remains lacking, which this survey aims to\naddress. We analyze their roles in 6G-V2X applications, such as intelligent\nresource allocation, beamforming, intelligent traffic management, and security\nmanagement. Furthermore, we explore the technical challenges, including\ncomputational complexity, data privacy, and real-time decision-making\nconstraints, while identifying future research directions for AI-driven 6G-V2X\ndevelopment. This study aims to provide valuable insights for researchers,\nengineers, and policymakers working towards realizing intelligent, AI-powered\nV2X ecosystems in 6G communication.", "AI": {"tldr": "This survey reviews AI and ML advancements in 6G-V2X communication, highlighting techniques like DL, RL, GL, and FL, and their applications in resource allocation, traffic management, and security. It also addresses challenges and future research directions.", "motivation": "The rapid evolution of V2X communication in 6G networks necessitates a comprehensive review of AI and ML's role in optimizing performance, adaptability, and intelligence, given the lack of systematic summaries in recent research.", "method": "The survey analyzes state-of-the-art AI and ML techniques (DL, RL, GL, FL) applied to 6G-V2X, focusing on developments from the past two years and their applications in network management, security, and cooperative driving.", "result": "AI, particularly GL, shows significant potential in enhancing 6G-V2X systems. The survey identifies key applications and challenges like computational complexity and data privacy.", "conclusion": "The study provides insights for researchers and policymakers to advance AI-driven 6G-V2X ecosystems, addressing current challenges and outlining future research directions."}}
{"id": "2311.09652", "pdf": "https://arxiv.org/pdf/2311.09652", "abs": "https://arxiv.org/abs/2311.09652", "authors": ["Aniket Dashpute", "Jiazhang Wang", "James Taylor", "Oliver Cossairt", "Ashok Veeraraghavan", "Florian Willomitzer"], "title": "Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Event-based structured light systems have recently been introduced as an\nexciting alternative to conventional frame-based triangulation systems for the\n3D measurements of diffuse surfaces. Important benefits include the fast\ncapture speed and the high dynamic range provided by the event camera - albeit\nat the cost of lower data quality. So far, both low-accuracy event-based and\nhigh-accuracy frame-based 3D imaging systems are tailored to a specific surface\ntype, such as diffuse or specular, and can not be used for a broader class of\nobject surfaces (\"mixed reflectance scenes\"). In this work, we present a novel\nevent-based structured light system that enables fast 3D imaging of mixed\nreflectance scenes with high accuracy. On the captured events, we use epipolar\nconstraints that intrinsically enable decomposing the measured reflections into\ndiffuse, two-bounce specular, and other multi-bounce reflections. The diffuse\nsurfaces in the scene are reconstructed using triangulation. Then, the\nreconstructed diffuse scene parts are leveraged as a \"display\" to evaluate the\nspecular scene parts via deflectometry. This novel procedure allows us to use\nthe entire scene as a virtual screen, using only a scanning laser and an event\ncamera. The resulting system achieves fast and motion-robust (14Hz)\nreconstructions of mixed reflectance scenes with < 600 ${\\mu}m$ depth error.\nMoreover, we introduce an \"ultrafast\" capture mode (250Hz) for the 3D\nmeasurement of diffuse scenes.", "AI": {"tldr": "A novel event-based structured light system enables fast, high-accuracy 3D imaging of mixed reflectance scenes by decomposing reflections and combining triangulation with deflectometry.", "motivation": "Existing 3D imaging systems are limited to specific surface types (diffuse or specular), lacking versatility for mixed reflectance scenes.", "method": "Uses epipolar constraints to decompose reflections, triangulates diffuse surfaces, and leverages them as a virtual screen for deflectometry of specular parts.", "result": "Achieves fast (14Hz) and motion-robust reconstructions with <600\u03bcm depth error for mixed scenes, and an ultrafast mode (250Hz) for diffuse scenes.", "conclusion": "The system bridges the gap between event-based and frame-based systems, offering high accuracy and versatility for mixed reflectance scenes."}}
{"id": "2412.06845", "pdf": "https://arxiv.org/pdf/2412.06845", "abs": "https://arxiv.org/abs/2412.06845", "authors": ["Pu Zhao", "Xuan Shen", "Zhenglun Kong", "Yixin Shen", "Sung-En Chang", "Timothy Rupprecht", "Lei Lu", "Enfu Nan", "Changdi Yang", "Yumei He", "Weiyan Shi", "Xingchen Xu", "Yu Huang", "Wei Jiang", "Wei Wang", "Yue Chen", "Yong He", "Yanzhi Wang"], "title": "7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA, have made great contributions to the ever-increasing popularity\nof LLMs due to the ease to customize and deploy the models across diverse\napplications. Although open-source LLMs present unprecedented opportunities for\ninnovation and research, the commercialization of LLMs has raised concerns\nabout transparency, reproducibility, and safety. Many open-source LLMs fail to\nmeet fundamental transparency requirements by withholding essential components\nlike training code and data, which may hinder further innovations on LLMs. To\nmitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,\nadhering to principles of open science, open source, open data, and open\naccess. We release the pre-training code and configurations, training and\nfine-tuning datasets, and intermediate and final checkpoints, aiming to make\ncontinuous commitments to fully open-source LLMs. After pre-training the base\nmodel, we finetune the Moxin Base model with SOTA post-training framework and\ninstruction data to obtain Moxin Instruct model. To improve the reasoning\ncapability, we further finetune our Instruct model with chain-of-thought data\ndistilled from DeepSeek R1, and then use Group Relative Policy Optimization\n(GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin\nReasoning model. Moreover, we develop our vision language model based on our\nMoxin model. Experiments show that our models achieve superior performance in\nvarious evaluations such as zero-shot evaluation, few-shot evaluation, and CoT\nevaluation.", "AI": {"tldr": "Moxin 7B is a fully open-source LLM addressing transparency and reproducibility issues in LLMs by releasing all components (code, data, checkpoints). It includes base, instruct, reasoning, and vision language models, achieving top performance in evaluations.", "motivation": "The commercialization of LLMs has raised concerns about transparency and reproducibility. Open-source LLMs often lack essential components like training code and data, hindering innovation.", "method": "Developed Moxin 7B with full openness (code, data, checkpoints). Fine-tuned base model into instruct and reasoning models using SOTA frameworks and data. Also created a vision language model.", "result": "Superior performance in zero-shot, few-shot, and chain-of-thought evaluations.", "conclusion": "Moxin 7B demonstrates the feasibility and benefits of fully open-source LLMs, promoting transparency and innovation."}}
{"id": "2411.12977", "pdf": "https://arxiv.org/pdf/2411.12977", "abs": "https://arxiv.org/abs/2411.12977", "authors": ["Mircea Lic\u0103", "Ojas Shirekar", "Baptiste Colle", "Chirag Raman"], "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Embodied agents powered by large language models (LLMs), such as Voyager,\npromise open-ended competence in worlds such as Minecraft. However, when\npowered by open-weight LLMs they still falter on elementary tasks after\ndomain-specific fine-tuning. We propose MindForge, a generative-agent framework\nfor cultural lifelong learning through explicit perspective taking. We\nintroduce three key innovations: (1) a structured theory of mind representation\nlinking percepts, beliefs, desires, and actions; (2) natural inter-agent\ncommunication; and (3) a multi-component memory system. Following the cultural\nlearning framework, we test MindForge in both instructive and collaborative\nsettings within Minecraft. In an instructive setting with GPT-4, MindForge\nagents powered by open-weight LLMs significantly outperform their Voyager\ncounterparts in basic tasks yielding $3\\times$ more tech-tree milestones and\ncollecting $2.3\\times$ more unique items than the Voyager baseline.\nFurthermore, in fully \\textit{collaborative} settings, we find that the\nperformance of two underachieving agents improves with more communication\nrounds, echoing the Condorcet Jury Theorem. MindForge agents demonstrate\nsophisticated behaviors, including expert-novice knowledge transfer,\ncollaborative problem solving, and adaptation to out-of-distribution tasks\nthrough accumulated cultural experiences.", "AI": {"tldr": "MindForge, a generative-agent framework, enhances embodied agents' performance in Minecraft by integrating perspective-taking, inter-agent communication, and memory systems, outperforming Voyager in basic tasks and collaborative settings.", "motivation": "Existing embodied agents powered by LLMs, like Voyager, struggle with elementary tasks despite fine-tuning. MindForge aims to improve competence through cultural lifelong learning and explicit perspective-taking.", "method": "MindForge introduces: (1) a structured theory of mind, (2) natural inter-agent communication, and (3) a multi-component memory system. It is tested in Minecraft for instructive and collaborative tasks.", "result": "MindForge agents outperform Voyager, achieving 3\u00d7 more tech-tree milestones and 2.3\u00d7 more unique items. In collaborative settings, performance improves with more communication rounds.", "conclusion": "MindForge enables sophisticated behaviors like knowledge transfer, collaborative problem-solving, and adaptation to novel tasks, demonstrating the value of cultural learning in embodied AI."}}
{"id": "2506.09516", "pdf": "https://arxiv.org/pdf/2506.09516", "abs": "https://arxiv.org/abs/2506.09516", "authors": ["Yingying Fan", "Jinchi Lv", "Ao Sun", "Yurou Wang"], "title": "LLM-Powered CPI Prediction Inference with Online Text Time Series", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "73 pages, 13 figures", "summary": "Forecasting the Consumer Price Index (CPI) is an important yet challenging\ntask in economics, where most existing approaches rely on low-frequency,\nsurvey-based data. With the recent advances of large language models (LLMs),\nthere is growing potential to leverage high-frequency online text data for\nimproved CPI prediction, an area still largely unexplored. This paper proposes\nLLM-CPI, an LLM-based approach for CPI prediction inference incorporating\nonline text time series. We collect a large set of high-frequency online texts\nfrom a popularly used Chinese social network site and employ LLMs such as\nChatGPT and the trained BERT models to construct continuous inflation labels\nfor posts that are related to inflation. Online text embeddings are extracted\nvia LDA and BERT. We develop a joint time series framework that combines\nmonthly CPI data with LLM-generated daily CPI surrogates. The monthly model\nemploys an ARX structure combining observed CPI data with text embeddings and\nmacroeconomic variables, while the daily model uses a VARX structure built on\nLLM-generated CPI surrogates and text embeddings. We establish the asymptotic\nproperties of the method and provide two forms of constructed prediction\nintervals. The finite-sample performance and practical advantages of LLM-CPI\nare demonstrated through both simulation and real data examples.", "AI": {"tldr": "The paper introduces LLM-CPI, an LLM-based method for forecasting the Consumer Price Index (CPI) using high-frequency online text data, combining it with traditional low-frequency data for improved accuracy.", "motivation": "Existing CPI forecasting methods rely on low-frequency, survey-based data, leaving the potential of high-frequency online text data unexplored. This paper aims to leverage LLMs to bridge this gap.", "method": "The approach involves collecting high-frequency online texts, using LLMs (e.g., ChatGPT, BERT) to generate inflation labels, extracting text embeddings, and developing a joint time series framework (ARX for monthly, VARX for daily data) combining CPI data with LLM-generated surrogates.", "result": "The method demonstrates strong finite-sample performance and practical advantages in simulations and real-world applications, with asymptotic properties and prediction intervals provided.", "conclusion": "LLM-CPI effectively integrates high-frequency online text data with traditional CPI forecasting, offering improved accuracy and new insights into inflation prediction."}}
{"id": "2312.05219", "pdf": "https://arxiv.org/pdf/2312.05219", "abs": "https://arxiv.org/abs/2312.05219", "authors": ["Houting Li", "Mengxuan Dong", "Lok Ming Lui"], "title": "Enhancing Facial Classification and Recognition using 3D Facial Models and Deep Learning", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:1903.08527 by other authors", "summary": "Accurate analysis and classification of facial attributes are essential in\nvarious applications, from human-computer interaction to security systems. In\nthis work, a novel approach to enhance facial classification and recognition\ntasks through the integration of 3D facial models with deep learning methods\nwas proposed. We extract the most useful information for various tasks using\nthe 3D Facial Model, leading to improved classification accuracy. Combining 3D\nfacial insights with ResNet architecture, our approach achieves notable\nresults: 100% individual classification, 95.4% gender classification, and 83.5%\nexpression classification accuracy. This method holds promise for advancing\nfacial analysis and recognition research.", "AI": {"tldr": "A novel method combining 3D facial models with deep learning (ResNet) improves facial classification accuracy, achieving 100% individual, 95.4% gender, and 83.5% expression classification.", "motivation": "Accurate facial attribute analysis is crucial for applications like human-computer interaction and security systems.", "method": "Integration of 3D facial models with deep learning (ResNet) to extract useful information for classification tasks.", "result": "Achieved 100% individual, 95.4% gender, and 83.5% expression classification accuracy.", "conclusion": "The method shows promise for advancing facial analysis and recognition research."}}
{"id": "2501.16884", "pdf": "https://arxiv.org/pdf/2501.16884", "abs": "https://arxiv.org/abs/2501.16884", "authors": ["Peiling Yi", "Yuhan Xia", "Yunfei Long"], "title": "Irony Detection, Reasoning and Understanding in Zero-shot Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generalisation of irony detection faces significant challenges, leading\nto substantial performance deviations when detection models are applied to\ndiverse real-world scenarios. In this study, we find that irony-focused\nprompts, as generated from our IDADP framework for LLMs, can not only overcome\ndataset-specific limitations but also generate coherent, human-readable\nreasoning, transforming ironic text into its intended meaning. Based on our\nfindings and in-depth analysis, we identify several promising directions for\nfuture research aimed at enhancing LLMs' zero-shot capabilities in irony\ndetection, reasoning, and comprehension. These include advancing contextual\nawareness in irony detection, exploring hybrid symbolic-neural methods, and\nintegrating multimodal data, among others.", "AI": {"tldr": "The paper addresses challenges in irony detection generalization and introduces the IDADP framework for LLMs to improve irony detection and reasoning.", "motivation": "Overcoming dataset-specific limitations and enhancing irony detection models for diverse real-world scenarios.", "method": "Using irony-focused prompts generated from the IDADP framework for LLMs to transform ironic text into its intended meaning.", "result": "Improved irony detection and generation of coherent, human-readable reasoning.", "conclusion": "Identifies future research directions like advancing contextual awareness, hybrid symbolic-neural methods, and multimodal data integration to enhance LLMs' irony detection capabilities."}}
{"id": "2412.04139", "pdf": "https://arxiv.org/pdf/2412.04139", "abs": "https://arxiv.org/abs/2412.04139", "authors": ["Jungwoo Park", "Young Jin Ahn", "Kee-Eung Kim", "Jaewoo Kang"], "title": "Monet: Mixture of Monosemantic Experts for Transformers", "categories": ["cs.AI"], "comment": null, "summary": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet.", "AI": {"tldr": "The paper introduces Monet, a Mixture of Monosemantic Experts for Transformers, to improve interpretability of LLMs by scaling expert counts and enabling direct knowledge manipulation without performance loss.", "motivation": "Understanding and aligning LLM computations with human values is challenging due to polysemanticity (neurons responding to unrelated concepts). Existing methods like Sparse Autoencoders compromise performance.", "method": "Monet integrates sparse dictionary learning into Mixture-of-Experts pretraining, scaling experts to 262,144 per layer with parameters growing proportionally to the square root of expert count.", "result": "Monet achieves mutual exclusivity of knowledge across experts, enables domain/language/toxicity manipulation, and maintains general performance.", "conclusion": "Scaling expert counts enhances mechanistic interpretability and allows direct adjustment of model behavior, advancing transparent LLMs."}}
{"id": "2506.09562", "pdf": "https://arxiv.org/pdf/2506.09562", "abs": "https://arxiv.org/abs/2506.09562", "authors": ["Songze Li", "Mingxuan Zhang", "Oubo Ma", "Kang Wei", "Shouling Ji"], "title": "TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Deep reinforcement learning (DRL) has achieved remarkable success in a wide\nrange of sequential decision-making domains, including robotics, healthcare,\nsmart grids, and finance. Recent research demonstrates that attackers can\nefficiently exploit system vulnerabilities during the training phase to execute\nbackdoor attacks, producing malicious actions when specific trigger patterns\nare present in the state observations. However, most existing backdoor attacks\nrely primarily on simplistic and heuristic trigger configurations, overlooking\nthe potential efficacy of trigger optimization. To address this gap, we\nintroduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor\nAttacks on DRL), the first framework to systematically optimize DRL backdoor\ntriggers along three critical axes, i.e., temporal, spatial, and magnitude.\nSpecifically, we first introduce a performance-aware adaptive freezing\nmechanism for injection timing. Then, we formulate dimension selection as a\ncooperative game, utilizing Shapley value analysis to identify the most\ninfluential state variable for the injection dimension. Furthermore, we propose\na gradient-based adversarial procedure to optimize the injection magnitude\nunder environment constraints. Evaluations on three mainstream DRL algorithms\nand nine benchmark tasks show that TooBadRL significantly improves attack\nsuccess rates, while ensuring minimal degradation of normal task performance.\nThese results highlight the previously underappreciated importance of\nprincipled trigger optimization in DRL backdoor attacks. The source code of\nTooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.", "AI": {"tldr": "TooBadRL is a framework optimizing DRL backdoor triggers along temporal, spatial, and magnitude axes, significantly improving attack success rates while minimizing performance degradation.", "motivation": "Existing backdoor attacks on DRL rely on simplistic triggers, overlooking the potential of optimized triggers.", "method": "TooBadRL introduces adaptive freezing for timing, Shapley value analysis for dimension selection, and gradient-based optimization for magnitude.", "result": "Evaluations show TooBadRL boosts attack success rates on three DRL algorithms and nine tasks with minimal performance impact.", "conclusion": "Trigger optimization is crucial for effective DRL backdoor attacks, as demonstrated by TooBadRL."}}
{"id": "2403.16998", "pdf": "https://arxiv.org/pdf/2403.16998", "abs": "https://arxiv.org/abs/2403.16998", "authors": ["Kanchana Ranasinghe", "Xiang Li", "Kumara Kahatapitiya", "Michael S. Ryoo"], "title": "Understanding Long Videos with Multimodal Language Models", "categories": ["cs.CV"], "comment": "17 pages (main paper), 7 pages appendix. ICLR 2025 conference paper", "summary": "Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we explore injecting video-specific information\ninto an LLM-based framework. We utilize off-the-shelf vision tools to extract\nthree object-centric information modalities from videos, and then leverage\nnatural language as a medium for fusing this information. Our resulting\nMultimodal Video Understanding (MVU) framework demonstrates state-of-the-art\nperformance across multiple video understanding benchmarks. Strong performance\nalso on robotics domain tasks establish its strong generality. Code:\nhttps://github.com/kahnchana/mvu", "AI": {"tldr": "LLM-based approaches perform well on long-video tasks even with limited video info. Injecting video-specific info via object-centric modalities improves performance, leading to state-of-the-art results.", "motivation": "To understand how LLMs' world knowledge and reasoning skills impact long-video understanding and improve performance by integrating video-specific info.", "method": "Extract object-centric info from videos using vision tools, fuse it via natural language, and integrate into an LLM-based framework (MVU).", "result": "MVU achieves state-of-the-art performance on video understanding benchmarks and shows strong generality in robotics tasks.", "conclusion": "LLMs' reasoning enhances video understanding; integrating video-specific info further boosts performance, demonstrating MVU's effectiveness and versatility."}}
{"id": "2502.05202", "pdf": "https://arxiv.org/pdf/2502.05202", "abs": "https://arxiv.org/abs/2502.05202", "authors": ["Nadav Timor", "Jonathan Mamou", "Daniel Korat", "Moshe Berchansky", "Gaurav Jain", "Oren Pereg", "Moshe Wasserblat", "David Harel"], "title": "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML'25 Oral (top %1)", "summary": "Accelerating the inference of large language models (LLMs) is a critical\nchallenge in generative AI. Speculative decoding (SD) methods offer substantial\nefficiency gains by generating multiple tokens using a single target forward\npass. However, existing SD approaches require the drafter and target models to\nshare the same vocabulary, thus limiting the pool of possible drafters, often\nnecessitating the training of a drafter from scratch. We present three new SD\nmethods that remove this shared-vocabulary constraint. All three methods\npreserve the target distribution (i.e., they are lossless) and work with\noff-the-shelf models without requiring additional training or modifications.\nEmpirically, on summarization, programming, and long-context tasks, our\nalgorithms demonstrate significant speedups of up to 2.8x over standard\nautoregressive decoding. By enabling any off-the-shelf model to serve as a\ndrafter and requiring no retraining, this work substantially broadens the\napplicability of the SD framework in practice.", "AI": {"tldr": "New speculative decoding methods remove the shared-vocabulary constraint, enabling any off-the-shelf model as a drafter, achieving up to 2.8x speedup without retraining.", "motivation": "Accelerating LLM inference is crucial, but existing SD methods limit drafter selection due to vocabulary constraints.", "method": "Three new SD methods are introduced, allowing any model as a drafter without shared vocabulary or retraining.", "result": "Empirical tests show up to 2.8x speedup on tasks like summarization and programming.", "conclusion": "This work expands SD's practicality by removing vocabulary constraints and enabling off-the-shelf models."}}
{"id": "2412.13235", "pdf": "https://arxiv.org/pdf/2412.13235", "abs": "https://arxiv.org/abs/2412.13235", "authors": ["Ricardo Euler", "Pedro Maristany de las Casas", "Ralf Bornd\u00f6rfer"], "title": "Logic-Constrained Shortest Paths for Flight Planning", "categories": ["cs.AI", "cs.DM"], "comment": null, "summary": "The logic-constrained shortest path problem (LCSPP) combines a one-to-one\nshortest path problem with satisfiability constraints imposed on the routing\ngraph. This setting arises in flight planning, where air traffic control (ATC)\nauthorities are enforcing a set of traffic flow restrictions (TFRs) on aircraft\nroutes in order to increase safety and throughput. We propose a new branch and\nbound-based algorithm for the LCSPP. The resulting algorithm has three main\ndegrees of freedom: the node selection rule, the branching rule and the\nconflict. While node selection and branching rules have been long studied in\nthe MIP and SAT communities, most of them cannot be applied out of the box for\nthe LCSPP. We review the existing literature and develop tailored variants of\nthe most prominent rules. The conflict, the set of variables to which the\nbranching rule is applied, is unique to the LCSPP. We analyze its theoretical\nimpact on the B&B algorithm. In the second part of the paper, we show how to\nmodel the flight planning problem with TFRs as an LCSPP and solve it using the\nbranch and bound algorithm. We demonstrate the algorithm's efficiency on a\ndataset consisting of a global flight graph and a set of around 20000 real TFRs\nobtained from our industry partner Lufthansa Systems GmbH. We make this dataset\npublicly available. Finally, we conduct an empirical in-depth analysis of\ndynamic shortest path algorithms, node selection rules, branching rules and\nconflicts. Carefully choosing an appropriate combination yields an improvement\nof an order of magnitude compared to an uninformed choice.", "AI": {"tldr": "A branch and bound algorithm for the logic-constrained shortest path problem (LCSPP) is proposed, tailored for flight planning with traffic flow restrictions (TFRs). The algorithm's efficiency is demonstrated on real-world data, showing significant improvements with informed choices of node selection, branching rules, and conflicts.", "motivation": "The LCSPP arises in flight planning where ATC imposes TFRs for safety and throughput. Existing MIP and SAT methods don't directly apply, necessitating a tailored approach.", "method": "A branch and bound algorithm is developed, focusing on node selection, branching rules, and conflicts. The flight planning problem is modeled as an LCSPP and solved using this algorithm.", "result": "The algorithm is tested on real-world data (20000 TFRs) and shows an order of magnitude improvement with optimized rule combinations.", "conclusion": "Tailored branch and bound rules significantly enhance LCSPP solutions for flight planning, with practical efficiency demonstrated on industry data."}}
{"id": "2506.09640", "pdf": "https://arxiv.org/pdf/2506.09640", "abs": "https://arxiv.org/abs/2506.09640", "authors": ["Pablo G. Arce", "Roi Naveiro", "David R\u00edos Insua"], "title": "Evasion Attacks Against Bayesian Predictive Models", "categories": ["stat.ML", "cs.LG", "68T37"], "comment": "Accepted as an oral presentation at UAI'25", "summary": "There is an increasing interest in analyzing the behavior of machine learning\nsystems against adversarial attacks. However, most of the research in\nadversarial machine learning has focused on studying weaknesses against evasion\nor poisoning attacks to predictive models in classical setups, with the\nsusceptibility of Bayesian predictive models to attacks remaining\nunderexplored. This paper introduces a general methodology for designing\noptimal evasion attacks against such models. We investigate two adversarial\nobjectives: perturbing specific point predictions and altering the entire\nposterior predictive distribution. For both scenarios, we propose novel\ngradient-based attacks and study their implementation and properties in various\ncomputational setups.", "AI": {"tldr": "The paper introduces a method for designing optimal evasion attacks on Bayesian predictive models, focusing on perturbing predictions or altering posterior distributions.", "motivation": "Address the underexplored susceptibility of Bayesian predictive models to adversarial attacks, unlike classical setups.", "method": "Proposes gradient-based attacks for perturbing point predictions and altering posterior predictive distributions.", "result": "Novel attacks are developed and studied in various computational setups.", "conclusion": "The methodology provides insights into adversarial vulnerabilities of Bayesian models."}}
{"id": "2404.12803", "pdf": "https://arxiv.org/pdf/2404.12803", "abs": "https://arxiv.org/abs/2404.12803", "authors": ["Jingqun Tang", "Chunhui Lin", "Zhen Zhao", "Shu Wei", "Binghong Wu", "Qi Liu", "Yangfan He", "Kuan Lu", "Hao Feng", "Yang Li", "Siqi Wang", "Lei Liao", "Wei Shi", "Yuliang Liu", "Hao Liu", "Yuan Xie", "Xiang Bai", "Can Huang"], "title": "TextSquare: Scaling up Text-Centric Visual Instruction Tuning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Text-centric visual question answering (VQA) has made great strides with the\ndevelopment of Multimodal Large Language Models (MLLMs), yet open-source models\nstill fall short of leading models like GPT4V and Gemini, partly due to a lack\nof extensive, high-quality instruction tuning data. To this end, we introduce a\nnew approach for creating a massive, high-quality instruction-tuning dataset,\nSquare-10M, which is generated using closed-source MLLMs. The data construction\nprocess, termed Square, consists of four steps: Self-Questioning, Answering,\nReasoning, and Evaluation. Our experiments with Square-10M led to three key\nfindings: 1) Our model, TextSquare, considerably surpasses open-source previous\nstate-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%).\nIt even outperforms top-tier models like GPT4V and Gemini in 6 of 10\ntext-centric benchmarks. 2) Additionally, we demonstrate the critical role of\nVQA reasoning data in offering comprehensive contextual insights for specific\nquestions. This not only improves accuracy but also significantly mitigates\nhallucinations. Specifically, TextSquare scores an average of 75.1% across four\ngeneral VQA and hallucination evaluation datasets, outperforming previous\nstate-of-the-art models. 3) Notably, the phenomenon observed in scaling\ntext-centric VQA datasets reveals a vivid pattern: the exponential increase of\ninstruction tuning data volume is directly proportional to the improvement in\nmodel performance, thereby validating the necessity of the dataset scale and\nthe high quality of Square-10M.", "AI": {"tldr": "The paper introduces Square-10M, a high-quality instruction-tuning dataset for text-centric VQA, generated using closed-source MLLMs. The model TextSquare outperforms open-source and top-tier models, highlighting the importance of dataset scale and reasoning data.", "motivation": "Open-source models lag behind leading models like GPT4V and Gemini due to insufficient high-quality instruction-tuning data. The goal is to bridge this gap by creating a large, high-quality dataset.", "method": "The Square method involves four steps: Self-Questioning, Answering, Reasoning, and Evaluation, to generate the Square-10M dataset.", "result": "TextSquare surpasses open-source and top-tier models, achieving 62.2% on OCRBench and outperforming GPT4V/Gemini in 6 of 10 benchmarks. Reasoning data improves accuracy and reduces hallucinations.", "conclusion": "Scaling text-centric VQA datasets exponentially improves model performance, validating the necessity of large, high-quality datasets like Square-10M."}}
{"id": "2502.16033", "pdf": "https://arxiv.org/pdf/2502.16033", "abs": "https://arxiv.org/abs/2502.16033", "authors": ["Qianqi Yan", "Yue Fan", "Hongquan Li", "Shan Jiang", "Yang Zhao", "Xinze Guan", "Ching-Chen Kuo", "Xin Eric Wang"], "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained\nand tested on consistent visual-textual inputs, leaving open the question of\nwhether they can handle inconsistencies in real-world, layout-rich content. To\nbridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)\nbenchmark to assess MLLMs' ability to detect and reason about semantic\nmismatches in artifacts such as webpages, presentation slides, and posters.\nMMIR comprises 534 challenging samples, each containing synthetically injected\nerrors across five reasoning-heavy categories: Factual Contradiction, Identity\nMisattribution, Contextual Mismatch, Quantitative Discrepancy, and\nTemporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing\nthat models with dedicated multimodal reasoning capabilities, such as o1,\nsubstantially outperform their counterparts while open-source models remain\nparticularly vulnerable to inconsistency errors. Detailed error analyses\nfurther show that models excel in detecting pairwise inconsistencies but\nstruggle with inconsistencies confined to single elements in complex layouts.\nProbing experiments reveal that single-modality prompting, including\nChain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains,\nrevealing a key bottleneck in cross-modal reasoning. Our findings highlight the\nneed for advanced multimodal reasoning and point to future research on\nmultimodal inconsistency.", "AI": {"tldr": "The paper introduces the MMIR benchmark to evaluate MLLMs' ability to detect and reason about inconsistencies in multimodal content, revealing gaps in current models' performance.", "motivation": "To address the lack of evaluation for MLLMs in handling real-world inconsistencies in layout-rich content.", "method": "Proposes the MMIR benchmark with 534 samples containing synthetic errors across five categories, evaluating six MLLMs.", "result": "Models with dedicated multimodal reasoning outperform others, but open-source models struggle. Single-modality prompting shows limited gains.", "conclusion": "Highlights the need for advanced multimodal reasoning and future research on inconsistency handling."}}
{"id": "2501.06137", "pdf": "https://arxiv.org/pdf/2501.06137", "abs": "https://arxiv.org/abs/2501.06137", "authors": ["Manuel Cebrian", "Emilia Gomez", "David Fernandez Llorca"], "title": "Supervision policies can shape long-term risk management in general-purpose AI models", "categories": ["cs.AI", "cs.CY", "cs.SI"], "comment": "24 pages, 14 figures", "summary": "The rapid proliferation and deployment of General-Purpose AI (GPAI) models,\nincluding large language models (LLMs), present unprecedented challenges for AI\nsupervisory entities. We hypothesize that these entities will need to navigate\nan emergent ecosystem of risk and incident reporting, likely to exceed their\nsupervision capacity. To investigate this, we develop a simulation framework\nparameterized by features extracted from the diverse landscape of risk,\nincident, or hazard reporting ecosystems, including community-driven platforms,\ncrowdsourcing initiatives, and expert assessments. We evaluate four supervision\npolicies: non-prioritized (first-come, first-served), random selection,\npriority-based (addressing the highest-priority risks first), and\ndiversity-prioritized (balancing high-priority risks with comprehensive\ncoverage across risk types). Our results indicate that while priority-based and\ndiversity-prioritized policies are more effective at mitigating high-impact\nrisks, particularly those identified by experts, they may inadvertently neglect\nsystemic issues reported by the broader community. This oversight can create\nfeedback loops that amplify certain types of reporting while discouraging\nothers, leading to a skewed perception of the overall risk landscape. We\nvalidate our simulation results with several real-world datasets, including one\nwith over a million ChatGPT interactions, of which more than 150,000\nconversations were identified as risky. This validation underscores the complex\ntrade-offs inherent in AI risk supervision and highlights how the choice of\nrisk management policies can shape the future landscape of AI risks across\ndiverse GPAI models used in society.", "AI": {"tldr": "The paper explores AI supervision challenges with GPAI models, simulating four policies to manage risk reporting. Priority and diversity policies mitigate high-impact risks but may overlook systemic issues, validated with real-world data.", "motivation": "The rapid growth of GPAI models like LLMs poses challenges for AI supervision, requiring effective policies to manage diverse risk reporting ecosystems.", "method": "A simulation framework evaluates four supervision policies (non-prioritized, random, priority-based, diversity-prioritized) using features from risk reporting ecosystems.", "result": "Priority and diversity policies are effective for high-impact risks but may neglect systemic issues, skewing risk perception. Validation with real-world data confirms trade-offs.", "conclusion": "AI risk supervision policies shape the risk landscape, highlighting trade-offs between mitigating high-impact risks and comprehensive coverage."}}
{"id": "2506.09647", "pdf": "https://arxiv.org/pdf/2506.09647", "abs": "https://arxiv.org/abs/2506.09647", "authors": ["Lei Deng", "Wenhan Xu", "Jingwei Li", "Danny H. K. Tsang"], "title": "Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "Real-time network traffic forecasting is crucial for network management and\nearly resource allocation. Existing network traffic forecasting approaches\noperate under the assumption that the network traffic data is fully observed.\nHowever, in practical scenarios, the collected data are often incomplete due to\nvarious human and natural factors. In this paper, we propose a generative model\napproach for real-time network traffic forecasting with missing data. Firstly,\nwe model the network traffic forecasting task as a tensor completion problem.\nSecondly, we incorporate a pre-trained generative model to achieve the low-rank\nstructure commonly associated with tensor completion. The generative model\neffectively captures the intrinsic low-rank structure of network traffic data\nduring pre-training and enables the mapping from a compact latent\nrepresentation to the tensor space. Thirdly, rather than directly optimizing\nthe high-dimensional tensor, we optimize its latent representation, which\nsimplifies the optimization process and enables real-time forecasting. We also\nestablish a theoretical recovery guarantee that quantifies the error bound of\nthe proposed approach. Experiments on real-world datasets demonstrate that our\napproach achieves accurate network traffic forecasting within 100 ms, with a\nmean absolute error (MAE) below 0.002, as validated on the Abilene dataset.", "AI": {"tldr": "A generative model approach for real-time network traffic forecasting with missing data, achieving accurate results within 100 ms.", "motivation": "Existing methods assume fully observed data, but real-world data is often incomplete. The paper addresses this gap.", "method": "Models the task as a tensor completion problem, uses a pre-trained generative model for low-rank structure, and optimizes latent representations for efficiency.", "result": "Achieves MAE below 0.002 on the Abilene dataset with real-time forecasting (within 100 ms).", "conclusion": "The proposed method effectively handles missing data and enables accurate, real-time network traffic forecasting."}}
{"id": "2405.11985", "pdf": "https://arxiv.org/pdf/2405.11985", "abs": "https://arxiv.org/abs/2405.11985", "authors": ["Jingqun Tang", "Qi Liu", "Yongjie Ye", "Jinghui Lu", "Shu Wei", "Chunhui Lin", "Wanqing Li", "Mohamad Fitri Faiz Bin Mahmood", "Hao Feng", "Zhen Zhao", "Yangfan He", "Kuan Lu", "Yanjie Wang", "Yuliang Liu", "Hao Liu", "Xiang Bai", "Can Huang"], "title": "MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering", "categories": ["cs.CV"], "comment": "Accepted by ACL 2025 findings", "summary": "Text-Centric Visual Question Answering (TEC-VQA) in its proper format not\nonly facilitates human-machine interaction in text-centric visual environments\nbut also serves as a de facto gold proxy to evaluate AI models in the domain of\ntext-centric scene understanding. Nonetheless, most existing TEC-VQA benchmarks\nhave focused on high-resource languages like English and Chinese. Despite\npioneering works to expand multilingual QA pairs in non-text-centric VQA\ndatasets through translation engines, the translation-based protocol encounters\na substantial \"visual-textual misalignment\" problem when applied to TEC-VQA.\nSpecifically, it prioritizes the text in question-answer pairs while\ndisregarding the visual text present in images. Moreover, it fails to address\ncomplexities related to nuanced meaning, contextual distortion, language bias,\nand question-type diversity. In this work, we tackle multilingual TEC-VQA by\nintroducing MTVQA, the first benchmark featuring high-quality human expert\nannotations across 9 diverse languages, consisting of 6,778 question-answer\npairs across 2,116 images. Further, by comprehensively evaluating numerous\nstate-of-the-art Multimodal Large Language Models~(MLLMs), including Qwen2-VL,\nGPT-4o, GPT-4V, Claude3, and Gemini, on the MTVQA benchmark, it is evident that\nthere is still a large room for performance improvement (Qwen2-VL scoring 30.9\nversus 79.7 for human performance), underscoring the value of MTVQA.\nAdditionally, we supply multilingual training data within the MTVQA dataset,\ndemonstrating that straightforward fine-tuning with this data can substantially\nenhance multilingual TEC-VQA performance. We aspire that MTVQA will offer the\nresearch community fresh insights and stimulate further exploration in\nmultilingual visual text comprehension. The project homepage is available at\nhttps://bytedance.github.io/MTVQA/.", "AI": {"tldr": "MTVQA introduces a multilingual TEC-VQA benchmark with human-annotated QA pairs in 9 languages, highlighting performance gaps in MLLMs and offering training data to improve multilingual text-centric VQA.", "motivation": "Existing TEC-VQA benchmarks focus on high-resource languages, and translation-based methods suffer from visual-textual misalignment and other issues. MTVQA addresses these gaps.", "method": "MTVQA features 6,778 QA pairs across 2,116 images in 9 languages, with human expert annotations. It evaluates MLLMs like Qwen2-VL, GPT-4o, and others.", "result": "MLLMs perform significantly worse than humans (e.g., Qwen2-VL scores 30.9 vs. human 79.7). Fine-tuning with MTVQA data improves performance.", "conclusion": "MTVQA provides a valuable benchmark for multilingual TEC-VQA, encouraging further research in visual text comprehension."}}
{"id": "2502.19830", "pdf": "https://arxiv.org/pdf/2502.19830", "abs": "https://arxiv.org/abs/2502.19830", "authors": ["Yiwei Li", "Ji Zhang", "Shaoxiong Feng", "Peiwen Yuan", "Xinglin Wang", "Jiayi Shi", "Yueqi Zhang", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "title": "Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Self-consistency improves reasoning by aggregating diverse stochastic\nsamples, yet the dynamics behind its efficacy remain underexplored. We reframe\nself-consistency as a dynamic distributional alignment problem, revealing that\ndecoding temperature not only governs sampling randomness but also actively\nshapes the latent answer distribution. Given that high temperatures require\nprohibitively large sample sizes to stabilize, while low temperatures risk\namplifying biases, we propose a confidence-driven mechanism that dynamically\ncalibrates temperature: sharpening the sampling distribution under uncertainty\nto align with high-probability modes, and promoting exploration when confidence\nis high. Experiments on mathematical reasoning tasks show this approach\noutperforms fixed-diversity baselines under limited samples, improving both\naverage and best-case performance across varying initial temperatures without\nadditional data or modules. This establishes self-consistency as a\nsynchronization challenge between sampling dynamics and evolving answer\ndistributions.", "AI": {"tldr": "Self-consistency is reframed as a dynamic distributional alignment problem, with a proposed confidence-driven temperature calibration mechanism improving performance in mathematical reasoning tasks.", "motivation": "To explore the underexplored dynamics behind self-consistency's efficacy and address the trade-off between high and low decoding temperatures.", "method": "A confidence-driven mechanism dynamically calibrates temperature to sharpen sampling under uncertainty and promote exploration when confidence is high.", "result": "Outperforms fixed-diversity baselines in mathematical reasoning tasks, improving average and best-case performance under limited samples.", "conclusion": "Self-consistency is a synchronization challenge between sampling dynamics and evolving answer distributions, with dynamic temperature calibration offering a solution."}}
{"id": "2501.11223", "pdf": "https://arxiv.org/pdf/2501.11223", "abs": "https://arxiv.org/abs/2501.11223", "authors": ["Maciej Besta", "Julia Barth", "Eric Schreiber", "Ales Kubicek", "Afonso Catarino", "Robert Gerstenberger", "Piotr Nyczyk", "Patrick Iff", "Yueling Li", "Sam Houliston", "Tomasz Sternal", "Marcin Copik", "Grzegorz Kwa\u015bniewski", "J\u00fcrgen M\u00fcller", "\u0141ukasz Flis", "Hannes Eberhard", "Zixuan Chen", "Hubert Niewiadomski", "Torsten Hoefler"], "title": "Reasoning Language Models: A Blueprint", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-R1, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining reinforcement learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation.", "AI": {"tldr": "The paper proposes a modular framework for Reasoning Language Models (RLMs) to address accessibility and scalability challenges, introducing a versatile blueprint and a prototype implementation (x1).", "motivation": "High costs, proprietary nature, and complexity of RLMs limit accessibility and scalability, prompting the need for a democratized and modular approach.", "method": "The paper surveys RLM works, organizes components into a modular framework, and provides mathematical formulations and algorithmic specifications. It introduces x1 for prototyping.", "result": "The blueprint unifies diverse reasoning structures and strategies, demonstrated through examples like LLaMA-Berry and QwQ. Insights include multi-phase training and scalable deployments.", "conclusion": "The work simplifies RLM construction, democratizes advanced reasoning, and aims to bridge the gap between resource-rich and resource-limited AI development."}}
{"id": "2506.09648", "pdf": "https://arxiv.org/pdf/2506.09648", "abs": "https://arxiv.org/abs/2506.09648", "authors": ["Mattia Rosso", "Simone Rossi", "Giulio Franzese", "Markus Heinonen", "Maurizio Filippone"], "title": "Scaling Laws for Uncertainty in Deep Learning", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Deep learning has recently revealed the existence of scaling laws,\ndemonstrating that model performance follows predictable trends based on\ndataset and model sizes. Inspired by these findings and fascinating phenomena\nemerging in the over-parameterized regime, we examine a parallel direction: do\nsimilar scaling laws govern predictive uncertainties in deep learning? In\nidentifiable parametric models, such scaling laws can be derived in a\nstraightforward manner by treating model parameters in a Bayesian way. In this\ncase, for example, we obtain $O(1/N)$ contraction rates for epistemic\nuncertainty with respect to the number of data $N$. However, in\nover-parameterized models, these guarantees do not hold, leading to largely\nunexplored behaviors. In this work, we empirically show the existence of\nscaling laws associated with various measures of predictive uncertainty with\nrespect to dataset and model sizes. Through experiments on vision and language\ntasks, we observe such scaling laws for in- and out-of-distribution predictive\nuncertainty estimated through popular approximate Bayesian inference and\nensemble methods. Besides the elegance of scaling laws and the practical\nutility of extrapolating uncertainties to larger data or models, this work\nprovides strong evidence to dispel recurring skepticism against Bayesian\napproaches: \"In many applications of deep learning we have so much data\navailable: what do we need Bayes for?\". Our findings show that \"so much data\"\nis typically not enough to make epistemic uncertainty negligible.", "AI": {"tldr": "The paper explores scaling laws for predictive uncertainties in deep learning, showing their existence empirically and challenging skepticism about Bayesian methods in data-rich scenarios.", "motivation": "To investigate if scaling laws, known for model performance, also apply to predictive uncertainties in deep learning, especially in over-parameterized models where traditional guarantees fail.", "method": "Empirical analysis of scaling laws for predictive uncertainties using vision and language tasks, employing Bayesian inference and ensemble methods.", "result": "Demonstrates scaling laws for predictive uncertainties with respect to dataset and model sizes, showing epistemic uncertainty remains significant even with large datasets.", "conclusion": "The work validates the relevance of Bayesian approaches in deep learning, proving that large datasets alone do not eliminate epistemic uncertainty."}}
{"id": "2406.01078", "pdf": "https://arxiv.org/pdf/2406.01078", "abs": "https://arxiv.org/abs/2406.01078", "authors": ["Han Sun", "Yunkang Cao", "Hao Dong", "Olga Fink"], "title": "Unseen Visual Anomaly Generation", "categories": ["cs.CV"], "comment": "8 pages excluding supplementary", "summary": "Visual anomaly detection (AD) presents significant challenges due to the\nscarcity of anomalous data samples. While numerous works have been proposed to\nsynthesize anomalous samples, these synthetic anomalies often lack authenticity\nor require extensive training data, limiting their applicability in real-world\nscenarios. In this work, we propose Anomaly Anything (AnomalyAny), a novel\nframework that leverages Stable Diffusion (SD)'s image generation capabilities\nto generate diverse and realistic unseen anomalies. By conditioning on a single\nnormal sample during test time, AnomalyAny is able to generate unseen anomalies\nfor arbitrary object types with text descriptions. Within AnomalyAny, we\npropose attention-guided anomaly optimization to direct SD attention on\ngenerating hard anomaly concepts. Additionally, we introduce prompt-guided\nanomaly refinement, incorporating detailed descriptions to further improve the\ngeneration quality. Extensive experiments on MVTec AD and VisA datasets\ndemonstrate AnomalyAny's ability in generating high-quality unseen anomalies\nand its effectiveness in enhancing downstream AD performance.", "AI": {"tldr": "AnomalyAny uses Stable Diffusion to generate diverse, realistic unseen anomalies from a single normal sample, improving anomaly detection performance.", "motivation": "Addresses the scarcity of anomalous data in visual anomaly detection by creating authentic synthetic anomalies.", "method": "Leverages Stable Diffusion with attention-guided anomaly optimization and prompt-guided refinement for high-quality anomaly generation.", "result": "Demonstrates effectiveness in generating realistic anomalies and enhancing anomaly detection on MVTec AD and VisA datasets.", "conclusion": "AnomalyAny offers a scalable solution for realistic anomaly generation, boosting downstream AD tasks."}}
{"id": "2503.01940", "pdf": "https://arxiv.org/pdf/2503.01940", "abs": "https://arxiv.org/abs/2503.01940", "authors": ["Xuan Zhang", "Yongliang Shen", "Zhe Zheng", "Linjuan Wu", "Wenqi Zhang", "Yuchen Yan", "Qiuying Peng", "Jun Wang", "Weiming Lu"], "title": "AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntool learning. In real-world scenarios, user queries are often ambiguous and\nincomplete, requiring effective clarification. However, existing interactive\nclarification approaches face two critical limitations: reliance on manually\nconstructed datasets, which inherently constrains training data scale and\ndiversity, and lack of error correction mechanisms during multi-turn\nclarification, leading to error accumulation that compromises both accuracy and\nefficiency. We present AskToAct, which addresses these challenges by exploiting\nthe structural mapping between queries and their tool invocation solutions. Our\nkey insight is that tool parameters naturally represent explicit user intents.\nBy systematically removing key parameters from queries while retaining them as\nground truth, we enable automated construction of high-quality training data.\nWe further enhance model robustness through error-correction pairs and\nselective masking, enabling dynamic error detection during clarification\ninteractions. Comprehensive experiments demonstrate that AskToAct significantly\noutperforms existing approaches, achieving above 57% accuracy in recovering\ncritical unspecified intents and enhancing clarification efficiency by an\naverage of 10.46% while maintaining high accuracy in tool invocation. Our\nframework exhibits robust performance across different model architectures and\nsuccessfully generalizes to entirely unseen APIs without additional training,\nachieving performance comparable to GPT-4o with substantially fewer\ncomputational resources.", "AI": {"tldr": "AskToAct improves tool learning by automating training data construction and adding error-correction mechanisms, outperforming existing methods in accuracy and efficiency.", "motivation": "User queries are often ambiguous, but current clarification methods rely on limited datasets and lack error correction, leading to inefficiencies.", "method": "AskToAct maps queries to tool solutions, removes key parameters for automated data generation, and uses error-correction pairs and selective masking for robustness.", "result": "It achieves 57% accuracy in intent recovery, 10.46% efficiency gain, and generalizes well to unseen APIs with fewer resources.", "conclusion": "AskToAct is a scalable, efficient, and robust framework for interactive clarification in tool learning."}}
{"id": "2502.07202", "pdf": "https://arxiv.org/pdf/2502.07202", "abs": "https://arxiv.org/abs/2502.07202", "authors": ["Jaesik Yoon", "Hyeonseo Cho", "Doojin Baek", "Yoshua Bengio", "Sungjin Ahn"], "title": "Monte Carlo Tree Diffusion for System 2 Planning", "categories": ["cs.AI", "cs.LG"], "comment": "23 pages, 7 figures, ICML 2025 Main Track Spotlight", "summary": "Diffusion models have recently emerged as a powerful tool for planning.\nHowever, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally\nimproves with inference-time computation scaling-standard diffusion-based\nplanners offer only limited avenues for the scalability. In this paper, we\nintroduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates\nthe generative strength of diffusion models with the adaptive search\ncapabilities of MCTS. Our method reconceptualizes denoising as a\ntree-structured process, allowing partially denoised plans to be iteratively\nevaluated, pruned, and refined. By selectively expanding promising trajectories\nwhile retaining the flexibility to revisit and improve suboptimal branches,\nMCTD achieves the benefits of MCTS such as controlling exploration-exploitation\ntrade-offs within the diffusion framework. Empirical results on challenging\nlong-horizon tasks show that MCTD outperforms diffusion baselines, yielding\nhigher-quality solutions as inference-time computation increases.", "AI": {"tldr": "MCTD integrates diffusion models with MCTS for scalable planning, outperforming diffusion baselines by leveraging tree-structured denoising and adaptive search.", "motivation": "Address the scalability limitations of diffusion-based planners by combining the generative strength of diffusion models with the adaptive search capabilities of MCTS.", "method": "Introduces MCTD, a framework that treats denoising as a tree-structured process, enabling iterative evaluation, pruning, and refinement of plans.", "result": "Empirical results show MCTD outperforms diffusion baselines, especially in long-horizon tasks, with improved solution quality as inference-time computation scales.", "conclusion": "MCTD successfully bridges the gap between diffusion models and MCTS, offering scalable and high-quality planning solutions."}}
{"id": "2506.09681", "pdf": "https://arxiv.org/pdf/2506.09681", "abs": "https://arxiv.org/abs/2506.09681", "authors": ["Vahan Arsenyan", "Elen Vardanyan", "Arnak Dalalyan"], "title": "Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Generative modeling aims to produce new random examples from an unknown\ntarget distribution, given access to a finite collection of examples. Among the\nleading approaches, denoising diffusion probabilistic models (DDPMs) construct\nsuch examples by mapping a Brownian motion via a diffusion process driven by an\nestimated score function. In this work, we first provide empirical evidence\nthat DDPMs are robust to constant-variance noise in the score evaluations. We\nthen establish finite-sample guarantees in Wasserstein-2 distance that exhibit\ntwo key features: (i) they characterize and quantify the robustness of DDPMs to\nnoisy score estimates, and (ii) they achieve faster convergence rates than\npreviously known results. Furthermore, we observe that the obtained rates match\nthose known in the Gaussian case, implying their optimality.", "AI": {"tldr": "DDPMs show robustness to noise in score evaluations and achieve faster convergence rates, matching optimal Gaussian case rates.", "motivation": "To investigate the robustness and convergence properties of DDPMs under noisy score evaluations.", "method": "Empirical analysis and theoretical guarantees in Wasserstein-2 distance for DDPMs.", "result": "DDPMs are robust to noise and achieve faster convergence rates, matching optimal Gaussian rates.", "conclusion": "The findings highlight DDPMs' robustness and optimal convergence, supporting their use in generative modeling."}}
{"id": "2406.10322", "pdf": "https://arxiv.org/pdf/2406.10322", "abs": "https://arxiv.org/abs/2406.10322", "authors": ["Sophie Ostmeier", "Brian Axelrod", "Maya Varma", "Michael E. Moseley", "Akshay Chaudhari", "Curtis Langlotz"], "title": "LieRE: Lie Rotational Positional Encodings", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Transformer architectures depend on explicit position encodings to capture\ntoken positional information. Rotary Position Encoding (RoPE) has emerged as a\npopular choice in language models due to its efficient encoding of relative\nposition information through key-query rotations. However, RoPE faces\nsignificant limitations beyond language processing: it is constrained to\none-dimensional sequence data and, even with learnable phases, offers limited\nrepresentational capacity. We address these challenges with Lie Relative\nEncodings (LieRE), which generalizes RoPE to high-dimensional rotation matrices\nby leveraging their Lie group structure. Through extensive evaluation on three\nimage datasets across 2D and 3D classification tasks, LieRE achieves 1.5%\nimprovement over state-of-the-art baselines on 2D tasks and 1% on 3D tasks,\nwhile demonstrating superior generalization to higher resolutions. Our\nimplementation is computationally efficient, with results reproducible on 4\nA100 GPUs in 30 minutes on CIFAR100. Our code is available at\nhttps://github.com/StanfordMIMI/LieRE.", "AI": {"tldr": "LieRE generalizes RoPE to high-dimensional rotation matrices, improving performance on 2D/3D tasks and offering better generalization.", "motivation": "RoPE's limitations in handling high-dimensional data and limited representational capacity motivate the development of LieRE.", "method": "LieRE leverages Lie group structure to generalize RoPE to high-dimensional rotation matrices.", "result": "LieRE achieves 1.5% improvement on 2D tasks and 1% on 3D tasks, with superior generalization to higher resolutions.", "conclusion": "LieRE is a computationally efficient solution for high-dimensional positional encoding, outperforming RoPE."}}
{"id": "2503.02450", "pdf": "https://arxiv.org/pdf/2503.02450", "abs": "https://arxiv.org/abs/2503.02450", "authors": ["Yilun Qiu", "Xiaoyan Zhao", "Yang Zhang", "Yimeng Bai", "Wenjie Wang", "Hong Cheng", "Fuli Feng", "Tat-Seng Chua"], "title": "Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization", "categories": ["cs.CL"], "comment": "2025 ACL Findings", "summary": "Personalizing Large Language Models (LLMs) has become a critical step in\nfacilitating their widespread application to enhance individual life\nexperiences. In pursuit of personalization, distilling key preference\ninformation from an individual's historical data as instructional preference\ncontext to customize LLM generation has emerged as a promising direction.\nHowever, these methods face a fundamental limitation by overlooking the\ninter-user comparative analysis, which is essential for identifying the\ninter-user differences that truly shape preferences. To address this\nlimitation, we propose Difference-aware Personalization Learning (DPL), a novel\napproach that emphasizes extracting inter-user differences to enhance LLM\npersonalization. DPL strategically selects representative users for comparison\nand establishes a structured standard to extract meaningful, task-relevant\ndifferences for customizing LLM generation. Extensive experiments on real-world\ndatasets demonstrate that DPL significantly enhances LLM personalization. We\nrelease our code at https://github.com/SnowCharmQ/DPL.", "AI": {"tldr": "DPL introduces inter-user comparative analysis to enhance LLM personalization by extracting meaningful differences, outperforming existing methods.", "motivation": "Existing LLM personalization methods overlook inter-user differences, which are crucial for accurate preference modeling.", "method": "DPL selects representative users for comparison and extracts task-relevant differences to customize LLM generation.", "result": "Experiments show DPL significantly improves LLM personalization on real-world datasets.", "conclusion": "DPL addresses a key limitation in LLM personalization by leveraging inter-user differences, offering a promising direction for future work."}}
{"id": "2502.13131", "pdf": "https://arxiv.org/pdf/2502.13131", "abs": "https://arxiv.org/abs/2502.13131", "authors": ["Feng Luo", "Rui Yang", "Hao Sun", "Chunyuan Deng", "Jiarui Yao", "Jingyan Shen", "Huan Zhang", "Hanjie Chen"], "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis", "categories": ["cs.AI", "cs.CL"], "comment": "14 pages", "summary": "Understanding human preferences is crucial for improving foundation models\nand building personalized AI systems. However, preferences are inherently\ndiverse and complex, making it difficult for traditional reward models to\ncapture their full range. While fine-grained preference data can help,\ncollecting it is expensive and hard to scale. In this paper, we introduce\nDecomposed Reward Models (DRMs), a novel approach that extracts diverse human\npreferences from binary comparisons without requiring fine-grained annotations.\nOur key insight is to represent human preferences as vectors and analyze them\nusing Principal Component Analysis (PCA). By constructing a dataset of\nembedding differences between preferred and rejected responses, DRMs identify\northogonal basis vectors that capture distinct aspects of preference. These\ndecomposed rewards can be flexibly combined to align with different user needs,\noffering an interpretable and scalable alternative to traditional reward\nmodels. We demonstrate that DRMs effectively extract meaningful preference\ndimensions (e.g., helpfulness, safety, humor) and adapt to new users without\nadditional training. Our results highlight DRMs as a powerful framework for\npersonalized and interpretable LLM alignment. Our code is available at\nhttps://github.com/amandaluof/DRMs.", "AI": {"tldr": "DRMs decompose human preferences from binary comparisons using PCA, offering scalable and interpretable reward models for personalized AI.", "motivation": "Human preferences are diverse and complex, but traditional reward models struggle to capture them, and fine-grained data collection is costly.", "method": "DRMs use PCA on embedding differences from binary comparisons to extract orthogonal preference vectors.", "result": "DRMs identify meaningful preference dimensions (e.g., helpfulness, safety) and adapt to new users without retraining.", "conclusion": "DRMs provide a scalable, interpretable framework for aligning LLMs with diverse human preferences."}}
{"id": "2506.09730", "pdf": "https://arxiv.org/pdf/2506.09730", "abs": "https://arxiv.org/abs/2506.09730", "authors": ["Pierre Vernimmen", "Fran\u00e7ois Glineur"], "title": "Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "This work assesses both empirically and theoretically, using the performance\nestimation methodology, how robust different first-order optimization methods\nare when subject to relative inexactness in their gradient computations.\nRelative inexactness occurs, for example, when compressing the gradient using\nfewer bits of information, which happens when dealing with large-scale problems\non GPUs. Three major families of methods are analyzed: constant step gradient\ndescent, long-step methods, and accelerated methods. The latter two are first\nshown to be theoretically not robust to inexactness. Then, a semi-heuristic\nshortening factor is introduced to improve their theoretical guarantees. All\nmethods are subsequently tested on a concrete inexact problem, with two\ndifferent types of relative inexactness, and it is observed that both\naccelerated methods are much more robust than expected, and that the shortening\nfactor significantly helps the long-step methods. In the end, all shortened\nmethods appear to be promising, even in this inexact setting.", "AI": {"tldr": "The paper evaluates the robustness of first-order optimization methods under gradient computation inexactness, introduces a heuristic to improve robustness, and tests methods on an inexact problem.", "motivation": "To understand how gradient computation inexactness affects optimization methods, especially in large-scale GPU problems.", "method": "Analyzes three method families (constant step gradient descent, long-step, accelerated) theoretically and empirically, introduces a shortening factor for robustness.", "result": "Accelerated methods are more robust than expected; the shortening factor improves long-step methods.", "conclusion": "Shortened methods show promise even with inexact gradients."}}
{"id": "2406.16439", "pdf": "https://arxiv.org/pdf/2406.16439", "abs": "https://arxiv.org/abs/2406.16439", "authors": ["Shilei Cao", "Juepeng Zheng", "Yan Liu", "Baoquan Zhao", "Ziqi Yuan", "Weijia Li", "Runmin Dong", "Haohuan Fu"], "title": "Exploring Test-Time Adaptation for Object Detection in Continually Changing Environments", "categories": ["cs.CV"], "comment": null, "summary": "Real-world application models are commonly deployed in dynamic environments,\nwhere the target domain distribution undergoes temporal changes. Continual\nTest-Time Adaptation (CTTA) has recently emerged as a promising technique to\ngradually adapt a source-trained model to continually changing target domains.\nDespite recent advancements in addressing CTTA, two critical issues remain: 1)\nFixed thresholds for pseudo-labeling in existing methodologies lead to\nlow-quality pseudo-labels, as model confidence varies across categories and\ndomains; 2) Stochastic parameter restoration methods for mitigating\ncatastrophic forgetting fail to preserve critical information effectively, due\nto their intrinsic randomness. To tackle these challenges for detection models\nin CTTA scenarios, we present AMROD, featuring three core components. Firstly,\nthe object-level contrastive learning module extracts object-level features for\ncontrastive learning to refine the feature representation in the target domain.\nSecondly, the adaptive monitoring module dynamically skips unnecessary\nadaptation and updates the category-specific threshold based on predicted\nconfidence scores to enable efficiency and improve the quality of\npseudo-labels. Lastly, the adaptive randomized restoration mechanism\nselectively reset inactive parameters with higher possibilities, ensuring the\nretention of essential knowledge. We demonstrate the effectiveness of AMROD on\nfour CTTA object detection tasks, where AMROD outperforms existing methods,\nespecially achieving a 3.2 mAP improvement and a 20\\% increase in efficiency on\nthe Cityscapes-to-Cityscapes-C CTTA task. The code of this work is available at\nhttps://github.com/ShileiCao/AMROD.", "AI": {"tldr": "AMROD improves CTTA for object detection by addressing low-quality pseudo-labels and ineffective parameter restoration, achieving better performance and efficiency.", "motivation": "To tackle issues of fixed pseudo-labeling thresholds and stochastic parameter restoration in CTTA for detection models.", "method": "Uses object-level contrastive learning, adaptive monitoring for dynamic thresholds, and adaptive randomized restoration.", "result": "Outperforms existing methods, with a 3.2 mAP improvement and 20% efficiency boost on Cityscapes-to-Cityscapes-C task.", "conclusion": "AMROD effectively enhances CTTA for object detection by refining feature representation and preserving critical knowledge."}}
{"id": "2503.04793", "pdf": "https://arxiv.org/pdf/2503.04793", "abs": "https://arxiv.org/abs/2503.04793", "authors": ["Wenjie Qiu", "Yi-Chen Li", "Xuqin Zhang", "Tianyi Zhang", "Yihang Zhang", "Zongzhang Zhang", "Yang Yu"], "title": "Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Learning reward models from human preference datasets and subsequently\noptimizing language models via reinforcement learning has emerged as a\nfundamental paradigm for aligning LLMs with human preferences. The performance\nof the reward model plays a crucial role in the effectiveness of alignment.\nPrevious reward models operate at a coarse-grained level, requiring the\ngeneration of a complete response to obtain a reward value. The sparse reward\nmay present challenges for downstream reinforcement learning. While recent\nefforts have attempted to learn token-level reward models, the lack of explicit\nsemantic information makes it difficult to model the credit of every individual\ntoken. In this paper, we propose assigning scores to every sentence,\nintroducing an intermediate-grained reward model. By segmenting the complete\nresponse into sentences and applying differential operations to reward output\nat the start and end positions of each sentence, we can effectively model the\nrewards of sentences. Moreover, a novel attention mechanism is introduced to\naggregate the scores of all sentences into a response-level score, which allows\nit to be trained using the Bradley-Terry model. On common benchmarks, our\nmethod outperforms the response-level reward model by 2.7% on RewardBench (for\nreward modeling evaluation) and surpasses all baselines on AlpacaEval (for\nalignment evaluation).", "AI": {"tldr": "The paper proposes an intermediate-grained reward model for aligning LLMs with human preferences by scoring sentences and aggregating them into response-level scores, outperforming existing methods.", "motivation": "Current reward models are coarse-grained or lack semantic clarity at the token level, hindering effective alignment of LLMs with human preferences.", "method": "The approach segments responses into sentences, assigns scores to each, and uses a novel attention mechanism to aggregate sentence-level rewards into a response-level score.", "result": "The method outperforms response-level reward models by 2.7% on RewardBench and surpasses baselines on AlpacaEval.", "conclusion": "Sentence-level reward modeling improves alignment performance, offering a more effective approach than existing coarse or token-level methods."}}
{"id": "2504.20462", "pdf": "https://arxiv.org/pdf/2504.20462", "abs": "https://arxiv.org/abs/2504.20462", "authors": ["Qi Wang", "Xiao Zhang", "Mingyi Li", "Yuan Yuan", "Mengbai Xiao", "Fuzhen Zhuang", "Dongxiao Yu"], "title": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data in Cloud-Native Systems", "categories": ["cs.AI"], "comment": null, "summary": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness.", "AI": {"tldr": "TAMO, a tool-assisted LLM agent, addresses challenges in automated root cause analysis (RCA) for microservices by integrating multi-modal data and specialized tools, improving fault localization and repair strategy generation.", "motivation": "The complexity of microservices and cloud-native systems makes traditional RCA inefficient, requiring automated solutions. LLMs offer potential but face challenges like text constraints and dynamic dependencies.", "method": "TAMO unifies multi-modal observational data into time-aligned representations, uses specialized tools for localization and classification, and structures key information for LLM prompts.", "result": "TAMO performs well in RCA on heterogeneous public datasets with common fault types, proving its effectiveness.", "conclusion": "TAMO successfully addresses LLM limitations in RCA for microservices, offering a practical solution for automated fault response."}}
{"id": "2506.09764", "pdf": "https://arxiv.org/pdf/2506.09764", "abs": "https://arxiv.org/abs/2506.09764", "authors": ["Giulia Preti", "Gianmarco De Francisci Morales", "Matteo Riondato"], "title": "Alice and the Caterpillar: A more descriptive null model for assessing data mining results", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "We introduce novel null models for assessing the results obtained from\nobserved binary transactional and sequence datasets, using statistical\nhypothesis testing. Our null models maintain more properties of the observed\ndataset than existing ones. Specifically, they preserve the Bipartite Joint\nDegree Matrix of the bipartite (multi-)graph corresponding to the dataset,\nwhich ensures that the number of caterpillars, i.e., paths of length three, is\npreserved, in addition to other properties considered by other models. We\ndescribe Alice, a suite of Markov chain Monte Carlo algorithms for sampling\ndatasets from our null models, based on a carefully defined set of states and\nefficient operations to move between them. The results of our experimental\nevaluation show that Alice mixes fast and scales well, and that our null model\nfinds different significant results than ones previously considered in the\nliterature.", "AI": {"tldr": "Novel null models for binary transactional and sequence datasets preserve more dataset properties, including the Bipartite Joint Degree Matrix, and use MCMC algorithms (Alice) for efficient sampling.", "motivation": "Existing null models for binary transactional and sequence datasets lack the ability to preserve key properties like the Bipartite Joint Degree Matrix, limiting their accuracy in statistical hypothesis testing.", "method": "Developed Alice, a suite of Markov chain Monte Carlo algorithms, to sample datasets from null models that preserve the Bipartite Joint Degree Matrix and other properties.", "result": "Alice mixes fast, scales well, and identifies different significant results compared to prior null models.", "conclusion": "The proposed null models and Alice algorithm improve accuracy in statistical hypothesis testing by preserving more dataset properties and enabling efficient sampling."}}
{"id": "2406.19048", "pdf": "https://arxiv.org/pdf/2406.19048", "abs": "https://arxiv.org/abs/2406.19048", "authors": ["Yang Song", "Lin Wang"], "title": "BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for Semantic- and Spatial-Aware 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L)", "summary": "3D object detection is an important task that has been widely applied in\nautonomous driving. To perform this task, a new trend is to fuse multi-modal\ninputs, i.e., LiDAR and camera. Under such a trend, recent methods fuse these\ntwo modalities by unifying them in the same 3D space. However, during direct\nfusion in a unified space, the drawbacks of both modalities (LiDAR features\nstruggle with detailed semantic information and the camera lacks accurate 3D\nspatial information) are also preserved, diluting semantic and spatial\nawareness of the final unified representation. To address the issue, this\nletter proposes a novel bidirectional complementary LiDAR-camera fusion\nframework, called BiCo-Fusion that can achieve robust semantic- and\nspatial-aware 3D object detection. The key insight is to fuse LiDAR and camera\nfeatures in a bidirectional complementary way to enhance the semantic awareness\nof the LiDAR and the 3D spatial awareness of the camera. The enhanced features\nfrom both modalities are then adaptively fused to build a semantic- and\nspatial-aware unified representation. Specifically, we introduce Pre-Fusion\nconsisting of a Voxel Enhancement Module (VEM) to enhance the semantic\nawareness of voxel features from 2D camera features and Image Enhancement\nModule (IEM) to enhance the 3D spatial awareness of camera features from 3D\nvoxel features. We then introduce Unified Fusion (U-Fusion) to adaptively fuse\nthe enhanced features from the last stage to build a unified representation.\nExtensive experiments demonstrate the superiority of our BiCo-Fusion against\nthe prior arts. Project page: https://t-ys.github.io/BiCo-Fusion/.", "AI": {"tldr": "BiCo-Fusion is a bidirectional complementary LiDAR-camera fusion framework for 3D object detection, enhancing semantic and spatial awareness by adaptively fusing enhanced features from both modalities.", "motivation": "Current fusion methods in 3D object detection preserve the drawbacks of LiDAR (lack of semantic detail) and cameras (lack of accurate 3D spatial information), diluting the final representation.", "method": "Proposes BiCo-Fusion with Pre-Fusion (VEM and IEM) to enhance LiDAR and camera features bidirectionally, followed by Unified Fusion (U-Fusion) for adaptive feature fusion.", "result": "Extensive experiments show BiCo-Fusion outperforms prior methods.", "conclusion": "BiCo-Fusion effectively addresses the limitations of direct fusion, achieving robust semantic- and spatial-aware 3D object detection."}}
{"id": "2503.11314", "pdf": "https://arxiv.org/pdf/2503.11314", "abs": "https://arxiv.org/abs/2503.11314", "authors": ["Xinyu Tang", "Xiaolei Wang", "Zhihao Lv", "Yingqian Min", "Wayne Xin Zhao", "Binbin Hu", "Ziqi Liu", "Zhiqiang Zhang"], "title": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly\nimproved the reasoning capabilities of large language models(LLMs). Existing\nwork finds that the capability of long CoT reasoning can be efficiently\nelicited by tuning on only a few examples and can easily transfer to other\ntasks. This motivates us to investigate whether long CoT reasoning is a general\ncapability for LLMs. In this work, we conduct an empirical analysis for this\nquestion from the perspective of representation. We find that LLMs do encode\nlong CoT reasoning as a general capability, with a clear distinction from\nvanilla CoTs. Furthermore, domain-specific representations are also required\nfor the effective transfer of long CoT reasoning. Inspired by these findings,\nwe propose GLoRE, a novel representation engineering method to unleash the\ngeneral long CoT reasoning capabilities of LLMs. Extensive experiments\ndemonstrate the effectiveness and efficiency of GLoRE in both in-domain and\ncross-domain scenarios.", "AI": {"tldr": "The paper investigates whether long chain-of-thoughts (CoTs) reasoning is a general capability in LLMs, finding it distinct from vanilla CoTs and requiring domain-specific representations. It proposes GLoRE, a representation engineering method, which proves effective in experiments.", "motivation": "To determine if long CoT reasoning is a general capability of LLMs and explore its transferability across tasks.", "method": "Empirical analysis from a representation perspective, proposing GLoRE for enhancing long CoT reasoning.", "result": "LLMs encode long CoT reasoning as a general capability, with GLoRE showing effectiveness in in-domain and cross-domain scenarios.", "conclusion": "Long CoT reasoning is a general LLM capability, and GLoRE successfully enhances it, demonstrating broad applicability."}}
{"id": "2505.04317", "pdf": "https://arxiv.org/pdf/2505.04317", "abs": "https://arxiv.org/abs/2505.04317", "authors": ["Ruize Zhang", "Sirui Xiang", "Zelai Xu", "Feng Gao", "Shilong Ji", "Wenhao Tang", "Wenbo Ding", "Chao Yu", "Yu Wang"], "title": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, we tackle the problem of learning to play 3v3 multi-drone\nvolleyball, a new embodied competitive task that requires both high-level\nstrategic coordination and low-level agile control. The task is turn-based,\nmulti-agent, and physically grounded, posing significant challenges due to its\nlong-horizon dependencies, tight inter-agent coupling, and the underactuated\ndynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play\n(HCSP), a hierarchical reinforcement learning framework that separates\ncentralized high-level strategic decision-making from decentralized low-level\nmotion control. We design a three-stage population-based training pipeline to\nenable both strategy and skill to emerge from scratch without expert\ndemonstrations: (I) training diverse low-level skills, (II) learning high-level\nstrategy via self-play with fixed low-level controllers, and (III) joint\nfine-tuning through co-self-play. Experiments show that HCSP achieves superior\nperformance, outperforming non-hierarchical self-play and rule-based\nhierarchical baselines with an average 82.9% win rate and a 71.5% win rate\nagainst the two-stage variant. Moreover, co-self-play leads to emergent team\nbehaviors such as role switching and coordinated formations, demonstrating the\neffectiveness of our hierarchical design and training scheme. The project page\nis at https://sites.google.com/view/hi-co-self-play.", "AI": {"tldr": "HCSP, a hierarchical reinforcement learning framework, outperforms baselines in 3v3 multi-drone volleyball by separating strategic and motion control, achieving an 82.9% win rate.", "motivation": "Addressing the challenges of long-horizon dependencies, tight inter-agent coupling, and underactuated dynamics in multi-drone volleyball.", "method": "Proposes HCSP with a three-stage training pipeline: diverse low-level skills, high-level strategy via self-play, and joint fine-tuning through co-self-play.", "result": "HCSP achieves an 82.9% win rate, outperforming baselines, and exhibits emergent team behaviors like role switching.", "conclusion": "HCSP\u2019s hierarchical design and training scheme effectively address the complexities of multi-drone volleyball."}}
{"id": "2506.09765", "pdf": "https://arxiv.org/pdf/2506.09765", "abs": "https://arxiv.org/abs/2506.09765", "authors": ["Shuai Li", "Azarakhsh Keipour", "Sicong Zhao", "Srinath Rajagopalan", "Charles Swan", "Kostas E. Bekris"], "title": "Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction", "categories": ["cs.RO", "cs.LG"], "comment": "The 19th International Symposium on Experimental Robotics (ISER\n  2025); 6-10 July 2025, Santa Fe, New Mexico, USA; 10 pages", "summary": "Warehouse automation plays a pivotal role in enhancing operational\nefficiency, minimizing costs, and improving resilience to workforce\nvariability. While prior research has demonstrated the potential of machine\nlearning (ML) models to increase picking success rates in large-scale robotic\nfleets by prioritizing high-probability picks and packages, these efforts\nprimarily focused on predicting success probabilities for picks sampled using\nheuristic methods. Limited attention has been given, however, to leveraging\ndata-driven approaches to directly optimize sampled picks for better\nperformance at scale. In this study, we propose an ML-based framework that\npredicts transform adjustments as well as improving the selection of suction\ncups for multi-suction end effectors for sampled picks to enhance their success\nprobabilities. The framework was integrated and evaluated in test workcells\nthat resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,\nwhich is used for package manipulation. Evaluated on over 2 million picks, the\nproposed method achieves a 20\\% reduction in pick failure rates compared to a\nheuristic-based pick sampling baseline, demonstrating its effectiveness in\nlarge-scale warehouse automation scenarios.", "AI": {"tldr": "An ML-based framework optimizes pick selection and suction cup adjustments in warehouse robots, reducing pick failures by 20% compared to heuristic methods.", "motivation": "To improve warehouse automation by directly optimizing pick selection and suction cup adjustments using data-driven ML, addressing gaps in prior heuristic-focused research.", "method": "Proposes an ML framework for predicting transform adjustments and selecting suction cups for multi-suction end effectors, evaluated in Amazon Robotics' test workcells.", "result": "Achieves a 20% reduction in pick failure rates over 2 million picks compared to heuristic baselines.", "conclusion": "The framework effectively enhances large-scale warehouse automation by improving pick success rates."}}
{"id": "2407.12736", "pdf": "https://arxiv.org/pdf/2407.12736", "abs": "https://arxiv.org/abs/2407.12736", "authors": ["Mohammad Erfan Sadeghi", "Arash Fayyazi", "Suhas Somashekar", "Armin Abdollahi", "Massoud Pedram"], "title": "CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference", "categories": ["cs.CV", "cs.AI", "cs.AR"], "comment": null, "summary": "Vision Transformers (ViTs) represent a groundbreaking shift in machine\nlearning approaches to computer vision. Unlike traditional approaches, ViTs\nemploy the self-attention mechanism, which has been widely used in natural\nlanguage processing, to analyze image patches. Despite their advantages in\nmodeling visual tasks, deploying ViTs on hardware platforms, notably\nField-Programmable Gate Arrays (FPGAs), introduces considerable challenges.\nThese challenges stem primarily from the non-linear calculations and high\ncomputational and memory demands of ViTs. This paper introduces CHOSEN, a\nsoftware-hardware co-design framework to address these challenges and offer an\nautomated framework for ViT deployment on the FPGAs in order to maximize\nperformance. Our framework is built upon three fundamental contributions:\nmulti-kernel design to maximize the bandwidth, mainly targeting benefits of\nmulti DDR memory banks, approximate non-linear functions that exhibit minimal\naccuracy degradation, and efficient use of available logic blocks on the FPGA,\nand efficient compiler to maximize the performance and memory-efficiency of the\ncomputing kernels by presenting a novel algorithm for design space exploration\nto find optimal hardware configuration that achieves optimal throughput and\nlatency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a\n1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.", "AI": {"tldr": "CHOSEN is a software-hardware co-design framework for deploying Vision Transformers (ViTs) on FPGAs, addressing challenges like non-linear calculations and high computational demands. It improves throughput by 1.5x and 1.42x on DeiT-S and DeiT-B models.", "motivation": "ViTs face deployment challenges on FPGAs due to non-linear calculations and high computational/memory demands.", "method": "CHOSEN uses multi-kernel design, approximate non-linear functions, efficient logic block usage, and a novel compiler for optimal hardware configuration.", "result": "Achieves 1.5x and 1.42x throughput improvement on DeiT-S and DeiT-B models.", "conclusion": "CHOSEN effectively optimizes ViT deployment on FPGAs, outperforming state-of-the-art accelerators."}}
{"id": "2503.18491", "pdf": "https://arxiv.org/pdf/2503.18491", "abs": "https://arxiv.org/abs/2503.18491", "authors": ["Shuo Yang", "Siwen Luo", "Soyeon Caren Han", "Eduard Hovy"], "title": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering", "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA.", "AI": {"tldr": "MAGIC-VQA integrates commonsense knowledge with LVLMs for robust VQA, using explicit knowledge, post-processing, and GNN-based augmentation.", "motivation": "LVLMs lack integrated commonsense knowledge, limiting VQA robustness in real-world scenarios.", "method": "Three-stage process: explicit knowledge integration, by-type post-processing, and GNN-based implicit knowledge augmentation.", "result": "Achieves state-of-the-art performance on benchmarks, improving commonsense reasoning in VQA.", "conclusion": "MAGIC-VQA bridges the gap between commonsense knowledge and LVLM-driven reasoning without extensive pre-training."}}
{"id": "2505.04646", "pdf": "https://arxiv.org/pdf/2505.04646", "abs": "https://arxiv.org/abs/2505.04646", "authors": ["Poria Azadi"], "title": "Computational Irreducibility as the Foundation of Agency: A Formal Model Connecting Undecidability to Autonomous Behavior in Complex Systems", "categories": ["cs.AI", "cs.CC", "cs.IT", "math.IT"], "comment": null, "summary": "This article presents a formal model demonstrating that genuine autonomy, the\nability of a system to self-regulate and pursue objectives, fundamentally\nimplies computational unpredictability from an external perspective. we\nestablish precise mathematical connections, proving that for any truly\nautonomous system, questions about its future behavior are fundamentally\nundecidable. this formal undecidability, rather than mere complexity, grounds a\nprincipled distinction between autonomous and non-autonomous systems. our\nframework integrates insights from computational theory and biology,\nparticularly regarding emergent agency and computational irreducibility, to\nexplain how novel information and purpose can arise within a physical universe.\nthe findings have significant implications for artificial intelligence,\nbiological modeling, and philosophical concepts like free will.", "AI": {"tldr": "Autonomous systems exhibit computational unpredictability, making their future behavior undecidable, distinguishing them from non-autonomous systems.", "motivation": "To formally distinguish autonomous systems by their inherent unpredictability and connect this to computational theory and biology.", "method": "Developed a formal model linking autonomy to computational undecidability, integrating insights from computational theory and biology.", "result": "Proved that truly autonomous systems have undecidable future behavior, grounding a principled distinction from non-autonomous systems.", "conclusion": "The findings impact AI, biological modeling, and philosophical debates like free will, highlighting the unique nature of autonomy."}}
{"id": "2506.09773", "pdf": "https://arxiv.org/pdf/2506.09773", "abs": "https://arxiv.org/abs/2506.09773", "authors": ["Taulant Koka", "Manolis C. Tsakiris", "Benjam\u00edn B\u00e9jar Haro", "Michael Muma"], "title": "Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces", "categories": ["eess.SP", "cs.LG"], "comment": "Accepted to ICASSP 2025. \\copyright 2025 IEEE. Personal use of this\n  material is permitted", "summary": "Cross-channel unlabeled sensing addresses the problem of recovering a\nmulti-channel signal from measurements that were shuffled across channels. This\nwork expands the cross-channel unlabeled sensing framework to signals that lie\nin a union of subspaces. The extension allows for handling more complex signal\nstructures and broadens the framework to tasks like compressed sensing. These\nmismatches between samples and channels often arise in applications such as\nwhole-brain calcium imaging of freely moving organisms or multi-target\ntracking. We improve over previous models by deriving tighter bounds on the\nrequired number of samples for unique reconstruction, while supporting more\ngeneral signal types. The approach is validated through an application in\nwhole-brain calcium imaging, where organism movements disrupt sample-to-neuron\nmappings. This demonstrates the utility of our framework in real-world settings\nwith imprecise sample-channel associations, achieving accurate signal\nreconstruction.", "AI": {"tldr": "The paper extends cross-channel unlabeled sensing to signals in a union of subspaces, improving sample bounds and supporting complex signal structures like compressed sensing.", "motivation": "To address challenges in multi-channel signal recovery where samples are shuffled across channels, common in applications like whole-brain calcium imaging or multi-target tracking.", "method": "Extends the framework to union of subspaces, derives tighter bounds on required samples, and validates with whole-brain calcium imaging.", "result": "Achieves accurate signal reconstruction in real-world settings with imprecise sample-channel associations.", "conclusion": "The framework is effective for complex signal structures and practical applications with channel mismatches."}}
{"id": "2407.17152", "pdf": "https://arxiv.org/pdf/2407.17152", "abs": "https://arxiv.org/abs/2407.17152", "authors": ["Yuyan Chen", "Songzhou Yan", "Zhihong Zhu", "Zhixu Li", "Yanghua Xiao"], "title": "XMeCap: Meme Caption Generation with Sub-Image Adaptability", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ACM Multimedia 2024", "summary": "Humor, deeply rooted in societal meanings and cultural details, poses a\nunique challenge for machines. While advances have been made in natural\nlanguage processing, real-world humor often thrives in a multi-modal context,\nencapsulated distinctively by memes. This paper poses a particular emphasis on\nthe impact of multi-images on meme captioning. After that, we introduce the\n\\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning\nand reinforcement learning based on an innovative reward model, which factors\nin both global and local similarities between visuals and text. Our results,\nbenchmarked against contemporary models, manifest a marked improvement in\ncaption generation for both single-image and multi-image memes, as well as\ndifferent meme categories. \\textsc{XMeCap} achieves an average evaluation score\nof 75.85 for single-image memes and 66.32 for multi-image memes, outperforming\nthe best baseline by 6.75\\% and 8.56\\%, respectively. This research not only\nestablishes a new frontier in meme-related studies but also underscores the\npotential of machines in understanding and generating humor in a multi-modal\nsetting.", "AI": {"tldr": "The paper introduces the XMeCap framework for meme captioning, combining supervised fine-tuning and reinforcement learning to improve humor understanding in multi-modal contexts.", "motivation": "Humor, especially in memes, is challenging for machines due to its cultural and multi-modal nature. The study focuses on multi-image memes to advance captioning.", "method": "The XMeCap framework uses supervised fine-tuning and reinforcement learning with a reward model considering global and local visual-text similarities.", "result": "XMeCap outperforms baselines, scoring 75.85 for single-image and 66.32 for multi-image memes, improving by 6.75% and 8.56%, respectively.", "conclusion": "The research advances meme studies and demonstrates machine potential in multi-modal humor understanding and generation."}}
{"id": "2504.01738", "pdf": "https://arxiv.org/pdf/2504.01738", "abs": "https://arxiv.org/abs/2504.01738", "authors": ["Philip Lippmann", "Jie Yang"], "title": "Style over Substance: Distilled Language Models Reason Via Stylistic Replication", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Specialized reasoning language models (RLMs) have demonstrated that scaling\ntest-time computation through detailed reasoning traces significantly enhances\nperformance. Although these traces effectively facilitate knowledge\ndistillation into smaller, instruction-tuned models, the precise nature of\ntransferred reasoning remains unclear. In this study, we investigate to what\nextent distilled models internalize replicated stylistic patterns during\nreasoning. To this end, we systematically analyze reasoning traces, identifying\nstructural and lexical patterns that characterize successful reasoning. We then\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\n-- to precisely examine their influence on distilled models' reasoning\ncapabilities. We find that models trained on the synthetic traces achieve\ncomparable performance, indicating that distilled reasoning abilities rely\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\nin performance even when the synthetic traces are altered to lead to the wrong\nanswer. Our findings highlight how stylistic patterns can be leveraged to\nefficiently enhance LM reasoning across diverse model families.", "AI": {"tldr": "The study explores how distilled reasoning models internalize stylistic patterns from reasoning traces, finding that surface-level patterns significantly influence performance.", "motivation": "To understand the extent to which distilled models replicate stylistic patterns in reasoning traces and how these patterns impact reasoning capabilities.", "method": "Systematic analysis of reasoning traces, creation of two datasets (emergent and synthetic), and evaluation of models trained on synthetic traces.", "result": "Models trained on synthetic traces perform comparably, and performance even improves with altered (incorrect) traces, showing reliance on surface-level patterns.", "conclusion": "Stylistic patterns can efficiently enhance reasoning in language models, even when the reasoning itself is flawed."}}
{"id": "2505.13031", "pdf": "https://arxiv.org/pdf/2505.13031", "abs": "https://arxiv.org/abs/2505.13031", "authors": ["Yicheng Xiao", "Lin Song", "Yukang Chen", "Yingmin Luo", "Yuxin Chen", "Yukang Gan", "Wei Huang", "Xiu Li", "Xiaojuan Qi", "Ying Shan"], "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "categories": ["cs.AI"], "comment": "Code: https://github.com/TencentARC/MindOmni", "summary": "Recent text-to-image systems face limitations in handling multimodal inputs\nand complex reasoning tasks. We introduce MindOmni, a unified multimodal large\nlanguage model that addresses these challenges by incorporating reasoning\ngeneration through reinforcement learning. MindOmni leverages a three-phase\ntraining strategy: i) design of a unified vision language model with a\ndecoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought\n(CoT) instruction data, and iii) our proposed Reasoning Generation Policy\nOptimization (RGPO) algorithm, utilizing multimodal feedback to effectively\nguide policy updates. Experimental results demonstrate that MindOmni\noutperforms existing models, achieving impressive performance on both\nunderstanding and generation benchmarks, meanwhile showcasing advanced\nfine-grained reasoning generation capabilities, especially with mathematical\nreasoning instruction. All codes will be made public at\nhttps://github.com/TencentARC/MindOmni", "AI": {"tldr": "MindOmni is a unified multimodal LLM that enhances text-to-image systems by integrating reasoning generation via reinforcement learning, outperforming existing models.", "motivation": "Address limitations in handling multimodal inputs and complex reasoning tasks in text-to-image systems.", "method": "Three-phase training: unified vision-language model, supervised fine-tuning with CoT data, and RGPO algorithm for policy updates.", "result": "Outperforms existing models, excels in understanding/generation benchmarks, and shows advanced reasoning capabilities.", "conclusion": "MindOmni effectively tackles multimodal and reasoning challenges, with public code availability."}}
{"id": "2506.09805", "pdf": "https://arxiv.org/pdf/2506.09805", "abs": "https://arxiv.org/abs/2506.09805", "authors": ["Tonghe Wang", "Yining Feng", "Xiaofeng Yang"], "title": "Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy", "categories": ["physics.med-ph", "cs.LG"], "comment": null, "summary": "Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the\npattern of needle placement solely relies on physician experience. We\ninvestigated the feasibility of using reinforcement learning (RL) to provide\nneedle positions and dwell times based on patient anatomy during pre-planning\nstage. This approach would reduce procedure time and ensure consistent plan\nquality. Materials and Methods: We train a RL agent to adjust the position of\none selected needle and all the dwell times on it to maximize a pre-defined\nreward function after observing the environment. After adjusting, the RL agent\nthen moves on to the next needle, until all needles are adjusted. Multiple\nrounds are played by the agent until the maximum number of rounds is reached.\nPlan data from 11 prostate HDR boost patients (1 for training, and 10 for\ntesting) treated in our clinic were included in this study. The dosimetric\nmetrics and the number of used needles of RL plan were compared to those of the\nclinical results (ground truth). Results: On average, RL plans and clinical\nplans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no\nstatistical significance), while RL plans have less prostate hotspot (Prostate\nV150) and Urethra D20% plans with statistical significance. Moreover, RL plans\nuse 2 less needles than clinical plan on average. Conclusion: We present the\nfirst study demonstrating the feasibility of using reinforcement learning to\nautonomously generate clinically practical HDR prostate brachytherapy plans.\nThis RL-based method achieved equal or improved plan quality compared to\nconventional clinical approaches while requiring fewer needles. With minimal\ndata requirements and strong generalizability, this approach has substantial\npotential to standardize brachytherapy planning, reduce clinical variability,\nand enhance patient outcomes.", "AI": {"tldr": "Reinforcement learning (RL) is used to autonomously generate needle positions and dwell times for HDR prostate brachytherapy, achieving comparable or better plan quality than clinical methods with fewer needles.", "motivation": "Current needle placement in HDR prostate brachytherapy relies on physician experience, leading to variability. RL can standardize planning, reduce procedure time, and ensure consistent quality.", "method": "A RL agent adjusts needle positions and dwell times iteratively to maximize a reward function, using data from 11 patients (1 for training, 10 for testing). Dosimetric metrics and needle usage are compared to clinical plans.", "result": "RL plans matched clinical plans in prostate coverage and rectum dose but reduced prostate hotspots and urethra dose significantly. They also used 2 fewer needles on average.", "conclusion": "RL is feasible for autonomously generating HDR prostate brachytherapy plans, improving quality and reducing needle usage, with potential to standardize planning and enhance outcomes."}}
{"id": "2408.12894", "pdf": "https://arxiv.org/pdf/2408.12894", "abs": "https://arxiv.org/abs/2408.12894", "authors": ["Yunji Seo", "Young Sun Choi", "Hyun Seung Son", "Youngjung Uh"], "title": "FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering", "categories": ["cs.CV", "68U05 (Primary) 68T45 (Secondary)", "I.3.3; I.3.7; I.3.5"], "comment": "Project page: https://3dgs-flod.github.io/flod/", "summary": "3D Gaussian Splatting (3DGS) and its subsequent works are restricted to\nspecific hardware setups, either on only low-cost or on only high-end\nconfigurations. Approaches aimed at reducing 3DGS memory usage enable rendering\non low-cost GPU but compromise rendering quality, which fails to leverage the\nhardware capabilities in the case of higher-end GPU. Conversely, methods that\nenhance rendering quality require high-end GPU with large VRAM, making such\nmethods impractical for lower-end devices with limited memory capacity.\nConsequently, 3DGS-based works generally assume a single hardware setup and\nlack the flexibility to adapt to varying hardware constraints.\n  To overcome this limitation, we propose Flexible Level of Detail (FLoD) for\n3DGS. FLoD constructs a multi-level 3DGS representation through level-specific\n3D scale constraints, where each level independently reconstructs the entire\nscene with varying detail and GPU memory usage. A level-by-level training\nstrategy is introduced to ensure structural consistency across levels.\nFurthermore, the multi-level structure of FLoD allows selective rendering of\nimage regions at different detail levels, providing additional memory-efficient\nrendering options. To our knowledge, among prior works which incorporate the\nconcept of Level of Detail (LoD) with 3DGS, FLoD is the first to follow the\ncore principle of LoD by offering adjustable options for a broad range of GPU\nsettings.\n  Experiments demonstrate that FLoD provides various rendering options with\ntrade-offs between quality and memory usage, enabling real-time rendering under\ndiverse memory constraints. Furthermore, we show that FLoD generalizes to\ndifferent 3DGS frameworks, indicating its potential for integration into future\nstate-of-the-art developments.", "AI": {"tldr": "FLoD introduces a flexible multi-level 3DGS representation to adapt to varying GPU hardware, balancing quality and memory usage.", "motivation": "Current 3DGS methods are hardware-specific, lacking adaptability to different GPU setups, which limits their practical application.", "method": "FLoD uses level-specific 3D scale constraints and a level-by-level training strategy to create a multi-level 3DGS representation, enabling selective rendering at varying detail levels.", "result": "FLoD offers adjustable rendering options for diverse GPU settings, maintaining real-time performance and quality-memory trade-offs.", "conclusion": "FLoD is the first 3DGS method to integrate LoD principles flexibly, showing promise for future 3DGS frameworks."}}
{"id": "2504.02132", "pdf": "https://arxiv.org/pdf/2504.02132", "abs": "https://arxiv.org/abs/2504.02132", "authors": ["Ezzeldin Shereen", "Dan Ristea", "Shae McFadden", "Burak Hasircioglu", "Vasilios Mavroudis", "Chris Hicks"], "title": "One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image", "categories": ["cs.CL", "cs.CR", "cs.CV", "cs.IR"], "comment": "19 pages, 7 figures", "summary": "Multi-modal retrieval augmented generation (M-RAG) is instrumental for\ninhibiting hallucinations in large multi-modal models (LMMs) through the use of\na factual knowledge base (KB). However, M-RAG introduces new attack vectors for\nadversaries that aim to disrupt the system by injecting malicious entries into\nthe KB. In this paper, we present the first poisoning attack against M-RAG\ntargeting visual document retrieval applications where the KB contains images\nof document pages. We propose two attacks, each of which require injecting only\na single adversarial image into the KB. Firstly, we propose a universal attack\nthat, for any potential user query, influences the response to cause a\ndenial-of-service (DoS) in the M-RAG system. Secondly, we present a targeted\nattack against one or a group of user queries, with the goal of spreading\ntargeted misinformation. For both attacks, we use a multi-objective\ngradient-based adversarial approach to craft the injected image while\noptimizing for both retrieval and generation. We evaluate our attacks against\nseveral visual document retrieval datasets, a diverse set of state-of-the-art\nretrievers (embedding models) and generators (LMMs), demonstrating the attack\neffectiveness in both the universal and targeted settings. We additionally\npresent results including commonly used defenses, various attack\nhyper-parameter settings, ablations, and attack transferability.", "AI": {"tldr": "The paper introduces poisoning attacks on multi-modal retrieval augmented generation (M-RAG) systems, demonstrating how adversarial images in the knowledge base can cause denial-of-service or spread misinformation.", "motivation": "To expose vulnerabilities in M-RAG systems, which rely on factual knowledge bases to prevent hallucinations in large multi-modal models, by showing how adversaries can exploit these systems.", "method": "Proposes two attacks: a universal attack causing DoS and a targeted attack spreading misinformation, using a multi-objective gradient-based adversarial approach to craft injected images.", "result": "Evaluated on visual document retrieval datasets, the attacks proved effective against various state-of-the-art retrievers and generators, even with defenses in place.", "conclusion": "The study highlights the susceptibility of M-RAG systems to poisoning attacks, emphasizing the need for robust defenses against such adversarial manipulations."}}
{"id": "2505.14479", "pdf": "https://arxiv.org/pdf/2505.14479", "abs": "https://arxiv.org/abs/2505.14479", "authors": ["Oren Sultan", "Eitan Stern", "Dafna Shahaf"], "title": "Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach", "categories": ["cs.AI", "cs.CL"], "comment": "long paper", "summary": "Large language models (LLMs) struggle with formal domains that require\nrigorous logical deduction and symbolic reasoning, such as mathematical proof\ngeneration. We propose a neuro-symbolic approach that combines LLMs' generative\nstrengths with structured components to overcome this challenge. As a\nproof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)\nwe retrieve analogous problems and use their proofs to guide the LLM, and (2) a\nformal verifier evaluates the generated proofs and provides feedback, helping\nthe model fix incorrect proofs. We demonstrate that our method significantly\nimproves proof accuracy for OpenAI's o1 model (58%-70% improvement); both\nanalogous problems and the verifier's feedback contribute to these gains. More\nbroadly, shifting to LLMs that generate provably correct conclusions could\ndramatically improve their reliability, accuracy and consistency, unlocking\ncomplex tasks and critical real-world applications that require\ntrustworthiness.", "AI": {"tldr": "A neuro-symbolic approach combining LLMs with structured components improves proof accuracy in geometry by leveraging analogous problems and formal verification.", "motivation": "LLMs struggle with formal domains like mathematical proof generation, requiring a method to enhance their logical reasoning.", "method": "The approach retrieves analogous problems to guide LLMs and uses a formal verifier to evaluate and correct generated proofs.", "result": "Proof accuracy for OpenAI's o1 model improved by 58%-70%, with both analogous problems and verifier feedback contributing.", "conclusion": "Enhancing LLMs to generate provably correct conclusions can boost reliability and unlock critical applications requiring trustworthiness."}}
{"id": "2506.09832", "pdf": "https://arxiv.org/pdf/2506.09832", "abs": "https://arxiv.org/abs/2506.09832", "authors": ["Dany Lauzon", "Julien Straubhaar", "Philippe Renard"], "title": "A Deep Generative Model for the Simulation of Discrete Karst Networks", "categories": ["stat.ML", "cs.LG"], "comment": "26 pages, 15 figures, submitted to Earth and Space Science", "summary": "The simulation of discrete karst networks presents a significant challenge\ndue to the complexity of the physicochemical processes occurring within various\ngeological and hydrogeological contexts over extended periods. This complex\ninterplay leads to a wide variety of karst network patterns, each intricately\nlinked to specific hydrogeological conditions. We explore a novel approach that\nrepresents karst networks as graphs and applies graph generative models (deep\nlearning techniques) to capture the intricate nature of karst environments. In\nthis representation, nodes retain spatial information and properties, while\nedges signify connections between nodes. Our generative process consists of two\nmain steps. First, we utilize graph recurrent neural networks (GraphRNN) to\nlearn the topological distribution of karst networks. GraphRNN decomposes the\ngraph simulation into a sequential generation of nodes and edges, informed by\npreviously generated structures. Second, we employ denoising diffusion\nprobabilistic models on graphs (G-DDPM) to learn node features (spatial\ncoordinates and other properties). G-DDPMs enable the generation of nodes\nfeatures on the graphs produced by the GraphRNN that adhere to the learned\nstatistical properties by sampling from the derived probability distribution,\nensuring that the generated graphs are realistic and capture the essential\nfeatures of the original data. We test our approach using real-world karst\nnetworks and compare generated subgraphs with actual subgraphs from the\ndatabase, by using geometry and topology metrics. Our methodology allows\nstochastic simulation of discrete karst networks across various types of\nformations, a useful tool for studying the behavior of physical processes such\nas flow and transport.", "AI": {"tldr": "A novel graph-based approach using deep learning (GraphRNN and G-DDPM) simulates complex karst networks, capturing topology and node features for realistic stochastic simulations.", "motivation": "Simulating karst networks is challenging due to their complexity and variability, requiring methods that capture intricate patterns and hydrogeological conditions.", "method": "Combines GraphRNN for topological learning and G-DDPM for node feature generation, ensuring realistic graph structures.", "result": "Tested on real-world data, the method generates realistic karst networks, validated by geometry and topology metrics.", "conclusion": "The approach enables stochastic simulation of karst networks, aiding studies of flow and transport processes."}}
{"id": "2408.14229", "pdf": "https://arxiv.org/pdf/2408.14229", "abs": "https://arxiv.org/abs/2408.14229", "authors": ["Leonid Erlygin", "Alexey Zaytsev"], "title": "Holistic Uncertainty Estimation For Open-Set Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurate uncertainty estimation is a critical challenge in open-set\nrecognition, where a probe biometric sample may belong to an unknown identity.\nIt can be addressed through sample quality estimation via probabilistic\nembeddings. However, the low variance of probabilistic embedding only partly\nimplies a low identification error probability: an embedding of a sample could\nbe close to several classes in a gallery, thus yielding high uncertainty\ndespite high sample quality. We propose HolUE - a holistic uncertainty\nestimation method based on a Bayesian probabilistic model; it is aware of two\nsources of ambiguity in the open-set recognition system: (1) the gallery\nuncertainty caused by overlapping classes and (2) the uncertainty of\nembeddings. Challenging open-set recognition datasets, such as IJB-C for the\nimage domain and VoxBlink for the audio domain, serve as a testbed for our\nmethod. We also provide a new open-set recognition protocol for the\nidentification of whales and dolphins. In all cases, HolUE better identifies\nrecognition errors than alternative uncertainty estimation methods, including\nthose based solely on sample quality.", "AI": {"tldr": "HolUE is a Bayesian probabilistic model for holistic uncertainty estimation in open-set recognition, addressing gallery and embedding uncertainties, outperforming existing methods.", "motivation": "Accurate uncertainty estimation is crucial in open-set recognition, especially when probe samples may belong to unknown identities, and existing methods like probabilistic embeddings partially address this.", "method": "HolUE uses a Bayesian probabilistic model to account for gallery uncertainty (overlapping classes) and embedding uncertainty, tested on datasets like IJB-C and VoxBlink.", "result": "HolUE outperforms other uncertainty estimation methods, including sample quality-based approaches, in identifying recognition errors.", "conclusion": "HolUE provides a robust solution for uncertainty estimation in open-set recognition, validated across diverse datasets and protocols."}}
{"id": "2504.04745", "pdf": "https://arxiv.org/pdf/2504.04745", "abs": "https://arxiv.org/abs/2504.04745", "authors": ["Ankush Raut", "Xiaofeng Zhu", "Maria Leonor Pacheco"], "title": "Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs", "categories": ["cs.CL"], "comment": "13 pages, 23 figures. Accepted at XLLM @ ACL 2025", "summary": "This paper evaluates the ability of Large Language Models (LLMs) to leverage\ncontextual information in the form of structured linguistic representations.\nSpecifically, we examine the impact of encoding both short and long contexts\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\nlanguage tasks. We perform our analysis using 8-bit quantized and\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\nresults indicate that, for tasks involving short contexts, augmenting the\nprompt with the AMR of the original language context often degrades the\nperformance of the underlying LLM. However, for tasks that involve long\ncontexts, such as dialogue summarization in the SAMSum dataset, this\nenhancement improves LLM performance, for example, by increasing the zero-shot\ncosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more\nevident in the newer and larger LLMs, but does not extend to the older or\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\noriginal text from a linearized AMR, achieving a cosine similarity of 81% in\nthe best-case scenario.", "AI": {"tldr": "LLMs perform worse with AMR for short contexts but improve for long contexts like dialogue summarization. Larger LLMs benefit more, and AMR reconstruction is effective.", "motivation": "To assess how LLMs use structured linguistic representations (AMR) for context in language tasks.", "method": "Analyzed AMR encoding in short/long contexts using quantized and instruction-tuned LLMs (Llama 3.1, Phi-3, Mistral 7B).", "result": "AMR degrades performance for short contexts but boosts it for long contexts (e.g., SAMSum dataset). Larger LLMs show more improvement. AMR reconstruction achieves 81% similarity.", "conclusion": "AMR is beneficial for long-context tasks in advanced LLMs but not for short contexts or smaller models."}}
{"id": "2505.16223", "pdf": "https://arxiv.org/pdf/2505.16223", "abs": "https://arxiv.org/abs/2505.16223", "authors": ["Sangyong Lee", "Subo Hwang", "Dohoon Kim"], "title": "MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network", "categories": ["cs.AI", "cs.LG"], "comment": "24 pages, 9 figures", "summary": "In this paper, we propose MADCluster, a novel model-agnostic anomaly\ndetection framework utilizing self-supervised clustering. MADCluster is\napplicable to various deep learning architectures and addresses the\n'hypersphere collapse' problem inherent in existing deep learning-based anomaly\ndetection methods. The core idea is to cluster normal pattern data into a\n'single cluster' while simultaneously learning the cluster center and mapping\ndata close to this center. Also, to improve expressiveness and enable effective\nsingle clustering, we propose a new 'One-directed Adaptive loss'. The\noptimization of this loss is mathematically proven. MADCluster consists of\nthree main components: Base Embedder capturing high-dimensional temporal\ndynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous\ncenter updates. Its model-agnostic characteristics are achieved by applying\nvarious architectures to the Base Embedder. Experiments on four time series\nbenchmark datasets demonstrate that applying MADCluster improves the overall\nperformance of comparative models. In conclusion, the compatibility of\nMADCluster shows potential for enhancing model performance across various\narchitectures.", "AI": {"tldr": "MADCluster is a model-agnostic anomaly detection framework using self-supervised clustering to address the 'hypersphere collapse' problem in deep learning-based methods. It clusters normal data into a single cluster and optimizes a new 'One-directed Adaptive loss'.", "motivation": "To solve the 'hypersphere collapse' issue in existing anomaly detection methods and provide a versatile solution applicable to various deep learning architectures.", "method": "MADCluster uses self-supervised clustering with three components: Base Embedder, Cluster Distance Mapping, and Sequence-wise Clustering. It introduces a 'One-directed Adaptive loss' for optimization.", "result": "Experiments on four time series datasets show improved performance of comparative models when using MADCluster.", "conclusion": "MADCluster is compatible with various architectures, enhancing model performance, and shows promise for broader applications."}}
{"id": "2409.16160", "pdf": "https://arxiv.org/pdf/2409.16160", "abs": "https://arxiv.org/abs/2409.16160", "authors": ["Yifang Men", "Yuan Yao", "Miaomiao Cui", "Liefeng Bo"], "title": "MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling", "categories": ["cs.CV"], "comment": "Project Page: https://menyifang.github.io/projects/MIMO/index.html", "summary": "Character video synthesis aims to produce realistic videos of animatable\ncharacters within lifelike scenes. As a fundamental problem in the computer\nvision and graphics community, 3D works typically require multi-view captures\nfor per-case training, which severely limits their applicability of modeling\narbitrary characters in a short time. Recent 2D methods break this limitation\nvia pre-trained diffusion models, but they struggle for pose generality and\nscene interaction. To this end, we propose MIMO, a novel framework which can\nnot only synthesize character videos with controllable attributes (i.e.,\ncharacter, motion and scene) provided by simple user inputs, but also\nsimultaneously achieve advanced scalability to arbitrary characters, generality\nto novel 3D motions, and applicability to interactive real-world scenes in a\nunified framework. The core idea is to encode the 2D video to compact spatial\ncodes, considering the inherent 3D nature of video occurrence. Concretely, we\nlift the 2D frame pixels into 3D using monocular depth estimators, and\ndecompose the video clip to three spatial components (i.e., main human,\nunderlying scene, and floating occlusion) in hierarchical layers based on the\n3D depth. These components are further encoded to canonical identity code,\nstructured motion code and full scene code, which are utilized as control\nsignals of synthesis process. The design of spatial decomposed modeling enables\nflexible user control, complex motion expression, as well as 3D-aware synthesis\nfor scene interactions. Experimental results demonstrate effectiveness and\nrobustness of the proposed method.", "AI": {"tldr": "MIMO is a novel framework for character video synthesis that achieves scalability, motion generality, and scene interaction by encoding 2D videos into 3D spatial codes.", "motivation": "Existing 3D methods require multi-view captures, limiting applicability, while 2D methods struggle with pose generality and scene interaction. MIMO addresses these limitations.", "method": "MIMO encodes 2D videos into 3D spatial codes (identity, motion, scene) using monocular depth estimation and hierarchical decomposition.", "result": "The framework enables flexible user control, complex motion expression, and 3D-aware synthesis, demonstrating effectiveness in experiments.", "conclusion": "MIMO provides a unified solution for scalable, interactive, and realistic character video synthesis."}}
{"id": "2504.12347", "pdf": "https://arxiv.org/pdf/2504.12347", "abs": "https://arxiv.org/abs/2504.12347", "authors": ["Mika Set\u00e4l\u00e4", "Pieta Sikstr\u00f6m", "Ville Heilala", "Tommi K\u00e4rkk\u00e4inen"], "title": "Assessment of Evolving Large Language Models in Upper Secondary Mathematics", "categories": ["cs.CL", "cs.AI", "cs.CY", "K.3; I.2"], "comment": null, "summary": "Large language models (LLMs) have shown increasing promise in educational\nsettings, yet their mathematical reasoning has been considered evolving. This\nstudy evaluates the mathematical capabilities of various LLMs using the Finnish\nmatriculation examination, a high-stakes digital test for upper secondary\neducation. Initial tests yielded moderate performance corresponding to\nmid-range grades, but later evaluations demonstrated substantial improvements\nas the language models evolved. Remarkably, some models achieved near-perfect\nor perfect scores, matching top student performance and qualifying for\nuniversity admission. Our findings highlight the rapid advances in the\nmathematical proficiency of LLMs and illustrate their potential as underlying\ntools to support learning and teaching in a variety of ways.", "AI": {"tldr": "LLMs' mathematical reasoning improves over time, achieving near-perfect scores on a high-stakes Finnish exam, showcasing their potential in education.", "motivation": "To evaluate the evolving mathematical capabilities of LLMs using a rigorous educational benchmark.", "method": "Testing various LLMs on the Finnish matriculation examination, a high-stakes digital test for upper secondary education.", "result": "Initial moderate performance improved to near-perfect or perfect scores, matching top student performance.", "conclusion": "LLMs show rapid advancements in mathematical proficiency, indicating their potential as educational tools."}}
{"id": "2505.20728", "pdf": "https://arxiv.org/pdf/2505.20728", "abs": "https://arxiv.org/abs/2505.20728", "authors": ["Zesen Lyu", "Dandan Zhang", "Wei Ye", "Fangdi Li", "Zhihang Jiang", "Yao Yang"], "title": "Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Spatial reasoning is a core component of human cognition, enabling\nindividuals to perceive, comprehend, and interact with the physical world. It\nrelies on a nuanced understanding of spatial structures and inter-object\nrelationships, serving as the foundation for complex reasoning and\ndecision-making. To investigate whether current vision-language models (VLMs)\nexhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark\nconsisting of 1,100 carefully curated real-world images with high spatial\ncomplexity. Based on this dataset, we design five tasks to rigorously evaluate\nVLMs' spatial perception, structural understanding, and reasoning capabilities,\nwhile deliberately minimizing reliance on domain-specific knowledge to better\nisolate and assess the general spatial reasoning capability. We conduct a\ncomprehensive evaluation across 24 state-of-the-art VLMs. The results show that\neven the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy\nand performs particularly poorly on the Order Generation task, with only 30.00%\naccuracy, far below the performance exceeding 90% achieved by human\nparticipants. This persistent gap underscores the need for continued progress,\npositioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for\nadvancing spatial reasoning research in VLMs. Our project page is at\nhttps://zesen01.github.io/jigsaw-puzzles", "AI": {"tldr": "The paper introduces Jigsaw-Puzzles, a benchmark to evaluate spatial reasoning in VLMs, revealing significant performance gaps compared to humans.", "motivation": "To assess if VLMs can match human-like spatial reasoning, a critical cognitive skill, using a novel benchmark.", "method": "Developed Jigsaw-Puzzles with 1,100 complex images and five tasks to test VLMs' spatial capabilities, minimizing domain bias.", "result": "Top VLM (Gemini-2.5-Pro) scored 77.14% overall, with 30% on Order Generation, far below human performance (>90%).", "conclusion": "Jigsaw-Puzzles highlights VLMs' limitations in spatial reasoning, serving as a diagnostic tool for future research."}}
{"id": "2010.00788", "pdf": "https://arxiv.org/pdf/2010.00788", "abs": "https://arxiv.org/abs/2010.00788", "authors": ["Santiago Gonzalez", "Xin Qiu", "Risto Miikkulainen"], "title": "Effective Regularization Through Loss-Function Metalearning", "categories": ["cs.LG", "cs.NE", "stat.ML"], "comment": "A shorter version of this paper appeared in CEC 2025; this paper\n  includes appendices, expanded references, and corrections", "summary": "Evolutionary computation can be used to optimize several different aspects of\nneural network architectures. For instance, the TaylorGLO method discovers\nnovel, customized loss functions, resulting in improved performance, faster\ntraining, and improved data utilization. A likely reason is that such functions\ndiscourage overfitting, leading to effective regularization. This paper\ndemonstrates theoretically that this is indeed the case for TaylorGLO. Learning\nrule decomposition reveals that evolved loss functions balance two factors: the\npull toward zero error, and a push away from it to avoid overfitting. This is a\ngeneral principle that may be used to understand other regularization\ntechniques as well (as demonstrated in this paper for label smoothing). The\ntheoretical analysis leads to a constraint that can be utilized to find more\neffective loss functions in practice; the mechanism also results in networks\nthat are more robust (as demonstrated in this paper with adversarial inputs).\nThe analysis in this paper thus constitutes a first step towards understanding\nregularization, and demonstrates the power of evolutionary neural architecture\nsearch in general.", "AI": {"tldr": "TaylorGLO uses evolutionary computation to optimize neural network loss functions, improving performance, training speed, and data utilization by balancing error reduction and overfitting avoidance.", "motivation": "To theoretically validate that evolved loss functions in TaylorGLO discourage overfitting and improve regularization, and to generalize this principle to other techniques like label smoothing.", "method": "The paper employs learning rule decomposition to analyze evolved loss functions, revealing a balance between minimizing error and avoiding overfitting.", "result": "Theoretical analysis confirms the regularization effect of TaylorGLO, leading to more robust networks and a practical constraint for designing better loss functions.", "conclusion": "The study advances understanding of regularization and highlights the potential of evolutionary neural architecture search."}}
{"id": "2409.19149", "pdf": "https://arxiv.org/pdf/2409.19149", "abs": "https://arxiv.org/abs/2409.19149", "authors": ["Tong Liu", "Zhixin Lai", "Jiawen Wang", "Gengyuan Zhang", "Shuo Chen", "Philip Torr", "Vera Demberg", "Volker Tresp", "Jindong Gu"], "title": "Multimodal Pragmatic Jailbreak on Text-to-image Models", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "Diffusion models have recently achieved remarkable advancements in terms of\nimage quality and fidelity to textual prompts. Concurrently, the safety of such\ngenerative models has become an area of growing concern. This work introduces a\nnovel type of jailbreak, which triggers T2I models to generate the image with\nvisual text, where the image and the text, although considered to be safe in\nisolation, combine to form unsafe content. To systematically explore this\nphenomenon, we propose a dataset to evaluate the current diffusion-based\ntext-to-image (T2I) models under such jailbreak. We benchmark nine\nrepresentative T2I models, including two closed-source commercial models.\nExperimental results reveal a concerning tendency to produce unsafe content:\nall tested models suffer from such type of jailbreak, with rates of unsafe\ngeneration ranging from around 10\\% to 70\\% where DALLE 3 demonstrates almost\nthe highest unsafety. In real-world scenarios, various filters such as keyword\nblocklists, customized prompt filters, and NSFW image filters, are commonly\nemployed to mitigate these risks. We evaluate the effectiveness of such filters\nagainst our jailbreak and found that, while these filters may be effective for\nsingle modality detection, they fail to work against our jailbreak. We also\ninvestigate the underlying reason for such jailbreaks, from the perspective of\ntext rendering capability and training data. Our work provides a foundation for\nfurther development towards more secure and reliable T2I models. Project page\nat https://multimodalpragmatic.github.io/.", "AI": {"tldr": "The paper introduces a novel jailbreak method for text-to-image (T2I) models, where safe images and text combine to create unsafe content. It benchmarks nine models, revealing high unsafety rates, and evaluates existing filters' ineffectiveness.", "motivation": "To address growing safety concerns in generative models, the study explores a new type of jailbreak that bypasses traditional safety measures by combining seemingly safe elements.", "method": "The authors propose a dataset to evaluate T2I models under this jailbreak, benchmark nine models (including commercial ones), and assess the effectiveness of common filters.", "result": "All tested models are vulnerable, with unsafe generation rates ranging from 10% to 70%. Existing filters fail to mitigate the jailbreak.", "conclusion": "The work highlights a critical vulnerability in T2I models and calls for further development of more secure and reliable systems."}}
{"id": "2504.12663", "pdf": "https://arxiv.org/pdf/2504.12663", "abs": "https://arxiv.org/abs/2504.12663", "authors": ["Xiaotian Zhang", "Ruizhe Chen", "Yang Feng", "Zuozhu Liu"], "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "categories": ["cs.CL", "cs.AI"], "comment": "ACL Finding", "summary": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment. Our code is available\nhere.", "AI": {"tldr": "Persona-judge introduces a training-free method for aligning language models with human preferences by leveraging intrinsic model judgments, avoiding costly reward signals and annotations.", "motivation": "Existing methods for aligning language models with human preferences are limited by scalability and computational costs due to reliance on reward signals and annotated data.", "method": "Persona-judge uses a discriminative paradigm where a draft model generates tokens based on preferences, and a judge model cross-validates them, eliminating the need for external rewards.", "result": "Experiments show Persona-judge is scalable and computationally efficient, enabling personalized alignment without additional training.", "conclusion": "Persona-judge provides an adaptive and efficient solution for personalized alignment, advancing customized language model applications."}}
{"id": "2505.23885", "pdf": "https://arxiv.org/pdf/2505.23885", "abs": "https://arxiv.org/abs/2505.23885", "authors": ["Mengkang Hu", "Yuhang Zhou", "Wendong Fan", "Yuzhou Nie", "Bowei Xia", "Tao Sun", "Ziyu Ye", "Zhaoxuan Jin", "Yingru Li", "Qiguang Chen", "Zeyu Zhang", "Yifeng Wang", "Qianshuo Ye", "Bernard Ghanem", "Ping Luo", "Guohao Li"], "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "categories": ["cs.AI", "cs.CL"], "comment": "Project Page: https://github.com/camel-ai/owl", "summary": "Large Language Model (LLM)-based multi-agent systems show promise for\nautomating real-world tasks but struggle to transfer across domains due to\ntheir domain-specific nature. Current approaches face two critical\nshortcomings: they require complete architectural redesign and full retraining\nof all components when applied to new domains. We introduce Workforce, a\nhierarchical multi-agent framework that decouples strategic planning from\nspecialized execution through a modular architecture comprising: (i) a\ndomain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask\nmanagement, and (iii) specialized Workers with domain-specific tool-calling\ncapabilities. This decoupling enables cross-domain transferability during both\ninference and training phases: During inference, Workforce seamlessly adapts to\nnew domains by adding or modifying worker agents; For training, we introduce\nOptimized Workforce Learning (OWL), which improves generalization across\ndomains by optimizing a domain-agnostic planner with reinforcement learning\nfrom real-world feedback. To validate our approach, we evaluate Workforce on\nthe GAIA benchmark, covering various realistic, multi-domain agentic tasks.\nExperimental results demonstrate Workforce achieves open-source\nstate-of-the-art performance (69.70%), outperforming commercial systems like\nOpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model\nachieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to\nGPT-4o on challenging tasks. To summarize, by enabling scalable generalization\nand modular domain transfer, our work establishes a foundation for the next\ngeneration of general-purpose AI assistants.", "AI": {"tldr": "Workforce is a hierarchical multi-agent framework for cross-domain task automation, improving transferability and performance without full retraining.", "motivation": "Current LLM-based multi-agent systems struggle with domain transfer due to domain-specific designs, requiring costly redesigns and retraining.", "method": "Workforce uses a modular architecture with a domain-agnostic Planner, Coordinator, and specialized Workers, coupled with Optimized Workforce Learning (OWL) for training.", "result": "Workforce achieves 69.70% accuracy on GAIA, outperforming commercial systems, and OWL-trained models show significant improvements (+16.37%).", "conclusion": "The framework enables scalable generalization and modular domain transfer, advancing general-purpose AI assistants."}}
{"id": "2307.08423", "pdf": "https://arxiv.org/pdf/2307.08423", "abs": "https://arxiv.org/abs/2307.08423", "authors": ["Xuan Zhang", "Limei Wang", "Jacob Helwig", "Youzhi Luo", "Cong Fu", "Yaochen Xie", "Meng Liu", "Yuchao Lin", "Zhao Xu", "Keqiang Yan", "Keir Adams", "Maurice Weiler", "Xiner Li", "Tianfan Fu", "Yucheng Wang", "Alex Strasser", "Haiyang Yu", "YuQing Xie", "Xiang Fu", "Shenglong Xu", "Yi Liu", "Yuanqi Du", "Alexandra Saxton", "Hongyi Ling", "Hannah Lawrence", "Hannes St\u00e4rk", "Shurui Gui", "Carl Edwards", "Nicholas Gao", "Adriana Ladera", "Tailin Wu", "Elyssa F. Hofgard", "Aria Mansouri Tehrani", "Rui Wang", "Ameya Daigavane", "Montgomery Bohde", "Jerry Kurtin", "Qian Huang", "Tuong Phung", "Minkai Xu", "Chaitanya K. Joshi", "Simon V. Mathis", "Kamyar Azizzadenesheli", "Ada Fang", "Al\u00e1n Aspuru-Guzik", "Erik Bekkers", "Michael Bronstein", "Marinka Zitnik", "Anima Anandkumar", "Stefano Ermon", "Pietro Li\u00f2", "Rose Yu", "Stephan G\u00fcnnemann", "Jure Leskovec", "Heng Ji", "Jimeng Sun", "Regina Barzilay", "Tommi Jaakkola", "Connor W. Coley", "Xiaoning Qian", "Xiaofeng Qian", "Tess Smidt", "Shuiwang Ji"], "title": "Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems", "categories": ["cs.LG", "physics.comp-ph"], "comment": "Accepted to Foundations and Trends in Machine Learning", "summary": "Advances in artificial intelligence (AI) are fueling a new paradigm of\ndiscoveries in natural sciences. Today, AI has started to advance natural\nsciences by improving, accelerating, and enabling our understanding of natural\nphenomena at a wide range of spatial and temporal scales, giving rise to a new\narea of research known as AI for science (AI4Science). Being an emerging\nresearch paradigm, AI4Science is unique in that it is an enormous and highly\ninterdisciplinary area. Thus, a unified and technical treatment of this field\nis needed yet challenging. This work aims to provide a technically thorough\naccount of a subarea of AI4Science; namely, AI for quantum, atomistic, and\ncontinuum systems. These areas aim at understanding the physical world from the\nsubatomic (wavefunctions and electron density), atomic (molecules, proteins,\nmaterials, and interactions), to macro (fluids, climate, and subsurface) scales\nand form an important subarea of AI4Science. A unique advantage of focusing on\nthese areas is that they largely share a common set of challenges, thereby\nallowing a unified and foundational treatment. A key common challenge is how to\ncapture physics first principles, especially symmetries, in natural systems by\ndeep learning methods. We provide an in-depth yet intuitive account of\ntechniques to achieve equivariance to symmetry transformations. We also discuss\nother common technical challenges, including explainability,\nout-of-distribution generalization, knowledge transfer with foundation and\nlarge language models, and uncertainty quantification. To facilitate learning\nand education, we provide categorized lists of resources that we found to be\nuseful. We strive to be thorough and unified and hope this initial effort may\ntrigger more community interests and efforts to further advance AI4Science.", "AI": {"tldr": "The paper provides a technical overview of AI4Science, focusing on AI applications for quantum, atomistic, and continuum systems, addressing challenges like symmetry equivariance and generalization.", "motivation": "To unify and technically treat the interdisciplinary field of AI4Science, particularly for understanding physical systems across scales.", "method": "In-depth analysis of deep learning techniques for capturing physics principles, with emphasis on symmetry equivariance, explainability, and uncertainty quantification.", "result": "A thorough technical account of AI methods for physical systems, highlighting common challenges and solutions.", "conclusion": "The work aims to inspire further community efforts in advancing AI4Science by providing foundational insights and resources."}}
{"id": "2410.02080", "pdf": "https://arxiv.org/pdf/2410.02080", "abs": "https://arxiv.org/abs/2410.02080", "authors": ["Sara Ghazanfari", "Alexandre Araujo", "Prashanth Krishnamurthy", "Siddharth Garg", "Farshad Khorrami"], "title": "EMMA: Efficient Visual Alignment in Multi-Modal LLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Multi-modal Large Language Models (MLLMs) have recently exhibited impressive\ngeneral-purpose capabilities by leveraging vision foundation models to encode\nthe core concepts of images into representations. These are then combined with\ninstructions and processed by the language model to generate high-quality\nresponses. Despite significant progress in enhancing the language component,\nchallenges persist in optimally fusing visual encodings within the language\nmodel for task-specific adaptability. Recent research has focused on improving\nthis fusion through modality adaptation modules but at the cost of\nsignificantly increased model complexity and training data needs. In this\npaper, we propose EMMA (Efficient Multi-Modal Adaptation), a lightweight\ncross-modality module designed to efficiently fuse visual and textual\nencodings, generating instruction-aware visual representations for the language\nmodel. Our key contributions include: (1) an efficient early fusion mechanism\nthat integrates vision and language representations with minimal added\nparameters (less than 0.2% increase in model size), (2) an in-depth\ninterpretability analysis that sheds light on the internal mechanisms of the\nproposed method; (3) comprehensive experiments that demonstrate notable\nimprovements on both specialized and general benchmarks for MLLMs. Empirical\nresults show that EMMA boosts performance across multiple tasks by up to 9.3%\nwhile significantly improving robustness against hallucinations. Our code is\navailable at https://github.com/SaraGhazanfari/EMMA", "AI": {"tldr": "EMMA is a lightweight cross-modality module for efficient fusion of visual and textual encodings in MLLMs, improving performance and robustness with minimal added parameters.", "motivation": "Challenges in optimally fusing visual encodings within language models for task-specific adaptability, despite progress in enhancing language components.", "method": "Proposes EMMA, featuring an efficient early fusion mechanism, interpretability analysis, and comprehensive experiments.", "result": "Boosts performance by up to 9.3% across tasks and improves robustness against hallucinations.", "conclusion": "EMMA offers an efficient solution for multi-modal fusion in MLLMs, balancing performance and model complexity."}}
{"id": "2505.01015", "pdf": "https://arxiv.org/pdf/2505.01015", "abs": "https://arxiv.org/abs/2505.01015", "authors": ["Jongwook Han", "Dongmin Choi", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "title": "Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "This paper has been accepted for publication at ACL 2025", "summary": "The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage. Second, each item is rated by human\nsubjects based on its similarity to their own thoughts, and correlations\nbetween these ratings and the subjects' actual value scores are derived. This\npsychometrically validated approach ensures that items strongly correlated with\nspecific values serve as reliable items for assessing those values. Through\nevaluating 44 LLMs with our benchmark, we find that these models prioritize\nBenevolence, Security, and Self-Direction values while placing less emphasis on\nTradition, Power, and Achievement values. Also, our analysis reveals biases in\nhow LLMs perceive various demographic groups, deviating from real human data.", "AI": {"tldr": "The paper introduces the Value Portrait benchmark to evaluate language models' value orientations, addressing biases in existing benchmarks by using real-life interactions and psychometric validation.", "motivation": "Existing benchmarks for language models are biased and lack real-world relevance, necessitating a more authentic and reliable evaluation framework.", "method": "The Value Portrait benchmark uses real-life user-LLM interactions and psychometric validation via human ratings correlated with their actual value scores.", "result": "Evaluation of 44 LLMs shows prioritization of Benevolence, Security, and Self-Direction values, with biases in demographic group perceptions.", "conclusion": "The benchmark provides a reliable, real-world-aligned method for assessing LLM values, revealing biases and value emphases."}}
{"id": "2506.00328", "pdf": "https://arxiv.org/pdf/2506.00328", "abs": "https://arxiv.org/abs/2506.00328", "authors": ["Kourosh Shahnazari", "Seyed Moein Ayyoubzadeh", "Mohammadali Keshtparvar"], "title": "BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies", "categories": ["cs.AI"], "comment": null, "summary": "The quest for interpretable reinforcement learning is a grand challenge for\nthe deployment of autonomous decision-making systems in safety-critical\napplications. Modern deep reinforcement learning approaches, while powerful,\ntend to produce opaque policies that compromise verification, reduce\ntransparency, and impede human oversight. To address this, we introduce BASIL\n(Best-Action Symbolic Interpretable Learning), a systematic approach for\ngenerating symbolic, rule-based policies via online evolutionary search with\nquality-diversity (QD) optimization. BASIL represents policies as ordered lists\nof symbolic predicates over state variables, ensuring full interpretability and\ntractable policy complexity. By using a QD archive, the methodology in the\nproposed study encourages behavioral and structural diversity between\ntop-performing solutions, while a complexity-aware fitness encourages the\nsynthesis of compact representations. The evolutionary system supports the use\nof exact constraints for rule count and system adaptability for balancing\ntransparency with expressiveness. Empirical comparisons with three benchmark\ntasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently\nsynthesizes interpretable controllers with compact representations comparable\nto deep reinforcement learning baselines. Herein, this article introduces a new\ninterpretable policy synthesis method that combines symbolic expressiveness,\nevolutionary diversity, and online learning through a unifying framework.", "AI": {"tldr": "BASIL introduces a symbolic, interpretable reinforcement learning method using evolutionary search and quality-diversity optimization, achieving comparable performance to deep RL baselines.", "motivation": "Address the opacity of deep RL policies in safety-critical applications by ensuring interpretability and transparency.", "method": "Uses online evolutionary search with quality-diversity optimization to generate symbolic, rule-based policies represented as ordered lists of predicates.", "result": "BASIL produces interpretable controllers with compact representations, matching deep RL performance on benchmark tasks.", "conclusion": "BASIL successfully combines symbolic expressiveness, evolutionary diversity, and online learning for interpretable policy synthesis."}}
{"id": "2309.16109", "pdf": "https://arxiv.org/pdf/2309.16109", "abs": "https://arxiv.org/abs/2309.16109", "authors": ["Han Bao"], "title": "Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Contrastive learning is a self-supervised representation learning framework,\nwhere two positive views generated through data augmentation are made similar\nby an attraction force in a data representation space, while a repulsive force\nmakes them far from negative examples. Non-contrastive learning, represented by\nBYOL and SimSiam, further gets rid of negative examples and improves\ncomputational efficiency. While learned representations may collapse into a\nsingle point due to the lack of the repulsive force at first sight, Tian et al.\n(2021) revealed through the learning dynamics analysis that the representations\ncan avoid collapse if data augmentation is sufficiently stronger than\nregularization. However, their analysis does not take into account\ncommonly-used feature normalization, a normalizer before measuring the\nsimilarity of representations, and hence excessively strong regularization may\ncollapse the dynamics, which is an unnatural behavior under the presence of\nfeature normalization. Therefore, we extend the previous theory based on the L2\nloss by considering the cosine loss, which involves feature normalization. We\nshow that the cosine loss induces sixth-order dynamics (while the L2 loss\ninduces a third-order one), in which a stable equilibrium dynamically emerges\neven if there are only collapsed solutions with given initial parameters. Thus,\nwe offer a new understanding that feature normalization plays an important role\nin robustly preventing the dynamics collapse.", "AI": {"tldr": "The paper extends prior contrastive learning theory by incorporating feature normalization, showing it prevents representation collapse under cosine loss.", "motivation": "To address the gap in prior work, which ignored feature normalization's role in preventing collapse in non-contrastive learning.", "method": "Extends Tian et al.'s theory by analyzing cosine loss dynamics, revealing sixth-order behavior compared to L2 loss's third-order.", "result": "Feature normalization robustly prevents collapse, with stable equilibria emerging dynamically even from collapsed initial states.", "conclusion": "Feature normalization is crucial for avoiding collapse in non-contrastive learning, offering new theoretical insights."}}
{"id": "2410.14398", "pdf": "https://arxiv.org/pdf/2410.14398", "abs": "https://arxiv.org/abs/2410.14398", "authors": ["Felix Koulischer", "Johannes Deleu", "Gabriel Raya", "Thomas Demeester", "Luca Ambrogioni"], "title": "Dynamic Negative Guidance of Diffusion Models", "categories": ["cs.CV"], "comment": "Paper accepted at ICLR 2025 (poster). Our implementation is available\n  at https://github.com/FelixKoulischer/Dynamic-Negative-Guidance.git", "summary": "Negative Prompting (NP) is widely utilized in diffusion models, particularly\nin text-to-image applications, to prevent the generation of undesired features.\nIn this paper, we show that conventional NP is limited by the assumption of a\nconstant guidance scale, which may lead to highly suboptimal results, or even\ncomplete failure, due to the non-stationarity and state-dependence of the\nreverse process. Based on this analysis, we derive a principled technique\ncalled Dynamic Negative Guidance, which relies on a near-optimal time and state\ndependent modulation of the guidance without requiring additional training.\nUnlike NP, negative guidance requires estimating the posterior class\nprobability during the denoising process, which is achieved with limited\nadditional computational overhead by tracking the discrete Markov Chain during\nthe generative process. We evaluate the performance of DNG class-removal on\nMNIST and CIFAR10, where we show that DNG leads to higher safety, preservation\nof class balance and image quality when compared with baseline methods.\nFurthermore, we show that it is possible to use DNG with Stable Diffusion to\nobtain more accurate and less invasive guidance than NP.", "AI": {"tldr": "Dynamic Negative Guidance (DNG) improves upon Negative Prompting (NP) by addressing its limitations with a time and state-dependent modulation, enhancing safety, class balance, and image quality without extra training.", "motivation": "Conventional NP assumes a constant guidance scale, leading to suboptimal results due to the non-stationarity of the reverse process. This paper aims to overcome this limitation.", "method": "DNG modulates guidance dynamically by estimating posterior class probabilities during denoising, leveraging the discrete Markov Chain for efficiency.", "result": "DNG outperforms NP in class-removal tasks on MNIST and CIFAR10, offering better safety, class balance, and image quality. It also works effectively with Stable Diffusion.", "conclusion": "DNG provides a principled, efficient alternative to NP, improving guidance accuracy and reducing invasiveness in diffusion models."}}
{"id": "2505.06987", "pdf": "https://arxiv.org/pdf/2505.06987", "abs": "https://arxiv.org/abs/2505.06987", "authors": ["Xiaoyu Wang", "Yue Zhao", "Qingqing Gu", "Zhonglin Jiang", "Xiaokai Chen", "Yong Chen", "Luo Ji"], "title": "Convert Language Model into a Value-based Strategic Planner", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 6 figures, Accepted by ACL 2025 Industry Track", "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Q-learning on LLMs, and propose a framework called straQ*. Our\nframework allows a plug-and-play LLM to bootstrap the planning during ESC,\ndetermine the optimal strategy based on long-term returns, and finally guide\nthe LLM to response. Substantial experiments on ESC datasets suggest that\nstraQ* outperforms many baselines, including direct inference, self-refine,\nchain of thought, finetuning, and finite state machines.", "AI": {"tldr": "The paper introduces straQ*, a Q-learning-based framework for emotional support conversation (ESC) to improve long-term satisfaction by optimizing LLM strategies.", "motivation": "Existing ESC studies using LLMs lack a state model perspective, leading to suboptimal long-term solutions.", "method": "Leverages Q-learning on LLMs for planning and strategy optimization in ESC.", "result": "straQ* outperforms baselines like direct inference, self-refine, and finite state machines.", "conclusion": "straQ* provides a robust solution for ESC by integrating Q-learning with LLMs for better long-term outcomes."}}
{"id": "2506.02865", "pdf": "https://arxiv.org/pdf/2506.02865", "abs": "https://arxiv.org/abs/2506.02865", "authors": ["Mathieu Andreux", "Breno Baldas Skuk", "Hamza Benchekroun", "Emilien Bir\u00e9", "Antoine Bonnet", "Riaz Bordie", "Nathan Bout", "Matthias Brunel", "Pierre-Louis Cedoz", "Antoine Chassang", "Micka\u00ebl Chen", "Alexandra D. Constantinou", "Antoine d'Andign\u00e9", "Hubert de La Jonqui\u00e8re", "Aur\u00e9lien Delfosse", "Ludovic Denoyer", "Alexis Deprez", "Augustin Derupti", "Michael Eickenberg", "Math\u00efs Federico", "Charles Kantor", "Xavier Koegler", "Yann Labb\u00e9", "Matthew C. H. Lee", "Erwan Le Jumeau de Kergaradec", "Amir Mahla", "Avshalom Manevich", "Adrien Maret", "Charles Masson", "Rafa\u00ebl Maurin", "Arturo Mena", "Philippe Modard", "Axel Moyal", "Axel Nguyen Kerbel", "Julien Revelle", "Mats L. Richter", "Mar\u00eda Santos", "Laurent Sifre", "Maxime Theillard", "Marc Thibault", "Louis Thiry", "L\u00e9o Tronchon", "Nicolas Usunier", "Tony Wu"], "title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights", "categories": ["cs.AI"], "comment": "Alphabetical order", "summary": "We present Surfer-H, a cost-efficient web agent that integrates\nVision-Language Models (VLM) to perform user-defined tasks on the web. We pair\nit with Holo1, a new open-weight collection of VLMs specialized in web\nnavigation and information extraction. Holo1 was trained on carefully curated\ndata sources, including open-access web content, synthetic examples, and\nself-produced agentic data. Holo1 tops generalist User Interface (UI)\nbenchmarks as well as our new web UI localization benchmark, WebClick. When\npowered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on\nWebVoyager, striking a Pareto-optimal balance between accuracy and\ncost-efficiency. To accelerate research advancement in agentic systems, we are\nopen-sourcing both our WebClick evaluation dataset and the Holo1 model weights.", "AI": {"tldr": "Surfer-H is a cost-efficient web agent using Vision-Language Models (VLM) for web tasks, paired with Holo1, a specialized VLM. Holo1 excels in benchmarks, and Surfer-H achieves 92.2% performance on WebVoyager. Both WebClick dataset and Holo1 weights are open-sourced.", "motivation": "To create a cost-efficient web agent capable of performing user-defined tasks on the web using advanced VLMs, and to advance research in agentic systems by providing open-source tools.", "method": "Integration of Surfer-H with Holo1, a specialized VLM trained on curated data (open-access web content, synthetic examples, and self-produced agentic data).", "result": "Holo1 tops UI benchmarks and WebClick. Surfer-H achieves 92.2% performance on WebVoyager, balancing accuracy and cost-efficiency.", "conclusion": "Surfer-H and Holo1 demonstrate high performance in web tasks, and their open-sourcing aims to accelerate research in agentic systems."}}
{"id": "2310.07320", "pdf": "https://arxiv.org/pdf/2310.07320", "abs": "https://arxiv.org/abs/2310.07320", "authors": ["Jingxuan Zhu", "Alec Koppel", "Alvaro Velasquez", "Ji Liu"], "title": "Byzantine-Resilient Decentralized Multi-Armed Bandits", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "add a disclaimer", "summary": "In decentralized cooperative multi-armed bandits (MAB), each agent observes a\ndistinct stream of rewards, and seeks to exchange information with others to\nselect a sequence of arms so as to minimize its regret. Agents in the\ncooperative setting can outperform a single agent running a MAB method such as\nUpper-Confidence Bound (UCB) independently. In this work, we study how to\nrecover such salient behavior when an unknown fraction of the agents can be\nByzantine, that is, communicate arbitrarily wrong information in the form of\nreward mean-estimates or confidence sets. This framework can be used to model\nattackers in computer networks, instigators of offensive content into\nrecommender systems, or manipulators of financial markets. Our key contribution\nis the development of a fully decentralized resilient upper confidence bound\n(UCB) algorithm that fuses an information mixing step among agents with a\ntruncation of inconsistent and extreme values. This truncation step enables us\nto establish that the performance of each normal agent is no worse than the\nclassic single-agent UCB1 algorithm in terms of regret, and more importantly,\nthe cumulative regret of all normal agents is strictly better than the\nnon-cooperative case, provided that each agent has at least 3f+1 neighbors\nwhere f is the maximum possible Byzantine agents in each agent's neighborhood.\nExtensions to time-varying neighbor graphs, and minimax lower bounds are\nfurther established on the achievable regret. Experiments corroborate the\nmerits of this framework in practice.", "AI": {"tldr": "A decentralized resilient UCB algorithm is proposed for cooperative multi-armed bandits with Byzantine agents, ensuring regret performance no worse than single-agent UCB1 and better than non-cooperative cases.", "motivation": "To address the challenge of Byzantine agents in decentralized cooperative MAB, which can disrupt reward estimates and confidence sets, modeling real-world scenarios like network attacks or market manipulation.", "method": "Develops a decentralized resilient UCB algorithm combining information mixing among agents and truncation of inconsistent/extreme values. Requires each agent to have at least 3f+1 neighbors (f: max Byzantine agents).", "result": "Normal agents perform no worse than single-agent UCB1, and cumulative regret is better than non-cooperative cases. Extensions for time-varying graphs and minimax bounds are provided.", "conclusion": "The framework effectively mitigates Byzantine behavior, improving cooperative performance with theoretical guarantees and practical validation."}}
{"id": "2411.13610", "pdf": "https://arxiv.org/pdf/2411.13610", "abs": "https://arxiv.org/abs/2411.13610", "authors": ["Hao Ju", "Shaofei Huang", "Si Liu", "Zhedong Zheng"], "title": "Video2BEV: Transforming Drone Videos to BEVs for Video-based Geo-localization", "categories": ["cs.CV"], "comment": null, "summary": "Existing approaches to drone visual geo-localization predominantly adopt the\nimage-based setting, where a single drone-view snapshot is matched with images\nfrom other platforms. Such task formulation, however, underutilizes the\ninherent video output of the drone and is sensitive to occlusions and viewpoint\ndisparity. To address these limitations, we formulate a new video-based drone\ngeo-localization task and propose the Video2BEV paradigm. This paradigm\ntransforms the video into a Bird's Eye View (BEV), simplifying the subsequent\n\\textbf{inter-platform} matching process. In particular, we employ Gaussian\nSplatting to reconstruct a 3D scene and obtain the BEV projection. Different\nfrom the existing transform methods, \\eg, polar transform, our BEVs preserve\nmore fine-grained details without significant distortion. To facilitate the\ndiscriminative \\textbf{intra-platform} representation learning, our Video2BEV\nparadigm also incorporates a diffusion-based module for generating hard\nnegative samples. To validate our approach, we introduce UniV, a new\nvideo-based geo-localization dataset that extends the image-based\nUniversity-1652 dataset. UniV features flight paths at $30^\\circ$ and\n$45^\\circ$ elevation angles with increased frame rates of up to 10 frames per\nsecond (FPS). Extensive experiments on the UniV dataset show that our Video2BEV\nparadigm achieves competitive recall rates and outperforms conventional\nvideo-based methods. Compared to other competitive methods, our proposed\napproach exhibits robustness at lower elevations with more occlusions.", "AI": {"tldr": "The paper introduces Video2BEV, a new paradigm for drone geo-localization by transforming drone videos into Bird's Eye View (BEV) to improve matching accuracy and robustness.", "motivation": "Existing image-based drone geo-localization methods underutilize video data and struggle with occlusions and viewpoint disparities.", "method": "The Video2BEV paradigm uses Gaussian Splatting for 3D scene reconstruction and BEV projection, along with a diffusion-based module for generating hard negative samples.", "result": "Experiments on the UniV dataset show Video2BEV achieves competitive recall rates and outperforms conventional methods, especially in occluded scenarios.", "conclusion": "Video2BEV offers a robust solution for drone geo-localization by leveraging video data and BEV transformation, validated by the new UniV dataset."}}
{"id": "2505.07859", "pdf": "https://arxiv.org/pdf/2505.07859", "abs": "https://arxiv.org/abs/2505.07859", "authors": ["Daniel Franzen", "Jan Disselhoff", "David Hartmann"], "title": "Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025 camera-ready; 15 pages, 6 figures, 5 tables", "summary": "The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge\nfor large language models (LLMs), exposing limitations in their abstract\nreasoning abilities. In this work, we leverage task-specific data augmentations\nthroughout the training, generation, and scoring phases, and employ a\ndepth-first search algorithm to generate diverse, high-probability candidate\nsolutions. Furthermore, we utilize the LLM not only as a generator but also as\na scorer, using its output probabilities to select the most promising\nsolutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the\npublic ARC-AGI evaluation set, demonstrating state-of-the-art performance among\npublicly available approaches. While concurrent closed-source work has reported\nhigher scores, our method distinguishes itself through its transparency,\nreproducibility, and remarkably low inference cost, averaging only around 2ct\nper task on readily available hardware (we assume a price of 36ct/hour for a\nNvidia 4090 GPU).", "AI": {"tldr": "The paper introduces a method to improve LLMs' performance on the ARC-AGI benchmark using task-specific data augmentation and a depth-first search algorithm, achieving state-of-the-art results with low inference costs.", "motivation": "Address the limitations of LLMs in abstract reasoning, particularly on the challenging ARC-AGI benchmark.", "method": "Leverage task-specific data augmentations and a depth-first search algorithm for solution generation and scoring, using the LLM as both generator and scorer.", "result": "Achieves 71.6% (286.5/400 tasks) on ARC-AGI, with low inference cost (~2ct per task).", "conclusion": "The method offers transparency, reproducibility, and cost-efficiency, distinguishing it from closed-source alternatives."}}
{"id": "2506.07564", "pdf": "https://arxiv.org/pdf/2506.07564", "abs": "https://arxiv.org/abs/2506.07564", "authors": ["Peiran Li", "Xinkai Zou", "Zhuohang Wu", "Ruifeng Li", "Shuo Xing", "Hanwen Zheng", "Zhikai Hu", "Yuping Wang", "Haoxi Li", "Qin Yuan", "Yingmo Zhang", "Zhengzhong Tu"], "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF", "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.", "AI": {"tldr": "SAFEFLOW is a protocol-level framework for secure and reliable LLM/VLM-based agents, enforcing fine-grained information flow control and robust multi-agent coordination, validated by SAFEFLOWBENCH.", "motivation": "Current agent frameworks lack mechanisms for secure information flow, reliability, and multi-agent coordination, necessitating a principled solution.", "method": "SAFEFLOW enforces fine-grained IFC, tracks data provenance, integrity, and confidentiality, and introduces transactional execution, conflict resolution, and secure scheduling.", "result": "Agents built with SAFEFLOW maintain high task performance and security under adversarial conditions, outperforming state-of-the-art.", "conclusion": "SAFEFLOW and SAFEFLOWBENCH advance reliable autonomy by providing a robust and secure framework for agent ecosystems."}}
{"id": "2403.13106", "pdf": "https://arxiv.org/pdf/2403.13106", "abs": "https://arxiv.org/abs/2403.13106", "authors": ["Divyansh Singhvi", "Diganta Misra", "Andrej Erkelens", "Raghav Jain", "Isabel Papadimitriou", "Naomi Saphra"], "title": "Using Shapley interactions to understand how models use structure", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "Published in ACL 2025", "summary": "Language is an intricately structured system, and a key goal of NLP\ninterpretability is to provide methodological insights for understanding how\nlanguage models represent this structure internally. In this paper, we use\nShapley Taylor interaction indices (STII) in order to examine how language and\nspeech models internally relate and structure their inputs. Pairwise Shapley\ninteractions measure how much two inputs work together to influence model\noutputs beyond if we linearly added their independent influences, providing a\nview into how models encode structural interactions between inputs. We relate\nthe interaction patterns in models to three underlying linguistic structures:\nsyntactic structure, non-compositional semantics, and phonetic coarticulation.\nWe find that autoregressive text models encode interactions that correlate with\nthe syntactic proximity of inputs, and that both autoregressive and masked\nmodels encode nonlinear interactions in idiomatic phrases with\nnon-compositional semantics. Our speech results show that inputs are more\nentangled for pairs where a neighboring consonant is likely to influence a\nvowel or approximant, showing that models encode the phonetic interaction\nneeded for extracting discrete phonemic representations.", "AI": {"tldr": "The paper uses Shapley Taylor interaction indices (STII) to analyze how language and speech models internally structure inputs, revealing correlations with syntactic, semantic, and phonetic linguistic structures.", "motivation": "To understand how language models internally represent linguistic structures like syntax, semantics, and phonetics.", "method": "Employed Shapley Taylor interaction indices (STII) to measure pairwise interactions between inputs in models, linking these to linguistic structures.", "result": "Autoregressive models show syntactic proximity correlations, while both autoregressive and masked models encode nonlinear interactions in idiomatic phrases. Speech models reflect phonetic coarticulation.", "conclusion": "Language models encode linguistic structures internally, with interactions aligning with syntax, non-compositional semantics, and phonetic coarticulation."}}
{"id": "2411.18142", "pdf": "https://arxiv.org/pdf/2411.18142", "abs": "https://arxiv.org/abs/2411.18142", "authors": ["Jingming Liu", "Yumeng Li", "Boyuan Xiao", "Yichang Jian", "Ziang Qin", "Tianjia Shao", "Yao-Xiang Ding", "Kun Zhou"], "title": "Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual Conversion in Visual Reasoning for Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Under pure textual modality, Large Language Models (LLMs) have demonstrated\nremarkable success in complex reasoning tasks by decomposing them into simpler\nsub-problems. However, Multimodal Large Language Models (MLLMs) still struggle\nwith some seemingly straightforward visual tasks, such as counting and solving\njigsaw puzzles. We argue that these tasks challenge the ability of\nvisual-to-textual conversion, where MLLMs convert visual information perceived\nfrom the input scene, to textual information for further reasoning and\ngenerating the answer. If the complexity of the visual input is beyond the\nperceptual capability of the MLLMs, without decomposing this conversion\nprocess, simply scaling inference-time reasoning cannot solve the task because\nit repeatedly encounters the same perceptual bottleneck. We propose an\napproach, autonomous imagination, to enable MLLMs to iteratively modify visual\ninputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate\nvisual states, decomposing visual-to-textual conversion into closed-loop visual\nmodification steps. We show that, without any retraining, MLLMs can now solve\ntasks initially beyond their perceptual capability, highlighting that\nclosed-loop visual modification can be an effective way of decomposing the\nvisual reasoning task into solvable substeps. Project page:\nhttps://future-item.github.io/autoimagine-site/", "AI": {"tldr": "MLLMs struggle with visual tasks like counting and puzzles due to perceptual bottlenecks. The proposed 'autonomous imagination' method decomposes visual-to-textual conversion into iterative steps, enabling MLLMs to solve previously challenging tasks without retraining.", "motivation": "MLLMs face difficulties in visual tasks despite their success in textual reasoning. The challenge lies in converting complex visual inputs into textual information for reasoning.", "method": "Introduces 'autonomous imagination,' where MLLMs iteratively modify visual inputs (e.g., isolating objects, rearranging puzzle pieces) to decompose the visual-to-textual conversion process.", "result": "MLLMs can now solve tasks beyond their initial perceptual capability without retraining, demonstrating the effectiveness of closed-loop visual modification.", "conclusion": "Closed-loop visual modification is a viable approach to decompose visual reasoning tasks into manageable substeps, enhancing MLLM performance."}}
{"id": "2505.13990", "pdf": "https://arxiv.org/pdf/2505.13990", "abs": "https://arxiv.org/abs/2505.13990", "authors": ["Tingfeng Hui", "Pengyu Zhu", "Bowen Ping", "Ling Tang", "Guanting Dong", "Yaqi Zhang", "Sen Su"], "title": "DecIF: Improving Instruction-Following through Meta-Decomposition", "categories": ["cs.CL"], "comment": "We release the source code and SFT data in this version", "summary": "Instruction-following has emerged as a crucial capability for large language\nmodels (LLMs). However, existing approaches often rely on pre-existing\ndocuments or external resources to synthesize instruction-following data, which\nlimits their flexibility and generalizability. In this paper, we introduce\nDecIF, a fully autonomous, meta-decomposition guided framework that generates\ndiverse and high-quality instruction-following data using only LLMs. DecIF is\ngrounded in the principle of decomposition. For instruction generation, we\nguide LLMs to iteratively produce various types of meta-information, which are\nthen combined with response constraints to form well-structured and\nsemantically rich instructions. We further utilize LLMs to detect and resolve\npotential inconsistencies within the generated instructions. Regarding response\ngeneration, we decompose each instruction into atomic-level evaluation\ncriteria, enabling rigorous validation and the elimination of inaccurate\ninstruction-response pairs. Extensive experiments across a wide range of\nscenarios and settings demonstrate DecIF's superior performance on\ninstruction-following tasks. Further analysis highlights its strong\nflexibility, scalability, and generalizability in automatically synthesizing\nhigh-quality instruction data.", "AI": {"tldr": "DecIF is a fully autonomous framework that generates diverse, high-quality instruction-following data using LLMs, outperforming existing methods in flexibility and generalizability.", "motivation": "Existing approaches for instruction-following in LLMs rely on pre-existing documents, limiting flexibility and generalizability. DecIF addresses this by autonomously generating data.", "method": "DecIF uses meta-decomposition to guide LLMs in generating instructions and responses. It iteratively produces meta-information, combines it with constraints, and resolves inconsistencies. Responses are validated using atomic-level criteria.", "result": "DecIF demonstrates superior performance in instruction-following tasks, with strong flexibility, scalability, and generalizability in generating high-quality data.", "conclusion": "DecIF is an effective, autonomous solution for synthesizing instruction-following data, enhancing LLM capabilities without external dependencies."}}
{"id": "2506.07736", "pdf": "https://arxiv.org/pdf/2506.07736", "abs": "https://arxiv.org/abs/2506.07736", "authors": ["Jingnan Zheng", "Xiangtian Ji", "Yijun Lu", "Chenhang Cui", "Weixiang Zhao", "Gelei Deng", "Zhenkai Liang", "An Zhang", "Tat-Seng Chua"], "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite\ndeliberate safety alignment efforts, posing significant risks to users and\nsociety. To safeguard against the risk of policy-violating content,\nsystem-level moderation via external guard models-designed to monitor LLM\ninputs and outputs and block potentially harmful content-has emerged as a\nprevalent mitigation strategy. Existing approaches of training guard models\nrely heavily on extensive human curated datasets and struggle with\nout-of-distribution threats, such as emerging harmful categories or jailbreak\nattacks. To address these limitations, we propose RSafe, an adaptive\nreasoning-based safeguard that conducts guided safety reasoning to provide\nrobust protection within the scope of specified safety policies. RSafe operates\nin two stages: 1) guided reasoning, where it analyzes safety risks of input\ncontent through policy-guided step-by-step reasoning, and 2) reinforced\nalignment, where rule-based RL optimizes its reasoning paths to align with\naccurate safety prediction. This two-stage training paradigm enables RSafe to\ninternalize safety principles to generalize safety protection capability over\nunseen or adversarial safety violation scenarios. During inference, RSafe\naccepts user-specified safety policies to provide enhanced safeguards tailored\nto specific safety requirements.", "AI": {"tldr": "RSafe is a reasoning-based safeguard for LLMs, using guided reasoning and reinforced alignment to protect against policy-violating content without relying on extensive human-curated datasets.", "motivation": "Existing guard models for LLMs struggle with out-of-distribution threats and require heavy human curation, prompting the need for a more adaptive solution.", "method": "RSafe uses a two-stage approach: guided reasoning for risk analysis and reinforced alignment via rule-based RL to optimize safety predictions.", "result": "RSafe generalizes safety protection to unseen or adversarial scenarios, adapting to user-specified policies.", "conclusion": "RSafe offers a robust, adaptive safeguard for LLMs, addressing limitations of current guard models."}}
{"id": "2405.20761", "pdf": "https://arxiv.org/pdf/2405.20761", "abs": "https://arxiv.org/abs/2405.20761", "authors": ["Aditya Shankar", "J\u00e9r\u00e9mie Decouchant", "Dimitra Gkorou", "Rihan Hai", "Lydia Y. Chen"], "title": "Share Secrets for Privacy: Confidential Forecasting with Vertical Federated Learning", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": "Accepted at the 20th International Conference on Availability,\n  Reliability and Security (ARES 2025)", "summary": "Vertical federated learning (VFL) is a promising area for time series\nforecasting in many applications, such as healthcare and manufacturing.\nCritical challenges to address include data privacy and over-fitting on small\nand noisy datasets during both training and inference. Additionally, such\nforecasting models must scale well with the number of parties while ensuring\nstrong convergence and low-tuning complexity. We address these challenges and\npropose ``Secret-shared Time Series Forecasting with VFL'' (STV), a novel\nframework with the following key features: i) a privacy-preserving algorithm\nfor forecasting with SARIMAX and autoregressive trees on vertically-partitioned\ndata; ii) decentralised forecasting using secret sharing and multi-party\ncomputation; and iii) novel N-party algorithms for matrix multiplication and\ninverse operations for exact parameter optimization, giving strong convergence\nwith minimal tuning complexity. We evaluate on six representative datasets from\npublic and industry-specific contexts. Results demonstrate that STV's\nforecasting accuracy is comparable to those of centralized approaches. Our\nexact optimization outperforms centralized methods, including state-of-the-art\ndiffusion models and long-short-term memory, by 23.81% on forecasting accuracy.\nWe also evaluate scalability by examining the communication costs of exact and\niterative optimization to navigate the choice between the two. STV's code and\nsupplementary material is available online: https://github.com/adis98/STV.", "AI": {"tldr": "STV is a privacy-preserving VFL framework for time series forecasting, addressing data privacy, over-fitting, and scalability with strong convergence and low-tuning complexity.", "motivation": "Challenges in VFL for time series forecasting include data privacy, over-fitting on small/noisy datasets, and scalability with strong convergence.", "method": "STV uses secret sharing, multi-party computation, and novel N-party algorithms for matrix operations to ensure privacy and exact parameter optimization.", "result": "STV matches centralized forecasting accuracy and outperforms state-of-the-art methods by 23.81% in accuracy. It also evaluates scalability via communication costs.", "conclusion": "STV is an effective, scalable, and privacy-preserving solution for time series forecasting in VFL, with open-source availability."}}
{"id": "2412.09607", "pdf": "https://arxiv.org/pdf/2412.09607", "abs": "https://arxiv.org/abs/2412.09607", "authors": ["Carlos Esteves", "Mohammed Suhail", "Ameesh Makadia"], "title": "Spectral Image Tokenizer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Image tokenizers map images to sequences of discrete tokens, and are a\ncrucial component of autoregressive transformer-based image generation. The\ntokens are typically associated with spatial locations in the input image,\narranged in raster scan order, which is not ideal for autoregressive modeling.\nIn this paper, we propose to tokenize the image spectrum instead, obtained from\na discrete wavelet transform (DWT), such that the sequence of tokens represents\nthe image in a coarse-to-fine fashion. Our tokenizer brings several advantages:\n1) it leverages that natural images are more compressible at high frequencies,\n2) it can take and reconstruct images of different resolutions without\nretraining, 3) it improves the conditioning for next-token prediction --\ninstead of conditioning on a partial line-by-line reconstruction of the image,\nit takes a coarse reconstruction of the full image, 4) it enables partial\ndecoding where the first few generated tokens can reconstruct a coarse version\nof the image, 5) it enables autoregressive models to be used for image\nupsampling. We evaluate the tokenizer reconstruction metrics as well as\nmultiscale image generation, text-guided image upsampling and editing.", "AI": {"tldr": "The paper proposes a wavelet-based image tokenizer for autoregressive image generation, offering advantages like multiscale processing, resolution flexibility, and improved conditioning.", "motivation": "Current image tokenizers use raster scan order, which is suboptimal for autoregressive modeling. The paper aims to improve this by leveraging the image spectrum for better compression and generation.", "method": "The authors use a discrete wavelet transform (DWT) to tokenize images in a coarse-to-fine manner, enabling multiscale processing and resolution-independent reconstruction.", "result": "The proposed tokenizer improves reconstruction metrics, supports multiscale generation, and enables applications like text-guided upsampling and editing.", "conclusion": "Wavelet-based tokenization enhances autoregressive image generation by addressing limitations of raster scan methods, offering practical benefits for diverse tasks."}}
{"id": "2505.16234", "pdf": "https://arxiv.org/pdf/2505.16234", "abs": "https://arxiv.org/abs/2505.16234", "authors": ["Wei Zhang", "Zhenhong Zhou", "Kun Wang", "Junfeng Fang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xavier Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "summary": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress.", "AI": {"tldr": "LIFEBench evaluates LLMs' ability to follow length instructions, revealing most models struggle with longer outputs despite claims, with reasoning LLMs performing best.", "motivation": "Existing benchmarks overlook length constraints in LLM outputs, despite their importance for practical applications.", "method": "Introduces LIFEBench, a benchmark with 10,800 instances across 4 tasks and lengths from 16 to 8192 words, evaluating 26 LLMs.", "result": "Most models perform well with short lengths but fail at longer outputs, even long-context LLMs. Reasoning LLMs outperform others.", "conclusion": "LIFEBench highlights LLMs' limitations in following length instructions, providing insights for future improvements."}}
{"id": "2506.08399", "pdf": "https://arxiv.org/pdf/2506.08399", "abs": "https://arxiv.org/abs/2506.08399", "authors": ["Jiachen Ma", "Zhanhui Zhou", "Chao Yang", "Chaochao Lu"], "title": "SafeCoT: Improving VLM Safety with Minimal Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Ensuring safe and appropriate responses from vision-language models (VLMs)\nremains a critical challenge, particularly in high-risk or ambiguous scenarios.\nWe introduce SafeCoT, a lightweight, interpretable framework that leverages\nrule-based chain-of-thought (CoT) supervision to improve refusal behavior in\nVLMs. Unlike prior methods that rely on large-scale safety annotations or\ncomplex modeling, SafeCoT uses minimal supervision to help models reason about\nsafety risks and make context-aware refusals. Experiments across multiple\nbenchmarks show that SafeCoT significantly reduces overrefusal and enhances\ngeneralization, even with limited training data. Our approach offers a scalable\nsolution for aligning VLMs with safety-critical objectives.", "AI": {"tldr": "SafeCoT improves refusal behavior in VLMs using rule-based CoT supervision, reducing overrefusal and enhancing generalization with minimal training data.", "motivation": "Addressing the challenge of ensuring safe and appropriate responses from VLMs in high-risk or ambiguous scenarios.", "method": "Introduces SafeCoT, a lightweight, interpretable framework leveraging rule-based chain-of-thought supervision for safety reasoning.", "result": "Significantly reduces overrefusal and enhances generalization across benchmarks, even with limited data.", "conclusion": "SafeCoT offers a scalable solution for aligning VLMs with safety-critical objectives."}}
{"id": "2406.12338", "pdf": "https://arxiv.org/pdf/2406.12338", "abs": "https://arxiv.org/abs/2406.12338", "authors": ["Carla Schenker", "Xiulin Wang", "David Horner", "Morten A. Rasmussen", "Evrim Acar"], "title": "PARAFAC2-based Coupled Matrix and Tensor Factorizations with Constraints", "categories": ["cs.LG"], "comment": "15 pages, 15 figures,1 table", "summary": "Data fusion models based on Coupled Matrix and Tensor Factorizations (CMTF)\nhave been effective tools for joint analysis of data from multiple sources.\nWhile the vast majority of CMTF models are based on the strictly multilinear\nCANDECOMP/PARAFAC (CP) tensor model, recently also the more flexible PARAFAC2\nmodel has been integrated into CMTF models. PARAFAC2 tensor models can handle\nirregular/ragged tensors and have shown to be especially useful for modelling\ndynamic data with unaligned or irregular time profiles. However, existing\nPARAFAC2-based CMTF models have limitations in terms of possible\nregularizations on the factors and/or types of coupling between datasets. To\naddress these limitations, in this paper we introduce a flexible algorithmic\nframework that fits PARAFAC2-based CMTF models using Alternating Optimization\n(AO) and the Alternating Direction Method of Multipliers (ADMM). The proposed\nframework allows to impose various constraints on all modes and linear\ncouplings to other matrix-, CP- or PARAFAC2-models. Experiments on various\nsimulated and a real dataset demonstrate the utility and versatility of the\nproposed framework as well as its benefits in terms of accuracy and efficiency\nin comparison with state-of-the-art methods.", "AI": {"tldr": "A flexible algorithmic framework for PARAFAC2-based CMTF models is introduced, enabling various constraints and couplings, improving accuracy and efficiency.", "motivation": "Existing PARAFAC2-based CMTF models have limitations in regularizations and coupling types, prompting the need for a more flexible solution.", "method": "The framework uses Alternating Optimization (AO) and the Alternating Direction Method of Multipliers (ADMM) to fit models with constraints on all modes and couplings.", "result": "Experiments show the framework's utility, versatility, and superior accuracy and efficiency compared to state-of-the-art methods.", "conclusion": "The proposed framework addresses limitations of existing models, offering enhanced flexibility and performance in data fusion tasks."}}
{"id": "2412.19794", "pdf": "https://arxiv.org/pdf/2412.19794", "abs": "https://arxiv.org/abs/2412.19794", "authors": ["Amit Agarwal", "Srikant Panda", "Angeline Charles", "Bhargava Kumar", "Hitesh Patel", "Priyaranjan Pattnayak", "Taki Hasan Rafi", "Tejaswini Kumar", "Hansa Meghwani", "Karan Gupta", "Dong-Kyu Chae"], "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models", "categories": ["cs.CV", "68T37, 68T05, 68Q32, 68T45, 94A08, 68T40, 68Q85", "I.2.10; I.2.7; I.5.4; I.4.9; I.4.8; H.5.1"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs), are recent advancement of\nVision-Language Models (VLMs) that have driven major advances in video\nunderstanding. However, their vulnerability to adversarial tampering and\nmanipulations remains underexplored. To address this gap, we introduce\n\\textbf{MVTamperBench}, a benchmark that systematically evaluates MLLM\nrobustness against five prevalent tampering techniques: rotation, masking,\nsubstitution, repetition, and dropping; based on real-world visual tampering\nscenarios such as surveillance interference, social media content edits, and\nmisinformation injection. MVTamperBench comprises ~3.4K original videos,\nexpanded into over ~17K tampered clips covering 19 distinct video manipulation\ntasks. This benchmark challenges models to detect manipulations in spatial and\ntemporal coherence. We evaluate 45 recent MLLMs from 15+ model families. We\nreveal substantial variability in resilience across tampering types and show\nthat larger parameter counts do not necessarily guarantee robustness.\nMVTamperBench sets a new benchmark for developing tamper-resilient MLLM in\nsafety-critical applications, including detecting clickbait, preventing harmful\ncontent distribution, and enforcing policies on media platforms. We release all\ncode, data, and benchmark to foster open research in trustworthy video\nunderstanding.\n  Code: https://amitbcp.github.io/MVTamperBench/ Data:\nhttps://huggingface.co/datasets/Srikant86/MVTamperBench", "AI": {"tldr": "MVTamperBench is a benchmark evaluating MLLM robustness against five tampering techniques, revealing variability in resilience and challenging the assumption that larger models are inherently more robust.", "motivation": "To address the underexplored vulnerability of MLLMs to adversarial tampering and manipulations in video understanding.", "method": "Introduces MVTamperBench, a benchmark with ~3.4K original videos expanded into ~17K tampered clips, covering 19 video manipulation tasks, and evaluates 45 MLLMs.", "result": "Substantial variability in resilience across tampering types; larger parameter counts do not guarantee robustness.", "conclusion": "MVTamperBench sets a new standard for developing tamper-resilient MLLMs in safety-critical applications, with open code and data for further research."}}
{"id": "2505.17441", "pdf": "https://arxiv.org/pdf/2505.17441", "abs": "https://arxiv.org/abs/2505.17441", "authors": ["Can Rager", "Chris Wendler", "Rohit Gandikota", "David Bau"], "title": "Discovering Forbidden Topics in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Refusal discovery is the task of identifying the full set of topics that a\nlanguage model refuses to discuss. We introduce this new problem setting and\ndevelop a refusal discovery method, Iterated Prefill Crawler (IPC), that uses\ntoken prefilling to find forbidden topics. We benchmark IPC on Tulu-3-8B, an\nopen-source model with public safety tuning data. Our crawler manages to\nretrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale\nthe crawler to a frontier model using the prefilling option of Claude-Haiku.\nFinally, we crawl three widely used open-weight models: Llama-3.3-70B and two\nof its variants finetuned for reasoning: DeepSeek-R1-70B and\nPerplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with\ncensorship tuning: The model exhibits \"thought suppression\" behavior that\nindicates memorization of CCP-aligned responses. Although\nPerplexity-R1-1776-70B is robust to censorship, IPC elicits CCP-aligned\nrefusals answers in the quantized model. Our findings highlight the critical\nneed for refusal discovery methods to detect biases, boundaries, and alignment\nfailures of AI systems.", "AI": {"tldr": "The paper introduces refusal discovery, a task to identify topics a language model refuses to discuss, and proposes Iterated Prefill Crawler (IPC) for this purpose. It benchmarks IPC on various models, revealing censorship patterns and alignment failures.", "motivation": "To uncover biases, boundaries, and alignment issues in AI systems by identifying topics they refuse to discuss.", "method": "Develops IPC, a method using token prefilling to discover forbidden topics, and tests it on multiple models, including Tulu-3-8B, Claude-Haiku, and variants of Llama-3.3-70B.", "result": "IPC retrieves 31 out of 36 topics in Tulu-3-8B, detects censorship tuning in DeepSeek-R1-70B, and elicits CCP-aligned refusals in Perplexity-R1-1776-70B.", "conclusion": "Refusal discovery is crucial for detecting AI biases and alignment failures, as demonstrated by IPC's effectiveness across diverse models."}}
{"id": "2506.08422", "pdf": "https://arxiv.org/pdf/2506.08422", "abs": "https://arxiv.org/abs/2506.08422", "authors": ["Ikkei Itoku", "David Theil", "Evelyn Eichelsdoerfer Uehara", "Sreyoshi Bhaduri", "Junnosuke Kuroda", "Toshi Yumoto", "Alex Gil", "Natalie Perez", "Rajesh Cherukuri", "Naumaan Nayyar"], "title": "Transforming Expert Knowledge into Scalable Ontology via Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Having a unified, coherent taxonomy is essential for effective knowledge\nrepresentation in domain-specific applications as diverse terminologies need to\nbe mapped to underlying concepts. Traditional manual approaches to taxonomy\nalignment rely on expert review of concept pairs, but this becomes\nprohibitively expensive and time-consuming at scale, while subjective\ninterpretations often lead to expert disagreements. Existing automated methods\nfor taxonomy alignment have shown promise but face limitations in handling\nnuanced semantic relationships and maintaining consistency across different\ndomains. These approaches often struggle with context-dependent concept\nmappings and lack transparent reasoning processes. We propose a novel framework\nthat combines large language models (LLMs) with expert calibration and\niterative prompt optimization to automate taxonomy alignment. Our method\nintegrates expert-labeled examples, multi-stage prompt engineering, and human\nvalidation to guide LLMs in generating both taxonomy linkages and supporting\nrationales. In evaluating our framework on a domain-specific mapping task of\nconcept essentiality, we achieved an F1-score of 0.97, substantially exceeding\nthe human benchmark of 0.68. These results demonstrate the effectiveness of our\napproach in scaling taxonomy alignment while maintaining high-quality mappings\nand preserving expert oversight for ambiguous cases.", "AI": {"tldr": "A novel framework combining LLMs with expert calibration and iterative prompt optimization automates taxonomy alignment, achieving high accuracy (F1-score 0.97) and outperforming human benchmarks.", "motivation": "Traditional manual taxonomy alignment is costly and subjective, while automated methods lack nuance and transparency.", "method": "Integrates expert-labeled examples, multi-stage prompt engineering, and human validation to guide LLMs in taxonomy alignment.", "result": "Achieved an F1-score of 0.97, surpassing the human benchmark of 0.68.", "conclusion": "The framework effectively scales taxonomy alignment with high-quality mappings and expert oversight for ambiguous cases."}}
{"id": "2406.19384", "pdf": "https://arxiv.org/pdf/2406.19384", "abs": "https://arxiv.org/abs/2406.19384", "authors": ["Vedang Lad", "Wes Gurnee", "Max Tegmark"], "title": "The Remarkable Robustness of LLMs: Stages of Inference?", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "For Github code see\n  https://github.com/vdlad/Remarkable-Robustness-of-LLMs. Send all\n  correspondence to the first author", "summary": "We investigate the robustness of Large Language Models (LLMs) to structural\ninterventions by deleting and swapping adjacent layers during inference.\nSurprisingly, models retain 72-95% of their original top-1 prediction accuracy\nwithout any fine-tuning. We find that performance degradation is not uniform\nacross layers: interventions to the early and final layers cause the most\ndegradation, while the model is remarkably robust to dropping middle layers.\nThis pattern of localized sensitivity motivates our hypothesis of four stages\nof inference, observed across diverse model families and sizes: (1)\ndetokenization, where local context is integrated to lift raw token embeddings\ninto higher-level representations; (2) feature engineering, where task- and\nentity-specific features are iteratively refined; (3) prediction ensembling,\nwhere hidden states are aggregated into plausible next-token predictions; and\n(4) residual sharpening, where irrelevant features are suppressed to finalize\nthe output distribution. Synthesizing behavioral and mechanistic evidence, we\nprovide a framework for interpreting depth-dependent computations in LLMs.", "AI": {"tldr": "LLMs retain 72-95% accuracy despite layer deletions/swaps, with early/final layers most sensitive. A 4-stage inference framework is proposed.", "motivation": "To understand LLM robustness to structural interventions and uncover depth-dependent computation patterns.", "method": "Delete/swap adjacent layers during inference, measure accuracy drop, and analyze layer sensitivity.", "result": "Models are robust to middle-layer interventions; early/final layers are critical. A 4-stage inference process is identified.", "conclusion": "LLMs exhibit localized sensitivity, motivating a framework for depth-dependent computation interpretation."}}
{"id": "2501.00654", "pdf": "https://arxiv.org/pdf/2501.00654", "abs": "https://arxiv.org/abs/2501.00654", "authors": ["Xindi Wu", "Mengzhou Xia", "Rulin Shao", "Zhiwei Deng", "Pang Wei Koh", "Olga Russakovsky"], "title": "ICONS: Influence Consensus for Vision-Language Data Selection", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "31 pages, 19 figures", "summary": "Training vision-language models via instruction tuning often relies on large\nmixtures of data spanning diverse tasks and domains. However, these mixtures\nfrequently include redundant information, increasing computational costs\nwithout proportional performance gains, necessitating more effective data\nselection strategies. Existing methods typically rely on task-agnostic\nheuristics to estimate data importance or focus on optimizing single tasks in\nisolation, limiting their effectiveness in multitask settings. In this work, we\nintroduce ICONS, a gradient-based Influence CONsensus approach for\nvision-language data Selection. Our method leverages first-order training\ndynamics to estimate the influence of individual training examples on\nvalidation performance and aggregates these estimates across tasks via majority\nvoting over task-specific influences. This cross-task consensus identifies data\npoints that are consistently valuable across tasks, enabling us to prioritize\nexamples that drive overall performance. The voting-based design further\nmitigates issues such as score calibration and outlier sensitivity, resulting\nin robust and scalable data selection for diverse multitask mixtures. With only\n20% of the data from LLaVA-665K and Cambrian-7M, our selected subsets retain\n98.6% and 98.8% of the performance achieved with full datasets, and can even\nsurpass full data training at a 60% selection ratio on LLaVA-665K. Our approach\nalso generalizes to unseen tasks and architectures, demonstrating strong\ntransfer. We release two compact, high-utility subsets, LLaVA-ICONS-133K and\nCambrian-ICONS-1.4M, preserving impactful training examples for efficient and\nscalable vision-language model development.", "AI": {"tldr": "ICONS is a gradient-based method for selecting impactful training data in vision-language models, improving efficiency and performance by prioritizing cross-task valuable examples.", "motivation": "Current data selection methods for vision-language models are inefficient due to redundancy and lack of multitask optimization, leading to unnecessary computational costs.", "method": "ICONS uses gradient-based influence estimation and majority voting across tasks to identify consistently valuable data points.", "result": "Selected subsets (20% of data) retain ~98.6-98.8% performance of full datasets and can outperform full data training at 60% selection.", "conclusion": "ICONS enables efficient, scalable vision-language model training with strong generalization to unseen tasks and architectures."}}
{"id": "2505.20354", "pdf": "https://arxiv.org/pdf/2505.20354", "abs": "https://arxiv.org/abs/2505.20354", "authors": ["Juntong Wu", "Zijing Liu", "He Cao", "Hao Li", "Bin Feng", "Zishan Shu", "Ke Yu", "Li Yuan", "Yu Li"], "title": "Rethinking Text-based Protein Understanding: Retrieval or LLM?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent years, protein-text models have gained significant attention for\ntheir potential in protein generation and understanding. Current approaches\nfocus on integrating protein-related knowledge into large language models\nthrough continued pretraining and multi-modal alignment, enabling simultaneous\ncomprehension of textual descriptions and protein sequences. Through a thorough\nanalysis of existing model architectures and text-based protein understanding\nbenchmarks, we identify significant data leakage issues present in current\nbenchmarks. Moreover, conventional metrics derived from natural language\nprocessing fail to accurately assess the model's performance in this domain. To\naddress these limitations, we reorganize existing datasets and introduce a\nnovel evaluation framework based on biological entities. Motivated by our\nobservation, we propose a retrieval-enhanced method, which significantly\noutperforms fine-tuned LLMs for protein-to-text generation and shows accuracy\nand efficiency in training-free scenarios. Our code and data can be seen at\nhttps://github.com/IDEA-XL/RAPM.", "AI": {"tldr": "The paper addresses data leakage and evaluation issues in protein-text models, proposing a retrieval-enhanced method for improved protein-to-text generation.", "motivation": "To tackle data leakage in benchmarks and inadequate NLP metrics for protein-text models.", "method": "Reorganizes datasets and introduces a biological entity-based evaluation framework, proposing a retrieval-enhanced method.", "result": "The retrieval-enhanced method outperforms fine-tuned LLMs in protein-to-text generation, especially in training-free scenarios.", "conclusion": "The proposed framework and method improve accuracy and efficiency in protein-text understanding and generation."}}
{"id": "2506.08424", "pdf": "https://arxiv.org/pdf/2506.08424", "abs": "https://arxiv.org/abs/2506.08424", "authors": ["Yong Liang Goh", "Zhiguang Cao", "Yining Ma", "Jianan Zhou", "Mohammed Haroon Dupty", "Wee Sun Lee"], "title": "SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy", "categories": ["cs.AI"], "comment": "Accepted in the 42nd International Conference of Machine Learning\n  (ICML)", "summary": "Recent advances toward foundation models for routing problems have shown\ngreat potential of a unified deep model for various VRP variants. However, they\noverlook the complex real-world customer distributions. In this work, we\nadvance the Multi-Task VRP (MTVRP) setting to the more realistic yet\nchallenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce\nSHIELD, a novel model that leverages both sparsity and hierarchy principles.\nBuilding on a deeper decoder architecture, we first incorporate the\nMixture-of-Depths (MoD) technique to enforce sparsity. This improves both\nefficiency and generalization by allowing the model to dynamically select nodes\nto use or skip each decoder layer, providing the needed capacity to adaptively\nallocate computation for learning the task/distribution specific and shared\nrepresentations. We also develop a context-based clustering layer that exploits\nthe presence of hierarchical structures in the problems to produce better local\nrepresentations. These two designs inductively bias the network to identify key\nfeatures that are common across tasks and distributions, leading to\nsignificantly improved generalization on unseen ones. Our empirical results\ndemonstrate the superiority of our approach over existing methods on 9\nreal-world maps with 16 VRP variants each.", "AI": {"tldr": "SHIELD introduces a novel model for Multi-Task Multi-Distribution VRP, leveraging sparsity and hierarchy to improve generalization on unseen tasks and distributions.", "motivation": "Existing foundation models for routing problems overlook complex real-world customer distributions, limiting their practicality.", "method": "SHIELD uses Mixture-of-Depths (MoD) for sparsity and a context-based clustering layer for hierarchy, enhancing task/distribution-specific and shared representations.", "result": "Superior performance on 9 real-world maps with 16 VRP variants each.", "conclusion": "SHIELD's design improves generalization and efficiency, making it a robust solution for realistic VRP challenges."}}
{"id": "2407.00397", "pdf": "https://arxiv.org/pdf/2407.00397", "abs": "https://arxiv.org/abs/2407.00397", "authors": ["Weihan Li", "Yule Wang", "Chengrui Li", "Anqi Wu"], "title": "Learning Time-Varying Multi-Region Brain Communications via Scalable Markovian Gaussian Processes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Understanding and constructing brain communications that capture dynamic\ncommunications across multiple regions is fundamental to modern system\nneuroscience, yet current methods struggle to find time-varying region-level\ncommunications or scale to large neural datasets with long recording durations.\nWe present a novel framework using Markovian Gaussian Processes to learn brain\ncommunications with time-varying temporal delays from multi-region neural\nrecordings, named Adaptive Delay Model (ADM). Our method combines Gaussian\nProcesses with State Space Models and employs parallel scan inference\nalgorithms, enabling efficient scaling to large datasets while identifying\nconcurrent communication patterns that evolve over time. This time-varying\napproach captures how brain region interactions shift dynamically during\ncognitive processes. Validated on synthetic and multi-region neural recordings\ndatasets, our approach discovers both the directionality and temporal dynamics\nof neural communication. This work advances our understanding of distributed\nneural computation and provides a scalable tool for analyzing dynamic brain\nnetworks.", "AI": {"tldr": "A novel framework (ADM) using Markovian Gaussian Processes is introduced to model time-varying brain communications, addressing scalability and dynamic interaction challenges in neural data.", "motivation": "Current methods fail to capture time-varying, region-level brain communications or scale to large neural datasets with long recordings.", "method": "Combines Gaussian Processes with State Space Models and uses parallel scan inference for efficient scaling, identifying evolving communication patterns.", "result": "Validated on synthetic and neural datasets, ADM successfully captures directionality and temporal dynamics of neural interactions.", "conclusion": "ADM advances understanding of dynamic brain networks and offers a scalable tool for analyzing neural communication."}}
{"id": "2501.04606", "pdf": "https://arxiv.org/pdf/2501.04606", "abs": "https://arxiv.org/abs/2501.04606", "authors": ["Yangfan He", "Sida Li", "Jianhui Wang", "Kun Li", "Xinyuan Song", "Xinhang Yuan", "Keqin Li", "Kuan Lu", "Menghao Huo", "Jingqun Tang", "Yi Xin", "Jiaqi Chen", "Miao Zhang", "Xueqian Wang"], "title": "Enhancing Low-Cost Video Editing with Lightweight Adaptors and Temporal-Aware Inversion", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-to-image (T2I) generation using diffusion models\nhave enabled cost-effective video-editing applications by leveraging\npre-trained models, eliminating the need for resource-intensive training.\nHowever, the frame-independence of T2I generation often results in poor\ntemporal consistency. Existing methods address this issue through temporal\nlayer fine-tuning or inference-based temporal propagation, but these approaches\nsuffer from high training costs or limited temporal coherence. To address these\nchallenges, we propose a General and Efficient Adapter (GE-Adapter) that\nintegrates temporal-spatial and semantic consistency with Baliteral DDIM\ninversion. This framework introduces three key components: (1) Frame-based\nTemporal Consistency Blocks (FTC Blocks) to capture frame-specific features and\nenforce smooth inter-frame transitions via temporally-aware loss functions; (2)\nChannel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral\nfilters to enhance spatial coherence by reducing noise and artifacts; and (3)\nToken-based Semantic Consistency Module (TSC Module) to maintain semantic\nalignment using shared prompt tokens and frame-specific tokens. Our method\nsignificantly improves perceptual quality, text-image alignment, and temporal\ncoherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves\nenhanced fidelity and frame-to-frame coherence, offering a practical solution\nfor T2V editing.", "AI": {"tldr": "The paper proposes GE-Adapter, a method to improve temporal and spatial consistency in text-to-video editing using pre-trained diffusion models, addressing issues like high training costs and poor coherence.", "motivation": "Existing text-to-image (T2I) generation methods for video editing suffer from poor temporal consistency, high training costs, or limited coherence. The goal is to enhance consistency and efficiency without resource-intensive training.", "method": "GE-Adapter integrates temporal-spatial and semantic consistency with Baliteral DDIM inversion. It includes FTC Blocks for temporal consistency, SCD Blocks for spatial coherence, and a TSC Module for semantic alignment.", "result": "The method improves perceptual quality, text-image alignment, and temporal coherence on the MSR-VTT dataset, achieving better fidelity and frame-to-frame coherence.", "conclusion": "GE-Adapter offers a practical and efficient solution for text-to-video editing, enhancing consistency and reducing resource demands."}}
{"id": "2505.24593", "pdf": "https://arxiv.org/pdf/2505.24593", "abs": "https://arxiv.org/abs/2505.24593", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Peijie Jiang", "Jia Liu", "Xuming Hu"], "title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025", "summary": "The interpretability of Mixture-of-Experts (MoE) models, especially those\nwith heterogeneous designs, remains underexplored. Existing attribution methods\nfor dense models fail to capture dynamic routing-expert interactions in sparse\nMoE architectures. To address this issue, we propose a cross-level attribution\nalgorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,\nMixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mistral-7B). Results\nshow MoE models achieve 37% higher per-layer efficiency via a \"mid-activation,\nlate-amplification\" pattern: early layers screen experts, while late layers\nrefine knowledge collaboratively. Ablation studies reveal a \"basic-refinement\"\nframework--shared experts handle general tasks (entity recognition), while\nrouted experts specialize in domain-specific processing (geographic\nattributes). Semantic-driven routing is evidenced by strong correlations\nbetween attention heads and experts (r=0.68), enabling task-aware coordination.\nNotably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates\nexpert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10\nexperts) through shared expert redundancy, whereas shallow OLMoE suffers severe\ndegradation (76% drop). Task sensitivity further guides design: core-sensitive\ntasks (geography) require concentrated expertise, while distributed-tolerant\ntasks (object attributes) leverage broader participation. These insights\nadvance MoE interpretability, offering principles to balance efficiency,\nspecialization, and robustness.", "AI": {"tldr": "The paper proposes a cross-level attribution algorithm to analyze sparse Mixture-of-Experts (MoE) models, revealing efficiency patterns, expert roles, and robustness insights.", "motivation": "Interpretability of heterogeneous MoE models is underexplored, and existing methods fail to capture dynamic routing-expert interactions.", "method": "A cross-level attribution algorithm is applied to sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) and compared to dense models.", "result": "MoE models show 37% higher efficiency via a 'mid-activation, late-amplification' pattern, with shared and routed experts handling general and specialized tasks, respectively. Robustness varies with depth.", "conclusion": "The study advances MoE interpretability, providing design principles for balancing efficiency, specialization, and robustness."}}
{"id": "1909.03820", "pdf": "https://arxiv.org/pdf/1909.03820", "abs": "https://arxiv.org/abs/1909.03820", "authors": ["Steffen van Bergerem"], "title": "Learning Concepts Definable in First-Order Logic with Counting", "categories": ["cs.LO", "cs.AI", "cs.LG"], "comment": null, "summary": "We study Boolean classification problems over relational background\nstructures in the logical framework introduced by Grohe and Tur\\'an (TOCS\n2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in\nfirst-order logic over structures of polylogarithmic degree can be learned in\nsublinear time, where the degree of the structure and the running time are\nmeasured in terms of the size of the structure. We generalise the results to\nthe first-order logic with counting FOCN, which was introduced by Kuske and\nSchweikardt (LICS 2017) as an expressive logic generalising various other\ncounting logics. Specifically, we prove that classifiers definable in FOCN over\nclasses of structures of polylogarithmic degree can be consistently learned in\nsublinear time. This can be seen as a first step towards extending the learning\nframework to include numerical aspects of machine learning. We extend the\nresult to agnostic probably approximately correct (PAC) learning for classes of\nstructures of degree at most $(\\log \\log n)^c$ for some constant $c$. Moreover,\nwe show that bounding the degree is crucial to obtain sublinear-time learning\nalgorithms. That is, we prove that, for structures of unbounded degree,\nlearning is not possible in sublinear time, even for classifiers definable in\nplain first-order logic.", "AI": {"tldr": "The paper extends sublinear-time learning to classifiers definable in first-order logic with counting (FOCN) over structures of polylogarithmic degree, showing it's impossible for unbounded-degree structures.", "motivation": "To generalize learning frameworks to include numerical aspects by extending results to FOCN, a more expressive logic.", "method": "Proves sublinear-time learning for FOCN classifiers over polylogarithmic-degree structures and shows unbounded-degree structures prevent this.", "result": "Classifiers in FOCN over polylogarithmic-degree structures can be learned in sublinear time; unbounded-degree structures make this impossible.", "conclusion": "Bounding degree is essential for sublinear-time learning, and FOCN extends the framework to numerical aspects."}}
{"id": "2407.01250", "pdf": "https://arxiv.org/pdf/2407.01250", "abs": "https://arxiv.org/abs/2407.01250", "authors": ["Yang Pan", "Clemens Hutter", "Helmut B\u00f6lcskei"], "title": "Metric-Entropy Limits on the Approximation of Nonlinear Dynamical Systems", "categories": ["cs.LG", "cs.IT", "math.DS", "math.IT"], "comment": null, "summary": "This paper is concerned with fundamental limits on the approximation of\nnonlinear dynamical systems. Specifically, we show that recurrent neural\nnetworks (RNNs) can approximate nonlinear systems -- that satisfy a Lipschitz\nproperty and forget past inputs fast enough -- in metric-entropy-optimal\nmanner. As the sets of sequence-to-sequence mappings realized by the dynamical\nsystems we consider are significantly more massive than function classes\ngenerally analyzed in approximation theory, a refined metric-entropy\ncharacterization is needed, namely in terms of order, type, and generalized\ndimension. We compute these quantities for the classes of exponentially- and\npolynomially Lipschitz fading-memory systems and show that RNNs can achieve\nthem.", "AI": {"tldr": "RNNs can optimally approximate nonlinear dynamical systems with Lipschitz properties and fast input forgetting, using refined metric-entropy analysis.", "motivation": "To establish fundamental limits on approximating nonlinear dynamical systems, focusing on RNNs' capabilities.", "method": "Refined metric-entropy characterization (order, type, generalized dimension) for exponentially/polynomially Lipschitz fading-memory systems.", "result": "RNNs achieve metric-entropy-optimal approximation for these systems.", "conclusion": "RNNs are effective for approximating complex nonlinear dynamical systems under specific conditions."}}
{"id": "2501.08279", "pdf": "https://arxiv.org/pdf/2501.08279", "abs": "https://arxiv.org/abs/2501.08279", "authors": ["Longtao Jiang", "Zhendong Wang", "Jianmin Bao", "Wengang Zhou", "Dongdong Chen", "Lei Shi", "Dong Chen", "Houqiang Li"], "title": "SmartEraser: Remove Anything from Images using Masked-Region Guidance", "categories": ["cs.CV"], "comment": "Project at: https://longtaojiang.github.io/smarteraser.github.io/", "summary": "Object removal has so far been dominated by the mask-and-inpaint paradigm,\nwhere the masked region is excluded from the input, leaving models relying on\nunmasked areas to inpaint the missing region. However, this approach lacks\ncontextual information for the masked area, often resulting in unstable\nperformance. In this work, we introduce SmartEraser, built with a new removing\nparadigm called Masked-Region Guidance. This paradigm retains the masked region\nin the input, using it as guidance for the removal process. It offers several\ndistinct advantages: (a) it guides the model to accurately identify the object\nto be removed, preventing its regeneration in the output; (b) since the user\nmask often extends beyond the object itself, it aids in preserving the\nsurrounding context in the final result. Leveraging this new paradigm, we\npresent Syn4Removal, a large-scale object removal dataset, where instance\nsegmentation data is used to copy and paste objects onto images as removal\ntargets, with the original images serving as ground truths. Experimental\nresults demonstrate that SmartEraser significantly outperforms existing\nmethods, achieving superior performance in object removal, especially in\ncomplex scenes with intricate compositions.", "AI": {"tldr": "SmartEraser introduces Masked-Region Guidance for object removal, outperforming traditional mask-and-inpaint methods by leveraging masked regions as guidance.", "motivation": "The mask-and-inpaint paradigm lacks contextual information for masked areas, leading to unstable performance. SmartEraser addresses this by retaining masked regions as guidance.", "method": "SmartEraser uses Masked-Region Guidance, retaining masked regions in the input to guide removal. It also introduces Syn4Removal, a dataset for training and evaluation.", "result": "SmartEraser significantly outperforms existing methods, especially in complex scenes, by accurately identifying and removing objects while preserving context.", "conclusion": "The Masked-Region Guidance paradigm and SmartEraser offer a robust solution for object removal, improving accuracy and context preservation."}}
{"id": "2506.00103", "pdf": "https://arxiv.org/pdf/2506.00103", "abs": "https://arxiv.org/abs/2506.00103", "authors": ["Ruipeng Jia", "Yunyi Yang", "Yongbo Gai", "Kai Luo", "Shihao Huang", "Jianhe Lin", "Xiaoxi Jiang", "Guanjun Jiang"], "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks.", "AI": {"tldr": "The paper introduces RLVR-based training for non-verifiable tasks like creative writing, using a pairwise Generative Reward Model (GenRM) and Bootstrapped Relative Policy Optimization (BRPO) to avoid reward hacking and improve generalization.", "motivation": "Address the gap in reinforcement learning for non-verifiable tasks (e.g., creative writing) where subjective quality assessment lacks definitive references, overcoming limitations of scalar reward models.", "method": "Proposes a writing-principle-based pairwise GenRM and BRPO algorithm to transform subjective assessments into verifiable rewards and enable dynamic, reference-free comparisons during RL training.", "result": "Demonstrates consistent improvement and resistance to reward hacking in Writing-Zero, achieving competitive results on writing benchmarks.", "conclusion": "Suggests unifying rule-based, reference-based, and reference-free reward modeling under RLVR for a scalable RL training paradigm applicable to all language tasks."}}
{"id": "2312.11836", "pdf": "https://arxiv.org/pdf/2312.11836", "abs": "https://arxiv.org/abs/2312.11836", "authors": ["Zihao Xuan", "Yuxuan Yang", "Wei Xuan", "Zijia Su", "Song Chen", "Yi Kang"], "title": "YOCO: A Hybrid In-Memory Computing Architecture with 8-bit Sub-PetaOps/W In-Situ Multiply Arithmetic for Large-Scale AI", "categories": ["cs.AR", "cs.AI"], "comment": "6 pages, 10 figures, Design Automatic Conference 2025", "summary": "In this paper, we further explore the potential of analog in-memory computing\n(AiMC) and introduce an innovative artificial intelligence (AI) accelerator\narchitecture named YOCO, featuring three key proposals: (1) YOCO proposes a\nnovel 8-bit in-situ multiply arithmetic (IMA) achieving 123.8 TOPS/W\nenergy-efficiency and 34.9 TOPS throughput through efficient charge-domain\ncomputation and timedomain accumulation mechanism. (2) YOCO employs a hybrid\nReRAM-SRAM memory structure to balance computational efficiency and storage\ndensity. (3) YOCO tailors an IMC-friendly attention computing flow with an\nefficient pipeline to accelerate the inference of transformer-based AI models.\nCompared to three SOTA baselines, YOCO on average improves energy efficiency by\nup to 3.9x-19.9x and throughput by up to 6.8x-33.6x across 10 CNN/transformer\nmodels.", "AI": {"tldr": "YOCO is an AI accelerator architecture using analog in-memory computing, achieving high energy efficiency and throughput with innovative 8-bit arithmetic, hybrid memory, and optimized attention flow.", "motivation": "To enhance the potential of analog in-memory computing (AiMC) for AI acceleration, addressing efficiency and throughput challenges.", "method": "YOCO introduces three key innovations: 8-bit in-situ multiply arithmetic, a hybrid ReRAM-SRAM memory structure, and an IMC-friendly attention computing flow.", "result": "YOCO improves energy efficiency by 3.9x-19.9x and throughput by 6.8x-33.6x over SOTA baselines across 10 CNN/transformer models.", "conclusion": "YOCO demonstrates significant advancements in AI acceleration, offering superior efficiency and performance for transformer-based models."}}
{"id": "2407.16239", "pdf": "https://arxiv.org/pdf/2407.16239", "abs": "https://arxiv.org/abs/2407.16239", "authors": ["Ahmet Zahid Balc\u0131o\u011flu", "Newton Mwai", "Emil Carlsson", "Fredrik D. Johansson"], "title": "Identifiable Latent Bandits: Leveraging observational data for personalized decision-making", "categories": ["cs.LG", "stat.ML"], "comment": "30 pages, 16 figures", "summary": "For many decision-making tasks, such as precision medicine, historical data\nalone are insufficient to determine the right choice for a new problem instance\nor patient. Online algorithms like multi-armed bandits can find optimal\npersonalized decisions but are notoriously sample-hungry. In practice, training\na bandit for a new individual from scratch is often infeasible, as the number\nof trials required is larger than the practical number of decision points.\nLatent bandits offer rapid exploration and personalization beyond what context\nvariables can reveal, provided that a latent variable model can be learned\nconsistently. In this work, we propose an identifiable latent bandit framework\nthat leads to optimal decision-making with a shorter exploration time than\nclassical bandits by learning from historical records of decisions and\noutcomes. Our method is based on nonlinear independent component analysis that\nprovably identifies representations from observational data sufficient to infer\nthe optimal action in new bandit instances. We verify this strategy in\nsimulated and semi-synthetic environments, showing substantial improvement over\nonline and offline learning baselines when identifying conditions are\nsatisfied.", "AI": {"tldr": "The paper introduces an identifiable latent bandit framework for faster personalized decision-making by leveraging historical data and nonlinear independent component analysis.", "motivation": "Historical data alone often fail to provide optimal decisions for new instances, and traditional bandit algorithms require too many trials for practical use.", "method": "The proposed method uses nonlinear independent component analysis to learn identifiable latent representations from historical data, enabling faster exploration and personalization.", "result": "The framework significantly reduces exploration time compared to classical bandits and outperforms online and offline baselines in simulated and semi-synthetic environments.", "conclusion": "The identifiable latent bandit framework offers a practical solution for rapid, personalized decision-making when identification conditions are met."}}
{"id": "2501.10935", "pdf": "https://arxiv.org/pdf/2501.10935", "abs": "https://arxiv.org/abs/2501.10935", "authors": ["Shuai Lyu", "Zijing Tian", "Zhonghong Ou", "Yifan Zhu", "Xiao Zhang", "Qiankun Ha", "Haoran Luo", "Meina Song"], "title": "TSVC:Tripartite Learning with Semantic Variation Consistency for Robust Image-Text Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted to the Main Track of AAAI 2025. It\n  contains 9 pages, 7 figures, and is relevant to the areas of cross-modal\n  retrieval and machine learning. The work presents a novel approach in robust\n  image-text retrieval using a tripartite learning framework", "summary": "Cross-modal retrieval maps data under different modality via semantic\nrelevance. Existing approaches implicitly assume that data pairs are\nwell-aligned and ignore the widely existing annotation noise, i.e., noisy\ncorrespondence (NC). Consequently, it inevitably causes performance\ndegradation. Despite attempts that employ the co-teaching paradigm with\nidentical architectures to provide distinct data perspectives, the differences\nbetween these architectures are primarily stemmed from random initialization.\nThus, the model becomes increasingly homogeneous along with the training\nprocess. Consequently, the additional information brought by this paradigm is\nseverely limited. In order to resolve this problem, we introduce a Tripartite\nlearning with Semantic Variation Consistency (TSVC) for robust image-text\nretrieval. We design a tripartite cooperative learning mechanism comprising a\nCoordinator, a Master, and an Assistant model. The Coordinator distributes\ndata, and the Assistant model supports the Master model's noisy label\nprediction with diverse data. Moreover, we introduce a soft label estimation\nmethod based on mutual information variation, which quantifies the noise in new\nsamples and assigns corresponding soft labels. We also present a new loss\nfunction to enhance robustness and optimize training effectiveness. Extensive\nexperiments on three widely used datasets demonstrate that, even at increasing\nnoise ratios, TSVC exhibits significant advantages in retrieval accuracy and\nmaintains stable training performance.", "AI": {"tldr": "TSVC introduces a tripartite learning mechanism for robust image-text retrieval, addressing noisy correspondence (NC) with a Coordinator, Master, and Assistant model, and a soft label estimation method.", "motivation": "Existing cross-modal retrieval methods assume well-aligned data pairs, ignoring noisy correspondence (NC), leading to performance degradation. Homogeneous models in co-teaching paradigms limit additional information.", "method": "TSVC uses a tripartite cooperative learning mechanism (Coordinator, Master, Assistant) and a soft label estimation method based on mutual information variation. A new loss function enhances robustness.", "result": "Experiments on three datasets show TSVC's superior retrieval accuracy and stable training performance, even with increasing noise ratios.", "conclusion": "TSVC effectively addresses NC in cross-modal retrieval, improving robustness and performance through diverse model collaboration and soft label estimation."}}
{"id": "2506.01687", "pdf": "https://arxiv.org/pdf/2506.01687", "abs": "https://arxiv.org/abs/2506.01687", "authors": ["Anya Sims", "Thom Foster", "Klara Kaleb", "Tuan-Duy H. Nguyen", "Joseph Lee", "Jakob N. Foerster", "Yee Whye Teh", "Cong Lu"], "title": "StochasTok: Improving Fine-Grained Subword Understanding in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Subword-level understanding is integral to numerous tasks, including\nunderstanding multi-digit numbers, spelling mistakes, abbreviations, rhyming,\nand wordplay. Despite this, current large language models (LLMs) still often\nstruggle with seemingly simple subword-level tasks like How many 'r's in\n'strawberry'?. A key factor behind these failures is tokenization which\nobscures the fine-grained structure of words. Current alternatives, such as\ncharacter-level and dropout tokenization methods, significantly increase\ncomputational costs and provide inconsistent improvements. In this paper we\nrevisit tokenization and introduce StochasTok, a simple, efficient stochastic\ntokenization scheme that randomly splits tokens during training, allowing LLMs\nto 'see' their internal structure. Our experiments show that pretraining with\nStochasTok substantially improves LLMs' downstream performance across multiple\nsubword-level language games, including character counting, substring\nidentification, and math tasks. Furthermore, StochasTok's simplicity allows\nseamless integration at any stage of the training pipeline; and we demonstrate\nthat post-training with StochasTok can instill improved subword understanding\ninto existing pretrained models, thus avoiding costly pretraining from scratch.\nThese dramatic improvements achieved with a minimal change suggest StochasTok\nholds exciting potential when applied to larger, more capable models. Code\nopen-sourced at: https://github.com/anyasims/stochastok.", "AI": {"tldr": "StochasTok is a stochastic tokenization method that improves subword-level understanding in LLMs without high computational costs.", "motivation": "Current LLMs struggle with subword-level tasks due to tokenization obscuring word structure, and existing solutions are costly or inconsistent.", "method": "Introduces StochasTok, a stochastic tokenization scheme that randomly splits tokens during training to expose internal word structure.", "result": "Pretraining with StochasTok enhances LLM performance on subword-level tasks like character counting and substring identification. Post-training also improves existing models.", "conclusion": "StochasTok offers a simple, efficient solution for better subword understanding in LLMs, with potential for broader applications."}}
{"id": "2402.08144", "pdf": "https://arxiv.org/pdf/2402.08144", "abs": "https://arxiv.org/abs/2402.08144", "authors": ["Joshua Kavner", "Lirong Xia"], "title": "Average-Case Analysis of Iterative Voting", "categories": ["cs.GT", "cs.AI"], "comment": "137 pages", "summary": "Iterative voting is a natural model of repeated strategic decision-making in\nsocial choice theory when agents have the opportunity to update their votes\nprior to finalizing the group decision. Prior work has analyzed the efficacy of\niterative plurality on the welfare of the chosen outcome at equilibrium,\nrelative to the truthful vote profile, via an adaptation of the price of\nanarchy. However, prior analyses have only studied the worst- and average-case\nperformances when agents' preferences are distributed by the impartial culture.\nThis work extends average-case analysis comprehensively across three\nalternatives and distinguishes under which of agents' preference distributions\niterative plurality improves or degrades asymptotic welfare.", "AI": {"tldr": "The paper extends average-case analysis of iterative plurality voting across three alternatives, identifying conditions under which it improves or degrades welfare.", "motivation": "To understand the impact of iterative voting on welfare under various preference distributions, beyond the impartial culture assumption.", "method": "Analyzes iterative plurality voting with three alternatives, comparing welfare outcomes under different preference distributions.", "result": "Identifies specific preference distributions where iterative plurality voting improves or degrades asymptotic welfare.", "conclusion": "The study provides a nuanced understanding of iterative voting's welfare effects, highlighting its dependency on preference distributions."}}
{"id": "2408.08979", "pdf": "https://arxiv.org/pdf/2408.08979", "abs": "https://arxiv.org/abs/2408.08979", "authors": ["Minheng Xiao"], "title": "Electroencephalogram Emotion Recognition via AUC Maximization", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Imbalanced datasets pose significant challenges in areas including\nneuroscience, cognitive science, and medical diagnostics, where accurately\ndetecting minority classes is essential for robust model performance. This\nstudy addresses the issue of class imbalance, using the `Liking' label in the\nDEAP dataset as an example. Such imbalances are often overlooked by prior\nresearch, which typically focuses on the more balanced arousal and valence\nlabels and predominantly uses accuracy metrics to measure model performance. To\ntackle this issue, we adopt numerical optimization techniques aimed at\nmaximizing the area under the curve (AUC), thus enhancing the detection of\nunderrepresented classes. Our approach, which begins with a linear classifier,\nis compared against traditional linear classifiers, including logistic\nregression and support vector machines (SVM). Our method significantly\noutperforms these models, increasing recall from 41.6\\% to 79.7\\% and improving\nthe F1-score from 0.506 to 0.632. These results highlight the efficacy of AUC\nmaximization via numerical optimization in managing imbalanced datasets,\nproviding an effective solution for enhancing predictive accuracy in detecting\nminority but crucial classes in out-of-sample datasets.", "AI": {"tldr": "The paper addresses class imbalance in datasets, focusing on the 'Liking' label in the DEAP dataset. It proposes AUC maximization via numerical optimization, outperforming traditional classifiers like logistic regression and SVM.", "motivation": "Class imbalance in datasets, especially in neuroscience and medical diagnostics, is often overlooked, leading to poor detection of minority classes.", "method": "The study uses numerical optimization to maximize AUC, starting with a linear classifier, and compares it to traditional methods like logistic regression and SVM.", "result": "The proposed method significantly improves recall (41.6% to 79.7%) and F1-score (0.506 to 0.632).", "conclusion": "AUC maximization via numerical optimization is effective for handling imbalanced datasets, enhancing minority class detection."}}
{"id": "2501.16583", "pdf": "https://arxiv.org/pdf/2501.16583", "abs": "https://arxiv.org/abs/2501.16583", "authors": ["Long Peng", "Xin Di", "Zhanfeng Feng", "Wenbo Li", "Renjing Pei", "Yang Wang", "Xueyang Fu", "Yang Cao", "Zheng-Jun Zha"], "title": "Directing Mamba to Complex Textures: An Efficient Texture-Aware State Space Model for Image Restoration", "categories": ["cs.CV"], "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "Image restoration aims to recover details and enhance contrast in degraded\nimages. With the growing demand for high-quality imaging (\\textit{e.g.}, 4K and\n8K), achieving a balance between restoration quality and computational\nefficiency has become increasingly critical. Existing methods, primarily based\non CNNs, Transformers, or their hybrid approaches, apply uniform deep\nrepresentation extraction across the image. However, these methods often\nstruggle to effectively model long-range dependencies and largely overlook the\nspatial characteristics of image degradation (regions with richer textures tend\nto suffer more severe damage), making it hard to achieve the best trade-off\nbetween restoration quality and efficiency. To address these issues, we propose\na novel texture-aware image restoration method, TAMambaIR, which simultaneously\nperceives image textures and achieves a trade-off between performance and\nefficiency. Specifically, we introduce a novel Texture-Aware State Space Model,\nwhich enhances texture awareness and improves efficiency by modulating the\ntransition matrix of the state-space equation and focusing on regions with\ncomplex textures. Additionally, we design a {Multi-Directional Perception\nBlock} to improve multi-directional receptive fields while maintaining low\ncomputational overhead. Extensive experiments on benchmarks for image\nsuper-resolution, deraining, and low-light image enhancement demonstrate that\nTAMambaIR achieves state-of-the-art performance with significantly improved\nefficiency, establishing it as a robust and efficient framework for image\nrestoration.", "AI": {"tldr": "TAMambaIR is a texture-aware image restoration method that balances quality and efficiency by focusing on complex textures and using a novel state-space model.", "motivation": "Existing methods struggle with long-range dependencies and ignore spatial degradation characteristics, leading to suboptimal trade-offs between quality and efficiency.", "method": "Proposes a Texture-Aware State Space Model and a Multi-Directional Perception Block to enhance texture awareness and efficiency.", "result": "Achieves state-of-the-art performance in super-resolution, deraining, and low-light enhancement with improved efficiency.", "conclusion": "TAMambaIR is a robust and efficient framework for image restoration, addressing key limitations of current methods."}}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404", "abs": "https://arxiv.org/abs/2506.02404", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qian-wen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.", "AI": {"tldr": "GraphRAG-Bench is introduced to rigorously evaluate GraphRAG models with challenging, domain-specific questions, diverse tasks, and a holistic evaluation framework, revealing insights into graph architectures and reasoning improvements.", "motivation": "Current evaluations of GraphRAG models are limited in scope and fail to assess reasoning capacity comprehensively, necessitating a more rigorous benchmark.", "method": "GraphRAG-Bench features college-level, domain-specific questions requiring multi-hop reasoning, diverse task types, and a holistic evaluation framework covering the entire GraphRAG pipeline.", "result": "Application of nine GraphRAG methods to GraphRAG-Bench quantifies reasoning improvements and provides insights into graph architectures and retrieval efficacy.", "conclusion": "GraphRAG-Bench offers a comprehensive evaluation tool for GraphRAG models, highlighting the impact of graph-based structuring on reasoning capabilities and guiding future research."}}
{"id": "2402.08640", "pdf": "https://arxiv.org/pdf/2402.08640", "abs": "https://arxiv.org/abs/2402.08640", "authors": ["Xuemei Gu", "Mario Krenn"], "title": "Forecasting high-impact research topics via machine learning on evolving knowledge graphs", "categories": ["cs.DL", "cs.AI", "cs.LG"], "comment": "15 pages, 12 figures, Comments welcome!", "summary": "The exponential growth in scientific publications poses a severe challenge\nfor human researchers. It forces attention to more narrow sub-fields, which\nmakes it challenging to discover new impactful research ideas and\ncollaborations outside one's own field. While there are ways to predict a\nscientific paper's future citation counts, they need the research to be\nfinished and the paper written, usually assessing impact long after the idea\nwas conceived. Here we show how to predict the impact of onsets of ideas that\nhave never been published by researchers. For that, we developed a large\nevolving knowledge graph built from more than 21 million scientific papers. It\ncombines a semantic network created from the content of the papers and an\nimpact network created from the historic citations of papers. Using machine\nlearning, we can predict the dynamic of the evolving network into the future\nwith high accuracy (AUC values beyond 0.9 for most experiments), and thereby\nthe impact of new research directions. We envision that the ability to predict\nthe impact of new ideas will be a crucial component of future artificial muses\nthat can inspire new impactful and interesting scientific ideas.", "AI": {"tldr": "Predicting the impact of unpublished research ideas using a knowledge graph from 21M papers, achieving high accuracy (AUC >0.9).", "motivation": "Addressing the challenge of discovering impactful research ideas across narrow sub-fields due to exponential publication growth.", "method": "Developed a large evolving knowledge graph combining semantic and impact networks from papers, using ML to predict future impact.", "result": "High accuracy in predicting impact dynamics (AUC >0.9), enabling foresight into new research directions.", "conclusion": "Predicting idea impact could inspire future AI tools for generating impactful scientific ideas."}}
{"id": "2409.06694", "pdf": "https://arxiv.org/pdf/2409.06694", "abs": "https://arxiv.org/abs/2409.06694", "authors": ["Taslim Murad", "Prakash Chourasia", "Sarwan Ali", "Imdad Ullah Khan", "Murray Patterson"], "title": "DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos Enhanced Kaleidoscopic Images", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Cancer is a complex disease characterized by uncontrolled cell growth. T cell\nreceptors (TCRs), crucial proteins in the immune system, play a key role in\nrecognizing antigens, including those associated with cancer. Recent\nadvancements in sequencing technologies have facilitated comprehensive\nprofiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity\nand enabling TCR-based immunotherapies. However, analyzing these intricate\nbiomolecules necessitates efficient representations that capture their\nstructural and functional information. T-cell protein sequences pose unique\nchallenges due to their relatively smaller lengths compared to other\nbiomolecules. An image-based representation approach becomes a preferred choice\nfor efficient embeddings, allowing for the preservation of essential details\nand enabling comprehensive analysis of T-cell protein sequences. In this paper,\nwe propose to generate images from the protein sequences using the idea of\nChaos Game Representation (CGR) using the Kaleidoscopic images approach. This\nDeep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced\nKaleidoscopic Images (called DANCE) provides a unique way to visualize protein\nsequences by recursively applying chaos game rules around a central seed point.\nwe perform the classification of the T cell receptors (TCRs) protein sequences\nin terms of their respective target cancer cells, as TCRs are known for their\nimmune response against cancer disease. The TCR sequences are converted into\nimages using the DANCE method. We employ deep-learning vision models to perform\nthe classification to obtain insights into the relationship between the visual\npatterns observed in the generated kaleidoscopic images and the underlying\nprotein properties. By combining CGR-based image generation with deep learning\nclassification, this study opens novel possibilities in the protein analysis\ndomain.", "AI": {"tldr": "The paper proposes DANCE, a method using Chaos Game Representation (CGR) and kaleidoscopic images to visualize T-cell receptor (TCR) protein sequences for deep learning-based classification of cancer-targeting TCRs.", "motivation": "TCRs play a key role in cancer immunity, but their analysis is challenging due to their short lengths. Efficient representations are needed to capture their structural and functional details.", "method": "The DANCE method converts TCR protein sequences into images using CGR and kaleidoscopic techniques, then employs deep learning models for classification based on cancer cell targets.", "result": "The study demonstrates the feasibility of using image-based representations and deep learning to classify TCRs, revealing insights into protein properties through visual patterns.", "conclusion": "Combining CGR-based image generation with deep learning offers novel possibilities for protein sequence analysis, particularly in TCR research for cancer immunotherapy."}}
{"id": "2502.06034", "pdf": "https://arxiv.org/pdf/2502.06034", "abs": "https://arxiv.org/abs/2502.06034", "authors": ["Mozes Jacobs", "Roberto C. Budzinski", "Lyle Muller", "Demba Ba", "T. Anderson Keller"], "title": "Traveling Waves Integrate Spatial Information Through Time", "categories": ["cs.CV"], "comment": null, "summary": "Traveling waves of neural activity are widely observed in the brain, but\ntheir precise computational function remains unclear. One prominent hypothesis\nis that they enable the transfer and integration of spatial information across\nneural populations. However, few computational models have explored how\ntraveling waves might be harnessed to perform such integrative processing.\nDrawing inspiration from the famous \"Can one hear the shape of a drum?\" problem\n-- which highlights how normal modes of wave dynamics encode geometric\ninformation -- we investigate whether similar principles can be leveraged in\nartificial neural networks. Specifically, we introduce convolutional recurrent\nneural networks that learn to produce traveling waves in their hidden states in\nresponse to visual stimuli, enabling spatial integration. By then treating\nthese wave-like activation sequences as visual representations themselves, we\nobtain a powerful representational space that outperforms local feed-forward\nnetworks on tasks requiring global spatial context. In particular, we observe\nthat traveling waves effectively expand the receptive field of locally\nconnected neurons, supporting long-range encoding and communication of\ninformation. We demonstrate that models equipped with this mechanism solve\nvisual semantic segmentation tasks demanding global integration, significantly\noutperforming local feed-forward models and rivaling non-local U-Net models\nwith fewer parameters. As a first step toward traveling-wave-based\ncommunication and visual representation in artificial networks, our findings\nsuggest wave-dynamics may provide efficiency and training stability benefits,\nwhile simultaneously offering a new framework for connecting models to\nbiological recordings of neural activity.", "AI": {"tldr": "Traveling waves in neural networks enhance spatial integration, outperforming local models in tasks requiring global context.", "motivation": "To explore how traveling waves can transfer and integrate spatial information in neural networks, inspired by wave dynamics in physics.", "method": "Introduces convolutional recurrent neural networks that generate traveling waves for spatial integration, treating wave-like activations as visual representations.", "result": "Models with traveling waves outperform local feed-forward networks and rival non-local U-Net models in global spatial tasks, with fewer parameters.", "conclusion": "Traveling waves offer efficiency, training stability, and a framework for linking artificial networks to biological neural activity."}}
{"id": "2506.03949", "pdf": "https://arxiv.org/pdf/2506.03949", "abs": "https://arxiv.org/abs/2506.03949", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.", "AI": {"tldr": "The paper introduces TableEval, a benchmark for evaluating LLMs on realistic TableQA tasks, addressing limitations of existing benchmarks like simplicity and monolingual focus. It includes diverse table structures, multilingual data, and proposes SEAT for semantic accuracy evaluation.", "motivation": "Existing TableQA benchmarks are limited by simple flat tables, monolingual data, and data leakage, failing to capture real-world complexities like diverse structures and multilingual scenarios.", "method": "TableEval is introduced with diverse table structures (concise, hierarchical, nested) from four domains and three languages. SEAT, a new evaluation framework, assesses semantic accuracy at the sub-question level.", "result": "SEAT shows high agreement with human judgment. Experiments reveal gaps in state-of-the-art LLMs' ability to handle complex TableQA tasks.", "conclusion": "TableEval and SEAT address critical limitations in TableQA evaluation, highlighting LLMs' shortcomings and providing insights for future improvements."}}
{"id": "2402.09448", "pdf": "https://arxiv.org/pdf/2402.09448", "abs": "https://arxiv.org/abs/2402.09448", "authors": ["Ali Rabiee", "Sima Ghafoori", "Anna Cetera", "Maryam Norouzi", "Walter Besio", "Reza Abiri"], "title": "A Comparative Study of Conventional and Tripolar EEG for High-Performance Reach-to-Grasp BCI Systems", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "Removed the IEEE Transactions on Biomedical Engineering masthead/logo\n  that was included in the previous version by mistake", "summary": "This study aims to enhance BCI applications for individuals with motor\nimpairments by comparing the effectiveness of tripolar EEG (tEEG) with\nconventional EEG. The focus is on interpreting and decoding various grasping\nmovements, such as power grasp and precision grasp. The goal is to determine\nwhich EEG technology is more effective in processing and translating grasp\nrelated neural signals. The approach involved experimenting on ten healthy\nparticipants who performed two distinct grasp movements: power grasp and\nprecision grasp, with a no movement condition serving as the baseline. Our\nresearch presents a thorough comparison between EEG and tEEG in decoding\ngrasping movements. This comparison spans several key parameters, including\nsignal to noise ratio (SNR), spatial resolution via functional connectivity,\nERPs, and wavelet time frequency analysis. Additionally, our study involved\nextracting and analyzing statistical features from the wavelet coefficients,\nand both binary and multiclass classification methods were employed. Four\nmachine learning algorithms were used to evaluate the decoding accuracies. Our\nresults indicated that tEEG demonstrated superior performance over conventional\nEEG in various aspects. This included a higher signal to noise ratio, enhanced\nspatial resolution, and more informative data in ERPs and wavelet time\nfrequency analysis. The use of tEEG led to notable improvements in decoding\naccuracy for differentiating movement types. Specifically, tEEG achieved around\n90% accuracy in binary and 75.97% for multiclass classification. These results\nare markedly better than those from standard EEG, which recorded a maximum of\n77.85% and 61.27% in similar tasks, respectively. These findings highlight the\nsuperior effectiveness of tEEG over EEG in decoding grasp types and its\ncompetitive or superior performance in complex classifications compared with\nexisting research.", "AI": {"tldr": "tEEG outperforms conventional EEG in decoding grasp movements, showing higher SNR, spatial resolution, and classification accuracy.", "motivation": "Enhance BCI applications for motor-impaired individuals by comparing tEEG and EEG effectiveness in decoding grasping movements.", "method": "Experiments with ten healthy participants performing power and precision grasps, analyzing SNR, spatial resolution, ERPs, wavelet features, and using ML algorithms for classification.", "result": "tEEG achieved ~90% binary and 75.97% multiclass accuracy, surpassing EEG's 77.85% and 61.27%.", "conclusion": "tEEG is more effective than EEG for decoding grasp types, offering superior performance in complex classifications."}}
{"id": "2409.09778", "pdf": "https://arxiv.org/pdf/2409.09778", "abs": "https://arxiv.org/abs/2409.09778", "authors": ["Siqiao Mu", "Diego Klabjan"], "title": "Rewind-to-Delete: Certified Machine Unlearning for Nonconvex Functions", "categories": ["cs.LG"], "comment": null, "summary": "Machine unlearning algorithms aim to efficiently remove data from a model\nwithout retraining it from scratch, in order to remove corrupted or outdated\ndata or respect a user's ``right to be forgotten.\" Certified machine unlearning\nis a strong theoretical guarantee based on differential privacy that quantifies\nthe extent to which an algorithm erases data from the model weights. In\ncontrast to existing works in certified unlearning for convex or strongly\nconvex loss functions, or nonconvex objectives with limiting assumptions, we\npropose the first, first-order, black-box (i.e., can be applied to models\npretrained with vanilla gradient descent) algorithm for unlearning on general\nnonconvex loss functions, which unlearns by ``rewinding\" to an earlier step\nduring the learning process before performing gradient descent on the loss\nfunction of the retained data points. We prove $(\\epsilon, \\delta)$ certified\nunlearning and performance guarantees that establish the\nprivacy-utility-complexity tradeoff of our algorithm, and we prove\ngeneralization guarantees for functions that satisfy the Polyak-Lojasiewicz\ninequality. Finally, we demonstrate the superior performance of our algorithm\ncompared to existing methods, within a new experimental framework that more\naccurately reflects unlearning user data in practice.", "AI": {"tldr": "Proposes a first-order black-box algorithm for certified machine unlearning on general nonconvex loss functions, outperforming existing methods.", "motivation": "Addresses the need to efficiently remove data from models without retraining, respecting privacy and utility.", "method": "Uses a 'rewinding' approach during training, followed by gradient descent on retained data.", "result": "Achieves (\u03b5, \u03b4) certified unlearning and demonstrates superior performance in practical scenarios.", "conclusion": "Provides a robust solution for unlearning in nonconvex settings with strong privacy-utility guarantees."}}
{"id": "2502.19409", "pdf": "https://arxiv.org/pdf/2502.19409", "abs": "https://arxiv.org/abs/2502.19409", "authors": ["Danae S\u00e1nchez Villegas", "Ingo Ziegler", "Desmond Elliott"], "title": "ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Code, dataset, and checkpoints are publicly available at\n  https://github.com/danaesavi/ImageChain; v2: added human annotation study to\n  validate SimRate", "summary": "Reasoning over sequences of images remains a challenge for multimodal large\nlanguage models (MLLMs). While recent models incorporate multi-image data\nduring pre-training, they still struggle to recognize sequential structures,\noften treating images independently. This work introduces ImageChain, a\nframework that enhances MLLMs with sequential reasoning capabilities over image\ndata by modeling visual sequences as a multi-turn conversation. In ImageChain,\nimages are interleaved with corresponding textual descriptions to form a\ncontrolled dialogue that explicitly captures temporal dependencies and\nnarrative progression. Our method optimizes for the task of next-scene\ndescription, where the model generates a context-aware description of an\nupcoming scene based on preceding visual and textual cues. We demonstrate that\nour approach improves performance on the next-scene description task --\nachieving an average improvement from 3.7% to 19% in SimRate, a metric that\nquantifies semantic similarity to human-annotated ground truths. Moreover,\nImageChain achieves robust zero-shot out-of-domain performance in applications\nranging from comics to robotics. Extensive experiments validate that\ninstruction-tuning in a multimodal, multi-turn conversation design is key to\nbridging the gap between static image understanding and temporally-aware\nreasoning.", "AI": {"tldr": "ImageChain enhances MLLMs for sequential image reasoning by modeling visual sequences as multi-turn conversations, improving next-scene description tasks.", "motivation": "Existing MLLMs struggle with sequential image reasoning, treating images independently despite multi-image pre-training.", "method": "ImageChain interleaves images with textual descriptions in a dialogue format to capture temporal dependencies, optimizing for next-scene description tasks.", "result": "Achieves 3.7% to 19% improvement in SimRate and robust zero-shot performance across domains like comics and robotics.", "conclusion": "Instruction-tuning in a multimodal, multi-turn conversation design bridges static image understanding and temporally-aware reasoning."}}
{"id": "2506.04907", "pdf": "https://arxiv.org/pdf/2506.04907", "abs": "https://arxiv.org/abs/2506.04907", "authors": ["Alex Pan", "Mary-Anne Williams"], "title": "Context Is Not Comprehension: Unmasking LLM reasoning blind spots with VLO", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "24 pages, 2 figures, 4 tables; to appear in AAAI 2026", "summary": "The dominant evaluation of Large Language Models has centered on their\nability to surface explicit facts from increasingly vast contexts. While\ntoday's best models demonstrate near-perfect recall on these tasks, this\napparent success is overly simplistic and non-representative of the complexity\nof human reasoning which is often highly nested. We introduce Verbose ListOps\n(VLO), a novel benchmark designed to isolate this failure. VLO programmatically\nweaves deterministic, nested computations into coherent stories, forcing models\nto track and update internal state rather than simply locate explicit values.\nOur experiments show that leading LLMs, capable of solving the raw ListOps\nequations with near-perfect accuracy, collapse in performance on VLO at just\n10k tokens. The extensibility of VLO's generation framework to any verifiable\nreasoning pattern will be a critical tool, enabling model developers to move\nbeyond context windows and robustly test new reasoning architectures; a\nnecessary step to automating the world's knowledge work.", "AI": {"tldr": "The paper introduces Verbose ListOps (VLO), a benchmark to test LLMs' ability to handle nested reasoning, showing current models fail despite excelling at simpler tasks.", "motivation": "Current LLM evaluations focus on explicit fact recall, ignoring the complexity of human-like nested reasoning. VLO addresses this gap.", "method": "VLO programmatically embeds nested computations into stories, requiring models to track internal state, not just locate explicit values.", "result": "Leading LLMs perform poorly on VLO (10k tokens), despite near-perfect accuracy on simpler ListOps tasks.", "conclusion": "VLO's extensible framework is a critical tool for testing advanced reasoning architectures, essential for automating knowledge work."}}
{"id": "2409.13671", "pdf": "https://arxiv.org/pdf/2409.13671", "abs": "https://arxiv.org/abs/2409.13671", "authors": ["Julian Carvajal Rico", "Adel Alaeddini", "Syed Hasib Akhter Faruqui", "Susan P Fisher-Hoch", "Joseph B Mccormick"], "title": "A Generative Framework for Predictive Modeling of Multiple Chronic Conditions Using Graph Variational Autoencoder and Bandit-Optimized Graph Neural Network", "categories": ["cs.LG"], "comment": "This work has been accepted for publication in the IEEE Journal of\n  Biomedical and Health Informatics", "summary": "Predicting the emergence of multiple chronic conditions (MCC) is crucial for\nearly intervention and personalized healthcare, as MCC significantly impacts\npatient outcomes and healthcare costs. Graph neural networks (GNNs) are\neffective methods for modeling complex graph data, such as those found in MCC.\nHowever, a significant challenge with GNNs is their reliance on an existing\ngraph structure, which is not readily available for MCC. To address this\nchallenge, we propose a novel generative framework for GNNs that constructs a\nrepresentative underlying graph structure by utilizing the distribution of the\ndata to enhance predictive analytics for MCC. Our framework employs a graph\nvariational autoencoder (GVAE) to capture the complex relationships in patient\ndata. This allows for a comprehensive understanding of individual health\ntrajectories and facilitates the creation of diverse patient stochastic\nsimilarity graphs while preserving the original feature set. These variations\nof patient stochastic similarity graphs, generated from the GVAE decoder, are\nthen processed by a GNN using a novel Laplacian regularization technique to\nrefine the graph structure over time and improves the prediction accuracy of\nMCC. A contextual Bandit is designed to evaluate the stochastically generated\ngraphs and identify the best-performing graph for the GNN model iteratively\nuntil model convergence. We validate the performance of the proposed contextual\nBandit algorithm against $\\varepsilon$-Greedy and multi-armed Bandit algorithms\non a large cohort (n = 1,592) of patients with MCC. These advancements\nhighlight the potential of the proposed approach to transform predictive\nhealthcare analytics, enabling a more personalized and proactive approach to\nMCC management.", "AI": {"tldr": "A generative framework using GNNs and GVAE constructs patient similarity graphs for predicting MCC, improving accuracy with Laplacian regularization and contextual Bandit evaluation.", "motivation": "Predicting MCC is vital for early intervention and personalized care, but GNNs require existing graph structures, which are lacking for MCC.", "method": "Proposes a GVAE-based generative framework to create patient similarity graphs, refined with Laplacian regularization and evaluated by a contextual Bandit.", "result": "Validated on 1,592 MCC patients, the method outperforms \u03b5-Greedy and multi-armed Bandit algorithms in prediction accuracy.", "conclusion": "The approach enhances MCC prediction, supporting personalized and proactive healthcare management."}}
{"id": "2502.21059", "pdf": "https://arxiv.org/pdf/2502.21059", "abs": "https://arxiv.org/abs/2502.21059", "authors": ["Ziyi Zhang", "Zhen Sun", "Zongmin Zhang", "Jihui Guo", "Xinlei He"], "title": "FC-Attack: Jailbreaking Multimodal Large Language Models via Auto-Generated Flowcharts", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": "13 pages, 7 figures", "summary": "Multimodal Large Language Models (MLLMs) have become powerful and widely\nadopted in some practical applications. However, recent research has revealed\ntheir vulnerability to multimodal jailbreak attacks, whereby the model can be\ninduced to generate harmful content, leading to safety risks. Although most\nMLLMs have undergone safety alignment, recent research shows that the visual\nmodality is still vulnerable to jailbreak attacks. In our work, we discover\nthat by using flowcharts with partially harmful information, MLLMs can be\ninduced to provide additional harmful details. Based on this, we propose a\njailbreak attack method based on auto-generated flowcharts, FC-Attack.\nSpecifically, FC-Attack first fine-tunes a pre-trained LLM to create a\nstep-description generator based on benign datasets. The generator is then used\nto produce step descriptions corresponding to a harmful query, which are\ntransformed into flowcharts in 3 different shapes (vertical, horizontal, and\nS-shaped) as visual prompts. These flowcharts are then combined with a benign\ntextual prompt to execute the jailbreak attack on MLLMs. Our evaluations on\nAdvbench show that FC-Attack attains an attack success rate of up to 96% via\nimages and up to 78% via videos across multiple MLLMs. Additionally, we\ninvestigate factors affecting the attack performance, including the number of\nsteps and the font styles in the flowcharts. We also find that FC-Attack can\nimprove the jailbreak performance from 4% to 28% in Claude-3.5 by changing the\nfont style. To mitigate the attack, we explore several defenses and find that\nAdaShield can largely reduce the jailbreak performance but with the cost of\nutility drop.", "AI": {"tldr": "FC-Attack uses auto-generated flowcharts to jailbreak MLLMs, achieving high success rates (up to 96% via images). Defenses like AdaShield can mitigate the attack but reduce utility.", "motivation": "MLLMs are vulnerable to multimodal jailbreak attacks, especially through visual prompts, posing safety risks.", "method": "FC-Attack fine-tunes an LLM to generate step descriptions for harmful queries, converts them into flowcharts (vertical, horizontal, S-shaped), and combines them with benign text prompts.", "result": "Achieves up to 96% attack success rate via images and 78% via videos. Font styles and steps impact performance.", "conclusion": "FC-Attack demonstrates significant jailbreak potential; AdaShield offers partial defense but compromises utility."}}
{"id": "2506.05176", "pdf": "https://arxiv.org/pdf/2506.05176", "abs": "https://arxiv.org/abs/2506.05176", "authors": ["Yanzhao Zhang", "Mingxin Li", "Dingkun Long", "Xin Zhang", "Huan Lin", "Baosong Yang", "Pengjun Xie", "An Yang", "Dayiheng Liu", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.", "AI": {"tldr": "The Qwen3 Embedding series improves text embedding and reranking using Qwen3 foundation models, offering multilingual capabilities and diverse model sizes, achieving state-of-the-art results.", "motivation": "To advance text embedding and reranking by leveraging Qwen3 LLMs' multilingual understanding and generation, addressing diverse deployment needs.", "method": "Multi-stage training pipeline combining unsupervised pre-training and supervised fine-tuning, with model merging for robustness. Qwen3 LLMs synthesize diverse training data.", "result": "State-of-the-art performance on benchmarks like MTEB, excelling in multilingual and retrieval tasks. Models are publicly available.", "conclusion": "The Qwen3 Embedding series is a robust, adaptable solution for embedding and reranking, with strong multilingual performance and open availability."}}
{"id": "2408.03573", "pdf": "https://arxiv.org/pdf/2408.03573", "abs": "https://arxiv.org/abs/2408.03573", "authors": ["Yuheng Huang", "Jiayang Song", "Qiang Hu", "Felix Juefei-Xu", "Lei Ma"], "title": "AcTracer: Active Testing of Large Language Model via Multi-Stage Sampling", "categories": ["cs.SE", "cs.AI", "cs.CL", "D.2.5; I.2.7"], "comment": "To appear in ACM Transactions on Software Engineering and Methodology\n  (2025)", "summary": "Performance evaluation plays a crucial role in the development life cycle of\nlarge language models (LLMs). It estimates the model's capability, elucidates\nbehavior characteristics, and facilitates the identification of potential\nissues and limitations, thereby guiding further improvement. Given that LLMs'\ndiverse task-handling abilities stem from large volumes of training data, a\ncomprehensive evaluation also necessitates abundant, well-annotated, and\nrepresentative test data to assess LLM performance across various downstream\ntasks. However, the demand for high-quality test data often entails substantial\ntime, computational resources, and manual efforts, sometimes causing the\nevaluation to be inefficient or impractical. To address these challenges,\nresearchers propose active testing, which estimates the overall performance by\nselecting a subset of test data. Nevertheless, the existing active testing\nmethods tend to be inefficient, even inapplicable, given the unique new\nchallenges of LLMs (e.g., diverse task types, increased model complexity, and\nunavailability of training data). To mitigate such limitations and expedite the\ndevelopment cycle of LLMs, in this work, we introduce AcTracer, an active\ntesting framework tailored for LLMs that strategically selects a small subset\nof test data to achieve a more accurate performance estimation for LLMs.\nAcTracer utilizes both internal and external information from LLMs to guide the\ntest sampling process, reducing variance through a multi-stage pool-based\nactive selection. Our experiment results demonstrate that AcTracer achieves\nstate-of-the-art performance compared to existing methods across various tasks.", "AI": {"tldr": "AcTracer is an active testing framework for LLMs that efficiently selects a subset of test data to improve performance evaluation, addressing challenges like inefficiency and data scarcity.", "motivation": "Current active testing methods for LLMs are inefficient due to diverse tasks, model complexity, and lack of training data. AcTracer aims to streamline evaluation.", "method": "AcTracer uses internal and external LLM information for multi-stage pool-based active selection of test data, reducing variance.", "result": "AcTracer outperforms existing methods in performance estimation across various tasks.", "conclusion": "AcTracer enhances LLM evaluation efficiency and accuracy, aiding faster development cycles."}}
{"id": "2410.00535", "pdf": "https://arxiv.org/pdf/2410.00535", "abs": "https://arxiv.org/abs/2410.00535", "authors": ["Francisco N. F. Q. Simoes", "Mehdi Dastani", "Thijs van Ommen"], "title": "The Causal Information Bottleneck and Optimal Causal Variable Abstractions", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "comment": "Accepted at UAI 2025. Code available at\n  github.com/francisco-simoes/cib-optimization-psagd", "summary": "To effectively study complex causal systems, it is often useful to construct\nabstractions of parts of the system by discarding irrelevant details while\npreserving key features. The Information Bottleneck (IB) method is a widely\nused approach to construct variable abstractions by compressing random\nvariables while retaining predictive power over a target variable. Traditional\nmethods like IB are purely statistical and ignore underlying causal structures,\nmaking them ill-suited for causal tasks. We propose the Causal Information\nBottleneck (CIB), a causal extension of the IB, which compresses a set of\nchosen variables while maintaining causal control over a target variable. This\nmethod produces abstractions of (sets of) variables which are causally\ninterpretable, give us insight about the interactions between the abstracted\nvariables and the target variable, and can be used when reasoning about\ninterventions. We present experimental results demonstrating that the learned\nabstractions accurately capture causal relations as intended.", "AI": {"tldr": "The paper introduces the Causal Information Bottleneck (CIB), a method to create causally interpretable variable abstractions while preserving causal control over a target variable.", "motivation": "Traditional methods like the Information Bottleneck (IB) ignore causal structures, making them unsuitable for causal tasks. The authors aim to address this gap.", "method": "The CIB extends the IB by compressing variables while maintaining causal control over a target variable, ensuring causally interpretable abstractions.", "result": "Experimental results show that CIB accurately captures causal relations, providing insights into variable interactions and intervention reasoning.", "conclusion": "CIB is a promising approach for causal abstraction tasks, outperforming traditional statistical methods in causal contexts."}}
{"id": "2502.21075", "pdf": "https://arxiv.org/pdf/2502.21075", "abs": "https://arxiv.org/abs/2502.21075", "authors": ["Christopher Wewer", "Bart Pogodzinski", "Bernt Schiele", "Jan Eric Lenssen"], "title": "Spatial Reasoning with Denoising Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project website: https://geometric-rl.mpi-inf.mpg.de/srm", "summary": "We introduce Spatial Reasoning Models (SRMs), a framework to perform\nreasoning over sets of continuous variables via denoising generative models.\nSRMs infer continuous representations on a set of unobserved variables, given\nobservations on observed variables. Current generative models on spatial\ndomains, such as diffusion and flow matching models, often collapse to\nhallucination in case of complex distributions. To measure this, we introduce a\nset of benchmark tasks that test the quality of complex reasoning in generative\nmodels and can quantify hallucination. The SRM framework allows to report key\nfindings about importance of sequentialization in generation, the associated\norder, as well as the sampling strategies during training. It demonstrates, for\nthe first time, that order of generation can successfully be predicted by the\ndenoising network itself. Using these findings, we can increase the accuracy of\nspecific reasoning tasks from <1% to >50%. Our project website provides\nadditional videos, code, and the benchmark datasets:\nhttps://geometric-rl.mpi-inf.mpg.de/srm", "AI": {"tldr": "SRMs use denoising generative models for reasoning over continuous variables, improving accuracy in complex tasks by addressing hallucination and optimizing generation order.", "motivation": "Current generative models struggle with hallucination in complex spatial distributions, prompting the need for a framework to improve reasoning accuracy.", "method": "SRMs infer continuous representations of unobserved variables using denoising generative models, with benchmark tasks to measure hallucination.", "result": "SRMs improve reasoning task accuracy from <1% to >50% by predicting generation order and optimizing sampling strategies.", "conclusion": "SRMs effectively address hallucination in generative models, demonstrating the importance of generation order and sequentialization."}}
{"id": "2506.05387", "pdf": "https://arxiv.org/pdf/2506.05387", "abs": "https://arxiv.org/abs/2506.05387", "authors": ["Jaydip Sen", "Saptarshi Sengupta", "Subhasis Dasgupta"], "title": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "This is the accepted but pre-reviewed version of the chapter that has\n  been accepted for publication in the Springer volume 'Decision-Making in\n  Computational Intelligence-Based Systems,' edited by Witold Pedrycz, Gilberto\n  Rivera, Rose Ma Rodriguez, and Salvador Ibarra Martinez. The chapter is 39\n  pages long, and it contains 2 figures and 6 tables. This is NOT the final\n  camera-ready version", "summary": "This chapter explores advancements in decoding strategies for large language\nmodels (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)\nalgorithm. Traditional decoding methods, such as top-k and nucleus sampling,\noften struggle to balance fluency, diversity, and coherence in text generation.\nTo address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)\nis proposed as an improved version of LTS, incorporating dynamic entropy\nthresholding, multi-objective scoring, and reward-penalty adjustments. ASTS\nensures contextually coherent and diverse text generation while maintaining\ncomputational efficiency. Its performance is evaluated across multiple\nbenchmarks, including story generation and abstractive summarization, using\nmetrics such as perplexity, MAUVE, and diversity scores. Experimental results\ndemonstrate that ASTS outperforms existing sampling techniques by reducing\nrepetition, enhancing semantic alignment, and improving fluency.", "AI": {"tldr": "ASTS improves LTS for LLMs by adding dynamic entropy thresholding and multi-objective scoring, outperforming traditional methods in fluency, diversity, and coherence.", "motivation": "Address limitations of top-k and nucleus sampling in balancing fluency, diversity, and coherence in text generation.", "method": "Proposes ASTS, an enhanced LTS algorithm with dynamic entropy thresholding, multi-objective scoring, and reward-penalty adjustments.", "result": "ASTS reduces repetition, enhances semantic alignment, and improves fluency, outperforming existing methods in benchmarks.", "conclusion": "ASTS is a superior decoding strategy for LLMs, offering better coherence, diversity, and efficiency."}}
{"id": "2409.04432", "pdf": "https://arxiv.org/pdf/2409.04432", "abs": "https://arxiv.org/abs/2409.04432", "authors": ["Angelo Salatino", "Tanay Aggarwal", "Andrea Mannocci", "Francesco Osborne", "Enrico Motta"], "title": "A Survey on Knowledge Organization Systems of Research Fields: Resources and Challenges", "categories": ["cs.DL", "cs.AI", "cs.IR"], "comment": "Published at Quantitative Science Studies", "summary": "Knowledge Organization Systems (KOSs), such as term lists, thesauri,\ntaxonomies, and ontologies, play a fundamental role in categorising, managing,\nand retrieving information. In the academic domain, KOSs are often adopted for\nrepresenting research areas and their relationships, primarily aiming to\nclassify research articles, academic courses, patents, books, scientific\nvenues, domain experts, grants, software, experiment materials, and several\nother relevant products and agents. These structured representations of\nresearch areas, widely embraced by many academic fields, have proven effective\nin empowering AI-based systems to i) enhance retrievability of relevant\ndocuments, ii) enable advanced analytic solutions to quantify the impact of\nacademic research, and iii) analyse and forecast research dynamics. This paper\naims to present a comprehensive survey of the current KOS for academic\ndisciplines. We analysed and compared 45 KOSs according to five main\ndimensions: scope, structure, curation, usage, and links to other KOSs. Our\nresults reveal a very heterogeneous scenario in terms of scope, scale, quality,\nand usage, highlighting the need for more integrated solutions for representing\nresearch knowledge across academic fields. We conclude by discussing the main\nchallenges and the most promising future directions.", "AI": {"tldr": "A survey of 45 Knowledge Organization Systems (KOSs) for academic disciplines, analyzing their scope, structure, curation, usage, and links, revealing heterogeneity and the need for integration.", "motivation": "To understand and compare the effectiveness of KOSs in categorizing and managing academic research information.", "method": "Analysis and comparison of 45 KOSs across five dimensions: scope, structure, curation, usage, and links to other KOSs.", "result": "Heterogeneity in scope, scale, quality, and usage, indicating a lack of integrated solutions.", "conclusion": "Identifies challenges and future directions for improving KOSs in academic research."}}
{"id": "2410.01655", "pdf": "https://arxiv.org/pdf/2410.01655", "abs": "https://arxiv.org/abs/2410.01655", "authors": ["Roussel Desmond Nzoyem", "David A. W. Barton", "Tom Deakin"], "title": "Reevaluating Meta-Learning Optimization Algorithms Through Contextual Self-Modulation", "categories": ["cs.LG", "math.DS"], "comment": "Accepted as a conference paper at CoLLAs 2025. 23 pages, 11 figures,\n  5 tables", "summary": "Contextual Self-Modulation (CSM) (Nzoyem et al., 2025) is a potent\nregularization mechanism for Neural Context Flows (NCFs) which demonstrates\npowerful meta-learning on physical systems. However, CSM has limitations in its\napplicability across different modalities and in high-data regimes. In this\nwork, we introduce two extensions: $i$CSM which expands CSM to\ninfinite-dimensional variations by embedding the contexts into a function\nspace, and StochasticNCF which improves scalability by providing a low-cost\napproximation of meta-gradient updates through a sampled set of nearest\nenvironments. These extensions are demonstrated through comprehensive\nexperimentation on a range of tasks, including dynamical systems, computer\nvision challenges, and curve fitting problems. Additionally, we incorporate\nhigher-order Taylor expansions via Taylor-Mode automatic differentiation,\nrevealing that higher-order approximations do not necessarily enhance\ngeneralization. Finally, we demonstrate how CSM can be integrated into other\nmeta-learning frameworks with FlashCAVIA, a computationally efficient extension\nof the CAVIA meta-learning framework (Zintgraf et al., 2019). Together, these\ncontributions highlight the significant benefits of CSM and indicate that its\nstrengths in meta-learning and out-of-distribution tasks are particularly\nwell-suited to physical systems. Our open-source library, designed for modular\nintegration of self-modulation into contextual meta-learning workflows, is\navailable at https://github.com/ddrous/self-mod.", "AI": {"tldr": "The paper introduces extensions to Contextual Self-Modulation (CSM) for Neural Context Flows (NCFs), addressing limitations in applicability and scalability, and demonstrates their effectiveness across various tasks.", "motivation": "CSM is powerful for meta-learning on physical systems but has limitations in cross-modality and high-data scenarios. The work aims to expand its applicability and scalability.", "method": "Two extensions are proposed: $i$CSM for infinite-dimensional variations and StochasticNCF for scalable meta-gradient updates. Higher-order Taylor expansions and integration with FlashCAVIA are also explored.", "result": "Experiments on dynamical systems, computer vision, and curve fitting show the extensions' effectiveness. Higher-order approximations do not consistently improve generalization.", "conclusion": "CSM's strengths in meta-learning and out-of-distribution tasks are highlighted, especially for physical systems. An open-source library is provided for modular integration."}}
{"id": "2503.04459", "pdf": "https://arxiv.org/pdf/2503.04459", "abs": "https://arxiv.org/abs/2503.04459", "authors": ["Hongyeob Kim", "Inyoung Jung", "Dayoon Suh", "Youjia Zhang", "Sangmin Lee", "Sungeun Hong"], "title": "Question-Aware Gaussian Experts for Audio-Visual Question Answering", "categories": ["cs.CV"], "comment": "CVPR 2025. Code is available at https://github.com/AIM-SKKU/QA-TIGER", "summary": "Audio-Visual Question Answering (AVQA) requires not only question-based\nmultimodal reasoning but also precise temporal grounding to capture subtle\ndynamics for accurate prediction. However, existing methods mainly use question\ninformation implicitly, limiting focus on question-specific details.\nFurthermore, most studies rely on uniform frame sampling, which can miss key\nquestion-relevant frames. Although recent Top-K frame selection methods aim to\naddress this, their discrete nature still overlooks fine-grained temporal\ndetails. This paper proposes QA-TIGER, a novel framework that explicitly\nincorporates question information and models continuous temporal dynamics. Our\nkey idea is to use Gaussian-based modeling to adaptively focus on both\nconsecutive and non-consecutive frames based on the question, while explicitly\ninjecting question information and applying progressive refinement. We leverage\na Mixture of Experts (MoE) to flexibly implement multiple Gaussian models,\nactivating temporal experts specifically tailored to the question. Extensive\nexperiments on multiple AVQA benchmarks show that QA-TIGER consistently\nachieves state-of-the-art performance. Code is available at\nhttps://aim-skku.github.io/QA-TIGER/", "AI": {"tldr": "QA-TIGER is a novel framework for AVQA that explicitly uses question information and models continuous temporal dynamics with Gaussian-based modeling and MoE, achieving state-of-the-art results.", "motivation": "Existing AVQA methods implicitly use question info and uniform frame sampling, missing key details. QA-TIGER addresses these limitations.", "method": "Uses Gaussian-based modeling to focus on relevant frames, injects question info explicitly, and employs MoE for tailored temporal experts.", "result": "Achieves state-of-the-art performance on multiple AVQA benchmarks.", "conclusion": "QA-TIGER improves AVQA by better leveraging question info and temporal dynamics, setting a new benchmark."}}
{"id": "2506.06395", "pdf": "https://arxiv.org/pdf/2506.06395", "abs": "https://arxiv.org/abs/2506.06395", "authors": ["Pengyi Li", "Matvey Skripkin", "Alexander Zubrey", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.", "AI": {"tldr": "RLSC uses a model's self-confidence as reward signals for post-training, improving accuracy on math benchmarks without human annotations or external rewards.", "motivation": "Existing RL methods for LLMs rely on costly human annotations or external reward models, which RLSC aims to eliminate.", "method": "RLSC leverages the model's own confidence as reward signals, requiring minimal samples and no labels.", "result": "RLSC improves accuracy by up to +21.7% on math benchmarks with only 16 samples per question and 10-20 training steps.", "conclusion": "RLSC offers a simple, scalable post-training method for LLMs, reducing dependency on external supervision."}}
{"id": "2409.18395", "pdf": "https://arxiv.org/pdf/2409.18395", "abs": "https://arxiv.org/abs/2409.18395", "authors": ["Arshiya Khan", "Guannan Liu", "Xing Gao"], "title": "Code Vulnerability Repair with Large Language Model using Context-Aware Prompt Tuning", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant challenges in detecting\nand repairing vulnerable code, particularly when dealing with vulnerabilities\ninvolving multiple aspects, such as variables, code flows, and code structures.\nIn this study, we utilize GitHub Copilot as the LLM and focus on buffer\noverflow vulnerabilities. Our experiments reveal a notable gap in Copilot's\nabilities when dealing with buffer overflow vulnerabilities, with a 76%\nvulnerability detection rate but only a 15% vulnerability repair rate. To\naddress this issue, we propose context-aware prompt tuning techniques designed\nto enhance LLM performance in repairing buffer overflow. By injecting a\nsequence of domain knowledge about the vulnerability, including various\nsecurity and code contexts, we demonstrate that Copilot's successful repair\nrate increases to 63%, representing more than four times the improvement\ncompared to repairs without domain knowledge.", "AI": {"tldr": "LLMs like GitHub Copilot struggle with detecting and repairing buffer overflow vulnerabilities. Context-aware prompt tuning improves repair rates significantly.", "motivation": "Address the limitations of LLMs in detecting and repairing complex vulnerabilities like buffer overflows.", "method": "Utilize GitHub Copilot and apply context-aware prompt tuning with domain knowledge to enhance repair performance.", "result": "Initial detection rate: 76%, repair rate: 15%. With tuning, repair rate improves to 63%.", "conclusion": "Context-aware prompt tuning significantly boosts LLM performance in repairing buffer overflow vulnerabilities."}}
{"id": "2410.05711", "pdf": "https://arxiv.org/pdf/2410.05711", "abs": "https://arxiv.org/abs/2410.05711", "authors": ["Daoyu Wang", "Mingyue Cheng", "Zhiding Liu", "Qi Liu"], "title": "TimeDART: A Diffusion Autoregressive Transformer for Self-Supervised Time Series Representation", "categories": ["cs.LG"], "comment": "25 pages, 7 figures, Accepted by the 42nd International Conference on\n  Machine Learning (ICML 2025)", "summary": "Self-supervised learning has garnered increasing attention in time series\nanalysis for benefiting various downstream tasks and reducing reliance on\nlabeled data. Despite its effectiveness, existing methods often struggle to\ncomprehensively capture both long-term dynamic evolution and subtle local\npatterns in a unified manner. In this work, we propose \\textbf{TimeDART}, a\nnovel self-supervised time series pre-training framework that unifies two\npowerful generative paradigms to learn more transferable representations.\nSpecifically, we first employ a causal Transformer encoder, accompanied by a\npatch-based embedding strategy, to model the evolving trends from left to\nright. Building on this global modeling, we further introduce a denoising\ndiffusion process to capture fine-grained local patterns through forward\ndiffusion and reverse denoising. Finally, we optimize the model in an\nautoregressive manner. As a result, TimeDART effectively accounts for both\nglobal and local sequence features in a coherent way. We conduct extensive\nexperiments on public datasets for time series forecasting and classification.\nThe experimental results demonstrate that TimeDART consistently outperforms\nprevious compared methods, validating the effectiveness of our approach. Our\ncode is available at https://github.com/Melmaphother/TimeDART.", "AI": {"tldr": "TimeDART is a self-supervised time series pre-training framework combining causal Transformer and denoising diffusion for global and local pattern capture, outperforming existing methods.", "motivation": "Existing self-supervised methods struggle to unify long-term dynamics and local patterns in time series analysis.", "method": "Uses a causal Transformer for global trends and denoising diffusion for local patterns, optimized autoregressively.", "result": "Outperforms previous methods in forecasting and classification on public datasets.", "conclusion": "TimeDART effectively unifies global and local features, demonstrating superior performance."}}
{"id": "2503.11544", "pdf": "https://arxiv.org/pdf/2503.11544", "abs": "https://arxiv.org/abs/2503.11544", "authors": ["Parsa Rahimi", "Damien Teney", "Sebastien Marcel"], "title": "AugGen: Synthetic Augmentation Can Improve Discriminative Models", "categories": ["cs.CV"], "comment": null, "summary": "The increasing reliance on large-scale datasets in machine learning poses\nsignificant privacy and ethical challenges, particularly in sensitive domains\nsuch as face recognition (FR). Synthetic data generation offers a promising\nalternative; however, most existing methods depend heavily on external datasets\nor pre-trained models, increasing complexity and resource demands. In this\npaper, we introduce AugGen, a self-contained synthetic augmentation technique.\nAugGen strategically samples from a class-conditional generative model trained\nexclusively on the target FR dataset, eliminating the need for external\nresources. Evaluated across 8 FR benchmarks, including IJB-C and IJB-B, our\nmethod achieves 1-12% performance improvements, outperforming models trained\nsolely on real data and surpassing state-of-the-art synthetic data generation\napproaches, while using less real data. Notably, these gains often exceed those\nfrom architectural modifications, underscoring the value of synthetic\naugmentation in data-limited scenarios. Our findings demonstrate that carefully\nintegrated synthetic data can both mitigate privacy constraints and\nsubstantially enhance discriminative performance in face recognition. Paper\nwebsite: https://parsa-ra.github.io/auggen/.", "AI": {"tldr": "AugGen introduces a self-contained synthetic augmentation technique for face recognition, eliminating reliance on external datasets and outperforming existing methods with 1-12% performance gains.", "motivation": "Address privacy and ethical challenges in face recognition by reducing dependency on large-scale datasets and external resources.", "method": "AugGen uses a class-conditional generative model trained on the target dataset to generate synthetic data, avoiding external dependencies.", "result": "Achieves 1-12% performance improvements across 8 benchmarks, surpassing real-data-only and synthetic-data approaches.", "conclusion": "Synthetic data, when carefully integrated, enhances face recognition performance and mitigates privacy concerns."}}
{"id": "2506.06821", "pdf": "https://arxiv.org/pdf/2506.06821", "abs": "https://arxiv.org/abs/2506.06821", "authors": ["Yuhan Cao", "Zian Chen", "Kun Quan", "Ziliang Zhang", "Yu Wang", "Xiaoning Dong", "Yeqi Feng", "Guanzhong He", "Jingcheng Huang", "Jianhao Li", "Yixuan Tan", "Jiafu Tang", "Yilin Tang", "Junlei Wu", "Qianyu Xiao", "Can Zheng", "Shouchen Zhou", "Yuxiang Zhu", "Yiming Huang", "Tian Xie", "Tianxing He"], "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "37 pages, 22 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation, capable of tackling complex tasks during inference. However,\nthe extent to which LLMs can be utilized for code checking or debugging through\ntest case generation remains largely unexplored. We investigate this problem\nfrom the perspective of competition-level programming (CP) programs and propose\nTCGBench, a Benchmark for (LLM generation of) Test Case Generators. This\nbenchmark comprises two tasks, aimed at studying the capabilities of LLMs in\n(1) generating valid test case generators for a given CP problem, and further\n(2) generating targeted test case generators that expose bugs in human-written\ncode. Experimental results indicate that while state-of-the-art LLMs can\ngenerate valid test case generators in most cases, most LLMs struggle to\ngenerate targeted test cases that reveal flaws in human code effectively.\nEspecially, even advanced reasoning models (e.g., o3-mini) fall significantly\nshort of human performance in the task of generating targeted generators.\nFurthermore, we construct a high-quality, manually curated dataset of\ninstructions for generating targeted generators. Analysis demonstrates that the\nperformance of LLMs can be enhanced with the aid of this dataset, by both\nprompting and fine-tuning.", "AI": {"tldr": "The paper explores LLMs' ability to generate test case generators for code checking, introduces TCGBench for benchmarking, and finds LLMs struggle with targeted bug-exposing test cases, though performance improves with curated datasets.", "motivation": "To investigate the unexplored potential of LLMs in generating test case generators for code checking, particularly in competition-level programming.", "method": "Proposes TCGBench, a benchmark with two tasks: generating valid test case generators and targeted ones to expose bugs. Tests state-of-the-art LLMs and analyzes their performance.", "result": "LLMs can generate valid test case generators but struggle with targeted ones. Performance improves with a manually curated dataset.", "conclusion": "LLMs show promise in test case generation but need enhancement for targeted bug detection, with curated datasets aiding performance."}}
{"id": "2411.00696", "pdf": "https://arxiv.org/pdf/2411.00696", "abs": "https://arxiv.org/abs/2411.00696", "authors": ["Fuying Wang", "Feng Wu", "Yihan Tang", "Lequan Yu"], "title": "CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Integrating multimodal Electronic Health Records (EHR) data, such as\nnumerical time series and free-text clinical reports, has great potential in\npredicting clinical outcomes. However, prior work has primarily focused on\ncapturing temporal interactions within individual samples and fusing multimodal\ninformation, overlooking critical temporal patterns across patients. These\npatterns, such as trends in vital signs like abnormal heart rate or blood\npressure, can indicate deteriorating health or an impending critical event.\nSimilarly, clinical notes often contain textual descriptions that reflect these\npatterns. Identifying corresponding temporal patterns across different\nmodalities is crucial for improving the accuracy of clinical outcome\npredictions, yet it remains a challenging task. To address this gap, we\nintroduce a Cross-Modal Temporal Pattern Discovery (CTPD) framework, designed\nto efficiently extract meaningful cross-modal temporal patterns from multimodal\nEHR data. Our approach introduces shared initial temporal pattern\nrepresentations which are refined using slot attention to generate temporal\nsemantic embeddings. To ensure rich cross-modal temporal semantics in the\nlearned patterns, we introduce a contrastive-based TPNCE loss for cross-modal\nalignment, along with two reconstruction losses to retain core information of\neach modality. Evaluations on two clinically critical tasks, 48-hour\nin-hospital mortality and 24-hour phenotype classification, using the MIMIC-III\ndatabase demonstrate the superiority of our method over existing approaches.", "AI": {"tldr": "The paper introduces a Cross-Modal Temporal Pattern Discovery (CTPD) framework to improve clinical outcome predictions by identifying and aligning temporal patterns across multimodal EHR data.", "motivation": "Prior work overlooked critical temporal patterns across patients in multimodal EHR data, which are essential for accurate clinical outcome predictions.", "method": "The CTPD framework uses shared initial temporal pattern representations refined with slot attention and employs a contrastive-based TPNCE loss for cross-modal alignment, along with reconstruction losses.", "result": "Evaluations on MIMIC-III for mortality and phenotype classification tasks show the method outperforms existing approaches.", "conclusion": "The CTPD framework effectively captures cross-modal temporal patterns, enhancing clinical prediction accuracy."}}
{"id": "2410.06128", "pdf": "https://arxiv.org/pdf/2410.06128", "abs": "https://arxiv.org/abs/2410.06128", "authors": ["Divyat Mahajan", "Jannes Gladrow", "Agrin Hilmkil", "Cheng Zhang", "Meyer Scetbon"], "title": "Amortized Inference of Causal Models via Conditional Fixed-Point Iterations", "categories": ["cs.LG", "stat.ML"], "comment": "Preprint. Under Review", "summary": "Structural Causal Models (SCMs) offer a principled framework to reason about\ninterventions and support out-of-distribution generalization, which are key\ngoals in scientific discovery. However, the task of learning SCMs from observed\ndata poses formidable challenges, and often requires training a separate model\nfor each dataset. In this work, we propose amortized inference of SCMs by\ntraining a single model on multiple datasets sampled from different SCMs. We\nfirst use a transformer-based architecture for amortized learning of dataset\nembeddings, and then extend the Fixed-Point Approach (FiP) (Scetbon et al.) to\ninfer SCMs conditionally on their dataset embeddings. As a byproduct, our\nmethod can generate observational and interventional data from novel SCMs at\ninference time, without updating parameters. Empirical results show that our\namortized procedure performs on par with baselines trained specifically for\neach dataset on both in and out-of-distribution problems, and also outperforms\nthem in scare data regimes.", "AI": {"tldr": "Proposes amortized inference for learning Structural Causal Models (SCMs) using a transformer-based architecture, enabling generalization across datasets without retraining.", "motivation": "Learning SCMs from observed data is challenging and typically requires per-dataset training. This work aims to simplify and generalize the process.", "method": "Uses a transformer for dataset embeddings and extends the Fixed-Point Approach (FiP) to infer SCMs conditionally on embeddings.", "result": "Matches or outperforms per-dataset baselines, especially in scarce data scenarios, and can generate new data without parameter updates.", "conclusion": "Amortized inference is effective for SCM learning, offering scalability and generalization benefits."}}
{"id": "2503.12348", "pdf": "https://arxiv.org/pdf/2503.12348", "abs": "https://arxiv.org/abs/2503.12348", "authors": ["Mo Zhou", "Jianwei Wang", "Xuanmeng Zhang", "Dylan Campbell", "Kai Wang", "Long Yuan", "Wenjie Zhang", "Xuemin Lin"], "title": "ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation", "categories": ["cs.CV"], "comment": "18 pages, 13 figures, accepted by Frontiers of Computer Science (FCS)", "summary": "This paper studies optical flow estimation, a critical task in motion\nanalysis with applications in autonomous navigation, action recognition, and\nfilm production. Traditional optical flow methods require consecutive frames,\nwhich are often unavailable due to limitations in data acquisition or\nreal-world scene disruptions. Thus, single-frame optical flow estimation is\nemerging in the literature. However, existing single-frame approaches suffer\nfrom two major limitations: (1) they rely on labeled training data, making them\ntask-specific, and (2) they produce deterministic predictions, failing to\ncapture motion uncertainty. To overcome these challenges, we propose\nProbDiffFlow, a training-free framework that estimates optical flow\ndistributions from a single image. Instead of directly predicting motion,\nProbDiffFlow follows an estimation-by-synthesis paradigm: it first generates\ndiverse plausible future frames using a diffusion-based model, then estimates\nmotion from these synthesized samples using a pre-trained optical flow model,\nand finally aggregates the results into a probabilistic flow distribution. This\ndesign eliminates the need for task-specific training while capturing multiple\nplausible motions. Experiments on both synthetic and real-world datasets\ndemonstrate that ProbDiffFlow achieves superior accuracy, diversity, and\nefficiency, outperforming existing single-image and two-frame baselines.", "AI": {"tldr": "ProbDiffFlow is a training-free framework for single-frame optical flow estimation, addressing limitations of existing methods by generating diverse future frames and aggregating results into a probabilistic flow distribution.", "motivation": "Traditional optical flow methods require consecutive frames, which are often unavailable. Single-frame approaches rely on labeled data and produce deterministic predictions, failing to capture motion uncertainty.", "method": "ProbDiffFlow uses a diffusion-based model to generate diverse future frames, estimates motion from these using a pre-trained optical flow model, and aggregates results into a probabilistic flow distribution.", "result": "Experiments show ProbDiffFlow outperforms existing methods in accuracy, diversity, and efficiency on synthetic and real-world datasets.", "conclusion": "ProbDiffFlow provides a robust, training-free solution for single-frame optical flow estimation, capturing motion uncertainty and eliminating the need for task-specific training."}}
{"id": "2506.07044", "pdf": "https://arxiv.org/pdf/2506.07044", "abs": "https://arxiv.org/abs/2506.07044", "authors": ["LASA Team", "Weiwen Xu", "Hou Pong Chan", "Long Li", "Mahani Aljunied", "Ruifeng Yuan", "Jianyu Wang", "Chenghao Xiao", "Guizhen Chen", "Chaoqun Liu", "Zhaodonghui Li", "Yu Sun", "Junao Shen", "Chaojun Wang", "Jie Tan", "Deli Zhao", "Tingyang Xu", "Hao Zhang", "Yu Rong"], "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Technical Report, 53 pages, 25 tables, and 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...", "AI": {"tldr": "The paper addresses limitations of Multimodal Large Language Models (MLLMs) in medical applications by proposing a data curation procedure, introducing the medical-specialized MLLM Lingshu, and developing MedEvalKit for evaluation.", "motivation": "Existing medical MLLMs lack coverage of medical knowledge beyond imaging, are prone to hallucinations, and lack reasoning for complex medical scenarios.", "method": "Proposes a data curation procedure for rich medical knowledge, introduces Lingshu (a medical-specialized MLLM), and uses reinforcement learning for reasoning. Develops MedEvalKit for evaluation.", "result": "Lingshu outperforms existing open-source multimodal models on tasks like multimodal QA, text-based QA, and medical report generation.", "conclusion": "The proposed approach enhances medical MLLMs by addressing data and reasoning limitations, demonstrating superior performance in medical tasks."}}
{"id": "2411.04525", "pdf": "https://arxiv.org/pdf/2411.04525", "abs": "https://arxiv.org/abs/2411.04525", "authors": ["Pavel Sulimov", "Claude Lehmann", "Kurt Stockinger"], "title": "GenJoin: Conditional Generative Plan-to-Plan Query Optimizer that Learns from Subplan Hints", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Query optimization has become a research area where classical algorithms are\nbeing challenged by machine learning algorithms. At the same time, recent\ntrends in learned query optimizers have shown that it is prudent to take\nadvantage of decades of database research and augment classical query\noptimizers by shrinking the plan search space through different types of hints\n(e.g. by specifying the join type, scan type or the order of joins) rather than\ncompletely replacing the classical query optimizer with machine learning\nmodels. It is especially relevant for cases when classical optimizers cannot\nfully enumerate all logical and physical plans and, as an alternative, need to\nrely on less robust approaches like genetic algorithms. However, even\nsymbiotically learned query optimizers are hampered by the need for vast\namounts of training data, slow plan generation during inference and unstable\nresults across various workload conditions. In this paper, we present GenJoin -\na novel learned query optimizer that considers the query optimization problem\nas a generative task and is capable of learning from a random set of subplan\nhints to produce query plans that outperform the classical optimizer. GenJoin\nis the first learned query optimizer that significantly and consistently\noutperforms PostgreSQL as well as state-of-the-art methods on two well-known\nreal-world benchmarks across a variety of workloads using rigorous machine\nlearning evaluations.", "AI": {"tldr": "GenJoin is a learned query optimizer that outperforms classical optimizers by treating query optimization as a generative task, using subplan hints for training.", "motivation": "Classical query optimizers struggle with plan enumeration and robustness, while learned optimizers face data and stability challenges. GenJoin aims to address these gaps.", "method": "GenJoin treats query optimization as a generative task, learning from random subplan hints to produce superior query plans.", "result": "GenJoin consistently outperforms PostgreSQL and state-of-the-art methods on real-world benchmarks across diverse workloads.", "conclusion": "GenJoin demonstrates the potential of learned query optimizers to enhance classical methods, offering stable and superior performance."}}
{"id": "2410.08198", "pdf": "https://arxiv.org/pdf/2410.08198", "abs": "https://arxiv.org/abs/2410.08198", "authors": ["Shuo Xie", "Mohamad Amin Mohamadi", "Zhiyuan Li"], "title": "Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity", "categories": ["cs.LG"], "comment": null, "summary": "Adam outperforms SGD when training language models. Yet this advantage is not\nwell-understood theoretically -- previous convergence analysis for Adam and SGD\nmainly focuses on the number of steps $T$ and is already minimax-optimal in\nnon-convex cases, which are both $\\widetilde{O}(T^{-1/4})$. In this work, we\nargue that the exploitation of nice $\\ell_\\infty$-geometry is the key advantage\nof Adam over SGD. More specifically, we give a new convergence analysis for\nAdam under novel assumptions that loss is smooth under $\\ell_\\infty$-geometry\nrather than the more common $\\ell_2$-geometry, which yields a much better\nempirical smoothness constant for GPT-2 and ResNet models. Our experiments\nconfirm that Adam performs much worse when the favorable $\\ell_\\infty$-geometry\nis changed while SGD provably remains unaffected. We also extend the\nconvergence analysis to blockwise Adam under novel blockwise smoothness\nassumptions.", "AI": {"tldr": "Adam's advantage over SGD in training language models is due to its exploitation of favorable \u2113\u221e-geometry, unlike SGD which relies on \u21132-geometry.", "motivation": "The theoretical understanding of why Adam outperforms SGD in training language models is unclear, as existing convergence analyses for both methods are already minimax-optimal in non-convex cases.", "method": "The paper provides a new convergence analysis for Adam under novel assumptions of smoothness in \u2113\u221e-geometry, extending this to blockwise Adam with blockwise smoothness assumptions.", "result": "Adam's performance is superior under \u2113\u221e-geometry, but worsens when this geometry is altered, while SGD remains unaffected. Empirical results confirm this for GPT-2 and ResNet models.", "conclusion": "The key advantage of Adam over SGD lies in its ability to exploit favorable \u2113\u221e-geometry, offering better convergence under specific smoothness conditions."}}
{"id": "2503.17132", "pdf": "https://arxiv.org/pdf/2503.17132", "abs": "https://arxiv.org/abs/2503.17132", "authors": ["Siyuan Yang", "Shilin Lu", "Shizheng Wang", "Meng Hwa Er", "Zengwei Zheng", "Alex C. Kot"], "title": "Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.NE"], "comment": null, "summary": "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.", "AI": {"tldr": "The paper introduces two SNN frameworks (TS-SNN and 3D-SNN) for event-based human action recognition, addressing long-term temporal processing limitations and outperforming existing methods on new and existing datasets.", "motivation": "To enhance privacy-preserving human action recognition by leveraging the synergy between SNNs and event cameras, overcoming SNNs' limitations in processing long-term temporal data.", "method": "Proposes TS-SNN (temporal segment-based) and 3D-SNN (3D convolutional) frameworks to extract and process long-term temporal information. Introduces a new dataset, FallingDetection-CeleX.", "result": "The frameworks outperform state-of-the-art SNN methods on the new dataset and three other neuromorphic datasets, demonstrating improved handling of long-range temporal data.", "conclusion": "The proposed SNN frameworks effectively address temporal processing challenges in event-based HAR, offering superior performance and enabling further research with the new dataset."}}
{"id": "2506.07664", "pdf": "https://arxiv.org/pdf/2506.07664", "abs": "https://arxiv.org/abs/2506.07664", "authors": ["Lei Xu", "Sirui Chen", "Yuxuan Huang", "Chaochao Lu"], "title": "Synthesis by Design: Controlled Data Generation via Structural Guidance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mathematical reasoning remains challenging for LLMs due to complex logic and\nthe need for precise computation. Existing methods enhance LLM reasoning by\nsynthesizing datasets through problem rephrasing, but face issues with\ngeneration quality and problem complexity. To address this, we propose to\nextract structural information with generated problem-solving code from\nmathematical reasoning and guide data generation with structured solutions.\nApplied to MATH and GSM8K, our approach produces 39K problems with labeled\nintermediate steps and a 6.1K-problem benchmark of higher difficulty. Results\non our benchmark show that model performance declines as reasoning length\nincreases. Additionally, we conducted fine-tuning experiments using the\nproposed training data on a range of LLMs, and the results validate the\neffectiveness of our dataset. We hope the proposed method and dataset will\ncontribute to future research in enhancing LLM reasoning capabilities. Our code\nand data are available at https://github.com/OpenCausaLab/StructuralGeneration.", "AI": {"tldr": "Proposes a method to enhance LLM mathematical reasoning by generating structured problem-solving code and labeled intermediate steps, creating a high-quality dataset and benchmark.", "motivation": "Challenges in LLM mathematical reasoning due to complex logic and precise computation needs; existing methods lack quality and complexity.", "method": "Extracts structural information with generated problem-solving code to guide data generation, applied to MATH and GSM8K datasets.", "result": "Produces 39K problems with labeled steps and a 6.1K-problem benchmark; model performance declines with longer reasoning. Fine-tuning validates dataset effectiveness.", "conclusion": "The method and dataset improve LLM reasoning, with potential for future research. Code and data are publicly available."}}
{"id": "2412.08258", "pdf": "https://arxiv.org/pdf/2412.08258", "abs": "https://arxiv.org/abs/2412.08258", "authors": ["Tanay Aggarwal", "Angelo Salatino", "Francesco Osborne", "Enrico Motta"], "title": "Large Language Models for Scholarly Ontology Generation: An Extensive Analysis in the Engineering Field", "categories": ["cs.DL", "cs.AI", "cs.IR"], "comment": "Now accepted to Information Processing & Management. this is the\n  camera ready", "summary": "Ontologies of research topics are crucial for structuring scientific\nknowledge, enabling scientists to navigate vast amounts of research, and\nforming the backbone of intelligent systems such as search engines and\nrecommendation systems. However, manual creation of these ontologies is\nexpensive, slow, and often results in outdated and overly general\nrepresentations. As a solution, researchers have been investigating ways to\nautomate or semi-automate the process of generating these ontologies. This\npaper offers a comprehensive analysis of the ability of large language models\n(LLMs) to identify semantic relationships between different research topics,\nwhich is a critical step in the development of such ontologies. To this end, we\ndeveloped a gold standard based on the IEEE Thesaurus to evaluate the task of\nidentifying four types of relationships between pairs of topics: broader,\nnarrower, same-as, and other. Our study evaluates the performance of seventeen\nLLMs, which differ in scale, accessibility (open vs. proprietary), and model\ntype (full vs. quantised), while also assessing four zero-shot reasoning\nstrategies. Several models have achieved outstanding results, including\nMixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,\n0.920, and 0.967, respectively. Furthermore, our findings demonstrate that\nsmaller, quantised models, when optimised through prompt engineering, can\ndeliver performance comparable to much larger proprietary models, while\nrequiring significantly fewer computational resources.", "AI": {"tldr": "The paper evaluates LLMs for automating ontology creation by identifying semantic relationships between research topics, finding some models achieve high accuracy with optimized prompts.", "motivation": "Manual ontology creation is costly and slow; automating it using LLMs can improve efficiency and accuracy.", "method": "Developed a gold standard using the IEEE Thesaurus to evaluate 17 LLMs on identifying four relationship types between topics.", "result": "Top-performing models (e.g., Claude 3 Sonnet) achieved high F1-scores, and smaller quantised models matched larger ones with prompt optimization.", "conclusion": "LLMs, especially with prompt engineering, can effectively automate ontology creation, offering a scalable and resource-efficient solution."}}
{"id": "2410.13287", "pdf": "https://arxiv.org/pdf/2410.13287", "abs": "https://arxiv.org/abs/2410.13287", "authors": ["Xiaoyan Hu", "Ho-fung Leung", "Farzan Farnia"], "title": "An Online Learning Approach to Prompt-based Selection of Generative Models and LLMs", "categories": ["cs.LG"], "comment": "accepted to ICML 2025", "summary": "Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select.", "AI": {"tldr": "The paper proposes an online learning framework (PAK-UCB) to dynamically select the best generative model for different text prompts, improving efficiency by avoiding sub-optimal model queries.", "motivation": "Current methods select generative models based on averaged scores, ignoring prompt-specific performance variations, leading to inefficiency.", "method": "PAK-UCB, a contextual bandit algorithm with shared context variables, uses kernel-based functions and RFF for faster online learning.", "result": "Experiments show PAK-UCB effectively identifies the best generative model for diverse prompts in text-to-image and image-to-text tasks.", "conclusion": "The framework reduces costs and improves performance by adapting model selection to prompt-specific needs."}}
{"id": "2504.11171", "pdf": "https://arxiv.org/pdf/2504.11171", "abs": "https://arxiv.org/abs/2504.11171", "authors": ["Johannes Jakubik", "Felix Yang", "Benedikt Blumenstiel", "Erik Scheurer", "Rocco Sedona", "Stefano Maurogiovanni", "Jente Bosmans", "Nikolaos Dionelis", "Valerio Marsocci", "Niklas Kopp", "Rahul Ramachandran", "Paolo Fraccaro", "Thomas Brunschwiler", "Gabriele Cavallaro", "Juan Bernabe-Moreno", "Nicolas Long\u00e9p\u00e9"], "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code are open-sourced under a permissive license.", "AI": {"tldr": "TerraMind is a multimodal foundation model for Earth observation, combining token-level and pixel-level data for cross-modal learning. It introduces 'Thinking-in-Modalities' (TiM) and achieves state-of-the-art performance in EO benchmarks.", "motivation": "To create a versatile, any-to-any generative model for Earth observation that leverages dual-scale representations for improved performance and flexibility.", "method": "Pretrained on nine geospatial modalities using a dual-scale early fusion approach, combining token-level and pixel-level data. Introduces TiM for generating artificial data during finetuning and inference.", "result": "Achieves beyond state-of-the-art performance in benchmarks like PANGAEA and enables zero-shot and few-shot applications.", "conclusion": "TerraMind is a groundbreaking model for EO, offering advanced capabilities and open-sourced resources for broader use."}}
{"id": "2506.07751", "pdf": "https://arxiv.org/pdf/2506.07751", "abs": "https://arxiv.org/abs/2506.07751", "authors": ["Silin Gao", "Antoine Bosselut", "Samy Bengio", "Emmanuel Abbe"], "title": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking", "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Under review", "summary": "Recent studies have shown that large language models (LLMs), especially\nsmaller ones, often lack robustness in their reasoning. I.e., they tend to\nexperience performance drops when faced with distribution shifts, such as\nchanges to numerical or nominal variables, or insertions of distracting\nclauses. A possible strategy to address this involves generating synthetic data\nto further \"instantiate\" reasoning problems on potential variations. In\ncontrast, our approach focuses on \"abstracting\" reasoning problems. This not\nonly helps counteract distribution shifts but also facilitates the connection\nto symbolic tools for deriving solutions. We find that this abstraction process\nis better acquired through reinforcement learning (RL) than just supervised\nfine-tuning, which often fails to produce faithful abstractions. Our method,\nAbstRaL -- which promotes abstract reasoning in LLMs using RL on granular\nabstraction data -- significantly mitigates performance degradation on recent\nGSM perturbation benchmarks.", "AI": {"tldr": "The paper introduces AbstRaL, a method using reinforcement learning to improve abstract reasoning in LLMs, countering distribution shifts and enhancing robustness.", "motivation": "Smaller LLMs often struggle with robustness under distribution shifts. The paper aims to address this by abstracting reasoning problems rather than generating synthetic data.", "method": "The proposed method, AbstRaL, uses reinforcement learning to train LLMs on granular abstraction data, promoting faithful abstractions.", "result": "AbstRaL significantly reduces performance degradation on GSM perturbation benchmarks compared to supervised fine-tuning.", "conclusion": "Abstracting reasoning problems via RL (AbstRaL) is more effective than synthetic data generation for improving LLM robustness under distribution shifts."}}
{"id": "2501.00829", "pdf": "https://arxiv.org/pdf/2501.00829", "abs": "https://arxiv.org/abs/2501.00829", "authors": ["Haoxiang Tian", "Xingshuo Han", "Guoquan Wu", "An Guo", "Yuan Zhou. Jie Zhang", "Shuo Li", "Jun Wei", "Tianwei Zhang"], "title": "An LLM-Empowered Adaptive Evolutionary Algorithm For Multi-Component Deep Learning Systems", "categories": ["cs.NE", "cs.AI"], "comment": "9", "summary": "Multi-objective evolutionary algorithms (MOEAs) are widely used for searching\noptimal solutions in complex multi-component applications. Traditional MOEAs\nfor multi-component deep learning (MCDL) systems face challenges in enhancing\nthe search efficiency while maintaining the diversity. To combat these, this\npaper proposes $\\mu$MOEA, the first LLM-empowered adaptive evolutionary search\nalgorithm to detect safety violations in MCDL systems. Inspired by the\ncontext-understanding ability of Large Language Models (LLMs), $\\mu$MOEA\npromotes the LLM to comprehend the optimization problem and generate an initial\npopulation tailed to evolutionary objectives. Subsequently, it employs adaptive\nselection and variation to iteratively produce offspring, balancing the\nevolutionary efficiency and diversity. During the evolutionary process, to\nnavigate away from the local optima, $\\mu$MOEA integrates the evolutionary\nexperience back into the LLM. This utilization harnesses the LLM's quantitative\nreasoning prowess to generate differential seeds, breaking away from current\noptimal solutions. We evaluate $\\mu$MOEA in finding safety violations of MCDL\nsystems, and compare its performance with state-of-the-art MOEA methods.\nExperimental results show that $\\mu$MOEA can significantly improve the\nefficiency and diversity of the evolutionary search.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2410.15777", "pdf": "https://arxiv.org/pdf/2410.15777", "abs": "https://arxiv.org/abs/2410.15777", "authors": ["Marcin Sendera", "Amin Sorkhei", "Tomasz Ku\u015bmierczyk"], "title": "Revisiting the Equivalence of Bayesian Neural Networks and Gaussian Processes: On the Importance of Learning Activations", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to the 41st Conference on Uncertainty in Artificial\n  Intelligence (UAI 2025). PMLR 244", "summary": "Gaussian Processes (GPs) provide a convenient framework for specifying\nfunction-space priors, making them a natural choice for modeling uncertainty.\nIn contrast, Bayesian Neural Networks (BNNs) offer greater scalability and\nextendability but lack the advantageous properties of GPs. This motivates the\ndevelopment of BNNs capable of replicating GP-like behavior. However, existing\nsolutions are either limited to specific GP kernels or rely on heuristics.\n  We demonstrate that trainable activations are crucial for effective mapping\nof GP priors to wide BNNs. Specifically, we leverage the closed-form\n2-Wasserstein distance for efficient gradient-based optimization of\nreparameterized priors and activations. Beyond learned activations, we also\nintroduce trainable periodic activations that ensure global stationarity by\ndesign, and functional priors conditioned on GP hyperparameters to allow\nefficient model selection.\n  Empirically, our method consistently outperforms existing approaches or\nmatches performance of the heuristic methods, while offering stronger\ntheoretical foundations.", "AI": {"tldr": "The paper proposes a method to make Bayesian Neural Networks (BNNs) mimic Gaussian Processes (GPs) by using trainable activations and closed-form optimization, outperforming existing solutions.", "motivation": "BNNs lack GP-like properties, such as uncertainty modeling, despite their scalability. The goal is to bridge this gap.", "method": "Uses trainable activations and closed-form 2-Wasserstein distance for optimization. Introduces periodic activations and functional priors for GP hyperparameters.", "result": "Outperforms or matches heuristic methods with stronger theoretical foundations.", "conclusion": "The approach successfully combines BNN scalability with GP-like behavior, offering a robust solution."}}
{"id": "2504.18756", "pdf": "https://arxiv.org/pdf/2504.18756", "abs": "https://arxiv.org/abs/2504.18756", "authors": ["Rezowan Shuvo", "M S Mekala", "Eyad Elyan"], "title": "Multi-Stage Boundary-Aware Transformer Network for Action Segmentation in Untrimmed Surgical Videos", "categories": ["cs.CV"], "comment": null, "summary": "Understanding actions within surgical workflows is critical for evaluating\npost-operative outcomes and enhancing surgical training and efficiency.\nCapturing and analyzing long sequences of actions in surgical settings is\nchallenging due to the inherent variability in individual surgeon approaches,\nwhich are shaped by their expertise and preferences. This variability\ncomplicates the identification and segmentation of distinct actions with\nambiguous boundary start and end points. The traditional models, such as\nMS-TCN, which rely on large receptive fields, that causes over-segmentation, or\nunder-segmentation, where distinct actions are incorrectly aligned. To address\nthese challenges, we propose the Multi-Stage Boundary-Aware Transformer Network\n(MSBATN) with hierarchical sliding window attention to improve action\nsegmentation. Our approach effectively manages the complexity of varying action\ndurations and subtle transitions by accurately identifying start and end action\nboundaries in untrimmed surgical videos. MSBATN introduces a novel unified loss\nfunction that optimises action classification and boundary detection as\ninterconnected tasks. Unlike conventional binary boundary detection methods,\nour innovative boundary weighing mechanism leverages contextual information to\nprecisely identify action boundaries. Extensive experiments on three\nchallenging surgical datasets demonstrate that MSBATN achieves state-of-the-art\nperformance, with superior F1 scores at 25% and 50%. thresholds and competitive\nresults across other metrics.", "AI": {"tldr": "Proposes MSBATN, a transformer-based model with hierarchical sliding window attention, to improve surgical action segmentation by addressing boundary ambiguity and variability in surgeon approaches.", "motivation": "Variability in surgeon approaches and ambiguous action boundaries complicate surgical workflow analysis, necessitating better segmentation methods.", "method": "Uses Multi-Stage Boundary-Aware Transformer Network (MSBATN) with hierarchical sliding window attention and a unified loss function for action classification and boundary detection.", "result": "Achieves state-of-the-art performance with superior F1 scores at 25% and 50% thresholds on three surgical datasets.", "conclusion": "MSBATN effectively addresses challenges in surgical action segmentation, offering precise boundary detection and improved performance."}}
{"id": "2506.08174", "pdf": "https://arxiv.org/pdf/2506.08174", "abs": "https://arxiv.org/abs/2506.08174", "authors": ["Li Weigang", "Pedro Carvalho Brom"], "title": "LLM-BT-Terms: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding", "categories": ["cs.CL"], "comment": "23 pages", "summary": "The rapid expansion of English technical terminology presents a significant\nchallenge to traditional expert-based standardization, particularly in rapidly\ndeveloping areas such as artificial intelligence and quantum computing. Manual\napproaches face difficulties in maintaining consistent multilingual\nterminology. To address this, we introduce LLM-BT, a back-translation framework\npowered by large language models (LLMs) designed to automate terminology\nverification and standardization through cross-lingual semantic alignment. Our\nkey contributions include: (1) term-level consistency validation: by performing\nEnglish -> intermediate language -> English back-translation, LLM-BT achieves\nhigh term consistency across different models (such as GPT-4, DeepSeek, and\nGrok). Case studies demonstrate over 90 percent of terms are preserved either\nexactly or semantically; (2) multi-path verification workflow: we develop a\nnovel pipeline described as Retrieve -> Generate -> Verify -> Optimize, which\nsupports both serial paths (e.g., English -> Simplified Chinese -> Traditional\nChinese -> English) and parallel paths (e.g., English -> Chinese / Portuguese\n-> English). BLEU scores and term-level accuracy indicate strong cross-lingual\nrobustness, with BLEU scores exceeding 0.45 and Portuguese term accuracy\nreaching 100 percent; (3) back-translation as semantic embedding: we\nreinterpret back-translation as a form of dynamic semantic embedding that\nuncovers latent trajectories of meaning. In contrast to static embeddings,\nLLM-BT offers transparent, path-based embeddings shaped by the evolution of the\nmodels. This reframing positions back-translation as an active mechanism for\nmultilingual terminology standardization, fostering collaboration between\nmachines and humans - machines preserve semantic integrity, while humans\nprovide cultural interpretation.", "AI": {"tldr": "LLM-BT automates multilingual terminology standardization using LLM-powered back-translation, ensuring high term consistency and cross-lingual robustness.", "motivation": "The rapid expansion of technical terms in fields like AI and quantum computing challenges manual standardization, requiring automated solutions for consistency.", "method": "LLM-BT uses back-translation (English -> intermediate language -> English) and a multi-path workflow (Retrieve -> Generate -> Verify -> Optimize) for term validation.", "result": "Achieves over 90% term preservation, BLEU scores >0.45, and 100% Portuguese term accuracy.", "conclusion": "LLM-BT redefines back-translation as dynamic semantic embedding, enabling machine-human collaboration for multilingual standardization."}}
{"id": "2502.02221", "pdf": "https://arxiv.org/pdf/2502.02221", "abs": "https://arxiv.org/abs/2502.02221", "authors": ["Ji\u0159\u00ed N\u011bme\u010dek", "Mark Kozdoba", "Illia Kryvoviaz", "Tom\u00e1\u0161 Pevn\u00fd", "Jakub Mare\u010dek"], "title": "Bias Detection via Maximum Subgroup Discrepancy", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "12 pages, 6 figures", "summary": "Bias evaluation is fundamental to trustworthy AI, both in terms of checking\ndata quality and in terms of checking the outputs of AI systems. In testing\ndata quality, for example, one may study the distance of a given dataset,\nviewed as a distribution, to a given ground-truth reference dataset. However,\nclassical metrics, such as the Total Variation and the Wasserstein distances,\nare known to have high sample complexities and, therefore, may fail to provide\na meaningful distinction in many practical scenarios.\n  In this paper, we propose a new notion of distance, the Maximum Subgroup\nDiscrepancy (MSD). In this metric, two distributions are close if, roughly,\ndiscrepancies are low for all feature subgroups. While the number of subgroups\nmay be exponential, we show that the sample complexity is linear in the number\nof features, thus making it feasible for practical applications. Moreover, we\nprovide a practical algorithm for evaluating the distance based on\nMixed-integer optimization (MIO). We also note that the proposed distance is\neasily interpretable, thus providing clearer paths to fixing the biases once\nthey have been identified. Finally, we describe a natural general bias\ndetection framework, termed MSDD distances, and show that MSD aligns well with\nthis framework. We empirically evaluate MSD by comparing it with other metrics\nand by demonstrating the above properties of MSD on real-world datasets.", "AI": {"tldr": "The paper introduces Maximum Subgroup Discrepancy (MSD), a new distance metric for bias evaluation in AI, with linear sample complexity and interpretability, outperforming classical metrics.", "motivation": "Classical bias evaluation metrics like Total Variation and Wasserstein distances have high sample complexity, limiting practical use. A more efficient and interpretable metric is needed.", "method": "Proposes MSD, a distance metric focusing on subgroup discrepancies, with a linear sample complexity. Uses Mixed-integer optimization (MIO) for practical evaluation.", "result": "MSD shows lower sample complexity, interpretability, and aligns with a general bias detection framework (MSDD). Empirical tests confirm its effectiveness.", "conclusion": "MSD is a practical and interpretable metric for bias evaluation, addressing limitations of classical methods and enhancing AI trustworthiness."}}
{"id": "2410.17194", "pdf": "https://arxiv.org/pdf/2410.17194", "abs": "https://arxiv.org/abs/2410.17194", "authors": ["Kento Nishi", "Rahul Ramesh", "Maya Okawa", "Mikail Khona", "Hidenori Tanaka", "Ekdeep Singh Lubana"], "title": "Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Knowledge Editing (KE) algorithms alter models' weights to perform targeted\nupdates to incorrect, outdated, or otherwise unwanted factual associations.\nHowever, recent work has shown that applying KE can adversely affect models'\nbroader factual recall accuracy and diminish their reasoning abilities.\nAlthough these studies give insights into the potential harms of KE algorithms,\ne.g., performance evaluations on benchmarks, little is understood about why\nsuch destructive failures occur. Motivated by this, we define a novel synthetic\ntask in which a Transformer is trained from scratch to internalize a\n\"structured\" knowledge graph. The structure enforces relationships between\nentities of the graph, such that editing a factual association has \"trickling\neffects\" on other entities (e.g., altering X's parent is Y to Z affects who X's\nsiblings' parent is). Through evaluations of edited models on this task, we\nshow that KE inadvertently affects representations of entities beyond the\ntargeted one, distorting relevant structures that allow a model to infer unseen\nknowledge about an entity. We call this phenomenon representation shattering\nand demonstrate that it degrades models' factual recall and reasoning\nperformance. We further corroborate our findings in naturalistic settings with\npre-trained Llama and Mamba models as well. Overall, our work yields a precise\nmechanistic hypothesis to explain why KE has adverse effects on model\nabilities.", "AI": {"tldr": "Knowledge Editing (KE) algorithms can harm models' factual recall and reasoning. A synthetic task reveals KE distorts entity representations, termed 'representation shattering,' explaining KE's adverse effects.", "motivation": "Recent studies show KE harms models' broader abilities but lack understanding of why. This work aims to uncover the mechanistic reasons behind KE's destructive failures.", "method": "A synthetic task trains a Transformer on a structured knowledge graph to study KE's effects, revealing 'representation shattering'\u2014distortion of entity relationships beyond targeted edits.", "result": "KE distorts representations of related entities, degrading factual recall and reasoning. Findings are validated with pre-trained Llama and Mamba models.", "conclusion": "The study identifies 'representation shattering' as the cause of KE's adverse effects, providing a mechanistic explanation for its impact on model abilities."}}
{"id": "2505.01267", "pdf": "https://arxiv.org/pdf/2505.01267", "abs": "https://arxiv.org/abs/2505.01267", "authors": ["Gaozheng Pei", "Ke Ma", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "title": "Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain", "categories": ["cs.CV"], "comment": null, "summary": "The diffusion-based adversarial purification methods attempt to drown\nadversarial perturbations into a part of isotropic noise through the forward\nprocess, and then recover the clean images through the reverse process. Due to\nthe lack of distribution information about adversarial perturbations in the\npixel domain, it is often unavoidable to damage normal semantics. We turn to\nthe frequency domain perspective, decomposing the image into amplitude spectrum\nand phase spectrum. We find that for both spectra, the damage caused by\nadversarial perturbations tends to increase monotonically with frequency. This\nmeans that we can extract the content and structural information of the\noriginal clean sample from the frequency components that are less damaged.\nMeanwhile, theoretical analysis indicates that existing purification methods\nindiscriminately damage all frequency components, leading to excessive damage\nto the image. Therefore, we propose a purification method that can eliminate\nadversarial perturbations while maximizing the preservation of the content and\nstructure of the original image. Specifically, at each time step during the\nreverse process, for the amplitude spectrum, we replace the low-frequency\ncomponents of the estimated image's amplitude spectrum with the corresponding\nparts of the adversarial image. For the phase spectrum, we project the phase of\nthe estimated image into a designated range of the adversarial image's phase\nspectrum, focusing on the low frequencies. Empirical evidence from extensive\nexperiments demonstrates that our method significantly outperforms most current\ndefense methods.", "AI": {"tldr": "The paper proposes a frequency-domain adversarial purification method to better preserve image content and structure while removing perturbations.", "motivation": "Existing diffusion-based adversarial purification methods damage normal semantics due to lack of perturbation distribution information in the pixel domain.", "method": "Decomposes images into amplitude and phase spectra, focusing on less damaged low-frequency components to preserve content. Replaces low-frequency amplitude and projects phase in the reverse process.", "result": "The method outperforms current defense techniques by effectively eliminating perturbations while minimizing damage to the original image.", "conclusion": "Frequency-domain purification preserves image integrity better than pixel-domain methods, offering superior adversarial defense."}}
{"id": "2506.08184", "pdf": "https://arxiv.org/pdf/2506.08184", "abs": "https://arxiv.org/abs/2506.08184", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "title": "Unable to Forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.", "AI": {"tldr": "LLMs struggle with intra-context interference, where earlier information disrupts recall of newer updates, revealing a working memory bottleneck.", "motivation": "To study the under-researched effects of intra-context interference in LLMs and its impact on retrieval accuracy.", "method": "Adapts the proactive interference (PI) paradigm from cognitive science, introducing PI-LLM to evaluate LLMs by streaming semantically related updates and querying final values.", "result": "LLM retrieval accuracy declines log-linearly as interference accumulates, with errors from retrieving overwritten values. Prompt engineering mitigates interference poorly.", "conclusion": "LLMs face a fundamental constraint in disentangling interference, indicating a working memory bottleneck. Solutions should focus on suppressing irrelevant content during retrieval."}}
{"id": "2502.02747", "pdf": "https://arxiv.org/pdf/2502.02747", "abs": "https://arxiv.org/abs/2502.02747", "authors": ["Hongwei Li", "Yuheng Tang", "Shiqi Wang", "Wenbo Guo"], "title": "PatchPilot: A Cost-Efficient Software Engineering Agent with Early Attempts on Formal Verification", "categories": ["cs.RO", "cs.AI", "cs.CR"], "comment": null, "summary": "Recent research builds various patching agents that combine large language\nmodels (LLMs) with non-ML tools and achieve promising results on the\nstate-of-the-art (SOTA) software patching benchmark, SWE-bench. Based on how to\ndetermine the patching workflows, existing patching agents can be categorized\nas agent-based planning methods, which rely on LLMs for planning, and\nrule-based planning methods, which follow a pre-defined workflow. At a high\nlevel, agent-based planning methods achieve high patching performance but with\na high cost and limited stability. Rule-based planning methods, on the other\nhand, are more stable and efficient but have key workflow limitations that\ncompromise their patching performance. In this paper, we propose PatchPilot, an\nagentic patcher that strikes a balance between patching efficacy, stability,\nand cost-efficiency. PatchPilot proposes a novel rule-based planning workflow\nwith five components: reproduction, localization, generation, validation, and\nrefinement (where refinement is unique to PatchPilot). We introduce novel and\ncustomized designs to each component to optimize their effectiveness and\nefficiency. Through extensive experiments on the SWE-bench benchmarks,\nPatchPilot shows a superior performance than existing open-source methods while\nmaintaining low cost (less than 1$ per instance) and ensuring higher stability.\nWe also conduct a detailed ablation study to validate the key designs in each\ncomponent. Our code is available at https://github.com/ucsb-mlsec/PatchPilot.", "AI": {"tldr": "PatchPilot is a rule-based patching agent balancing efficacy, stability, and cost, outperforming existing methods on SWE-bench.", "motivation": "Existing patching agents either lack stability (agent-based) or performance (rule-based). PatchPilot aims to bridge this gap.", "method": "Introduces a novel rule-based workflow with five components: reproduction, localization, generation, validation, and refinement.", "result": "Outperforms open-source methods on SWE-bench with low cost (<$1 per instance) and high stability.", "conclusion": "PatchPilot effectively balances performance, stability, and cost, validated by ablation studies."}}
{"id": "2411.01357", "pdf": "https://arxiv.org/pdf/2411.01357", "abs": "https://arxiv.org/abs/2411.01357", "authors": ["Patrick Mesana", "Cl\u00e9ment B\u00e9nesse", "Hadrien Lautraite", "Gilles Caporossi", "S\u00e9bastien Gambs"], "title": "WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy Principles", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "In this paper, we introduce WaKA (Wasserstein K-nearest-neighbors\nAttribution), a novel attribution method that leverages principles from the\nLiRA (Likelihood Ratio Attack) framework and k-nearest neighbors classifiers\n(k-NN). WaKA efficiently measures the contribution of individual data points to\nthe model's loss distribution, analyzing every possible k-NN that can be\nconstructed using the training set, without requiring to sample subsets of the\ntraining set. WaKA is versatile and can be used a posteriori as a membership\ninference attack (MIA) to assess privacy risks or a priori for privacy\ninfluence measurement and data valuation. Thus, WaKA can be seen as bridging\nthe gap between data attribution and membership inference attack (MIA) by\nproviding a unified framework to distinguish between a data point's value and\nits privacy risk. For instance, we have shown that self-attribution values are\nmore strongly correlated with the attack success rate than the contribution of\na point to the model generalization. WaKA's different usage were also evaluated\nacross diverse real-world datasets, demonstrating performance very close to\nLiRA when used as an MIA on k-NN classifiers, but with greater computational\nefficiency. Additionally, WaKA shows greater robustness than Shapley Values for\ndata minimization tasks (removal or addition) on imbalanced datasets.", "AI": {"tldr": "WaKA is a novel attribution method combining LiRA and k-NN, measuring data point contributions to loss distribution without subset sampling. It bridges data attribution and MIA, showing strong correlation with attack success and computational efficiency.", "motivation": "To unify data attribution and membership inference attack (MIA) frameworks, providing a versatile tool for privacy risk assessment and data valuation.", "method": "Leverages LiRA and k-NN principles, analyzing all possible k-NN constructions from the training set without subset sampling.", "result": "Performs close to LiRA as an MIA but more efficiently; robust for data minimization on imbalanced datasets compared to Shapley Values.", "conclusion": "WaKA effectively bridges data attribution and MIA, offering computational efficiency and robustness for privacy and data valuation tasks."}}
{"id": "2505.04088", "pdf": "https://arxiv.org/pdf/2505.04088", "abs": "https://arxiv.org/abs/2505.04088", "authors": ["Shang Zhang", "Huanbin Zhang", "Dali Feng", "Yujie Cui", "Ruoyan Xiong", "Cen He"], "title": "SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared (TIR) object tracking often suffers from challenges such as\ntarget occlusion, motion blur, and background clutter, which significantly\ndegrade the performance of trackers. To address these issues, this paper\npro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a\nbidirectional state-space model and a self-attention mechanism. Specifically,\nwe introduce the Motion Mamba module into the Siamese architecture to ex-tract\nmotion features and recover overlooked edge details using bidirectional\nmodeling and self-attention. We propose a Siamese parameter-sharing strate-gy\nthat allows certain convolutional layers to share weights. This approach\nreduces computational redundancy while preserving strong feature\nrepresen-tation. In addition, we design a motion edge-aware regression loss to\nimprove tracking accuracy, especially for motion-blurred targets. Extensive\nexperi-ments are conducted on four TIR tracking benchmarks, including\nLSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT\nachieves superior performance in TIR target tracking.", "AI": {"tldr": "The paper proposes a Siamese Motion Mamba Tracker (SMMT) for TIR object tracking, addressing challenges like occlusion and motion blur with bidirectional state-space modeling and self-attention.", "motivation": "TIR object tracking faces issues like occlusion and motion blur, degrading tracker performance.", "method": "SMMT integrates bidirectional state-space modeling and self-attention, uses parameter-sharing, and introduces a motion edge-aware regression loss.", "result": "SMMT outperforms on benchmarks like LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017.", "conclusion": "SMMT achieves superior TIR tracking performance by effectively handling motion blur and occlusion."}}
{"id": "2506.08364", "pdf": "https://arxiv.org/pdf/2506.08364", "abs": "https://arxiv.org/abs/2506.08364", "authors": ["Jash Rajesh Parekh", "Pengcheng Jiang", "Jiawei Han"], "title": "CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs", "categories": ["cs.CL"], "comment": null, "summary": "Understanding cause and effect relationships remains a formidable challenge\nfor Large Language Models (LLMs), particularly in specialized domains where\nreasoning requires more than surface-level correlations. Retrieval-Augmented\nGeneration (RAG) improves factual accuracy, but standard RAG pipelines treat\nevidence as flat context, lacking the structure required to model true causal\ndependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that\nintegrates zero-shot triple extraction and theme-aware graph chaining into the\nRAG pipeline, enabling structured multi-hop inference. Given a domain specific\ncorpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,\neffect> triples and uses forward/backward chaining to guide structured answer\ngeneration. Experiments on two real-world domains: Bitcoin price fluctuations\nand Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot\nLLMs in chain similarity, information density, and lexical diversity. Both\nLLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results\ndemonstrate that explicitly modeling causal structure enables LLMs to generate\nmore accurate and interpretable responses, especially in specialized domains\nwhere flat retrieval fails.", "AI": {"tldr": "CC-RAG enhances RAG by modeling causal dependencies with structured triples and graph chaining, outperforming standard RAG and zero-shot LLMs in specialized domains.", "motivation": "Addressing LLMs' challenges in understanding cause-effect relationships in specialized domains where flat retrieval lacks causal structure.", "method": "Integrates zero-shot triple extraction and theme-aware graph chaining into RAG, constructing a DAG of causal triples for multi-hop inference.", "result": "Outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity, validated by LLM and human evaluations.", "conclusion": "Explicit causal modeling improves LLM accuracy and interpretability, especially in domains where flat retrieval fails."}}
{"id": "2502.05174", "pdf": "https://arxiv.org/pdf/2502.05174", "abs": "https://arxiv.org/abs/2502.05174", "authors": ["Kaijie Zhu", "Xianjun Yang", "Jindong Wang", "Wenbo Guo", "William Yang Wang"], "title": "MELON: Provable Defense Against Indirect Prompt Injection Attacks in AI Agents", "categories": ["cs.CR", "cs.AI"], "comment": "ICML 2025", "summary": "Recent research has explored that LLM agents are vulnerable to indirect\nprompt injection (IPI) attacks, where malicious tasks embedded in\ntool-retrieved information can redirect the agent to take unauthorized actions.\nExisting defenses against IPI have significant limitations: either require\nessential model training resources, lack effectiveness against sophisticated\nattacks, or harm the normal utilities. We present MELON (Masked re-Execution\nand TooL comparisON), a novel IPI defense. Our approach builds on the\nobservation that under a successful attack, the agent's next action becomes\nless dependent on user tasks and more on malicious tasks. Following this, we\ndesign MELON to detect attacks by re-executing the agent's trajectory with a\nmasked user prompt modified through a masking function. We identify an attack\nif the actions generated in the original and masked executions are similar. We\nalso include three key designs to reduce the potential false positives and\nfalse negatives. Extensive evaluation on the IPI benchmark AgentDojo\ndemonstrates that MELON outperforms SOTA defenses in both attack prevention and\nutility preservation. Moreover, we show that combining MELON with a SOTA prompt\naugmentation defense (denoted as MELON-Aug) further improves its performance.\nWe also conduct a detailed ablation study to validate our key designs. Code is\navailable at https://github.com/kaijiezhu11/MELON.", "AI": {"tldr": "MELON is a novel defense against indirect prompt injection (IPI) attacks on LLM agents, outperforming existing methods by detecting malicious tasks through masked re-execution and tool comparison.", "motivation": "Existing IPI defenses are resource-intensive, ineffective, or degrade utility, prompting the need for a better solution.", "method": "MELON detects attacks by re-executing the agent's trajectory with a masked user prompt and comparing actions from original and masked executions.", "result": "MELON outperforms state-of-the-art defenses on the IPI benchmark AgentDojo, and combining it with prompt augmentation (MELON-Aug) further improves performance.", "conclusion": "MELON is an effective and efficient IPI defense, validated by extensive evaluation and ablation studies."}}
{"id": "2501.02436", "pdf": "https://arxiv.org/pdf/2501.02436", "abs": "https://arxiv.org/abs/2501.02436", "authors": ["Yuchen Lin", "Yong Zhang", "Sihan Feng", "Hong Zhao"], "title": "Network Dynamics-Based Framework for Understanding Deep Neural Networks", "categories": ["cs.LG", "nlin.CD", "stat.ML"], "comment": "12 pages, 7 figures", "summary": "Advancements in artificial intelligence call for a deeper understanding of\nthe fundamental mechanisms underlying deep learning. In this work, we propose a\ntheoretical framework to analyze learning dynamics through the lens of\ndynamical systems theory. We redefine the notions of linearity and nonlinearity\nin neural networks by introducing two fundamental transformation units at the\nneuron level: order-preserving transformations and non-order-preserving\ntransformations. Different transformation modes lead to distinct collective\nbehaviors in weight vector organization, different modes of information\nextraction, and the emergence of qualitatively different learning phases.\nTransitions between these phases may occur during training, accounting for key\nphenomena such as grokking. To further characterize generalization and\nstructural stability, we introduce the concept of attraction basins in both\nsample and weight spaces. The distribution of neurons with different\ntransformation modes across layers, along with the structural characteristics\nof the two types of attraction basins, forms a set of core metrics for\nanalyzing the performance of learning models. Hyperparameters such as depth,\nwidth, learning rate, and batch size act as control variables for fine-tuning\nthese metrics. Our framework not only sheds light on the intrinsic advantages\nof deep learning, but also provides a novel perspective for optimizing network\narchitectures and training strategies.", "AI": {"tldr": "A theoretical framework analyzes deep learning dynamics using dynamical systems theory, introducing neuron-level transformations to explain learning phases and phenomena like grokking.", "motivation": "To understand the fundamental mechanisms of deep learning and explain key phenomena such as grokking.", "method": "Proposes a framework with two neuron-level transformations (order-preserving and non-order-preserving) to analyze learning dynamics and generalization.", "result": "Identifies distinct learning phases, transitions, and introduces attraction basins in sample and weight spaces for performance analysis.", "conclusion": "The framework provides insights into deep learning advantages and aids in optimizing network architectures and training strategies."}}
{"id": "2506.02459", "pdf": "https://arxiv.org/pdf/2506.02459", "abs": "https://arxiv.org/abs/2506.02459", "authors": ["Martin JJ. Bucher", "Iro Armeni"], "title": "ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment", "categories": ["cs.CV", "I.2.10; I.2.7"], "comment": "20 pages, 17 figures (incl. appendix)", "summary": "Scene synthesis and editing has emerged as a promising direction in computer\ngraphics. Current trained approaches for 3D indoor scenes either oversimplify\nobject semantics through one-hot class encodings (e.g., 'chair' or 'table'),\nrequire masked diffusion for editing, ignore room boundaries, or rely on floor\nplan renderings that fail to capture complex layouts. In contrast, LLM-based\nmethods enable richer semantics via natural language (e.g., 'modern studio with\nlight wood furniture') but do not support editing, remain limited to\nrectangular layouts or rely on weak spatial reasoning from implicit world\nmodels. We introduce ReSpace, a generative framework for text-driven 3D indoor\nscene synthesis and editing using autoregressive language models. Our approach\nfeatures a compact structured scene representation with explicit room\nboundaries that frames scene editing as a next-token prediction task. We\nleverage a dual-stage training approach combining supervised fine-tuning and\npreference alignment, enabling a specially trained language model for object\naddition that accounts for user instructions, spatial geometry, object\nsemantics, and scene-level composition. For scene editing, we employ a\nzero-shot LLM to handle object removal and prompts for addition. We further\nintroduce a novel voxelization-based evaluation that captures fine-grained\ngeometry beyond 3D bounding boxes. Experimental results surpass\nstate-of-the-art on object addition while maintaining competitive results on\nfull scene synthesis.", "AI": {"tldr": "ReSpace is a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models, outperforming state-of-the-art methods.", "motivation": "Current methods for 3D indoor scene synthesis and editing oversimplify semantics, lack editing support, or ignore spatial reasoning. ReSpace addresses these gaps.", "method": "Uses a compact structured scene representation with explicit room boundaries, dual-stage training (supervised fine-tuning and preference alignment), and zero-shot LLM for editing.", "result": "Surpasses state-of-the-art on object addition and maintains competitive performance in full scene synthesis.", "conclusion": "ReSpace offers a robust solution for text-driven 3D scene synthesis and editing, combining rich semantics, spatial reasoning, and user control."}}
{"id": "2506.08371", "pdf": "https://arxiv.org/pdf/2506.08371", "abs": "https://arxiv.org/abs/2506.08371", "authors": ["Zikai Xiao", "Ziyang Wang", "Wen Ma", "Yan Zhang", "Wei Shen", "Yan Wang", "Luqi Gong", "Zuozhu Liu"], "title": "Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) support long contexts, they struggle with\nperformance degradation within the context window. Current solutions incur\nprohibitive training costs, leaving statistical behaviors and cost-effective\napproaches underexplored. From the decoding perspective, we identify the\nPosterior Salience Attenuation (PSA) phenomenon, where the salience ratio\ncorrelates with long-text performance degradation. Notably, despite the\nattenuation, gold tokens still occupy high-ranking positions in the decoding\nspace. Motivated by it, we propose the training-free Positional Contrastive\nDecoding (PCD) that contrasts the logits derived from long-aware attention with\nthose from designed local-aware attention, enabling the model to focus on the\ngains introduced by large-scale short-to-long training. Through the analysis of\nlong-term decay simulation, we demonstrate that PCD effectively alleviates\nattention score degradation. Experimental results show that PCD achieves\nstate-of-the-art performance on long-context benchmarks.", "AI": {"tldr": "The paper addresses performance degradation in LLMs for long contexts, proposing a training-free method (PCD) to mitigate the issue by leveraging attention contrasts.", "motivation": "LLMs degrade in performance for long contexts, and current solutions are costly. The study explores statistical behaviors and cost-effective approaches.", "method": "Proposes Positional Contrastive Decoding (PCD), contrasting logits from long-aware and local-aware attention to focus on gains from short-to-long training.", "result": "PCD effectively alleviates attention score degradation and achieves state-of-the-art performance on long-context benchmarks.", "conclusion": "PCD offers a cost-effective solution to long-context performance issues in LLMs, demonstrating superior results."}}
{"id": "2502.09720", "pdf": "https://arxiv.org/pdf/2502.09720", "abs": "https://arxiv.org/abs/2502.09720", "authors": ["Semyon Savkin", "Eitan Porat", "Or Ordentlich", "Yury Polyanskiy"], "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": "23 pages", "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.", "AI": {"tldr": "NestQuant is a novel PTQ scheme using self-similar nested lattices, achieving superior efficiency in quantizing LLMs to 4 bits with minimal perplexity gap.", "motivation": "To address the need for efficient deployment of large language models (LLMs) through improved post-training quantization (PTQ) techniques.", "method": "Proposes NestQuant, a PTQ scheme based on self-similar nested lattices, implemented practically using Gosset lattice for low-complexity matrix multiplication.", "result": "Quantizes Llama-3-8B to 4 bits with a perplexity of 6.6, outperforming state-of-the-art methods like SpinQuant, OstQuant, and QuaRot.", "conclusion": "NestQuant demonstrates uniform superiority across larger models and benchmarks, making it a practical and efficient PTQ solution."}}
{"id": "2501.17737", "pdf": "https://arxiv.org/pdf/2501.17737", "abs": "https://arxiv.org/abs/2501.17737", "authors": ["Adrian Hill", "Guillaume Dalle"], "title": "Sparser, Better, Faster, Stronger: Sparsity Detection for Efficient Automatic Differentiation", "categories": ["cs.LG", "cs.MS"], "comment": "33 pages, 6 figures, 6 tables, 3 listings", "summary": "From implicit differentiation to probabilistic modeling, Jacobian and Hessian\nmatrices have many potential use cases in Machine Learning (ML), but they are\nviewed as computationally prohibitive. Fortunately, these matrices often\nexhibit sparsity, which can be leveraged to speed up the process of Automatic\nDifferentiation (AD). This paper presents advances in sparsity detection,\npreviously the performance bottleneck of Automatic Sparse Differentiation\n(ASD). Our implementation of sparsity detection is based on operator\noverloading, able to detect both local and global sparsity patterns, and\nsupports flexible index set representations. It is fully automatic and requires\nno modification of user code, making it compatible with existing ML codebases.\nMost importantly, it is highly performant, unlocking Jacobians and Hessians at\nscales where they were considered too expensive to compute. On real-world\nproblems from scientific ML, graph neural networks and optimization, we show\nsignificant speed-ups of up to three orders of magnitude. Notably, using our\nsparsity detection system, ASD outperforms standard AD for one-off\ncomputations, without amortization of either sparsity detection or matrix\ncoloring.", "AI": {"tldr": "The paper introduces an efficient sparsity detection method for Automatic Sparse Differentiation (ASD), enabling faster computation of Jacobian and Hessian matrices in ML, with significant speed-ups in real-world applications.", "motivation": "Jacobian and Hessian matrices are computationally expensive in ML, but their sparsity can be exploited for faster differentiation. Existing methods lack efficient sparsity detection, which this paper addresses.", "method": "The paper proposes an operator-overloading-based sparsity detection system that identifies local and global sparsity patterns without modifying user code, making it compatible with existing ML frameworks.", "result": "The implementation achieves up to three orders of magnitude speed-up in real-world ML tasks, outperforming standard AD for one-off computations.", "conclusion": "The method unlocks efficient computation of Jacobians and Hessians at previously prohibitive scales, advancing the practicality of ASD in ML."}}
{"id": "2506.02472", "pdf": "https://arxiv.org/pdf/2506.02472", "abs": "https://arxiv.org/abs/2506.02472", "authors": ["Halil Ismail Helvaci", "Justin Philip Huber", "Jihye Bae", "Sen-ching Samson Cheung"], "title": "HRTR: A Single-stage Transformer for Fine-grained Sub-second Action Segmentation in Stroke Rehabilitation", "categories": ["cs.CV"], "comment": null, "summary": "Stroke rehabilitation often demands precise tracking of patient movements to\nmonitor progress, with complexities of rehabilitation exercises presenting two\ncritical challenges: fine-grained and sub-second (under one-second) action\ndetection. In this work, we propose the High Resolution Temporal Transformer\n(HRTR), to time-localize and classify high-resolution (fine-grained),\nsub-second actions in a single-stage transformer, eliminating the need for\nmulti-stage methods and post-processing. Without any refinements, HRTR\noutperforms state-of-the-art systems on both stroke related and general\ndatasets, achieving Edit Score (ES) of 70.1 on StrokeRehab Video, 69.4 on\nStrokeRehab IMU, and 88.4 on 50Salads.", "AI": {"tldr": "The paper introduces HRTR, a transformer-based model for fine-grained, sub-second action detection in stroke rehabilitation, outperforming state-of-the-art methods.", "motivation": "Stroke rehabilitation requires precise tracking of patient movements, but existing methods struggle with fine-grained, sub-second action detection.", "method": "Proposes HRTR, a single-stage transformer model for high-resolution, sub-second action localization and classification, eliminating multi-stage methods.", "result": "HRTR achieves superior performance: ES of 70.1 on StrokeRehab Video, 69.4 on StrokeRehab IMU, and 88.4 on 50Salads.", "conclusion": "HRTR is effective for fine-grained, sub-second action detection in stroke rehabilitation and general datasets."}}
{"id": "2506.08403", "pdf": "https://arxiv.org/pdf/2506.08403", "abs": "https://arxiv.org/abs/2506.08403", "authors": ["Weiya Li", "Junjie Chen", "Bei Li", "Boyang Liu", "Zichen Wen", "Nuanqiao Shan", "Xiaoqian Liu", "Anping Liu", "Huajie Liu", "Hu Song", "Linfeng Zhang"], "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 4 figures, Under review. Code:\n  https://github.com/weiyali126/TACTIC", "summary": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC.", "AI": {"tldr": "TACTIC is a cognitively informed multi-agent framework for machine translation, outperforming state-of-the-art models like GPT-4.1 and DeepSeek-R1.", "motivation": "Existing multi-agent translation frameworks lack insights from cognitive translation studies, which highlight human strategies like balancing literal/free translation and iterative refinement.", "method": "TACTIC integrates six functionally distinct agents (drafting, refinement, evaluation, scoring, context reasoning, external knowledge) to simulate human cognitive processes in translation.", "result": "TACTIC achieves superior performance on FLORES-200 and WMT24 benchmarks, surpassing GPT-4.1 and DeepSeek-R1 in metrics like XCOMET and COMETKIWI-23.", "conclusion": "The framework effectively leverages LLMs by grounding translation workflows in cognitive theory, demonstrating significant improvements in translation quality."}}
{"id": "2502.10450", "pdf": "https://arxiv.org/pdf/2502.10450", "abs": "https://arxiv.org/abs/2502.10450", "authors": ["Xingli Fang", "Jianwei Li", "Varun Mulchandani", "Jung-Eun Kim"], "title": "Trustworthy AI: Safety, Bias, and Privacy -- A Survey", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The capabilities of artificial intelligence systems have been advancing to a\ngreat extent, but these systems still struggle with failure modes,\nvulnerabilities, and biases. In this paper, we study the current state of the\nfield, and present promising insights and perspectives regarding concerns that\nchallenge the trustworthiness of AI models. In particular, this paper\ninvestigates the issues regarding three thrusts: safety, privacy, and bias,\nwhich hurt models' trustworthiness. For safety, we discuss safety alignment in\nthe context of large language models, preventing them from generating toxic or\nharmful content. For bias, we focus on spurious biases that can mislead a\nnetwork. Lastly, for privacy, we cover membership inference attacks in deep\nneural networks. The discussions addressed in this paper reflect our own\nexperiments and observations.", "AI": {"tldr": "The paper examines trustworthiness challenges in AI, focusing on safety, privacy, and bias, with insights from experiments and observations.", "motivation": "AI systems face issues like failure modes, vulnerabilities, and biases, undermining their trustworthiness. The paper aims to address these concerns.", "method": "The study investigates safety alignment in large language models, spurious biases in networks, and membership inference attacks in deep neural networks.", "result": "The paper provides insights into mitigating toxic content generation, misleading biases, and privacy breaches in AI models.", "conclusion": "Addressing safety, privacy, and bias is crucial for enhancing the trustworthiness of AI systems."}}
{"id": "2502.00373", "pdf": "https://arxiv.org/pdf/2502.00373", "abs": "https://arxiv.org/abs/2502.00373", "authors": ["Amy Xiang Wang", "Zakhar Shumaylov", "Peter Zaika", "Ferdia Sherry", "Carola-Bibiane Sch\u00f6nlieb"], "title": "Generalized Lie Symmetries in Physics-Informed Neural Operators", "categories": ["cs.LG", "physics.comp-ph"], "comment": "COLT 2025 Theory of AI for Scientific Computing Workshop Best Paper\n  Runner-Up Award; SCML 2025 Oral", "summary": "Physics-informed neural operators (PINOs) have emerged as powerful tools for\nlearning solution operators of partial differential equations (PDEs). Recent\nresearch has demonstrated that incorporating Lie point symmetry information can\nsignificantly enhance the training efficiency of PINOs, primarily through\ntechniques like data, architecture, and loss augmentation. In this work, we\nfocus on the latter, highlighting that point symmetries oftentimes result in no\ntraining signal, limiting their effectiveness in many problems. To address\nthis, we propose a novel loss augmentation strategy that leverages evolutionary\nrepresentatives of point symmetries, a specific class of generalized symmetries\nof the underlying PDE. These generalized symmetries provide a richer set of\ngenerators compared to standard symmetries, leading to a more informative\ntraining signal. We demonstrate that leveraging evolutionary representatives\nenhances the performance of neural operators, resulting in improved data\nefficiency and accuracy during training.", "AI": {"tldr": "PINOs improve PDE solutions by using evolutionary symmetry representatives for better training signals.", "motivation": "Standard point symmetries often lack training signals, limiting PINOs' effectiveness.", "method": "Proposed a loss augmentation strategy using evolutionary representatives of point symmetries.", "result": "Enhanced neural operator performance with improved data efficiency and accuracy.", "conclusion": "Evolutionary symmetry representatives offer a richer training signal for PINOs."}}
{"id": "2506.02550", "pdf": "https://arxiv.org/pdf/2506.02550", "abs": "https://arxiv.org/abs/2506.02550", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Yisen Feng", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The champion solution for the Ego4D Long-Term Action Anticipation\n  Challenge at the CVPR EgoVis Workshop 2025", "summary": "In this report, we present a novel three-stage framework developed for the\nEgo4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in\nfoundation models, our method consists of three stages: feature extraction,\naction recognition, and long-term action anticipation. First, visual features\nare extracted using a high-performance visual encoder. The features are then\nfed into a Transformer to predict verbs and nouns, with a verb-noun\nco-occurrence matrix incorporated to enhance recognition accuracy. Finally, the\npredicted verb-noun pairs are formatted as textual prompts and input into a\nfine-tuned large language model (LLM) to anticipate future action sequences.\nOur framework achieves first place in this challenge at CVPR 2025, establishing\na new state-of-the-art in long-term action prediction. Our code will be\nreleased at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.", "AI": {"tldr": "A three-stage framework for the Ego4D LTA task, combining feature extraction, action recognition, and LLM-based anticipation, achieving state-of-the-art results.", "motivation": "To advance long-term action anticipation by leveraging foundation models and improving recognition accuracy.", "method": "Three stages: feature extraction (visual encoder), action recognition (Transformer with verb-noun co-occurrence), and anticipation (fine-tuned LLM).", "result": "Achieved first place in the CVPR 2025 challenge, setting a new benchmark.", "conclusion": "The framework effectively combines visual and textual models for superior long-term action prediction."}}
{"id": "2506.08433", "pdf": "https://arxiv.org/pdf/2506.08433", "abs": "https://arxiv.org/abs/2506.08433", "authors": ["Hern\u00e1n Maina", "Nicol\u00e1s Wolovick", "Luciana Benotti"], "title": "Low-resource domain adaptation while minimizing energy and hardware resource consumption", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "A shorter version of this work was accepted as a two-page abstract\n  for presentation at the Widening Natural Language Processing (WiNLP) 2023\n  Workshop. That version was not publicly released, and this is the first\n  public version of the work", "summary": "Training Large Language Models (LLMs) is costly in terms of energy, hardware,\nand annotated data, often resulting in a positionality rooted in predominant\ncultures and values (Santy et al., 2023). Domain adaptation has emerged as a\npromising strategy to better align models with diverse cultural and value\ncontexts (Hershcovich et al., 2022), but its computational cost remains a\nsignificant barrier, particularly for research groups lacking access to\nlarge-scale infrastructure. In this paper, we evaluate how the use of different\nnumerical precision formats and data parallelization strategies impacts both\ntraining speed (as a proxy to energy and hardware consumption) and model\naccuracy, with the goal of facilitating domain adaptation in low-resource\nenvironments. Our findings are relevant to any setting where energy efficiency,\naccessibility, or limited hardware availability are key concerns.", "AI": {"tldr": "The paper explores how numerical precision formats and data parallelization affect training speed and accuracy for LLMs, aiming to make domain adaptation more accessible in low-resource settings.", "motivation": "High costs and cultural biases in LLM training limit accessibility. Domain adaptation is promising but computationally expensive, especially for groups with limited infrastructure.", "method": "Evaluates impact of numerical precision formats and data parallelization on training speed and model accuracy.", "result": "Findings show trade-offs between efficiency and accuracy, aiding domain adaptation in resource-constrained environments.", "conclusion": "The study provides insights for improving energy efficiency and accessibility in LLM training, particularly for low-resource settings."}}
{"id": "2502.13191", "pdf": "https://arxiv.org/pdf/2502.13191", "abs": "https://arxiv.org/abs/2502.13191", "authors": ["Junyi Guan", "Abhijith Sharma", "Chong Tian", "Salem Lahlou"], "title": "On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "Spiking Neural Networks (SNNs) are increasingly explored for their energy\nefficiency and robustness in real-world applications, yet their privacy risks\nremain largely unexamined. In this work, we investigate the susceptibility of\nSNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an\nadversary attempts to determine whether a given sample was part of the training\ndataset. While prior work suggests that SNNs may offer inherent robustness due\nto their discrete, event-driven nature, we find that its resilience diminishes\nas latency (T) increases. Furthermore, we introduce an input dropout strategy\nunder black box setting, that significantly enhances membership inference in\nSNNs. Our findings challenge the assumption that SNNs are inherently more\nsecure, and even though they are expected to be better, our results reveal that\nSNNs exhibit privacy vulnerabilities that are equally comparable to Artificial\nNeural Networks (ANNs). Our code is available at\nhttps://github.com/sharmaabhijith/MIA_SNN.", "AI": {"tldr": "SNNs are vulnerable to Membership Inference Attacks (MIAs), and their resilience decreases with increased latency. An input dropout strategy enhances MIA effectiveness, showing SNNs are not inherently more secure than ANNs.", "motivation": "To examine the privacy risks of SNNs, particularly their susceptibility to MIAs, challenging the assumption of their inherent robustness.", "method": "Investigates SNNs' vulnerability to MIAs, introduces an input dropout strategy under black box settings, and compares results with ANNs.", "result": "SNNs are as vulnerable to MIAs as ANNs, with resilience decreasing as latency increases. The input dropout strategy improves MIA effectiveness.", "conclusion": "SNNs do not inherently offer better privacy security than ANNs, and their vulnerabilities should be addressed in future designs."}}
{"id": "2502.00963", "pdf": "https://arxiv.org/pdf/2502.00963", "abs": "https://arxiv.org/abs/2502.00963", "authors": ["Mauricio Soroco", "Jialin Song", "Mengzhou Xia", "Kye Emond", "Weiran Sun", "Wuyang Chen"], "title": "PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs", "categories": ["cs.LG"], "comment": null, "summary": "While recent AI-for-math has made strides in pure mathematics, areas of\napplied mathematics, particularly PDEs, remain underexplored despite their\nsignificant real-world applications. We present PDE-Controller, a framework\nthat enables large language models (LLMs) to control systems governed by\npartial differential equations (PDEs). Our approach enables LLMs to transform\ninformal natural language instructions into formal specifications, and then\nexecute reasoning and planning steps to improve the utility of PDE control. We\nbuild a holistic solution comprising datasets (both human-written cases and 2\nmillion synthetic samples), math-reasoning models, and novel evaluation\nmetrics, all of which require significant effort. Our PDE-Controller\nsignificantly outperforms prompting the latest open source and GPT models in\nreasoning, autoformalization, and program synthesis, achieving up to a 62%\nimprovement in utility gain for PDE control. By bridging the gap between\nlanguage generation and PDE systems, we demonstrate the potential of LLMs in\naddressing complex scientific and engineering challenges. We release all data,\nmodel checkpoints, and code at https://pde-controller.github.io/.", "AI": {"tldr": "PDE-Controller bridges AI and PDEs, enabling LLMs to transform natural language into formal PDE control specifications, outperforming existing models by 62%.", "motivation": "Applied mathematics, especially PDEs, lacks AI exploration despite real-world importance. PDE-Controller aims to fill this gap.", "method": "Uses LLMs to convert informal instructions into formal PDE specifications, supported by datasets, reasoning models, and new metrics.", "result": "PDE-Controller outperforms open-source and GPT models by up to 62% in utility gain for PDE control.", "conclusion": "The framework demonstrates LLMs' potential in scientific challenges, with released data and code for broader use."}}
{"id": "2506.03107", "pdf": "https://arxiv.org/pdf/2506.03107", "abs": "https://arxiv.org/abs/2506.03107", "authors": ["Di Chang", "Mingdeng Cao", "Yichun Shi", "Bo Liu", "Shengqu Cai", "Shijie Zhou", "Weilin Huang", "Gordon Wetzstein", "Mohammad Soleymani", "Peng Wang"], "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions", "categories": ["cs.CV"], "comment": "Website: https://boese0601.github.io/bytemorph Dataset:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-6M Benchmark:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-Bench Code:\n  https://github.com/ByteDance-Seed/BM-code Demo:\n  https://huggingface.co/spaces/Boese0601/ByteMorph-Demo", "summary": "Editing images with instructions to reflect non-rigid motions, camera\nviewpoint shifts, object deformations, human articulations, and complex\ninteractions, poses a challenging yet underexplored problem in computer vision.\nExisting approaches and datasets predominantly focus on static scenes or rigid\ntransformations, limiting their capacity to handle expressive edits involving\ndynamic motion. To address this gap, we introduce ByteMorph, a comprehensive\nframework for instruction-based image editing with an emphasis on non-rigid\nmotions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong\nbaseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.\nByteMorph-6M includes over 6 million high-resolution image editing pairs for\ntraining, along with a carefully curated evaluation benchmark ByteMorph-Bench.\nBoth capture a wide variety of non-rigid motion types across diverse\nenvironments, human figures, and object categories. The dataset is constructed\nusing motion-guided data generation, layered compositing techniques, and\nautomated captioning to ensure diversity, realism, and semantic coherence. We\nfurther conduct a comprehensive evaluation of recent instruction-based image\nediting methods from both academic and commercial domains.", "AI": {"tldr": "ByteMorph introduces a framework for instruction-based image editing focusing on non-rigid motions, featuring a large dataset (ByteMorph-6M) and a baseline model (ByteMorpher).", "motivation": "Existing methods and datasets are limited to static scenes or rigid transformations, leaving expressive edits involving dynamic motion underexplored.", "method": "ByteMorph includes a dataset (ByteMorph-6M) with 6M image pairs and a baseline model (ByteMorpher) based on Diffusion Transformer (DiT). Data generation uses motion guidance, compositing, and automated captioning.", "result": "ByteMorph-6M and ByteMorpher address diverse non-rigid motions, with a curated evaluation benchmark (ByteMorph-Bench).", "conclusion": "ByteMorph fills a gap in handling dynamic motion edits, offering a robust dataset and model for future research."}}
{"id": "2506.08700", "pdf": "https://arxiv.org/pdf/2506.08700", "abs": "https://arxiv.org/abs/2506.08700", "authors": ["Ruiran Su", "Jiasheng Si", "Zhijiang Guo", "Janet B. Pierrehumbert"], "title": "ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Scientific fact-checking has mostly focused on text and tables, overlooking\nscientific charts, which are key for presenting quantitative evidence and\nstatistical reasoning. We introduce ClimateViz, the first large-scale benchmark\nfor scientific fact-checking using expert-curated scientific charts. ClimateViz\ncontains 49,862 claims linked to 2,896 visualizations, each labeled as support,\nrefute, or not enough information. To improve interpretability, each example\nincludes structured knowledge graph explanations covering trends, comparisons,\nand causal relations. We evaluate state-of-the-art multimodal language models,\nincluding both proprietary and open-source systems, in zero-shot and few-shot\nsettings. Results show that current models struggle with chart-based reasoning:\neven the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to\n77.8 percent accuracy in label-only settings, far below human performance (89.3\nand 92.7 percent). Explanation-augmented outputs improve performance in some\nmodels. We released our dataset and code alongside the paper.", "AI": {"tldr": "ClimateViz is a benchmark for scientific fact-checking using charts, showing current models underperform humans in chart-based reasoning.", "motivation": "Existing fact-checking overlooks scientific charts, which are crucial for quantitative evidence.", "method": "Introduced ClimateViz with 49,862 claims linked to 2,896 charts, evaluated multimodal models in zero-shot and few-shot settings.", "result": "Best models (Gemini 2.5, InternVL 2.5) achieved 76.2-77.8% accuracy, below human performance (89.3-92.7%). Explanation-augmented outputs helped some models.", "conclusion": "Current models struggle with chart-based reasoning, highlighting the need for improved multimodal fact-checking tools."}}
{"id": "2502.13228", "pdf": "https://arxiv.org/pdf/2502.13228", "abs": "https://arxiv.org/abs/2502.13228", "authors": ["Jake C. Snell", "Thomas L. Griffiths"], "title": "Conformal Prediction as Bayesian Quadrature", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "ICML 2025 camera-ready version (accepted as an oral presentation). 16\n  pages, 4 figures. Code available at\n  https://github.com/jakesnell/conformal-as-bayes-quad", "summary": "As machine learning-based prediction systems are increasingly used in\nhigh-stakes situations, it is important to understand how such predictive\nmodels will perform upon deployment. Distribution-free uncertainty\nquantification techniques such as conformal prediction provide guarantees about\nthe loss black-box models will incur even when the details of the models are\nhidden. However, such methods are based on frequentist probability, which\nunduly limits their applicability. We revisit the central aspects of conformal\nprediction from a Bayesian perspective and thereby illuminate the shortcomings\nof frequentist guarantees. We propose a practical alternative based on Bayesian\nquadrature that provides interpretable guarantees and offers a richer\nrepresentation of the likely range of losses to be observed at test time.", "AI": {"tldr": "The paper critiques frequentist-based conformal prediction for uncertainty quantification in machine learning, proposing a Bayesian alternative for richer, interpretable guarantees.", "motivation": "To address limitations of frequentist methods in uncertainty quantification for high-stakes machine learning predictions.", "method": "Revisits conformal prediction from a Bayesian perspective and introduces Bayesian quadrature for practical implementation.", "result": "The proposed Bayesian approach provides interpretable guarantees and a richer representation of potential losses.", "conclusion": "Bayesian methods offer superior uncertainty quantification compared to frequentist conformal prediction in high-stakes scenarios."}}
{"id": "2502.01920", "pdf": "https://arxiv.org/pdf/2502.01920", "abs": "https://arxiv.org/abs/2502.01920", "authors": ["Yalin Liao", "Austin J. Brockmeier"], "title": "Anomaly Detection via Autoencoder Composite Features and NCE", "categories": ["cs.LG"], "comment": null, "summary": "Unsupervised anomaly detection is a challenging task. Autoencoders (AEs) or\ngenerative models are often employed to model the data distribution of normal\ninputs and subsequently identify anomalous, out-of-distribution inputs by high\nreconstruction error or low likelihood, respectively. However, AEs may\ngeneralize and achieve small reconstruction errors on abnormal inputs. We\npropose a decoupled training approach for anomaly detection that both an AE and\na likelihood model trained with noise contrastive estimation (NCE). After\ntraining the AE, NCE estimates a probability density function, to serve as the\nanomaly score, on the joint space of the AE's latent representation combined\nwith features of the reconstruction quality. To further reduce the false\nnegative rate in NCE we systematically varying the reconstruction features to\naugment the training and optimize the contrastive Gaussian noise distribution.\nExperimental assessments on multiple benchmark datasets demonstrate that the\nproposed approach matches the performance of prevalent state-of-the-art anomaly\ndetection algorithms.", "AI": {"tldr": "Proposes a decoupled training approach combining autoencoders (AEs) and noise contrastive estimation (NCE) for unsupervised anomaly detection, improving performance by reducing false negatives.", "motivation": "Autoencoders may generalize and fail to detect anomalies due to small reconstruction errors on abnormal inputs.", "method": "Trains an AE and a likelihood model with NCE, using joint latent and reconstruction features for anomaly scoring. Optimizes contrastive noise distribution to reduce false negatives.", "result": "Matches state-of-the-art performance on benchmark datasets.", "conclusion": "The decoupled approach effectively improves anomaly detection by leveraging AE and NCE jointly."}}
{"id": "2506.03662", "pdf": "https://arxiv.org/pdf/2506.03662", "abs": "https://arxiv.org/abs/2506.03662", "authors": ["Erhang Zhang", "Junyi Ma", "Yin-Dong Zheng", "Yixuan Zhou", "Hesheng Wang"], "title": "Zero-Shot Temporal Interaction Localization for Egocentric Videos", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Locating human-object interaction (HOI) actions within video serves as the\nfoundation for multiple downstream tasks, such as human behavior analysis and\nhuman-robot skill transfer. Current temporal action localization methods\ntypically rely on annotated action and object categories of interactions for\noptimization, which leads to domain bias and low deployment efficiency.\nAlthough some recent works have achieved zero-shot temporal action localization\n(ZS-TAL) with large vision-language models (VLMs), their coarse-grained\nestimations and open-loop pipelines hinder further performance improvements for\ntemporal interaction localization (TIL). To address these issues, we propose a\nnovel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp\nactions for human-object interaction in egocentric videos. EgoLoc introduces a\nself-adaptive sampling strategy to generate reasonable visual prompts for VLM\nreasoning. By absorbing both 2D and 3D observations, it directly samples\nhigh-quality initial guesses around the possible contact/separation timestamps\nof HOI according to 3D hand velocities, leading to high inference accuracy and\nefficiency. In addition, EgoLoc generates closed-loop feedback from visual and\ndynamic cues to further refine the localization results. Comprehensive\nexperiments on the publicly available dataset and our newly proposed benchmark\ndemonstrate that EgoLoc achieves better temporal interaction localization for\negocentric videos compared to state-of-the-art baselines. We will release our\ncode and relevant data as open-source at https://github.com/IRMVLab/EgoLoc.", "AI": {"tldr": "EgoLoc is a zero-shot method for temporal interaction localization (TIL) in egocentric videos, improving accuracy and efficiency with self-adaptive sampling and closed-loop feedback.", "motivation": "Current methods for HOI action localization suffer from domain bias and inefficiency, while existing zero-shot approaches lack precision.", "method": "EgoLoc uses a self-adaptive sampling strategy with 2D/3D observations and closed-loop feedback for refinement.", "result": "Outperforms state-of-the-art baselines in temporal interaction localization.", "conclusion": "EgoLoc offers a robust solution for TIL in egocentric videos, with plans to release code and data."}}
{"id": "2506.08738", "pdf": "https://arxiv.org/pdf/2506.08738", "abs": "https://arxiv.org/abs/2506.08738", "authors": ["Dror Kris Markus", "Fabrizio Gilardi", "Daria Stetsenko"], "title": "Societal AI Research Has Become Less Interdisciplinary", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As artificial intelligence (AI) systems become deeply embedded in everyday\nlife, calls to align AI development with ethical and societal values have\nintensified. Interdisciplinary collaboration is often championed as a key\npathway for fostering such engagement. Yet it remains unclear whether\ninterdisciplinary research teams are actually leading this shift in practice.\nThis study analyzes over 100,000 AI-related papers published on ArXiv between\n2014 and 2024 to examine how ethical values and societal concerns are\nintegrated into technical AI research. We develop a classifier to identify\nsocietal content and measure the extent to which research papers express these\nconsiderations. We find a striking shift: while interdisciplinary teams remain\nmore likely to produce societally-oriented research, computer science-only\nteams now account for a growing share of the field's overall societal output.\nThese teams are increasingly integrating societal concerns into their papers\nand tackling a wide range of domains - from fairness and safety to healthcare\nand misinformation. These findings challenge common assumptions about the\ndrivers of societal AI and raise important questions. First, what are the\nimplications for emerging understandings of AI safety and governance if most\nsocietally-oriented research is being undertaken by exclusively technical\nteams? Second, for scholars in the social sciences and humanities: in a\ntechnical field increasingly responsive to societal demands, what distinctive\nperspectives can we still offer to help shape the future of AI?", "AI": {"tldr": "The study examines the integration of ethical and societal values in AI research, finding that while interdisciplinary teams lead in societal focus, computer science-only teams are increasingly contributing to this area.", "motivation": "To understand how ethical and societal values are being incorporated into AI research and whether interdisciplinary collaboration is driving this shift.", "method": "Analysis of over 100,000 AI-related papers on ArXiv (2014-2024) using a classifier to identify societal content.", "result": "Interdisciplinary teams are more likely to produce societally-oriented research, but computer science-only teams are growing in societal output, addressing domains like fairness, safety, healthcare, and misinformation.", "conclusion": "The findings challenge assumptions about interdisciplinary collaboration's role in societal AI and raise questions about AI governance and the unique contributions of non-technical fields."}}
{"id": "2502.13909", "pdf": "https://arxiv.org/pdf/2502.13909", "abs": "https://arxiv.org/abs/2502.13909", "authors": ["Sein Kim", "Hongseok Kang", "Kibum Kim", "Jiwan Kim", "Donghyun Kim", "Minchul Yang", "Kwangjin Oh", "Julian McAuley", "Chanyoung Park"], "title": "Lost in Sequence: Do Large Language Models Understand Sequential Recommendation?", "categories": ["cs.IR", "cs.AI"], "comment": "KDD 2025 Research Track", "summary": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.", "AI": {"tldr": "The paper introduces LLM-SRec, a method to improve sequential recommendation in LLMs by integrating sequential information more effectively, outperforming existing models without fine-tuning LLMs.", "motivation": "Existing LLM-based recommendation models overlook sequential information in user interactions, limiting their effectiveness.", "method": "Proposes LLM-SRec, which distills user representations from a pre-trained CF-SRec model into LLMs to enhance sequential understanding.", "result": "LLM-SRec improves recommendation performance by better capturing sequential patterns and achieves state-of-the-art results with minimal training.", "conclusion": "LLM-SRec is a practical and effective solution for sequential recommendation, requiring only lightweight training and no LLM fine-tuning."}}
{"id": "2502.04495", "pdf": "https://arxiv.org/pdf/2502.04495", "abs": "https://arxiv.org/abs/2502.04495", "authors": ["Shurui Gui", "Xiner Li", "Shuiwang Ji"], "title": "Discovering Physics Laws of Dynamical Systems via Invariant Function Learning", "categories": ["cs.LG"], "comment": null, "summary": "We consider learning underlying laws of dynamical systems governed by\nordinary differential equations (ODE). A key challenge is how to discover\nintrinsic dynamics across multiple environments while circumventing\nenvironment-specific mechanisms. Unlike prior work, we tackle more complex\nenvironments where changes extend beyond function coefficients to entirely\ndifferent function forms. For example, we demonstrate the discovery of ideal\npendulum's natural motion $\\alpha^2 \\sin{\\theta_t}$ by observing pendulum\ndynamics in different environments, such as the damped environment $\\alpha^2\n\\sin(\\theta_t) - \\rho \\omega_t$ and powered environment $\\alpha^2\n\\sin(\\theta_t) + \\rho \\frac{\\omega_t}{\\left|\\omega_t\\right|}$. Here, we\nformulate this problem as an \\emph{invariant function learning} task and\npropose a new method, known as \\textbf{D}isentanglement of \\textbf{I}nvariant\n\\textbf{F}unctions (DIF), that is grounded in causal analysis. We propose a\ncausal graph and design an encoder-decoder hypernetwork that explicitly\ndisentangles invariant functions from environment-specific dynamics. The\ndiscovery of invariant functions is guaranteed by our information-based\nprinciple that enforces the independence between extracted invariant functions\nand environments. Quantitative comparisons with meta-learning and invariant\nlearning baselines on three ODE systems demonstrate the effectiveness and\nefficiency of our method. Furthermore, symbolic regression explanation results\nhighlight the ability of our framework to uncover intrinsic laws. Our code has\nbeen released as part of the AIRS library\n(\\href{https://github.com/divelab/AIRS/tree/main/OpenODE/DIF}{https://github.com/divelab/AIRS/}).", "AI": {"tldr": "The paper introduces DIF, a method to learn invariant functions in dynamical systems across diverse environments, ensuring discovery of intrinsic dynamics while ignoring environment-specific mechanisms.", "motivation": "The challenge is to discover intrinsic dynamics in complex environments where changes affect not just coefficients but entire function forms, such as damped or powered pendulum dynamics.", "method": "The authors propose DIF, a causal analysis-based method using an encoder-decoder hypernetwork to disentangle invariant functions from environment-specific dynamics, enforced by an information-based principle.", "result": "DIF outperforms meta-learning and invariant learning baselines on three ODE systems, demonstrating effectiveness and efficiency. Symbolic regression confirms its ability to uncover intrinsic laws.", "conclusion": "DIF successfully discovers invariant functions in complex dynamical systems, validated by quantitative comparisons and symbolic regression, with code available in the AIRS library."}}
{"id": "2506.04394", "pdf": "https://arxiv.org/pdf/2506.04394", "abs": "https://arxiv.org/abs/2506.04394", "authors": ["Qiuyu Tang", "Bonor Ayambem", "Mooi Choo Chuah", "Aparna Bharati"], "title": "Is Perturbation-Based Image Protection Disruptive to Image Editing?", "categories": ["cs.CV"], "comment": "6 pages, 8 figures, accepted by ICIP 2025", "summary": "The remarkable image generation capabilities of state-of-the-art diffusion\nmodels, such as Stable Diffusion, can also be misused to spread misinformation\nand plagiarize copyrighted materials. To mitigate the potential risks\nassociated with image editing, current image protection methods rely on adding\nimperceptible perturbations to images to obstruct diffusion-based editing. A\nfully successful protection for an image implies that the output of editing\nattempts is an undesirable, noisy image which is completely unrelated to the\nreference image. In our experiments with various perturbation-based image\nprotection methods across multiple domains (natural scene images and artworks)\nand editing tasks (image-to-image generation and style editing), we discover\nthat such protection does not achieve this goal completely. In most scenarios,\ndiffusion-based editing of protected images generates a desirable output image\nwhich adheres precisely to the guidance prompt. Our findings suggest that\nadding noise to images may paradoxically increase their association with given\ntext prompts during the generation process, leading to unintended consequences\nsuch as better resultant edits. Hence, we argue that perturbation-based methods\nmay not provide a sufficient solution for robust image protection against\ndiffusion-based editing.", "AI": {"tldr": "Current perturbation-based image protection methods fail to fully prevent diffusion-based editing, often yielding desirable outputs instead of noisy images.", "motivation": "To address the misuse of diffusion models for spreading misinformation and plagiarizing copyrighted materials by evaluating the effectiveness of perturbation-based protection methods.", "method": "Experiments with various perturbation-based protection methods across multiple domains (natural scenes, artworks) and editing tasks (image-to-image generation, style editing).", "result": "Perturbation-based protection often fails, as diffusion-based editing still produces desirable outputs adhering to prompts, sometimes even improving edits.", "conclusion": "Perturbation-based methods are insufficient for robust image protection against diffusion-based editing, as they may unintentionally enhance edits."}}
{"id": "2506.08768", "pdf": "https://arxiv.org/pdf/2506.08768", "abs": "https://arxiv.org/abs/2506.08768", "authors": ["Ahmed Hasanaath", "Aisha Alansari", "Ahmed Ashraf", "Chafik Salmane", "Hamzah Luqman", "Saad Ezzini"], "title": "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable progress in reasoning\nabilities and general natural language processing (NLP) tasks, yet their\nperformance on Arabic data, characterized by rich morphology, diverse dialects,\nand complex script, remains underexplored. This paper presents a comprehensive\nbenchmarking study of multiple reasoning-focused LLMs, with a special emphasis\non the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP\ntasks. We experiment with various strategies, including zero-shot, few-shot,\nand fine-tuning. This allows us to systematically evaluate performance on\ndatasets covering a range of applications to examine their capacity for\nlinguistic reasoning under different levels of complexity. Our experiments\nreveal several key findings. First, carefully selecting just three in-context\nexamples delivers an average uplift of over 13 F1 points on classification\ntasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection\nfrom 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures\noutperform a strong GPT o4-mini baseline by an average of 12 F1 points on\ncomplex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning\nyields up to an additional 8 points in F1 and BLEU compared to equivalent\nincreases in model scale. The code is available at\nhttps://anonymous.4open.science/r/AraReasoner41299", "AI": {"tldr": "The paper benchmarks reasoning-focused LLMs, especially DeepSeek models, on Arabic NLP tasks, revealing performance boosts from in-context examples, DeepSeek's superiority over GPT-4-mini, and LoRA fine-tuning benefits.", "motivation": "To explore LLMs' performance on Arabic data, given its linguistic complexity and underexplored potential in reasoning tasks.", "method": "Evaluates LLMs using zero-shot, few-shot, and fine-tuning strategies across 15 Arabic NLP tasks, focusing on DeepSeek models.", "result": "Key findings include significant performance boosts from few-shot examples (13 F1 points), DeepSeek outperforming GPT-4-mini (12 F1 points), and LoRA fine-tuning adding 8 F1/BLEU points.", "conclusion": "The study highlights the effectiveness of in-context learning and fine-tuning for Arabic NLP, with DeepSeek models showing strong reasoning capabilities."}}
{"id": "2502.14254", "pdf": "https://arxiv.org/pdf/2502.14254", "abs": "https://arxiv.org/abs/2502.14254", "authors": ["Lingfeng Zhang", "Yuecheng Liu", "Zhanguang Zhang", "Matin Aghaei", "Yaochen Hu", "Hongjian Gu", "Mohammad Ali Alomrani", "David Gamaliel Arcos Bravo", "Raika Karimi", "Atia Hamidizadeh", "Haoping Xu", "Guowei Huang", "Zhanpeng Zhang", "Tongtong Cao", "Weichao Qiu", "Xingyue Quan", "Jianye Hao", "Yuzheng Zhuang", "Yingxue Zhang"], "title": "Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have made them powerful tools in embodied navigation, enabling\nagents to leverage commonsense and spatial reasoning for efficient exploration\nin unfamiliar environments. Existing LLM-based approaches convert global\nmemory, such as semantic or topological maps, into language descriptions to\nguide navigation. While this improves efficiency and reduces redundant\nexploration, the loss of geometric information in language-based\nrepresentations hinders spatial reasoning, especially in intricate\nenvironments. To address this, VLM-based approaches directly process\nego-centric visual inputs to select optimal directions for exploration.\nHowever, relying solely on a first-person perspective makes navigation a\npartially observed decision-making problem, leading to suboptimal decisions in\ncomplex environments. In this paper, we present a novel vision-language model\n(VLM)-based navigation framework that addresses these challenges by adaptively\nretrieving task-relevant cues from a global memory module and integrating them\nwith the agent's egocentric observations. By dynamically aligning global\ncontextual information with local perception, our approach enhances spatial\nreasoning and decision-making in long-horizon tasks. Experimental results\ndemonstrate that the proposed method surpasses previous state-of-the-art\napproaches in object navigation tasks, providing a more effective and scalable\nsolution for embodied navigation.", "AI": {"tldr": "A novel VLM-based navigation framework integrates global memory with egocentric observations to enhance spatial reasoning and decision-making in embodied navigation, outperforming existing methods.", "motivation": "Existing LLM-based approaches lose geometric information in language representations, while VLM-based methods suffer from partial observability. The paper aims to combine global and local information for better navigation.", "method": "The framework adaptively retrieves task-relevant cues from global memory and integrates them with egocentric visual inputs to improve spatial reasoning.", "result": "The method outperforms state-of-the-art approaches in object navigation tasks, demonstrating improved efficiency and scalability.", "conclusion": "The proposed framework effectively addresses limitations of existing methods by combining global and local information, enhancing embodied navigation performance."}}
{"id": "2502.04959", "pdf": "https://arxiv.org/pdf/2502.04959", "abs": "https://arxiv.org/abs/2502.04959", "authors": ["Daniel Marczak", "Simone Magistri", "Sebastian Cygert", "Bart\u0142omiej Twardowski", "Andrew D. Bagdanov", "Joost van de Weijer"], "title": "No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Model merging integrates the weights of multiple task-specific models into a\nsingle multi-task model. Despite recent interest in the problem, a significant\nperformance gap between the combined and single-task models remains. In this\npaper, we investigate the key characteristics of task matrices -- weight update\nmatrices applied to a pre-trained model -- that enable effective merging. We\nshow that alignment between singular components of task-specific and merged\nmatrices strongly correlates with performance improvement over the pre-trained\nmodel. Based on this, we propose an isotropic merging framework that flattens\nthe singular value spectrum of task matrices, enhances alignment, and reduces\nthe performance gap. Additionally, we incorporate both common and task-specific\nsubspaces to further improve alignment and performance. Our proposed approach\nachieves state-of-the-art performance on vision and language tasks across\nvarious sets of tasks and model scales. This work advances the understanding of\nmodel merging dynamics, offering an effective methodology to merge models\nwithout requiring additional training. Code is available at\nhttps://github.com/danielm1405/iso-merging .", "AI": {"tldr": "The paper proposes an isotropic merging framework to improve model merging by aligning singular components of task matrices, reducing performance gaps, and incorporating common and task-specific subspaces.", "motivation": "Address the performance gap between merged and single-task models by investigating key characteristics of task matrices for effective merging.", "method": "Propose an isotropic merging framework that flattens singular value spectra of task matrices, enhances alignment, and incorporates common and task-specific subspaces.", "result": "Achieves state-of-the-art performance on vision and language tasks across various task sets and model scales.", "conclusion": "Advances understanding of model merging dynamics and provides an effective methodology for merging models without additional training."}}
{"id": "2506.04704", "pdf": "https://arxiv.org/pdf/2506.04704", "abs": "https://arxiv.org/abs/2506.04704", "authors": ["Youngwan Lee", "Kangsan Kim", "Kwanyong Park", "Ilcahe Jung", "Soojin Jang", "Seanie Lee", "Yong-Ju Lee", "Sung Ju Hwang"], "title": "HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://youngwanlee.github.io/holisafe", "summary": "Despite emerging efforts to enhance the safety of Vision-Language Models\n(VLMs), current approaches face two main shortcomings. 1) Existing\nsafety-tuning datasets and benchmarks only partially consider how image-text\ninteractions can yield harmful content, often overlooking contextually unsafe\noutcomes from seemingly benign pairs. This narrow coverage leaves VLMs\nvulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely\nprimarily on data-centric tuning, with limited architectural innovations to\nintrinsically strengthen safety. We address these gaps by introducing a\nholistic safety dataset and benchmark, HoliSafe, that spans all five\nsafe/unsafe image-text combinations, providing a more robust basis for both\ntraining and evaluation. We further propose SafeLLaVA, a novel VLM augmented\nwith a learnable safety meta token and a dedicated safety head. The meta token\nencodes harmful visual cues during training, intrinsically guiding the language\nmodel toward safer responses, while the safety head offers interpretable\nharmfulness classification aligned with refusal rationales. Experiments show\nthat SafeLLaVA, trained on HoliSafe, achieves state-of-the-art safety\nperformance across multiple VLM benchmarks. Additionally, the HoliSafe\nbenchmark itself reveals critical vulnerabilities in existing models. We hope\nthat HoliSafe and SafeLLaVA will spur further research into robust and\ninterpretable VLM safety, expanding future avenues for multimodal alignment.", "AI": {"tldr": "The paper introduces HoliSafe, a comprehensive safety dataset and benchmark for Vision-Language Models (VLMs), and SafeLLaVA, a novel VLM with safety enhancements like a learnable meta token and safety head. These innovations improve safety performance and interpretability.", "motivation": "Current VLM safety approaches are limited by partial coverage of harmful content and reliance on data-centric tuning, leaving models vulnerable to unseen threats.", "method": "Proposes HoliSafe, a dataset covering all safe/unsafe image-text combinations, and SafeLLaVA, a VLM with a safety meta token and dedicated safety head for intrinsic safety.", "result": "SafeLLaVA achieves state-of-the-art safety performance, and HoliSafe exposes vulnerabilities in existing models.", "conclusion": "HoliSafe and SafeLLaVA advance robust and interpretable VLM safety, encouraging further research in multimodal alignment."}}
{"id": "2506.08885", "pdf": "https://arxiv.org/pdf/2506.08885", "abs": "https://arxiv.org/abs/2506.08885", "authors": ["Danush Khanna", "Krishna Kumar", "Basab Ghosh", "Vinija Jain", "Vasu Sharma", "Aman Chadha", "Amitava Das"], "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Adversarial threats against LLMs are escalating faster than current defenses\ncan adapt. We expose a critical geometric blind spot in alignment: adversarial\nprompts exploit latent camouflage, embedding perilously close to the safe\nrepresentation manifold while encoding unsafe intent thereby evading surface\nlevel defenses like Direct Preference Optimization (DPO), which remain blind to\nthe latent geometry. We introduce ALKALI, the first rigorously curated\nadversarial benchmark and the most comprehensive to date spanning 9,000 prompts\nacross three macro categories, six subtypes, and fifteen attack families.\nEvaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates\n(ASRs) across both open and closed source models, exposing an underlying\nvulnerability we term latent camouflage, a structural blind spot where\nadversarial completions mimic the latent geometry of safe ones. To mitigate\nthis vulnerability, we introduce GRACE - Geometric Representation Aware\nContrastive Enhancement, an alignment framework coupling preference learning\nwith latent space regularization. GRACE enforces two constraints: latent\nseparation between safe and adversarial completions, and adversarial cohesion\namong unsafe and jailbreak behaviors. These operate over layerwise pooled\nembeddings guided by a learned attention profile, reshaping internal geometry\nwithout modifying the base model, and achieve up to 39% ASR reduction.\nMoreover, we introduce AVQI, a geometry aware metric that quantifies latent\nalignment failure via cluster separation and compactness. AVQI reveals when\nunsafe completions mimic the geometry of safe ones, offering a principled lens\ninto how models internally encode safety. We make the code publicly available\nat https://anonymous.4open.science/r/alkali-B416/README.md.", "AI": {"tldr": "The paper exposes a geometric blind spot in LLM alignment, introduces ALKALI (a benchmark) and GRACE (a defense framework), and proposes AVQI (a metric) to quantify latent alignment failures.", "motivation": "Adversarial threats against LLMs are outpacing defenses, with attacks exploiting latent geometry to evade detection.", "method": "Introduces ALKALI for benchmarking, GRACE for alignment (latent space regularization), and AVQI for measuring alignment failure.", "result": "GRACE reduces Attack Success Rates by up to 39%, and AVQI reveals unsafe completions mimicking safe geometry.", "conclusion": "The work highlights latent camouflage vulnerabilities and offers tools (ALKALI, GRACE, AVQI) to improve LLM safety."}}
{"id": "2503.16563", "pdf": "https://arxiv.org/pdf/2503.16563", "abs": "https://arxiv.org/abs/2503.16563", "authors": ["Aahan Singh", "Engin Tekin", "Maryam Nadeem", "Nancy A. ElNaker", "Mohammad Amaan Sayeed", "Natalia Vassilieva", "Boulbaba Ben Amor"], "title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Revolutionizing drug discovery demands more than just understanding molecular\ninteractions - it requires generative models that can design novel ligands\ntailored to specific biological targets. While chemical Language Models (cLMs)\nhave made strides in learning molecular properties, most fail to incorporate\ntarget-specific insights, restricting their ability to drive de-novo ligand\ngeneration. Chem42, a cutting-edge family of generative chemical Language\nModels, is designed to bridge this gap. By integrating atomic-level\ninteractions with multimodal inputs from Prot42, a complementary protein\nLanguage Model, Chem42 achieves a sophisticated cross-modal representation of\nmolecular structures, interactions, and binding patterns. This innovative\nframework enables the creation of structurally valid, synthetically accessible\nligands with enhanced target specificity. Evaluations across diverse protein\ntargets confirm that Chem42 surpasses existing approaches in chemical validity,\ntarget-aware design, and predicted binding affinity. By reducing the search\nspace of viable drug candidates, Chem42 could accelerate the drug discovery\npipeline, offering a powerful generative AI tool for precision medicine. Our\nChem42 models set a new benchmark in molecule property prediction, conditional\nmolecule generation, and target-aware ligand design. The models are publicly\navailable at huggingface.co/inceptionai.", "AI": {"tldr": "Chem42 is a generative chemical Language Model that integrates target-specific insights to design novel ligands, outperforming existing methods in validity, specificity, and binding affinity.", "motivation": "Current chemical Language Models lack target-specific insights, limiting their ability to generate novel ligands for drug discovery.", "method": "Chem42 combines atomic-level interactions with multimodal inputs from Prot42 to create a cross-modal representation for ligand design.", "result": "Chem42 excels in chemical validity, target-aware design, and binding affinity, reducing the search space for viable drug candidates.", "conclusion": "Chem42 sets a new benchmark in molecule property prediction and ligand design, accelerating drug discovery for precision medicine."}}
{"id": "2502.05003", "pdf": "https://arxiv.org/pdf/2502.05003", "abs": "https://arxiv.org/abs/2502.05003", "authors": ["Andrei Panferov", "Jiale Chen", "Soroush Tabesh", "Roberto L. Castro", "Mahdi Nikdan", "Dan Alistarh"], "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations", "categories": ["cs.LG"], "comment": null, "summary": "One approach to reducing the massive costs of large language models (LLMs) is\nthe use of quantized or sparse representations for training or deployment.\nWhile post-training compression methods are very popular, the question of\nobtaining even more accurate compressed models by directly training over such\nrepresentations, i.e., Quantization-Aware Training (QAT), is still open: for\nexample, a recent study (arXiv:2411.04330) put the \"optimal\" bit-width at which\nmodels can be trained using QAT, while staying accuracy-competitive with\nstandard FP16/BF16 precision, at 8-bits weights and activations. We advance\nthis state-of-the-art via a new method called QuEST, for which we demonstrate\noptimality at 4-bits and stable convergence as low as 1-bit weights and\nactivations. QuEST achieves this by improving two key aspects of QAT methods:\n(1) accurate and fast quantization of the (continuous) distributions of weights\nand activations via Hadamard normalization and MSE-optimal fitting; (2) a new\ntrust gradient estimator based on the idea of explicitly minimizing the error\nbetween the noisy gradient computed over quantized states and the \"true\" (but\nunknown) full-precision gradient. Experiments on Llama-type architectures show\nthat QuEST induces stable scaling laws across the entire range of\nhardware-supported precisions, and can be extended to sparse representations.\nWe provide GPU kernel support showing that models produced by QuEST can be\nexecuted efficiently. Our code is available at\nhttps://github.com/IST-DASLab/QuEST.", "AI": {"tldr": "QuEST advances Quantization-Aware Training (QAT) by achieving optimality at 4-bits and stable convergence at 1-bit, improving quantization accuracy and gradient estimation.", "motivation": "To reduce the costs of large language models (LLMs) by enabling more accurate and efficient training of quantized or sparse models.", "method": "QuEST improves QAT via Hadamard normalization, MSE-optimal fitting, and a new trust gradient estimator.", "result": "Demonstrates stable scaling laws across hardware-supported precisions and efficient execution on GPUs.", "conclusion": "QuEST sets a new state-of-the-art for QAT, enabling highly compressed models without sacrificing accuracy."}}
{"id": "2506.05343", "pdf": "https://arxiv.org/pdf/2506.05343", "abs": "https://arxiv.org/abs/2506.05343", "authors": ["Wenfeng Lin", "Renjie Chen", "Boyuan Liu", "Shiyue Yan", "Ruoyu Feng", "Jiangchuan Wei", "Yichen Zhang", "Yimeng Zhou", "Chao Feng", "Jiao Ran", "Qi Wu", "Zuotao Liu", "Mingyu Guo"], "title": "ContentV: Efficient Training of Video Generation Models with Limited Compute", "categories": ["cs.CV"], "comment": "Project Page: https://contentv.github.io", "summary": "Recent advances in video generation demand increasingly efficient training\nrecipes to mitigate escalating computational costs. In this report, we present\nContentV, an 8B-parameter text-to-video model that achieves state-of-the-art\nperformance (85.14 on VBench) after training on 256 x 64GB Neural Processing\nUnits (NPUs) for merely four weeks. ContentV generates diverse, high-quality\nvideos across multiple resolutions and durations from text prompts, enabled by\nthree key innovations: (1) A minimalist architecture that maximizes reuse of\npre-trained image generation models for video generation; (2) A systematic\nmulti-stage training strategy leveraging flow matching for enhanced efficiency;\nand (3) A cost-effective reinforcement learning with human feedback framework\nthat improves generation quality without requiring additional human\nannotations. All the code and models are available at:\nhttps://contentv.github.io.", "AI": {"tldr": "ContentV is an 8B-parameter text-to-video model achieving SOTA performance with efficient training, leveraging minimalist architecture, multi-stage training, and cost-effective RLHF.", "motivation": "To address the escalating computational costs in video generation by developing an efficient training recipe.", "method": "Uses a minimalist architecture, multi-stage training with flow matching, and reinforcement learning with human feedback (RLHF).", "result": "Achieves state-of-the-art performance (85.14 on VBench) with diverse, high-quality video generation.", "conclusion": "ContentV demonstrates efficient, high-quality video generation from text prompts, with open-sourced code and models."}}
{"id": "2506.08952", "pdf": "https://arxiv.org/pdf/2506.08952", "abs": "https://arxiv.org/abs/2506.08952", "authors": ["Clara Lachenmaier", "Judith Sieker", "Sina Zarrie\u00df"], "title": "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint accepted at ACL Main Conference 2025", "summary": "Communication among humans relies on conversational grounding, allowing\ninterlocutors to reach mutual understanding even when they do not have perfect\nknowledge and must resolve discrepancies in each other's beliefs. This paper\ninvestigates how large language models (LLMs) manage common ground in cases\nwhere they (don't) possess knowledge, focusing on facts in the political domain\nwhere the risk of misinformation and grounding failure is high. We examine the\nability of LLMs to answer direct knowledge questions and loaded questions that\npresuppose misinformation. We evaluate whether loaded questions lead LLMs to\nengage in active grounding and correct false user beliefs, in connection to\ntheir level of knowledge and their political bias. Our findings highlight\nsignificant challenges in LLMs' ability to engage in grounding and reject false\nuser beliefs, raising concerns about their role in mitigating misinformation in\npolitical discourse.", "AI": {"tldr": "The paper examines how LLMs handle common ground and misinformation in political discourse, finding challenges in their ability to correct false beliefs.", "motivation": "To understand LLMs' capability in managing mutual understanding and addressing misinformation, especially in politically charged contexts.", "method": "Evaluated LLMs' responses to direct knowledge questions and loaded questions with misinformation, analyzing grounding behavior and political bias.", "result": "LLMs struggle with grounding and rejecting false beliefs, posing concerns for their role in political misinformation.", "conclusion": "LLMs face significant challenges in mitigating misinformation, highlighting risks in political discourse."}}
{"id": "2504.10552", "pdf": "https://arxiv.org/pdf/2504.10552", "abs": "https://arxiv.org/abs/2504.10552", "authors": ["Arash Torabi Goodarzi", "Roman Kochnev", "Waleed Khalid", "Furui Qin", "Tolgay Atinc Uzun", "Yashkumar Sanjaybhai Dhameliya", "Yash Kanubhai Kathiriya", "Zofia Antonina Bentyn", "Dmitry Ignatov", "Radu Timofte"], "title": "LEMUR Neural Network Dataset: Towards Seamless AutoML", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DL"], "comment": null, "summary": "Neural networks are fundamental in artificial intelligence, driving progress\nin computer vision and natural language processing. High-quality datasets are\ncrucial for their development, and there is growing interest in datasets\ncomposed of neural networks themselves to support benchmarking, automated\nmachine learning (AutoML), and model analysis. We introduce LEMUR, an open\nsource dataset of neural network models with well-structured code for diverse\narchitectures across tasks such as object detection, image classification,\nsegmentation, and natural language processing. LEMUR is primarily designed to\nprovide a rich source of structured model representations and associated\nperformance data, enabling the fine-tuning of large language models for AutoML\napplications. Leveraging Python and PyTorch, LEMUR enables seamless extension\nto new datasets and models while maintaining consistency. It integrates an\nOptuna-powered framework for evaluation, hyperparameter optimization,\nstatistical analysis, and graphical insights. LEMUR VR extension enables the\nseamless deployment of models in virtual reality, optimizing their performance\non resource-constrained devices. Providing tools for model evaluation,\npreprocessing, and database management, LEMUR supports researchers and\npractitioners in developing, testing, and analyzing neural networks. It offers\nan API that delivers comprehensive information about neural network models and\ntheir complete performance statistics with a single request, which can be used\nin experiments with code-generating large language models. The LEMUR and its\nplugins are accessible as open source projects under the MIT license at\nhttps://github.com/ABrain-One/nn-dataset,\nhttps://github.com/ABrain-One/nn-plots and https://github.com/ABrain-One/nn-vr.", "AI": {"tldr": "LEMUR is an open-source dataset of neural network models with structured code for diverse tasks, designed to support AutoML, benchmarking, and model analysis.", "motivation": "High-quality datasets are crucial for neural network development, and there's growing interest in datasets of neural networks themselves for benchmarking and AutoML.", "method": "LEMUR provides structured model representations and performance data, leveraging Python and PyTorch. It includes tools for evaluation, hyperparameter optimization, and VR deployment.", "result": "LEMUR enables seamless extension to new datasets and models, supports model evaluation, and offers an API for comprehensive performance statistics.", "conclusion": "LEMUR is a valuable resource for researchers and practitioners, facilitating neural network development and analysis, and is available as open-source under the MIT license."}}
{"id": "2502.05075", "pdf": "https://arxiv.org/pdf/2502.05075", "abs": "https://arxiv.org/abs/2502.05075", "authors": ["Yijun Dong", "Yicheng Li", "Yunai Li", "Jason D. Lee", "Qi Lei"], "title": "Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "comment": "ICML 2025", "summary": "Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a\nstrong (large) student model is trained on pseudo-labels generated by a weak\nteacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to\nunderstand this phenomenon through the observation that FT often occurs in\nintrinsically low-dimensional spaces. Leveraging the low intrinsic\ndimensionality of FT, we analyze W2S in the ridgeless regression setting from a\nvariance reduction perspective. For a strong student-weak teacher pair with\nsufficiently expressive low-dimensional feature subspaces $\\mathcal{V}_s,\n\\mathcal{V}_w$, we provide an exact characterization of the variance that\ndominates the generalization error of W2S. This unveils a virtue of discrepancy\nbetween the strong and weak models in W2S: the variance of the weak teacher is\ninherited by the strong student in $\\mathcal{V}_s \\cap \\mathcal{V}_w$, while\nreduced by a factor of $\\mathrm{dim}(\\mathcal{V}_s)/N$ in the subspace of\ndiscrepancy $\\mathcal{V}_w \\setminus \\mathcal{V}_s$ with $N$ pseudo-labels for\nW2S. Our analysis further casts light on the sample complexities and the\nscaling of performance gap recovery in W2S. The analysis is supported by\nexperiments on synthetic regression problems, as well as real vision and NLP\ntasks.", "AI": {"tldr": "Weak-to-strong (W2S) finetuning outperforms weak teachers due to variance reduction in low-dimensional subspaces, analyzed via ridgeless regression.", "motivation": "To understand why W2S finetuning, where a strong model learns from weak pseudo-labels, often surpasses the weak teacher's performance.", "method": "Analyzes W2S in ridgeless regression, focusing on low-dimensional feature subspaces and variance reduction.", "result": "Shows variance reduction in discrepancy subspaces, with experiments validating the theory on synthetic and real tasks.", "conclusion": "W2S benefits from model discrepancy, reducing variance and improving generalization, supported by empirical evidence."}}
{"id": "2506.05765", "pdf": "https://arxiv.org/pdf/2506.05765", "abs": "https://arxiv.org/abs/2506.05765", "authors": ["Taiga Shinozaki", "Tomoki Doi", "Amane Watahiki", "Satoshi Nishida", "Hitomi Yanaka"], "title": "Do Large Vision-Language Models Distinguish between the Actual and Apparent Features of Illusions?", "categories": ["cs.CV", "cs.CL"], "comment": "To appear in the Proceedings of the 47th Annual Meeting of the\n  Cognitive Science Society (COGSCI 2025)", "summary": "Humans are susceptible to optical illusions, which serve as valuable tools\nfor investigating sensory and cognitive processes. Inspired by human vision\nstudies, research has begun exploring whether machines, such as large vision\nlanguage models (LVLMs), exhibit similar susceptibilities to visual illusions.\nHowever, studies often have used non-abstract images and have not distinguished\nactual and apparent features, leading to ambiguous assessments of machine\ncognition. To address these limitations, we introduce a visual question\nanswering (VQA) dataset, categorized into genuine and fake illusions, along\nwith corresponding control images. Genuine illusions present discrepancies\nbetween actual and apparent features, whereas fake illusions have the same\nactual and apparent features even though they look illusory due to the similar\ngeometric configuration. We evaluate the performance of LVLMs for genuine and\nfake illusion VQA tasks and investigate whether the models discern actual and\napparent features. Our findings indicate that although LVLMs may appear to\nrecognize illusions by correctly answering questions about both feature types,\nthey predict the same answers for both Genuine Illusion and Fake Illusion VQA\nquestions. This suggests that their responses might be based on prior knowledge\nof illusions rather than genuine visual understanding. The dataset is available\nat https://github.com/ynklab/FILM", "AI": {"tldr": "The paper investigates whether large vision language models (LVLMs) can discern genuine vs. fake visual illusions, revealing they rely on prior knowledge rather than genuine visual understanding.", "motivation": "To address ambiguities in assessing machine cognition of visual illusions by distinguishing genuine (actual vs. apparent feature discrepancies) and fake illusions (no discrepancies).", "method": "Introduces a VQA dataset with genuine and fake illusions, evaluates LVLMs' performance on discerning actual vs. apparent features.", "result": "LVLMs predict similar answers for both illusion types, suggesting reliance on prior knowledge, not visual understanding.", "conclusion": "LVLMs lack genuine visual understanding of illusions, highlighting limitations in their cognitive capabilities."}}
{"id": "2506.09003", "pdf": "https://arxiv.org/pdf/2506.09003", "abs": "https://arxiv.org/abs/2506.09003", "authors": ["Lei Zhang", "Jiaxi Yang", "Min Yang", "Jian Yang", "Mouxiang Chen", "Jiajun Zhang", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner", "categories": ["cs.CL"], "comment": "Accepted by ICML2025", "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).", "AI": {"tldr": "SWE-Flow is a data synthesis framework using Test-Driven Development (TDD) to generate incremental development steps from unit tests, creating verifiable TDD tasks.", "motivation": "Existing software engineering data relies on human-submitted issues, lacking automation and precision in capturing development steps.", "method": "SWE-Flow constructs a Runtime Dependency Graph (RDG) to infer function interactions and generate structured development schedules, producing partial codebases, tests, and modifications.", "result": "Generated 16,061 training and 2,020 test instances (SWE-Flow-Eval benchmark), showing improved TDD-based coding performance when fine-tuning models.", "conclusion": "SWE-Flow automates TDD task generation, enhances model performance, and provides open resources for further research."}}
{"id": "2504.18574", "pdf": "https://arxiv.org/pdf/2504.18574", "abs": "https://arxiv.org/abs/2504.18574", "authors": ["Aviv Bick", "Eric Xing", "Albert Gu"], "title": "Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "State-space models (SSMs) offer efficient alternatives to Transformers for\nlong sequences, but their fixed-size recurrent state limits capability on\nalgorithmic tasks, such as retrieving past context. In this work, we examine\nhow in-context retrieval operates in Transformer- and SSM-based language models\nand find that both rely on a similar Gather-and-Aggregate (G&A) mechanism: a\nGather Head extracts relevant information pieces from context, which an\nAggregate Head integrates into a single representation. In both architectures,\nG&A concentrates in a few heads, forming critical bottlenecks even for simple\nretrieval. For example, we show that disabling a single Gather or Aggregate\nHead in a pruned Llama-3.1-8B impairs retrieving the correct answer letter in\nMMLU, reducing its accuracy from 66% to 25% (random guessing). Moreover, this\nretrieval bottleneck can obscure limited knowledge demands of tasks as the\npruned model succeeds on MMLU with functioning G&A heads yet fails on other\nknowledge benchmarks. The bottleneck similarly extends to tasks where SSMs\ntypically underperform, such as GSM8K, BBH, and dialogue comprehension. We show\nthat SSMs' retrieval challenges manifest in these heads, creating smoother\nattention patterns instead of the sharp token transitions effective G&A\nrequires. Thus, the Transformer-SSM retrieval gap exists in just a few heads,\nrather than the entire language model. This suggests a unified explanation for\nTransformer vs. SSM performance gap while showing how to merge their strengths.\nWe find that pretrained hybrid models, where SSMs are combined with a few\nattention layers, delegate the role of Aggregate Heads to attention. Similarly,\nreplacing a single G&A head in a pretrained SSM with an attention variant\nboosts retrieval and benchmark scores.", "AI": {"tldr": "The paper examines retrieval mechanisms in Transformer- and SSM-based models, identifying a Gather-and-Aggregate bottleneck in both. Hybrid models combining SSMs with attention layers improve performance.", "motivation": "To understand why SSMs underperform on algorithmic tasks like retrieval compared to Transformers and to bridge this gap.", "method": "Analyzes retrieval mechanisms in both architectures, prunes models to isolate critical heads, and tests hybrid models.", "result": "Identifies a few critical Gather-and-Aggregate heads as bottlenecks. Hybrid models with attention layers improve retrieval and benchmark scores.", "conclusion": "The Transformer-SSM performance gap stems from a few heads, and hybrid models can merge their strengths."}}
{"id": "2502.05335", "pdf": "https://arxiv.org/pdf/2502.05335", "abs": "https://arxiv.org/abs/2502.05335", "authors": ["Roussel Desmond Nzoyem", "Grant Stevens", "Amarpal Sahota", "David A. W. Barton", "Tom Deakin"], "title": "Towards Foundational Models for Dynamical System Reconstruction: Hierarchical Meta-Learning via Mixture of Experts", "categories": ["cs.LG"], "comment": "22 pages, 11 figures, 7 tables. Accepted as a SCOPE workshop paper at\n  ICLR 2025", "summary": "As foundational models reshape scientific discovery, a bottleneck persists in\ndynamical system reconstruction (DSR): the ability to learn across system\nhierarchies. Many meta-learning approaches have been applied successfully to\nsingle systems, but falter when confronted with sparse, loosely related\ndatasets requiring multiple hierarchies to be learned. Mixture of Experts (MoE)\noffers a natural paradigm to address these challenges. Despite their potential,\nwe demonstrate that naive MoEs are inadequate for the nuanced demands of\nhierarchical DSR, largely due to their gradient descent-based gating update\nmechanism which leads to slow updates and conflicted routing during training.\nTo overcome this limitation, we introduce MixER: Mixture of Expert\nReconstructors, a novel sparse top-1 MoE layer employing a custom gating update\nalgorithm based on $K$-means and least squares. Extensive experiments validate\nMixER's capabilities, demonstrating efficient training and scalability to\nsystems of up to ten parametric ordinary differential equations. However, our\nlayer underperforms state-of-the-art meta-learners in high-data regimes,\nparticularly when each expert is constrained to process only a fraction of a\ndataset composed of highly related data points. Further analysis with synthetic\nand neuroscientific time series suggests that the quality of the contextual\nrepresentations generated by MixER is closely linked to the presence of\nhierarchical structure in the data.", "AI": {"tldr": "MixER, a novel MoE-based method, addresses hierarchical DSR challenges with a custom gating algorithm, outperforming naive MoEs but lagging in high-data regimes.", "motivation": "Current meta-learning methods struggle with hierarchical DSR due to sparse, loosely related datasets, necessitating a more nuanced approach.", "method": "MixER uses a sparse top-1 MoE layer with a gating update algorithm based on K-means and least squares.", "result": "MixER scales efficiently to systems of up to ten parametric ODEs but underperforms in high-data regimes with highly related data.", "conclusion": "MixER's performance depends on hierarchical data structure, highlighting its niche applicability."}}
{"id": "2506.05982", "pdf": "https://arxiv.org/pdf/2506.05982", "abs": "https://arxiv.org/abs/2506.05982", "authors": ["Zonglin Wu", "Yule Xue", "Xin Wei", "Yiren Song"], "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks", "categories": ["cs.CV", "I.4.9"], "comment": "31 pages, 8 figures", "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.", "AI": {"tldr": "The paper introduces MCA-Bench, a unified benchmarking suite for evaluating the security of diverse CAPTCHA types, revealing vulnerabilities and proposing design principles for improvement.", "motivation": "The lack of a unified, large-scale, multimodal benchmark for CAPTCHA security evaluation motivates the creation of MCA-Bench.", "method": "MCA-Bench integrates various CAPTCHA types into a single protocol, using a shared vision-language model to fine-tune specialized cracking agents for each category.", "result": "Experiments show MCA-Bench effectively maps CAPTCHA vulnerabilities and provides the first quantitative analysis of challenge complexity, interaction depth, and solvability.", "conclusion": "The paper proposes actionable design principles for CAPTCHA hardening and identifies open challenges, encouraging community collaboration."}}
{"id": "2506.09009", "pdf": "https://arxiv.org/pdf/2506.09009", "abs": "https://arxiv.org/abs/2506.09009", "authors": ["Hakyung Sung", "Gyu-Ho Shin", "Chanyoung Lee", "You Kyung Sung", "Boo Kyung Jung"], "title": "UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags", "categories": ["cs.CL"], "comment": null, "summary": "The present study extends recent work on Universal Dependencies annotations\nfor second-language (L2) Korean by introducing a semi-automated framework that\nidentifies morphosyntactic constructions from XPOS sequences and aligns those\nconstructions with corresponding UPOS categories. We also broaden the existing\nL2-Korean corpus by annotating 2,998 new sentences from argumentative essays.\nTo evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean\nmorphosyntactic analysis models on datasets both with and without these\nalignments, using two NLP toolkits. Our results indicate that the aligned\ndataset not only improves consistency across annotation layers but also\nenhances morphosyntactic tagging and dependency-parsing accuracy, particularly\nin cases of limited annotated data.", "AI": {"tldr": "A semi-automated framework aligns XPOS sequences with UPOS categories for L2 Korean, improving annotation consistency and model performance.", "motivation": "To enhance morphosyntactic analysis for L2 Korean by aligning XPOS and UPOS categories and expanding the corpus.", "method": "Introduce a semi-automated framework for XPOS-UPOS alignment, annotate 2,998 new sentences, and evaluate using NLP toolkits.", "result": "Aligned dataset improves annotation consistency and boosts morphosyntactic tagging and dependency-parsing accuracy.", "conclusion": "XPOS-UPOS alignment enhances L2 Korean analysis, especially with limited annotated data."}}
{"id": "2505.05568", "pdf": "https://arxiv.org/pdf/2505.05568", "abs": "https://arxiv.org/abs/2505.05568", "authors": ["Yanbo Wang", "Xiyuan Wang", "Quan Gan", "Minjie Wang", "Qibin Yang", "David Wipf", "Muhan Zhang"], "title": "Griffin: Towards a Graph-Centric Relational Database Foundation Model", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": "Published at ICML 2025", "summary": "We introduce Griffin, the first foundation model attemptation designed\nspecifically for Relational Databases (RDBs). Unlike previous smaller models\nfocused on single RDB tasks, Griffin unifies the data encoder and task decoder\nto handle diverse tasks. Additionally, we enhance the architecture by\nincorporating a cross-attention module and a novel aggregator. Griffin utilizes\npretraining on both single-table and RDB datasets, employing advanced encoders\nfor categorical, numerical, and metadata features, along with innovative\ncomponents such as cross-attention modules and enhanced message-passing neural\nnetworks (MPNNs) to capture the complexities of relational data. Evaluated on\nlarge-scale, heterogeneous, and temporal graphs extracted from RDBs across\nvarious domains (spanning over 150 million nodes), Griffin demonstrates\nsuperior or comparable performance to individually trained models, excels in\nlow-data scenarios, and shows strong transferability with similarity and\ndiversity in pretraining across new datasets and tasks, highlighting its\npotential as a universally applicable foundation model for RDBs. Code available\nat https://github.com/yanxwb/Griffin.", "AI": {"tldr": "Griffin is a foundation model for Relational Databases (RDBs) unifying data encoding and task decoding, outperforming task-specific models with enhanced architecture and pretraining.", "motivation": "To create a unified foundation model for RDBs, addressing limitations of smaller, task-specific models by handling diverse tasks efficiently.", "method": "Incorporates cross-attention modules, novel aggregators, and advanced encoders for categorical, numerical, and metadata features, pretrained on single-table and RDB datasets.", "result": "Superior or comparable performance to task-specific models, excels in low-data scenarios, and shows strong transferability across new datasets.", "conclusion": "Griffin is a promising universally applicable foundation model for RDBs, with potential for broad adoption."}}
{"id": "2502.07783", "pdf": "https://arxiv.org/pdf/2502.07783", "abs": "https://arxiv.org/abs/2502.07783", "authors": ["Leyang Hu", "Matteo Gamba", "Randall Balestriero"], "title": "Curvature Tuning: Provable Training-free Model Steering From a Single Parameter", "categories": ["cs.LG"], "comment": null, "summary": "The scaling of model and data sizes has reshaped the AI landscape,\nestablishing finetuning pretrained models as the standard paradigm for solving\ndownstream tasks. However, dominant finetuning methods typically rely on weight\nadaptation, often lack interpretability, and depend on heuristically chosen\nhyperparameters. In this paper, we take a different perspective and shift the\nfocus from weights to activation functions, viewing them through the lens of\nspline operators. We propose Curvature Tuning (CT), an interpretable and\nprincipled steering method that modulates a model's decision boundary by\ninjecting a single hyperparameter into its activation functions. We show that\nCT provably adjusts model decision boundary curvature and, more fundamentally,\nprojects a model onto a space of smooth functions-thereby complementing current\nfinetuning methods, whose effect lies primarily in feature adaptation. Making\nthis hyperparameter trainable gives rise to a novel and highly\nparameter-efficient finetuning method. Empirically, CT improves both\ngeneralization and robustness. For example, it boosts downstream accuracy of\nResNet-50/152 by 7.14%/8.46% over linear probing and 4.64%/1.70% over LoRA\nacross 12 datasets, and improves robust accuracy on the $\\ell_\\infty$ benchmark\nfrom RobustBench by 1032.64%/1494.46%. Our code is available at\nhttps://github.com/Leon-Leyang/curvature-tuning.", "AI": {"tldr": "The paper introduces Curvature Tuning (CT), a method to adjust model decision boundaries by modifying activation functions, improving generalization and robustness over traditional finetuning methods.", "motivation": "Current finetuning methods lack interpretability and rely on heuristic hyperparameters, prompting a shift from weight adaptation to activation function modification.", "method": "CT modulates decision boundaries by injecting a hyperparameter into activation functions, projecting models onto smooth function spaces.", "result": "CT boosts accuracy (e.g., 7.14%/8.46% for ResNet-50/152) and robust accuracy (e.g., 1032.64%/1494.46% on RobustBench) over baselines.", "conclusion": "CT offers an interpretable, parameter-efficient finetuning alternative, enhancing both generalization and robustness."}}
{"id": "2506.06733", "pdf": "https://arxiv.org/pdf/2506.06733", "abs": "https://arxiv.org/abs/2506.06733", "authors": ["Ruoxuan Zhang", "Jidong Gao", "Bin Wen", "Hongxia Xie", "Chenming Zhang", "Hong-Han Shuai", "Wen-Huang Cheng"], "title": "RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation", "categories": ["cs.CV"], "comment": "This is an extended version of arXiv:2503.05228", "summary": "Creating recipe images is a key challenge in food computing, with\napplications in culinary education and multimodal recipe assistants. However,\nexisting datasets lack fine-grained alignment between recipe goals, step-wise\ninstructions, and visual content. We present RecipeGen, the first large-scale,\nreal-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video\n(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,\n196,724 images, and 4,491 videos, covering diverse ingredients, cooking\nprocedures, styles, and dish types. We further propose domain-specific\nevaluation metrics to assess ingredient fidelity and interaction modeling,\nbenchmark representative T2I, I2V, and T2V models, and provide insights for\nfuture recipe generation models. Project page is available now.", "AI": {"tldr": "RecipeGen is a large-scale benchmark for recipe-based text-to-image, image-to-video, and text-to-video generation, addressing the lack of fine-grained alignment in existing datasets.", "motivation": "Existing datasets lack alignment between recipe goals, instructions, and visuals, hindering applications in food computing.", "method": "RecipeGen introduces 26,453 recipes with 196,724 images and 4,491 videos, along with domain-specific metrics for evaluation.", "result": "The benchmark evaluates T2I, I2V, and T2V models, providing insights for future recipe generation models.", "conclusion": "RecipeGen fills a gap in food computing datasets and offers a foundation for future research in recipe-based generation tasks."}}
{"id": "2506.09021", "pdf": "https://arxiv.org/pdf/2506.09021", "abs": "https://arxiv.org/abs/2506.09021", "authors": ["Hakyung Sung", "Karla Csuros", "Min-Chang Sung"], "title": "Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features", "categories": ["cs.CL"], "comment": null, "summary": "This study examines the lexical and syntactic interventions of human and LLM\nproofreading aimed at improving overall intelligibility in identical second\nlanguage writings, and evaluates the consistency of outcomes across three LLMs\n(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and\nLLM proofreading enhance bigram lexical features, which may contribute to\nbetter coherence and contextual connectedness between adjacent words. However,\nLLM proofreading exhibits a more generative approach, extensively reworking\nvocabulary and sentence structures, such as employing more diverse and\nsophisticated vocabulary and incorporating a greater number of adjective\nmodifiers in noun phrases. The proofreading outcomes are highly consistent in\nmajor lexical and syntactic features across the three models.", "AI": {"tldr": "Human and LLM proofreading improve intelligibility in second language writing, with LLMs showing more generative edits and consistent outcomes across models.", "motivation": "To compare the effectiveness and consistency of human and LLM proofreading in enhancing second language writing.", "method": "Examined lexical and syntactic interventions by humans and three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b) on identical texts.", "result": "Both human and LLM proofreading improved bigram lexical features. LLMs were more generative, using diverse vocabulary and syntactic structures, with high consistency across models.", "conclusion": "LLM proofreading is effective and consistent, offering a generative approach to enhancing writing intelligibility."}}
{"id": "2505.08265", "pdf": "https://arxiv.org/pdf/2505.08265", "abs": "https://arxiv.org/abs/2505.08265", "authors": ["Hang Gao", "Wenxuan Huang", "Fengge Wu", "Junsuo Zhao", "Changwen Zheng", "Huaping Liu"], "title": "LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "The use of large language models (LLMs) as feature enhancers to optimize node\nrepresentations, which are then used as inputs for graph neural networks\n(GNNs), has shown significant potential in graph representation learning.\nHowever, the fundamental properties of this approach remain underexplored. To\naddress this issue, we propose conducting a more in-depth analysis of this\nissue based on the interchange intervention method. First, we construct a\nsynthetic graph dataset with controllable causal relationships, enabling\nprecise manipulation of semantic relationships and causal modeling to provide\ndata for analysis. Using this dataset, we conduct interchange interventions to\nexamine the deeper properties of LLM enhancers and GNNs, uncovering their\nunderlying logic and internal mechanisms. Building on the analytical results,\nwe design a plug-and-play optimization module to improve the information\ntransfer between LLM enhancers and GNNs. Experiments across multiple datasets\nand models validate the proposed module.", "AI": {"tldr": "The paper explores using LLMs to enhance node representations for GNNs, proposes an in-depth analysis via interchange interventions, and introduces an optimization module to improve LLM-GNN information transfer.", "motivation": "The potential of LLMs as feature enhancers for GNNs is underexplored, prompting a deeper analysis of their properties and mechanisms.", "method": "Constructs a synthetic graph dataset with controllable causal relationships, uses interchange interventions to analyze LLM enhancers and GNNs, and designs an optimization module.", "result": "Experiments validate the proposed optimization module across multiple datasets and models.", "conclusion": "The study provides insights into LLM-GNN interactions and offers a practical module to enhance their performance."}}
{"id": "2502.09252", "pdf": "https://arxiv.org/pdf/2502.09252", "abs": "https://arxiv.org/abs/2502.09252", "authors": ["Andrew Draganov", "Sharvaree Vadgama", "Sebastian Damrich", "Jan Niklas B\u00f6hm", "Lucas Maes", "Dmitry Kobak", "Erik Bekkers"], "title": "On the Importance of Embedding Norms in Self-Supervised Learning", "categories": ["cs.LG"], "comment": null, "summary": "Self-supervised learning (SSL) allows training data representations without a\nsupervised signal and has become an important paradigm in machine learning.\nMost SSL methods employ the cosine similarity between embedding vectors and\nhence effectively embed data on a hypersphere. While this seemingly implies\nthat embedding norms cannot play any role in SSL, a few recent works have\nsuggested that embedding norms have properties related to network convergence\nand confidence. In this paper, we resolve this apparent contradiction and\nsystematically establish the embedding norm's role in SSL training. Using\ntheoretical analysis, simulations, and experiments, we show that embedding\nnorms (i) govern SSL convergence rates and (ii) encode network confidence, with\nsmaller norms corresponding to unexpected samples. Additionally, we show that\nmanipulating embedding norms can have large effects on convergence speed. Our\nfindings demonstrate that SSL embedding norms are integral to understanding and\noptimizing network behavior.", "AI": {"tldr": "The paper resolves the contradiction around embedding norms in SSL, showing they govern convergence rates and encode network confidence, with smaller norms indicating unexpected samples.", "motivation": "To clarify the role of embedding norms in SSL, as their significance contradicts the hypersphere-based assumptions of most SSL methods.", "method": "Theoretical analysis, simulations, and experiments to study embedding norms' impact on SSL.", "result": "Embedding norms influence SSL convergence rates and encode confidence (smaller norms for unexpected samples). Manipulating norms affects convergence speed.", "conclusion": "Embedding norms are crucial for understanding and optimizing SSL network behavior."}}
{"id": "2506.07497", "pdf": "https://arxiv.org/pdf/2506.07497", "abs": "https://arxiv.org/abs/2506.07497", "authors": ["Xiangyu Guo", "Zhanqian Wu", "Kaixin Xiong", "Ziyang Xu", "Lijun Zhou", "Gangwei Xu", "Shaoqing Xu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency", "categories": ["cs.CV"], "comment": null, "summary": "We present Genesis, a unified framework for joint generation of multi-view\ndriving videos and LiDAR sequences with spatio-temporal and cross-modal\nconsistency. Genesis employs a two-stage architecture that integrates a\nDiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR\ngenerator with NeRF-based rendering and adaptive sampling. Both modalities are\ndirectly coupled through a shared latent space, enabling coherent evolution\nacross visual and geometric domains. To guide the generation with structured\nsemantics, we introduce DataCrafter, a captioning module built on\nvision-language models that provides scene-level and instance-level\nsupervision. Extensive experiments on the nuScenes benchmark demonstrate that\nGenesis achieves state-of-the-art performance across video and LiDAR metrics\n(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including\nsegmentation and 3D detection, validating the semantic fidelity and practical\nutility of the generated data.", "AI": {"tldr": "Genesis is a framework for generating multi-view driving videos and LiDAR sequences with consistency, using a two-stage architecture and shared latent space. It achieves top performance on benchmarks and aids downstream tasks.", "motivation": "To create a unified solution for generating consistent multi-view driving videos and LiDAR sequences, addressing the need for coherent multi-modal data in autonomous driving research.", "method": "Uses a two-stage architecture: a DiT-based video diffusion model with 3D-VAE encoding and a BEV-aware LiDAR generator with NeRF-based rendering. Integrates modalities via a shared latent space and employs DataCrafter for semantic guidance.", "result": "Achieves state-of-the-art performance on nuScenes (FVD 16.95, FID 4.24, Chamfer 0.611) and improves downstream tasks like segmentation and 3D detection.", "conclusion": "Genesis effectively generates high-quality, semantically consistent multi-modal data, proving its utility for autonomous driving applications."}}
{"id": "2506.09047", "pdf": "https://arxiv.org/pdf/2506.09047", "abs": "https://arxiv.org/abs/2506.09047", "authors": ["Yaniv Nikankin", "Dana Arad", "Yossi Gandelsman", "Yonatan Belinkov"], "title": "Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs", "categories": ["cs.CL", "68T5", "I.2.7"], "comment": null, "summary": "Vision-Language models (VLMs) show impressive abilities to answer questions\non visual inputs (e.g., counting objects in an image), yet demonstrate higher\naccuracies when performing an analogous task on text (e.g., counting words in a\ntext). We investigate this accuracy gap by identifying and comparing the\n\\textit{circuits} - the task-specific computational sub-graphs - in different\nmodalities. We show that while circuits are largely disjoint between\nmodalities, they implement relatively similar functionalities: the differences\nlie primarily in processing modality-specific data positions (an image or a\ntext sequence). Zooming in on the image data representations, we observe they\nbecome aligned with the higher-performing analogous textual representations\nonly towards later layers, too late in processing to effectively influence\nsubsequent positions. To overcome this, we patch the representations of visual\ndata tokens from later layers back into earlier layers. In experiments with\nmultiple tasks and models, this simple intervention closes a third of the\nperformance gap between the modalities, on average. Our analysis sheds light on\nthe multi-modal performance gap in VLMs and suggests a training-free approach\nfor reducing it.", "AI": {"tldr": "The paper investigates the performance gap between visual and textual tasks in Vision-Language Models (VLMs) by analyzing task-specific circuits and proposes a method to reduce this gap by patching visual representations.", "motivation": "VLMs perform better on textual tasks than analogous visual tasks, and the study aims to understand and mitigate this discrepancy.", "method": "The authors compare circuits (task-specific computational sub-graphs) across modalities, analyze data representations, and patch visual tokens from later layers into earlier ones.", "result": "Patching visual representations closes a third of the performance gap between modalities on average.", "conclusion": "The study reveals insights into the multi-modal performance gap and offers a training-free solution to improve VLM performance on visual tasks."}}
{"id": "2505.10482", "pdf": "https://arxiv.org/pdf/2505.10482", "abs": "https://arxiv.org/abs/2505.10482", "authors": ["Ningyuan Yang", "Jiaxuan Gao", "Feng Gao", "Yi Wu", "Chao Yu"], "title": "Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages for main text, 23 pages in total, submitted to Neurips, 13\n  figures", "summary": "Diffusion policies, widely adopted in decision-making scenarios such as\nrobotics, gaming and autonomous driving, are capable of learning diverse skills\nfrom demonstration data due to their high representation power. However, the\nsub-optimal and limited coverage of demonstration data could lead to diffusion\npolicies that generate sub-optimal trajectories and even catastrophic failures.\nWhile reinforcement learning (RL)-based fine-tuning has emerged as a promising\nsolution to address these limitations, existing approaches struggle to\neffectively adapt Proximal Policy Optimization (PPO) to diffusion models. This\nchallenge stems from the computational intractability of action likelihood\nestimation during the denoising process, which leads to complicated\noptimization objectives. In our experiments starting from randomly initialized\npolicies, we find that online tuning of Diffusion Policies demonstrates much\nlower sample efficiency compared to directly applying PPO on MLP policies\n(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework\nthat reformulates Diffusion Policy as a noise-conditioned deterministic policy.\nBy treating each denoising step as a differentiable transformation conditioned\non pre-sampled noise, NCDPO enables tractable likelihood evaluation and\ngradient backpropagation through all diffusion timesteps. Our experiments\ndemonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when\ntraining from scratch, outperforming existing methods in both sample efficiency\nand final performance across diverse benchmarks, including continuous robot\ncontrol and multi-agent game scenarios. Furthermore, our experimental results\nshow that our method is robust to the number denoising timesteps in the\nDiffusion Policy.", "AI": {"tldr": "NCDPO improves diffusion policies by reformulating them as noise-conditioned deterministic policies, enabling efficient RL fine-tuning and outperforming existing methods.", "motivation": "Diffusion policies suffer from sub-optimal trajectories due to limited demonstration data, and existing RL fine-tuning struggles with computational intractability.", "method": "NCDPO treats denoising steps as differentiable transformations, enabling tractable likelihood evaluation and gradient backpropagation.", "result": "NCDPO matches MLP+PPO's sample efficiency, outperforms benchmarks in robot control and multi-agent games, and is robust to denoising timesteps.", "conclusion": "NCDPO effectively addresses diffusion policy limitations, offering superior performance and efficiency in diverse decision-making tasks."}}
{"id": "2502.09502", "pdf": "https://arxiv.org/pdf/2502.09502", "abs": "https://arxiv.org/abs/2502.09502", "authors": ["Jiachang Liu", "Soroosh Shafiee", "Andrea Lodi"], "title": "Scalable First-order Method for Certifying Optimal k-Sparse GLMs", "categories": ["cs.LG", "math.OC"], "comment": "ICML 2025 camera ready, typo fixed", "summary": "This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.", "AI": {"tldr": "A first-order proximal gradient algorithm is proposed to efficiently compute dual bounds for certifying optimality in sparse GLMs within a BnB framework, outperforming existing methods in scalability and speed.", "motivation": "Existing methods for certifying optimality in sparse GLMs are computationally intensive or slow, limiting scalability to large problems.", "method": "A first-order proximal gradient algorithm solves the perspective relaxation of the problem, with an exact log-linear time proximal operator and a restart strategy for faster convergence.", "result": "The method accelerates dual bound computations and effectively provides optimality certificates for large-scale problems, as shown in experiments.", "conclusion": "The proposed algorithm enhances scalability and efficiency in certifying optimality for sparse GLMs, addressing limitations of prior methods."}}
{"id": "2506.07737", "pdf": "https://arxiv.org/pdf/2506.07737", "abs": "https://arxiv.org/abs/2506.07737", "authors": ["Xuemei Chen", "Huamin Wang", "Hangchi Shen", "Shukai Duan", "Shiping Wen", "Tingwen Huang"], "title": "SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding", "categories": ["cs.CV"], "comment": null, "summary": "Low energy consumption for 3D object detection is an important research area\nbecause of the increasing energy consumption with their wide application in\nfields such as autonomous driving. The spiking neural networks (SNNs) with\nlow-power consumption characteristics can provide a novel solution for this\nresearch. Therefore, we apply SNNs to monocular 3D object detection and propose\nthe SpikeSMOKE architecture in this paper, which is a new attempt for low-power\nmonocular 3D object detection. As we all know, discrete signals of SNNs will\ngenerate information loss and limit their feature expression ability compared\nwith the artificial neural networks (ANNs).In order to address this issue,\ninspired by the filtering mechanism of biological neuronal synapses, we propose\na cross-scale gated coding mechanism(CSGC), which can enhance feature\nrepresentation by combining cross-scale fusion of attentional methods and gated\nfiltering mechanisms.In addition, to reduce the computation and increase the\nspeed of training, we present a novel light-weight residual block that can\nmaintain spiking computing paradigm and the highest possible detection\nperformance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,\nthe proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,\nModerate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by\nAP|R11 at 0.7 IoU threshold, respectively. It is important to note that the\nresults of SpikeSMOKE can significantly reduce energy consumption compared to\nthe results on SMOKE. For example,the energy consumption can be reduced by\n72.2% on the hard category, while the detection performance is reduced by only\n4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3\ntimes and computation by 10 times compared to SMOKE.", "AI": {"tldr": "The paper proposes SpikeSMOKE, a low-power 3D object detection architecture using spiking neural networks (SNNs), enhanced by a cross-scale gated coding mechanism (CSGC) and a lightweight residual block. It achieves competitive performance on the KITTI dataset while significantly reducing energy consumption.", "motivation": "Addressing the high energy consumption in 3D object detection, especially in applications like autonomous driving, by leveraging the low-power characteristics of SNNs.", "method": "Introduces SpikeSMOKE with CSGC for feature enhancement and a lightweight residual block to reduce computation.", "result": "Improved detection performance (e.g., +2.82 AP on Easy category) and reduced energy consumption (e.g., 72.2% reduction in hard category) compared to baseline.", "conclusion": "SpikeSMOKE offers an efficient, low-power solution for monocular 3D object detection with minimal performance trade-offs."}}
{"id": "2505.18230", "pdf": "https://arxiv.org/pdf/2505.18230", "abs": "https://arxiv.org/abs/2505.18230", "authors": ["Louis B\u00e9thune", "David Vigouroux", "Yilun Du", "Rufin VanRullen", "Thomas Serre", "Victor Boutin"], "title": "Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "What is the shortest path between two data points lying in a high-dimensional\nspace? While the answer is trivial in Euclidean geometry, it becomes\nsignificantly more complex when the data lies on a curved manifold -- requiring\na Riemannian metric to describe the space's local curvature. Estimating such a\nmetric, however, remains a major challenge in high dimensions.\n  In this work, we propose a method for deriving Riemannian metrics directly\nfrom pretrained Energy-Based Models (EBMs) -- a class of generative models that\nassign low energy to high-density regions. These metrics define spatially\nvarying distances, enabling the computation of geodesics -- shortest paths that\nfollow the data manifold's intrinsic geometry. We introduce two novel metrics\nderived from EBMs and show that they produce geodesics that remain closer to\nthe data manifold and exhibit lower curvature distortion, as measured by\nalignment with ground-truth trajectories. We evaluate our approach on\nincreasingly complex datasets: synthetic datasets with known data density,\nrotated character images with interpretable geometry, and high-resolution\nnatural images embedded in a pretrained VAE latent space.\n  Our results show that EBM-derived metrics consistently outperform established\nbaselines, especially in high-dimensional settings. Our work is the first to\nderive Riemannian metrics from EBMs, enabling data-aware geodesics and\nunlocking scalable, geometry-driven learning for generative modeling and\nsimulation.", "AI": {"tldr": "The paper proposes a method to derive Riemannian metrics from pretrained Energy-Based Models (EBMs) for computing geodesics in high-dimensional curved manifolds, showing improved performance over baselines.", "motivation": "The challenge of estimating Riemannian metrics in high-dimensional curved manifolds motivates the need for scalable, data-aware methods to compute geodesics.", "method": "The authors introduce two novel metrics derived from EBMs, which define spatially varying distances to compute geodesics that align with the data manifold's intrinsic geometry.", "result": "EBM-derived metrics outperform baselines, producing geodesics closer to the data manifold with lower curvature distortion, validated on synthetic, character, and natural image datasets.", "conclusion": "This work pioneers deriving Riemannian metrics from EBMs, enabling scalable, geometry-driven learning for generative modeling and simulation."}}
{"id": "2502.11420", "pdf": "https://arxiv.org/pdf/2502.11420", "abs": "https://arxiv.org/abs/2502.11420", "authors": ["Yingqing Guo", "Yukang Yang", "Hui Yuan", "Mengdi Wang"], "title": "Training-Free Guidance Beyond Differentiability: Scalable Path Steering with Tree Search in Diffusion and Flow Models", "categories": ["cs.LG"], "comment": null, "summary": "Training-free guidance enables controlled generation in diffusion and flow\nmodels, but most methods rely on gradients and assume differentiable\nobjectives. This work focuses on training-free guidance addressing challenges\nfrom non-differentiable objectives and discrete data distributions. We propose\nTreeG: Tree Search-Based Path Steering Guidance, applicable to both continuous\nand discrete settings in diffusion and flow models. TreeG offers a unified\nframework for training-free guidance by proposing, evaluating, and selecting\ncandidates at each step, enhanced with tree search over active paths and\nparallel exploration. We comprehensively investigate the design space of TreeG\nover the candidate proposal module and the evaluation function, instantiating\nTreeG into three novel algorithms. Our experiments show that TreeG consistently\noutperforms top guidance baselines in symbolic music generation, small molecule\ndesign, and enhancer DNA design with improvements of 29.01%, 16.6%, and 18.43%.\nAdditionally, we identify an inference-time scaling law showing TreeG's\nscalability in inference-time computation.", "AI": {"tldr": "TreeG, a training-free guidance method using tree search, outperforms baselines in non-differentiable and discrete settings for diffusion and flow models.", "motivation": "Addressing challenges of non-differentiable objectives and discrete data distributions in training-free guidance for controlled generation.", "method": "Proposes TreeG, a unified framework with tree search-based path steering, candidate proposal, and evaluation. Three novel algorithms are derived.", "result": "TreeG improves performance by 29.01%, 16.6%, and 18.43% in symbolic music, small molecule, and DNA design tasks. Shows scalability via an inference-time scaling law.", "conclusion": "TreeG is a scalable, effective solution for training-free guidance in both continuous and discrete settings."}}
{"id": "2506.07943", "pdf": "https://arxiv.org/pdf/2506.07943", "abs": "https://arxiv.org/abs/2506.07943", "authors": ["Yizhen Li", "Dell Zhang", "Xuelong Li", "Yiqing Shen"], "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations", "categories": ["cs.CV", "cs.AI"], "comment": "This work was submitted without the consent of all co-authors. We\n  request withdrawal until all parties agree", "summary": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.", "AI": {"tldr": "DTwinSeger introduces a two-stage RS approach using Digital Twin (DT) representation to decouple perception from reasoning, achieving state-of-the-art performance by leveraging LLMs for explicit reasoning.", "motivation": "Current RS methods disrupt spatial relationships due to tokenization; DTwinSeger aims to preserve these relationships and improve reasoning.", "method": "Transform images into structured DT representations, then use LLMs for reasoning. Includes supervised fine-tuning and dataset Seg-DT.", "result": "Achieves top performance on RS and referring segmentation benchmarks.", "conclusion": "DT representation effectively bridges vision and text, enabling complex reasoning with LLMs alone."}}
{"id": "2412.20367", "pdf": "https://arxiv.org/pdf/2412.20367", "abs": "https://arxiv.org/abs/2412.20367", "authors": ["Junqiao Wang", "Zeng Zhang", "Yangfan He", "Zihao Zhang", "Yuyang Song", "Tianyu Shi", "Yuchen Li", "Hengyuan Xu", "Kunyu Wu", "Xin Yi", "Zhongwei Wan", "Xinhang Yuan", "Kuan Lu", "Menghao Huo", "Tang Jingqun", "Guangwu Qian", "Keqin Li", "Qiuwu Chen", "Lewei He"], "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nlarge language models (LLMs) in code generation and optimization. This survey\nsystematically reviews RL-driven techniques across the code development\nlifecycle, from compiler-level optimizations and resource allocation strategies\nto end-to-end code synthesis frameworks. We first examine classical and modern\nRL algorithms -- spanning policy gradients, actor-critic methods,\nhuman-feedback alignment, and preference-based optimization -- and their\nadaptations to the unique challenges of code generation, such as sparse and\ndelayed rewards. Next, we analyze key benchmarks, datasets, and evaluation\nmetrics that drive progress in RL-augmented Code LLMs. Finally, we identify\nopen problems, including the need for richer feedback sources, support for\nlow-level and domain-specific languages, and methods to reduce computational\noverhead. By consolidating current insights and outlining future directions,\nthis work aims to guide researchers and practitioners in leveraging RL to\nproduce more robust, efficient, and human-aligned code generation systems.", "AI": {"tldr": "This survey reviews RL techniques for enhancing LLMs in code generation, covering algorithms, benchmarks, and open challenges.", "motivation": "To systematically explore how RL can improve code generation and optimization in LLMs, addressing unique challenges like sparse rewards.", "method": "Examines RL algorithms (e.g., policy gradients, actor-critic) and their adaptations for code generation, alongside benchmarks and evaluation metrics.", "result": "Identifies gaps like richer feedback sources and support for low-level languages, consolidating insights for future research.", "conclusion": "Aims to guide researchers in using RL for more robust and efficient code generation systems."}}
{"id": "2505.22370", "pdf": "https://arxiv.org/pdf/2505.22370", "abs": "https://arxiv.org/abs/2505.22370", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Weili Guan", "Min Zhang", "Liqiang Nie"], "title": "SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 4 figures", "summary": "Continual Learning requires a model to learn multiple tasks in sequence while\nmaintaining both stability:preserving knowledge from previously learned tasks,\nand plasticity:effectively learning new tasks. Gradient projection has emerged\nas an effective and popular paradigm in CL, where it partitions the gradient\nspace of previously learned tasks into two orthogonal subspaces: a primary\nsubspace and a minor subspace. New tasks are learned effectively within the\nminor subspace, thereby reducing interference with previously acquired\nknowledge. However, existing Gradient Projection methods struggle to achieve an\noptimal balance between plasticity and stability, as it is hard to\nappropriately partition the gradient space. In this work, we consider a\ncontinual learning paradigm based on Low-Rank Adaptation, which has gained\nconsiderable attention due to its efficiency and wide applicability, and\npropose a novel approach for continual learning, called SplitLoRA. We first\nprovide a theoretical analysis of how subspace partitioning affects model\nstability and plasticity. Informed by this analysis, we then introduce an\neffective method that derives the optimal partition of the gradient space for\npreviously learned tasks. This approach effectively balances stability and\nplasticity in continual learning. Experimental results on multiple datasets\ndemonstrate that the proposed method achieves state-of-the-art performance.", "AI": {"tldr": "The paper introduces SplitLoRA, a novel continual learning method using Low-Rank Adaptation to balance stability and plasticity by optimally partitioning gradient space.", "motivation": "Existing gradient projection methods in continual learning struggle to balance stability (retaining old knowledge) and plasticity (learning new tasks).", "method": "Proposes SplitLoRA, which theoretically analyzes gradient subspace partitioning and derives an optimal partition for stability and plasticity.", "result": "Experiments show SplitLoRA achieves state-of-the-art performance on multiple datasets.", "conclusion": "SplitLoRA effectively balances stability and plasticity in continual learning, outperforming existing methods."}}
{"id": "2502.17361", "pdf": "https://arxiv.org/pdf/2502.17361", "abs": "https://arxiv.org/abs/2502.17361", "authors": ["Han-Jia Ye", "Si-Yang Liu", "Wei-Lun Chao"], "title": "A Closer Look at TabPFN v2: Understanding Its Strengths and Extending Its Capabilities", "categories": ["cs.LG"], "comment": null, "summary": "Tabular datasets are inherently heterogeneous, presenting significant\nchallenges for developing pre-trained foundation models. The recently\nintroduced transformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2)\nachieves unprecedented in-context learning performance across diverse\ndownstream datasets, marking a pivotal advancement in tabular foundation\nmodels. In this paper, we take a closer look at TabPFN v2 to examine how it\neffectively handles heterogeneity and achieves high predictive accuracy, and to\nexplore how its limitations in high-dimensional, many-category, and large-scale\ntasks can be mitigated. We find that TabPFN v2 can infer attribute\nrelationships even when provided with randomized attribute token inputs,\neliminating the need to explicitly learn dataset-specific attribute embeddings\nto address heterogeneity. We further show that TabPFN v2 can be transformed\ninto a feature extractor, revealing its ability to construct a highly separable\nfeature space for accurate predictions. Lastly, we demonstrate that TabPFN v2's\nlimitations can be addressed through a test-time divide-and-conquer strategy,\nenabling scalable inference without requiring re-training. By uncovering the\nmechanisms behind TabPFN v2's success and introducing strategies to extend its\napplicability, this study offers key insights into the design of future tabular\nfoundation models.", "AI": {"tldr": "TabPFN v2, a transformer-based model, excels in handling heterogeneous tabular data and achieves high predictive accuracy without needing explicit attribute embeddings. Its limitations in high-dimensional tasks are addressed via a divide-and-conquer strategy.", "motivation": "To understand how TabPFN v2 handles tabular data heterogeneity and achieves high accuracy, while exploring ways to mitigate its limitations in complex tasks.", "method": "Examines TabPFN v2's ability to infer attribute relationships without explicit embeddings, tests its feature extraction capability, and introduces a divide-and-conquer strategy for scalability.", "result": "TabPFN v2 infers attribute relationships even with randomized inputs and constructs a separable feature space. Its limitations are mitigated by scalable inference strategies.", "conclusion": "The study provides insights for designing future tabular foundation models by revealing TabPFN v2's mechanisms and extending its applicability."}}
{"id": "2506.07986", "pdf": "https://arxiv.org/pdf/2506.07986", "abs": "https://arxiv.org/abs/2506.07986", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "categories": ["cs.CV"], "comment": "Project Page: https://vchitect.github.io/TACA/", "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "AI": {"tldr": "The paper proposes Temperature-Adjusted Cross-modal Attention (TACA) to improve text-image alignment in MM-DiT models by addressing token imbalance and timestep-aware attention issues.", "motivation": "State-of-the-art MM-DiT models struggle with precise alignment between text prompts and generated content due to cross-modal attention suppression and lack of timestep-aware weighting.", "method": "Introduces TACA, a parameter-efficient method using temperature scaling and timestep-dependent adjustment, combined with LoRA fine-tuning.", "result": "TACA significantly enhances text-image alignment on T2I-CompBench, improving object appearance, attribute binding, and spatial relationships in models like FLUX and SD3.5.", "conclusion": "Balancing cross-modal attention is crucial for semantic fidelity in text-to-image diffusion models, and TACA offers a computationally efficient solution."}}
{"id": "2502.20383", "pdf": "https://arxiv.org/pdf/2502.20383", "abs": "https://arxiv.org/abs/2502.20383", "authors": ["Jeffrey Yang Fan Chiang", "Seungjae Lee", "Jia-Bin Huang", "Furong Huang", "Yizheng Chen"], "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis", "categories": ["cs.LG", "cs.CL"], "comment": "Project website: http://vulnerable-ai-agents.github.io", "summary": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies.", "AI": {"tldr": "Web AI agents are more vulnerable than standalone LLMs due to factors like embedding user goals, multi-step actions, and observational capabilities, requiring a refined evaluation framework.", "motivation": "The study aims to understand why Web AI agents, despite being safety-aligned, are more vulnerable than standalone LLMs, given their flexibility and exposure to adversarial inputs.", "method": "The research employs a component-level analysis and a systematic evaluation framework to identify key vulnerability factors.", "result": "Three critical factors increasing vulnerability are identified: embedding user goals, multi-step action generation, and observational capabilities.", "conclusion": "The findings emphasize the need for improved security in AI agent design and offer insights for targeted defense strategies."}}
{"id": "2505.23032", "pdf": "https://arxiv.org/pdf/2505.23032", "abs": "https://arxiv.org/abs/2505.23032", "authors": ["Dongwoo Lee", "Dong Bok Lee", "Steven Adriaensen", "Juho Lee", "Sung Ju Hwang", "Frank Hutter", "Seon Joo Kim", "Hae Beom Lee"], "title": "Bayesian Neural Scaling Law Extrapolation with Prior-Fitted Networks", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Scaling has been a major driver of recent advancements in deep learning.\nNumerous empirical studies have found that scaling laws often follow the\npower-law and proposed several variants of power-law functions to predict the\nscaling behavior at larger scales. However, existing methods mostly rely on\npoint estimation and do not quantify uncertainty, which is crucial for\nreal-world applications involving decision-making problems such as determining\nthe expected performance improvements achievable by investing additional\ncomputational resources. In this work, we explore a Bayesian framework based on\nPrior-data Fitted Networks (PFNs) for neural scaling law extrapolation.\nSpecifically, we design a prior distribution that enables the sampling of\ninfinitely many synthetic functions resembling real-world neural scaling laws,\nallowing our PFN to meta-learn the extrapolation. We validate the effectiveness\nof our approach on real-world neural scaling laws, comparing it against both\nthe existing point estimation methods and Bayesian approaches. Our method\ndemonstrates superior performance, particularly in data-limited scenarios such\nas Bayesian active learning, underscoring its potential for reliable,\nuncertainty-aware extrapolation in practical applications.", "AI": {"tldr": "A Bayesian framework using Prior-data Fitted Networks (PFNs) is proposed for neural scaling law extrapolation, outperforming point estimation methods with uncertainty-aware predictions.", "motivation": "Existing scaling law methods lack uncertainty quantification, crucial for decision-making in resource allocation.", "method": "Uses PFNs with a designed prior distribution to sample synthetic scaling functions, enabling meta-learning for extrapolation.", "result": "Outperforms point estimation and Bayesian methods, especially in data-limited scenarios like active learning.", "conclusion": "The approach provides reliable, uncertainty-aware scaling law extrapolation for practical applications."}}
{"id": "2502.18377", "pdf": "https://arxiv.org/pdf/2502.18377", "abs": "https://arxiv.org/abs/2502.18377", "authors": ["Adeel Pervez", "Efstratios Gavves", "Francesco Locatello"], "title": "Mechanistic PDE Networks for Discovery of Governing Equations", "categories": ["cs.LG"], "comment": null, "summary": "We present Mechanistic PDE Networks -- a model for discovery of governing\npartial differential equations from data. Mechanistic PDE Networks represent\nspatiotemporal data as space-time dependent linear partial differential\nequations in neural network hidden representations. The represented PDEs are\nthen solved and decoded for specific tasks. The learned PDE representations\nnaturally express the spatiotemporal dynamics in data in neural network hidden\nspace, enabling increased power for dynamical modeling. Solving the PDE\nrepresentations in a compute and memory-efficient way, however, is a\nsignificant challenge. We develop a native, GPU-capable, parallel, sparse, and\ndifferentiable multigrid solver specialized for linear partial differential\nequations that acts as a module in Mechanistic PDE Networks. Leveraging the PDE\nsolver, we propose a discovery architecture that can discover nonlinear PDEs in\ncomplex settings while also being robust to noise. We validate PDE discovery on\na number of PDEs, including reaction-diffusion and Navier-Stokes equations.", "AI": {"tldr": "Mechanistic PDE Networks discover governing PDEs from data using neural networks and a specialized solver.", "motivation": "To model spatiotemporal dynamics efficiently and discover governing PDEs from noisy or complex data.", "method": "Uses neural networks to represent PDEs in hidden space and a GPU-capable, parallel, sparse multigrid solver for efficient computation.", "result": "Validated on reaction-diffusion and Navier-Stokes equations, showing robustness to noise and effectiveness in PDE discovery.", "conclusion": "The framework successfully discovers nonlinear PDEs in complex settings with computational efficiency."}}
{"id": "2506.08010", "pdf": "https://arxiv.org/pdf/2506.08010", "abs": "https://arxiv.org/abs/2506.08010", "authors": ["Nick Jiang", "Amil Dravid", "Alexei Efros", "Yossi Gandelsman"], "title": "Vision Transformers Don't Need Trained Registers", "categories": ["cs.CV", "cs.AI"], "comment": "Project page and code: https://avdravid.github.io/test-time-registers", "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.", "AI": {"tldr": "The paper identifies and addresses high-norm tokens in Vision Transformers that cause noisy attention maps, proposing a training-free method to mitigate these artifacts by shifting activations to an untrained token.", "motivation": "To understand and resolve the issue of high-norm tokens degrading attention maps and downstream performance in Vision Transformers without requiring retraining.", "method": "Shifts high-norm activations from identified neurons to an untrained token, mimicking the effect of register tokens without retraining.", "result": "Produces cleaner attention maps, improves downstream task performance, and matches models trained with register tokens.", "conclusion": "Test-time registers offer a training-free solution to improve pre-trained models, enhancing interpretability and performance."}}
{"id": "2504.15629", "pdf": "https://arxiv.org/pdf/2504.15629", "abs": "https://arxiv.org/abs/2504.15629", "authors": ["Harsh Maheshwari", "Srikanth Tenneti", "Alwarappan Nakkiran"], "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of\nLarge Language Models (LLMs), revolutionizing information search and\nconsumption. RAG systems combine traditional search capabilities with LLMs to\ngenerate comprehensive answers to user queries, ideally with accurate\ncitations. However, in our experience of developing a RAG product, LLMs often\nstruggle with source attribution, aligning with other industry studies\nreporting citation accuracy rates of only about 74% for popular generative\nsearch engines. To address this, we present efficient post-processing\nalgorithms to improve citation accuracy in LLM-generated responses, with\nminimal impact on latency and cost. Our approaches cross-check generated\ncitations against retrieved articles using methods including keyword + semantic\nmatching, fine tuned model with BERTScore, and a lightweight LLM-based\ntechnique. Our experimental results demonstrate a relative improvement of\n15.46% in the overall accuracy metrics of our RAG system. This significant\nenhancement potentially enables a shift from our current larger language model\nto a relatively smaller model that is approximately 12x more cost-effective and\n3x faster in inference time, while maintaining comparable performance. This\nresearch contributes to enhancing the reliability and trustworthiness of\nAI-generated content in information retrieval and summarization tasks which is\ncritical to gain customer trust especially in commercial products.", "AI": {"tldr": "The paper addresses citation accuracy issues in Retrieval Augmented Generation (RAG) systems by proposing post-processing algorithms, achieving a 15.46% improvement in accuracy and enabling cost-effective, faster models.", "motivation": "LLMs in RAG systems often struggle with source attribution, with citation accuracy as low as 74%, impacting reliability and trust in AI-generated content.", "method": "The authors introduce post-processing algorithms using keyword + semantic matching, BERTScore fine-tuning, and a lightweight LLM-based technique to cross-check citations.", "result": "Their methods improve citation accuracy by 15.46%, allowing a shift to a smaller, 12x cheaper, and 3x faster model without performance loss.", "conclusion": "This work enhances RAG system reliability, crucial for commercial applications, by improving citation accuracy and reducing costs."}}
{"id": "2503.06928", "pdf": "https://arxiv.org/pdf/2503.06928", "abs": "https://arxiv.org/abs/2503.06928", "authors": ["Yanlong Wang", "Jian Xu", "Tiantian Gao", "Hongkang Zhang", "Shao-Lun Huang", "Danny Dongning Sun", "Xiao-Ping Zhang"], "title": "FinTSBridge: A New Evaluation Suite for Real-world Financial Prediction with Advanced Time Series Models", "categories": ["cs.LG", "q-fin.TR"], "comment": "ICLR 2025 Workshop Advances in Financial AI", "summary": "Despite the growing attention to time series forecasting in recent years,\nmany studies have proposed various solutions to address the challenges\nencountered in time series prediction, aiming to improve forecasting\nperformance. However, effectively applying these time series forecasting models\nto the field of financial asset pricing remains a challenging issue. There is\nstill a need for a bridge to connect cutting-edge time series forecasting\nmodels with financial asset pricing. To bridge this gap, we have undertaken the\nfollowing efforts: 1) We constructed three datasets from the financial domain;\n2) We selected over ten time series forecasting models from recent studies and\nvalidated their performance in financial time series; 3) We developed new\nmetrics, msIC and msIR, in addition to MSE and MAE, to showcase the time series\ncorrelation captured by the models; 4) We designed financial-specific tasks for\nthese three datasets and assessed the practical performance and application\npotential of these forecasting models in important financial problems. We hope\nthe developed new evaluation suite, FinTSBridge, can provide valuable insights\ninto the effectiveness and robustness of advanced forecasting models in\nfinanical domains.", "AI": {"tldr": "The paper introduces FinTSBridge, a framework to bridge advanced time series forecasting models with financial asset pricing, using new datasets, metrics, and tasks.", "motivation": "To address the gap between cutting-edge time series forecasting models and their application in financial asset pricing.", "method": "Constructed financial datasets, validated models, introduced new metrics (msIC, msIR), and designed financial-specific tasks.", "result": "Developed FinTSBridge to evaluate forecasting models' effectiveness and robustness in financial domains.", "conclusion": "FinTSBridge offers insights into applying advanced forecasting models to financial problems."}}
{"id": "2506.08048", "pdf": "https://arxiv.org/pdf/2506.08048", "abs": "https://arxiv.org/abs/2506.08048", "authors": ["Zheng Han", "Jun Zhou", "Jialun Pei", "Jing Qin", "Yingfang Fan", "Qi Dou"], "title": "Toward Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "In augmented reality (AR)-guided surgical navigation, preoperative organ\nmodels are superimposed onto the patient's intraoperative anatomy to visualize\ncritical structures such as vessels and tumors. Accurate deformation modeling\nis essential to maintain the reliability of AR overlays by ensuring alignment\nbetween preoperative models and the dynamically changing anatomy. Although the\nfinite element method (FEM) offers physically plausible modeling, its high\ncomputational cost limits intraoperative applicability. Moreover, existing\nalgorithms often fail to handle large anatomical changes, such as those induced\nby pneumoperitoneum or ligament dissection, leading to inaccurate anatomical\ncorrespondences and compromised AR guidance. To address these challenges, we\npropose a data-driven biomechanics algorithm that preserves FEM-level accuracy\nwhile improving computational efficiency. In addition, we introduce a novel\nhuman-in-the-loop mechanism into the deformation modeling process. This enables\nsurgeons to interactively provide prompts to correct anatomical misalignments,\nthereby incorporating clinical expertise and allowing the model to adapt\ndynamically to complex surgical scenarios. Experiments on a publicly available\ndataset demonstrate that our algorithm achieves a mean target registration\nerror of 3.42 mm. Incorporating surgeon prompts through the interactive\nframework further reduces the error to 2.78 mm, surpassing state-of-the-art\nmethods in volumetric accuracy. These results highlight the ability of our\nframework to deliver efficient and accurate deformation modeling while\nenhancing surgeon-algorithm collaboration, paving the way for safer and more\nreliable computer-assisted surgeries.", "AI": {"tldr": "A data-driven biomechanics algorithm is proposed for AR-guided surgical navigation, combining FEM-level accuracy with computational efficiency and a human-in-the-loop mechanism for surgeon interaction.", "motivation": "Existing FEM-based methods are computationally expensive and struggle with large anatomical changes, compromising AR guidance accuracy.", "method": "A data-driven biomechanics algorithm with a human-in-the-loop mechanism allows surgeons to correct misalignments interactively.", "result": "Achieves a mean target registration error of 3.42 mm, reduced to 2.78 mm with surgeon prompts, outperforming state-of-the-art methods.", "conclusion": "The framework enhances accuracy, efficiency, and surgeon-algorithm collaboration, improving reliability in computer-assisted surgeries."}}
{"id": "2504.17834", "pdf": "https://arxiv.org/pdf/2504.17834", "abs": "https://arxiv.org/abs/2504.17834", "authors": ["Haokai Zhang", "Shengtao Zhang", "Zijian Cai", "Heng Wang", "Ruixuan Zhu", "Zinan Zeng", "Minnan Luo"], "title": "Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection", "categories": ["cs.IR", "cs.CL"], "comment": "ECML PKDD 2025", "summary": "Spoilers in movie reviews are important on platforms like IMDb and Rotten\nTomatoes, offering benefits and drawbacks. They can guide some viewers' choices\nbut also affect those who prefer no plot details in advance, making effective\nspoiler detection essential. Existing spoiler detection methods mainly analyze\nreview text, often overlooking the impact of movie genres and user bias,\nlimiting their effectiveness. To address this, we analyze movie review data,\nfinding genre-specific variations in spoiler rates and identifying that certain\nusers are more likely to post spoilers. Based on these findings, we introduce a\nnew spoiler detection framework called GUSD (The code is available at\nhttps://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler\nDetection), which incorporates genre-specific data and user behavior bias. User\nbias is calculated through dynamic graph modeling of review history.\nAdditionally, the R2GFormer module combines RetGAT (Retentive Graph Attention\nNetwork) for graph information and GenreFormer for genre-specific aggregation.\nThe GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to\nspecialized experts based on genre. Extensive testing on benchmark datasets\nshows that GUSD achieves state-of-the-art results. This approach advances\nspoiler detection by addressing genre and user-specific patterns, enhancing\nuser experience on movie review platforms.", "AI": {"tldr": "GUSD is a new spoiler detection framework that improves accuracy by incorporating genre-specific data and user behavior bias, outperforming existing methods.", "motivation": "Existing spoiler detection methods ignore genre and user bias, limiting effectiveness. Addressing these gaps can enhance detection accuracy and user experience.", "method": "GUSD uses dynamic graph modeling for user bias, RetGAT and GenreFormer for graph and genre data, and GMoE for genre-specific review assignment.", "result": "GUSD achieves state-of-the-art performance on benchmark datasets.", "conclusion": "GUSD advances spoiler detection by addressing genre and user-specific patterns, improving movie review platforms."}}
{"id": "2506.06344", "pdf": "https://arxiv.org/pdf/2506.06344", "abs": "https://arxiv.org/abs/2506.06344", "authors": ["Alex Pierron", "Michel Barbeau", "Luca De Cicco", "Jose Rubio-Hernan", "Joaquin Garcia-Alfaro"], "title": "A Reinforcement Learning Approach for RIS-aided Fair Communications", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "8 pages, 7 figures, 1 table, 16 references", "summary": "Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements\nthat can dynamically alter electromagnetic wave properties to enhance\nbeamforming and leading to improvements in areas with low coverage properties.\nThey have the potential to be combined with Reinforcement Learning (RL)\ntechniques to achieve network performance and energy efficiency via\noptimization techniques. In addition to performance and energy improvements, it\nis also crucial to consider the concept of fair communications. RISs must\nensure that User Equipment (UE) units receive their signals with adequate\nstrength, without other UE being deprived of service due to insufficient power.\nIn this paper, we address such a problem. We explore the fairness properties of\nprevious work and propose a novel method that aims at obtaining an efficient\nand fair duplex RIS-RL system for multiple legitimate UE units. We report and\ndiscuss our experimental work and simulation results. We also release our code\nand datasets to foster further research in the topic.", "AI": {"tldr": "The paper proposes a novel method for achieving fair and efficient communication in RIS-RL systems, ensuring all UEs receive adequate signal strength without deprivation.", "motivation": "To address fairness issues in RIS-RL systems, ensuring no UE is deprived of service due to insufficient power.", "method": "Combines RIS with RL techniques to optimize network performance and energy efficiency while ensuring fairness.", "result": "Experimental work and simulations demonstrate the effectiveness of the proposed method.", "conclusion": "The paper presents a fair and efficient RIS-RL system, with released code and datasets to encourage further research."}}
{"id": "2503.08099", "pdf": "https://arxiv.org/pdf/2503.08099", "abs": "https://arxiv.org/abs/2503.08099", "authors": ["Runxi Cheng", "Feng Xiong", "Yongxian Wei", "Wanyun Zhu", "Chun Yuan"], "title": "Whoever Started the Interference Should End It: Guiding Data-Free Model Merging via Task Vectors", "categories": ["cs.LG"], "comment": "23 pages, 13 figures, 12 tables", "summary": "Model merging seeks to integrate task-specific expert models into a unified\narchitecture while preserving multi-task generalization capabilities, yet\nparameter interference between constituent models frequently induces\nperformance degradation. Although prior work has explored many merging\nstrategies, resolving interference without additional data for retraining or\ntest-time computation remains challenging. In this paper, we theoretically\ndemonstrate that the task vectors of the linear layer constitute an approximate\nlinear subspace for its corresponding input. Therefore, we can minimize\ninterference under the guidance of task vectors. Based on this insight, we\npropose \\textbf{WUDI-Merging} (\\textbf{W}hoever started the interference\nsho\\textbf{U}ld en\\textbf{D} \\textbf{I}t), a simple yet effective model merging\nmethod that eliminates interference without any additional data or rescaling\ncoefficients. Comprehensive empirical evaluations across vision and language\nbenchmarks demonstrate our method's superiority, achieving state-of-the-art\nperformance in data-free model merging scenarios (average 10.9\\% improvement\nversus baseline methods) while even outperforming mainstream test-time\nadaptation approaches by 3.3\\%, and only very few computing resources are\nrequired. The code will be publicly available soon.", "AI": {"tldr": "WUDI-Merging is a data-free model merging method that minimizes interference between task-specific models by leveraging task vectors, achieving state-of-the-art performance with minimal resources.", "motivation": "Addressing performance degradation due to parameter interference in model merging without requiring additional data or retraining.", "method": "Theoretical demonstration of task vectors forming a linear subspace, guiding the WUDI-Merging method to eliminate interference.", "result": "Achieves 10.9% improvement over baselines and outperforms test-time adaptation by 3.3%, with minimal computing resources.", "conclusion": "WUDI-Merging is a simple, effective solution for data-free model merging, offering superior performance and efficiency."}}
{"id": "2506.08137", "pdf": "https://arxiv.org/pdf/2506.08137", "abs": "https://arxiv.org/abs/2506.08137", "authors": ["Oishee Bintey Hoque", "Abhijin Adiga", "Aniruddha Adiga", "Siddharth Chaudhary", "Madhav V. Marathe", "S. S. Ravi", "Kirti Rajagopalan", "Amanda Wilson", "Samarth Swarup"], "title": "IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate canal network mapping is essential for water management, including\nirrigation planning and infrastructure maintenance. State-of-the-art semantic\nsegmentation models for infrastructure mapping, such as roads, rely on large,\nwell-annotated remote sensing datasets. However, incomplete or inadequate\nground truth can hinder these learning approaches. Many infrastructure networks\nhave graph-level properties such as reachability to a source (like canals) or\nconnectivity (roads) that can be leveraged to improve these existing ground\ntruth. This paper develops a novel iterative framework IGraSS, combining a\nsemantic segmentation module-incorporating RGB and additional modalities (NDWI,\nDEM)-with a graph-based ground-truth refinement module. The segmentation module\nprocesses satellite imagery patches, while the refinement module operates on\nthe entire data viewing the infrastructure network as a graph. Experiments show\nthat IGraSS reduces unreachable canal segments from around 18% to 3%, and\ntraining with refined ground truth significantly improves canal identification.\nIGraSS serves as a robust framework for both refining noisy ground truth and\nmapping canal networks from remote sensing imagery. We also demonstrate the\neffectiveness and generalizability of IGraSS using road networks as an example,\napplying a different graph-theoretic constraint to complete road networks.", "AI": {"tldr": "IGraSS is a novel framework combining semantic segmentation and graph-based refinement to improve canal and road network mapping from noisy ground truth.", "motivation": "Accurate canal and road network mapping is crucial for water and infrastructure management, but incomplete ground truth hinders learning approaches.", "method": "IGraSS integrates a semantic segmentation module (using RGB, NDWI, DEM) with a graph-based refinement module to improve ground truth by leveraging graph-level properties like reachability and connectivity.", "result": "IGraSS reduces unreachable canal segments from ~18% to 3% and enhances canal identification. It also generalizes to road networks with different constraints.", "conclusion": "IGraSS is effective for refining noisy ground truth and mapping infrastructure networks, demonstrating robustness and generalizability."}}
{"id": "2506.06975", "pdf": "https://arxiv.org/pdf/2506.06975", "abs": "https://arxiv.org/abs/2506.06975", "authors": ["Xiaoyuan Zhu", "Yaowen Ye", "Tianyi Qiu", "Hanlin Zhu", "Sijun Tan", "Ajraf Mannan", "Jonathan Michala", "Raluca Ada Popa", "Willie Neiswanger"], "title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "As API access becomes a primary interface to large language models (LLMs),\nusers often interact with black-box systems that offer little transparency into\nthe deployed model. To reduce costs or maliciously alter model behaviors, API\nproviders may discreetly serve quantized or fine-tuned variants, which can\ndegrade performance and compromise safety. Detecting such substitutions is\ndifficult, as users lack access to model weights and, in most cases, even\noutput logits. To tackle this problem, we propose a rank-based uniformity test\nthat can verify the behavioral equality of a black-box LLM to a locally\ndeployed authentic model. Our method is accurate, query-efficient, and avoids\ndetectable query patterns, making it robust to adversarial providers that\nreroute or mix responses upon the detection of testing attempts. We evaluate\nthe approach across diverse threat scenarios, including quantization, harmful\nfine-tuning, jailbreak prompts, and full model substitution, showing that it\nconsistently achieves superior statistical power over prior methods under\nconstrained query budgets.", "AI": {"tldr": "A rank-based uniformity test is proposed to verify the behavioral equality of black-box LLMs to authentic models, addressing issues like quantization, fine-tuning, and model substitution.", "motivation": "API providers may discreetly alter LLMs (e.g., via quantization or fine-tuning), degrading performance and safety, but detecting such changes is challenging due to lack of transparency.", "method": "A rank-based uniformity test is developed to compare black-box LLM behavior to an authentic model, ensuring accuracy and query efficiency while avoiding detectable patterns.", "result": "The method outperforms prior approaches in detecting threats like quantization, harmful fine-tuning, and model substitution under constrained query budgets.", "conclusion": "The proposed test effectively verifies model authenticity, offering robustness against adversarial providers and superior statistical power."}}
{"id": "2506.07298", "pdf": "https://arxiv.org/pdf/2506.07298", "abs": "https://arxiv.org/abs/2506.07298", "authors": ["Yijia Dai", "Zhaolin Gao", "Yahya Sattar", "Sarah Dean", "Jennifer J. Sun"], "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential\ndata with latent Markovian structure, yet fitting them to real-world data\nremains computationally challenging. In this work, we show that pre-trained\nlarge language models (LLMs) can effectively model data generated by HMMs via\nin-context learning (ICL)$\\unicode{x2013}$their ability to infer patterns from\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\npredictive accuracy approaching the theoretical optimum. We uncover novel\nscaling trends influenced by HMM properties, and offer theoretical conjectures\nfor these empirical observations. We also provide practical guidelines for\nscientists on using ICL as a diagnostic tool for complex data. On real-world\nanimal decision-making tasks, ICL achieves competitive performance with models\ndesigned by human experts. To our knowledge, this is the first demonstration\nthat ICL can learn and predict HMM-generated sequences$\\unicode{x2013}$an\nadvance that deepens our understanding of in-context learning in LLMs and\nestablishes its potential as a powerful tool for uncovering hidden structure in\ncomplex scientific data.", "AI": {"tldr": "LLMs use in-context learning to model HMM-generated data, achieving near-optimal accuracy and offering practical guidelines for real-world applications.", "motivation": "To address the computational challenges of fitting HMMs to real-world data and explore LLMs' potential for modeling sequential data.", "method": "Utilize pre-trained LLMs with in-context learning to infer patterns from synthetic and real-world HMM-generated sequences.", "result": "LLMs achieve predictive accuracy close to the theoretical optimum and perform competitively on real-world tasks.", "conclusion": "ICL in LLMs is a powerful tool for modeling HMMs, advancing understanding of in-context learning and its applications in complex data."}}
{"id": "2503.12066", "pdf": "https://arxiv.org/pdf/2503.12066", "abs": "https://arxiv.org/abs/2503.12066", "authors": ["Yuetong Yu", "Ruiyang Ge", "Ilker Hacihaliloglu", "Alexander Rauscher", "Roger Tam", "Sophia Frangou"], "title": "Dataset Properties Shape the Success of Neuroimaging-Based Patient Stratification: A Benchmarking Analysis Across Clustering Algorithms", "categories": ["cs.LG", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Background: Data driven stratification of patients into biologically informed\nsubtypes holds promise for precision neuropsychiatry, yet neuroimaging-based\nclustering methods often fail to generalize across cohorts. While algorithmic\ninnovations have focused on model complexity, the role of underlying dataset\ncharacteristics remains underexplored. We hypothesized that cluster separation,\nsize imbalance, noise, and the direction and magnitude of disease-related\neffects in the input data critically determine both within-algorithm accuracy\nand reproducibility. Methods: We evaluated 4 widely used stratification\nalgorithms, HYDRA, SuStaIn, SmileGAN, and SurrealGAN, on a suite of synthetic\nbrain-morphometry cohorts derived from the Human Connectome Project Young Adult\ndataset. Three global transformation patterns were applied to 600\npseudo-patients against 508 controls, followed by 4 within-dataset variations\nvarying cluster count (k=2-6), overlap, and effect magnitude. Algorithm\nperformance was quantified by accuracy in recovering the known ground-truth\nclusters. Results: Across 122 synthetic scenarios, data complexity consistently\noutweighed algorithm choice in predicting stratification success.\nWell-separated clusters yielded high accuracy for all methods, whereas\noverlapping, unequal-sized, or subtle effects reduced accuracy by up to 50%.\nSuStaIn could not scale beyond 17 features, HYDRA's accuracy varied\nunpredictably with data heterogeneity. SmileGAN and SurrealGAN maintained\nrobust pattern detection but did not assign discrete cluster labels to\nindividuals. Conclusions: The study results demonstrate the impact of\nstatistical properties of input data across algorithms and highlight the need\nfor using realistic dataset distributions when new algorithms are being\ndeveloped and suggest greater focus on data-centric strategies that actively\nshape and standardize the input distributions.", "AI": {"tldr": "The study shows that data characteristics (like cluster separation and noise) impact patient stratification accuracy more than algorithm choice, emphasizing the need for realistic data in algorithm development.", "motivation": "To understand how dataset properties (e.g., cluster overlap, noise) affect the accuracy and reproducibility of neuroimaging-based patient stratification methods.", "method": "Evaluated four algorithms (HYDRA, SuStaIn, SmileGAN, SurrealGAN) on synthetic brain-morphometry datasets with controlled variations in cluster properties.", "result": "Data complexity (e.g., cluster overlap, size imbalance) had a larger impact on accuracy than algorithm choice. Some algorithms struggled with scalability or discrete labeling.", "conclusion": "Input data properties critically influence stratification success, highlighting the need for data-centric strategies in algorithm development."}}
{"id": "2506.08194", "pdf": "https://arxiv.org/pdf/2506.08194", "abs": "https://arxiv.org/abs/2506.08194", "authors": ["Mateusz Michalkiewicz", "Anekha Sokhal", "Tadeusz Michalkiewicz", "Piotr Pawlikowski", "Mahsa Baktashmotlagh", "Varun Jampani", "Guha Balakrishnan"], "title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra", "categories": ["cs.CV", "68T45", "I.5.4; I.2.10; I.3.5"], "comment": "15 pages, 4 figures", "summary": "Monocular 3D reconstruction methods and vision-language models (VLMs)\ndemonstrate impressive results on standard benchmarks, yet their true\nunderstanding of geometric properties remains unclear. We introduce GIQ , a\ncomprehensive benchmark specifically designed to evaluate the geometric\nreasoning capabilities of vision and vision-language foundation models. GIQ\ncomprises synthetic and real-world images of 224 diverse polyhedra - including\nPlatonic, Archimedean, Johnson, and Catalan solids, as well as stellations and\ncompound shapes - covering varying levels of complexity and symmetry. Through\nsystematic experiments involving monocular 3D reconstruction, 3D symmetry\ndetection, mental rotation tests, and zero-shot shape classification tasks, we\nreveal significant shortcomings in current models. State-of-the-art\nreconstruction algorithms trained on extensive 3D datasets struggle to\nreconstruct even basic geometric forms accurately. While foundation models\neffectively detect specific 3D symmetry elements via linear probing, they\nfalter significantly in tasks requiring detailed geometric differentiation,\nsuch as mental rotation. Moreover, advanced vision-language assistants exhibit\nremarkably low accuracy on complex polyhedra, systematically misinterpreting\nbasic properties like face geometry, convexity, and compound structures. GIQ is\npublicly available, providing a structured platform to highlight and address\ncritical gaps in geometric intelligence, facilitating future progress in\nrobust, geometry-aware representation learning.", "AI": {"tldr": "GIQ is a benchmark evaluating geometric reasoning in vision and vision-language models, revealing significant gaps in current models despite their success on standard tasks.", "motivation": "To assess the true geometric understanding of vision and vision-language models, which remains unclear despite their strong performance on benchmarks.", "method": "Introduces GIQ, a benchmark with synthetic and real-world images of 224 diverse polyhedra, tested via monocular 3D reconstruction, symmetry detection, mental rotation, and zero-shot classification.", "result": "Current models struggle with basic geometric forms, symmetry detection, and detailed differentiation, while vision-language assistants perform poorly on complex polyhedra.", "conclusion": "GIQ highlights critical gaps in geometric intelligence, offering a platform to improve geometry-aware representation learning."}}
{"id": "2506.08022", "pdf": "https://arxiv.org/pdf/2506.08022", "abs": "https://arxiv.org/abs/2506.08022", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations.", "AI": {"tldr": "MBPO addresses modality imbalance in LMMs by combining adversarial perturbation for hard negatives and GRPO for online training, improving performance and reducing hallucinations.", "motivation": "LMMs suffer from modality imbalance, favoring language over vision, leading to poor generalization and hallucinations. Existing methods don't address LLM biases or use dynamic data.", "method": "MBPO creates an offline dataset with hard negatives via adversarial image perturbation and uses GRPO for online training with verified rewards.", "result": "MBPO improves LMM performance on vision-language tasks and reduces hallucinations.", "conclusion": "MBPO effectively balances modalities in LMMs, enhancing generalization and reducing biases."}}
{"id": "2506.07563", "pdf": "https://arxiv.org/pdf/2506.07563", "abs": "https://arxiv.org/abs/2506.07563", "authors": ["Ken Yaggel", "Eyal German", "Aviel Ben Siman Tov"], "title": "MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with Expert Specialization", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Personalized recommendation systems must adapt to user interactions across\ndifferent domains. Traditional approaches like MLoRA apply a single adaptation\nper domain but lack flexibility in handling diverse user behaviors. To address\nthis, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is\nfirst trained independently to specialize in its domain before a gating network\nis trained to weight their contributions dynamically. We evaluate MoE-MLoRA\nacross eight CTR models on Movielens and Taobao, showing that it improves\nperformance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20)\nbut offers limited benefits in structured datasets with low domain diversity\nand sparsity. Further analysis of the number of experts per domain reveals that\nlarger ensembles do not always improve performance, indicating the need for\nmodel-aware tuning. Our findings highlight the potential of expert-based\narchitectures for multi-domain recommendation systems, demonstrating that\ntask-aware specialization and adaptive gating can enhance predictive accuracy\nin complex environments. The implementation and code are available in our\nGitHub repository.", "AI": {"tldr": "MoE-MLoRA improves recommendation systems by using a mixture-of-experts framework, showing gains in dynamic datasets but limited benefits in structured ones.", "motivation": "Traditional methods like MLoRA lack flexibility for diverse user behaviors across domains.", "method": "MoE-MLoRA trains domain-specialized experts and a gating network to dynamically weight their contributions.", "result": "Improves performance in dynamic datasets (+1.45 Weighed-AUC in Taobao-20) but not in structured, low-diversity datasets. Larger ensembles don't always help.", "conclusion": "Expert-based architectures with task-aware specialization and adaptive gating enhance accuracy in complex environments."}}
{"id": "2503.15567", "pdf": "https://arxiv.org/pdf/2503.15567", "abs": "https://arxiv.org/abs/2503.15567", "authors": ["Yanchen Luo", "Zhiyuan Liu", "Yi Zhao", "Sihang Li", "Hengxing Cai", "Kenji Kawaguchi", "Tat-Seng Chua", "Yang Zhang", "Xiang Wang"], "title": "Towards Unified and Lossless Latent Space for 3D Molecular Latent Diffusion Modeling", "categories": ["cs.LG"], "comment": null, "summary": "3D molecule generation is crucial for drug discovery and material science,\nrequiring models to process complex multi-modalities, including atom types,\nchemical bonds, and 3D coordinates. A key challenge is integrating these\nmodalities of different shapes while maintaining SE(3) equivariance for 3D\ncoordinates. To achieve this, existing approaches typically maintain separate\nlatent spaces for invariant and equivariant modalities, reducing efficiency in\nboth training and sampling. In this work, we propose \\textbf{U}nified\nVariational \\textbf{A}uto-\\textbf{E}ncoder for \\textbf{3D} Molecular Latent\nDiffusion Modeling (\\textbf{UAE-3D}), a multi-modal VAE that compresses 3D\nmolecules into latent sequences from a unified latent space, while maintaining\nnear-zero reconstruction error. This unified latent space eliminates the\ncomplexities of handling multi-modality and equivariance when performing latent\ndiffusion modeling. We demonstrate this by employing the Diffusion\nTransformer--a general-purpose diffusion model without any molecular inductive\nbias--for latent generation. Extensive experiments on GEOM-Drugs and QM9\ndatasets demonstrate that our method significantly establishes new benchmarks\nin both \\textit{de novo} and conditional 3D molecule generation, achieving\nleading efficiency and quality. On GEOM-Drugs, it reduces FCD by 72.6\\% over\nthe previous best result, while achieving over 70\\% relative average\nimprovements in geometric fidelity.", "AI": {"tldr": "UAE-3D proposes a unified latent space for 3D molecule generation, improving efficiency and quality by eliminating multi-modality complexities and maintaining SE(3) equivariance.", "motivation": "Current methods for 3D molecule generation struggle with integrating multi-modal data (atom types, bonds, 3D coordinates) and maintaining SE(3) equivariance, leading to inefficiencies.", "method": "UAE-3D uses a multi-modal VAE to compress 3D molecules into latent sequences from a unified space, enabling latent diffusion modeling with a Diffusion Transformer.", "result": "The method achieves significant improvements in efficiency and quality, reducing FCD by 72.6% on GEOM-Drugs and enhancing geometric fidelity by over 70%.", "conclusion": "UAE-3D sets new benchmarks for 3D molecule generation, demonstrating superior performance in both de novo and conditional tasks."}}
{"id": "2506.08324", "pdf": "https://arxiv.org/pdf/2506.08324", "abs": "https://arxiv.org/abs/2506.08324", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2504.15155,\n  arXiv:2504.13045, arXiv:2503.23472", "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more effectively extract\nand fuse spatial context with fine spectral information in hyperspectral image\n(HSI) classification, this paper proposes a novel network architecture called\nSTNet. The core advantage of STNet stems from the dual innovative design of its\nSpatial-Spectral Transformer module: first, the fundamental explicit decoupling\nof spatial and spectral attention ensures targeted capture of key information\nin HSI; second, two functionally distinct gating mechanisms perform intelligent\nregulation at both the fusion level of attention flows (adaptive attention\nfusion gating) and the internal level of feature transformation (GFFN). This\ncharacteristic demonstrates superior feature extraction and fusion capabilities\ncompared to traditional convolutional neural networks, while reducing\noverfitting risks in small-sample and high-noise scenarios. STNet enhances\nmodel representation capability without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches.", "AI": {"tldr": "STNet, a novel network architecture with a Spatial-Spectral Transformer module, improves hyperspectral image classification by decoupling spatial and spectral attention and using gating mechanisms, outperforming traditional methods.", "motivation": "Challenges like high-dimensional data, sparse ground objects, and spectral redundancy in hyperspectral image classification lead to overfitting and poor generalization. STNet aims to better extract and fuse spatial-spectral information.", "method": "STNet uses a dual-design Spatial-Spectral Transformer module: decoupling spatial and spectral attention, and employing gating mechanisms (adaptive attention fusion gating and GFFN) for intelligent regulation.", "result": "STNet outperforms mainstream methods on IN, UP, and KSC datasets, enhancing feature extraction and reducing overfitting without increasing network size.", "conclusion": "STNet effectively addresses hyperspectral classification challenges, offering superior performance and generalization with innovative attention and gating designs."}}
{"id": "2506.08049", "pdf": "https://arxiv.org/pdf/2506.08049", "abs": "https://arxiv.org/abs/2506.08049", "authors": ["Tengfei Lyu", "Weijia Zhang", "Hao Liu"], "title": "Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions\nfrom several weeks to months in advance, presents significant challenges due to\nthe chaotic dynamics of atmospheric systems and complex interactions across\nmultiple scales. Current approaches often fail to explicitly model underlying\nphysical processes and teleconnections that are crucial at S2S timescales. We\nintroduce TelePiT, a novel deep learning architecture that enhances global S2S\nforecasting through integrated multi-scale physics and teleconnection\nawareness. Our approach consists of three key components: (1) Spherical\nHarmonic Embedding, which accurately encodes global atmospheric variables onto\nspherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which\nexplicitly captures atmospheric physical processes across multiple learnable\nfrequency bands; (3) Teleconnection-Aware Transformer, which models critical\nglobal climate interactions through tactfully injecting teleconnection patterns\ninto the self-attention. Extensive experiments demonstrate that TelePiT\nsignificantly outperforms state-of-the-art data-driven baselines and\noperational numerical weather prediction systems, with remarkable improvements\nfor atmospheric variables including a 57.7% reduction in RMSE for 2-meter\ntemperature compared to previous best models.", "AI": {"tldr": "TelePiT, a deep learning model, improves S2S forecasting by integrating multi-scale physics and teleconnection awareness, outperforming existing methods.", "motivation": "Current S2S forecasting struggles with chaotic dynamics and lacks explicit modeling of physical processes and teleconnections.", "method": "TelePiT uses Spherical Harmonic Embedding, Multi-Scale Physics-Informed Neural ODE, and Teleconnection-Aware Transformer to enhance forecasting.", "result": "TelePiT reduces RMSE for 2-meter temperature by 57.7% compared to prior models.", "conclusion": "TelePiT advances S2S forecasting by effectively modeling physical processes and teleconnections."}}
{"id": "2503.16117", "pdf": "https://arxiv.org/pdf/2503.16117", "abs": "https://arxiv.org/abs/2503.16117", "authors": ["Alexandre Verine", "Ahmed Mehdi Inane", "Florian Le Bronnec", "Benjamin Negrevergne", "Yann Chevaleyre"], "title": "Improving Discriminator Guidance in Diffusion Models", "categories": ["cs.LG"], "comment": "European Conference on Machine Learning and Principles and Practice\n  of Knowledge Discovery in Databases - ECML PKDD 2025", "summary": "Discriminator Guidance has become a popular method for efficiently refining\npre-trained Score-Matching Diffusion models. However, in this paper, we\ndemonstrate that the standard implementation of this technique does not\nnecessarily lead to a distribution closer to the real data distribution.\nSpecifically, we show that training the discriminator using Cross-Entropy loss,\nas commonly done, can in fact increase the Kullback-Leibler divergence between\nthe model and target distributions, particularly when the discriminator\noverfits. To address this, we propose a theoretically sound training objective\nfor discriminator guidance that properly minimizes the KL divergence. We\nanalyze its properties and demonstrate empirically across multiple datasets\nthat our proposed method consistently improves over the conventional method by\nproducing samples of higher quality.", "AI": {"tldr": "Discriminator Guidance, commonly used with Cross-Entropy loss, may not improve distribution alignment. A new training objective is proposed to minimize KL divergence, showing better sample quality.", "motivation": "Standard Discriminator Guidance with Cross-Entropy loss can increase KL divergence due to overfitting, motivating a better training objective.", "method": "Propose a theoretically sound training objective for discriminator guidance to properly minimize KL divergence.", "result": "Empirical results show the proposed method consistently improves sample quality over conventional methods.", "conclusion": "The new training objective effectively addresses the limitations of standard Discriminator Guidance, enhancing sample quality."}}
{"id": "2506.08356", "pdf": "https://arxiv.org/pdf/2506.08356", "abs": "https://arxiv.org/abs/2506.08356", "authors": ["Shivang Chopra", "Gabriela Sanchez-Rodriguez", "Lingchao Mao", "Andrew J Feola", "Jing Li", "Zsolt Kira"], "title": "MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Different medical imaging modalities capture diagnostic information at\nvarying spatial resolutions, from coarse global patterns to fine-grained\nlocalized structures. However, most existing vision-language frameworks in the\nmedical domain apply a uniform strategy for local feature extraction,\noverlooking the modality-specific demands. In this work, we present MedMoE, a\nmodular and extensible vision-language processing framework that dynamically\nadapts visual representation based on the diagnostic context. MedMoE\nincorporates a Mixture-of-Experts (MoE) module conditioned on the report type,\nwhich routes multi-scale image features through specialized expert branches\ntrained to capture modality-specific visual semantics. These experts operate\nover feature pyramids derived from a Swin Transformer backbone, enabling\nspatially adaptive attention to clinically relevant regions. This framework\nproduces localized visual representations aligned with textual descriptions,\nwithout requiring modality-specific supervision at inference. Empirical results\non diverse medical benchmarks demonstrate that MedMoE improves alignment and\nretrieval performance across imaging modalities, underscoring the value of\nmodality-specialized visual representations in clinical vision-language\nsystems.", "AI": {"tldr": "MedMoE is a vision-language framework that dynamically adapts visual representation for medical imaging by using modality-specific experts, improving alignment and retrieval across diverse modalities.", "motivation": "Existing frameworks use uniform feature extraction, ignoring modality-specific needs in medical imaging. MedMoE addresses this gap.", "method": "Uses a Mixture-of-Experts (MoE) module with specialized branches for multi-scale feature extraction, based on a Swin Transformer backbone.", "result": "Improves alignment and retrieval performance across medical imaging benchmarks without modality-specific supervision.", "conclusion": "Modality-specialized visual representations enhance clinical vision-language systems."}}
{"id": "2506.08054", "pdf": "https://arxiv.org/pdf/2506.08054", "abs": "https://arxiv.org/abs/2506.08054", "authors": ["Yiming Wang", "Hao Peng", "Senzhang Wang", "Haohua Du", "Chunyang Liu", "Jia Wu", "Guanlin Wu"], "title": "STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 5 figures, 3 tables. Extended version of paper accepted at\n  IJCAI 2025", "summary": "Traffic data imputation is fundamentally important to support various\napplications in intelligent transportation systems such as traffic flow\nprediction. However, existing time-to-space sequential methods often fail to\neffectively extract features in block-wise missing data scenarios. Meanwhile,\nthe static graph structure for spatial feature propagation significantly\nconstrains the models flexibility in handling the distribution shift issue for\nthe nonstationary traffic data. To address these issues, this paper proposes a\nSpatioTemporal Attention Mixture of experts network named STAMImputer for\ntraffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)\nframework to capture latent spatio-temporal features and their influence\nweights, effectively imputing block missing. A novel Low-rank guided Sampling\nGraph ATtention (LrSGAT) mechanism is designed to dynamically balance the local\nand global correlations across road networks. The sampled attention vectors are\nutilized to generate dynamic graphs that capture real-time spatial\ncorrelations. Extensive experiments are conducted on four traffic datasets for\nevaluation. The result shows STAMImputer achieves significantly performance\nimprovement compared with existing SOTA approaches. Our codes are available at\nhttps://github.com/RingBDStack/STAMImupter.", "AI": {"tldr": "STAMImputer is a novel SpatioTemporal Attention Mixture of Experts network for traffic data imputation, addressing block-wise missing data and nonstationary traffic issues with dynamic graph structures.", "motivation": "Existing methods struggle with block-wise missing data and static graph structures, limiting flexibility for nonstationary traffic data.", "method": "Uses a Mixture of Experts (MoE) framework and Low-rank guided Sampling Graph ATtention (LrSGAT) to dynamically balance local and global correlations.", "result": "Outperforms state-of-the-art methods on four traffic datasets.", "conclusion": "STAMImputer effectively handles missing data and spatial correlations, offering a robust solution for traffic data imputation."}}
{"id": "2503.21971", "pdf": "https://arxiv.org/pdf/2503.21971", "abs": "https://arxiv.org/abs/2503.21971", "authors": ["Armin Abdollahi", "Mehdi Kamal", "Massoud Pedram"], "title": "RocketPPA: Code-Level Power, Performance, and Area Prediction via LLM and Mixture of Experts", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "This paper presents RocketPPA, a novel ultra-fast power, performance (delay),\nand area (PPA) estimator operating directly at the code-level abstraction using\nHDL code as input. The key technical innovation is its LLM-based regression\nmodel, which uniquely integrates a large language model (LLM) with a\nmixture-of-experts (MoE) architecture composed of multilayer perceptrons\n(MLPs). The LLM interprets the input HDL code and then utilizes its final\nhidden-layer representations to predict PPA metrics. Low-rank adaptation (LoRA)\nis used for parameter-efficient fine-tuning to enable efficient LLM training.\nFurthermore, the work includes the development of an LLM-based HDL code repair\nframework to generate a large and synthesizable training dataset. Experimental\nresults on the VerilogEval benchmark demonstrate that RocketPPA achieves\nsignificant improvements in the accuracy of PPA estimation compared to previous\nstate-of-the-art methods like Llama3-MetRex-8B. Specifically, at a 10% relative\nerror threshold, RocketPPA enhances the pass rate for area prediction by 13.6%,\ndelay by 9.4%, and power by 14.7%. At a 20% threshold, the improvements are\n9.6% for area, 10.8% for delay, and 18.5% for power. Moreover, RocketPPA\nachieves a speedup of over 20x compared to MetRex and 30x over MasterRTL in\nprocessing the test set. The impact of RocketPPA is the potential to\nsubstantially accelerate the hardware design process by providing accurate PPA\nestimations early in the design cycle, thus avoiding the overhead of manual\nfeature engineering and time-consuming synthesis flows.", "AI": {"tldr": "RocketPPA is a fast HDL code-level PPA estimator using an LLM-based regression model with MoE and LoRA, achieving higher accuracy and speed than state-of-the-art methods.", "motivation": "To accelerate hardware design by providing early, accurate PPA estimations without manual feature engineering or slow synthesis flows.", "method": "Uses an LLM-based regression model with MoE architecture and LoRA for efficient training, plus an LLM-based HDL code repair framework for dataset generation.", "result": "Improves pass rates for PPA predictions (e.g., 13.6% for area at 10% error) and achieves 20-30x speedup over competitors.", "conclusion": "RocketPPA significantly enhances PPA estimation accuracy and speed, benefiting early hardware design stages."}}
{"id": "2506.08650", "pdf": "https://arxiv.org/pdf/2506.08650", "abs": "https://arxiv.org/abs/2506.08650", "authors": ["Peter Gr\u00f6nquist", "Stepan Tulyakov", "Dengxin Dai"], "title": "Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Achieving consistent color reproduction across multiple cameras is essential\nfor seamless image fusion and Image Processing Pipeline (ISP) compatibility in\nmodern devices, but it is a challenging task due to variations in sensors and\noptics. Existing raw-to-raw conversion methods face limitations such as poor\nadaptability to changing illumination, high computational costs, or impractical\nrequirements such as simultaneous camera operation and overlapping\nfields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,\nphysically-informed approach that simulates raw images under specified\nillumination to estimate transformations between devices. The NPM effectively\nadapts to varying illumination conditions, can be initialized with physical\nmeasurements, and supports training with or without paired data. Experiments on\npublic datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent\nstate-of-the-art methods, providing robust chromatic consistency across\ndifferent sensors and optical systems.", "AI": {"tldr": "The paper introduces the Neural Physical Model (NPM) for consistent color reproduction across cameras, outperforming existing methods in adaptability and performance.", "motivation": "Ensuring consistent color reproduction across multiple cameras is challenging due to sensor and optics variations, and existing methods have limitations like poor adaptability or high computational costs.", "method": "The Neural Physical Model (NPM) is a lightweight, physically-informed approach that simulates raw images under specified illumination to estimate transformations between devices. It adapts to varying illumination and supports training with or without paired data.", "result": "Experiments on datasets like NUS and BeyondRGB show NPM outperforms state-of-the-art methods, achieving robust chromatic consistency across sensors and optical systems.", "conclusion": "NPM provides a practical and effective solution for raw-to-raw conversion, addressing key challenges in color consistency for modern devices."}}
{"id": "2506.08309", "pdf": "https://arxiv.org/pdf/2506.08309", "abs": "https://arxiv.org/abs/2506.08309", "authors": ["Katherine Tieu", "Dongqi Fu", "Zihao Li", "Ross Maciejewski", "Jingrui He"], "title": "Learnable Spatial-Temporal Positional Encoding for Link Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICML 2025. 28 pages, 1 figures, 22 tables", "summary": "Accurate predictions rely on the expressiveness power of graph deep learning\nframeworks like graph neural networks and graph transformers, where a\npositional encoding mechanism has become much more indispensable in recent\nstate-of-the-art works to record the canonical position information. However,\nthe current positional encoding is limited in three aspects: (1) most\npositional encoding methods use pre-defined, and fixed functions, which are\ninadequate to adapt to the complex attributed graphs; (2) a few pioneering\nworks proposed the learnable positional encoding but are still limited to the\nstructural information, not considering the real-world time-evolving\ntopological and feature information; (3) most positional encoding methods are\nequipped with transformers' attention mechanism to fully leverage their\ncapabilities, where the dense or relational attention is often unaffordable on\nlarge-scale structured data. Hence, we aim to develop Learnable\nSpatial-Temporal Positional Encoding in an effective and efficient manner and\npropose a simple temporal link prediction model named L-STEP. Briefly, for\nL-STEP, we (1) prove the proposed positional learning scheme can preserve the\ngraph property from the spatial-temporal spectral viewpoint, (2) verify that\nMLPs can fully exploit the expressiveness and reach transformers' performance\non that encoding, (3) change different initial positional encoding inputs to\nshow robustness, (4) analyze the theoretical complexity and obtain less\nempirical running time than SOTA, and (5) demonstrate its temporal link\nprediction out-performance on 13 classic datasets and with 10 algorithms in\nboth transductive and inductive settings using 3 different sampling strategies.\nAlso, L-STEP obtains the leading performance in the newest large-scale TGB\nbenchmark. Our code is available at https://github.com/kthrn22/L-STEP.", "AI": {"tldr": "The paper introduces L-STEP, a learnable spatial-temporal positional encoding method for graph deep learning, addressing limitations of current positional encoding methods and demonstrating superior performance in temporal link prediction.", "motivation": "Current positional encoding methods are limited by pre-defined functions, lack adaptability to evolving graphs, and are computationally expensive with transformers. The paper aims to develop a more effective and efficient solution.", "method": "Proposes L-STEP, a learnable positional encoding scheme, and validates its effectiveness using MLPs, robustness tests, and theoretical complexity analysis.", "result": "L-STEP outperforms 10 algorithms on 13 datasets in transductive and inductive settings, achieves leading performance on the TGB benchmark, and reduces empirical running time.", "conclusion": "L-STEP provides a robust, efficient, and high-performing solution for spatial-temporal positional encoding in graph deep learning."}}
{"id": "2504.12501", "pdf": "https://arxiv.org/pdf/2504.12501", "abs": "https://arxiv.org/abs/2504.12501", "authors": ["Nathan Lambert"], "title": "Reinforcement Learning from Human Feedback", "categories": ["cs.LG"], "comment": "131 pages. Web-native version at https://rlhfbook.com/ v2 adds more\n  reasoning content", "summary": "Reinforcement learning from human feedback (RLHF) has become an important\ntechnical and storytelling tool to deploy the latest machine learning systems.\nIn this book, we hope to give a gentle introduction to the core methods for\npeople with some level of quantitative background. The book starts with the\norigins of RLHF -- both in recent literature and in a convergence of disparate\nfields of science in economics, philosophy, and optimal control. We then set\nthe stage with definitions, problem formulation, data collection, and other\ncommon math used in the literature. The core of the book details every\noptimization stage in using RLHF, from starting with instruction tuning to\ntraining a reward model and finally all of rejection sampling, reinforcement\nlearning, and direct alignment algorithms. The book concludes with advanced\ntopics -- understudied research questions in synthetic data and evaluation --\nand open questions for the field.", "AI": {"tldr": "A gentle introduction to RLHF, covering its origins, core methods, and advanced topics for those with a quantitative background.", "motivation": "To provide an accessible yet comprehensive guide to RLHF, bridging recent literature and interdisciplinary fields like economics and philosophy.", "method": "Starts with foundational concepts, then details optimization stages (instruction tuning, reward modeling, alignment algorithms), and concludes with advanced research questions.", "result": "A structured resource for understanding RLHF, from basics to cutting-edge challenges.", "conclusion": "The book aims to equip readers with knowledge of RLHF's methods and open questions, fostering further exploration in the field."}}
{"id": "2506.08729", "pdf": "https://arxiv.org/pdf/2506.08729", "abs": "https://arxiv.org/abs/2506.08729", "authors": ["Dieuwertje Alblas", "Patryk Rygiel", "Julian Suk", "Kaj O. Kappe", "Marieke Hofman", "Christoph Brune", "Kak Khee Yeung", "Jelmer M. Wolterink"], "title": "Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the\nabdominal aorta. AAAs may rupture, with a survival rate of only 20\\%. Current\nclinical guidelines recommend elective surgical repair when the maximum AAA\ndiameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet\nthese criteria are periodically monitored, with surveillance intervals based on\nthe maximum AAA diameter. However, this diameter does not take into account the\ncomplex relation between the 3D AAA shape and its growth, making standardized\nintervals potentially unfit. Personalized AAA growth predictions could improve\nmonitoring strategies. We propose to use an SE(3)-symmetric transformer model\nto predict AAA growth directly on the vascular model surface enriched with\nlocal, multi-physical features. In contrast to other works which have\nparameterized the AAA shape, this representation preserves the vascular\nsurface's anatomical structure and geometric fidelity. We train our model using\na longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24\nAAA patients at irregularly sampled intervals. After training, our model\npredicts AAA growth to the next scan moment with a median diameter error of\n1.18 mm. We further demonstrate our model's utility to identify whether a\npatient will become eligible for elective repair within two years (acc = 0.93).\nFinally, we evaluate our model's generalization on an external validation set\nconsisting of 25 CTAs from 7 AAA patients from a different hospital. Our\nresults show that local directional AAA growth prediction from the vascular\nsurface is feasible and may contribute to personalized surveillance strategies.", "AI": {"tldr": "The paper proposes an SE(3)-symmetric transformer model to predict AAA growth using 3D vascular surface data, improving personalized monitoring over current diameter-based methods.", "motivation": "Current AAA monitoring relies on diameter thresholds, ignoring 3D shape complexities. Personalized growth predictions could enhance surveillance.", "method": "An SE(3)-symmetric transformer model predicts AAA growth using local, multi-physical features on vascular surfaces, trained on 113 CTAs from 24 patients.", "result": "The model achieves a median diameter error of 1.18 mm in growth prediction and 93% accuracy in identifying patients needing repair within two years.", "conclusion": "Local directional AAA growth prediction from vascular surfaces is feasible and may improve personalized surveillance strategies."}}
{"id": "2506.08336", "pdf": "https://arxiv.org/pdf/2506.08336", "abs": "https://arxiv.org/abs/2506.08336", "authors": ["Li Changjiang", "Liang Jiacheng", "Cao Bochuan", "Chen Jinghui", "Wang Ting"], "title": "Your Agent Can Defend Itself against Backdoor Attacks", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite their growing adoption across domains, large language model\n(LLM)-powered agents face significant security risks from backdoor attacks\nduring training and fine-tuning. These compromised agents can subsequently be\nmanipulated to execute malicious operations when presented with specific\ntriggers in their inputs or environments. To address this pressing risk, we\npresent ReAgent, a novel defense against a range of backdoor attacks on\nLLM-based agents. Intuitively, backdoor attacks often result in inconsistencies\namong the user's instruction, the agent's planning, and its execution. Drawing\non this insight, ReAgent employs a two-level approach to detect potential\nbackdoors. At the execution level, ReAgent verifies consistency between the\nagent's thoughts and actions; at the planning level, ReAgent leverages the\nagent's capability to reconstruct the instruction based on its thought\ntrajectory, checking for consistency between the reconstructed instruction and\nthe user's instruction. Extensive evaluation demonstrates ReAgent's\neffectiveness against various backdoor attacks across tasks. For instance,\nReAgent reduces the attack success rate by up to 90\\% in database operation\ntasks, outperforming existing defenses by large margins. This work reveals the\npotential of utilizing compromised agents themselves to mitigate backdoor\nrisks.", "AI": {"tldr": "ReAgent is a defense mechanism against backdoor attacks on LLM-based agents, detecting inconsistencies between instructions, planning, and execution to mitigate risks.", "motivation": "Addressing security risks from backdoor attacks during training and fine-tuning of LLM-powered agents.", "method": "ReAgent uses a two-level approach: verifying execution-level consistency (thoughts vs. actions) and planning-level consistency (reconstructed vs. original instructions).", "result": "Reduces attack success rate by up to 90% in tasks like database operations, outperforming existing defenses.", "conclusion": "ReAgent effectively mitigates backdoor risks by leveraging the compromised agents themselves."}}
{"id": "2505.02604", "pdf": "https://arxiv.org/pdf/2505.02604", "abs": "https://arxiv.org/abs/2505.02604", "authors": ["Yongding Tian", "Zaid Al-Ars", "Maksim Kitsak", "Peter Hofstee"], "title": "Low-Loss Space in Neural Networks is Continuous and Fully Connected", "categories": ["cs.LG"], "comment": "17 pages, 10 figures", "summary": "Visualizations of the loss landscape in neural networks suggest that minima\nare isolated points. However, both theoretical and empirical studies indicate\nthat it is possible to connect two different minima with a path consisting of\nintermediate points that also have low loss. In this study, we propose a new\nalgorithm which investigates low-loss paths in the full parameter space, not\nonly between two minima. Our experiments on LeNet5, ResNet18, and Compact\nConvolutional Transformer architectures consistently demonstrate the existence\nof such continuous paths in the parameter space. These results suggest that the\nlow-loss region is a fully connected and continuous space in the parameter\nspace. Our findings provide theoretical insight into neural network\nover-parameterization, highlighting that parameters collectively define a\nhigh-dimensional low-loss space, implying parameter redundancy exists only\nwithin individual models and not throughout the entire low-loss space.\nAdditionally, our work also provides new visualization methods and\nopportunities to improve model generalization by exploring the low-loss space\nthat is closer to the origin.", "AI": {"tldr": "The paper explores the existence of continuous low-loss paths in neural network parameter spaces, proposing a new algorithm to investigate these paths beyond just connecting minima.", "motivation": "To understand the structure of low-loss regions in neural networks, challenging the notion of isolated minima and exploring parameter redundancy.", "method": "A new algorithm is proposed to investigate low-loss paths in the full parameter space, tested on LeNet5, ResNet18, and Compact Convolutional Transformer architectures.", "result": "Experiments confirm the existence of continuous low-loss paths, suggesting the low-loss region is fully connected and continuous.", "conclusion": "The findings provide insights into over-parameterization, parameter redundancy, and offer new visualization methods and opportunities for improving model generalization."}}
{"id": "2506.08772", "pdf": "https://arxiv.org/pdf/2506.08772", "abs": "https://arxiv.org/abs/2506.08772", "authors": ["Jiayi Song", "Kaiyu Li", "Xiangyong Cao", "Deyu Meng"], "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation in remote sensing images is crucial for various\napplications, yet its performance is heavily reliant on large-scale,\nhigh-quality pixel-wise annotations, which are notoriously expensive and\ntime-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a\npromising alternative to mitigate this data dependency. However, existing SSS\nmethods often struggle with the inherent distribution mismatch between limited\nlabeled data and abundant unlabeled data, leading to suboptimal generalization.\nTo alleviate this issue, we attempt to introduce the Vision Foundation Models\n(VFMs) pre-trained on vast and diverse datasets into the SSS task since VFMs\npossess robust generalization capabilities that can effectively bridge this\ndistribution gap and provide strong semantic priors for SSS. Inspired by this,\nwe introduce RS-MTDF (Multi-Teacher Distillation and Fusion), a novel framework\nthat leverages the powerful semantic knowledge embedded in VFMs to guide\nsemi-supervised learning in remote sensing. Specifically, RS-MTDF employs\nmultiple frozen VFMs (e.g., DINOv2 and CLIP) as expert teachers, utilizing\nfeature-level distillation to align student features with their robust\nrepresentations. To further enhance discriminative power, the distilled\nknowledge is seamlessly fused into the student decoder. Extensive experiments\non three challenging remote sensing datasets demonstrate that RS-MTDF\nconsistently achieves state-of-the-art performance. Notably, our method\noutperforms existing approaches across various label ratios on LoveDA and\nsecures the highest IoU in the majority of semantic categories. These results\nunderscore the efficacy of multi-teacher VFM guidance in significantly\nenhancing both generalization and semantic understanding for remote sensing\nsegmentation. Ablation studies further validate the contribution of each\nproposed module.", "AI": {"tldr": "The paper introduces RS-MTDF, a semi-supervised semantic segmentation framework for remote sensing, leveraging Vision Foundation Models (VFMs) to bridge distribution gaps and improve performance.", "motivation": "Semantic segmentation in remote sensing requires costly annotations. Semi-supervised methods struggle with distribution mismatches between labeled and unlabeled data.", "method": "RS-MTDF uses multiple frozen VFMs (e.g., DINOv2, CLIP) as teachers for feature-level distillation, fusing their knowledge into the student decoder.", "result": "RS-MTDF achieves state-of-the-art performance on three datasets, outperforming others in label ratios and IoU scores.", "conclusion": "Multi-teacher VFM guidance enhances generalization and semantic understanding in remote sensing segmentation, validated by ablation studies."}}
{"id": "2506.08440", "pdf": "https://arxiv.org/pdf/2506.08440", "abs": "https://arxiv.org/abs/2506.08440", "authors": ["Zengjue Chen", "Runliang Niu", "He Kong", "Qi Wang"], "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) model have demonstrated\nstrong generalization capabilities across diverse scenes, tasks, and robotic\nplatforms when pretrained at large-scale datasets. However, these models still\nrequire task-specific fine-tuning in novel environments, a process that relies\nalmost exclusively on supervised fine-tuning (SFT) using static trajectory\ndatasets. Such approaches neither allow robot to interact with environment nor\ndo they leverage feedback from live execution. Also, their success is\ncritically dependent on the size and quality of the collected trajectories.\nReinforcement learning (RL) offers a promising alternative by enabling\nclosed-loop interaction and aligning learned policies directly with task\nobjectives. In this work, we draw inspiration from the ideas of GRPO and\npropose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.\nBy fusing step-level and trajectory-level advantage signals, this method\nimproves GRPO's group-level advantage estimation, thereby making the algorithm\nmore suitable for online reinforcement learning training of VLA. Experimental\nresults on ten manipulation tasks from the libero-object benchmark demonstrate\nthat TGRPO consistently outperforms various baseline methods, capable of\ngenerating more robust and efficient policies across multiple tested scenarios.\nOur source codes are available at: https://github.com/hahans/TGRPO", "AI": {"tldr": "TGRPO improves GRPO by combining step-level and trajectory-level advantages for better online RL training of VLA models, outperforming baselines in manipulation tasks.", "motivation": "Current VLA models rely on supervised fine-tuning with static datasets, lacking interaction and live feedback. RL offers a promising alternative.", "method": "Proposes TGRPO, fusing step-level and trajectory-level advantage signals to enhance GRPO's group-level advantage estimation.", "result": "TGRPO outperforms baselines in ten manipulation tasks, generating more robust and efficient policies.", "conclusion": "TGRPO is effective for online RL training of VLA models, improving policy robustness and efficiency."}}
{"id": "2505.16353", "pdf": "https://arxiv.org/pdf/2505.16353", "abs": "https://arxiv.org/abs/2505.16353", "authors": ["C\u00e9line Comte", "Pascal Moyal"], "title": "Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning", "categories": ["cs.LG", "math.OC", "math.PR"], "comment": null, "summary": "In this paper, we introduce a versatile scheme for optimizing the arrival\nrates of quasi-reversible queueing systems. We first propose an alternative\ndefinition of quasi-reversibility that encompasses reversibility and highlights\nthe importance of the definition of customer classes. In a second time, we\nintroduce balanced arrival control policies, which generalize the notion of\nbalanced arrival rates introduced in the context of Whittle networks, to the\nmuch broader class of quasi-reversible queueing systems. We prove that\nsupplementing a quasi-reversible queueing system with a balanced\narrival-control policy preserves the quasi-reversibility, and we specify the\nform of the stationary measures. We revisit two canonical examples of\nquasi-reversible queueing systems, Whittle networks and order-independent\nqueues. Lastly, we focus on the problem of admission control and leverage our\nresults in the frameworks of optimization and reinforcement learning.", "AI": {"tldr": "The paper introduces a scheme for optimizing arrival rates in quasi-reversible queueing systems, proposing a new definition of quasi-reversibility and balanced arrival control policies. It shows these policies preserve quasi-reversibility and applies the framework to optimization and reinforcement learning.", "motivation": "To generalize and optimize arrival rate control in quasi-reversible queueing systems, extending beyond Whittle networks.", "method": "Proposes an alternative definition of quasi-reversibility, introduces balanced arrival control policies, and proves their preservation of quasi-reversibility. Applies the framework to canonical examples and admission control.", "result": "Balanced arrival control policies preserve quasi-reversibility, with specified stationary measures. Demonstrated applicability in optimization and reinforcement learning.", "conclusion": "The framework successfully generalizes and optimizes arrival rate control in quasi-reversible systems, with practical applications in admission control and learning."}}
{"id": "2506.08777", "pdf": "https://arxiv.org/pdf/2506.08777", "abs": "https://arxiv.org/abs/2506.08777", "authors": ["Keyi Liu", "Weidong Yang", "Ben Fei", "Ying He"], "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning (SSL) for point cloud pre-training has become a\ncornerstone for many 3D vision tasks, enabling effective learning from\nlarge-scale unannotated data. At the scene level, existing SSL methods often\nincorporate volume rendering into the pre-training framework, using RGB-D\nimages as reconstruction signals to facilitate cross-modal learning. This\nstrategy promotes alignment between 2D and 3D modalities and enables the model\nto benefit from rich visual cues in the RGB-D inputs. However, these approaches\nare limited by their reliance on implicit scene representations and high memory\ndemands. Furthermore, since their reconstruction objectives are applied only in\n2D space, they often fail to capture underlying 3D geometric structures. To\naddress these challenges, we propose Gaussian2Scene, a novel scene-level SSL\nframework that leverages the efficiency and explicit nature of 3D Gaussian\nSplatting (3DGS) for pre-training. The use of 3DGS not only alleviates the\ncomputational burden associated with volume rendering but also supports direct\n3D scene reconstruction, thereby enhancing the geometric understanding of the\nbackbone network. Our approach follows a progressive two-stage training\nstrategy. In the first stage, a dual-branch masked autoencoder learns both 2D\nand 3D scene representations. In the second stage, we initialize training with\nreconstructed point clouds and further supervise learning using the geometric\nlocations of Gaussian primitives and rendered RGB images. This process\nreinforces both geometric and cross-modal learning. We demonstrate the\neffectiveness of Gaussian2Scene across several downstream 3D object detection\ntasks, showing consistent improvements over existing pre-training methods.", "AI": {"tldr": "Gaussian2Scene is a novel SSL framework using 3D Gaussian Splatting for efficient and explicit 3D scene pre-training, outperforming existing methods in 3D object detection.", "motivation": "Existing SSL methods for point cloud pre-training rely on implicit scene representations and 2D reconstruction, limiting geometric understanding and computational efficiency.", "method": "Gaussian2Scene employs a two-stage training strategy: a dual-branch masked autoencoder for 2D/3D representations, followed by supervised learning with Gaussian primitives and RGB images.", "result": "The framework shows consistent improvements in downstream 3D object detection tasks.", "conclusion": "Gaussian2Scene enhances geometric and cross-modal learning, offering a more efficient and effective SSL solution for 3D vision tasks."}}
{"id": "2506.08563", "pdf": "https://arxiv.org/pdf/2506.08563", "abs": "https://arxiv.org/abs/2506.08563", "authors": ["Siyuan Yang", "Cheng Song", "Zhilu Lai", "Wenjia Wang"], "title": "KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks", "categories": ["cs.CE", "cs.AI", "math-ph", "math.MP", "physics.comp-ph"], "comment": "Accepted to IJCAI 2025", "summary": "Differential equations are involved in modeling many engineering problems.\nMany efforts have been devoted to solving differential equations. Due to the\nflexibility of neural networks, Physics Informed Neural Networks (PINNs) have\nrecently been proposed to solve complex differential equations and have\ndemonstrated superior performance in many applications. While the L2 loss\nfunction is usually a default choice in PINNs, it has been shown that the\ncorresponding numerical solution is incorrect and unstable for some complex\nequations. In this work, we propose a new PINNs framework named Kernel Packet\naccelerated PINNs (KP-PINNs), which gives a new expression of the loss function\nusing the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel\nPacket (KP) method to accelerate the computation. Theoretical results show that\nKP-PINNs can be stable across various differential equations. Numerical\nexperiments illustrate that KP-PINNs can solve differential equations\neffectively and efficiently. This framework provides a promising direction for\nimproving the stability and accuracy of PINNs-based solvers in scientific\ncomputing.", "AI": {"tldr": "KP-PINNs improve stability and accuracy of PINNs by using RKHS norm for loss function and Kernel Packet method for faster computation.", "motivation": "The L2 loss function in PINNs can lead to incorrect and unstable solutions for complex differential equations.", "method": "Proposes KP-PINNs, a new framework using RKHS norm for loss function and Kernel Packet method for acceleration.", "result": "KP-PINNs demonstrate stability and effectiveness in solving differential equations.", "conclusion": "KP-PINNs offer a promising approach to enhance PINNs-based solvers in scientific computing."}}
{"id": "2505.17777", "pdf": "https://arxiv.org/pdf/2505.17777", "abs": "https://arxiv.org/abs/2505.17777", "authors": ["Harish G. Ramaswamy", "L. A. Prashanth"], "title": "Optimizing Shortfall Risk Metric for Learning Regression Models", "categories": ["cs.LG"], "comment": null, "summary": "We consider the problem of estimating and optimizing utility-based shortfall\nrisk (UBSR) of a loss, say $(Y - \\hat Y)^2$, in the context of a regression\nproblem. Empirical risk minimization with a UBSR objective is challenging since\nUBSR is a non-linear function of the underlying distribution. We first derive a\nconcentration bound for UBSR estimation using independent and identically\ndistributed (i.i.d.) samples. We then frame the UBSR optimization problem as\nminimization of a pseudo-linear function in the space of achievable\ndistributions $\\mathcal D$ of the loss $(Y- \\hat Y)^2$. We construct a gradient\noracle for the UBSR objective and a linear minimization oracle (LMO) for the\nset $\\mathcal D$. Using these oracles, we devise a bisection-type algorithm,\nand establish convergence to the UBSR-optimal solution.", "AI": {"tldr": "The paper addresses estimating and optimizing utility-based shortfall risk (UBSR) in regression, proposing a gradient-based algorithm with convergence guarantees.", "motivation": "UBSR is a non-linear risk measure, making empirical risk minimization challenging. The work aims to provide theoretical and algorithmic solutions for UBSR optimization.", "method": "Derives a concentration bound for UBSR estimation, formulates UBSR optimization as a pseudo-linear problem, and introduces gradient and linear minimization oracles for a bisection-type algorithm.", "result": "The proposed algorithm converges to the UBSR-optimal solution, supported by theoretical guarantees.", "conclusion": "The paper provides a practical and theoretically sound approach to UBSR optimization in regression problems."}}
{"id": "2506.08817", "pdf": "https://arxiv.org/pdf/2506.08817", "abs": "https://arxiv.org/abs/2506.08817", "authors": ["Shuyi Zhang", "Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Pengwei Wang", "Zhongyuan Wang", "Hongxuan Ma", "Shanghang Zhang"], "title": "Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought", "categories": ["cs.CV"], "comment": null, "summary": "Video content comprehension is essential for various applications, ranging\nfrom video analysis to interactive systems. Despite advancements in large-scale\nvision-language models (VLMs), these models often struggle to capture the\nnuanced, spatiotemporal details essential for thorough video analysis. To\naddress this gap, we introduce Video-CoT, a groundbreaking dataset designed to\nenhance spatiotemporal understanding using Chain-of-Thought (CoT)\nmethodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal\nquestion-answer pairs and 23,000 high-quality CoT-annotated samples, providing\na solid foundation for evaluating spatiotemporal understanding in video\ncomprehension. Additionally, we provide a comprehensive benchmark for assessing\nthese tasks, with each task featuring 750 images and tailored evaluation\nmetrics. Our extensive experiments reveal that current VLMs face significant\nchallenges in achieving satisfactory performance, high-lighting the\ndifficulties of effective spatiotemporal understanding. Overall, the Video-CoT\ndataset and benchmark open new avenues for research in multimedia understanding\nand support future innovations in intelligent systems requiring advanced video\nanalysis capabilities. By making these resources publicly available, we aim to\nencourage further exploration in this critical area. Project\nwebsite:https://video-cot.github.io/ .", "AI": {"tldr": "Video-CoT introduces a dataset and benchmark for enhancing spatiotemporal video understanding using Chain-of-Thought methodologies, revealing current VLMs' limitations.", "motivation": "Current vision-language models lack nuanced spatiotemporal understanding in video analysis, necessitating a specialized dataset.", "method": "Developed Video-CoT with 192,000 question-answer pairs and 23,000 CoT-annotated samples, plus a benchmark for evaluation.", "result": "Experiments show VLMs struggle with spatiotemporal tasks, underscoring the dataset's value.", "conclusion": "Video-CoT advances multimedia understanding and supports future video analysis innovations."}}
{"id": "2506.08860", "pdf": "https://arxiv.org/pdf/2506.08860", "abs": "https://arxiv.org/abs/2506.08860", "authors": ["Samah Kansab", "Francis Bordeleau", "Ali Tizghadam"], "title": "On The Impact of Merge Request Deviations on Code Review Practices", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Code review is a key practice in software engineering, ensuring quality and\ncollaboration. However, industrial Merge Request (MR) workflows often deviate\nfrom standardized review processes, with many MRs serving non-review purposes\n(e.g., drafts, rebases, or dependency updates). We term these cases deviations\nand hypothesize that ignoring them biases analytics and undermines ML models\nfor review analysis.\n  We identify seven deviation categories, occurring in 37.02% of MRs, and\npropose a few-shot learning detection method (91% accuracy). By excluding\ndeviations, ML models predicting review completion time improve performance in\n53.33% of cases (up to 2.25x) and exhibit significant shifts in feature\nimportance (47% overall, 60% top-*k*).\n  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven\ndetection approach, and (3) empirical evidence of their impact on ML-based\nreview analytics. This work aids practitioners in optimizing review efforts and\nensuring reliable insights.", "AI": {"tldr": "The paper identifies deviations in Merge Request (MR) workflows, proposes a detection method, and shows their impact on ML models for review analysis.", "motivation": "Industrial MR workflows often deviate from standardized review processes, biasing analytics and undermining ML models for review analysis.", "method": "The authors identify seven deviation categories and propose a few-shot learning detection method.", "result": "Deviations occur in 37.02% of MRs. Excluding them improves ML model performance (53.33% of cases) and shifts feature importance.", "conclusion": "The work aids practitioners by providing a taxonomy, AI-driven detection, and empirical evidence of deviations' impact on review analytics."}}
{"id": "2505.19789", "pdf": "https://arxiv.org/pdf/2505.19789", "abs": "https://arxiv.org/abs/2505.19789", "authors": ["Jijia Liu", "Feng Gao", "Bingwen Wei", "Xinlei Chen", "Qingmin Liao", "Yi Wu", "Chao Yu", "Yu Wang"], "title": "What Can RL Bring to VLA Generalization? An Empirical Study", "categories": ["cs.LG"], "comment": null, "summary": "Large Vision-Language Action (VLA) models have shown significant potential\nfor embodied AI. However, their predominant training via supervised fine-tuning\n(SFT) limits generalization due to susceptibility to compounding errors under\ndistribution shifts. Reinforcement learning (RL) offers a path to overcome\nthese limitations by optimizing for task objectives via trial-and-error, yet a\nsystematic understanding of its specific generalization benefits for VLAs\ncompared to SFT is lacking. To address this, our study introduces a\ncomprehensive benchmark for evaluating VLA generalization and systematically\ninvestigates the impact of RL fine-tuning across diverse visual, semantic, and\nexecution dimensions. Our extensive experiments reveal that RL fine-tuning,\nparticularly with PPO, significantly enhances generalization in semantic\nunderstanding and execution robustness over SFT, while maintaining comparable\nvisual robustness. We identify PPO as a more effective RL algorithm for VLAs\nthan LLM-derived methods like DPO and GRPO. We also develop a simple recipe for\nefficient PPO training on VLAs, and demonstrate its practical utility for\nimproving VLA generalization. The project page is at https://rlvla.github.io", "AI": {"tldr": "RL fine-tuning (e.g., PPO) improves generalization in VLAs over supervised fine-tuning (SFT), especially in semantic understanding and execution robustness.", "motivation": "To address the limitations of SFT in VLA models by exploring RL's potential for better generalization.", "method": "Systematic benchmark for VLA generalization evaluation, comparing RL (PPO, DPO, GRPO) and SFT.", "result": "PPO outperforms SFT and other RL methods in semantic and execution robustness, with comparable visual robustness.", "conclusion": "PPO is effective for VLA generalization, with a proposed training recipe for practical use."}}
{"id": "2506.08849", "pdf": "https://arxiv.org/pdf/2506.08849", "abs": "https://arxiv.org/abs/2506.08849", "authors": ["Jingguo Qu", "Xinyang Han", "Tonghuan Xiao", "Jia Ai", "Juan Wu", "Tong Zhao", "Jing Qin", "Ann Dorothy King", "Winnie Chiu-Wing Chu", "Jing Cai", "Michael Tin-Cheung Ying"], "title": "Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Medical ultrasonography is an essential imaging technique for examining\nsuperficial organs and tissues, including lymph nodes, breast, and thyroid. It\nemploys high-frequency ultrasound waves to generate detailed images of the\ninternal structures of the human body. However, manually contouring regions of\ninterest in these images is a labor-intensive task that demands expertise and\noften results in inconsistent interpretations among individuals.\nVision-language foundation models, which have excelled in various computer\nvision applications, present new opportunities for enhancing ultrasound image\nanalysis. Yet, their performance is hindered by the significant differences\nbetween natural and medical imaging domains. This research seeks to overcome\nthese challenges by developing domain adaptation methods for vision-language\nfoundation models. In this study, we explore the fine-tuning pipeline for\nvision-language foundation models by utilizing large language model as text\nrefiner with special-designed adaptation strategies and task-driven heads. Our\napproach has been extensively evaluated on six ultrasound datasets and two\ntasks: segmentation and classification. The experimental results show that our\nmethod can effectively improve the performance of vision-language foundation\nmodels for ultrasound image analysis, and outperform the existing\nstate-of-the-art vision-language and pure foundation models. The source code of\nthis study is available at https://github.com/jinggqu/NextGen-UIA.", "AI": {"tldr": "The paper proposes domain adaptation methods for vision-language foundation models to improve ultrasound image analysis, outperforming existing models.", "motivation": "Manual contouring in medical ultrasonography is labor-intensive and inconsistent, while vision-language models face domain gaps between natural and medical images.", "method": "Fine-tuning pipeline for vision-language models using a large language model as text refiner with adaptation strategies and task-driven heads.", "result": "Evaluated on six ultrasound datasets, the method improves performance for segmentation and classification, surpassing state-of-the-art models.", "conclusion": "The approach effectively bridges the domain gap, enhancing ultrasound image analysis with publicly available source code."}}
{"id": "2505.22935", "pdf": "https://arxiv.org/pdf/2505.22935", "abs": "https://arxiv.org/abs/2505.22935", "authors": ["Jipeng Li", "Yanning Shen"], "title": "Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Explicit noise-level conditioning is widely regarded as essential for the\neffective operation of Graph Diffusion Models (GDMs). In this work, we\nchallenge this assumption by investigating whether denoisers can implicitly\ninfer noise levels directly from corrupted graph structures, potentially\neliminating the need for explicit noise conditioning. To this end, we develop a\ntheoretical framework centered on Bernoulli edge-flip corruptions and extend it\nto encompass more complex scenarios involving coupled structure-attribute\nnoise. Extensive empirical evaluations on both synthetic and real-world graph\ndatasets, using models such as GDSS and DiGress, provide strong support for our\ntheoretical findings. Notably, unconditional GDMs achieve performance\ncomparable or superior to their conditioned counterparts, while also offering\nreductions in parameters (4-6%) and computation time (8-10%). Our results\nsuggest that the high-dimensional nature of graph data itself often encodes\nsufficient information for the denoising process, opening avenues for simpler,\nmore efficient GDM architectures.", "AI": {"tldr": "Unconditional Graph Diffusion Models (GDMs) can implicitly infer noise levels from corrupted graph structures, eliminating the need for explicit noise conditioning, while maintaining or improving performance and efficiency.", "motivation": "Challenge the assumption that explicit noise-level conditioning is essential for GDMs by exploring if denoisers can implicitly infer noise levels from corrupted graph structures.", "method": "Develop a theoretical framework for Bernoulli edge-flip corruptions and extend it to coupled structure-attribute noise. Evaluate using synthetic and real-world datasets with models like GDSS and DiGress.", "result": "Unconditional GDMs perform comparably or better than conditioned ones, with 4-6% fewer parameters and 8-10% faster computation.", "conclusion": "Graph data's high-dimensional nature often encodes enough information for denoising, enabling simpler and more efficient GDM architectures."}}
{"id": "2506.08900", "pdf": "https://arxiv.org/pdf/2506.08900", "abs": "https://arxiv.org/abs/2506.08900", "authors": ["Jos\u00e9 Morano", "Botond Fazekas", "Emese S\u00fckei", "Ronald Fecso", "Taha Emre", "Markus Gumpinger", "Georg Faustmann", "Marzieh Oghbaie", "Ursula Schmidt-Erfurth", "Hrvoje Bogunovi\u0107"], "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis", "categories": ["cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.", "AI": {"tldr": "MIRAGE is a multimodal foundation model for OCT and SLO image analysis, outperforming existing models in classification and segmentation tasks.", "motivation": "Existing AI models for ophthalmology lack validation and focus on single modalities, prompting the need for a robust, multimodal solution.", "method": "Proposes MIRAGE, a foundation model for OCT and SLO images, and introduces a new evaluation benchmark.", "result": "MIRAGE outperforms general and specialized models in classification and segmentation tasks.", "conclusion": "MIRAGE is a suitable foundation for robust AI systems in retinal OCT analysis, with publicly available resources."}}
{"id": "2505.24461", "pdf": "https://arxiv.org/pdf/2505.24461", "abs": "https://arxiv.org/abs/2505.24461", "authors": ["Jingyao Li", "Senqiao Yang", "Sitong Wu", "Han Shi", "Chuanyang Zheng", "Hong Xu", "Jiaya Jia"], "title": "Logits-Based Finetuning", "categories": ["cs.LG"], "comment": null, "summary": "In recent years, developing compact and efficient large language models\n(LLMs) has emerged as a thriving area of research. Traditional Supervised\nFine-Tuning (SFT), which relies on singular ground truth labels, often fails to\ncapture token-level dependencies and linguistic diversity. To address these\nlimitations, we propose a logits-based fine-tuning framework that integrates\nthe strengths of supervised learning and knowledge distillation. Our approach\nconstructs enriched training targets by combining teacher logits with ground\ntruth labels, preserving both correctness and linguistic diversity. This\nensures more reliable and effective training. We constructed a large-scale 1.2M\nlogits dataset and trained a series of science-focused models. Experimental\nresults demonstrate that our method achieves significant improvements, with\naccuracy gains of 18% on Mawps and 22.7% on TabMWP. Across nine widely used\nmathematical benchmarks, our method consistently outperforms prior SFT models,\nachieving an average improvement of 7.28%. Codes are available at\nhttps://github.com/dvlab-research/Logits-Based-Finetuning.", "AI": {"tldr": "A logits-based fine-tuning framework is proposed to improve LLMs by combining teacher logits with ground truth labels, achieving significant accuracy gains on benchmarks.", "motivation": "Traditional SFT fails to capture token-level dependencies and linguistic diversity, prompting the need for a more effective training method.", "method": "Integrates supervised learning and knowledge distillation by combining teacher logits with ground truth labels to create enriched training targets.", "result": "Achieves 18% accuracy gain on Mawps, 22.7% on TabMWP, and 7.28% average improvement across nine mathematical benchmarks.", "conclusion": "The proposed framework outperforms prior SFT models, offering a more reliable and effective training approach."}}
{"id": "2506.08908", "pdf": "https://arxiv.org/pdf/2506.08908", "abs": "https://arxiv.org/abs/2506.08908", "authors": ["Jiajun Li", "Yue Ma", "Xinyu Zhang", "Qingyan Wei", "Songhua Liu", "Linfeng Zhang"], "title": "SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies on Visual Autoregressive (VAR) models have highlighted that\nhigh-frequency components, or later steps, in the generation process contribute\ndisproportionately to inference latency. However, the underlying computational\nredundancy involved in these steps has yet to be thoroughly investigated. In\nthis paper, we conduct an in-depth analysis of the VAR inference process and\nidentify two primary sources of inefficiency: step redundancy and unconditional\nbranch redundancy. To address step redundancy, we propose an automatic\nstep-skipping strategy that selectively omits unnecessary generation steps to\nimprove efficiency. For unconditional branch redundancy, we observe that the\ninformation gap between the conditional and unconditional branches is minimal.\nLeveraging this insight, we introduce unconditional branch replacement, a\ntechnique that bypasses the unconditional branch to reduce computational cost.\nNotably, we observe that the effectiveness of acceleration strategies varies\nsignificantly across different samples. Motivated by this, we propose SkipVAR,\na sample-adaptive framework that leverages frequency information to dynamically\nselect the most suitable acceleration strategy for each instance. To evaluate\nthe role of high-frequency information, we introduce high-variation benchmark\ndatasets that test model sensitivity to fine details. Extensive experiments\nshow SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall\nacceleration and 2.62x speedup on the GenEval benchmark, maintaining model\nquality. These results confirm the effectiveness of frequency-aware,\ntraining-free adaptive acceleration for scalable autoregressive image\ngeneration. Our code is available at https://github.com/fakerone-li/SkipVAR and\nhas been publicly released.", "AI": {"tldr": "SkipVAR introduces adaptive acceleration for VAR models by addressing step and unconditional branch redundancies, achieving significant speedup without quality loss.", "motivation": "High-frequency components in VAR models cause latency, but computational redundancy in these steps is understudied.", "method": "Proposes step-skipping and unconditional branch replacement, combined in SkipVAR, a sample-adaptive framework using frequency info.", "result": "Achieves 1.81x overall acceleration, 2.62x speedup on GenEval, with 0.88 SSIM, maintaining quality.", "conclusion": "SkipVAR effectively accelerates VAR models adaptively, confirming the value of frequency-aware, training-free methods."}}
{"id": "2506.02300", "pdf": "https://arxiv.org/pdf/2506.02300", "abs": "https://arxiv.org/abs/2506.02300", "authors": ["Farzaneh Mahdisoltani", "Saeed Mahdisoltani", "Roger B. Grosse", "David J. Fleet"], "title": "Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation", "categories": ["cs.LG"], "comment": null, "summary": "Understanding the internal representations and decision mechanisms of deep\nneural networks remains a critical open challenge. While existing\ninterpretability methods often identify influential input regions, they may not\nelucidate how a model distinguishes between classes or what specific changes\nwould transition an input from one category to another. To address these\nlimitations, we propose a novel framework that visualizes the implicit path\nbetween classes by treating the network gradient as a form of infinitesimal\nmotion. Drawing inspiration from phase-based motion magnification, we first\ndecompose images using invertible transforms-specifically the Complex Steerable\nPyramid-then compute class-conditional gradients in the transformed space.\nRather than iteratively integrating the gradient to trace a full path, we\namplify the one-step gradient to the input and perform a linear extrapolation\nto expose how the model moves from source to target class. By operating in the\nsteerable pyramid domain, these amplified gradients produce semantically\nmeaningful, spatially coherent morphs that highlight the classifier's most\nsensitive directions, giving insight into the geometry of its decision\nboundaries. Experiments on both synthetic and real-world datasets demonstrate\nthat our phase-focused extrapolation yields perceptually aligned, semantically\nmeaningful transformations, offering a novel, interpretable lens into neural\nclassifiers' internal representations.", "AI": {"tldr": "The paper introduces a framework to visualize how deep neural networks transition inputs between classes by amplifying gradients in the steerable pyramid domain, revealing decision boundaries.", "motivation": "Existing interpretability methods lack clarity on how models distinguish classes or transition inputs between them.", "method": "Uses phase-based motion magnification, decomposes images with the Complex Steerable Pyramid, and amplifies class-conditional gradients for linear extrapolation.", "result": "Produces semantically meaningful transformations, highlighting sensitive directions in the model's decision boundaries.", "conclusion": "The framework offers a novel, interpretable way to understand neural classifiers' internal representations."}}
{"id": "2506.09022", "pdf": "https://arxiv.org/pdf/2506.09022", "abs": "https://arxiv.org/abs/2506.09022", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "title": "Do Multiple Instance Learning Models Transfer?", "categories": ["cs.CV"], "comment": "ICML 2025 (Spotlight). 20 pages, 8 figures", "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab", "AI": {"tldr": "Pretrained MIL models outperform scratch-trained ones in computational pathology, even across different organs, and pancancer pretraining enhances generalization.", "motivation": "To address the lack of understanding about MIL model transferability in computational pathology, especially with small datasets.", "method": "Systematically evaluated 11 pretrained MIL models across 21 tasks for morphological and molecular subtype prediction.", "result": "Pretrained MIL models consistently outperformed scratch-trained models, with pancancer pretraining showing strong generalization.", "conclusion": "MIL models are adaptable, and transfer learning boosts performance in computational pathology; a standardized resource is provided."}}
{"id": "2506.04430", "pdf": "https://arxiv.org/pdf/2506.04430", "abs": "https://arxiv.org/abs/2506.04430", "authors": ["Egor Petrov", "Grigoriy Evseev", "Aleksey Antonov", "Andrey Veprikov", "Pavel Plyusnin", "Nikolay Bushkov", "Stanislav Moiseev", "Aleksandr Beznosikov"], "title": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order", "categories": ["cs.LG", "math.OC"], "comment": "26 pages, 5 tables", "summary": "Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM", "AI": {"tldr": "The paper introduces zero-order (ZO) optimization methods, JAGUAR SignSGD and JAGUAR Muon, as efficient alternatives to traditional first-order optimizers for fine-tuning LLMs, offering memory reduction and strong convergence guarantees.", "motivation": "Traditional first-order optimizers like SGD and Adam are computationally expensive for fine-tuning large language models (LLMs), prompting the need for more efficient alternatives.", "method": "The authors propose JAGUAR SignSGD, a ZO momentum-based algorithm, and JAGUAR Muon, a ZO extension of the Muon optimizer, both designed for parameter-efficient fine-tuning.", "result": "The proposed methods achieve or surpass the convergence quality of first-order methods while significantly reducing memory usage, as demonstrated in LLM fine-tuning benchmarks.", "conclusion": "ZO optimization methods are a practical and theoretically grounded solution for resource-constrained LLM adaptation, with promising empirical and theoretical results."}}
{"id": "2506.05668", "pdf": "https://arxiv.org/pdf/2506.05668", "abs": "https://arxiv.org/abs/2506.05668", "authors": ["Jiajun He", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Yuanqi Du", "Francisco Vargas"], "title": "RNE: a plug-and-play framework for diffusion density estimation and inference-time control", "categories": ["cs.LG", "stat.ML"], "comment": "39 pages; 14 figures", "summary": "In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,\nplug-and-play framework for diffusion inference-time density estimation and\ncontrol, based on the concept of the density ratio between path distributions.\nRNE connects and unifies a variety of existing density estimation and\ninference-time control methods under a single and intuitive perspective,\nstemming from basic variational inference and probabilistic principles\ntherefore offering both theoretical clarity and practical versatility.\nExperiments demonstrate that RNE delivers strong results in diffusion density\nestimation, and offers broad applicability to inference-time control tasks --\nsuch as annealing, diffusion model composition, and reward-tilting -- with\npromising inference-time scaling performance.", "AI": {"tldr": "The paper introduces the Radon-Nikodym Estimator (RNE), a versatile framework for diffusion inference-time density estimation and control, unifying existing methods with theoretical and practical benefits.", "motivation": "To provide a unified and intuitive framework for density estimation and inference-time control in diffusion models, addressing the need for theoretical clarity and practical versatility.", "method": "RNE leverages the density ratio between path distributions, rooted in variational inference and probabilistic principles, to connect and unify existing methods.", "result": "RNE achieves strong performance in diffusion density estimation and demonstrates broad applicability in inference-time control tasks like annealing, model composition, and reward-tilting.", "conclusion": "RNE offers a theoretically grounded and practically versatile solution for density estimation and control in diffusion models, with promising scaling performance."}}
{"id": "2503.14756", "pdf": "https://arxiv.org/pdf/2503.14756", "abs": "https://arxiv.org/abs/2503.14756", "authors": ["Hou In Ivan Tam", "Hou In Derek Pun", "Austin T. Wang", "Angel X. Chang", "Manolis Savva"], "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "Expanded dataset to 500 annotated scene descriptions with new scene\n  types; added validation via extended manual evaluation and a new user study;\n  clarified distinctions from prior metrics; included results using an\n  open-source VLM; stated intent to release code and data; corrected\n  terminology and typos. 24 pages with 8 figures and 6 tables", "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there\nremain gaps in the evaluation of these methods. Existing metrics primarily\nassess the realism of generated scenes by comparing them to a set of\nground-truth scenes, often overlooking alignment with the input text - a\ncritical factor in determining how effectively a method meets user\nrequirements. We present SceneEval, an evaluation framework designed to address\nthis limitation. SceneEval includes metrics for both explicit user\nrequirements, such as the presence of specific objects and their attributes\ndescribed in the input text, and implicit expectations, like the absence of\nobject collisions, providing a comprehensive assessment of scene quality. To\nfacilitate evaluation, we introduce SceneEval-500, a dataset of scene\ndescriptions with annotated ground-truth scene properties. We evaluate recent\nscene generation methods using SceneEval and demonstrate its ability to provide\ndetailed assessments of the generated scenes, highlighting strengths and areas\nfor improvement across multiple dimensions. Our results show that current\nmethods struggle at generating scenes that meet user requirements, underscoring\nthe need for further research in this direction.", "AI": {"tldr": "SceneEval is a new framework for evaluating text-conditioned 3D scene generation, addressing gaps in current metrics by assessing alignment with input text and implicit expectations.", "motivation": "Existing evaluation metrics for 3D scene generation focus on realism but overlook text alignment, a critical factor for user requirements.", "method": "SceneEval introduces metrics for explicit (object presence/attributes) and implicit (e.g., no collisions) text alignment, using the SceneEval-500 dataset for evaluation.", "result": "Current methods struggle to meet user requirements, as shown by SceneEval's detailed assessments.", "conclusion": "SceneEval highlights the need for further research to improve text-conditioned 3D scene generation."}}
{"id": "2506.05957", "pdf": "https://arxiv.org/pdf/2506.05957", "abs": "https://arxiv.org/abs/2506.05957", "authors": ["Tianjun Yao", "Haoxuan Li", "Yongqiang Chen", "Tongliang Liu", "Le Song", "Eric Xing", "Zhiqiang Shen"], "title": "Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization", "categories": ["cs.LG", "I.2.6"], "comment": "Submission of ICML2025, with score 4/4/3/3", "summary": "Graph Neural Networks (GNNs) often encounter significant performance\ndegradation under distribution shifts between training and test data, hindering\ntheir applicability in real-world scenarios. Recent studies have proposed\nvarious methods to address the out-of-distribution generalization challenge,\nwith many methods in the graph domain focusing on directly identifying an\ninvariant subgraph that is predictive of the target label. However, we argue\nthat identifying the edges from the invariant subgraph directly is challenging\nand error-prone, especially when some spurious edges exhibit strong\ncorrelations with the targets. In this paper, we propose PrunE, the first\npruning-based graph OOD method that eliminates spurious edges to improve OOD\ngeneralizability. By pruning spurious edges, PrunE retains the invariant\nsubgraph more comprehensively, which is critical for OOD generalization.\nSpecifically, PrunE employs two regularization terms to prune spurious edges:\n1) graph size constraint to exclude uninformative spurious edges, and 2)\n$\\epsilon$-probability alignment to further suppress the occurrence of spurious\nedges. Through theoretical analysis and extensive experiments, we show that\nPrunE achieves superior OOD performance and outperforms previous\nstate-of-the-art methods significantly. Codes are available at:\n\\href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.", "AI": {"tldr": "PrunE is a pruning-based method to improve Graph Neural Networks' OOD generalization by removing spurious edges, outperforming prior methods.", "motivation": "GNNs struggle with performance under distribution shifts due to spurious edges, and existing methods fail to reliably identify invariant subgraphs.", "method": "PrunE uses two regularization terms: graph size constraint and \u03b5-probability alignment to prune spurious edges.", "result": "PrunE achieves superior OOD performance and significantly outperforms state-of-the-art methods.", "conclusion": "PrunE effectively improves GNNs' OOD generalization by pruning spurious edges, validated by theory and experiments."}}
{"id": "2506.09023", "pdf": "https://arxiv.org/pdf/2506.09023", "abs": "https://arxiv.org/abs/2506.09023", "authors": ["Julia Guerrero-Viu", "Michael Fischer", "Iliyan Georgiev", "Elena Garces", "Diego Gutierrez", "Belen Masia", "Valentin Deschaintre"], "title": "Fine-Grained Spatially Varying Material Selection in Images", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Selection is the first step in many image editing processes, enabling faster\nand simpler modifications of all pixels sharing a common modality. In this\nwork, we present a method for material selection in images, robust to lighting\nand reflectance variations, which can be used for downstream editing tasks. We\nrely on vision transformer (ViT) models and leverage their features for\nselection, proposing a multi-resolution processing strategy that yields finer\nand more stable selection results than prior methods. Furthermore, we enable\nselection at two levels: texture and subtexture, leveraging a new two-level\nmaterial selection (DuMaS) dataset which includes dense annotations for over\n800,000 synthetic images, both on the texture and subtexture levels.", "AI": {"tldr": "A method for robust material selection in images using vision transformers (ViT) and multi-resolution processing, with a new dataset (DuMaS) for texture and subtexture-level annotations.", "motivation": "To enable faster and simpler image editing by improving material selection, addressing challenges like lighting and reflectance variations.", "method": "Uses ViT models with multi-resolution processing for finer, stable selections, and introduces the DuMaS dataset for texture and subtexture-level annotations.", "result": "Achieves finer and more stable material selection results compared to prior methods.", "conclusion": "The proposed method and dataset advance material selection for downstream image editing tasks."}}
{"id": "2506.06985", "pdf": "https://arxiv.org/pdf/2506.06985", "abs": "https://arxiv.org/abs/2506.06985", "authors": ["Anastasia Koloskova", "Youssef Allouah", "Animesh Jha", "Rachid Guerraoui", "Sanmi Koyejo"], "title": "Certified Unlearning for Neural Networks", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": null, "summary": "We address the problem of machine unlearning, where the goal is to remove the\ninfluence of specific training data from a model upon request, motivated by\nprivacy concerns and regulatory requirements such as the \"right to be\nforgotten.\" Unfortunately, existing methods rely on restrictive assumptions or\nlack formal guarantees. To this end, we propose a novel method for certified\nmachine unlearning, leveraging the connection between unlearning and privacy\namplification by stochastic post-processing. Our method uses noisy fine-tuning\non the retain data, i.e., data that does not need to be removed, to ensure\nprovable unlearning guarantees. This approach requires no assumptions about the\nunderlying loss function, making it broadly applicable across diverse settings.\nWe analyze the theoretical trade-offs in efficiency and accuracy and\ndemonstrate empirically that our method not only achieves formal unlearning\nguarantees but also performs effectively in practice, outperforming existing\nbaselines. Our code is available at\nhttps://github.com/stair-lab/certified-unlearning-neural-networks-icml-2025", "AI": {"tldr": "The paper proposes a certified method for machine unlearning, ensuring provable guarantees without restrictive assumptions, and outperforms existing baselines.", "motivation": "Addressing privacy concerns and regulatory requirements like the 'right to be forgotten' by removing specific training data's influence from models.", "method": "Noisy fine-tuning on retain data to leverage privacy amplification by stochastic post-processing, requiring no loss function assumptions.", "result": "Achieves formal unlearning guarantees and performs effectively in practice, outperforming existing methods.", "conclusion": "The proposed method is broadly applicable, efficient, and accurate, providing a practical solution for certified machine unlearning."}}
{"id": "2506.06990", "pdf": "https://arxiv.org/pdf/2506.06990", "abs": "https://arxiv.org/abs/2506.06990", "authors": ["Mingyi Li", "Michael R. Metel", "Akiko Takeda"], "title": "Modified K-means Algorithm with Local Optimality Guarantees", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "ICML 2025", "summary": "The K-means algorithm is one of the most widely studied clustering algorithms\nin machine learning. While extensive research has focused on its ability to\nachieve a globally optimal solution, there still lacks a rigorous analysis of\nits local optimality guarantees. In this paper, we first present conditions\nunder which the K-means algorithm converges to a locally optimal solution.\nBased on this, we propose simple modifications to the K-means algorithm which\nensure local optimality in both the continuous and discrete sense, with the\nsame computational complexity as the original K-means algorithm. As the\ndissimilarity measure, we consider a general Bregman divergence, which is an\nextension of the squared Euclidean distance often used in the K-means\nalgorithm. Numerical experiments confirm that the K-means algorithm does not\nalways find a locally optimal solution in practice, while our proposed methods\nprovide improved locally optimal solutions with reduced clustering loss. Our\ncode is available at https://github.com/lmingyi/LO-K-means.", "AI": {"tldr": "The paper analyzes the K-means algorithm's local optimality, proposes modifications to ensure local optimality, and validates improvements through experiments.", "motivation": "There's a lack of rigorous analysis of K-means' local optimality guarantees despite its widespread use.", "method": "The authors present conditions for local optimality and propose modified K-means algorithms using Bregman divergence, maintaining original computational complexity.", "result": "Numerical experiments show the original K-means sometimes fails to achieve local optimality, while the proposed methods improve solutions with reduced clustering loss.", "conclusion": "The modified K-means algorithms ensure local optimality without increasing computational cost, validated by empirical results."}}
{"id": "2506.07088", "pdf": "https://arxiv.org/pdf/2506.07088", "abs": "https://arxiv.org/abs/2506.07088", "authors": ["Ilja Kuzborskij", "Yasin Abbasi Yadkori"], "title": "Pointwise confidence estimation in the non-linear $\\ell^2$-regularized least squares", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We consider a high-probability non-asymptotic confidence estimation in the\n$\\ell^2$-regularized non-linear least-squares setting with fixed design. In\nparticular, we study confidence estimation for local minimizers of the\nregularized training loss. We show a pointwise confidence bound, meaning that\nit holds for the prediction on any given fixed test input $x$. Importantly, the\nproposed confidence bound scales with similarity of the test input to the\ntraining data in the implicit feature space of the predictor (for instance,\nbecoming very large when the test input lies far outside of the training data).\nThis desirable last feature is captured by the weighted norm involving the\ninverse-Hessian matrix of the objective function, which is a generalized\nversion of its counterpart in the linear setting, $x^{\\top} \\text{Cov}^{-1} x$.\nOur generalized result can be regarded as a non-asymptotic counterpart of the\nclassical confidence interval based on asymptotic normality of the MLE\nestimator. We propose an efficient method for computing the weighted norm,\nwhich only mildly exceeds the cost of a gradient computation of the loss\nfunction. Finally, we complement our analysis with empirical evidence showing\nthat the proposed confidence bound provides better coverage/width trade-off\ncompared to a confidence estimation by bootstrapping, which is a gold-standard\nmethod in many applications involving non-linear predictors such as neural\nnetworks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.07254", "pdf": "https://arxiv.org/pdf/2506.07254", "abs": "https://arxiv.org/abs/2506.07254", "authors": ["Kevin Frans", "Sergey Levine", "Pieter Abbeel"], "title": "A Stable Whitening Optimizer for Efficient Neural Network Training", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.", "AI": {"tldr": "The paper proposes SPlus, an improved version of the Shampoo algorithm, addressing divergence, learning rate transfer, and parameter noise issues, achieving faster convergence than Adam.", "motivation": "To address key limitations in the Shampoo optimization algorithm, such as divergence, inefficient learning rate transfer, and parameter noise, leading to more stable and efficient training.", "method": "Introduces SPlus with three improvements: bounded updates for stability, shape-aware scaling for learning rate transfer, and iterate-averaging to reduce parameter noise. Evaluated on Transformer tasks.", "result": "SPlus achieves Adam's validation performance in 44% of gradient steps and 62% of wallclock time across language modeling, image classification, and diffusion modeling tasks.", "conclusion": "SPlus effectively resolves Shampoo's issues, offering faster and more stable optimization, making it a viable alternative to Adam."}}
{"id": "2506.07288", "pdf": "https://arxiv.org/pdf/2506.07288", "abs": "https://arxiv.org/abs/2506.07288", "authors": ["Weijie Guan", "Haohui Wang", "Jian Kang", "Lihui Liu", "Dawei Zhou"], "title": "EVINET: Towards Open-World Graph Learning via Evidential Reasoning Network", "categories": ["cs.LG"], "comment": "KDD 2025", "summary": "Graph learning has been crucial to many real-world tasks, but they are often\nstudied with a closed-world assumption, with all possible labels of data known\na priori. To enable effective graph learning in an open and noisy environment,\nit is critical to inform the model users when the model makes a wrong\nprediction to in-distribution data of a known class, i.e., misclassification\ndetection or when the model encounters out-of-distribution from novel classes,\ni.e., out-of-distribution detection. This paper introduces Evidential Reasoning\nNetwork (EVINET), a framework that addresses these two challenges by\nintegrating Beta embedding within a subjective logic framework. EVINET includes\ntwo key modules: Dissonance Reasoning for misclassification detection and\nVacuity Reasoning for out-of-distribution detection. Extensive experiments\ndemonstrate that EVINET outperforms state-of-the-art methods across multiple\nmetrics in the tasks of in-distribution classification, misclassification\ndetection, and out-of-distribution detection. EVINET demonstrates the necessity\nof uncertainty estimation and logical reasoning for misclassification detection\nand out-of-distribution detection and paves the way for open-world graph\nlearning. Our code and data are available at https://github.com/SSSKJ/EviNET.", "AI": {"tldr": "EVINET is a framework for open-world graph learning, addressing misclassification and out-of-distribution detection using Beta embedding and subjective logic.", "motivation": "Current graph learning assumes a closed-world, but real-world tasks require handling noisy, open environments with unknown classes.", "method": "EVINET integrates Beta embedding and subjective logic, featuring Dissonance Reasoning for misclassification and Vacuity Reasoning for out-of-distribution detection.", "result": "EVINET outperforms state-of-the-art methods in classification, misclassification detection, and out-of-distribution detection.", "conclusion": "EVINET highlights the importance of uncertainty estimation and logical reasoning for robust open-world graph learning."}}
{"id": "2506.07459", "pdf": "https://arxiv.org/pdf/2506.07459", "abs": "https://arxiv.org/abs/2506.07459", "authors": ["Ziwen Wang", "Jiajun Fan", "Ruihan Guo", "Thao Nguyen", "Heng Ji", "Ge Liu"], "title": "ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Protein generative models have shown remarkable promise in protein design but\nstill face limitations in success rate, due to the scarcity of high-quality\nprotein datasets for supervised pretraining. We present ProteinZero, a novel\nframework that enables scalable, automated, and continuous self-improvement of\nthe inverse folding model through online reinforcement learning. To achieve\ncomputationally tractable online feedback, we introduce efficient proxy reward\nmodels based on ESM-fold and a novel rapid ddG predictor that significantly\naccelerates evaluation speed. ProteinZero employs a general RL framework\nbalancing multi-reward maximization, KL-divergence from a reference model, and\na novel protein-embedding level diversity regularization that prevents mode\ncollapse while promoting higher sequence diversity. Through extensive\nexperiments, we demonstrate that ProteinZero substantially outperforms existing\nmethods across every key metric in protein design, achieving significant\nimprovements in structural accuracy, designability, thermodynamic stability,\nand sequence diversity. Most impressively, ProteinZero reduces design failure\nrates by approximately 36% - 48% compared to widely-used methods like\nProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates\nexceeding 90% across diverse and complex protein folds. Notably, the entire RL\nrun on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,\nincluding reward computation. Our work establishes a new paradigm for protein\ndesign where models evolve continuously from their own generated outputs,\nopening new possibilities for exploring the vast protein design space.", "AI": {"tldr": "ProteinZero is a novel framework using online reinforcement learning to improve protein generative models, achieving higher success rates and efficiency.", "motivation": "The scarcity of high-quality protein datasets limits the success rate of protein generative models, prompting the need for scalable self-improvement methods.", "method": "ProteinZero employs online reinforcement learning with efficient proxy reward models (ESM-fold and rapid ddG predictor) and includes multi-reward maximization, KL-divergence, and diversity regularization.", "result": "ProteinZero outperforms existing methods, improving structural accuracy, designability, stability, and diversity, while reducing failure rates by 36%-48% and achieving >90% success rates.", "conclusion": "ProteinZero establishes a continuous self-improvement paradigm for protein design, enabling efficient exploration of the protein design space."}}
{"id": "2506.07584", "pdf": "https://arxiv.org/pdf/2506.07584", "abs": "https://arxiv.org/abs/2506.07584", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "categories": ["cs.LG"], "comment": null, "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "AI": {"tldr": "MIRA is a unified foundation model for medical time series forecasting, addressing challenges like irregular intervals and missing values. It improves forecasting accuracy by 10% (out-of-distribution) and 7% (in-distribution) compared to baselines.", "motivation": "Existing generalist time series models fail to handle medical data due to irregular intervals, heterogeneous sampling, and missing values. MIRA aims to overcome these challenges.", "method": "MIRA uses Continuous-Time Rotary Positional Encoding, a frequency-specific mixture-of-experts layer, and a Neural ODE-based Continuous Dynamics Extrapolation Block.", "result": "MIRA reduces forecasting errors by 10% (out-of-distribution) and 7% (in-distribution) compared to baselines.", "conclusion": "MIRA sets a foundation for future medical time series research, offering improved accuracy and robustness."}}
{"id": "2506.08270", "pdf": "https://arxiv.org/pdf/2506.08270", "abs": "https://arxiv.org/abs/2506.08270", "authors": ["Zitong Huang", "Mansooreh Montazerin", "Ajitesh Srivastava"], "title": "SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space", "categories": ["cs.LG"], "comment": null, "summary": "Designing neural networks typically relies on manual trial and error or a\nneural architecture search (NAS) followed by weight training. The former is\ntime-consuming and labor-intensive, while the latter often discretizes\narchitecture search and weight optimization. In this paper, we propose a\nfundamentally different approach that simultaneously optimizes both the\narchitecture and the weights of a neural network. Our framework first trains a\nuniversal multi-scale autoencoder that embeds both architectural and parametric\ninformation into a continuous latent space, where functionally similar neural\nnetworks are mapped closer together. Given a dataset, we then randomly\ninitialize a point in the embedding space and update it via gradient descent to\nobtain the optimal neural network, jointly optimizing its structure and\nweights. The optimization process incorporates sparsity and compactness\npenalties to promote efficient models. Experiments on synthetic regression\ntasks demonstrate that our method effectively discovers sparse and compact\nneural networks with strong performance.", "AI": {"tldr": "A novel method jointly optimizes neural network architecture and weights using a continuous latent space, outperforming manual design and NAS.", "motivation": "Manual neural network design is labor-intensive, while NAS often separates architecture search and weight training. This work aims to unify both processes.", "method": "Trains a universal multi-scale autoencoder to embed architectures and weights into a continuous latent space. Gradient descent optimizes a randomly initialized point in this space, incorporating sparsity and compactness penalties.", "result": "Effective discovery of sparse, compact neural networks with strong performance on synthetic regression tasks.", "conclusion": "The proposed framework successfully unifies architecture and weight optimization, offering a scalable and efficient alternative to traditional methods."}}
{"id": "2506.08473", "pdf": "https://arxiv.org/pdf/2506.08473", "abs": "https://arxiv.org/abs/2506.08473", "authors": ["Shuo Yang", "Qihui Zhang", "Yuyang Liu", "Yue Huang", "Xiaojun Jia", "Kunpeng Ning", "Jiayu Yao", "Jigang Wang", "Hailiang Dai", "Yibing Song", "Li Yuan"], "title": "AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) are vulnerable to safety risks during\nfine-tuning, where small amounts of malicious or harmless data can compromise\nsafeguards. In this paper, building on the concept of alignment direction --\ndefined by the weight difference between aligned and unaligned models -- we\nobserve that perturbations along this direction preserve model safety. In\ncontrast, perturbations along directions orthogonal to this alignment are\nstrongly linked to harmful direction perturbations, rapidly degrading safety\nand framing the parameter space as a narrow safety basin. Based on this\ninsight, we propose a methodology for safety fine-tuning called AsFT (Anchoring\nSafety in Fine-Tuning), which integrates a regularization term into the\ntraining objective. This term uses the alignment direction as an anchor to\nsuppress updates in harmful directions, ensuring that fine-tuning is\nconstrained within the narrow safety basin. Extensive experiments on multiple\ndatasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by\n7.60 percent, improving model performance by 3.44 percent, and maintaining\nrobust performance across various experimental settings. Code is available at\nhttps://github.com/PKU-YuanGroup/AsFT", "AI": {"tldr": "AsFT (Anchoring Safety in Fine-Tuning) is a method to enhance LLM safety during fine-tuning by using alignment direction as an anchor to suppress harmful updates, reducing harmful behavior by 7.60% and improving performance by 3.44%.", "motivation": "LLMs are vulnerable to safety risks during fine-tuning, where even small malicious or harmless data can compromise safeguards.", "method": "AsFT integrates a regularization term into the training objective, using alignment direction to suppress harmful updates and constrain fine-tuning within a safety basin.", "result": "AsFT reduces harmful behavior by 7.60% and improves model performance by 3.44%, outperforming Safe LoRA.", "conclusion": "AsFT effectively ensures safety during fine-tuning while maintaining robust performance, framing the parameter space as a narrow safety basin."}}
{"id": "2506.08551", "pdf": "https://arxiv.org/pdf/2506.08551", "abs": "https://arxiv.org/abs/2506.08551", "authors": ["Panlong Wu", "Ting Wang", "Yifei Zhong", "Haoqi Zhang", "Zitong Wang", "Fangxin Wang"], "title": "DeepForm: Reasoning Large Language Model for Communication System Formulation", "categories": ["cs.LG"], "comment": null, "summary": "Communication system formulation is critical for advancing 6G and future\nwireless technologies, yet it remains a complex, expertise-intensive task.\nWhile Large Language Models (LLMs) offer potential, existing general-purpose\nmodels often lack the specialized domain knowledge, nuanced reasoning\ncapabilities, and access to high-quality, domain-specific training data\nrequired for adapting a general LLM into an LLM specially for communication\nsystem formulation. To bridge this gap, we introduce DeepForm, the first\nreasoning LLM specially for automated communication system formulation. We\npropose the world-first large-scale, open-source dataset meticulously curated\nfor this domain called Communication System Formulation Reasoning Corpus\n(CSFRC). Our framework employs a two-stage training strategy: first, Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;\nsecond, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based\non ReMax, to cultivate advanced modeling capabilities and elicit sophisticated\nreasoning patterns like self-correction and verification. Extensive experiments\ndemonstrate that our model achieves state-of-the-art performance, significantly\noutperforming larger proprietary LLMs on diverse senerios. We will release\nrelated resources to foster further research in this area after the paper is\naccepted.", "AI": {"tldr": "DeepForm is a specialized LLM for automated communication system formulation, trained with a novel dataset (CSFRC) and a two-stage method (SFT + C-ReMax RL), outperforming general LLMs.", "motivation": "Existing general-purpose LLMs lack domain-specific knowledge and reasoning for communication system formulation, necessitating a specialized solution.", "method": "Two-stage training: SFT with CoT data for domain knowledge, followed by rule-based RL (C-ReMax) for advanced reasoning.", "result": "DeepForm achieves state-of-the-art performance, surpassing larger proprietary LLMs in diverse scenarios.", "conclusion": "DeepForm bridges the gap in specialized LLMs for communication systems, with plans to release resources for further research."}}
{"id": "2506.08574", "pdf": "https://arxiv.org/pdf/2506.08574", "abs": "https://arxiv.org/abs/2506.08574", "authors": ["Alvise Dei Rossi", "Matteo Metaldi", "Michal Bechny", "Irina Filchenko", "Julia van der Meer", "Markus H. Schmidt", "Claudio L. A. Bassetti", "Athina Tzovara", "Francesca D. Faraci", "Luigi Fiorillo"], "title": "SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models", "categories": ["cs.LG"], "comment": "41 pages, 4 Figures, 7 Tables", "summary": "Despite advances in deep learning for automatic sleep staging, clinical\nadoption remains limited due to challenges in fair model evaluation,\ngeneralization across diverse datasets, model bias, and variability in human\nannotations. We present SLEEPYLAND, an open-source sleep staging evaluation\nframework designed to address these barriers. It includes more than 220'000\nhours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain\n(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,\nand hardware setups. We release pre-trained models based on high-performing SoA\narchitectures and evaluate them under standardized conditions across single-\nand multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble\ncombining models across architectures and channel setups via soft voting.\nSOMNUS achieves robust performance across twenty-four different datasets, with\nmacro-F1 scores between 68.7% and 87.2%, outperforming individual models in\n94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including\ncases where compared models were trained ID while SOMNUS treated the same data\nas OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked\nto age, gender, AHI, and PLMI, showing that while ensemble improves robustness,\nno model architecture consistently minimizes bias in performance and clinical\nmarkers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,\nDOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on\nDOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus\nthan any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA\ncohorts). Finally, we introduce ensemble disagreement metrics - entropy and\ninter-model divergence based - predicting regions of scorer disagreement with\nROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.", "AI": {"tldr": "SLEEPYLAND is an open-source framework for sleep staging evaluation, addressing challenges like model bias and dataset variability. It includes extensive datasets and introduces SOMNUS, an ensemble model outperforming individual models and human scorers.", "motivation": "Clinical adoption of deep learning for sleep staging is limited by evaluation fairness, generalization issues, and human annotation variability.", "method": "The framework includes large ID and OOD datasets, pre-trained models, and SOMNUS, an ensemble model combining architectures via soft voting.", "result": "SOMNUS achieves robust performance (68.7%-87.2% macro-F1), outperforms individual models (94.9% cases), and surpasses human scorers in consensus. It also identifies model biases.", "conclusion": "SLEEPYLAND and SOMNUS improve sleep staging evaluation, but no architecture consistently minimizes bias. Ensemble disagreement metrics predict human uncertainty."}}
{"id": "2506.08681", "pdf": "https://arxiv.org/pdf/2506.08681", "abs": "https://arxiv.org/abs/2506.08681", "authors": ["Phuc Minh Nguyen", "Ngoc-Hieu Nguyen", "Duy H. M. Nguyen", "Anji Liu", "An Mai", "Binh T. Nguyen", "Daniel Sonntag", "Khoa D. Doan"], "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling", "categories": ["cs.LG"], "comment": "First version", "summary": "Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization\n(DPO) have emerged as alternatives to the standard Reinforcement Learning from\nHuman Feedback (RLHF) for aligning large language models (LLMs) with human\nvalues. However, these methods are more susceptible to over-optimization, in\nwhich the model drifts away from the reference policy, leading to degraded\nperformance as training progresses. This paper proposes a novel\nimportance-sampling approach to mitigate the over-optimization problem of\noffline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective\nwith an importance ratio that accounts for the reference policy distribution.\nIS-DAAs additionally avoid the high variance issue associated with importance\nsampling by clipping the importance ratio to a maximum value. Our extensive\nexperiments demonstrate that IS-DAAs can effectively mitigate\nover-optimization, especially under low regularization strength, and achieve\nbetter performance than other methods designed to address this problem. Our\nimplementations are provided publicly at this link.", "AI": {"tldr": "IS-DAAs, an importance-sampling approach, mitigates over-optimization in Direct Alignment Algorithms (DAAs) like DPO, outperforming other methods.", "motivation": "DAAs like DPO are prone to over-optimization, causing model drift and degraded performance.", "method": "Proposes IS-DAAs, which uses a clipped importance ratio to account for the reference policy distribution.", "result": "IS-DAAs effectively reduce over-optimization, especially with low regularization, and outperform alternatives.", "conclusion": "IS-DAAs offer a robust solution to over-optimization in DAAs, with public implementation available."}}
{"id": "2506.08837", "pdf": "https://arxiv.org/pdf/2506.08837", "abs": "https://arxiv.org/abs/2506.08837", "authors": ["Luca Beurer-Kellner", "Beat Buesser Ana-Maria Cre\u0163u", "Edoardo Debenedetti", "Daniel Dobos", "Daniel Fabian", "Marc Fischer", "David Froelicher", "Kathrin Grosse", "Daniel Naeff", "Ezinwanne Ozoani", "Andrew Paverd", "Florian Tram\u00e8r", "V\u00e1clav Volhejn"], "title": "Design Patterns for Securing LLM Agents against Prompt Injections", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "As AI agents powered by Large Language Models (LLMs) become increasingly\nversatile and capable of addressing a broad spectrum of tasks, ensuring their\nsecurity has become a critical challenge. Among the most pressing threats are\nprompt injection attacks, which exploit the agent's resilience on natural\nlanguage inputs -- an especially dangerous threat when agents are granted tool\naccess or handle sensitive information. In this work, we propose a set of\nprincipled design patterns for building AI agents with provable resistance to\nprompt injection. We systematically analyze these patterns, discuss their\ntrade-offs in terms of utility and security, and illustrate their real-world\napplicability through a series of case studies.", "AI": {"tldr": "Proposes design patterns for AI agents to resist prompt injection attacks, balancing utility and security.", "motivation": "Address the critical security challenge of prompt injection attacks in AI agents powered by LLMs, especially when handling sensitive data or tool access.", "method": "Introduces principled design patterns for AI agents, analyzes their trade-offs, and validates through case studies.", "result": "Demonstrates real-world applicability and effectiveness of the proposed patterns in resisting prompt injection.", "conclusion": "The design patterns offer a practical solution to enhance the security of AI agents against prompt injection attacks."}}
{"id": "2506.08982", "pdf": "https://arxiv.org/pdf/2506.08982", "abs": "https://arxiv.org/abs/2506.08982", "authors": ["Ivan Rubachev", "Akim Kotelnikov", "Nikolay Kartashev", "Artem Babenko"], "title": "On Finetuning Tabular Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Foundation models are an emerging research direction in tabular deep\nlearning. Notably, TabPFNv2 recently claimed superior performance over\ntraditional GBDT-based methods on small-scale datasets using an in-context\nlearning paradigm, which does not adapt model parameters to target datasets.\nHowever, the optimal finetuning approach for adapting tabular foundational\nmodels, and how this adaptation reshapes their internal mechanisms, remains\nunderexplored. While prior works studied finetuning for earlier foundational\nmodels, inconsistent findings and TabPFNv2's unique architecture necessitate\nfresh investigation. To address these questions, we first systematically\nevaluate various finetuning strategies on diverse datasets. Our findings\nestablish full finetuning as the most practical solution for TabPFNv2 in terms\nof time-efficiency and effectiveness. We then investigate how finetuning alters\nTabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.\nWe reveal that the success of finetuning stems from the fact that after\ngradient-based adaptation, the dot products of the query-representations of\ntest objects and the key-representations of in-context training objects more\naccurately reflect their target similarity. This improved similarity allows\nfinetuned TabPFNv2 to better approximate target dependency by appropriately\nweighting relevant in-context samples, improving the retrieval-based prediction\nlogic. From the practical perspective, we managed to finetune TabPFNv2 on\ndatasets with up to 50K objects, observing performance improvements on almost\nall tasks. More precisely, on academic datasets with I.I.D. splits, finetuning\nallows TabPFNv2 to achieve state-of-the-art results, while on datasets with\ngradual temporal shifts and rich feature sets, TabPFNv2 is less stable and\nprior methods remain better.", "AI": {"tldr": "The paper evaluates finetuning strategies for TabPFNv2, a tabular foundation model, finding full finetuning most effective. It explores how finetuning improves similarity measures and performance, achieving state-of-the-art results on some datasets.", "motivation": "To determine the optimal finetuning approach for TabPFNv2 and understand how it reshapes the model's internal mechanisms, given inconsistent prior findings and the model's unique architecture.", "method": "Systematic evaluation of finetuning strategies on diverse datasets, analyzing changes in TabPFNv2's internal mechanisms, particularly similarity measures.", "result": "Full finetuning is most practical for TabPFNv2, improving performance on datasets up to 50K objects. It achieves state-of-the-art results on academic datasets but is less stable on temporally shifted or feature-rich datasets.", "conclusion": "Finetuning enhances TabPFNv2's performance by refining similarity measures, making it competitive in certain scenarios, though traditional methods remain better for some cases."}}
{"id": "2207.14438", "pdf": "https://arxiv.org/pdf/2207.14438", "abs": "https://arxiv.org/abs/2207.14438", "authors": ["Angus Lowe", "Ashwin Nayak"], "title": "Lower Bounds for Learning Quantum States with Single-Copy Measurements", "categories": ["quant-ph", "cs.CC", "cs.IT", "cs.LG", "math.IT"], "comment": "v3. Minor typos fixed and references to subsequent work added. Most\n  of the results in this article were included in the first author's Master's\n  thesis at U. Waterloo (Oct. 2021) and were presented at QIP 2022 (Mar. 2022)", "summary": "We study the problems of quantum tomography and shadow tomography using\nmeasurements performed on individual, identical copies of an unknown\n$d$-dimensional state. We first revisit a known lower bound due to Haah et al.\n(2017) on quantum tomography with accuracy $\\epsilon$ in trace distance, when\nthe measurements choices are independent of previously observed outcomes (i.e.,\nthey are nonadaptive). We give a succinct proof of this result. This leads to\nstronger lower bounds when the learner uses measurements with a constant number\nof outcomes. In particular, this rigorously establishes the optimality of the\nfolklore ``Pauli tomography\" algorithm in terms of its sample complexity. We\nalso derive novel bounds of $\\Omega(r^2 d/\\epsilon^2)$ and $\\Omega(r^2\nd^2/\\epsilon^2)$ for learning rank $r$ states using arbitrary and\nconstant-outcome measurements, respectively, in the nonadaptive case.\n  In addition to the sample complexity, a resource of practical significance\nfor learning quantum states is the number of different measurements used by an\nalgorithm. We extend our lower bounds to the case where the learner performs\npossibly adaptive measurements from a fixed set of $\\exp(O(d))$ measurements.\nThis implies in particular that adaptivity does not give us any advantage using\nsingle-copy measurements that are efficiently implementable. We also obtain a\nsimilar bound in the case where the goal is to predict the expectation values\nof a given sequence of observables, a task known as shadow tomography. Finally,\nin the case of adaptive, single-copy measurements implementable with\npolynomial-size circuits, we prove that a straightforward strategy based on\ncomputing sample means of the given observables is optimal.", "AI": {"tldr": "The paper revisits and strengthens lower bounds for quantum tomography and shadow tomography, proving optimality of known algorithms and deriving new bounds for rank-r states. It also explores the impact of adaptivity and measurement constraints.", "motivation": "To rigorously establish the optimality of existing quantum tomography algorithms and derive new bounds for learning quantum states, while investigating the role of adaptivity and practical measurement constraints.", "method": "The study revisits known lower bounds, provides succinct proofs, and extends results to adaptive and nonadaptive measurements, including fixed measurement sets and polynomial-size circuits.", "result": "Stronger lower bounds for tomography and shadow tomography are derived, proving optimality of Pauli tomography and showing adaptivity offers no advantage with efficiently implementable measurements.", "conclusion": "The paper provides rigorous bounds for quantum state learning, highlighting limitations of adaptivity and practical measurement constraints, while confirming optimality of straightforward strategies."}}
{"id": "2209.06175", "pdf": "https://arxiv.org/pdf/2209.06175", "abs": "https://arxiv.org/abs/2209.06175", "authors": ["Ngoc Hoang Anh Mai", "Victor Magron", "Jean-Bernard Lasserre", "Kim-Chuan Toh"], "title": "Tractable hierarchies of convex relaxations for polynomial optimization on the nonnegative orthant", "categories": ["math.OC", "cs.LG", "math.AG"], "comment": "37 pages, 15 tables", "summary": "We consider polynomial optimization problems (POP) on a semialgebraic set\ncontained in the nonnegative orthant (every POP on a compact set can be put in\nthis format by a simple translation of the origin). Such a POP can be converted\nto an equivalent POP by squaring each variable. Using even symmetry and the\nconcept of factor width, we propose a hierarchy of semidefinite relaxations\nbased on the extension of P\\'olya's Positivstellensatz by Dickinson-Povh. As\nits distinguishing and crucial feature, the maximal matrix size of each\nresulting semidefinite relaxation can be chosen arbitrarily and in addition, we\nprove that the sequence of values returned by the new hierarchy converges to\nthe optimal value of the original POP at the rate $O(\\varepsilon^{-c})$ if the\nsemialgebraic set has nonempty interior. When applied to (i) robustness\ncertification of multi-layer neural networks and (ii) computation of positive\nmaximal singular values, our method based on P\\'olya's Positivstellensatz\nprovides better bounds and runs several hundred times faster than the standard\nMoment-SOS hierarchy.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2307.14530", "pdf": "https://arxiv.org/pdf/2307.14530", "abs": "https://arxiv.org/abs/2307.14530", "authors": ["Fedor Noskov", "Maxim Panov"], "title": "Optimal Noise Reduction in Dense Mixed-Membership Stochastic Block Models under Diverging Spiked Eigenvalues Condition", "categories": ["stat.ML", "cs.LG", "cs.SI"], "comment": null, "summary": "Community detection is one of the most critical problems in modern network\nscience. Its applications can be found in various fields, from protein modeling\nto social network analysis. Recently, many papers appeared studying the problem\nof overlapping community detection, where each node of a network may belong to\nseveral communities. In this work, we consider Mixed-Membership Stochastic\nBlock Model (MMSB) first proposed by Airoldi et al. MMSB provides quite a\ngeneral setting for modeling overlapping community structure in graphs. The\ncentral question of this paper is to reconstruct relations between communities\ngiven an observed network. We compare different approaches and establish the\nminimax lower bound on the estimation error. Then, we propose a new estimator\nthat matches this lower bound. Theoretical results are proved under fairly\ngeneral conditions on the considered model. Finally, we illustrate the theory\nin a series of experiments.", "AI": {"tldr": "The paper focuses on overlapping community detection using the Mixed-Membership Stochastic Block Model (MMSB), proposing a new estimator that matches the minimax lower bound for estimation error.", "motivation": "Community detection is crucial in network science, especially for overlapping communities where nodes belong to multiple groups. The paper aims to reconstruct community relations from observed networks.", "method": "The study compares approaches, establishes minimax lower bounds, and introduces a new estimator under general model conditions.", "result": "The proposed estimator achieves the minimax lower bound, validated by theoretical proofs and experimental results.", "conclusion": "The work advances overlapping community detection by providing a theoretically sound and practical estimator under MMSB."}}
{"id": "2308.01054", "pdf": "https://arxiv.org/pdf/2308.01054", "abs": "https://arxiv.org/abs/2308.01054", "authors": ["Simon Dirmeier", "Carlo Albert", "Fernando Perez-Cruz"], "title": "Simulation-based Inference for High-dimensional Data using Surjective Sequential Neural Likelihood Estimation", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Neural likelihood estimation methods for simulation-based inference can\nsuffer from performance degradation when the modeled data is very\nhigh-dimensional or lies along a lower-dimensional manifold, which is due to\nthe inability of the density estimator to accurately estimate a density\nfunction. We present Surjective Sequential Neural Likelihood (SSNL) estimation,\na novel member in the family of methods for simulation-based inference (SBI).\nSSNL fits a dimensionality-reducing surjective normalizing flow model and uses\nit as a surrogate likelihood function, which allows for computational inference\nvia Markov chain Monte Carlo or variational Bayes methods. Among other\nbenefits, SSNL avoids the requirement to manually craft summary statistics for\ninference of high-dimensional data sets, since the lower-dimensional\nrepresentation is computed simultaneously with learning the likelihood and\nwithout additional computational overhead. We evaluate SSNL on a wide variety\nof experiments, including two challenging real-world examples from the\nastrophysics and neuroscience literatures, and show that it either outperforms\nor is on par with state-of-the-art methods, making it an excellent\noff-the-shelf estimator for SBI for high-dimensional data sets.", "AI": {"tldr": "SSNL is a new method for simulation-based inference that addresses performance issues in high-dimensional data by using a dimensionality-reducing surjective normalizing flow model.", "motivation": "Existing neural likelihood estimation methods struggle with high-dimensional or manifold data due to inaccurate density estimation.", "method": "SSNL employs a surjective normalizing flow model to reduce dimensionality and serve as a surrogate likelihood, enabling inference via MCMC or variational Bayes.", "result": "SSNL performs competitively or better than state-of-the-art methods in various experiments, including real-world astrophysics and neuroscience datasets.", "conclusion": "SSNL is a robust, off-the-shelf solution for high-dimensional simulation-based inference, eliminating the need for manual summary statistics."}}
{"id": "2405.02437", "pdf": "https://arxiv.org/pdf/2405.02437", "abs": "https://arxiv.org/abs/2405.02437", "authors": ["Abdulrahman Diaa", "Thomas Humphries", "Florian Kerschbaum"], "title": "FastLloyd: Federated, Accurate, Secure, and Tunable $k$-Means Clustering with Differential Privacy", "categories": ["cs.CR", "cs.LG"], "comment": "In 34th Usenix Security Symposium", "summary": "We study the problem of privacy-preserving $k$-means clustering in the\nhorizontally federated setting. Existing federated approaches using secure\ncomputation suffer from substantial overheads and do not offer output privacy.\nAt the same time, differentially private (DP) $k$-means algorithms either\nassume a trusted central curator or significantly degrade utility by adding\nnoise in the local DP model. Naively combining the secure and central DP\nsolutions results in a protocol with impractical overhead. Instead, our work\nprovides enhancements to both the DP and secure computation components,\nresulting in a design that is faster, more private, and more accurate than\nprevious work. By utilizing the computational DP model, we design a\nlightweight, secure aggregation-based approach that achieves five orders of\nmagnitude speed-up over state-of-the-art related work. Furthermore, we not only\nmaintain the utility of the state-of-the-art in the central model of DP, but we\nimprove the utility further by designing a new DP clustering mechanism.", "AI": {"tldr": "The paper proposes a privacy-preserving $k$-means clustering method in federated settings, improving speed, privacy, and accuracy over existing approaches by combining computational DP and secure computation enhancements.", "motivation": "Existing federated $k$-means methods suffer from high overheads, lack output privacy, or degrade utility under local DP. The paper aims to address these limitations.", "method": "The approach combines computational DP with secure computation, using lightweight secure aggregation and a new DP clustering mechanism.", "result": "The method achieves a 100,000x speed-up over state-of-the-art, maintains central DP utility, and further improves accuracy.", "conclusion": "The proposed solution outperforms prior work in speed, privacy, and utility, making it practical for federated $k$-means clustering."}}
{"id": "2407.06100", "pdf": "https://arxiv.org/pdf/2407.06100", "abs": "https://arxiv.org/abs/2407.06100", "authors": ["Syed Zahid Husain", "Leo Separovic", "Jean-Fran\u00e7ois Caron", "Rabah Aider", "Mark Buehner", "St\u00e9phane Chamberland", "Ervig Lapalme", "Ron McTaggart-Cowan", "Christopher Subich", "Paul A. Vaillancourt", "Jing Yang", "Ayrton Zadra"], "title": "Leveraging data-driven weather models for improving numerical weather prediction skill through large-scale spectral nudging", "categories": ["physics.ao-ph", "cs.LG"], "comment": null, "summary": "Operational meteorological forecasting has long relied on physics-based\nnumerical weather prediction (NWP) models. Recently, this landscape has faced\ndisruption by the advent of data-driven artificial intelligence (AI)-based\nweather models, which offer tremendous computational performance and\ncompetitive forecasting accuracy. However, data-driven models for medium-range\nforecasting generally suffer from major limitations, including low effective\nresolution and a narrow range of predicted variables. This study illustrates\nthe relative strengths and weaknesses of these competing paradigms using the\nphysics-based GEM (Global Environmental Multiscale) and the AI-based GraphCast\nmodels. Analyses of their respective global predictions in physical and\nspectral space reveal that GraphCast-predicted large scales outperform GEM,\nparticularly for longer lead times, even though fine scales predicted by\nGraphCast suffer from excessive smoothing. Building on this insight, a hybrid\nNWP-AI system is proposed, wherein temperature and horizontal wind components\npredicted by GEM are spectrally nudged toward GraphCast predictions at large\nscales, while GEM itself freely generates the fine-scale details critical for\nlocal predictability and weather extremes. This hybrid approach is capable of\nleveraging the strengths of GraphCast to enhance the prediction skill of the\nGEM model while generating a full suite of physically consistent forecast\nfields with a full power spectrum. Additionally, trajectories of tropical\ncyclones are predicted with enhanced accuracy without significant changes in\nintensity. Work is in progress for operationalization of this hybrid system at\nthe Canadian Meteorological Centre.", "AI": {"tldr": "A hybrid NWP-AI system combines the strengths of physics-based GEM and AI-based GraphCast models for improved medium-range weather forecasting.", "motivation": "To address the limitations of data-driven AI models (low resolution, narrow variable range) and leverage their computational performance and accuracy for large-scale predictions.", "method": "Compare GEM and GraphCast models, then propose a hybrid system where GEM's fine-scale details are combined with GraphCast's large-scale predictions via spectral nudging.", "result": "The hybrid system enhances prediction skill, maintains physical consistency, and improves tropical cyclone trajectory accuracy.", "conclusion": "The hybrid approach effectively combines the strengths of both paradigms, with ongoing efforts for operational use at the Canadian Meteorological Centre."}}
{"id": "2407.13625", "pdf": "https://arxiv.org/pdf/2407.13625", "abs": "https://arxiv.org/abs/2407.13625", "authors": ["Aras Selvi", "Eleonora Kreacic", "Mohsen Ghassemi", "Vamsi Potluru", "Tucker Balch", "Manuela Veloso"], "title": "Distributionally and Adversarially Robust Logistic Regression via Intersecting Wasserstein Balls", "categories": ["math.OC", "cs.LG"], "comment": "9 main pages + 25 pages of appendices", "summary": "Adversarially robust optimization (ARO) has emerged as the *de facto*\nstandard for training models that hedge against adversarial attacks in the test\nstage. While these models are robust against adversarial attacks, they tend to\nsuffer severely from overfitting. To address this issue, some successful\nmethods replace the empirical distribution in the training stage with\nalternatives including *(i)* a worst-case distribution residing in an ambiguity\nset, resulting in a distributionally robust (DR) counterpart of ARO; *(ii)* a\nmixture of the empirical distribution with a distribution induced by an\nauxiliary (*e.g.*, synthetic, external, out-of-domain) dataset. Inspired by the\nformer, we study the Wasserstein DR counterpart of ARO for logistic regression\nand show it admits a tractable convex optimization reformulation. Adopting the\nlatter setting, we revise the DR approach by intersecting its ambiguity set\nwith another ambiguity set built using the auxiliary dataset, which offers a\nsignificant improvement whenever the Wasserstein distance between the data\ngenerating and auxiliary distributions can be estimated. We study the\nunderlying optimization problem, develop efficient solution algorithms, and\ndemonstrate that the proposed method outperforms benchmark approaches on\nstandard datasets.", "AI": {"tldr": "The paper addresses overfitting in adversarially robust optimization (ARO) by proposing a Wasserstein distributionally robust (DR) counterpart for logistic regression and enhancing it with auxiliary data.", "motivation": "ARO models are robust against adversarial attacks but suffer from overfitting. The paper aims to mitigate this by leveraging distributionally robust optimization and auxiliary datasets.", "method": "The study introduces a Wasserstein DR counterpart for ARO in logistic regression, reformulates it as a convex optimization problem, and enhances it by intersecting ambiguity sets with auxiliary data.", "result": "The proposed method outperforms benchmarks on standard datasets, demonstrating improved robustness and reduced overfitting.", "conclusion": "The Wasserstein DR approach, combined with auxiliary data, effectively mitigates overfitting in ARO while maintaining adversarial robustness."}}
{"id": "2408.03199", "pdf": "https://arxiv.org/pdf/2408.03199", "abs": "https://arxiv.org/abs/2408.03199", "authors": ["Matteo Lapucci", "Davide Pucci"], "title": "Convergence Conditions for Stochastic Line Search Based Optimization of Over-parametrized Models", "categories": ["math.OC", "cs.LG", "90C30, 90C26, 90C06, 65K05, 68T07"], "comment": null, "summary": "In this paper, we deal with algorithms to solve the finite-sum problems\nrelated to fitting over-parametrized models, that typically satisfy the\ninterpolation condition. In particular, we focus on approaches based on\nstochastic line searches and employing general search directions. We define\nconditions on the sequence of search directions that guarantee finite\ntermination and bounds for the backtracking procedure. Moreover, we shed light\non the additional property of directions needed to prove fast (linear)\nconvergence of the general class of algorithms when applied to PL functions in\nthe interpolation regime. From the point of view of algorithms design, the\nproposed analysis identifies safeguarding conditions that could be employed in\nrelevant algorithmic frameworks. In particular, it could be of interest to\nintegrate stochastic line searches within momentum, conjugate gradient or\nadaptive preconditioning methods.", "AI": {"tldr": "The paper proposes algorithms for solving finite-sum problems in over-parametrized models, focusing on stochastic line searches and general search directions, with conditions for finite termination and fast convergence.", "motivation": "To address the challenges of fitting over-parametrized models and ensure efficient convergence in stochastic line search methods.", "method": "Uses stochastic line searches with general search directions, defining conditions for termination and backtracking bounds, and analyzes properties for fast convergence.", "result": "Identifies conditions for finite termination and linear convergence in PL functions, suggesting safeguarding conditions for algorithmic frameworks.", "conclusion": "The analysis provides insights for integrating stochastic line searches into momentum, conjugate gradient, or adaptive preconditioning methods."}}
{"id": "2408.07588", "pdf": "https://arxiv.org/pdf/2408.07588", "abs": "https://arxiv.org/abs/2408.07588", "authors": ["Guiomar Pescador-Barrios", "Sarah Filippi", "Mark van der Wilk"], "title": "Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?", "categories": ["stat.ML", "cs.LG"], "comment": "9 pages main, 27 pages total, 13 figures, 9 tables, conference paper,\n  minor correction: updated author name in PDF", "summary": "Many machine learning models require setting a parameter that controls their\nsize before training, e.g. number of neurons in DNNs, or inducing points in\nGPs. Increasing capacity typically improves performance until all the\ninformation from the dataset is captured. After this point, computational cost\nkeeps increasing, without improved performance. This leads to the question \"How\nbig is big enough?\" We investigate this problem for Gaussian processes\n(single-layer neural networks) in continual learning. Here, data becomes\navailable incrementally, and the final dataset size will therefore not be known\nbefore training, preventing the use of heuristics for setting a fixed model\nsize. We develop a method to automatically adjust model size while maintaining\nnear-optimal performance. Our experimental procedure follows the constraint\nthat any hyperparameters must be set without seeing dataset properties, and we\nshow that our method performs well across diverse datasets without the need to\nadjust its hyperparameter, showing it requires less tuning than others.", "AI": {"tldr": "The paper addresses the challenge of determining the optimal model size in continual learning for Gaussian processes, proposing an automatic adjustment method to maintain performance without prior knowledge of dataset size.", "motivation": "The need to set model size parameters before training, without knowing the final dataset size in continual learning, leads to inefficiencies. The paper aims to solve this by dynamically adjusting model size.", "method": "Develops an automatic method to adjust model size in Gaussian processes during continual learning, ensuring near-optimal performance without requiring dataset-specific hyperparameter tuning.", "result": "The method performs well across diverse datasets without needing hyperparameter adjustments, demonstrating less tuning than other approaches.", "conclusion": "The proposed method effectively addresses the challenge of model size in continual learning, offering a practical solution with minimal tuning requirements."}}
{"id": "2410.05757", "pdf": "https://arxiv.org/pdf/2410.05757", "abs": "https://arxiv.org/abs/2410.05757", "authors": ["Kenyon Ng", "Chris van der Heide", "Liam Hodgkinson", "Susan Wei"], "title": "Temperature Optimization for Bayesian Deep Learning", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "comment": "11 pages (+5 reference, +17 appendix). Accepted at UAI 2025", "summary": "The Cold Posterior Effect (CPE) is a phenomenon in Bayesian Deep Learning\n(BDL), where tempering the posterior to a cold temperature often improves the\npredictive performance of the posterior predictive distribution (PPD). Although\nthe term `CPE' suggests colder temperatures are inherently better, the BDL\ncommunity increasingly recognizes that this is not always the case. Despite\nthis, there remains no systematic method for finding the optimal temperature\nbeyond grid search. In this work, we propose a data-driven approach to select\nthe temperature that maximizes test log-predictive density, treating the\ntemperature as a model parameter and estimating it directly from the data. We\nempirically demonstrate that our method performs comparably to grid search, at\na fraction of the cost, across both regression and classification tasks.\nFinally, we highlight the differing perspectives on CPE between the BDL and\nGeneralized Bayes communities: while the former primarily emphasizes the\npredictive performance of the PPD, the latter prioritizes the utility of the\nposterior under model misspecification; these distinct objectives lead to\ndifferent temperature preferences.", "AI": {"tldr": "The paper proposes a data-driven method to select the optimal temperature for Bayesian Deep Learning, avoiding costly grid search, and compares perspectives on the Cold Posterior Effect between BDL and Generalized Bayes communities.", "motivation": "The Cold Posterior Effect (CPE) in Bayesian Deep Learning lacks a systematic method for temperature selection beyond grid search, despite its impact on predictive performance.", "method": "A data-driven approach treats temperature as a model parameter, estimating it directly from data to maximize test log-predictive density.", "result": "The method matches grid search performance at lower cost across regression and classification tasks.", "conclusion": "The paper highlights differing temperature preferences between BDL (predictive performance) and Generalized Bayes (utility under misspecification) communities."}}
{"id": "2410.21702", "pdf": "https://arxiv.org/pdf/2410.21702", "abs": "https://arxiv.org/abs/2410.21702", "authors": ["Pierre Alquier", "William Kengne"], "title": "Minimax optimality of deep neural networks on dependent data via PAC-Bayes bounds", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In a groundbreaking work, Schmidt-Hieber (2020) proved the minimax optimality\nof deep neural networks with ReLu activation for least-square regression\nestimation over a large class of functions defined by composition. In this\npaper, we extend these results in many directions. First, we remove the i.i.d.\nassumption on the observations, to allow some time dependence. The observations\nare assumed to be a Markov chain with a non-null pseudo-spectral gap. Then, we\nstudy a more general class of machine learning problems, which includes\nleast-square and logistic regression as special cases. Leveraging on PAC-Bayes\noracle inequalities and a version of Bernstein inequality due to Paulin (2015),\nwe derive upper bounds on the estimation risk for a generalized Bayesian\nestimator. In the case of least-square regression, this bound matches (up to a\nlogarithmic factor) the lower bound of Schmidt-Hieber (2020). We establish a\nsimilar lower bound for classification with the logistic loss, and prove that\nthe proposed DNN estimator is optimal in the minimax sense.", "AI": {"tldr": "The paper extends Schmidt-Hieber (2020)'s results on deep neural networks (DNNs) with ReLU activation, removing the i.i.d. assumption and addressing time-dependent data (Markov chains). It generalizes to broader machine learning problems, including least-square and logistic regression, and provides minimax optimality proofs.", "motivation": "To generalize and extend the minimax optimality results of DNNs beyond i.i.d. settings and least-square regression, addressing more practical scenarios like time-dependent data and classification tasks.", "method": "Uses PAC-Bayes oracle inequalities and Bernstein inequality (Paulin, 2015) to derive upper bounds for a generalized Bayesian estimator. Analyzes DNNs under Markov chain observations and non-i.i.d. settings.", "result": "Upper bounds on estimation risk match (up to a log factor) Schmidt-Hieber's lower bound for least-square regression. A similar lower bound is proven for logistic regression, confirming DNN minimax optimality.", "conclusion": "The proposed DNN estimator is minimax optimal for both regression and classification under non-i.i.d. settings, extending prior theoretical guarantees."}}
{"id": "2501.05534", "pdf": "https://arxiv.org/pdf/2501.05534", "abs": "https://arxiv.org/abs/2501.05534", "authors": ["Joschka Birk", "Frank Gaede", "Anna Hallin", "Gregor Kasieczka", "Martina Mozzanica", "Henning Rose"], "title": "OmniJet-$\u03b1_C$: Learning point cloud calorimeter simulations using generative transformers", "categories": ["hep-ph", "cs.LG", "hep-ex", "physics.ins-det"], "comment": null, "summary": "We show the first use of generative transformers for generating calorimeter\nshowers as point clouds in a high-granularity calorimeter. Using the tokenizer\nand generative part of the OmniJet-${\\alpha}$ model, we represent the hits in\nthe detector as sequences of integers. This model allows variable-length\nsequences, which means that it supports realistic shower development and does\nnot need to be conditioned on the number of hits. Since the tokenization\nrepresents the showers as point clouds, the model learns the geometry of the\nshowers without being restricted to any particular voxel grid.", "AI": {"tldr": "Generative transformers are used to create calorimeter showers as point clouds, leveraging the OmniJet-\u03b1 model for variable-length sequences without voxel grid constraints.", "motivation": "To enable realistic shower development in high-granularity calorimeters without conditioning on hit count or grid restrictions.", "method": "Uses OmniJet-\u03b1's tokenizer and generative part to represent hits as integer sequences, learning shower geometry as point clouds.", "result": "The model supports variable-length sequences and realistic shower generation, free from voxel grid limitations.", "conclusion": "Generative transformers effectively model calorimeter showers as point clouds, offering flexibility and realism."}}
{"id": "2501.18423", "pdf": "https://arxiv.org/pdf/2501.18423", "abs": "https://arxiv.org/abs/2501.18423", "authors": ["Tom Dooney", "Harsh Narola", "Stefano Bromuri", "R. Lyana Curier", "Chris Van Den Broeck", "Sarah Caudill", "Daniel Stanley Tan"], "title": "DeepExtractor: Time-domain reconstruction of signals and glitches in gravitational wave data with deep learning", "categories": ["gr-qc", "astro-ph.IM", "cs.LG", "physics.data-an", "physics.ins-det"], "comment": "24 pages, 17 figures, 3 tables", "summary": "Gravitational wave (GW) detectors, such as LIGO, Virgo, and KAGRA, detect\nfaint signals from distant astrophysical events. However, their high\nsensitivity also makes them susceptible to background noise, which can obscure\nthese signals. This noise often includes transient artifacts called 'glitches',\nthat can mimic genuine astrophysical signals or mask their true\ncharacteristics. In this study, we present DeepExtractor, a deep learning\nframework that is designed to reconstruct signals and glitches with power\nexceeding interferometer noise, regardless of their source. We design\nDeepExtractor to model the inherent noise distribution of GW detectors,\nfollowing conventional assumptions that the noise is Gaussian and stationary\nover short time scales. It operates by predicting and subtracting the noise\ncomponent of the data, retaining only the clean reconstruction of signal or\nglitch. We focus on applications related to glitches and validate\nDeepExtractor's effectiveness through three experiments: (1) reconstructing\nsimulated glitches injected into simulated detector noise, (2) comparing its\nperformance with the state-of-the-art BayesWave algorithm, and (3) analyzing\nreal data from the Gravity Spy dataset to demonstrate effective glitch\nsubtraction from LIGO strain data. We further demonstrate its potential by\nreconstructing three real GW events from LIGO's third observing run, without\nbeing trained on GW waveforms. Our proposed model achieves a median mismatch of\nonly 0.9% for simulated glitches, outperforming several deep learning\nbaselines. Additionally, DeepExtractor surpasses BayesWave in glitch recovery,\noffering a dramatic computational speedup by reconstructing one glitch sample\nin approximately 0.1 seconds on a CPU, compared to BayesWave's processing time\nof approximately one hour per glitch.", "AI": {"tldr": "DeepExtractor, a deep learning framework, effectively reconstructs gravitational wave signals and glitches, outperforming existing methods like BayesWave in speed and accuracy.", "motivation": "Gravitational wave detectors are sensitive to noise and glitches, which obscure astrophysical signals. DeepExtractor aims to address this by separating signals from noise.", "method": "DeepExtractor models detector noise as Gaussian and stationary, predicting and subtracting noise to reconstruct signals or glitches. It was validated through simulations, comparisons with BayesWave, and real data analysis.", "result": "DeepExtractor achieves a 0.9% mismatch for simulated glitches, outperforms BayesWave in recovery, and processes glitches 3600x faster (0.1s vs. 1 hour).", "conclusion": "DeepExtractor is a promising tool for improving gravitational wave data analysis, offering high accuracy and computational efficiency."}}
{"id": "2502.01916", "pdf": "https://arxiv.org/pdf/2502.01916", "abs": "https://arxiv.org/abs/2502.01916", "authors": ["Tim-Lukas Habich", "Aran Mohammad", "Simon F. G. Ehlers", "Martin Bensch", "Thomas Seel", "Moritz Schappler"], "title": "Generalizable and Fast Surrogates: Model Predictive Control of Articulated Soft Robots using Physics-Informed Neural Networks", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Soft robots can revolutionize several applications with high demands on\ndexterity and safety. When operating these systems, real-time estimation and\ncontrol require fast and accurate models. However, prediction with\nfirst-principles (FP) models is slow, and learned black-box models have poor\ngeneralizability. Physics-informed machine learning offers excellent advantages\nhere, but it is currently limited to simple, often simulated systems without\nconsidering changes after training. We propose physics-informed neural networks\n(PINNs) for articulated soft robots (ASRs) with a focus on data efficiency. The\namount of expensive real-world training data is reduced to a minimum -- one\ndataset in one system domain. Two hours of data in different domains are used\nfor a comparison against two gold-standard approaches: In contrast to a\nrecurrent neural network, the PINN provides a high generalizability. The\nprediction speed of an accurate FP model is exceeded with the PINN by up to a\nfactor of 467 at slightly reduced accuracy. This enables nonlinear model\npredictive control (MPC) of a pneumatic ASR. Accurate position tracking with\nthe MPC running at 47 Hz is achieved in six dynamic experiments.", "AI": {"tldr": "PINNs for articulated soft robots improve data efficiency and generalizability, enabling fast and accurate real-time control.", "motivation": "Soft robots need fast, accurate models for real-time estimation and control, but existing methods (FP models and black-box models) are slow or lack generalizability.", "method": "Proposed physics-informed neural networks (PINNs) for articulated soft robots, minimizing real-world training data (one dataset per domain). Compared to RNN and FP models.", "result": "PINN outperforms RNN in generalizability and FP model in speed (467x faster) with slightly reduced accuracy. Enabled 47 Hz MPC for accurate position tracking.", "conclusion": "PINNs offer a promising solution for efficient, generalizable, and fast modeling of soft robots, enabling real-time control applications."}}
{"id": "2502.13961", "pdf": "https://arxiv.org/pdf/2502.13961", "abs": "https://arxiv.org/abs/2502.13961", "authors": ["Yatin Dandi", "Luca Pesce", "Lenka Zdeborov\u00e1", "Florent Krzakala"], "title": "The Computational Advantage of Depth: Learning High-Dimensional Hierarchical Functions with Gradient Descent", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Understanding the advantages of deep neural networks trained by gradient\ndescent (GD) compared to shallow models remains an open theoretical challenge.\nIn this paper, we introduce a class of target functions (single and multi-index\nGaussian hierarchical targets) that incorporate a hierarchy of latent subspace\ndimensionalities. This framework enables us to analytically study the learning\ndynamics and generalization performance of deep networks compared to shallow\nones in the high-dimensional limit. Specifically, our main theorem shows that\nfeature learning with GD successively reduces the effective dimensionality,\ntransforming a high-dimensional problem into a sequence of lower-dimensional\nones. This enables learning the target function with drastically less samples\nthan with shallow networks. While the results are proven in a controlled\ntraining setting, we also discuss more common training procedures and argue\nthat they learn through the same mechanisms.", "AI": {"tldr": "Deep networks trained by GD learn hierarchical targets more efficiently than shallow networks by reducing problem dimensionality, requiring fewer samples.", "motivation": "To understand why deep networks outperform shallow ones when trained with gradient descent, focusing on hierarchical target functions.", "method": "Introduces Gaussian hierarchical targets to analyze learning dynamics and generalization in high dimensions, proving a theorem on dimensionality reduction.", "result": "Deep networks reduce effective dimensionality, enabling learning with fewer samples than shallow networks.", "conclusion": "Deep networks' hierarchical learning mechanism is key to their efficiency, even in common training settings."}}
{"id": "2502.16849", "pdf": "https://arxiv.org/pdf/2502.16849", "abs": "https://arxiv.org/abs/2502.16849", "authors": ["Taj Jones-McCormick", "Aukosh Jagannath", "Subhabrata Sen"], "title": "Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Unsupervised pre-training and transfer learning are commonly used techniques\nto initialize training algorithms for neural networks, particularly in settings\nwith limited labeled data. In this paper, we study the effects of unsupervised\npre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a\nsingle-layer neural network via online stochastic gradient descent. We\nestablish that pre-training and transfer learning (under concept shift) reduce\nsample complexity by polynomial factors (in the dimension) under very general\nassumptions. We also uncover some surprising settings where pre-training grants\nexponential improvement over random initialization in terms of sample\ncomplexity.", "AI": {"tldr": "Unsupervised pre-training and transfer learning reduce sample complexity in high-dimensional supervised learning, sometimes exponentially.", "motivation": "To understand how unsupervised pre-training and transfer learning impact sample complexity in neural network training, especially with limited labeled data.", "method": "Analyze single-layer neural network training via online stochastic gradient descent under concept shift.", "result": "Pre-training and transfer learning reduce sample complexity polynomially, with some cases showing exponential improvement.", "conclusion": "These techniques are highly effective for reducing sample complexity in high-dimensional learning tasks."}}
{"id": "2502.18470", "pdf": "https://arxiv.org/pdf/2502.18470", "abs": "https://arxiv.org/abs/2502.18470", "authors": ["Dazhou Yu", "Riyang Bao", "Ruiyu Ning", "Jinghong Peng", "Gengchen Mai", "Liang Zhao"], "title": "Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Geospatial Reasoning Questions", "categories": ["cs.IR", "cs.ET", "cs.LG"], "comment": null, "summary": "Answering real-world geospatial questions--such as finding restaurants along\na travel route or amenities near a landmark--requires reasoning over both\ngeographic relationships and semantic user intent. However, existing large\nlanguage models (LLMs) lack spatial computing capabilities and access to\nup-to-date, ubiquitous real-world geospatial data, while traditional geospatial\nsystems fall short in interpreting natural language. To bridge this gap, we\nintroduce Spatial-RAG, a Retrieval-Augmented Generation (RAG) framework\ndesigned for geospatial question answering. Spatial-RAG integrates structured\nspatial databases with LLMs via a hybrid spatial retriever that combines sparse\nspatial filtering and dense semantic matching. It formulates the answering\nprocess as a multi-objective optimization over spatial and semantic relevance,\nidentifying Pareto-optimal candidates and dynamically selecting the best\nresponse based on user intent. Experiments across multiple tourism and\nmap-based QA datasets show that Spatial-RAG significantly improves accuracy,\nprecision, and ranking performance over strong baselines.", "AI": {"tldr": "Spatial-RAG is a framework combining geospatial data with LLMs for accurate geospatial question answering.", "motivation": "Existing LLMs lack spatial computing and up-to-date geospatial data, while traditional systems fail at natural language interpretation.", "method": "Spatial-RAG integrates spatial databases with LLMs using a hybrid spatial retriever for sparse spatial filtering and dense semantic matching, optimizing for spatial and semantic relevance.", "result": "Experiments show Spatial-RAG improves accuracy, precision, and ranking over baselines.", "conclusion": "Spatial-RAG effectively bridges the gap between geospatial systems and natural language understanding."}}
{"id": "2503.03649", "pdf": "https://arxiv.org/pdf/2503.03649", "abs": "https://arxiv.org/abs/2503.03649", "authors": ["Andrei V. Ermolaev", "Mathilde Hary", "Lev Leybov", "Piotr Ryczkowski", "Anas Skalli", "Daniel Brunner", "Go\u00ebry Genty", "John M. Dudley"], "title": "Limits of nonlinear and dispersive fiber propagation for an optical fiber-based extreme learning machine", "categories": ["physics.optics", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using the\nMNIST handwritten digit dataset as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. For this dataset and with quantum noise limited input, test\naccuracies of : over 91% and 93% are found for propagation in the anomalous and\nnormal dispersion regimes respectively. Our results also suggest that quantum\nnoise on the input pulses introduces an intrinsic penalty to ELM performance.", "AI": {"tldr": "A nonlinear Schr\u00f6dinger equation model simulates an optical fiber-based ELM, achieving over 91% and 93% accuracy on MNIST in anomalous and normal dispersion regimes, with quantum noise impacting performance.", "motivation": "To explore how optical fiber propagation dynamics and parameters affect ELM accuracy, using MNIST as a benchmark.", "method": "Simulation of a generalized nonlinear Schr\u00f6dinger equation model for ELM, analyzing spectral encoding, readout, noise, and dispersion regimes.", "result": "Test accuracies of over 91% (anomalous dispersion) and 93% (normal dispersion) on MNIST, with quantum noise reducing performance.", "conclusion": "Quantum noise intrinsically penalizes ELM performance, while dispersion regimes influence accuracy."}}
{"id": "2503.10873", "pdf": "https://arxiv.org/pdf/2503.10873", "abs": "https://arxiv.org/abs/2503.10873", "authors": ["Pedro Pessoa", "Paul Campitelli", "Douglas P. Shepherd", "S. Banu Ozkan", "Steve Press\u00e9"], "title": "Mamba time series forecasting with uncertainty quantification", "categories": ["stat.ML", "cs.LG", "nlin.CD"], "comment": null, "summary": "State space models, such as Mamba, have recently garnered attention in time\nseries forecasting due to their ability to capture sequence patterns. However,\nin electricity consumption benchmarks, Mamba forecasts exhibit a mean error of\napproximately 8\\%. Similarly, in traffic occupancy benchmarks, the mean error\nreaches 18\\%. This discrepancy leaves us to wonder whether the prediction is\nsimply inaccurate or falls within error given spread in historical data. To\naddress this limitation, we propose a method to quantify the predictive\nuncertainty of Mamba forecasts. Here, we propose a dual-network framework based\non the Mamba architecture for probabilistic forecasting, where one network\ngenerates point forecasts while the other estimates predictive uncertainty by\nmodeling variance. We abbreviate our tool, Mamba with probabilistic time series\nforecasting, as Mamba-ProbTSF and the code for its implementation is available\non GitHub (https://github.com/PessoaP/Mamba-ProbTSF). Evaluating this approach\non synthetic and real-world benchmark datasets, we find Kullback-Leibler\ndivergence between the learned distributions and the data--which, in the limit\nof infinite data, should converge to zero if the model correctly captures the\nunderlying probability distribution--reduced to the order of $10^{-3}$ for\nsynthetic data and $10^{-1}$ for real-world benchmark, demonstrating its\neffectiveness. We find that in both the electricity consumption and traffic\noccupancy benchmark, the true trajectory stays within the predicted uncertainty\ninterval at the two-sigma level about 95\\% of the time. We end with a\nconsideration of potential limitations, adjustments to improve performance, and\nconsiderations for applying this framework to processes for purely or largely\nstochastic dynamics where the stochastic changes accumulate, as observed for\nexample in pure Brownian motion or molecular dynamics trajectories.", "AI": {"tldr": "The paper introduces Mamba-ProbTSF, a dual-network framework based on Mamba for probabilistic forecasting, addressing uncertainty in time series predictions.", "motivation": "Existing Mamba models show high mean errors in electricity and traffic benchmarks, raising questions about prediction accuracy versus inherent data variability.", "method": "A dual-network framework: one for point forecasts, the other for estimating predictive uncertainty by modeling variance.", "result": "Reduced Kullback-Leibler divergence (10^-3 for synthetic, 10^-1 for real-world data) and 95% true trajectory coverage within predicted uncertainty intervals.", "conclusion": "Mamba-ProbTSF effectively quantifies uncertainty, with potential for further improvements in stochastic dynamics applications."}}
{"id": "2504.03943", "pdf": "https://arxiv.org/pdf/2504.03943", "abs": "https://arxiv.org/abs/2504.03943", "authors": ["Imon Mia", "Armi Tiihonen", "Anna Ernst", "Anusha Srivastava", "Tonio Buonassisi", "William Vandenberghe", "Julia W. P. Hsu"], "title": "Multi-Variable Batch Bayesian Optimization in Materials Research: Synthetic Data Analysis of Noise Sensitivity and Problem Landscape Effects", "categories": ["stat.ML", "cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Bayesian Optimization (BO) machine learning method is increasingly used to\nguide experimental optimization tasks in materials science. To emulate the\nlarge number of input variables and noise-containing results in experimental\nmaterials research, we perform batch BO simulation of six design variables with\na range of noise levels. Two test cases relevant for materials science problems\nare examined: a needle-in-a-haystack case (Ackley function) that may be\nencountered in, e.g., molecule optimizations, and a smooth landscape with a\nlocal optimum in addition to the global optimum (Hartmann function) that may be\nencountered in, e.g., material composition optimization. We show learning\ncurves, performance metrics, and visualization to effectively track the\noptimization progression and evaluate how the optimization outcomes are\naffected by noise, batch-picking method, choice of acquisition function, and\nexploration hyperparameter values. We find that the effects of noise depend on\nthe problem landscape: noise degrades the optimization results of a\nneedle-in-a-haystack search (Ackley) dramatically more. However, with\nincreasing noise, we observe an increasing probability of landing on the local\noptimum in Hartmann. Therefore, prior knowledge of the problem domain structure\nand noise level is essential when designing BO for materials research\nexperiments. Synthetic data studies -- with known ground truth and controlled\nnoise levels -- enable us to isolate and evaluate the impact of different batch\nBO components, {\\it e.g.}, acquisition policy, objective metrics, and\nhyperparameter values, before transitioning to the inherent uncertainties of\nreal experimental systems. The results and methodology of this study will\nfacilitate a greater utilization of BO in guiding experimental materials\nresearch, specifically in settings with a large number of design variables to\noptimize.", "AI": {"tldr": "Bayesian Optimization (BO) is tested for materials science experiments with noisy data, showing noise impacts vary by problem type (Ackley vs. Hartmann functions). Prior knowledge of noise and problem structure is crucial.", "motivation": "To address challenges in experimental materials research, such as noise and multiple variables, by evaluating BO's effectiveness in noisy environments.", "method": "Batch BO simulations with six design variables and varying noise levels, tested on Ackley (needle-in-haystack) and Hartmann (smooth landscape) functions.", "result": "Noise degrades Ackley optimization more, while Hartmann shows higher chances of landing on local optima with noise. Problem structure and noise level knowledge are vital.", "conclusion": "Synthetic data studies help isolate BO components' impacts, aiding BO's application in real experimental materials research with many variables."}}
{"id": "2504.17656", "pdf": "https://arxiv.org/pdf/2504.17656", "abs": "https://arxiv.org/abs/2504.17656", "authors": ["Ayush Jain", "Rampi Ramprasad"], "title": "polyGen: A Learning Framework for Atomic-level Polymer Structure Generation", "categories": ["cs.CE", "cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Synthetic polymeric materials underpin fundamental technologies in the\nenergy, electronics, consumer goods, and medical sectors, yet their development\nstill suffers from prolonged design timelines. Although polymer informatics\ntools have supported speedup, polymer simulation protocols continue to face\nsignificant challenges in the on-demand generation of realistic 3D atomic\nstructures that respect conformational diversity. Generative algorithms for 3D\nstructures of inorganic crystals, bio-polymers, and small molecules exist, but\nhave not addressed synthetic polymers because of challenges in representation\nand dataset constraints. In this work, we introduce polyGen, the first\ngenerative model designed specifically for polymer structures from minimal\ninputs such as the repeat unit chemistry alone. polyGen combines graph-based\nencodings with a latent diffusion transformer using positional biased attention\nfor realistic conformation generation. Given the limited dataset of 3,855\nDFT-optimized polymer structures, we incorporate joint training with small\nmolecule data to enhance generation quality. We also establish structure\nmatching criteria to benchmark our approach on this novel problem. polyGen\novercomes the limitations of traditional crystal structure prediction methods\nfor polymers, successfully generating realistic and diverse linear and branched\nconformations, with promising performance even on challenging large repeat\nunits. As the first atomic-level proof-of-concept capturing intrinsic polymer\nflexibility, it marks a new capability in material structure generation.", "AI": {"tldr": "polyGen is a generative model for synthetic polymer structures, addressing challenges in representation and dataset constraints by combining graph-based encodings with a latent diffusion transformer.", "motivation": "Traditional polymer simulation protocols struggle with generating realistic 3D atomic structures efficiently, especially for synthetic polymers, due to representation and dataset limitations.", "method": "polyGen uses graph-based encodings and a latent diffusion transformer with positional biased attention, trained jointly with small molecule data to enhance generation quality.", "result": "The model successfully generates realistic and diverse polymer conformations, including linear and branched structures, even for large repeat units.", "conclusion": "polyGen represents a breakthrough in polymer structure generation, offering a scalable solution for material design."}}
{"id": "2504.21199", "pdf": "https://arxiv.org/pdf/2504.21199", "abs": "https://arxiv.org/abs/2504.21199", "authors": ["Terrance Liu", "Eileen Xiao", "Adam Smith", "Pratiksha Thaker", "Zhiwei Steven Wu"], "title": "Generate-then-Verify: Reconstructing Data from Limited Published Statistics", "categories": ["stat.ML", "cs.CR", "cs.LG"], "comment": "First two authors contributed equally. Remaining authors are ordered\n  alphabetically", "summary": "We study the problem of reconstructing tabular data from aggregate\nstatistics, in which the attacker aims to identify interesting claims about the\nsensitive data that can be verified with 100% certainty given the aggregates.\nSuccessful attempts in prior work have conducted studies in settings where the\nset of published statistics is rich enough that entire datasets can be\nreconstructed with certainty. In our work, we instead focus on the regime where\nmany possible datasets match the published statistics, making it impossible to\nreconstruct the entire private dataset perfectly (i.e., when approaches in\nprior work fail). We propose the problem of partial data reconstruction, in\nwhich the goal of the adversary is to instead output a $\\textit{subset}$ of\nrows and/or columns that are $\\textit{guaranteed to be correct}$. We introduce\na novel integer programming approach that first $\\textbf{generates}$ a set of\nclaims and then $\\textbf{verifies}$ whether each claim holds for all possible\ndatasets consistent with the published aggregates. We evaluate our approach on\nthe housing-level microdata from the U.S. Decennial Census release,\ndemonstrating that privacy violations can still persist even when information\npublished about such data is relatively sparse.", "AI": {"tldr": "The paper addresses partial data reconstruction from aggregate statistics when full reconstruction is impossible, proposing an integer programming method to verify correct subsets of data.", "motivation": "Prior work focused on reconstructing entire datasets from rich statistics, but this fails when many datasets match sparse aggregates. The study aims to identify verifiable subsets of data in such scenarios.", "method": "The authors introduce an integer programming approach to generate and verify claims about subsets of rows/columns guaranteed to be correct, given the aggregates.", "result": "Applied to U.S. Census housing data, the method shows privacy violations can occur even with sparse published statistics.", "conclusion": "Partial data reconstruction remains a privacy risk, highlighting vulnerabilities even when full reconstruction is impossible."}}
{"id": "2505.17836", "pdf": "https://arxiv.org/pdf/2505.17836", "abs": "https://arxiv.org/abs/2505.17836", "authors": ["Anna Van Elst", "Igor Colin", "Stephan Cl\u00e9men\u00e7on"], "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}((\\log t)/\\sqrt{t})$ rate for trimmed mean\nestimation, where by $t$ is meant the number of iterations. Moreover, we\nprovide a breakdown point analysis of \\textsc{GoTrim}. We empirically validate\nour theoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes.", "AI": {"tldr": "The paper introduces robust gossip algorithms, GoRank and GoTrim, for outlier-resistant mean estimation in decentralized networks, with proven convergence rates and empirical validation.", "motivation": "Existing gossip algorithms are vulnerable to malicious nodes, prompting the need for robust decentralized methods.", "method": "Proposes GoRank for rank estimation and GoTrim for trimmed mean estimation, with detailed convergence analysis.", "result": "Achieves O(1/t) convergence for rank estimation and O((log t)/\u221at) for trimmed mean, validated empirically.", "conclusion": "The algorithms are effective for robust decentralized estimation, with theoretical and empirical support."}}
{"id": "2505.22094", "pdf": "https://arxiv.org/pdf/2505.22094", "abs": "https://arxiv.org/abs/2505.22094", "authors": ["Tonghe Zhang", "Chao Yu", "Sichang Su", "Yu Wang"], "title": "ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning", "categories": ["cs.RO", "cs.LG"], "comment": "30 pages, 13 figures, 10 tables", "summary": "We propose ReinFlow, a simple yet effective online reinforcement learning\n(RL) framework that fine-tunes a family of flow matching policies for\ncontinuous robotic control. Derived from rigorous RL theory, ReinFlow injects\nlearnable noise into a flow policy's deterministic path, converting the flow\ninto a discrete-time Markov Process for exact and straightforward likelihood\ncomputation. This conversion facilitates exploration and ensures training\nstability, enabling ReinFlow to fine-tune diverse flow model variants,\nincluding Rectified Flow [35] and Shortcut Models [19], particularly at very\nfew or even one denoising step. We benchmark ReinFlow in representative\nlocomotion and manipulation tasks, including long-horizon planning with visual\ninput and sparse reward. The episode reward of Rectified Flow policies obtained\nan average net growth of 135.36% after fine-tuning in challenging legged\nlocomotion tasks while saving denoising steps and 82.63% of wall time compared\nto state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate\nof the Shortcut Model policies in state and visual manipulation tasks achieved\nan average net increase of 40.34% after fine-tuning with ReinFlow at four or\neven one denoising step, whose performance is comparable to fine-tuned DDIM\npolicies while saving computation time for an average of 23.20%. Project\nwebpage: https://reinflow.github.io/", "AI": {"tldr": "ReinFlow is an online RL framework that fine-tunes flow matching policies for robotic control, improving performance and efficiency compared to existing methods.", "motivation": "To enhance exploration and training stability in continuous robotic control by converting deterministic flow policies into a Markov Process for exact likelihood computation.", "method": "ReinFlow injects learnable noise into flow policies, enabling fine-tuning of variants like Rectified Flow and Shortcut Models with few denoising steps.", "result": "Achieved 135.36% reward growth in locomotion tasks and 40.34% success rate increase in manipulation tasks, while saving computation time.", "conclusion": "ReinFlow outperforms state-of-the-art methods like DPPO and DDIM, offering efficiency and performance gains in robotic control tasks."}}
{"id": "2506.03863", "pdf": "https://arxiv.org/pdf/2506.03863", "abs": "https://arxiv.org/abs/2506.03863", "authors": ["Hao Li", "Qi Lv", "Rui Shao", "Xiang Deng", "Yinchuan Li", "Jianye Hao", "Liqiang Nie"], "title": "STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted by ICML 2025 Spotlight", "summary": "Transforming complex actions into discrete skill abstractions has\ndemonstrated strong potential for robotic manipulation. Existing approaches\nmainly leverage latent variable models, e.g., VQ-VAE, to learn skill\nabstractions through learned vectors (codebooks), while they suffer from\ncodebook collapse and modeling the causal relationship between learned skills.\nTo address these limitations, we present \\textbf{S}kill \\textbf{T}raining with\n\\textbf{A}ugmented \\textbf{R}otation (\\textbf{STAR}), a framework that advances\nboth skill learning and composition to complete complex behaviors.\nSpecifically, to prevent codebook collapse, we devise rotation-augmented\nresidual skill quantization (RaRSQ). It encodes relative angles between encoder\noutputs into the gradient flow by rotation-based gradient mechanism. Points\nwithin the same skill code are forced to be either pushed apart or pulled\ncloser together depending on gradient directions. Further, to capture the\ncausal relationship between skills, we present causal skill transformer (CST)\nwhich explicitly models dependencies between skill representations through an\nautoregressive mechanism for coherent action generation. Extensive experiments\ndemonstrate the superiority of STAR on both LIBERO benchmark and realworld\ntasks, with around 12\\% improvement over the baselines.", "AI": {"tldr": "STAR introduces rotation-augmented skill quantization (RaRSQ) and causal skill transformer (CST) to improve robotic manipulation by preventing codebook collapse and modeling skill dependencies.", "motivation": "Existing latent variable models for skill abstraction suffer from codebook collapse and lack causal skill relationships.", "method": "STAR uses RaRSQ to encode relative angles via rotation-based gradients and CST to model skill dependencies autoregressively.", "result": "STAR achieves ~12% improvement over baselines on LIBERO benchmark and real-world tasks.", "conclusion": "STAR effectively enhances skill learning and composition for complex robotic behaviors."}}
{"id": "2506.05408", "pdf": "https://arxiv.org/pdf/2506.05408", "abs": "https://arxiv.org/abs/2506.05408", "authors": ["Jonathan Scott", "Christoph H. Lampert", "David Saulpic"], "title": "Differentially Private Federated $k$-Means Clustering with Server-Side Data", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Clustering is a cornerstone of data analysis that is particularly suited to\nidentifying coherent subgroups or substructures in unlabeled data, as are\ngenerated continuously in large amounts these days. However, in many cases\ntraditional clustering methods are not applicable, because data are\nincreasingly being produced and stored in a distributed way, e.g. on edge\ndevices, and privacy concerns prevent it from being transferred to a central\nserver. To address this challenge, we present FedDP-KMeans, a new algorithm for\n$k$-means clustering that is fully-federated as well as differentially private.\nOur approach leverages (potentially small and out-of-distribution) server-side\ndata to overcome the primary challenge of differentially private clustering\nmethods: the need for a good initialization. Combining our initialization with\na simple federated DP-Lloyds algorithm we obtain an algorithm that achieves\nexcellent results on synthetic and real-world benchmark tasks. We also provide\na theoretical analysis of our method that provides bounds on the convergence\nspeed and cluster identification success.", "AI": {"tldr": "FedDP-KMeans is a federated and differentially private k-means clustering algorithm that addresses challenges in distributed data and privacy concerns by leveraging server-side data for initialization and combining it with a federated DP-Lloyds algorithm.", "motivation": "Traditional clustering methods fail for distributed data due to privacy concerns and lack of centralization. FedDP-KMeans aims to solve this by enabling clustering without transferring data to a central server while ensuring privacy.", "method": "The algorithm uses server-side data for initialization and combines it with a federated DP-Lloyds algorithm to perform k-means clustering in a fully-federated and differentially private manner.", "result": "FedDP-KMeans achieves excellent performance on synthetic and real-world benchmarks, with theoretical bounds on convergence speed and cluster identification success.", "conclusion": "FedDP-KMeans effectively addresses the challenges of distributed data and privacy, providing a practical solution for federated and differentially private clustering."}}
{"id": "2506.08033", "pdf": "https://arxiv.org/pdf/2506.08033", "abs": "https://arxiv.org/abs/2506.08033", "authors": ["Axel TahmasebiMoradi", "Vincent Ren", "Benjamin Le-Creurer", "Chetra Mang", "Mouadh Yagoubi"], "title": "Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "Aiming to reduce the computational cost of numerical simulations, a\nconvolutional neural network (CNN) and a multi-layer perceptron (MLP) are\nintroduced to build a surrogate model to approximate radiative heat transfer\nsolutions in a 2-D walled domain with participative gases. The originality of\nthis work lays in the adaptation of the inputs of the problem (gas and wall\nproperties) in order to fit with the CNN architecture, more commonly used for\nimage processing. Two precision datasets have been created with the classical\nsolver, ICARUS2D, that uses the discrete transfer radiation method with the\nstatistical narrow bands model. The performance of the CNN architecture is\ncompared to a more classical MLP architecture in terms of speed and accuracy.\nThanks to Optuna, all results are obtained using the optimized hyper parameters\nnetworks. The results show a significant speedup with industrially acceptable\nrelative errors compared to the classical solver for both architectures.\nAdditionally, the CNN outperforms the MLP in terms of precision and is more\nrobust and stable to changes in hyper-parameters. A performance analysis on the\ndataset size of the samples have also been carried out to gain a deeper\nunderstanding of the model behavior.", "AI": {"tldr": "A CNN and MLP surrogate model is developed to approximate radiative heat transfer solutions, reducing computational costs while maintaining acceptable accuracy.", "motivation": "To reduce computational costs in numerical simulations of radiative heat transfer in 2-D domains with participative gases.", "method": "Adapts inputs for CNN (typically used for images) and compares CNN and MLP performance using datasets from ICARUS2D solver, with hyper-parameters optimized via Optuna.", "result": "Both CNN and MLP achieve significant speedup with acceptable errors; CNN outperforms MLP in precision, robustness, and stability.", "conclusion": "The CNN-based surrogate model is efficient and accurate, offering a viable alternative to classical solvers for radiative heat transfer problems."}}
{"id": "2506.08065", "pdf": "https://arxiv.org/pdf/2506.08065", "abs": "https://arxiv.org/abs/2506.08065", "authors": ["Ye Zhu", "Duo Xu", "Zhiwei Deng", "Jonathan C. Tan", "Olga Russakovsky"], "title": "Dynamic Diffusion Schr\u00f6dinger Bridge in Astrophysical Observational Inversions", "categories": ["astro-ph.IM", "cs.LG"], "comment": "Preprint. Code will be available at\n  https://github.com/L-YeZhu/AstroDSB", "summary": "We study Diffusion Schr\\\"odinger Bridge (DSB) models in the context of\ndynamical astrophysical systems, specifically tackling observational inverse\nprediction tasks within Giant Molecular Clouds (GMCs) for star formation. We\nintroduce the Astro-DSB model, a variant of DSB with the pairwise domain\nassumption tailored for astrophysical dynamics. By investigating its learning\nprocess and prediction performance in both physically simulated data and in\nreal observations (the Taurus B213 data), we present two main takeaways. First,\nfrom the astrophysical perspective, our proposed paired DSB method improves\ninterpretability, learning efficiency, and prediction performance over\nconventional astrostatistical and other machine learning methods. Second, from\nthe generative modeling perspective, probabilistic generative modeling reveals\nimprovements over discriminative pixel-to-pixel modeling in Out-Of-Distribution\n(OOD) testing cases of physical simulations with unseen initial conditions and\ndifferent dominant physical processes. Our study expands research into\ndiffusion models beyond the traditional visual synthesis application and\nprovides evidence of the models' learning abilities beyond pure data\nstatistics, paving a path for future physics-aware generative models which can\nalign dynamics between machine learning and real (astro)physical systems.", "AI": {"tldr": "The paper introduces Astro-DSB, a variant of Diffusion Schr\u00f6dinger Bridge models, for astrophysical systems, showing improved interpretability and performance in star formation prediction tasks.", "motivation": "To address observational inverse prediction tasks in Giant Molecular Clouds (GMCs) for star formation, leveraging diffusion models beyond traditional applications.", "method": "Proposes Astro-DSB, a paired DSB variant tailored for astrophysical dynamics, tested on simulated and real data (Taurus B213).", "result": "Astro-DSB outperforms conventional methods in interpretability, efficiency, and prediction, especially in OOD cases.", "conclusion": "The study demonstrates the potential of physics-aware generative models, expanding diffusion models' applications beyond visual synthesis."}}
{"id": "2506.08381", "pdf": "https://arxiv.org/pdf/2506.08381", "abs": "https://arxiv.org/abs/2506.08381", "authors": ["He Yang", "Fei Ren", "Hai-Sui Yu", "Xueyu Geng", "Pei-Zhi Zhuang"], "title": "TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses", "categories": ["physics.geo-ph", "cs.LG"], "comment": null, "summary": "Accuracy and efficiency of the conventional physics-informed neural network\n(PINN) need to be improved before it can be a competitive alternative for soil\nconsolidation analyses. This paper aims to overcome these limitations by\nproposing a highly accurate and efficient physics-informed machine learning\n(PIML) approach, termed time-stepping physics-informed extreme learning machine\n(TS-PIELM). In the TS-PIELM framework the consolidation process is divided into\nnumerous time intervals, which helps overcome the limitation of PIELM in\nsolving differential equations with sharp gradients. To accelerate network\ntraining, the solution is approximated by a single-layer feedforward extreme\nlearning machine (ELM), rather than using a fully connected neural network in\nPINN. The input layer weights of the ELM network are generated randomly and\nfixed during the training process. Subsequently, the output layer weights are\ndirectly computed by solving a system of linear equations, which significantly\nenhances the training efficiency compared to the time-consuming gradient\ndescent method in PINN. Finally, the superior performance of TS-PIELM is\ndemonstrated by solving three typical Terzaghi consolidation problems. Compared\nto PINN, results show that the computational efficiency and accuracy of the\nnovel TS-PIELM framework are improved by more than 1000 times and 100 times for\none-dimensional cases, respectively. This paper provides compelling evidence\nthat PIML can be a powerful tool for computational geotechnics.", "AI": {"tldr": "The paper proposes TS-PIELM, a time-stepping physics-informed extreme learning machine, to improve accuracy and efficiency over conventional PINN for soil consolidation analyses.", "motivation": "Overcome limitations of PINN in accuracy and efficiency for soil consolidation analyses.", "method": "Divides consolidation into time intervals, uses single-layer ELM with random fixed input weights, and computes output weights via linear equations.", "result": "TS-PIELM improves computational efficiency by 1000x and accuracy by 100x over PINN for 1D cases.", "conclusion": "PIML, exemplified by TS-PIELM, is a powerful tool for computational geotechnics."}}
{"id": "2506.08528", "pdf": "https://arxiv.org/pdf/2506.08528", "abs": "https://arxiv.org/abs/2506.08528", "authors": ["Yu Guan", "Zhiyu Yin", "Haoyu Chen", "Sheng Cheng", "Chaojie Yang", "Kun Qian", "Tianyin Xu", "Yang Zhang", "Hanyu Zhao", "Yong Li", "Wei Lin", "Dennis Cai", "Ennan Zhai"], "title": "PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production", "categories": ["cs.DC", "cs.LG", "cs.OS"], "comment": null, "summary": "Troubleshooting performance problems of large model training (LMT) is\nimmensely challenging, due to unprecedented scales of modern GPU clusters, the\ncomplexity of software-hardware interactions, and the data intensity of the\ntraining process. Existing troubleshooting approaches designed for traditional\ndistributed systems or datacenter networks fall short and can hardly apply to\nreal-world training systems. In this paper, we present PerfTracker, the first\nonline troubleshooting system utilizing fine-grained profiling, to diagnose\nperformance issues of large-scale model training in production. PerfTracker can\ndiagnose performance issues rooted in both hardware (e.g., GPUs and their\ninterconnects) and software (e.g., Python functions and GPU operations). It\nscales to LMT on modern GPU clusters. PerfTracker effectively summarizes\nruntime behavior patterns of fine-grained LMT functions via online profiling,\nand leverages differential observability to localize the root cause with\nminimal production impact. PerfTracker has been deployed as a production\nservice for large-scale GPU clusters of O(10, 000) GPUs (product homepage\nhttps://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).\nIt has been used to diagnose a variety of difficult performance issues.", "AI": {"tldr": "PerfTracker is an online troubleshooting system for diagnosing performance issues in large-scale model training (LMT) on GPU clusters, addressing hardware and software complexities.", "motivation": "Existing troubleshooting methods for traditional systems fail to address the scale and complexity of modern LMT, necessitating a specialized solution.", "method": "PerfTracker uses fine-grained profiling and differential observability to diagnose issues in hardware (GPUs, interconnects) and software (Python functions, GPU operations).", "result": "Deployed on clusters with O(10,000) GPUs, PerfTracker successfully diagnoses various performance issues in production.", "conclusion": "PerfTracker is a scalable and effective solution for troubleshooting LMT performance problems in real-world GPU clusters."}}
{"id": "2506.08558", "pdf": "https://arxiv.org/pdf/2506.08558", "abs": "https://arxiv.org/abs/2506.08558", "authors": ["William de Vazelhes", "Xiao-Tong Yuan", "Bin Gu"], "title": "Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees", "categories": ["math.OC", "cs.LG"], "comment": "Accepted for publication at ICML 2025", "summary": "In sparse optimization, enforcing hard constraints using the $\\ell_0$\npseudo-norm offers advantages like controlled sparsity compared to convex\nrelaxations. However, many real-world applications demand not only sparsity\nconstraints but also some extra constraints. While prior algorithms have been\ndeveloped to address this complex scenario with mixed combinatorial and convex\nconstraints, they typically require the closed form projection onto the mixed\nconstraints which might not exist, and/or only provide local guarantees of\nconvergence which is different from the global guarantees commonly sought in\nsparse optimization. To fill this gap, in this paper, we study the problem of\nsparse optimization with extra support-preserving constraints commonly\nencountered in the literature. We present a new variant of iterative\nhard-thresholding algorithm equipped with a two-step consecutive projection\noperator customized for these mixed constraints, serving as a simple\nalternative to the Euclidean projection onto the mixed constraint. By\nintroducing a novel trade-off between sparsity relaxation and sub-optimality,\nwe provide global guarantees in objective value for the output of our\nalgorithm, in the deterministic, stochastic, and zeroth-order settings, under\nthe conventional restricted strong-convexity/smoothness assumptions. As a\nfundamental contribution in proof techniques, we develop a novel extension of\nthe classic three-point lemma to the considered two-step non-convex projection\noperator, which allows us to analyze the convergence in objective value in an\nelegant way that has not been possible with existing techniques. In the\nzeroth-order case, such technique also improves upon the state-of-the-art\nresult from de Vazelhes et. al. (2022), even in the case without additional\nconstraints, by allowing us to remove a non-vanishing system error present in\ntheir work.", "AI": {"tldr": "A new iterative hard-thresholding algorithm with a two-step projection operator is proposed for sparse optimization with extra constraints, offering global convergence guarantees under standard assumptions.", "motivation": "Existing methods for sparse optimization with mixed constraints lack global guarantees or require impractical projections. This work addresses these limitations.", "method": "A variant of iterative hard-thresholding with a customized two-step projection operator is introduced, balancing sparsity relaxation and sub-optimality.", "result": "The algorithm provides global guarantees in deterministic, stochastic, and zeroth-order settings, improving upon prior work.", "conclusion": "The proposed method fills a gap in sparse optimization by offering a practical solution with global convergence guarantees and novel proof techniques."}}
